<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami---1039">TPAMI - 1039</h2>
<ul>
<li><details>
<summary>
(2023). Event transformer<span
class="math inline"><sup>+</sup></span>. A multi-purpose solution for
efficient event data processing. <em>TPAMI</em>, <em>45</em>(12),
16013–16020. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras record sparse illumination changes with high temporal resolution and high dynamic range. Thanks to their sparse recording and low consumption, they are increasingly used in applications such as AR/VR and autonomous driving. Current top-performing methods often ignore specific event-data properties, leading to the development of generic but computationally expensive algorithms, while event-aware methods do not perform as well. We propose Event Transformer$^+$+ , that improves our seminal work EvT with a refined patch-based event representation and a more robust backbone to achieve more accurate results, while still benefiting from event-data sparsity to increase its efficiency. Additionally, we show how our system can work with different data modalities and propose specific output heads, for event-stream classification (i.e. action recognition) and per-pixel predictions (dense depth estimation). Evaluation results show better performance to the state-of-the-art while requiring minimal computation resources, both on GPU and CPU.},
  archive      = {J_TPAMI},
  author       = {Alberto Sabater and Luis Montesano and Ana C. Murillo},
  doi          = {10.1109/TPAMI.2023.3311336},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {16013-16020},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Event transformer$^+$. a multi-purpose solution for efficient event data processing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised semantic segmentation via box-driven
masking and filling rate shifting. <em>TPAMI</em>, <em>45</em>(12),
15996–16012. (<a
href="https://doi.org/10.1109/TPAMI.2023.3301302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has achieved huge progress via adopting deep Fully Convolutional Networks (FCN). However, the performance of FCN-based models severely rely on the amounts of pixel-level annotations which are expensive and time-consuming. Considering that bounding boxes also contain abundant semantic and objective information, an intuitive solution is to learn the segmentation with weak supervisions from the bounding boxes. How to make full use of the class-level and region-level supervisions from bounding boxes to estimate the uncertain regions is the critical challenge for the weakly supervised learning task. In this paper, we propose a mixture model to address this problem. First, we introduce a box-driven class-wise masking model (BCM) to remove irrelevant regions of each class. Moreover, based on the pixel-level segment proposal generated from the bounding box supervision, we calculate the mean filling rates of each class to serve as an important prior cue to guide the model ignoring the wrongly labeled pixels in proposals. To realize the more fine-grained supervision at instance-level, we further propose the anchor-based filling rate shifting module. Unlike previous methods that directly train models with the generated noisy proposals, our method can adjust the model learning dynamically with the adaptive segmentation loss. Thus it can help reduce the negative impacts from wrongly labeled proposals. Besides, based on the learned high-quality proposals with above pipeline, we explore to further boost the performance through two-stage learning. The proposed method is evaluated on the challenging PASCAL VOC 2012 benchmark and achieves 74.9 $\%$ and 76.4 $\%$ mean IoU accuracy under weakly and semi-supervised modes, respectively. Extensive experimental results show that the proposed method is effective and is on par with, or even better than current state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Chunfeng Song and Wanli Ouyang and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2023.3301302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15996-16012},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Weakly supervised semantic segmentation via box-driven masking and filling rate shifting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wavelet approximation-aware residual network for single
image deraining. <em>TPAMI</em>, <em>45</em>(12), 15979–15995. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been made great progress on single image deraining based on deep convolutional neural networks (CNNs). In most existing deep deraining methods, CNNs aim to learn a direct mapping from rainy images to clean rain-less images, and their architectures are becoming more and more complex. However, due to the limitation of mixing rain with object edges and background, it is difficult to separate rain and object/background, and the edge details of the image cannot be effectively recovered in the reconstruction process. To address this problem, we propose a novel wavelet approximation-aware residual network (WAAR), wherein rain is effectively removed from both low-frequency structures and high-frequency details at each level separately, especially in low-frequency sub-images at each level. After wavelet transform, we propose novel approximation aware (AAM) and approximation level blending (ALB) mechanisms to further aid the low-frequency networks at each level recover the structure and texture of low-frequency sub-images recursively, while the high frequency network can effectively eliminate rain streaks through block connection and achieve different degrees of edge detail enhancement by adjusting hyperparameters. In addition, we also introduce block connection to enrich the high-frequency details in the high-frequency network, which is favorable for obtaining potential interdependencies between high- and low-frequency features. Experimental results indicate that the proposed WAAR exhibits strong performance in reconstructing clean and rain-free images, recovering real and undistorted texture structures, and enhancing image edges in comparison with the state-of-the-art approaches on synthetic and real image datasets. It shows the effectiveness of our method, especially on image edges and texture details.},
  archive      = {J_TPAMI},
  author       = {Wei-Yen Hsu and Wei-Chi Chang},
  doi          = {10.1109/TPAMI.2023.3307666},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15979-15995},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Wavelet approximation-aware residual network for single image deraining},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vertical layering of quantized neural networks for
heterogeneous inference. <em>TPAMI</em>, <em>45</em>(12), 15964–15978.
(<a href="https://doi.org/10.1109/TPAMI.2023.3319045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although considerable progress has been obtained in neural network quantization for efficient inference, existing methods are not scalable to heterogeneous devices as one dedicated model needs to be trained, transmitted, and stored for one specific hardware setting, incurring considerable costs in model training and maintenance. In this paper, we study a new vertical-layered representation of neural network weights for encapsulating all quantized models into a single one. It represents weights as a group of bits (i.e., vertical layers) organized from the most significant bit (also called the basic layer) to less significant bits (i.e., enhance layers). Hence, a neural network with an arbitrary quantization precision can be obtained by adding corresponding enhance layers to the basic layer. However, we empirically find that models obtained with existing quantization methods suffer severe performance degradation if they are adapted to vertical-layered weight representation. To this end, we propose a simple once quantization-aware training (QAT) scheme for obtaining high-performance vertical-layered models. Our design incorporates a cascade downsampling mechanism with the multi-objective optimization employed to train the shared source model weights such that they can be updated simultaneously, considering the performance of all networks. After the model is trained, to construct a vertical-layered network, the lowest bit-width quantized weights become the basic layer, and every bit dropped along the downsampling process act as an enhance layer. Our design is extensively evaluated on CIFAR-100 and ImageNet datasets. Experiments show that the proposed vertical-layered representation and developed once QAT scheme are effective in embodying multiple quantized networks into a single one and allow one-time training, and it delivers comparable performance as that of quantized models tailored to any specific bit-width.},
  archive      = {J_TPAMI},
  author       = {Hai Wu and Ruifei He and Haoru Tan and Xiaojuan Qi and Kaibin Huang},
  doi          = {10.1109/TPAMI.2023.3319045},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15964-15978},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vertical layering of quantized neural networks for heterogeneous inference},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vectorized evidential learning for weakly-supervised
temporal action localization. <em>TPAMI</em>, <em>45</em>(12),
15949–15963. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of videos, weakly-supervised temporal action localization (WS-TAL) task has become a promising research direction in pattern analysis and machine learning. WS-TAL aims to detect and localize action instances with only video-level labels during training. Modern approaches have achieved impressive progress via powerful deep neural networks. However, robust and reliable WS-TAL remains challenging and underexplored due to considerable uncertainty caused by weak supervision, noisy evaluation environment, and unknown categories in the open world. To this end, we propose a new paradigm, named vectorized evidential learning (VEL), to explore local-to-global evidence collection for facilitating model performance. Specifically, a series of learnable meta-action units (MAUs) are automatically constructed, which serve as fundamental elements constituting diverse action categories. Since the same meta-action unit can manifest as distinct action components within different action categories, we leverage MAUs and category representations to dynamically and adaptively learn action components and action-component relations. After performing uncertainty estimation at both category-level and unit-level, the local evidence from action components is accumulated and optimized under the Subject Logic theory. Extensive experiments on the regular, noisy, and open-set settings of three popular benchmarks show that VEL consistently obtains more robust and reliable action localization performance than state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Junyu Gao and Mengyuan Chen and Changsheng Xu},
  doi          = {10.1109/TPAMI.2023.3311447},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15949-15963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vectorized evidential learning for weakly-supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Value-function-based sequential minimization for bi-level
optimization. <em>TPAMI</em>, <em>45</em>(12), 15930–15948. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based Bi-Level Optimization (BLO) methods have been widely applied to handle modern learning tasks. However, most existing strategies are theoretically designed based on restrictive assumptions (e.g., convexity of the lower-level sub-problem), and computationally not applicable for high-dimensional tasks. Moreover, there are almost no gradient-based methods able to solve BLO in those challenging scenarios, such as BLO with functional constraints and pessimistic BLO. In this work, by reformulating BLO into approximated single-level problems, we provide a new algorithm, named Bi-level Value-Function-based Sequential Minimization (BVFSM), to address the above issues. Specifically, BVFSM constructs a series of value-function-based approximations, and thus avoids repeated calculations of recurrent gradient and Hessian inverse required by existing approaches, time-consuming especially for high-dimensional tasks. We also extend BVFSM to address BLO with additional functional constraints. More importantly, BVFSM can be used for the challenging pessimistic BLO, which has never been properly solved before. In theory, we prove the asymptotic convergence of BVFSM on these types of BLO, in which the restrictive lower-level convexity assumption is discarded. To our best knowledge, this is the first gradient-based algorithm that can solve different kinds of BLO (e.g., optimistic, pessimistic, and with constraints) with solid convergence guarantees. Extensive experiments verify the theoretical investigations and demonstrate our superiority on various real-world applications.},
  archive      = {J_TPAMI},
  author       = {Risheng Liu and Xuan Liu and Shangzhi Zeng and Jin Zhang and Yixuan Zhang},
  doi          = {10.1109/TPAMI.2023.3303227},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15930-15948},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Value-function-based sequential minimization for bi-level optimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised local discrimination for medical images.
<em>TPAMI</em>, <em>45</em>(12), 15912–15929. (<a
href="https://doi.org/10.1109/TPAMI.2023.3299038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning, which aims to capture general representation from unlabeled images to initialize the medical analysis models, has been proven effective in alleviating the high demand for expensive annotations. Current methods mainly focus on instance-wise comparisons to learn the global discriminative features, however, pretermitting the local details to distinguish tiny anatomical structures, lesions, and tissues. To address this challenge, in this paper, we propose a general unsupervised representation learning framework, named local discrimination (LD), to learn local discriminative features for medical images by closely embedding semantically similar pixels and identifying regions of similar structures across different images. Specifically, this model is equipped with an embedding module for pixel-wise embedding and a clustering module for generating segmentation. And these two modules are unified by optimizing our novel region discrimination loss function in a mutually beneficial mechanism, which enables our model to reflect structure information as well as measure pixel-wise and region-wise similarity. Furthermore, based on LD, we propose a center-sensitive one-shot landmark localization algorithm and a shape-guided cross-modality segmentation model to foster the generalizability of our model. When transferred to downstream tasks, the learned representation by our method shows a better generalization, outperforming representation from 18 state-of-the-art (SOTA) methods and winning 9 out of all 12 downstream tasks. Especially for the challenging lesion segmentation tasks, the proposed method achieves significantly better performance.},
  archive      = {J_TPAMI},
  author       = {Huai Chen and Renzhen Wang and Xiuying Wang and Jieyu Li and Qu Fang and Hui Li and Jianhao Bai and Qing Peng and Deyu Meng and Lisheng Wang},
  doi          = {10.1109/TPAMI.2023.3299038},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15912-15929},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised local discrimination for medical images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware dual-evidential learning for
weakly-supervised temporal action localization. <em>TPAMI</em>,
<em>45</em>(12), 15896–15911. (<a
href="https://doi.org/10.1109/TPAMI.2023.3308571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised temporal action localization (WTAL) aims to localize the action instances and recognize their categories with only video-level labels. Despite great progress, existing methods suffer from severe action-background ambiguity, which mainly arises from background noise and neglect of non-salient action snippets. To address this issue, we propose a generalized evidential deep learning (EDL) framework for WTAL, called Uncertainty-aware Dual-Evidential Learning (UDEL), which extends the traditional paradigm of EDL to adapt to the weakly-supervised multi-label classification goal with the guidance of epistemic and aleatoric uncertainties, of which the former comes from models lacking knowledge, while the latter comes from the inherent properties of samples themselves. Specifically, targeting excluding the undesirable background snippets, we fuse the video-level epistemic and aleatoric uncertainties to measure the interference of background noise to video-level prediction. Then, the snippet-level aleatoric uncertainty is further deduced for progressive mutual learning, which gradually focuses on the entire action instances in an “easy-to-hard” manner and encourages the snippet-level epistemic uncertainty to be complementary with the foreground attention scores. Extensive experiments show that UDEL achieves state-of-the-art performance on four public benchmarks. Our code is available in github/mengyuanchen2021/UDEL .},
  archive      = {J_TPAMI},
  author       = {Mengyuan Chen and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TPAMI.2023.3308571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15896-15911},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uncertainty-aware dual-evidential learning for weakly-supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust your good friends: Source-free domain adaptation by
reciprocal neighborhood clustering. <em>TPAMI</em>, <em>45</em>(12),
15883–15895. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g., due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might not align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors. To aggregate information with more context, we consider expanded neighborhoods with small affinity values. Furthermore, we consider the density around each target sample, which can alleviate the negative impact of potential outliers. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets.},
  archive      = {J_TPAMI},
  author       = {Shiqi Yang and Yaxing Wang and Joost van de Weijer and Luis Herranz and Shangling Jui and Jian Yang},
  doi          = {10.1109/TPAMI.2023.3310791},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15883-15895},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Trust your good friends: Source-free domain adaptation by reciprocal neighborhood clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree recovery by dynamic programming. <em>TPAMI</em>,
<em>45</em>(12), 15870–15882. (<a
href="https://doi.org/10.1109/TPAMI.2023.3299868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-like structures are common, naturally occurring objects that are of interest to many fields of study, such as plant science and biomedicine. Analysis of these structures is typically based on skeletons extracted from captured data, which often contain spurious cycles that need to be removed. We propose a dynamic programming algorithm for solving the NP-hard tree recovery problem formulated by (Estrada et al. 2015), which seeks a least-cost partitioning of the graph nodes that yields a directed tree. Our algorithm finds the optimal solution by iteratively contracting the graph via node-merging until the problem can be trivially solved. By carefully designing the merging sequence, our algorithm can efficiently recover optimal trees for many real-world data where (Estrada et al. 2015) only produces sub-optimal solutions. We also propose an approximate variant of dynamic programming using beam search, which can process graphs containing thousands of cycles with significantly improved optimality and efficiency compared with (Estrada et al. 2015).},
  archive      = {J_TPAMI},
  author       = {Gustavo Gratacós and Ayan Chakrabarti and Tao Ju},
  doi          = {10.1109/TPAMI.2023.3299868},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15870-15882},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tree recovery by dynamic programming},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards scalable multi-view reconstruction of geometry and
materials. <em>TPAMI</em>, <em>45</em>(12), 15850–15869. (<a
href="https://doi.org/10.1109/TPAMI.2023.3314348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for joint recovery of camera pose, object geometry and spatially-varying Bidirectional Reflectance Distribution Function (svBRDF) of 3D scenes that exceed object-scale and hence cannot be captured with stationary light stages. The input are high-resolution RGB-D images captured by a mobile, hand-held capture system with point lights for active illumination. Compared to previous works that jointly estimate geometry and materials from a hand-held scanner, we formulate this problem using a single objective function that can be minimized using off-the-shelf gradient-based solvers. To facilitate scalability to large numbers of observation views and optimization variables, we introduce a distributed optimization algorithm that reconstructs 2.5D keyframe-based representations of the scene. A novel multi-view consistency regularizer effectively synchronizes neighboring keyframes such that the local optimization results allow for seamless integration into a globally consistent 3D model. We provide a study on the importance of each component in our formulation and show that our method compares favorably to baselines. We further demonstrate that our method accurately reconstructs various objects and materials and allows for expansion to spatially larger scenes. We believe that this work represents a significant step towards making geometry and material estimation from hand-held scanners scalable.},
  archive      = {J_TPAMI},
  author       = {Carolin Schmitt and Božidar Antić and Andrei Neculai and Joo Ho Lee and Andreas Geiger},
  doi          = {10.1109/TPAMI.2023.3314348},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15850-15869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards scalable multi-view reconstruction of geometry and materials},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards real-world visual tracking with temporal contexts.
<em>TPAMI</em>, <em>45</em>(12), 15834–15849. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking has made significant improvements in the past few decades. Most existing state-of-the-art trackers 1) merely aim for performance in ideal conditions while overlooking the real-world conditions; 2) adopt the tracking-by-detection paradigm, neglecting rich temporal contexts; 3) only integrate the temporal information into the template, where temporal contexts among consecutive frames are far from being fully utilized. To handle those problems, we propose a two-level framework (TCTrack) that can exploit temporal contexts efficiently. Based on it, we propose a stronger version for real-world visual tracking, i.e., TCTrack++. It boils down to two levels: features and similarity maps . Specifically, for feature extraction, we propose an attention-based temporally adaptive convolution to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights. For similarity map refinement, we introduce an adaptive temporal transformer to encode the temporal knowledge efficiently and decode it for the accurate refinement of the similarity map. To further improve the performance, we additionally introduce a curriculum learning strategy. Also, we adopt online evaluation to measure performance in real-world conditions. Exhaustive experiments on 8 well-known benchmarks demonstrate the superiority of TCTrack++. Real-world tests directly verify that TCTrack++ can be readily used in real-world applications.},
  archive      = {J_TPAMI},
  author       = {Ziang Cao and Ziyuan Huang and Liang Pan and Shiwei Zhang and Ziwei Liu and Changhong Fu},
  doi          = {10.1109/TPAMI.2023.3307174},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15834-15849},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards real-world visual tracking with temporal contexts},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards neural charged particle tracking in digital tracking
calorimeters with reinforcement learning. <em>TPAMI</em>,
<em>45</em>(12), 15820–15833. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel technique for reconstructing charged particles in digital tracking calorimeters using reinforcement learning aiming to benefit from the rapid progress and success of neural network architectures without the dependency on simulated or manually-labeled data. Here we optimize by trial-and-error a behavior policy acting as an approximation to the full combinatorial optimization problem, maximizing the physical plausibility of sampled trajectories. In modern processing pipelines used in high energy physics and related applications, tracking plays an essential role allowing to identify and follow charged particle trajectories traversing particle detectors. Due to the high multiplicity of charged particles and their physical interactions, randomly deflecting the particles, the reconstruction is a challenging undertaking, requiring fast, accurate and robust algorithms. Our approach works on graph-structured data, capturing track hypotheses through edge connections between particles in the detector layers. We demonstrate in a comprehensive study on simulated data for a particle detector used for proton computed tomography, the high potential as well as the competitiveness of our approach compared to a heuristic search algorithm and a model trained on ground truth. Finally, we point out limitations of our approach, guiding towards a robust foundation for further development of reinforcement learning based tracking.},
  archive      = {J_TPAMI},
  author       = {Tobias Kortus and Ralf Keidel and Nicolas R. Gauger and Bergen pCT Collaboration},
  doi          = {10.1109/TPAMI.2023.3305027},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15820-15833},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards neural charged particle tracking in digital tracking calorimeters with reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Towards a deeper understanding of global covariance pooling
in deep learning: An optimization perspective. <em>TPAMI</em>,
<em>45</em>(12), 15802–15819. (<a
href="https://doi.org/10.1109/TPAMI.2023.3321392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global covariance pooling (GCP) as an effective alternative to global average pooling has shown good capacity to improve deep convolutional neural networks (CNNs) in a variety of vision tasks. Although promising performance, it is still an open problem on how GCP (especially its post-normalization) works in deep learning. In this paper, we make the effort towards understanding the effect of GCP on deep learning from an optimization perspective. Specifically, we first analyze behavior of GCP with matrix power normalization on optimization loss and gradient computation of deep architectures. Our findings show that GCP can improve Lipschitzness of optimization loss and achieve flatter local minima, while improving gradient predictiveness and functioning as a special pre-conditioner on gradients. Then, we explore the effect of post-normalization on GCP from the model optimization perspective, which encourages us to propose a simple yet effective normalization, namely DropCov. Based on above findings, we point out several merits of deep GCP that have not been recognized previously or fully explored, including faster convergence, stronger model robustness and better generalization across tasks. Extensive experimental results using both CNNs and vision transformers on diversified vision tasks provide strong support to our findings while verifying the effectiveness of our method.},
  archive      = {J_TPAMI},
  author       = {Qilong Wang and Zhaolin Zhang and Mingze Gao and Jiangtao Xie and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu},
  doi          = {10.1109/TPAMI.2023.3321392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15802-15819},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards a deeper understanding of global covariance pooling in deep learning: An optimization perspective},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). TokenCut: Segmenting objects in images and videos with
self-supervised transformer and normalized cut. <em>TPAMI</em>,
<em>45</em>(12), 15790–15801. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe a graph-based algorithm that uses the features obtained by a self-supervised transformer to detect and segment salient objects in images and videos. With this approach, the image patches that compose an image or video are organised into a fully connected graph, in which the edge between each pair of patches is labeled with a similarity score based on the features learned by the transformer. Detection and segmentation of salient objects can then be formulated as a graph-cut problem and solved using the classical Normalized Cut algorithm. Despite the simplicity of this approach, it achieves state-of-the-art results on several common image and video detection and segmentation tasks. For unsupervised object discovery, this approach outperforms the competing approaches by a margin of 6.1\%, 5.7\%, and 2.6\% when tested with the VOC07, VOC12, and COCO20 K datasets. For the unsupervised saliency detection task in images, this method improves the score for Intersection over Union (IoU) by 4.4\%, 5.6\% and 5.2\%. When tested with the ECSSD, DUTS, and DUT-OMRON datasets. This method also achieves competitive results for unsupervised video object segmentation tasks with the DAVIS, SegTV2, and FBMS datasets.},
  archive      = {J_TPAMI},
  author       = {Yangtao Wang and Xi Shen and Yuan Yuan and Yuming Du and Maomao Li and Shell Xu Hu and James L. Crowley and Dominique Vaufreydaz},
  doi          = {10.1109/TPAMI.2023.3305122},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15790-15801},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TokenCut: Segmenting objects in images and videos with self-supervised transformer and normalized cut},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tighter regret analysis and optimization of online federated
learning. <em>TPAMI</em>, <em>45</em>(12), 15772–15789. (<a
href="https://doi.org/10.1109/TPAMI.2023.3316672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), it is generally assumed that all data are placed at clients in the beginning of machine learning (ML) optimization (i.e., offline learning). However, in many real-world applications, ML tasks are expected to proceed in an online fashion, wherein data samples are generated as a function of time and each client has to predict a label (or make a decision) upon receiving an incoming data. To this end, online FL (OFL) has been introduced, which aims at learning a sequence of global models from distributed streaming data such that a cumulative regret is minimized. In this framework, the vanilla method (named FedOGD) by combining online gradient descent and model averaging, which is regarded as the counterpart of FedSGD in the standard FL. Despite its asymptotic optimality, FedOGD suffers from high communication costs. In this paper, we present a communication-efficient OFL method by means of intermittent transmission (enabled by client subsampling and periodic transmission) and gradient quantization. For the first time, we derive the regret bound which can reflect the impact of data-heterogeneity and communication-efficient techniques. Based on our tighter analysis, we optimize the key parameters of OFedIQ such as sampling rate, transmission period, and quantization bits. Also, we prove that the optimized OFedIQ asymptotically achieves the performance of FedOGD while reducing the communication costs by 99\%. Via experiments with real datasets, we validate the effectiveness of our algorithm on various online ML tasks.},
  archive      = {J_TPAMI},
  author       = {Dohyeok Kwon and Jonghwan Park and Songnam Hong},
  doi          = {10.1109/TPAMI.2023.3316672},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15772-15789},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tighter regret analysis and optimization of online federated learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Teach-DETR: Better training DETR with teachers.
<em>TPAMI</em>, <em>45</em>(12), 15759–15771. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel training scheme, namely Teach-DETR, to better train DETR-based detectors from versatile types of teacher detectors. We show that the predicted boxes from teacher detectors are effective medium to transfer knowledge of teacher detectors, which could be either RCNN-based or DETR-based detectors, to train a more accurate and robust DETR model. This new training scheme can easily incorporate the predicted boxes from multiple teacher detectors, each of which provides parallel supervisions to the student DETR. Our strategy introduces no additional parameters and adds negligible computational cost to the original detector during training. During inference, Teach-DETR brings zero additional overhead and maintains the merit of requiring no non-maximum suppression. Extensive experiments show that our method leads to consistent improvement for various DETR-based detectors. Specifically, we improve the state-of-the-art detector DINO Zhang et al. 2022 with Swin-Large Liu et al. 2021 backbone, 4-scale feature pyramid and 36-epoch training schedule, from 57.8\% to 58.9\% in terms of mean average precision on COCO 2017 val set.},
  archive      = {J_TPAMI},
  author       = {Linjiang Huang and Kaixin Lu and Guanglu Song and Liang Wang and Si Liu and Yu Liu and Hongsheng Li},
  doi          = {10.1109/TPAMI.2023.3319387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15759-15771},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Teach-DETR: Better training DETR with teachers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Supervision adaptation balancing in-distribution
generalization and out-of-distribution detection. <em>TPAMI</em>,
<em>45</em>(12), 15743–15758. (<a
href="https://doi.org/10.1109/TPAMI.2023.3321869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrepancy between in-distribution (ID) and out-of-distribution (OOD) samples can lead to distributional vulnerability in deep neural networks, which can subsequently lead to high-confidence predictions for OOD samples. This is mainly due to the absence of OOD samples during training, which fails to constrain the network properly. To tackle this issue, several state-of-the-art methods include adding extra OOD samples to training and assign them with manually-defined labels. However, this practice can introduce unreliable labeling, negatively affecting ID classification. The distributional vulnerability presents a critical challenge for non-IID deep learning, which aims for OOD-tolerant ID classification by balancing ID generalization and OOD detection. In this paper, we introduce a novel supervision adaptation approach to generate adaptive supervision information for OOD samples, making them more compatible with ID samples. First, we measure the dependency between ID samples and their labels using mutual information, revealing that the supervision information can be represented in terms of negative probabilities across all classes. Second, we investigate data correlations between ID and OOD samples by solving a series of binary regression problems, with the goal of refining the supervision information for more distinctly separable ID classes. Our extensive experiments on four advanced network architectures, two ID datasets, and eleven diversified OOD datasets demonstrate the efficacy of our supervision adaptation approach in improving both ID classification and OOD detection capabilities.},
  archive      = {J_TPAMI},
  author       = {Zhilin Zhao and Longbing Cao and Kun-Yu Lin},
  doi          = {10.1109/TPAMI.2023.3321869},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15743-15758},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Supervision adaptation balancing in-distribution generalization and out-of-distribution detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StudioGAN: A taxonomy and benchmark of GANs for image
synthesis. <em>TPAMI</em>, <em>45</em>(12), 15725–15742. (<a
href="https://doi.org/10.1109/TPAMI.2023.3306436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 12 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN and StyleGAN series in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models (e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN .},
  archive      = {J_TPAMI},
  author       = {Minguk Kang and Joonghyuk Shin and Jaesik Park},
  doi          = {10.1109/TPAMI.2023.3306436},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15725-15742},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StudioGAN: A taxonomy and benchmark of GANs for image synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured knowledge distillation for accurate and efficient
object detection. <em>TPAMI</em>, <em>45</em>(12), 15706–15724. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation, which aims to transfer the knowledge learned by a cumbersome teacher model to a lightweight student model, has become one of the most popular and effective techniques in computer vision. However, many previous knowledge distillation methods are designed for image classification and fail in more challenging tasks such as object detection. In this paper, we first suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of knowledge distillation on the relation among different pixels. Then, we propose a structured knowledge distillation scheme, including attention-guided distillation and non-local distillation to address the two issues, respectively. Attention-guided distillation is proposed to find the crucial pixels of foreground objects with an attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experimental results have demonstrated the effectiveness of our method on thirteen kinds of object detection models with twelve comparison methods for both object detection and instance segmentation. For instance, Faster RCNN with our distillation achieves 43.9 mAP on MS COCO2017, which is 4.1 higher than the baseline. Additionally, we show that our method is also beneficial to the robustness and domain generalization ability of detectors. Codes and model weights have been released on GitHub 1 .},
  archive      = {J_TPAMI},
  author       = {Linfeng Zhang and Kaisheng Ma},
  doi          = {10.1109/TPAMI.2023.3300470},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15706-15724},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structured knowledge distillation for accurate and efficient object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StructNeRF: Neural radiance fields for indoor scenes with
structural hints. <em>TPAMI</em>, <em>45</em>(12), 15694–15705. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with densely captured input images. However, the geometry of NeRF is extremely under-constrained given sparse views, resulting in significant degradation of novel view synthesis quality. Inspired by self-supervised depth estimation methods, we propose StructNeRF, a solution to novel view synthesis for indoor scenes with sparse inputs. StructNeRF leverages the structural hints naturally embedded in multi-view inputs to handle the unconstrained geometry issue in NeRF. Specifically, it tackles the texture and non-texture regions respectively: a patch-based multi-view consistent photometric loss is proposed to constrain the geometry of textured regions; for non-textured ones, we explicitly restrict them to be 3D consistent planes. Through the dense self-supervised depth constraints, our method improves both the geometry and the view synthesis performance of NeRF without any additional training on external data. Extensive experiments on several real-world datasets demonstrate that StructNeRF shows superior or comparable performance compared to state-of-the-art methods (e.g. NeRF, DSNeRF, RegNeRF, Dense Depth Priors, MonoSDF, etc.) for indoor scenes with sparse inputs both quantitatively and qualitatively.},
  archive      = {J_TPAMI},
  author       = {Zheng Chen and Chen Wang and Yuan-Chen Guo and Song-Hai Zhang},
  doi          = {10.1109/TPAMI.2023.3305295},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15694-15705},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StructNeRF: Neural radiance fields for indoor scenes with structural hints},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STAR-TM: STructure aware reconstruction of textured mesh
from single image. <em>TPAMI</em>, <em>45</em>(12), 15680–15693. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for single-view 3D reconstruction of textured meshes , with a focus to address the primary challenge surrounding texture inference and transfer. Our key observation is that learning textured reconstruction in a structure-aware and globally consistent manner is effective in handling the severe ill-posedness of the texturing problem and significant variations in object pose and texture details. Specifically, we perform structured mesh reconstruction, via a retrieval-and-assembly approach, to produce a set of genus-zero parts parameterized by deformable boxes and endowed with semantic information. For texturing, we first transfer visible colors from the input image onto the unified UV texture space of the deformable boxes. Then we combine a learned transformer model for per-part texture completion with a global consistency loss to optimize inter-part texture consistency. Our texture completion model operates in a VQ-VAE embedding space and is trained end-to-end, with the transformer training enhanced with retrieved texture instances to improve texture completion performance amid significant occlusion. Extensive experiments demonstrate higher-quality textured mesh reconstruction obtained by our method over state-of-the-art alternatives, both quantitatively and qualitatively, as reflected by a better recovery of texture coherence and details.},
  archive      = {J_TPAMI},
  author       = {Tong Wu and Lin Gao and Ling-Xiao Zhang and Yu-Kun Lai and Hao Zhang},
  doi          = {10.1109/TPAMI.2023.3305630},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15680-15693},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {STAR-TM: STructure aware reconstruction of textured mesh from single image},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPTS v2: Single-point scene text spotting. <em>TPAMI</em>,
<em>45</em>(12), 15665–15679. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. Our new framework, SPTS v2, allows us to train high-performing text-spotting models using a single-point annotation. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel, which significantly reduces the requirement of the length of the sequence. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Comprehensive experiments on various existing benchmark datasets demonstrate the SPTS v2 can outperform previous state-of-the-art single-point text spotters with fewer parameters while achieving 19× faster inference speed. Within the context of our SPTS v2 framework, our experiments suggest a potential preference for single-point representation in scene text spotting when compared to other representations. Such an attempt provides a significant opportunity for scene text spotting applications beyond the realms of existing paradigms.},
  archive      = {J_TPAMI},
  author       = {Yuliang Liu and Jiaxin Zhang and Dezhi Peng and Mingxin Huang and Xinyu Wang and Jingqun Tang and Can Huang and Dahua Lin and Chunhua Shen and Xiang Bai and Lianwen Jin},
  doi          = {10.1109/TPAMI.2023.3312285},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15665-15679},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SPTS v2: Single-point scene text spotting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse r-CNN: An end-to-end framework for object detection.
<em>TPAMI</em>, <em>45</em>(12), 15650–15664. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection serves as one of most fundamental computer vision tasks. Existing works on object detection heavily rely on dense object candidates, such as $k$ anchor boxes pre-defined on all grids of an image feature map of size $H\times W$ . In this paper, we present Sparse R-CNN, a very simple and sparse method for object detection in images. In our method, a fixed sparse set of learned object proposals ( $ N$ in total) are provided to the object recognition head to perform classification and localization. By replacing $HWk$ (up to hundreds of thousands) hand-designed object candidates with $N$ (e.g., 100) learnable proposals, Sparse R-CNN makes all efforts related to object candidates design and one-to-many label assignment completely obsolete. More importantly, Sparse R-CNN directly outputs predictions without the non-maximum suppression (NMS) post-processing procedure. Thus, it establishes an end-to-end object detection framework. Sparse R-CNN demonstrates highly competitive accuracy, run-time and training convergence performance with the well-established detector baselines on the challenging COCO dataset and CrowdHuman dataset. We hope that our work can inspire re-thinking the convention of dense prior in object detectors and designing new high-performance detectors.},
  archive      = {J_TPAMI},
  author       = {Peize Sun and Rufeng Zhang and Yi Jiang and Tao Kong and Chenfeng Xu and Wei Zhan and Masayoshi Tomizuka and Zehuan Yuan and Ping Luo},
  doi          = {10.1109/TPAMI.2023.3292030},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15650-15664},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse R-CNN: An end-to-end framework for object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse bayesian learning for end-to-end EEG decoding.
<em>TPAMI</em>, <em>45</em>(12), 15632–15649. (<a
href="https://doi.org/10.1109/TPAMI.2023.3299568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding brain activity from non-invasive electroencephalography (EEG) is crucial for brain-computer interfaces (BCIs) and the study of brain disorders. Notably, end-to-end EEG decoding has gained widespread popularity in recent years owing to the remarkable advances in deep learning research. However, many EEG studies suffer from limited sample sizes, making it difficult for existing deep learning models to effectively generalize to highly noisy EEG data. To address this fundamental limitation, this paper proposes a novel end-to-end EEG decoding algorithm that utilizes a low-rank weight matrix to encode both spatio-temporal filters and the classifier, all optimized under a principled sparse Bayesian learning (SBL) framework. Importantly, this SBL framework also enables us to learn hyperparameters that optimally penalize the model in a Bayesian fashion. The proposed decoding algorithm is systematically benchmarked on five motor imagery BCI EEG datasets ( $N=192$ ) and an emotion recognition EEG dataset ( $N=45$ ), in comparison with several contemporary algorithms, including end-to-end deep-learning-based EEG decoding algorithms. The classification results demonstrate that our algorithm significantly outperforms the competing algorithms while yielding neurophysiologically meaningful spatio-temporal patterns. Our algorithm therefore advances the state-of-the-art by providing a novel EEG-tailored machine learning tool for decoding brain activity.},
  archive      = {J_TPAMI},
  author       = {Wenlong Wang and Feifei Qi and David Paul Wipf and Chang Cai and Tianyou Yu and Yuanqing Li and Yu Zhang and Zhuliang Yu and Wei Wu},
  doi          = {10.1109/TPAMI.2023.3299568},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15632-15649},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse bayesian learning for end-to-end EEG decoding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SERE: Exploring feature self-relation for self-supervised
transformer. <em>TPAMI</em>, <em>45</em>(12), 15619–15631. (<a
href="https://doi.org/10.1109/TPAMI.2023.3309979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning representations with self-supervision for convolutional networks (CNN) has been validated to be effective for vision tasks. As an alternative to CNN, vision transformers (ViT) have strong representation ability with spatial self-attention and channel-level feedforward networks. Recent works reveal that self-supervised learning helps unleash the great potential of ViT. Still, most works follow self-supervised strategies designed for CNN, e.g., instance-level discrimination of samples, but they ignore the properties of ViT. We observe that relational modeling on spatial and channel dimensions distinguishes ViT from other networks. To enforce this property, we explore the feature SE lf- RE lation (SERE) for training self-supervised ViT. Specifically, instead of conducting self-supervised learning solely on feature embeddings from multiple views, we utilize the feature self-relations, i.e., spatial/channel self-relations, for self-supervised learning. Self-relation based learning further enhances the relation modeling ability of ViT, resulting in stronger representations that stably improve performance on multiple downstream tasks.},
  archive      = {J_TPAMI},
  author       = {Zhong-Yu Li and Shanghua Gao and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2023.3309979},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15619-15631},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SERE: Exploring feature self-relation for self-supervised transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised contrastive representation learning for
semi-supervised time-series classification. <em>TPAMI</em>,
<em>45</em>(12), 15604–15618. (<a
href="https://doi.org/10.1109/TPAMI.2023.3308189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel T ime- S eries representation learning framework via T emporal and C ontextual C ontrasting ( TS-TCC ) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a C lass- A ware TS- TCC ( CA-TCC ) that benefits from the available few labeled data to further improve representations learned by TS-TCC. Specifically, we leverage the robust pseudo labels produced by TS-TCC to realize a class-aware contrastive loss. Extensive experiments show that the linear evaluation of the features learned by our proposed framework performs comparably with the fully supervised training. Additionally, our framework shows high efficiency in few labeled data and transfer learning scenarios.},
  archive      = {J_TPAMI},
  author       = {Emadeldeen Eldele and Mohamed Ragab and Zhenghua Chen and Min Wu and Chee-Keong Kwoh and Xiaoli Li and Cuntai Guan},
  doi          = {10.1109/TPAMI.2023.3308189},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15604-15618},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised contrastive representation learning for semi-supervised time-series classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-scalable tanh (stan): Multi-scale solutions for
physics-informed neural networks. <em>TPAMI</em>, <em>45</em>(12),
15588–15603. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential equations are fundamental in modeling numerous physical systems, including thermal, manufacturing, and meteorological systems. Traditionally, numerical methods often approximate the solutions of complex systems modeled by differential equations. With the advent of modern deep learning, Physics-informed Neural Networks (PINNs) are evolving as a new paradigm for solving differential equations with a pseudo-closed form solution. Unlike numerical methods, the PINNs can solve the differential equations mesh-free, integrate the experimental data, and resolve challenging inverse problems. However, one of the limitations of PINNs is the poor training caused by using the activation functions designed typically for purely data-driven problems. This work proposes a scalable $\tanh$ -based activation function for PINNs to improve learning the solutions of differential equations. The proposed Self-scalable $\tanh$ (Stan) function is smooth, non-saturating, and has a trainable parameter. It can allow an easy flow of gradients and enable systematic scaling of the input-output mapping during training. Various forward problems to solve differential equations and inverse problems to find the parameters of differential equations demonstrate that the Stan activation function can achieve better training and more accurate predictions than the existing activation functions for PINN in the literature.},
  archive      = {J_TPAMI},
  author       = {Raghav Gnanasambandam and Bo Shen and Jihoon Chung and Xubo Yue and Zhenyu Kong},
  doi          = {10.1109/TPAMI.2023.3307688},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15588-15603},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-scalable tanh (Stan): Multi-scale solutions for physics-informed neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Second-order unsupervised feature selection via knowledge
contrastive distillation. <em>TPAMI</em>, <em>45</em>(12), 15577–15587.
(<a href="https://doi.org/10.1109/TPAMI.2023.3311617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection aims to select a subset from the original features that are most useful for the downstream tasks without external guidance information. While most unsupervised feature selection methods focus on ranking features based on the intrinsic properties of data, most of them do not pay much attention to the relationships between features, which often leads to redundancy among the selected features. In this paper, we propose a two-stage S econd- O rder unsupervised F eature selection via knowledge contrastive dis T illation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for unsupervised feature selection. In the first stage, we learn a sparse attention matrix that can represent second-order relations between features by contrastively distilling the intrinsic structure. In the second stage, we build a relational graph based on the learned attention matrix and perform graph segmentation. To this end, we conduct feature selection by only selecting one feature from each cluster to decrease the feature redundancy. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods, which demonstrates the effectiveness of our proposed method. Moreover, we also provide rich in-depth experiments to further explore several key factors of SOFT.},
  archive      = {J_TPAMI},
  author       = {Han Yue and Jundong Li and Hongfu Liu},
  doi          = {10.1109/TPAMI.2023.3311617},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15577-15587},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Second-order unsupervised feature selection via knowledge contrastive distillation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SceneDreamer: Unbounded 3D scene generation from 2D image
collections. <em>TPAMI</em>, <em>45</em>(12), 15562–15576. (<a
href="https://doi.org/10.1109/TPAMI.2023.3321857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present SceneDreamer , an unconditional generative model for unbounded 3D scenes, which synthesizes large-scale 3D landscapes from random noise. Our framework is learned from in-the-wild 2D image collections only, without any 3D annotations. At the core of SceneDreamer is a principled learning paradigm comprising: 1) an efficient yet expressive 3D scene representation, 2) a generative scene parameterization, and 3) an effective renderer that can leverage the knowledge from 2D images. Our approach begins with an efficient bird’s-eye-view (BEV) representation generated from simplex noise, which includes a height field for surface elevation and a semantic field for detailed scene semantics. This BEV scene representation enables: 1) representing a 3D scene with quadratic complexity, 2) disentangled geometry and semantics, and 3) efficient training. Moreover, we propose a novel generative neural hash grid to parameterize the latent space based on 3D positions and scene semantics, aiming to encode generalizable features across various scenes. Lastly, a neural volumetric renderer, learned from 2D image collections through adversarial training, is employed to produce photorealistic images. Extensive experiments demonstrate the effectiveness of SceneDreamer and superiority over state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.},
  archive      = {J_TPAMI},
  author       = {Zhaoxi Chen and Guangcong Wang and Ziwei Liu},
  doi          = {10.1109/TPAMI.2023.3321857},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15562-15576},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SceneDreamer: Unbounded 3D scene generation from 2D image collections},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAN: Side adapter network for open-vocabulary semantic
segmentation. <em>TPAMI</em>, <em>45</em>(12), 15546–15561. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on open-vocabulary semantic segmentation, where a well optimized model is able to segment arbitrary categories that appear in an image. To achieve this goal, we present a novel framework termed Side Adapter Network, or SAN for short. Our design principles are three-fold: 1) Recent large-scale vision-language models (e.g. CLIP) show promising open-vocabulary image classification capability; it is training-economized to adapt a pre-trained CLIP model to open-vocabulary semantic segmentation. 2) Our SAN model should be both lightweight and effective in order to reduce the inference cost–to achieve this, we fuse the CLIP model&#39;s intermediate features to enhance the representation capability of the SAN model, and drive the CLIP model to focus on the informative areas of an image with the aid of the attention biases predicted by a side adapter network. 3) Our approach should empower mainstream segmentation architectures to have the capability of open-vocabulary segmentation–we present P-SAN and R-SAN, to support widely adopted pixel-wise segmentation and region-wise segmentation, respectively. Experimentally, our approach achieves state-of-the-art performance on 5 commonly used benchmarks while having much less trainable parameters and GFLOPs. For instance, our R-SAN outperforms previous best method OvSeg by +2.3 averaged mIoU across all benchmarks while using only 6\% of trainable parameters and less than 1\% of GFLOPs. In addition, we also conduct a comprehensive analysis of the open-vocabulary semantic segmentation datasets and verify the feasibility of transferring a well optimzied R-SAN model to video segmentation task.},
  archive      = {J_TPAMI},
  author       = {Mengde Xu and Zheng Zhang and Fangyun Wei and Han Hu and Xiang Bai},
  doi          = {10.1109/TPAMI.2023.3311618},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15546-15561},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SAN: Side adapter network for open-vocabulary semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust reflection removal with flash-only cues in the wild.
<em>TPAMI</em>, <em>45</em>(12), 15530–15545. (<a
href="https://doi.org/10.1109/TPAMI.2023.3314972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple yet effective reflection-free cue for robust reflection removal from a pair of flash and ambient (no-flash) images. The reflection-free cue exploits a flash-only image obtained by subtracting the ambient image from the corresponding flash image in raw data space. The flash-only image is equivalent to an image taken in a dark environment with only a flash on. This flash-only image is visually reflection-free and thus can provide robust cues to infer the reflection in the ambient image. Since the flash-only image usually has artifacts, we further propose a dedicated model that not only utilizes the reflection-free cue but also avoids introducing artifacts, which helps accurately estimate reflection and transmission. Our experiments on real-world images with various types of reflection demonstrate the effectiveness of our model with reflection-free flash-only cues: our model outperforms state-of-the-art reflection removal approaches by more than 5.23 dB in PSNR. We extend our approach to handheld photography to address the misalignment between the flash and no-flash pair. With misaligned training data and the alignment module, our aligned model outperforms our previous version by more than 3.19 dB in PSNR on a misaligned dataset. We also study using linear RGB images as training data.},
  archive      = {J_TPAMI},
  author       = {Chenyang Lei and Xudong Jiang and Qifeng Chen},
  doi          = {10.1109/TPAMI.2023.3314972},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15530-15545},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust reflection removal with flash-only cues in the wild},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rewarded semi-supervised re-identification on identities
rarely crossing camera views. <em>TPAMI</em>, <em>45</em>(12),
15512–15529. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised person re-identification (Re-ID) is an important approach for alleviating annotation costs when learning to match person images across camera views. Most existing works assume that training data contains abundant identities crossing camera views. However, this assumption is not true in many real-world applications, especially when images are captured in nonadjacent scenes for Re-ID in wider areas, where the identities rarely cross camera views. In this work, we operate semi-supervised Re-ID under a relaxed assumption of identities rarely crossing camera views, which is still largely ignored in existing methods. Since the identities rarely cross camera views, the underlying sample relations across camera views become much more uncertain, and deteriorate the noise accumulation problem in many advanced Re-ID methods that apply pseudo labeling for associating visually similar samples. To quantify such uncertainty, we parameterize the probabilistic relations between samples in a relation discovery objective for pseudo label training. Then, we introduce reward quantified by identification performance on a few labeled data to guide learning dynamic relations between samples for reducing uncertainty. Our strategy is called the Rewarded Relation Discovery (R $^{2}$ D), of which the rewarded learning paradigm is under-explored in existing pseudo labeling methods. To further reduce the uncertainty in sample relations, we perform multiple relation discovery objectives learning to discover probabilistic relations based on different prior knowledge of intra-camera affinity and cross-camera style variation, and fuse the complementary knowledge of different probabilistic relations by similarity distillation. To better evaluate semi-supervised Re-ID on identities rarely crossing camera views, we collect a new real-world dataset called REID-CBD, and perform simulation on benchmark datasets. Experiment results show that our method outperforms a wide range of semi-supervised and unsupervised learning methods.},
  archive      = {J_TPAMI},
  author       = {Ancong Wu and Wenhang Ge and Wei-Shi Zheng},
  doi          = {10.1109/TPAMI.2023.3292936},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15512-15529},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rewarded semi-supervised re-identification on identities rarely crossing camera views},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting AUC-oriented adversarial training with
loss-agnostic perturbations. <em>TPAMI</em>, <em>45</em>(12),
15494–15511. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Area Under the ROC curve (AUC) is a popular metric for long-tail classification. Many efforts have been devoted to AUC optimization methods in the past decades. However, little exploration has been done to make them survive adversarial attacks. Among the few exceptions, AdAUC presents an early trial for AUC-oriented adversarial training with a convergence guarantee. This algorithm generates the adversarial perturbations globally for all the training examples. However, it implicitly assumes that the attackers must know in advance that the victim is using an AUC-based loss function and training technique, which is too strong to be met in real-world scenarios. Moreover, whether a straightforward generalization bound for AdAUC exists is unclear due to the technical difficulties in decomposing each adversarial example. By carefully revisiting the AUC-orient adversarial training problem, we present three reformulations of the original objective function and propose an inducing algorithm. On top of this, we can show that: 1) Under mild conditions, AdAUC can be optimized equivalently with score-based or instance-wise-loss-based perturbations, which is compatible with most of the popular adversarial example generation methods. 2) AUC-oriented AT does have an explicit error bound to ensure its generalization ability. 3) One can construct a fast SVRG-based gradient descent-ascent algorithm to accelerate the AdAUC method. Finally, the extensive experimental results show the performance and robustness of our algorithm in five long-tail datasets.},
  archive      = {J_TPAMI},
  author       = {Zhiyong Yang and Qianqian Xu and Wenzheng Hou and Shilong Bao and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2023.3303934},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15494-15511},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting AUC-oriented adversarial training with loss-agnostic perturbations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reverse engineering of generative models: Inferring model
hyperparameters from generated images. <em>TPAMI</em>, <em>45</em>(12),
15477–15493. (<a
href="https://doi.org/10.1109/TPAMI.2023.3301451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. Identifying and understanding manipulated media are crucial to mitigate the social concerns on the potential misuse of GMs. We propose to perform reverse engineering of GMs to infer model hyperparameters from the images generated by these models. We define a novel problem, “model parsing”, as estimating GM network architectures and training loss functions by examining their generated images – a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a fake image dataset with 100 K images generated by 116 different GMs. Extensive experiments show encouraging results in parsing the hyperparameters of the unseen models. Finally, our fingerprint estimation can be leveraged for deepfake detection and image attribution, as we show by reporting SOTA results on both the deepfake detection (Celeb-DF) and image attribution benchmarks.},
  archive      = {J_TPAMI},
  author       = {Vishal Asnani and Xi Yin and Tal Hassner and Xiaoming Liu},
  doi          = {10.1109/TPAMI.2023.3301451},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15477-15493},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reverse engineering of generative models: Inferring model hyperparameters from generated images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). RestoreFormer++: Towards real-world blind face restoration
from undegraded key-value pairs. <em>TPAMI</em>, <em>45</em>(12),
15462–15476. (<a
href="https://doi.org/10.1109/TPAMI.2023.3315753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind face restoration aims at recovering high-quality face images from those with unknown degradations. Current algorithms mainly introduce priors to complement high-quality details and achieve impressive progress. However, most of these algorithms ignore abundant contextual information in the face and its interplay with the priors, leading to sub-optimal performance. Moreover, they pay less attention to the gap between the synthetic and real-world scenarios, limiting the robustness and generalization to real-world applications. In this work, we propose RestoreFormer++, which on the one hand introduces fully-spatial attention mechanisms to model the contextual information and the interplay with the priors, and on the other hand, explores an extending degrading model to help generate more realistic degraded face images to alleviate the synthetic-to-real-world gap. Compared with current algorithms, RestoreFormer++ has several crucial benefits. First, instead of using a multi-head self-attention mechanism like the traditional visual transformer, we introduce multi-head cross-attention over multi-scale features to fully explore spatial interactions between corrupted information and high-quality priors. In this way, it can facilitate RestoreFormer++ to restore face images with higher realness and fidelity. Second, in contrast to the recognition-oriented dictionary, we learn a reconstruction-oriented dictionary as priors, which contains more diverse high-quality facial details and better accords with the restoration target. Third, we introduce an extending degrading model that contains more realistic degraded scenarios for training data synthesizing, and thus helps to enhance the robustness and generalization of our RestoreFormer++ model. Extensive experiments show that RestoreFormer++ outperforms state-of-the-art algorithms on both synthetic and real-world datasets.},
  archive      = {J_TPAMI},
  author       = {Zhouxia Wang and Jiawei Zhang and Tianshui Chen and Wenping Wang and Ping Luo},
  doi          = {10.1109/TPAMI.2023.3315753},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15462-15476},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RestoreFormer++: Towards real-world blind face restoration from undegraded key-value pairs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability-aware restoration framework for 4D spectral
photoacoustic data. <em>TPAMI</em>, <em>45</em>(12), 15445–15461. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral photoacoustic imaging (PAI) is a new technology that is able to provide 3D geometric structure associated with 1D wavelength-dependent absorption information of the interior of a target in a non-invasive manner. It has potentially broad applications in clinical and medical diagnosis. Unfortunately, the usability of spectral PAI is severely affected by a time-consuming data scanning process and complex noise. Therefore in this study, we propose a reliability-aware restoration framework to recover clean 4D data from incomplete and noisy observations. To the best of our knowledge, this is the first attempt for the 4D spectral PA data restoration problem that solves data completion and denoising simultaneously. We first present a sequence of analyses, including modeling of data reliability in the depth and spectral domains, developing an adaptive correlation graph, and analyzing local patch orientation. On the basis of these analyses, we explore global sparsity and local self-similarity for restoration. We demonstrated the effectiveness of our proposed approach through experiments on real data captured from patients, where our approach outperformed the state-of-the-art methods in both objective evaluation and subjective assessment.},
  archive      = {J_TPAMI},
  author       = {Weihang Liao and Art Subpa-asa and Yuta Asano and Yinqiang Zheng and Hiroki Kajita and Nobuaki Imanishi and Takayuki Yagi and Sadakazu Aiso and Kazuo Kishi and Imari Sato},
  doi          = {10.1109/TPAMI.2023.3310981},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15445-15461},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reliability-aware restoration framework for 4D spectral photoacoustic data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized optimal transport layers for generalized global
pooling operations. <em>TPAMI</em>, <em>45</em>(12), 15426–15444. (<a
href="https://doi.org/10.1109/TPAMI.2023.3314661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global pooling is one of the most significant operations in many machine learning models and tasks, which works for information fusion and structured data (like sets and graphs) representation. However, without solid mathematical fundamentals, its practical implementations often depend on empirical mechanisms and thus lead to sub-optimal, even unsatisfactory performance. In this work, we develop a novel and generalized global pooling framework through the lens of optimal transport. The proposed framework is interpretable from the perspective of expectation-maximization. Essentially, it aims at learning an optimal transport across sample indices and feature dimensions, making the corresponding pooling operation maximize the conditional expectation of input data. We demonstrate that most existing pooling methods are equivalent to solving a regularized optimal transport (ROT) problem with different specializations, and more sophisticated pooling operations can be implemented by hierarchically solving multiple ROT problems. Making the parameters of the ROT problem learnable, we develop a family of regularized optimal transport pooling (ROTP) layers. We implement the ROTP layers as a new kind of deep implicit layer. Their model architectures correspond to different optimization algorithms. We test our ROTP layers in several representative set-level machine learning scenarios, including multi-instance learning (MIL), graph classification, graph set representation, and image classification. Experimental results show that applying our ROTP layers can reduce the difficulty of the design and selection of global pooling — our ROTP layers may either imitate some existing global pooling methods or lead to some new pooling layers fitting data better.},
  archive      = {J_TPAMI},
  author       = {Hongteng Xu and Minjie Cheng},
  doi          = {10.1109/TPAMI.2023.3314661},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15426-15444},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Regularized optimal transport layers for generalized global pooling operations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recovering 3D human mesh from monocular images: A survey.
<em>TPAMI</em>, <em>45</em>(12), 15406–15425. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human pose and shape from monocular images is a long-standing problem in computer vision. Since the release of statistical body models, 3D human mesh recovery has been drawing broader attention. With the same goal of obtaining well-aligned and physically plausible mesh results, two paradigms have been developed to overcome challenges in the 2D-to-3D lifting process: i) an optimization-based paradigm, where different data terms and regularization terms are exploited as optimization objectives; and ii) a regression-based paradigm, where deep learning techniques are embraced to solve the problem in an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving the quality of 3D mesh labels for a wide range of datasets. Though remarkable progress has been achieved in the past decade, the task is still challenging due to flexible body motions, diverse appearances, complex environments, and insufficient in-the-wild annotations. To the best of our knowledge, this is the first survey that focuses on the task of monocular 3D human mesh recovery. We start with the introduction of body models and then elaborate recovery frameworks and training objectives by providing in-depth analyses of their strengths and weaknesses. We also summarize datasets, evaluation metrics, and benchmark results. Open issues and future directions are discussed in the end, hoping to motivate researchers and facilitate their research in this area.},
  archive      = {J_TPAMI},
  author       = {Yating Tian and Hongwen Zhang and Yebin Liu and Limin Wang},
  doi          = {10.1109/TPAMI.2023.3298850},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15406-15425},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recovering 3D human mesh from monocular images: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconstruction guided meta-learning for few shot open set
recognition. <em>TPAMI</em>, <em>45</em>(12), 15394–15405. (<a
href="https://doi.org/10.1109/TPAMI.2023.3320731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, we are constrained to learn classifiers from very limited data (few-shot classification). The task becomes even more challenging if it is also required to identify samples from unknown categories (open-set classification). Learning a good abstraction for a class with very few samples is extremely difficult, especially under open-set settings. As a result, open-set recognition has received limited attention in the few-shot setting. However, it is a critical task in many applications like environmental monitoring, where the number of labeled examples for each class is limited. Existing few-shot open-set recognition (FSOSR) methods rely on thresholding schemes, with some considering uniform probability for open-class samples. However, this approach is often inaccurate, especially for fine-grained categorization, and makes them highly sensitive to the choice of a threshold. To address these concerns, we propose Reconstructing Exemplar-based Few-shot Open-set ClaSsifier (ReFOCS). By using a novel exemplar reconstruction-based meta-learning strategy ReFOCS streamlines FSOSR eliminating the need for a carefully tuned threshold by learning to be self-aware of the openness of a sample. The exemplars, act as class representatives and can be either provided in the training dataset or estimated in the feature domain. By testing on a wide variety of datasets, we show ReFOCS to outperform multiple state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Sayak Nag and Dripta S. Raychaudhuri and Sujoy Paul and Amit K. Roy-Chowdhury},
  doi          = {10.1109/TPAMI.2023.3320731},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15394-15405},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstruction guided meta-learning for few shot open set recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QDTrack: Quasi-dense similarity learning for appearance-only
multiple object tracking. <em>TPAMI</em>, <em>45</em>(12), 15380–15393.
(<a href="https://doi.org/10.1109/TPAMI.2023.3301975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions in images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of object regions on a pair of images for contrastive learning. We combine this similarity learning with multiple existing object detectors to build Quasi-Dense Tracking (QDTrack), which does not require displacement regression or motion priors. We find that the resulting distinctive feature space admits a simple nearest neighbor search at inference time for object association. In addition, we show that our similarity learning scheme is not limited to video data, but can learn effective instance similarity even from static input, enabling a competitive tracking performance without training on videos or using tracking supervision. We conduct extensive experiments on a wide variety of popular MOT benchmarks. We find that, despite its simplicity, QDTrack rivals the performance of state-of-the-art tracking methods on all benchmarks and sets a new state-of-the-art on the large-scale BDD100K MOT benchmark, while introducing negligible computational overhead to the detector.},
  archive      = {J_TPAMI},
  author       = {Tobias Fischer and Thomas E. Huang and Jiangmiao Pang and Linlu Qiu and Haofeng Chen and Trevor Darrell and Fisher Yu},
  doi          = {10.1109/TPAMI.2023.3301975},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15380-15393},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {QDTrack: Quasi-dense similarity learning for appearance-only multiple object tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting label distribution from tie-allowed multi-label
ranking. <em>TPAMI</em>, <em>45</em>(12), 15364–15379. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution offers more information about label polysemy than logical label. There are presently two approaches to obtaining label distributions: LDL (label distribution learning) and LE (label enhancement). In LDL, experts must annotate training instances with label distributions, and a predictive function is trained on this training set to obtain label distributions. In LE, experts must annotate instances with logical labels, and label distributions are recovered from them. However, LDL is limited by expensive annotations, and LE has no performance guarantee. Therefore, we investigate how to predict label distribution from TMLR (tie-allowed multi-label ranking) which is a compromise on annotation cost but has good performance guarantees. On the one hand, we theoretically dissect the relationship between TMLR and label distribution. We define EAE (expected approximation error) to quantify the quality of an annotation, provide EAE bounds for TMLR, and derive the optimal range of label distributions corresponding to a given TMLR annotation. On the other hand, we propose a framework for predicting label distribution from TMLR via conditional Dirichlet mixtures. This framework blends the procedures of recovering and learning label distributions end-to-end and allows us to effortlessly encode our knowledge by a semi-adaptive scoring function. Extensive experiments validate our proposal.},
  archive      = {J_TPAMI},
  author       = {Yunan Lu and Weiwei Li and Huaxiong Li and Xiuyi Jia},
  doi          = {10.1109/TPAMI.2023.3300310},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15364-15379},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Predicting label distribution from tie-allowed multi-label ranking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Positive-unlabeled learning with label distribution
alignment. <em>TPAMI</em>, <em>45</em>(12), 15345–15363. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive-Unlabeled (PU) data arise frequently in a wide range of fields such as medical diagnosis, anomaly analysis and personalized advertising. The absence of any known negative labels makes it very challenging to learn binary classifiers from such data. Many state-of-the-art methods reformulate the original classification risk with individual risks over positive and unlabeled data, and explicitly minimize the risk of classifying unlabeled data as negative. This, however, usually leads to classifiers with a bias toward negative predictions, i.e., they tend to recognize most unlabeled data as negative. In this paper, we propose a label distribution alignment formulation for PU learning to alleviate this issue. Specifically, we align the distribution of predicted labels with the ground-truth, which is constant for a given class prior. In this way, the proportion of samples predicted as negative is explicitly controlled from a global perspective, and thus the bias toward negative predictions could be intrinsically eliminated. On top of this, we further introduce the idea of functional margins to enhance the model&#39;s discriminability, and derive a margin-based learning framework named Positive-Unlabeled learning with Label Distribution Alignment (PULDA) . This framework is also combined with the class prior estimation process for practical scenarios, and theoretically supported by a generalization analysis. Moreover, a stochastic mini-batch optimization algorithm based on the exponential moving average strategy is tailored for this problem with a convergence guarantee. Finally, comprehensive empirical results demonstrate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Yangbangyan Jiang and Qianqian Xu and Yunrui Zhao and Zhiyong Yang and Peisong Wen and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2023.3319431},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15345-15363},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Positive-unlabeled learning with label distribution alignment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Positive-negative receptive field reasoning for
omni-supervised 3D segmentation. <em>TPAMI</em>, <em>45</em>(12),
15328–15344. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden features in the neural networks usually fail to learn informative representation for 3D segmentation as supervisions are only given on output prediction, while this can be solved by omni-scale supervision on intermediate layers. In this paper, we bring the first omni-scale supervision method to 3D segmentation via the proposed gradual Receptive Field Component Reasoning (RFCR), where target Receptive Field Component Codes (RFCCs) is designed to record categories within receptive fields for hidden units in the encoder. Then, target RFCCs will supervise the decoder to gradually infer the RFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the semantic labels. To purchase more supervisions, we also propose an RFCR-NL model with complementary negative codes (i.e., Negative RFCCs, NRFCCs) with negative learning. Because many hidden features are inactive with tiny magnitudes and make minor contributions to RFCC prediction, we propose Feature Densification with a centrifugal potential to obtain more unambiguous features, and it is in effect equivalent to entropy regularization over features. More active features can unleash the potential of omni-supervision method. We embed our method into three prevailing backbones, which are significantly improved in all three datasets on both fully and weakly supervised segmentation tasks and achieve competitive performances.},
  archive      = {J_TPAMI},
  author       = {Xin Tan and Qihang Ma and Jingyu Gong and Jiachen Xu and Zhizhong Zhang and Haichuan Song and Yanyun Qu and Yuan Xie and Lizhuang Ma},
  doi          = {10.1109/TPAMI.2023.3319470},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15328-15344},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Positive-negative receptive field reasoning for omni-supervised 3D segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAC-bayes bounds for bandit problems: A survey and
experimental comparison. <em>TPAMI</em>, <em>45</em>(12), 15308–15327.
(<a href="https://doi.org/10.1109/TPAMI.2023.3305381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PAC-Bayes has recently re-emerged as an effective theory with which one can derive principled learning algorithms with tight performance guarantees. However, applications of PAC-Bayes to bandit problems are relatively rare, which is a great misfortune. Many decision-making problems in healthcare, finance and natural sciences can be modelled as bandit problems. In many of these applications, principled algorithms with strong performance guarantees would be very much appreciated. This survey provides an overview of PAC-Bayes bounds for bandit problems and an experimental comparison of these bounds. On the one hand, we found that PAC-Bayes bounds are a useful tool for designing offline bandit algorithms with performance guarantees. In our experiments, a PAC-Bayesian offline contextual bandit algorithm was able to learn randomised neural network polices with competitive expected reward and non-vacuous performance guarantees. On the other hand, the PAC-Bayesian online bandit algorithms that we tested had loose cumulative regret bounds. We conclude by discussing some topics for future work on PAC-Bayesian bandit algorithms.},
  archive      = {J_TPAMI},
  author       = {Hamish Flynn and David Reeb and Melih Kandemir and Jan Peters},
  doi          = {10.1109/TPAMI.2023.3305381},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15308-15327},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PAC-bayes bounds for bandit problems: A survey and experimental comparison},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the power of gradual network alignment using
dual-perception similarities. <em>TPAMI</em>, <em>45</em>(12),
15292–15307. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once , they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose ${\sf Grad\text{-}Align}$ , a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, ${\sf Grad\text{-}Align}$ first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss , a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures including the multi-layer embedding similarity as well as the Tversky similarity , an asymmetric set similarity using the Tversky index applicable to networks with different scales. Additionally, we incorporate an edge augmentation module into ${\sf Grad\text{-}Align}$ to reinforce the structural consistency. Through comprehensive experiments using real-world and synthetic datasets, we empirically demonstrate that ${\sf Grad\text{-}Align}$ consistently outperforms state-of-the-art NA methods.},
  archive      = {J_TPAMI},
  author       = {Jin-Duk Park and Cong Tran and Won-Yong Shin and Xin Cao},
  doi          = {10.1109/TPAMI.2023.3300877},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15292-15307},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the power of gradual network alignment using dual-perception similarities},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Omni-training: Bridging pre-training and meta-training for
few-shot learning. <em>TPAMI</em>, <em>45</em>(12), 15275–15291. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to fast adapt a deep model from a few examples. While pre-training and meta-training can create deep models powerful for few-shot generalization, we find that pre-training and meta-training focus respectively on cross-domain transferability and cross-task transferability, which restricts their data efficiency in the entangled settings of domain shift and task shift. We thus propose the Omni-Training framework to seamlessly bridge pre-training and meta-training for data-efficient few-shot learning. Our first contribution is a tri-flow Omni-Net architecture. Besides the joint representation flow, Omni-Net introduces two parallel flows for pre-training and meta-training, responsible for improving domain transferability and task transferability respectively. Omni-Net further coordinates the parallel flows by routing their representations via the joint-flow, enabling knowledge transfer across flows. Our second contribution is the Omni-Loss, which introduces a self-distillation strategy separately on the pre-training and meta-training objectives for boosting knowledge transfer throughout different training stages. Omni-Training is a general framework to accommodate many existing algorithms. Evaluations justify that our single framework consistently and clearly outperforms the individual state-of-the-art methods on both cross-task and cross-domain settings in a variety of classification, regression and reinforcement learning problems.},
  archive      = {J_TPAMI},
  author       = {Yang Shu and Zhangjie Cao and Jinghan Gao and Jianmin Wang and Philip S. Yu and Mingsheng Long},
  doi          = {10.1109/TPAMI.2023.3319517},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15275-15291},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Omni-training: Bridging pre-training and meta-training for few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Offline model-based adaptable policy learning for
decision-making in out-of-support regions. <em>TPAMI</em>,
<em>45</em>(12), 15260–15274. (<a
href="https://doi.org/10.1109/TPAMI.2023.3317131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning, a promising direction to avoid online trial-and-error costs is learning from an offline dataset. Current offline reinforcement learning methods commonly learn in the policy space constrained to in-support regions by the offline dataset, in order to ensure the robustness of the outcome policies. Such constraints, however, also limit the potential of the outcome policies. In this paper, to release the potential of offline policy learning, we investigate the decision-making problems in out-of-support regions directly and propose offline Model-based Adaptable Policy LEarning (MAPLE). By this approach, instead of learning in in-support regions, we learn an adaptable policy that can adapt its behavior in out-of-support regions when deployed. We give a practical implementation of MAPLE via meta-learning techniques and ensemble model learning techniques. We conduct experiments on MuJoCo locomotion tasks with offline datasets. The results show that the proposed method can make robust decisions in out-of-support regions and achieve better performance than SOTA algorithms.},
  archive      = {J_TPAMI},
  author       = {Xiong-Hui Chen and Fan-Ming Luo and Yang Yu and Qingyang Li and Zhiwei Qin and Wenjie Shang and Jieping Ye},
  doi          = {10.1109/TPAMI.2023.3317131},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15260-15274},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Offline model-based adaptable policy learning for decision-making in out-of-support regions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NPT-loss: Demystifying face recognition losses with nearest
proxies triplet. <em>TPAMI</em>, <em>45</em>(12), 15249–15259. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition (FR) using deep convolutional neural networks (DCNNs) has seen remarkable success in recent years. One key ingredient of DCNN-based FR is the design of a loss function that ensures discrimination between various identities. The state-of-the-art (SOTA) solutions utilise normalised Softmax loss with additive and/or multiplicative margins. Despite being popular and effective, these losses are justified only intuitively with little theoretical explanations. In this work, we show that under the LogSumExp (LSE) approximation, the SOTA Softmax losses become equivalent to a proxy-triplet loss that focuses on nearest-neighbour negative proxies only. This motivates us to propose a variant of the proxy-triplet loss, entitled Nearest Proxies Triplet (NPT) loss, which unlike SOTA solutions, converges for a wider range of hyper-parameters and offers flexibility in proxy selection and thus outperforms SOTA techniques. We generalise many SOTA losses into a single framework and give theoretical justifications for the assertion that minimising the proposed loss ensures a minimum separability between all identities. We also show that the proposed loss has an implicit mechanism of hard-sample mining. We conduct extensive experiments using various DCNN architectures on a number of FR benchmarks to demonstrate the efficacy of the proposed scheme over SOTA methods.},
  archive      = {J_TPAMI},
  author       = {Syed Safwan Khalid and Muhammad Awais and Zhen-Hua Feng and Chi-Ho Chan and Ammarah Farooq and Ali Akbari and Josef Kittler},
  doi          = {10.1109/TPAMI.2022.3162705},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15249-15259},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NPT-loss: Demystifying face recognition losses with nearest proxies triplet},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NOPE-SAC: Neural one-plane RANSAC for sparse-view planar 3D
reconstruction. <em>TPAMI</em>, <em>45</em>(12), 15233–15248. (<a
href="https://doi.org/10.1109/TPAMI.2023.3314745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the challenging two-view 3D reconstruction problem in a rigorous sparse-view configuration, which is suffering from insufficient correspondences in the input image pairs for camera pose estimation. We present a novel Neural One-PlanE RANSAC framework (termed NOPE-SAC in short) that exerts excellent capability of neural networks to learn one-plane pose hypotheses from 3D plane correspondences. Building on the top of a Siamese network for plane detection, our NOPE-SAC first generates putative plane correspondences with a coarse initial pose. It then feeds the learned 3D plane correspondences into shared MLPs to estimate the one-plane camera pose hypotheses, which are subsequently reweighed in a RANSAC manner to obtain the final camera pose. Because the neural one-plane pose minimizes the number of plane correspondences for adaptive pose hypotheses generation, it enables stable pose voting and reliable pose refinement with a few of plane correspondences for the sparse-view inputs. In the experiments, we demonstrate that our NOPE-SAC significantly improves the camera pose estimation for the two-view inputs with severe viewpoint changes, setting several new state-of-the-art performances on two challenging benchmarks, i.e., MatterPort3D and ScanNet, for sparse-view 3D reconstruction.},
  archive      = {J_TPAMI},
  author       = {Bin Tan and Nan Xue and Tianfu Wu and Gui-Song Xia},
  doi          = {10.1109/TPAMI.2023.3314745},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15233-15248},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NOPE-SAC: Neural one-plane RANSAC for sparse-view planar 3D reconstruction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuroZoom: Denoising and super resolving neuromorphic events
and spikes. <em>TPAMI</em>, <em>45</em>(12), 15219–15232. (<a
href="https://doi.org/10.1109/TPAMI.2023.3304486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic cameras are emerging imaging technology that has advantages over conventional imaging sensors in several aspects including dynamic range, sensing latency, and power consumption. However, the signal-to-noise level and the spatial resolution still fall behind the state of conventional imaging sensors. In this article, we address the denoising and super-resolution problem for modern neuromorphic cameras. We employ 3D U-Net as the backbone neural architecture for such a task. The networks are trained and tested on two types of neuromorphic cameras: a dynamic vision sensor and a spike camera. Their pixels generate signals asynchronously, the former is based on perceived light changes and the latter is based on accumulated light intensity. To collect the datasets for training such networks, we design a display-camera system to record high frame-rate videos at multiple resolutions, providing supervision for denoising and super-resolution. The networks are trained in a noise-to-noise fashion, where the two ends of the network are unfiltered noisy data. The output of the networks has been tested for downstream applications including event-based visual object tracking and image reconstruction. Experimental results demonstrate the effectiveness of improving the quality of neuromorphic events and spikes, and the corresponding improvement to downstream applications with state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Peiqi Duan and Yi Ma and Xinyu Zhou and Xinyu Shi and Zihao W. Wang and Tiejun Huang and Boxin Shi},
  doi          = {10.1109/TPAMI.2023.3304486},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15219-15232},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NeuroZoom: Denoising and super resolving neuromorphic events and spikes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural maximum a posteriori estimation on unpaired data for
motion deblurring. <em>TPAMI</em>, <em>45</em>(12), 15203–15218. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world dynamic scene deblurring has long been a challenging task since paired blurry-sharp training data is unavailable. Conventional Maximum A Posteriori estimation and deep learning-based deblurring methods are restricted by handcrafted priors and synthetic blurry-sharp training pairs respectively, thereby failing to generalize to real dynamic blurriness. To this end, we propose a Neural Maximum A Posteriori (NeurMAP) estimation framework for training neural networks to recover blind motion information and sharp content from unpaired data. The proposed NeruMAP consists of a motion estimation network and a deblurring network which are trained jointly to model the (re)blurring process (i.e. likelihood function). Meanwhile, the motion estimation network is trained to explore the motion information in images by applying implicit dynamic motion prior, and in return enforces the deblurring network training (i.e. providing sharp image prior). The proposed NeurMAP is an orthogonal approach to existing deblurring neural networks, and is the first framework that enables training image deblurring networks on unpaired datasets. Experiments demonstrate our superiority on both quantitative metrics and visual quality over State-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Youjian Zhang and Chaoyue Wang and Dacheng Tao},
  doi          = {10.1109/TPAMI.2023.3303450},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15203-15218},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural maximum a posteriori estimation on unpaired data for motion deblurring},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighbourhood representative sampling for efficient
end-to-end video quality assessment. <em>TPAMI</em>, <em>45</em>(12),
15185–15202. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased resolution of real-world videos presents a dilemma between efficiency and accuracy for deep Video Quality Assessment (VQA). On the one hand, keeping the original resolution will lead to unacceptable computational costs. On the other hand, existing practices, such as resizing or cropping, will change the quality of original videos due to difference in details or loss of contents, and are henceforth harmful to quality assessment. With obtained insight from the studies of spatial-temporal redundancy in the human visual system, visual quality around a neighbourhood has high probability to be similar, and this motivates us to investigate an effective quality-sensitive neighbourhood representative sampling scheme for VQA. In this work, we propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS), and the resultant samples are named fragments . In St-GMS, full-resolution videos are first divided into mini-cubes with predefined spatial-temporal grids, then the temporal-aligned quality representatives are sampled to compose the fragments that serve as inputs for VQA. In addition, we design the Fragment Attention Network (FANet), a network architecture tailored specifically for fragments. With fragments and FANet, the proposed FAST-VQA and FasterVQA (with an improved sampling scheme) achieves up to 1612× efficiency than the existing state-of-the-art, meanwhile achieving significantly better performance on all relevant VQA benchmarks.},
  archive      = {J_TPAMI},
  author       = {Haoning Wu and Chaofeng Chen and Liang Liao and Jingwen Hou and Wenxiu Sun and Qiong Yan and Jinwei Gu and Weisi Lin},
  doi          = {10.1109/TPAMI.2023.3319332},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15185-15202},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neighbourhood representative sampling for efficient end-to-end video quality assessment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual-assistance learning for object detection.
<em>TPAMI</em>, <em>45</em>(12), 15171–15184. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a fundamental yet challenging task in computer vision. Despite the great strides made over recent years, modern detectors may still produce unsatisfactory performance due to certain factors, such as non-universal object features and single regression manner. In this paper, we draw on the idea of mutual-assistance (MA) learning and accordingly propose a robust one-stage detector, referred as MADet, to address these weaknesses. First, the spirit of MA is manifested in the head design of the detector. Decoupled classification and regression features are reintegrated to provide shared offsets, avoiding inconsistency between feature-prediction pairs induced by zero or erroneous offsets. Second, the spirit of MA is captured in the optimization paradigm of the detector. Both anchor-based and anchor-free regression fashions are utilized jointly to boost the capability to retrieve objects with various characteristics, especially for large aspect ratios, occlusion from similar-sized objects, etc. Furthermore, we meticulously devise a quality assessment mechanism to facilitate adaptive sample selection and loss term reweighting. Extensive experiments on standard benchmarks verify the effectiveness of our approach. On MS-COCO, MADet achieves 42.5\% AP with vanilla ResNet50 backbone, dramatically surpassing multiple strong baselines and setting a new state of the art.},
  archive      = {J_TPAMI},
  author       = {Xingxing Xie and Chunbo Lang and Shicheng Miao and Gong Cheng and Ke Li and Junwei Han},
  doi          = {10.1109/TPAMI.2023.3319634},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15171-15184},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mutual-assistance learning for object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view discrete clustering: A concise model.
<em>TPAMI</em>, <em>45</em>(12), 15154–15170. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most existing graph-based multi-view clustering methods, the eigen-decomposition of the graph Laplacian matrix followed by a post-processing step is a standard configuration to obtain the target discrete cluster indicator matrix. However, we can naturally realize that the results obtained by the two-stage process will deviate from that obtained by directly solving the primal clustering problem. In addition, it is essential to properly integrate the information from different views for the enhancement of the performance of multi-view clustering. To this end, we propose a concise model referred to as Multi-view Discrete Clustering (MDC), aiming at directly solving the primal problem of multi-view graph clustering. We automatically weigh the view-specific similarity matrix, and the discrete indicator matrix is directly obtained by performing clustering on the aggregated similarity matrix without any post-processing to best serve graph clustering. More importantly, our model does not introduce an additive, nor does it has any hyper-parameters to be tuned. An efficient optimization algorithm is designed to solve the resultant objective problem. Extensive experimental results on both synthetic and real benchmark datasets verify the superiority of the proposed model.},
  archive      = {J_TPAMI},
  author       = {Qianyao Qiang and Bin Zhang and Fei Wang and Feiping Nie},
  doi          = {10.1109/TPAMI.2023.3319700},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15154-15170},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-view discrete clustering: A concise model},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view deep gaussian processes for supervised learning.
<em>TPAMI</em>, <em>45</em>(12), 15137–15153. (<a
href="https://doi.org/10.1109/TPAMI.2023.3316671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning is a widely studied topic in machine learning, which considers learning with multiple views of samples to improve the prediction performance. Even though some approaches have sprung up recently, it is still challenging to jointly explore information contained in different views. Multi-view deep Gaussian processes have shown strong advantages in unsupervised representation learning. However, they are limited when dealing with labeled multi-view data for supervised learning, and ignore the application potential of uncertainty estimation. In this paper, we propose a supervised multi-view deep Gaussian process model (named SupMvDGP), which uses the label of the views to further improve the performance, and takes the quantitative uncertainty estimation as a supplement to assist humans to make better use of prediction. According to the diversity of views, the SupMvDGP can establish asymmetric depth structure to better model different views, so as to make full use of the property of each view. We provide a variational inference method to effectively solve the complex model. Finally, we conduct comprehensive comparative experiments on multiple real world datasets to evaluate the performance of SupMvDGP. The experimental results show that the SupMvDGP achieves the state-of-the-art results in multiple tasks, which verifies the effectiveness and superiority of the proposed approach. Meanwhile, we provide a case study to show that the SupMvDGP has the ability to provide uncertainty estimation than alternative deep models, which can alert people to better treat the prediction results in high-risk applications.},
  archive      = {J_TPAMI},
  author       = {Wenbo Dong and Shiliang Sun},
  doi          = {10.1109/TPAMI.2023.3316671},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15137-15153},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-view deep gaussian processes for supervised learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale dynamic graph representation for biometric
recognition with occlusions. <em>TPAMI</em>, <em>45</em>(12),
15120–15136. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is a common problem with biometric recognition in the wild. The generalization ability of CNNs greatly decreases due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrating the merits of both CNNs and graph models to overcome occlusion problems in biometric recognition, called multiscale dynamic graph representation (MS-DGR). More specifically, a group of deep features reflected on certain subregions is recrafted into a feature graph (FG). Each node inside the FG is deemed to characterize a specific local region of the input sample, and the edges imply the co-occurrence of non-occluded regions. By analyzing the similarities of the node representations and measuring the topological structures stored in the adjacent matrix, the proposed framework leverages dynamic graph matching to judiciously discard the nodes corresponding to the occluded parts. The multiscale strategy is further incorporated to attain more diverse nodes representing regions of various sizes. Furthermore, the proposed framework exhibits a more illustrative and reasonable inference by showing the paired nodes. Extensive experiments demonstrate the superiority of the proposed framework, which boosts the accuracy in both natural and occlusion-simulated cases by a large margin compared with that of baseline methods.},
  archive      = {J_TPAMI},
  author       = {Min Ren and Yunlong Wang and Yuhao Zhu and Kunbo Zhang and Zhenan Sun},
  doi          = {10.1109/TPAMI.2023.3298836},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15120-15136},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiscale dynamic graph representation for biometric recognition with occlusions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal image synthesis and editing: The generative AI
era. <em>TPAMI</em>, <em>45</em>(12), 15098–15119. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an introduction to different guidance modalities in image synthesis and editing, and then describe multimodal image synthesis and editing approaches extensively according to their model types. After that, we describe benchmark datasets and evaluation metrics as well as corresponding experimental results. Finally, we provide insights about the current research challenges and possible directions for future research.},
  archive      = {J_TPAMI},
  author       = {Fangneng Zhan and Yingchen Yu and Rongliang Wu and Jiahui Zhang and Shijian Lu and Lingjie Liu and Adam Kortylewski and Christian Theobalt and Eric Xing},
  doi          = {10.1109/TPAMI.2023.3305243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15098-15119},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multimodal image synthesis and editing: The generative AI era},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular depth estimation for glass walls with context: A
new dataset and method. <em>TPAMI</em>, <em>45</em>(12), 15081–15097.
(<a href="https://doi.org/10.1109/TPAMI.2023.3308551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional monocular depth estimation assumes that all objects are reliably visible in the RGB color domain. However, this is not always the case as more and more buildings are decorated with transparent glass walls. This problem has not been explored due to the difficulties in annotating the depth levels of glass walls, as commercial depth sensors cannot provide correct feedbacks on transparent objects. Furthermore, estimating depths from transparent glass walls requires the aids of surrounding context, which has not been considered in prior works. To cope with this problem, we introduce the first Glass Walls Depth Dataset (GW-Depth dataset). We annotate the depth levels of transparent glass walls by propagating the context depth values within neighboring flat areas, and the glass segmentation mask and instance level line segments of glass edges are also provided. On the other hand, a tailored monocular depth estimation method is proposed to fully activate the glass wall contextual understanding. First, we propose to exploit the glass structure context by incorporating the structural prior knowledge embedded in glass boundary line segment detections. Furthermore, to make our method adaptive to scenes without structure context where the glass boundary is either absent in the image or too narrow to be recognized, we propose to derive a reflection context by utilizing the depth reliable points sampled according to the variance between two depth estimations from different resolutions. High-resolution depth is thus estimated by the weighted summation of depths by those reliable points. Extensive experiments are conducted to evaluate the effectiveness of the proposed dual context design. Superior performances of our method is also demonstrated by comparing with state-of-the-art methods. We present the first feasible solution for monocular depth estimation in the presence of glass walls, which can be widely adopted in autonomous navigation.},
  archive      = {J_TPAMI},
  author       = {Yuan Liang and Bailin Deng and Wenxi Liu and Jing Qin and Shengfeng He},
  doi          = {10.1109/TPAMI.2023.3308551},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15081-15097},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Monocular depth estimation for glass walls with context: A new dataset and method},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling noisy annotations for point-wise supervision.
<em>TPAMI</em>, <em>45</em>(12), 15065–15080. (<a
href="https://doi.org/10.1109/TPAMI.2023.3299753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-wise supervision is widely adopted in computer vision tasks such as crowd counting and human pose estimation. In practice, the noise in point annotations may affect the performance and robustness of algorithm significantly. In this paper, we investigate the effect of annotation noise in point-wise supervision and propose a series of robust loss functions for different tasks. In particular, the point annotation noise includes spatial-shift noise, missing-point noise, and duplicate-point noise. The spatial-shift noise is the most common one, and exists in crowd counting, pose estimation, visual tracking, etc , while the missing-point and duplicate-point noises usually appear in dense annotations, such as crowd counting. In this paper, we first consider the shift noise by modeling the real locations as random variables and the annotated points as noisy observations. The probability density function of the intermediate representation (a smooth heat map generated from dot annotations) is derived and the negative log likelihood is used as the loss function to naturally model the shift uncertainty in the intermediate representation. The missing and duplicate noise are further modeled by an empirical way with the assumption that the noise appears at high density region with a high probability. We apply the method to crowd counting, human pose estimation and visual tracking, propose robust loss functions for those tasks, and achieve superior performance and robustness on widely used datasets.},
  archive      = {J_TPAMI},
  author       = {Jia Wan and Qiangqiang Wu and Antoni B. Chan},
  doi          = {10.1109/TPAMI.2023.3299753},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15065-15080},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Modeling noisy annotations for point-wise supervision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modality exploration, retrieval and adaptation for
trajectory prediction. <em>TPAMI</em>, <em>45</em>(12), 15051–15064. (<a
href="https://doi.org/10.1109/TPAMI.2023.3316389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting future trajectories of dynamic agents is inherently riddled with uncertainty. Given a certain historical observation, there are multiple plausible future movements people can perform. Notably, these possible movements are usually centralized around a few representative motion patterns, e.g. acceleration, deceleration, turning, etc. In this paper, we propose a novel prediction scheme which explores human behavior modality representations from real-world trajectory data to discover such motion patterns and further uses them to aid in trajectory prediction. To explore potential behavior modalities, we introduce a deep feature clustering process on trajectory features and each cluster can represent a type of modality. Intuitively, each modality is naturally a class, and a classification network can be adopted to retrieve highly probable modalities about to happen in the future according to historical observations. On account of a wide variety of cues existing in the observation (e.g. agents’ motion states, semantics of the scene, etc.), we further design a gated aggregation module to fuse different types of cues into a unified feature. Finally, an adaptation process is proposed to adapt a certain modality to specific historical observations and generate fine-grained prediction results. Extensive experiments on four widely-used benchmarks show the superiority of our proposed approach.},
  archive      = {J_TPAMI},
  author       = {Jianhua Sun and Yuxuan Li and Liang Chai and Cewu Lu},
  doi          = {10.1109/TPAMI.2023.3316389},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15051-15064},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Modality exploration, retrieval and adaptation for trajectory prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MNET++: Music-driven pluralistic dancing toward multiple
dance genre synthesis. <em>TPAMI</em>, <em>45</em>(12), 15036–15050. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous task-specific variants of autoregressive networks have been developed for dance generation. Nonetheless, a severe limitation remains in that all existing algorithms can return repeated patterns for a given initial pose, which may be inferior. We examine and analyze several key challenges of previous works, and propose variations in both model architecture (namely MNET++) and training methods to address these. In particular, we devise the beat synchronizer and dance synthesizer. First, generated dance should be locally and globally consistent with given music beats, circumvent repetitive patterns, and look realistic. To achieve this, the beat synchronizer implicitly catches the rhythm enabling it to stay in sync with the music as it dances. Then, the dance synthesizer infers the dance motions in a seamless patch-by-patch manner conditioned by music. Second, to generate diverse dance lines, adversarial learning is performed by leveraging the transformer architecture. Furthermore, MNET++ learns a dance genre-aware latent representation that is scalable for multiple domains to provide fine-grained user control according to the dance genre. Compared with the state-of-the-art methods, our method synthesizes plausible and diverse outputs according to multiple dance genres as well as generates remarkable dance sequences qualitatively and quantitatively.},
  archive      = {J_TPAMI},
  author       = {Jinwoo Kim and Beom Kwon and Jongyoo Kim and Sanghoon Lee},
  doi          = {10.1109/TPAMI.2023.3312092},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15036-15050},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MNET++: Music-driven pluralistic dancing toward multiple dance genre synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMT: Cross domain few-shot learning via meta-memory
transfer. <em>TPAMI</em>, <em>45</em>(12), 15018–15035. (<a
href="https://doi.org/10.1109/TPAMI.2023.3306352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel categories solely relying on a few labeled samples, with existing few-shot methods primarily focusing on the categories sampled from the same distribution. Nevertheless, this assumption cannot always be ensured, and the actual domain shift problem significantly reduces the performance of few-shot learning. To remedy this problem, we investigate an interesting and challenging cross-domain few-shot learning task, where the training and testing tasks employ different domains. Specifically, we propose a Meta-Memory scheme to bridge the domain gap between source and target domains, leveraging style-memory and content-memory components. The former stores intra-domain style information from source domain instances and provides a richer feature distribution. The latter stores semantic information through exploration of knowledge of different categories. Under the contrastive learning strategy, our model effectively alleviates the cross-domain problem in few-shot learning. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on cross-domain few-shot semantic segmentation tasks on the COCO-20 $^{i}$ , PASCAL-5 $^{i}$ , FSS-1000, and SUIM datasets and positively affects few-shot classification tasks on Meta-Dataset.},
  archive      = {J_TPAMI},
  author       = {Wenjian Wang and Lijuan Duan and Yuxi Wang and Junsong Fan and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2023.3306352},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15018-15035},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MMT: Cross domain few-shot learning via meta-memory transfer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigating AC and DC interference in multi-ToF-camera
environments. <em>TPAMI</em>, <em>45</em>(12), 15005–15017. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-camera interference (MCI) is an important challenge faced by continuous-wave time-of-flight (C-ToF) cameras. In the presence of other cameras, a C-ToF camera may receive light from other cameras’ sources, resulting in potentially large depth errors. We propose stochastic exposure coding (SEC), a novel approach to mitigate MCI. In SEC, the camera integration time is divided into multiple time slots. Each camera is turned on during a slot with an optimal probability to avoid interference while maintaining high signal-to-noise ratio (SNR). The proposed approach has the following benefits. First, SEC can filter out both the AC and DC components of interfering signals effectively, which simultaneously achieves high SNR and mitigates depth errors. Second, time-slotting in SEC enables 3D imaging without saturation in the high photon flux regime. Third, the energy savings due to camera turning on during only a fraction of integration time can be utilized to amplify the source peak power, which increases the robustness of SEC to ambient light. Lastly, SEC can be implemented without modifying the C-ToF camera&#39;s coding functions, and thus, can be used with a wide range of cameras with minimal changes. We demonstrate the performance benefits of SEC with thorough theoretical analysis, simulations and real experiments, across a wide range of imaging scenarios.},
  archive      = {J_TPAMI},
  author       = {Jongho Lee and Mohit Gupta},
  doi          = {10.1109/TPAMI.2023.3307564},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {15005-15017},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mitigating AC and DC interference in multi-ToF-camera environments},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lottery jackpots exist in pre-trained models.
<em>TPAMI</em>, <em>45</em>(12), 14990–15004. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning is an effective approach to reduce network complexity with acceptable performance compromise. Existing studies achieve the sparsity of neural networks via time-consuming weight training or complex searching on networks with expanded width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks without the involvement of weight training, termed “lottery jackpots”, exist in pre-trained models with unexpanded width. Our presented lottery jackpots are traceable through empirical and theoretical outcomes. For example, we obtain a lottery jackpot that has only 10\% parameters and still reaches the performance of the original dense VGGNet-19 without any modifications on the pre-trained weights on CIFAR-10. Furthermore, we improve the efficiency for searching lottery jackpots from two perspectives. First, we observe that the sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among which, the magnitude-based pruning results in the most similar mask with ours. In compliance with this insight, we initialize our sparse mask using the magnitude-based pruning, resulting in at least 3× cost reduction on the lottery jackpot searching while achieving comparable or even better performance. Second, we conduct an in-depth analysis of the searching process for lottery jackpots. Our theoretical result suggests that the decrease in training loss during weight searching can be disturbed by the dependency between weights in modern networks. To mitigate this, we propose a novel short restriction method to restrict change of masks that may have potential negative impacts on the training loss, which leads to a faster convergence and reduced oscillation for searching lottery jackpots. Consequently, our searched lottery jackpot removes 90\% weights in ResNet-50, while it easily obtains more than 70\% top-1 accuracy using only 5 searching epochs on ImageNet.},
  archive      = {J_TPAMI},
  author       = {Yuxin Zhang and Mingbao Lin and Yunshan Zhong and Fei Chao and Rongrong Ji},
  doi          = {10.1109/TPAMI.2023.3311783},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14990-15004},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lottery jackpots exist in pre-trained models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long and short-range dependency graph structure learning
framework on point cloud. <em>TPAMI</em>, <em>45</em>(12), 14975–14989.
(<a href="https://doi.org/10.1109/TPAMI.2023.3298711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks can effectively process geometric data and thus have been successfully used in point cloud data representation. However, existing graph-based methods usually adopt the K-nearest neighbor (KNN) algorithm to construct graphs, which may not be optimal for point cloud analysis tasks, owning to the solution of KNN is independent of network training. In this paper, we propose a novel graph structure learning convolutional neural network (GSLCN) for multiple point cloud analysis tasks. The fundamental concept is to propose a general graph structure learning architecture (GSL) that builds long-range and short-range dependency graphs. To learn optimal graphs that best serve to extract local features and investigate global contextual information, respectively, we integrated the GSL with the designed graph convolution operator under a unified framework. Furthermore, we design the graph structure losses with some prior knowledge to guide graph learning during network training. The main benefit is that given labels and prior knowledge are taken into account in GSLCN, providing useful supervised information to build graphs and thus facilitating the graph convolution operation for the point cloud. Experimental results on challenging benchmarks demonstrate that the proposed framework achieves excellent performance for point cloud classification, part segmentation, and semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Jiye Liang and Zijin Du and Jianqing Liang and Kaixuan Yao and Feilong Cao},
  doi          = {10.1109/TPAMI.2023.3298711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14975-14989},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Long and short-range dependency graph structure learning framework on point cloud},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight pixel difference networks for efficient visual
representation learning. <em>TPAMI</em>, <em>45</em>(12), 14956–14974.
(<a href="https://doi.org/10.1109/TPAMI.2023.3300513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been tremendous efforts in developing lightweight Deep Neural Networks (DNNs) with satisfactory accuracy, which can enable the ubiquitous deployment of DNNs in edge devices. The core challenge of developing compact and efficient DNNs lies in how to balance the competing goals of achieving high accuracy and high efficiency. In this paper we propose two novel types of convolutions, dubbed Pixel Difference Convolution (PDC) and Binary PDC (Bi-PDC) which enjoy the following benefits: capturing higher-order local differential information, computationally efficient, and able to be integrated with existing DNNs. With PDC and Bi-PDC, we further present two lightweight deep networks named Pixel Difference Networks (PiDiNet) and Binary PiDiNet (Bi-PiDiNet) respectively to learn highly efficient yet more accurate representations for visual tasks including edge detection and object recognition. Extensive experiments on popular datasets (BSDS500, ImageNet, LFW, YTF, etc.) show that PiDiNet and Bi-PiDiNet achieve the best accuracy-efficiency trade-off. For edge detection, PiDiNet is the first network that can be trained without ImageNet, and can achieve the human-level performance on BSDS500 at 100 FPS and with $&amp;lt; $ 1 M parameters. For object recognition, among existing Binary DNNs, Bi-PiDiNet achieves the best accuracy and a nearly $2\times$ reduction of computational cost on ResNet18.},
  archive      = {J_TPAMI},
  author       = {Zhuo Su and Jiehua Zhang and Longguang Wang and Hua Zhang and Zhen Liu and Matti Pietikäinen and Li Liu},
  doi          = {10.1109/TPAMI.2023.3300513},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14956-14974},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lightweight pixel difference networks for efficient visual representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LibFewShot: A comprehensive library for few-shot learning.
<em>TPAMI</em>, <em>45</em>(12), 14938–14955. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning, especially few-shot image classification, has received increasing attention and witnessed significant advances in recent years. Some recent studies implicitly show that many generic techniques or “tricks”, such as data augmentation, pre-training, knowledge distillation, and self-supervision, may greatly boost the performance of a few-shot learning method. Moreover, different works may employ different software platforms, backbone architectures and input image sizes, making fair comparisons difficult and practitioners struggle with reproducibility. To address these situations, we propose a comprehensive library for few-shot learning (LibFewShot) by re-implementing eighteen state-of-the-art few-shot learning methods in a unified framework with the same single codebase in PyTorch. Furthermore, based on LibFewShot, we provide comprehensive evaluations on multiple benchmarks with various backbone architectures to evaluate common pitfalls and effects of different training tricks. In addition, with respect to the recent doubts on the necessity of meta- or episodic-training mechanism, our evaluation results confirm that such a mechanism is still necessary especially when combined with pre-training. We hope our work can not only lower the barriers for beginners to enter the area of few-shot learning but also elucidate the effects of nontrivial tricks to facilitate intrinsic research on few-shot learning.},
  archive      = {J_TPAMI},
  author       = {Wenbin Li and Ziyi Wang and Xuesong Yang and Chuanqi Dong and Pinzhuo Tian and Tiexin Qin and Jing Huo and Yinghuan Shi and Lei Wang and Yang Gao and Jiebo Luo},
  doi          = {10.1109/TPAMI.2023.3312125},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14938-14955},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LibFewShot: A comprehensive library for few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning gait representation from massive unlabelled walking
videos: A benchmark. <em>TPAMI</em>, <em>45</em>(12), 14920–14937. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait depicts individuals’ unique and distinguishing walking patterns and has become one of the most promising biometric features for human identification. As a fine-grained recognition task, gait recognition is easily affected by many factors and usually requires a large amount of completely annotated data that is costly and insatiable. This paper proposes a large-scale self-supervised benchmark for gait recognition with contrastive learning, aiming to learn the general gait representation from massive unlabelled walking videos for practical applications via offering informative walking priors and diverse real-world variations. Specifically, we collect a large-scale unlabelled gait dataset GaitLU-1M consisting of 1.02M walking sequences and propose a conceptually simple yet empirically powerful baseline model GaitSSB. Experimentally, we evaluate the pre-trained model on four widely-used gait benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer learning. The unsupervised results are comparable to or even better than the early model-based and GEI-based methods. After transfer learning, GaitSSB outperforms existing methods by a large margin in most cases, and also showcases the superior generalization capacity. Further experiments indicate that the pre-training can save about 50\% and 80\% annotation costs of GREW and Gait3D. Theoretically, we discuss the critical issues for gait-specific contrastive framework and present some insights for further study. As far as we know, GaitLU-1M is the first large-scale unlabelled gait dataset, and GaitSSB is the first method that achieves remarkable unsupervised results on the aforementioned benchmarks.},
  archive      = {J_TPAMI},
  author       = {Chao Fan and Saihui Hou and Jilong Wang and Yongzhen Huang and Shiqi Yu},
  doi          = {10.1109/TPAMI.2023.3312419},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14920-14937},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning gait representation from massive unlabelled walking videos: A benchmark},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from partially labeled data for multi-organ and
tumor segmentation. <em>TPAMI</em>, <em>45</em>(12), 14905–14919. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image benchmarks for the segmentation of organs and tumors suffer from the partially labeling issue due to its intensive cost of labor and expertise. Current mainstream approaches follow the practice of one network solving one task. With this pipeline, not only the performance is limited by the typically small dataset of a single task, but also the computation cost linearly increases with the number of tasks. To address this, we propose a Transformer based dynamic on-demand network (TransDoDNet) that learns to segment organs and tumors on multiple partially labeled datasets. Specifically, TransDoDNet has a hybrid backbone that is composed of the convolutional neural network and Transformer. A dynamic head enables the network to accomplish multiple segmentation tasks flexibly. Unlike existing approaches that fix kernels after training, the kernels in the dynamic head are generated adaptively by the Transformer, which employs the self-attention mechanism to model long-range organ-wise dependencies and decodes the organ embedding that can represent each organ. We create a large-scale partially labeled Multi-Organ and Tumor Segmentation benchmark, termed MOTS, and demonstrate the superior performance of our TransDoDNet over other competitors on seven organ and tumor segmentation tasks. This study also provides a general 3D medical image segmentation model, which has been pre-trained on the large-scale MOTS benchmark and has demonstrated advanced performance over current predominant self-supervised learning methods.},
  archive      = {J_TPAMI},
  author       = {Yutong Xie and Jianpeng Zhang and Yong Xia and Chunhua Shen},
  doi          = {10.1109/TPAMI.2023.3312587},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14905-14919},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning from partially labeled data for multi-organ and tumor segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning degradation-robust spatiotemporal
frequency-transformer for video super-resolution. <em>TPAMI</em>,
<em>45</em>(12), 14888–14904. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Super-Resolution (VSR) aims to restore high-resolution (HR) videos from low-resolution (LR) videos. Existing VSR techniques usually recover HR frames by extracting pertinent textures from nearby frames with known degradation processes. Despite significant progress, grand challenges remain to effectively extract and transmit high-quality textures from high-degraded low-quality sequences, such as blur, additive noises, and compression artifacts. This work proposes a novel degradation-robust Frequency-Transformer (FTVSR++) for handling low-quality videos that carry out self-attention in a combined space-time-frequency domain. First, video frames are split into patches and each patch is transformed into spectral maps in which each channel represents a frequency band. It permits a fine-grained self-attention on each frequency band so that real visual texture can be distinguished from artifacts. Second, a novel dual frequency attention (DFA) mechanism is proposed to capture the global and local frequency relations, which can handle different complicated degradation processes in real-world scenarios. Third, we explore different self-attention schemes for video processing in the frequency domain and discover that a “divided attention” which conducts joint space-frequency attention before applying temporal-frequency attention, leads to the best video enhancement quality. Extensive experiments on three widely-used VSR datasets show that FTVSR++ outperforms state-of-the-art methods on different low-quality videos with clear visual margins.},
  archive      = {J_TPAMI},
  author       = {Zhongwei Qiu and Huan Yang and Jianlong Fu and Daochang Liu and Chang Xu and Dongmei Fu},
  doi          = {10.1109/TPAMI.2023.3312166},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14888-14904},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning degradation-robust spatiotemporal frequency-transformer for video super-resolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning canonical embeddings for unsupervised shape
correspondence with locally linear transformations. <em>TPAMI</em>,
<em>45</em>(12), 14872–14887. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to unsupervised shape correspondence learning between pairs of point clouds. We make the first attempt to adapt the classical locally linear embedding algorithm (LLE)—originally designed for nonlinear dimensionality reduction—for shape correspondence. The key idea is to find dense correspondences between shapes by first obtaining high-dimensional neighborhood-preserving embeddings of low-dimensional point clouds and subsequently aligning the source and target embeddings using locally linear transformations. We demonstrate that learning the embedding using a new LLE-inspired point cloud reconstruction objective results in accurate shape correspondences. More specifically, the approach comprises an end-to-end learnable framework of extracting high-dimensional neighborhood-preserving embeddings, estimating locally linear transformations in the embedding space, and reconstructing shapes via divergence measure-based alignment of probability density functions built over reconstructed and target shapes. Our approach enforces embeddings of shapes in correspondence to lie in the same universal/canonical embedding space, which eventually helps regularize the learning process and leads to a simple nearest neighbors approach between shape embeddings for finding reliable correspondences. Comprehensive experiments show that the new method makes noticeable improvements over state-of-the-art approaches on standard shape correspondence benchmark datasets covering both human and nonhuman shapes.},
  archive      = {J_TPAMI},
  author       = {Pan He and Patrick Emami and Sanjay Ranka and Anand Rangarajan},
  doi          = {10.1109/TPAMI.2023.3307592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14872-14887},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning canonical embeddings for unsupervised shape correspondence with locally linear transformations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label efficient regularization and propagation for graph
node classification. <em>TPAMI</em>, <em>45</em>(12), 14856–14871. (<a
href="https://doi.org/10.1109/TPAMI.2023.3309970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An enhanced label propagation (LP) method called GraphHop was proposed recently. It outperforms graph convolutional networks (GCNs) in the semi-supervised node classification task on various networks. Although the performance of GraphHop was explained intuitively with joint node attribute and label signal smoothening, its rigorous mathematical treatment is lacking. In this paper, we propose a label efficient regularization and propagation (LERP) framework for graph node classification, and present an alternate optimization procedure for its solution. Furthermore, we show that GraphHop only offers an approximate solution to this framework and has two drawbacks. First, it includes all nodes in the classifier training without taking the reliability of pseudo-labeled nodes into account in the label update step. Second, it provides a rough approximation to the optimum of a subproblem in the label aggregation step. Based on the LERP framework, we propose a new method, named the LERP method, to solve these two shortcomings. LERP determines reliable pseudo-labels adaptively during the alternate optimization and provides a better approximation to the optimum with computational efficiency. Theoretical convergence of LERP is guaranteed. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of LERP. That is, LERP outperforms all benchmarking methods, including GraphHop, consistently on five common test datasets, two large-scale networks, and an object recognition task at extremely low label rates (i.e., 1, 2, 4, 8, 16, and 20 labeled samples per class).},
  archive      = {J_TPAMI},
  author       = {Tian Xie and Rajgopal Kannan and C.-C. Jay Kuo},
  doi          = {10.1109/TPAMI.2023.3309970},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14856-14871},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Label efficient regularization and propagation for graph node classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-induced multiple kernel fuzzy clustering.
<em>TPAMI</em>, <em>45</em>(12), 14838–14855. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of domain knowledge opens new horizons to fuzzy clustering. Then knowledge-driven and data-driven fuzzy clustering methods come into being. To address the challenges of inadequate extraction mechanism and imperfect fusion mode in such class of methods, we propose the Knowledge-induced Multiple Kernel Fuzzy Clustering (KMKFC) algorithm. First, to extract knowledge points better, the Relative Density-based Knowledge Extraction (RDKE) method is proposed to extract high-density knowledge points close to cluster centers of real data structure, and provide initialized cluster centers. Moreover, the multiple kernel mechanism is introduced to improve the adaptability of clustering algorithm and map data to high-dimensional space, so as to better discover the differences between the data and obtain superior clustering results. Second, knowledge points generated by RDKE are integrated into KMKFC through a knowledge-influence matrix to guide the iterative process of KMKFC. Third, we also provide a strategy of automatically obtaining knowledge points, and thus propose the RDKE with Automatic knowledge acquisition (RDKE-A) method and the corresponding KMKFC-A algorithm. Then we prove the convergence of KMKFC and KMKFC-A. Finally, experimental studies demonstrate that the KMKFC and KMKFC-A algorithms perform better than thirteen comparison algorithms with regard to four evaluation indexes and the convergence speed.},
  archive      = {J_TPAMI},
  author       = {Yiming Tang and Zhifu Pan and Xianghui Hu and Witold Pedrycz and Renhao Chen},
  doi          = {10.1109/TPAMI.2023.3298629},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14838-14855},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Knowledge-induced multiple kernel fuzzy clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive NeRF geometry editing with shape priors.
<em>TPAMI</em>, <em>45</em>(12), 14821–14837. (<a
href="https://doi.org/10.1109/TPAMI.2023.3315068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRFs) have shown great potential for tasks like novel view synthesis of static 3D scenes. Since NeRFs are trained on a large number of input images, it is not trivial to change their content afterwards. Previous methods to modify NeRFs provide some control but they do not support direct shape deformation which is common for geometry representations like triangle meshes. In this paper, we present a NeRF geometry editing method that first extracts a triangle mesh representation of the geometry inside a NeRF. This mesh can be modified by any 3D modeling tool (we use ARAP mesh deformation). The mesh deformation is then extended into a volume deformation around the shape which establishes a mapping between ray queries to the deformed NeRF and the corresponding queries to the original NeRF. The basic shape editing mechanism is extended towards more powerful and more meaningful editing handles by generating box abstractions of the NeRF shapes which provide an intuitive interface to the user. By additionally assigning semantic labels, we can even identify and combine parts from different objects. We demonstrate the performance and quality of our method in a number of experiments on synthetic data as well as real captured scenes.},
  archive      = {J_TPAMI},
  author       = {Yu-Jie Yuan and Yang-Tian Sun and Yu-Kun Lai and Yuewen Ma and Rongfei Jia and Leif Kobbelt and Lin Gao},
  doi          = {10.1109/TPAMI.2023.3315068},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14821-14837},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interactive NeRF geometry editing with shape priors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information bottleneck and aggregated learning.
<em>TPAMI</em>, <em>45</em>(12), 14807–14820. (<a
href="https://doi.org/10.1109/TPAMI.2023.3302150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning a neural network classifier. Under the information bottleneck (IB) principle, we associate with this classification problem a representation learning problem, which we call “IB learning”. We show that IB learning is, in fact, equivalent to a special class of the quantization problem. The classical results in rate-distortion theory then suggest that IB learning can benefit from a “vector quantization” approach, namely, simultaneously learning the representations of multiple input objects. Such an approach assisted with some variational techniques, result in a novel learning framework, “Aggregated Learning”, for classification with neural network models. In this framework, several objects are jointly classified by a single neural network. The effectiveness of this framework is verified through extensive experiments on standard image recognition and text classification tasks.},
  archive      = {J_TPAMI},
  author       = {Masoumeh Soflaei and Richong Zhang and Hongyu Guo and Ali Al-Bashabsheh and Yongyi Mao},
  doi          = {10.1109/TPAMI.2023.3302150},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14807-14820},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Information bottleneck and aggregated learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental learning for simultaneous augmentation of
feature and class. <em>TPAMI</em>, <em>45</em>(12), 14789–14806. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of new data collection ways in many dynamic environment applications, the samples are gathered gradually in the accumulated feature spaces. With the incorporation of new type features, it may result in the augmentation of class numbers. For instance, in activity recognition, using the old features during warm-up, we can separate different warm-up exercises. With the accumulation of new attributes obtained from newly added sensors, we can better separate the newly appeared formal exercises. Learning for such simultaneous augmentation of feature and class is crucial but rarely studied, particularly when the labeled samples with full observations are limited. In this paper, we tackle this problem by proposing a novel incremental learning method for Simultaneous Augmentation of Feature and Class (SAFC) in a two-stage way. To guarantee the reusability of the model trained on previous data, we add a regularizer in the current model, which can provide solid prior in training the new classifier. We also present the theoretical analyses about the generalization bound, which can validate the efficiency of model inheritance. After solving the one-shot problem, we also extend it to multi-shot. Experimental results demonstrate the effectiveness of our approaches, together with their effectiveness in activity recognition applications.},
  archive      = {J_TPAMI},
  author       = {Chenping Hou and Shilin Gu and Chao Xu and Yuhua Qian},
  doi          = {10.1109/TPAMI.2023.3307670},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14789-14806},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Incremental learning for simultaneous augmentation of feature and class},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-to-image translation with disentangled latent vectors
for face editing. <em>TPAMI</em>, <em>45</em>(12), 14777–14788. (<a
href="https://doi.org/10.1109/TPAMI.2023.3308102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an image-to-image translation framework for facial attribute editing with disentangled interpretable latent directions. Facial attribute editing task faces the challenges of targeted attribute editing with controllable strength and disentanglement in the representations of attributes to preserve the other attributes during edits. For this goal, inspired by the latent space factorization works of fixed pretrained GANs, we design the attribute editing by latent space factorization, and for each attribute, we learn a linear direction that is orthogonal to the others. We train these directions with orthogonality constraints and disentanglement losses. To project images to semantically organized latent spaces, we set an encoder-decoder architecture with attention-based skip connections. We extensively compare with previous image translation algorithms and editing with pretrained GAN works. Our extensive experiments show that our method significantly improves over the state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Yusuf Dalva and Hamza Pehlivan and Oyku Irmak Hatipoglu and Cansu Moran and Aysegul Dundar},
  doi          = {10.1109/TPAMI.2023.3308102},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14777-14788},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image-to-image translation with disentangled latent vectors for face editing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hunter: Exploring high-order consistency for point cloud
registration with severe outliers. <em>TPAMI</em>, <em>45</em>(12),
14760–14776. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After decades of investigation, point cloud registration is still a challenging task in practice, especially when the correspondences are contaminated by a large number of outliers. It may result in a rapidly decreasing probability of generating a hypothesis close to the true transformation, leading to the failure of point cloud registration. To tackle this problem, we propose a transformation estimation method, named Hunter, for robust point cloud registration with severe outliers. The core of Hunter is to design a global-to-local exploration scheme to robustly find the correct correspondences. The global exploration aims to exploit guided sampling to generate promising initial alignments. To this end, a hypergraph-based consistency reasoning module is introduced to learn the high-order consistency among correct correspondences, which is able to yield a more distinct inlier cluster that facilitates the generation of all-inlier hypotheses. Moreover, we propose a preference-based local exploration module that exploits the preference information of top- $k$ promising hypotheses to find a better transformation. This module can efficiently obtain multiple reliable transformation hypotheses by using a multi-initialization searching strategy. Finally, we present a distance-angle based hypothesis selection criterion to choose the most reliable transformation, which can avoid selecting symmetrically aligned false transformations. Experimental results on simulated, indoor, and outdoor datasets, demonstrate that Hunter can achieve significant superiority over the state-of-the-art methods, including both learning-based and traditional methods (as shown in Fig. 1). Moreover, experimental results also indicate that Hunter can achieve more stable performance compared with all other methods with severe outliers.},
  archive      = {J_TPAMI},
  author       = {Runzhao Yao and Shaoyi Du and Wenting Cui and Aixue Ye and Feng Wen and Hongbo Zhang and Zhiqiang Tian and Yue Gao},
  doi          = {10.1109/TPAMI.2023.3312592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14760-14776},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hunter: Exploring high-order consistency for point cloud registration with severe outliers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-guided reinforcement learning with sim-to-real
transfer for autonomous navigation. <em>TPAMI</em>, <em>45</em>(12),
14745–14759. (<a
href="https://doi.org/10.1109/TPAMI.2023.3314762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a promising approach in unmanned ground vehicles (UGVs) applications, but limited computing resource makes it challenging to deploy a well-behaved RL strategy with sophisticated neural networks. Meanwhile, the training of RL on navigation tasks is difficult, which requires a carefully-designed reward function and a large number of interactions, yet RL navigation can still fail due to many corner cases. This shows the limited intelligence of current RL methods, thereby prompting us to rethink combining RL with human intelligence. In this paper, a human-guided RL framework is proposed to improve RL performance both during learning in the simulator and deployment in the real world. The framework allows humans to intervene in RL&#39;s control progress and provide demonstrations as needed, thereby improving RL&#39;s capabilities. An innovative human-guided RL algorithm is proposed that utilizes a series of mechanisms to improve the effectiveness of human guidance, including human-guided learning objective, prioritized human experience replay, and human intervention-based reward shaping. Our RL method is trained in simulation and then transferred to the real world, and we develop a denoised representation for domain adaptation to mitigate the simulation-to-real gap. Our method is validated through simulations and real-world experiments to navigate UGVs in diverse and dynamic environments based only on tiny neural networks and image inputs. Our method performs better in goal-reaching and safety than existing learning- and model-based navigation approaches and is robust to changes in input features and ego kinetics. Furthermore, our method allows small-scale human demonstrations to be used to improve the trained RL agent and learn expected behaviors online.},
  archive      = {J_TPAMI},
  author       = {Jingda Wu and Yanxin Zhou and Haohan Yang and Zhiyu Huang and Chen Lv},
  doi          = {10.1109/TPAMI.2023.3314762},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14745-14759},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human-guided reinforcement learning with sim-to-real transfer for autonomous navigation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistically-attracted wireframe parsing: From supervised to
self-supervised learning. <em>TPAMI</em>, <em>45</em>(12), 14727–14744.
(<a href="https://doi.org/10.1109/TPAMI.2023.3312749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Holistically-Attracted Wireframe Parsing (HAWP), a method for geometric analysis of 2D images containing wireframes formed by line segments and junctions. HAWP utilizes a parsimonious Holistic Attraction (HAT) field representation that encodes line segments using a closed-form 4D geometric vector field. The proposed HAWP consists of three sequential components empowered by end-to-end and HAT-driven designs: 1) generating a dense set of line segments from HAT fields and endpoint proposals from heatmaps, 2) binding the dense line segments to sparse endpoint proposals to produce initial wireframes, and 3) filtering false positive proposals through a novel endpoint-decoupled line-of-interest aligning (EPD LOIAlign) module that captures the co-occurrence between endpoint proposals and HAT fields for better verification. Thanks to our novel designs, HAWPv2 shows strong performance in fully supervised learning, while HAWPv3 excels in self-supervised learning, achieving superior repeatability scores and efficient training (24 GPU hours on a single GPU). Furthermore, HAWPv3 exhibits a promising potential for wireframe parsing in out-of-distribution images without providing ground truth labels of wireframes.},
  archive      = {J_TPAMI},
  author       = {Nan Xue and Tianfu Wu and Song Bai and Fu-Dong Wang and Gui-Song Xia and Liangpei Zhang and Philip H. S. Torr},
  doi          = {10.1109/TPAMI.2023.3312749},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14727-14744},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Holistically-attracted wireframe parsing: From supervised to self-supervised learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher order fractal belief rényi divergence with its
applications in pattern classification. <em>TPAMI</em>, <em>45</em>(12),
14709–14726. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information can be quantified and expressed by uncertainty, and improving the decision level of uncertain information is vital in modeling and processing uncertain information. Dempster-Shafer evidence theory can model and process uncertain information effectively. However, the Dempster combination rule may provide counter-intuitive results when dealing with highly conflicting information, leading to a decline in decision level. Thus, measuring conflict is significant in the improvement of decision level. Motivated by this issue, this paper proposes a novel method to measure the discrepancy between bodies of evidence. First, the model of dynamic fractal probability transformation is proposed to effectively obtain more information about the non-specificity of basic belief assignments (BBAs). Then, we propose the higher-order fractal belief Rényi divergence (HOFBReD). HOFBReD can effectively measure the discrepancy between BBAs. Moreover, it is the first belief Rényi divergence that can measure the discrepancy between BBAs with dynamic fractal probability transformation. HoFBReD has several properties in terms of probability transformation as well as measurement. When the dynamic fractal probability transformation ends, HoFBReD is equivalent to measuring the Rényi divergence between the pignistic probability transformations of BBAs. When the BBAs degenerate to the probability distributions, HoFBReD will also degenerate to or be related to several well-known divergences. In addition, based on HoFBReD, a novel multisource information fusion algorithm is proposed. A pattern classification experiment with real-world datasets is presented to compare the proposed algorithm with other methods. The experiment results indicate that the proposed algorithm has a higher average pattern recognition accuracy with all datasets than other methods. The proposed discrepancy measurement method and multisource information algorithm contribute to the improvement of decision level.},
  archive      = {J_TPAMI},
  author       = {Yingcheng Huang and Fuyuan Xiao and Zehong Cao and Chin-Teng Lin},
  doi          = {10.1109/TPAMI.2023.3310594},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14709-14726},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Higher order fractal belief rényi divergence with its applications in pattern classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hierarchical optimization-derived learning. <em>TPAMI</em>,
<em>45</em>(12), 14693–14708. (<a
href="https://doi.org/10.1109/TPAMI.2023.3315333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, by utilizing optimization techniques to formulate the propagation of deep model, a variety of so-called Optimization-Derived Learning (ODL) approaches have been proposed to address diverse learning and vision tasks. Although having achieved relatively satisfying practical performance, there still exist fundamental issues in existing ODL methods. In particular, current ODL methods tend to consider model constructing and learning as two separate phases, and thus fail to formulate their underlying coupling and depending relationship. In this work, we first establish a new framework, named Hierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviors of optimization-derived model construction and its corresponding learning process. Then we rigorously prove the joint convergence of these two sub-tasks, from the perspectives of both approximation quality and stationary analysis. To our best knowledge, this is the first theoretical guarantee for these two coupled ODL components: optimization and learning. We further demonstrate the flexibility of our framework by applying HODL to challenging learning tasks, which have not been properly addressed by existing ODL methods. Finally, we conduct extensive experiments on both synthetic data and real applications in vision and other learning tasks to verify the theoretical properties and practical performance of HODL in various application scenarios.},
  archive      = {J_TPAMI},
  author       = {Risheng Liu and Xuan Liu and Shangzhi Zeng and Jin Zhang and Yixuan Zhang},
  doi          = {10.1109/TPAMI.2023.3315333},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14693-14708},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical optimization-derived learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical attention-based age estimation and bias
analysis. <em>TPAMI</em>, <em>45</em>(12), 14682–14692. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a Deep Learning approach to estimate age from facial images. First, we introduce a novel attention-based approach to image augmentation-aggregation, which allows multiple image augmentations to be adaptively aggregated using a Transformer-Encoder. A hierarchical probabilistic regression model is then proposed that combines discrete probabilistic age estimates with an ensemble of regressors. Each regressor is adapted and trained to refine the probability estimate over a given age range. We show that our age estimation scheme outperforms current schemes and provides a new state-of-the-art age estimation accuracy when applied to the MORPH II and CACD datasets. We also present an analysis of the biases in the results of the state-of-the-art age estimates.},
  archive      = {J_TPAMI},
  author       = {Shakediel Hiba and Yosi Keller},
  doi          = {10.1109/TPAMI.2023.3319472},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14682-14692},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical attention-based age estimation and bias analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HexNet: An orientation-aware deep learning framework for
omni-directional input. <em>TPAMI</em>, <em>45</em>(12), 14665–14681.
(<a href="https://doi.org/10.1109/TPAMI.2023.3307152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While omni-directional sensors provide holistic representations typical deep learning frameworks reduce the benefits by introducing distortions and discontinuities as spherical data is supplied as planar input. On the other hand, recent spherical convolutional neural networks (CNNs) often require significant memory and parameters, thus enabling execution only at very low resolutions and shallow architectures. We propose HexNet, an orientation-aware deep learning framework for spherical signals, that allows for fast computation as we exploit standard planar network operations on an efficiently arranged projection of the sphere. Furthermore, we introduce a graph-based version for partial spheres, allowing us to compete at high-resolution with planar CNNs using residual network architectures. Our kernels operate on the tangent of the sphere and thus standard feature weights, pretrained on perspective data, can be transferred, enabling spherical pretraining on ImageNet. As our design is free of distortions and discontinuity, our orientation-aware CNN becomes a new state of the art for semantic segmentation on the recent 2D3DS dataset, and the omni-directional version of SYNTHIA introduced in this work. Moreover, we experimentally show the benefit of our spherical representation over standard images on the Cityscapes dataset by reducing distortion effects of planar CNNs. We implement object detection for the spherical domain. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
  archive      = {J_TPAMI},
  author       = {Chao Zhang and Stephan Liwicki and Sen He and William Smith and Roberto Cipolla},
  doi          = {10.1109/TPAMI.2023.3307152},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14665-14681},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HexNet: An orientation-aware deep learning framework for omni-directional input},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling multi-class problem by intuitionistic fuzzy twin
support vector machines based on relative density information.
<em>TPAMI</em>, <em>45</em>(12), 14653–14664. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intuitionistic fuzzy twin support vector machine (IFTSVM) merges the idea of the intuitionistic fuzzy set (IFS) with the twin support vector machine (TSVM), which can reduce the negative impact of noise and outliers. However, this technique is not suitable for multi-class and high-dimensional feature space problems. Furthermore, the computational complexity of IFTSVM is high because it uses the membership and non-membership functions to build a score function. We propose a new version of IFTSVM by using relative density information. This idea approximates the probability density distribution in multi-dimensional continuous space by computing the K-nearest-neighbor distance of each training sample. Then, we evaluate all the training points by a one-versus-one-versus-rest strategy to construct the k-class classification hyperplanes. A coordinate descent system is utilized to reduce the computational complexity of the training. The bootstrap technique with a 95 $\%$ confidence interval and Friedman test are conducted to quantify the significance of the performance improvements observed in numerical evaluations. Experiments on 24 benchmark datasets demonstrate the proposed method produces promising results as compared with other support vector machine models reported in the literature.},
  archive      = {J_TPAMI},
  author       = {Salim Rezvani and Junhao Wu},
  doi          = {10.1109/TPAMI.2023.3310908},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14653-14664},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Handling multi-class problem by intuitionistic fuzzy twin support vector machines based on relative density information},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H4MER: Human 4D modeling by learning neural compositional
representation with transformer. <em>TPAMI</em>, <em>45</em>(12),
14639–14652. (<a
href="https://doi.org/10.1109/TPAMI.2023.3313311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive results achieved by deep learning based 3D reconstruction, the techniques of directly learning to model 4D human captures with detailed geometry have been less studied. This work presents a novel neural compositional representation for Human 4D Modeling with transformER (H4MER). Specifically, our H4MER is a compact and compositional representation for dynamic human by exploiting the human body prior from the widely used SMPL parametric model. Thus, H4MER can represent a dynamic 3D human over a temporal span with the codes of shape, initial pose, motion and auxiliaries. A simple yet effective linear motion model is proposed to provide a rough and regularized motion estimation, followed by per-frame compensation for pose and geometry details with the residual encoded in the auxiliary codes. We present a novel Transformer-based feature extractor and conditional GRU decoder to facilitate learning and improve the representation capability. Extensive experiments demonstrate our method is not only effective in recovering dynamic human with accurate motion and detailed geometry, but also amenable to various 4D human related tasks, including monocular video fitting, motion retargeting, 4D completion, and future prediction.},
  archive      = {J_TPAMI},
  author       = {Boyan Jiang and Yinda Zhang and Jingyang Huo and Xiangyang Xue and Yanwei Fu},
  doi          = {10.1109/TPAMI.2023.3313311},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14639-14652},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {H4MER: Human 4D modeling by learning neural compositional representation with transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-time convolutional neural networks: Architecture and
theoretical analysis. <em>TPAMI</em>, <em>45</em>(12), 14625–14638. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devising and analysing learning models for spatiotemporal network data is of importance for tasks including forecasting, anomaly detection, and multi-agent coordination, among others. Graph Convolutional Neural Networks (GCNNs) are an established approach to learn from time-invariant network data. The graph convolution operation offers a principled approach to aggregate information and offers mathematical analysis by exploring tools from graph signal processing. This analysis provides insights into the equivariance properties of GCNNs; spectral behaviour of the learned filters; and the stability to graph perturbations, which arise from support perturbations or uncertainties. However, extending the convolutional learning and respective analysis to the spatiotemporal domain is challenging because spatiotemporal data have more intrinsic dependencies. Hence, a higher flexibility to capture jointly the spatial and temporal dependencies is required to learn meaningful higher-order representations. Here, we leverage product graphs to represent the spatiotemporal dependencies in the data and introduce Graph-Time Convolutional Neural Networks (GTCNNs) as a principled architecture. We also introduce a parametric product graph to learn the spatiotemporal coupling. The convolution principle further allows a similar mathematical tractability as for GCNNs. In particular, the stability result shows GTCNNs are stable to spatial perturbations. owever, there is an implicit trade-off between discriminability and robustness; i.e., the more complex the model, the less stable. Extensive numerical results on benchmark datasets corroborate our findings and show the GTCNN compares favorably with state-of-the-art solutions. We anticipate the GTCNN to be a starting point for more sophisticated models that achieve good performance but are also fundamentally grounded.},
  archive      = {J_TPAMI},
  author       = {Mohammad Sabbaqi and Elvin Isufi},
  doi          = {10.1109/TPAMI.2023.3311912},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14625-14638},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph-time convolutional neural networks: Architecture and theoretical analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative multi-label zero-shot learning. <em>TPAMI</em>,
<em>45</em>(12), 14611–14624. (<a
href="https://doi.org/10.1109/TPAMI.2023.3295772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label zero-shot learning strives to classify images into multiple unseen categories for which no data is available during training. The test samples can additionally contain seen categories in the generalized variant. Existing approaches rely on learning either shared or label-specific attention from the seen classes. Nevertheless, computing reliable attention maps for unseen classes during inference in a multi-label setting is still a challenge. In contrast, state-of-the-art single-label generative adversarial network (GAN) based approaches learn to directly synthesize the class-specific visual features from the corresponding class attribute embeddings. However, synthesizing multi-label features from GANs is still unexplored in the context of zero-shot setting. When multiple objects occur jointly in a single image, a critical question is how to effectively fuse multi-class information. In this work, we introduce different fusion approaches at the attribute-level, feature-level and cross-level (across attribute and feature-levels) for synthesizing multi-label features from their corresponding multi-label class embeddings. To the best of our knowledge, our work is the first to tackle the problem of multi-label feature synthesis in the (generalized) zero-shot setting. Our cross-level fusion-based generative approach outperforms the state-of-the-art on three zero-shot benchmarks: NUS-WIDE, Open Images and MS COCO. Furthermore, we show the generalization capabilities of our fusion approach in the zero-shot detection task on MS COCO, achieving favorable performance against existing methods.},
  archive      = {J_TPAMI},
  author       = {Akshita Gupta and Sanath Narayan and Salman Khan and Fahad Shahbaz Khan and Ling Shao and Joost van de Weijer},
  doi          = {10.1109/TPAMI.2023.3295772},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14611-14624},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generative multi-label zero-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained species recognition with privileged pooling:
Better sample efficiency through supervised attention. <em>TPAMI</em>,
<em>45</em>(12), 14575–14589. (<a
href="https://doi.org/10.1109/TPAMI.2023.3316718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a scheme for supervised image classification that uses privileged information, in the form of keypoint annotations for the training data, to learn strong models from small and/or biased training sets. Our main motivation is the recognition of animal species for ecological applications such as biodiversity modelling, which is challenging because of long-tailed species distributions due to rare species, and strong dataset biases such as repetitive scene background in camera traps. To counteract these challenges, we propose a visual attention mechanism that is supervised via keypoint annotations that highlight important object parts. This privileged information, implemented as a novel privileged pooling operation, is only required during training and helps the model to focus on regions that are discriminative. In experiments with three different animal species datasets, we show that deep networks with privileged pooling can use small training sets more efficiently and generalize better.},
  archive      = {J_TPAMI},
  author       = {Andrés C. Rodríguez and Stefano D’Aronco and Konrad Schindler and Jan Dirk Wegner},
  doi          = {10.1109/TPAMI.2023.3316718},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14575-14589},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fine-grained species recognition with privileged pooling: Better sample efficiency through supervised attention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine detailed texture learning for 3D meshes with generative
models. <em>TPAMI</em>, <em>45</em>(12), 14563–14574. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method to achieve fine detailed texture learning for 3D models that are reconstructed from both multi-view and single-view images. The framework is posed as an adaptation problem and is done progressively where in the first stage, we focus on learning accurate geometry, whereas in the second stage, we focus on learning the texture with a generative adversarial network. The contributions of the paper are in the generative learning pipeline where we propose two improvements. First, since the learned textures should be spatially aligned, we propose an attention mechanism that relies on the learnable positions of pixels. Second, since discriminator receives aligned texture maps, we augment its input with a learnable embedding which improves the feedback to the generator. We achieve significant improvements on multi-view sequences from Tripod dataset as well as on single-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method achieves superior 3D textured models compared to the previous works.},
  archive      = {J_TPAMI},
  author       = {Aysegul Dundar and Jun Gao and Andrew Tao and Bryan Catanzaro},
  doi          = {10.1109/TPAMI.2023.3319429},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14563-14574},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fine detailed texture learning for 3D meshes with generative models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast-SNN: Fast spiking neural network by converting
quantized ANN. <em>TPAMI</em>, <em>45</em>(12), 14546–14562. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have shown advantages in computation and energy efficiency over traditional artificial neural networks (ANNs) thanks to their event-driven representations. SNNs also replace weight multiplications in ANNs with additions, which are more energy-efficient and less computationally intensive. However, it remains a challenge to train deep SNNs due to the discrete spiking function. A popular approach to circumvent this challenge is ANN-to-SNN conversion. However, due to the quantization error and accumulating error, it often requires lots of time steps (high inference latency) to achieve high performance, which negates SNN&#39;s advantages. To this end, this paper proposes Fast-SNN that achieves high performance with low latency. We demonstrate the equivalent mapping between temporal quantization in SNNs and spatial quantization in ANNs, based on which the minimization of the quantization error is transferred to quantized ANN training. With the minimization of the quantization error, we show that the sequential error is the primary cause of the accumulating error, which is addressed by introducing a signed IF neuron model and a layer-wise fine-tuning mechanism. Our method achieves state-of-the-art performance and low latency on various computer vision tasks, including image classification, object detection, and semantic segmentation. Codes are available at: https://github.com/yangfan-hu/Fast-SNN .},
  archive      = {J_TPAMI},
  author       = {Yangfan Hu and Qian Zheng and Xudong Jiang and Gang Pan},
  doi          = {10.1109/TPAMI.2023.3275769},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14546-14562},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast-SNN: Fast spiking neural network by converting quantized ANN},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FaceScape: 3D facial dataset and benchmark for single-view
3D face reconstruction. <em>TPAMI</em>, <em>45</em>(12), 14528–14545.
(<a href="https://doi.org/10.1109/TPAMI.2023.3307338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a large-scale detailed 3D face dataset, FaceScape , and the corresponding benchmark to evaluate single-view facial 3D reconstruction. By training on FaceScape data, a novel algorithm is proposed to predict elaborate riggable 3D face models from a single image input. FaceScape dataset releases 16,940 textured 3D faces, captured from 847 subjects and each with 20 specific expressions. The 3D models contain the pore-level facial geometry that is also processed to be topologically uniform. These fine 3D facial models can be represented as a 3D morphable model for coarse shapes and displacement maps for detailed geometry. Taking advantage of the large-scale and high-accuracy dataset, a novel algorithm is further proposed to learn the expression-specific dynamic details using a deep neural network. The learned relationship serves as the foundation of our 3D face prediction system from a single image input. Different from most previous methods, our predicted 3D models are riggable with highly detailed geometry under different expressions. We also use FaceScape data to generate the in-the-wild and in-the-lab benchmark to evaluate recent methods of single-view face reconstruction. The accuracy is reported and analyzed on the dimensions of camera pose and focal length, which provides a faithful and comprehensive evaluation and reveals new challenges. The unprecedented dataset, benchmark, and code have been released to the public for research purpose.},
  archive      = {J_TPAMI},
  author       = {Hao Zhu and Haotian Yang and Longwei Guo and Yidi Zhang and Yanru Wang and Mingkai Huang and Menghua Wu and Qiu Shen and Ruigang Yang and Xun Cao},
  doi          = {10.1109/TPAMI.2023.3307338},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14528-14545},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FaceScape: 3D facial dataset and benchmark for single-view 3D face reconstruction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving domain generalization via latent structure-aware
sequential autoencoder. <em>TPAMI</em>, <em>45</em>(12), 14514–14527.
(<a href="https://doi.org/10.1109/TPAMI.2023.3319984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) refers to the problem of generalizing machine learning systems to out-of-distribution (OOD) data with knowledge learned from several provided source domains. Most prior works confine themselves to stationary and discrete environments to tackle such generalization issue arising from OOD data. However, in practice, many tasks in non-stationary environments (e.g., autonomous-driving car system, sensor measurement) involve more complex and continuously evolving domain drift, emerging new challenges for model deployment. In this paper, we first formulate this setting as the problem of evolving domain generalization . To deal with the continuously changing domains, we propose MMD-LSAE, a novel framework that learns to capture the evolving patterns among domains for better generalization. Specifically, MMD-LSAE characterizes OOD data in non-stationary environments with two types of distribution shifts: covariate shift and concept shift , and employs deep autoencoder modules to infer their dynamics in latent space separately. In these modules, the inferred posterior distributions of latent codes are optimized to align with their corresponding prior distributions via minimizing maximum mean discrepancy (MMD). We theoretically verify that MMD-LSAE has the inherent capability to implicitly facilitate mutual information maximization, which can promote superior representation learning and improved generalization of the model. Furthermore, the experimental results on both synthetic and real-world datasets show that our proposed approach can consistently achieve favorable performance based on the evolving domain generalization setting.},
  archive      = {J_TPAMI},
  author       = {Tiexin Qin and Shiqi Wang and Haoliang Li},
  doi          = {10.1109/TPAMI.2023.3319984},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14514-14527},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evolving domain generalization via latent structure-aware sequential autoencoder},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the generalization ability of super-resolution
networks. <em>TPAMI</em>, <em>45</em>(12), 14497–14513. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance and generalization ability are two important aspects to evaluate the deep learning models. However, research on the generalization ability of Super-Resolution (SR) networks is currently absent. Assessing the generalization ability of deep models not only helps us to understand their intrinsic mechanisms, but also allows us to quantitatively measure their applicability boundaries, which is important for unrestricted real-world applications. To this end, we make the first attempt to propose a Generalization Assessment Index for SR networks, namely SRGA. SRGA exploits the statistical characteristics of the internal features of deep networks to measure the generalization ability. Specially, it is a non-parametric and non-learning metric. To better validate our method, we collect a patch-based image evaluation set (PIES) that includes both synthetic and real-world images, covering a wide range of degradations. With SRGA and PIES dataset, we benchmark existing SR models on the generalization ability. This work provides insights and tools for future research on model generalization in low-level vision.},
  archive      = {J_TPAMI},
  author       = {Yihao Liu and Hengyuan Zhao and Jinjin Gu and Yu Qiao and Chao Dong},
  doi          = {10.1109/TPAMI.2023.3312313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14497-14513},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evaluating the generalization ability of super-resolution networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end one-shot human parsing. <em>TPAMI</em>,
<em>45</em>(12), 14481–14496. (<a
href="https://doi.org/10.1109/TPAMI.2023.3301672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous human parsing methods are limited to parsing humans into pre-defined classes, which is inflexible for practical fashion applications that often have new fashion item classes. In this paper, we define a novel one-shot human parsing (OSHP) task that requires parsing humans into an open set of classes defined by any test example. During training, only base classes are exposed, which only overlap with part of the test-time classes. To address three main challenges in OSHP, i.e., small sizes, testing bias, and similar parts, we devise an End-to-end One-shot human Parsing Network (EOP-Net). Firstly, an end-to-end human parsing framework is proposed to parse the query image into both coarse-grained and fine-grained human classes, which embeds rich semantic information that is shared across different granularities to identify the small-sized human classes. Then, we gradually smooth the training-time static prototypes to get robust class representations. Moreover, we employ a dynamic objective to encourage the network enhancing features’ representational capability in the early training phase while improving features’ transferability in the late training phase. Therefore, our method can quickly adapt to the novel classes and mitigate the testing bias issue. In addition, we add a contrastive loss at the prototype level to enforce inter-class distances, thereby discriminating the similar parts. For comprehensive evaluations on the new task, we tailor three existing popular human parsing benchmarks to the OSHP task. Experiments demonstrate that EOP-Net outperforms representative one-shot segmentation models by large margins and serves as a strong baseline for further research.},
  archive      = {J_TPAMI},
  author       = {Haoyu He and Jing Zhang and Bohan Zhuang and Jianfei Cai and Dacheng Tao},
  doi          = {10.1109/TPAMI.2023.3301672},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14481-14496},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {End-to-end one-shot human parsing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient spatially sparse inference for conditional GANs
and diffusion models. <em>TPAMI</em>, <em>45</em>(12), 14465–14480. (<a
href="https://doi.org/10.1109/TPAMI.2023.3316020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users prone to gradually edit the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about 1\%-area edits, SIGE accelerates DDPM by $3.0\times$ on NVIDIA RTX 3090 and $4.6\times$ on Apple M1 Pro GPU, Stable Diffusion by $7.2\times$ on 3090, and GauGAN by $5.6\times$ on 3090 and $5.2\times$ on M1 Pro GPU. Compared to our conference paper, we enhance SIGE to accommodate attention layers and apply it to Stable Diffusion. Additionally, we offer support for Apple M1 Pro GPU and include more results to substantiate the efficacy of our method.},
  archive      = {J_TPAMI},
  author       = {Muyang Li and Ji Lin and Chenlin Meng and Stefano Ermon and Song Han and Jun-Yan Zhu},
  doi          = {10.1109/TPAMI.2023.3316020},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14465-14480},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient spatially sparse inference for conditional GANs and diffusion models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient federated learning via local adaptive amended
optimizer with linear speedup. <em>TPAMI</em>, <em>45</em>(12),
14453–14464. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer ( FedLADA ), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of FedLADA with a linear speedup property on the non-convex case under the partial participation settings. Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficacy of our proposed FedLADA , which could greatly reduce the communication rounds and achieves higher accuracy than several baselines.},
  archive      = {J_TPAMI},
  author       = {Yan Sun and Li Shen and Hao Sun and Liang Ding and Dacheng Tao},
  doi          = {10.1109/TPAMI.2023.3300886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14453-14464},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient federated learning via local adaptive amended optimizer with linear speedup},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge guided GANs with multi-scale contrastive learning for
semantic image synthesis. <em>TPAMI</em>, <em>45</em>(12), 14435–14452.
(<a href="https://doi.org/10.1109/TPAMI.2023.3298721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel e dge guided g enerative a dversarial n etwork with c ontrastive learning (ECGAN) for the challenging semantic image synthesis task. Although considerable improvements have been achieved by the community in the recent period, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it challenging to synthesize local details and structures; 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects); 3) Existing semantic image synthesis methods focus on modeling “local” semantic information from a single input semantic layout. However, they ignore “global” semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use the edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. Edge information is produced by a convolutional generator and introduces detailed structure information. To tackle 2), we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout to preserve the semantic information. To tackle 3), inspired by current methods in contrastive learning, we propose a novel contrastive learning method, which aims to enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes. We further propose a novel multi-scale contrastive learning method that aims to push same-class features from different scales closer together being able to capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts from different scales. Experiments on three challenging datasets show that our methods achieve significantly better results than state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Guolei Sun and Nicu Sebe and Luc Van Gool},
  doi          = {10.1109/TPAMI.2023.3298721},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14435-14452},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Edge guided GANs with multi-scale contrastive learning for semantic image synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Dynamic loss for robust learning. <em>TPAMI</em>,
<em>45</em>(12), 14420–14434. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise and class imbalance are common challenges encountered in real-world datasets. Existing approaches for robust learning often focus on addressing either label noise or class imbalance individually, resulting in suboptimal performance when both biases are present. To bridge this gap, this work introduces a novel meta-learning-based dynamic loss that adapts the objective functions during the training process to effectively learn a classifier from long-tailed noisy data. Specifically, our dynamic loss consists of two components: a label corrector and a margin generator. The label corrector is responsible for correcting noisy labels, while the margin generator generates per-class classification margins by capturing the underlying data distribution and the learning state of the classifier. In addition, we employ a hierarchical sampling strategy that enriches a small amount of unbiased metadata with diverse and challenging samples. This enables the joint optimization of the two components in the dynamic loss through meta-learning, allowing the classifier to effectively adapt to clean and balanced test data. Extensive experiments conducted on multiple real-world and synthetic datasets with various types of data biases, including CIFAR-10/100, Animal-10N, ImageNet-LT, and Webvision, demonstrate that our method achieves state-of-the-art accuracy.},
  archive      = {J_TPAMI},
  author       = {Shenwang Jiang and Jianan Li and Jizhou Zhang and Ying Wang and Tingfa Xu},
  doi          = {10.1109/TPAMI.2023.3311636},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14420-14434},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic loss for robust learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic keypoint detection network for image matching.
<em>TPAMI</em>, <em>45</em>(12), 14404–14419. (<a
href="https://doi.org/10.1109/TPAMI.2023.3307889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing effective correspondences between a pair of images is difficult due to real-world challenges such as illumination, viewpoint and scale variations. Modern detector-based methods typically learn fixed detectors from a given dataset, which is hard to extract repeatable and reliable keypoints for various images with extreme appearance changes and weakly textured scenes. To deal with this problem, we propose a novel Dynamic Keypoint Detection Network (DKDNet) for robust image matching via a dynamic keypoint feature learning module and a guided heatmap activator. The proposed DKDNet enjoys several merits. First, the proposed dynamic keypoint feature learning module can generate adaptive keypoint features via the attention mechanism, which is flexibly updated with the current input image and can capture keypoints with different patterns. Second, the guided heatmap activator can effectively fuse multi-group keypoint heatmaps by fully considering the importance of different feature channels, which can realize more robust keypoint detection. Extensive experimental results on four standard benchmarks demonstrate that our DKDNet outperforms state-of-the-art image-matching methods by a large margin. Specifically, our DKDNet can outperform the best image-matching method by 2.1\% in AUC@ 3px on HPatches, 3.74\% in AUC@ $5^\circ$ on ScanNet, 7.14\% in AUC@ $5^\circ$ on MegaDepth and 12.32\% in AUC@ $5^\circ$ on YFCC100M.},
  archive      = {J_TPAMI},
  author       = {Yuan Gao and Jianfeng He and Tianzhu Zhang and Zhe Zhang and Yongdong Zhang},
  doi          = {10.1109/TPAMI.2023.3307889},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14404-14419},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic keypoint detection network for image matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). DreamStone: Image as a stepping stone for text-guided 3D
shape generation. <em>TPAMI</em>, <em>45</em>(12), 14385–14403. (<a
href="https://doi.org/10.1109/TPAMI.2023.3321329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new text-guided 3D shape generation approach DreamStone that uses images as a stepping stone to bridge the gap between the text and shape modalities for generating 3D shapes without requiring paired text and 3D data. The core of our approach is a two-stage feature-space alignment strategy that leverages a pre-trained single-view reconstruction (SVR) model to map CLIP features to shapes: to begin with, map the CLIP image feature to the detail-rich 3D shape space of the SVR model, then map the CLIP text feature to the 3D shape space through encouraging the CLIP-consistency between the rendered images and the input text. Besides, to extend beyond the generative capability of the SVR model, we design the text-guided 3D shape stylization module that can enhance the output shapes with novel structures and textures. Further, we exploit pre-trained text-to-image diffusion models to enhance the generative diversity, fidelity, and stylization capability. Our approach is generic, flexible, and scalable. It can be easily integrated with various SVR models to expand the generative space and improve the generative fidelity. Extensive experimental results demonstrate that our approach outperforms the state-of-the-art methods in terms of generative quality and consistency with the input text.},
  archive      = {J_TPAMI},
  author       = {Zhengzhe Liu and Peng Dai and Ruihui Li and Xiaojuan Qi and Chi-Wing Fu},
  doi          = {10.1109/TPAMI.2023.3321329},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14385-14403},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamStone: Image as a stepping stone for text-guided 3D shape generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DPCN++: Differentiable phase correlation network for
versatile pose registration. <em>TPAMI</em>, <em>45</em>(12),
14366–14384. (<a
href="https://doi.org/10.1109/TPAMI.2023.3317501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose registration is critical in vision and robotics. This article focuses on the challenging task of initialization-free pose registration up to 7DoF for homogeneous and heterogeneous measurements. While recent learning-based methods show promise using differentiable solvers, they either rely on heuristically defined correspondences or require initialization. Phase correlation seeks solutions in the spectral domain and is correspondence-free and initialization-free. Following this, we propose a differentiable solver and combine it with simple feature extraction networks, namely DPCN++. It can perform registration for homo/hetero inputs and generalizes well on unseen objects. Specifically, the feature extraction networks first learn dense feature grids from a pair of homogeneous/heterogeneous measurements. These feature grids are then transformed into a translation and scale invariant spectrum representation based on Fourier transform and spherical radial aggregation, decoupling translation and scale from rotation. Next, the rotation, scale, and translation are independently and efficiently estimated in the spectrum step-by-step. The entire pipeline is differentiable and trained end-to-end. We evaluate DCPN++ on a wide range of tasks taking different input modalities, including 2D bird’s-eye view images, 3D object and scene measurements, and medical images. Experimental results demonstrate that DCPN++ outperforms both classical and learning-based baselines, especially on partially observed and heterogeneous measurements.},
  archive      = {J_TPAMI},
  author       = {Zexi Chen and Yiyi Liao and Haozhe Du and Haodong Zhang and Xuecheng Xu and Haojian Lu and Rong Xiong and Yue Wang},
  doi          = {10.1109/TPAMI.2023.3317501},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14366-14384},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DPCN++: Differentiable phase correlation network for versatile pose registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adaptive object detection via balancing between
self-training and adversarial learning. <em>TPAMI</em>, <em>45</em>(12),
14353–14365. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based object detectors struggle generalizing to a new target domain bearing significant variations in object and background. Most current methods align domains by using image or instance-level adversarial feature alignment. This often suffers due to unwanted background and lacks class-specific alignment. A straightforward approach to promote class-level alignment is to use high confidence predictions on unlabeled domain as pseudo-labels. These predictions are often noisy since model is poorly calibrated under domain shift. In this paper, we propose to leverage model&#39;s predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. We develop a technique to quantify predictive uncertainty on class assignments and bounding-box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-training, whereas the ones with higher uncertainty are used to generate tiles for adversarial feature alignment. This synergy between tiling around uncertain object regions and generating pseudo-labels from highly certain object regions allows capturing both image and instance-level context during the model adaptation. We report thorough ablation study to reveal the impact of different components in our approach. Results on five diverse and challenging adaptation scenarios show that our approach outperforms existing state-of-the-art methods with noticeable margins.},
  archive      = {J_TPAMI},
  author       = {Muhammad Akhtar Munir and Muhammad Haris Khan and M. Saquib Sarfraz and Mohsen Ali},
  doi          = {10.1109/TPAMI.2023.3290135},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14353-14365},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Domain adaptive object detection via balancing between self-training and adversarial learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Distributionally robust memory evolution with generalized
divergence for continual learning. <em>TPAMI</em>, <em>45</em>(12),
14337–14352. (<a
href="https://doi.org/10.1109/TPAMI.2023.3317874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) aims to learn a non-stationary data distribution and not forget previous knowledge. The effectiveness of existing approaches that rely on memory replay can decrease over time as the model tends to overfit the stored examples. As a result, the model&#39;s ability to generalize well is significantly constrained. Additionally, these methods often overlook the inherent uncertainty in the memory data distribution, which differs significantly from the distribution of all previous data examples. To overcome these issues, we propose a principled memory evolution framework that dynamically adjusts the memory data distribution. This evolution is achieved by employing distributionally robust optimization (DRO) to make the memory buffer increasingly difficult to memorize. We consider two types of constraints in DRO: $f$ -divergence and Wasserstein ball constraints. For $f$ -divergence constraint, we derive a family of methods to evolve the memory buffer data in the continuous probability measure space with Wasserstein gradient flow (WGF). For Wasserstein ball constraint, we directly solve it in the euclidean space. Extensive experiments on existing benchmarks demonstrate the effectiveness of the proposed methods for alleviating forgetting. As a by-product of the proposed framework, our method is more robust to adversarial examples than compared CL methods.},
  archive      = {J_TPAMI},
  author       = {Zhenyi Wang and Li Shen and Tiehang Duan and Qiuling Suo and Le Fang and Wei Liu and Mingchen Gao},
  doi          = {10.1109/TPAMI.2023.3317874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14337-14352},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Distributionally robust memory evolution with generalized divergence for continual learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Discrete and balanced spectral clustering with scalability.
<em>TPAMI</em>, <em>45</em>(12), 14321–14336. (<a
href="https://doi.org/10.1109/TPAMI.2023.3311828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral Clustering (SC) has been the main subject of intensive research due to its remarkable clustering performance. Despite its successes, most existing SC methods suffer from several critical issues. First, they typically involve two independent stages, i.e., learning the continuous relaxation matrix followed by the discretization of the cluster indicator matrix. This two-stage approach can result in suboptimal solutions that negatively impact the clustering performance. Second, these methods are hard to maintain the balance property of clusters inherent in many real-world data, which restricts their practical applicability. Finally, these methods are computationally expensive and hence unable to handle large-scale datasets. In light of these limitations, we present a novel Discrete and Balanced Spectral Clustering with Scalability (DBSC) model that integrates the learning the continuous relaxation matrix and the discrete cluster indicator matrix into a single step. Moreover, the proposed model also maintains the size of each cluster approximately equal, thereby achieving soft-balanced clustering. What&#39;s more, the DBSC model incorporates an anchor-based strategy to improve its scalability to large-scale datasets. The experimental results demonstrate that our proposed model outperforms existing methods in terms of both clustering performance and balance performance. Specifically, the clustering accuracy of DBSC on CMUPIE data achieved a 17.93\% improvement compared with that of the SOTA methods (LABIN, EBSC, etc.).},
  archive      = {J_TPAMI},
  author       = {Rong Wang and Huimin Chen and Yihang Lu and Qianrong Zhang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TPAMI.2023.3311828},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14321-14336},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discrete and balanced spectral clustering with scalability},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digging into uncertainty-based pseudo-label for robust
stereo matching. <em>TPAMI</em>, <em>45</em>(12), 14301–14320. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the domain differences and unbalanced disparity distribution across multiple datasets, current stereo matching approaches are commonly limited to a specific dataset and generalize poorly to others. Such domain shift issue is usually addressed by substantial adaptation on costly target-domain ground-truth data, which cannot be easily obtained in practical settings. In this paper, we propose to dig into uncertainty estimation for robust stereo matching. Specifically, to balance the disparity distribution, we employ a pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching space, in this way driving the network progressively prune out the space of unlikely correspondences. Then, to solve the limited ground truth data, an uncertainty-based pseudo-label is proposed to adapt the pre-trained model to the new domain, where pixel-level and area-level uncertainty estimation are proposed to filter out the high-uncertainty pixels of predicted disparity maps and generate sparse while reliable pseudo-labels to align the domain gap. Experimentally, our method shows strong cross-domain, adapt, and joint generalization and obtains 1st place on the stereo task of Robust Vision Challenge 2020. Additionally, our uncertainty-based pseudo-labels can be extended to train monocular depth estimation networks in an unsupervised way and even achieves comparable performance with the supervised methods.},
  archive      = {J_TPAMI},
  author       = {Zhelun Shen and Xibin Song and Yuchao Dai and Dingfu Zhou and Zhibo Rao and Liangjun Zhang},
  doi          = {10.1109/TPAMI.2023.3300976},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14301-14320},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Digging into uncertainty-based pseudo-label for robust stereo matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CycleMLP: A MLP-like architecture for dense visual
predictions. <em>TPAMI</em>, <em>45</em>(12), 14284–14300. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a simple yet effective multilayer perceptron (MLP) architecture, namely CycleMLP, which is a versatile neural backbone network capable of solving various tasks of dense visual predictions such as object detection, segmentation, and human pose estimation. Compared to recent advanced MLP architectures such as MLP-Mixer (Tolstikhin et al. 2021), ResMLP (Touvron et al. 2021), and gMLP (Liu et al. 2021), whose architectures are sensitive to image size and are infeasible in dense prediction tasks, CycleMLP has two appealing advantages: 1) CycleMLP can cope with various spatial sizes of images; 2) CycleMLP achieves linear computational complexity with respect to the image size by using local windows. In contrast, previous MLPs have $O(N^{2})$ computational complexity due to their full connections in space. 3) The relationship between convolution, multi-head self-attention in Transformer, and CycleMLP are discussed through an intuitive theoretical analysis. We build a family of models that can surpass state-of-the-art MLP and Transformer models e.g., Swin Transformer (Liu et al. 2021), while using fewer parameters and FLOPs. CycleMLP expands the MLP-like models’ applicability, making them versatile backbone networks that achieve competitive results on dense prediction tasks For example, CycleMLP-Tiny outperforms Swin-Tiny by 1.3\% mIoU on ADE20 K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.},
  archive      = {J_TPAMI},
  author       = {Shoufa Chen and Enze Xie and Chongjian Ge and Runjian Chen and Ding Liang and Ping Luo},
  doi          = {10.1109/TPAMI.2023.3303397},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14284-14300},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CycleMLP: A MLP-like architecture for dense visual predictions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlation recurrent units: A novel neural architecture for
improving the predictive performance of time-series data.
<em>TPAMI</em>, <em>45</em>(12), 14266–14283. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series forecasting (TSF) is a traditional problem in the field of artificial intelligence, and models such as recurrent neural network, long short-term memory, and gate recurrent units have contributed to improving its predictive accuracy. Furthermore, model structures have been proposed to combine time-series decomposition methods such as seasonal-trend decomposition using LOESS. However, this approach is learned in an independent model for each component, and therefore, it cannot learn the relationships between the time-series components. In this study, we propose a new neural architecture called a correlation recurrent unit (CRU) that can perform time-series decomposition within a neural cell and learn correlations (autocorrelation and correlation) between each decomposition component. The proposed neural architecture was evaluated through comparative experiments with previous studies using four univariate and four multivariate time-series datasets. The results showed that long- and short-term predictive performance was improved by more than 10\%. The experimental results indicate that the proposed CRU is an excellent method for TSF problems compared to other neural architectures.},
  archive      = {J_TPAMI},
  author       = {Sunghyun Sim and Dohee Kim and Hyerim Bae},
  doi          = {10.1109/TPAMI.2023.3319557},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14266-14283},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Correlation recurrent units: A novel neural architecture for improving the predictive performance of time-series data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive vulnerability evaluation of face recognition
systems to template inversion attacks via 3D face reconstruction.
<em>TPAMI</em>, <em>45</em>(12), 14248–14265. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we comprehensively evaluate the vulnerability of state-of-the-art face recognition systems to template inversion attacks using 3D face reconstruction. We propose a new method (called GaFaR) to reconstruct 3D faces from facial templates using a pretrained geometry-aware face generation network, and train a mapping from facial templates to the intermediate latent space of the face generator network. We train our mapping with a semi-supervised approach using real and synthetic face images. For real face images, we use a generative adversarial network (GAN)-based framework to learn the distribution of generator intermediate latent space. For synthetic face images, we directly learn the mapping from facial templates to the generator intermediate latent code. Furthermore, to improve the success attack rate, we use two optimization methods on the camera parameters of the GNeRF model. We propose our method in the whitebox and blackbox attacks against face recognition systems and compare the transferability of our attack with state-of-the-art methods across other face recognition systems on the MOBIO and LFW datasets. We also perform practical presentation attacks on face recognition systems using the digital screen replay and printed photographs, and evaluate the vulnerability of face recognition systems to different template inversion attacks.},
  archive      = {J_TPAMI},
  author       = {Hatef Otroshi Shahreza and Sébastien Marcel},
  doi          = {10.1109/TPAMI.2023.3312123},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14248-14265},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Comprehensive vulnerability evaluation of face recognition systems to template inversion attacks via 3D face reconstruction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compositional semantic mix for domain adaptation in point
cloud segmentation. <em>TPAMI</em>, <em>45</em>(12), 14234–14247. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning models for 3D point cloud semantic segmentation exhibit limited generalization capabilities when trained and tested on data captured with different sensors or in varying environments due to domain shift. Domain adaptation methods can be employed to mitigate this domain shift, for instance, by simulating sensor noise, developing domain-agnostic generators, or training point cloud completion networks. Often, these methods are tailored for range view maps or necessitate multi-modal input. In contrast, domain adaptation in the image domain can be executed through sample mixing, which emphasizes input data manipulation rather than employing distinct adaptation modules. In this study, we introduce compositional semantic mixing for point cloud domain adaptation, representing the first unsupervised domain adaptation technique for point cloud segmentation based on semantic and geometric sample mixing. We present a two-branch symmetric network architecture capable of concurrently processing point clouds from a source domain (e.g. synthetic) and point clouds from a target domain (e.g. real-world). Each branch operates within one domain by integrating selected data fragments from the other domain and utilizing semantic information derived from source labels and target (pseudo) labels. Additionally, our method can leverage a limited number of human point-level annotations (semi-supervised) to further enhance performance. We assess our approach in both synthetic-to-real and real-to-real scenarios using LiDAR datasets and demonstrate that it significantly outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.},
  archive      = {J_TPAMI},
  author       = {Cristiano Saltori and Fabio Galasso and Giuseppe Fiameni and Nicu Sebe and Fabio Poiesi and Elisa Ricci},
  doi          = {10.1109/TPAMI.2023.3310261},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14234-14247},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Compositional semantic mix for domain adaptation in point cloud segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coarse-to-fine multi-scene pose regression with
transformers. <em>TPAMI</em>, <em>45</em>(12), 14222–14233. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach Shavit et al. (2021) by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.},
  archive      = {J_TPAMI},
  author       = {Yoli Shavit and Ron Ferens and Yosi Keller},
  doi          = {10.1109/TPAMI.2023.3310929},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14222-14233},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Coarse-to-fine multi-scene pose regression with transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bailando++: 3D dance GPT with choreographic memory.
<em>TPAMI</em>, <em>45</em>(12), 14192–14207. (<a
href="https://doi.org/10.1109/TPAMI.2023.3319435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our proposed music-to-dance framework, Bailando ++, addresses the challenges of driving 3D characters to dance in a way that follows the constraints of choreography norms and maintains temporal coherency with different music genres. Bailando ++ consists of two components: a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequences, and an actor-critic Generative Pre-trained Transformer (GPT) that composes these units into a fluent dance coherent to the music. In particular, to synchronize the diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a novel beat-align reward function. Additionally, we consider learning human dance poses in the rotation domain to avoid body distortions incompatible with human morphology, and introduce a musical contextual encoding to allow the motion GPT to grasp longer-term patterns of music. Our experiments on the standard benchmark show that Bailando ++ achieves state-of-the-art performance both qualitatively and quantitatively, with the added benefit of the unsupervised discovery of human-interpretable dancing-style poses in the choreographic memory.},
  archive      = {J_TPAMI},
  author       = {Li Siyao and Weijiang Yu and Tianpei Gu and Chunze Lin and Quan Wang and Chen Qian and Chen Change Loy and Ziwei Liu},
  doi          = {10.1109/TPAMI.2023.3319435},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14192-14207},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bailando++: 3D dance GPT with choreographic memory},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Background-aware classification activation map for weakly
supervised object localization. <em>TPAMI</em>, <em>45</em>(12),
14175–14191. (<a
href="https://doi.org/10.1109/TPAMI.2023.3309621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) relaxes the requirement of dense annotations for object localization by using image-level annotation to supervise the learning process. However, most WSOL methods only focus on forcing the object classifier to produce high activation score on object parts without considering the influence of background locations, causing excessive background activations and ill-pose background score searching. Based on this point, our work proposes a novel mechanism called the background-aware classification activation map (B-CAM) to add background awareness for WSOL training. Besides aggregating an object image-level feature for supervision, our B-CAM produces an additional background image-level feature to represent the pure-background sample. This additional feature can provide background cues for the object classifier to suppress the background activations on object localization maps. Moreover, our B-CAM also trained a background classifier with image-level annotation to produce adaptive background scores when determining the binary localization mask. Experiments indicate the effectiveness of the proposed B-CAM on four different types of WSOL benchmarks, including CUB-200, ILSVRC, OpenImages, and VOC2012 datasets.},
  archive      = {J_TPAMI},
  author       = {Lei Zhu and Qi She and Qian Chen and Xiangxi Meng and Mufeng Geng and Lujia Jin and Yibao Zhang and Qiushi Ren and Yanye Lu},
  doi          = {10.1109/TPAMI.2023.3309621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14175-14191},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Background-aware classification activation map for weakly supervised object localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AUC-oriented domain adaptation: From theory to algorithm.
<em>TPAMI</em>, <em>45</em>(12), 14161–14174. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Area Under the ROC curve (AUC) is a crucial metric for machine learning, which is often a reasonable choice for applications like disease prediction and fraud detection where the datasets often exhibit a long-tail nature. However, most of the existing AUC-oriented learning methods assume that the training data and test data are drawn from the same distribution. How to deal with domain shift remains widely open. This paper presents an early trial to attack AUC-oriented Unsupervised Domain Adaptation (UDA) (denoted as AUCUDA hence after). Specifically, we first construct a generalization bound that exploits a new distributional discrepancy for AUC. The critical challenge is that the AUC risk could not be expressed as a sum of independent loss terms, making the standard theoretical technique unavailable. We propose a new result that not only addresses the interdependency issue but also brings a much sharper bound with weaker assumptions about the loss function. Turning theory into practice, the original discrepancy requires complete annotations on the target domain, which is incompatible with UDA. To fix this issue, we propose a pseudo-labeling strategy and present an end-to-end training framework. Finally, empirical studies over five real-world datasets speak to the efficacy of our framework.},
  archive      = {J_TPAMI},
  author       = {Zhiyong Yang and Qianqian Xu and Shilong Bao and Peisong Wen and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2023.3303943},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14161-14174},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AUC-oriented domain adaptation: From theory to algorithm},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-guided collaborative learning for partial person
re-identification. <em>TPAMI</em>, <em>45</em>(12), 14144–14160. (<a
href="https://doi.org/10.1109/TPAMI.2023.3312302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial person re-identification (ReID) aims to solve the problem of image spatial misalignment due to occlusions or out-of-views. Despite significant progress through the introduction of additional information, such as human pose landmarks, mask maps, and spatial information, partial person ReID remains challenging due to noisy keypoints and impressionable pedestrian representations. To address these issues, we propose a unified attribute-guided collaborative learning scheme for partial person ReID. Specifically, we introduce an adaptive threshold-guided masked graph convolutional network that can dynamically remove untrustworthy edges to suppress the diffusion of noisy keypoints. Furthermore, we incorporate human attributes and devise a cyclic heterogeneous graph convolutional network to effectively fuse cross-modal pedestrian information through intra- and inter-graph interaction, resulting in robust pedestrian representations. Finally, to enhance keypoint representation learning, we design a novel part-based similarity constraint based on the axisymmetric characteristic of the human body. Extensive experiments on multiple public datasets have shown that our model achieves superior performance compared to other state-of-the-art baselines.},
  archive      = {J_TPAMI},
  author       = {Haoyu Zhang and Meng Liu and Yuhong Li and Ming Yan and Zan Gao and Xiaojun Chang and Liqiang Nie},
  doi          = {10.1109/TPAMI.2023.3312302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14144-14160},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attribute-guided collaborative learning for partial person re-identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial data augmentation for HMM-based anomaly
detection. <em>TPAMI</em>, <em>45</em>(12), 14131–14143. (<a
href="https://doi.org/10.1109/TPAMI.2023.3303099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we concentrate on the detection of anomalous behaviors in systems operating in the physical world and for which it is usually not possible to have a complete set of all possible anomalies in advance. We present a data augmentation and retraining approach based on adversarial learning for improving anomaly detection. In particular, we first define a method for generating adversarial examples for anomaly detectors based on Hidden Markov Models (HMMs). Then, we present a data augmentation and retraining technique that uses these adversarial examples to improve anomaly detection performance. Finally, we evaluate our adversarial data augmentation and retraining approach on four datasets showing that it achieves a statistically significant performance improvement and enhances the robustness to adversarial attacks. Key differences from the state-of-the-art on adversarial data augmentation are the focus on multivariate time series (as opposed to images), the context of one-class classification (in contrast to standard multi-class classification), and the use of HMMs (in contrast to neural networks).},
  archive      = {J_TPAMI},
  author       = {Alberto Castellini and Francesco Masillo and Davide Azzalini and Francesco Amigoni and Alessandro Farinelli},
  doi          = {10.1109/TPAMI.2023.3303099},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14131-14143},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial data augmentation for HMM-based anomaly detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaPoinTr: Diverse point cloud completion with adaptive
geometry-aware transformers. <em>TPAMI</em>, <em>45</em>(12),
14114–14130. (<a
href="https://doi.org/10.1109/TPAMI.2023.3309253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Transformer encoder-decoder architecture, called PoinTr, which reformulates point cloud completion as a set-to-set translation problem and employs a geometry-aware block to model local geometric relationships explicitly. The migration of Transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Taking a step towards more complicated and diverse situations, we further propose AdaPoinTr by developing an adaptive query generation mechanism and designing a novel denoising task during completing a point cloud. Coupling these two techniques enables us to train the model efficiently and effectively: we reduce training time (by 15x or more) and improve completion performance (over 20\%). Additionally, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect real-world scenarios to promote future research. We also show our method can be extended to the scene-level point cloud completion scenario by designing a new geometry-enhanced semantic scene completion framework. Extensive experiments on the existing and newly-proposed datasets demonstrate the effectiveness of our method, which attains 6.53 CD on PCN, 0.81 CD on ShapeNet-55 and 0.392 MMD on real-world KITTI, surpassing other work by a large margin and establishing new state-of-the-arts on various benchmarks. Most notably, AdaPoinTr can achieve such promising performance with higher throughputs and fewer FLOPs compared with the previous best methods in practice.},
  archive      = {J_TPAMI},
  author       = {Xumin Yu and Yongming Rao and Ziyi Wang and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2023.3309253},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14114-14130},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AdaPoinTr: Diverse point cloud completion with adaptive geometry-aware transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ActiveZero++: Mixed domain learning stereo and
confidence-based depth completion with zero annotation. <em>TPAMI</em>,
<em>45</em>(12), 14098–14113. (<a
href="https://doi.org/10.1109/TPAMI.2023.3305399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based stereo methods usually require a large scale dataset with depth, however obtaining accurate depth in the real domain is difficult, but groundtruth depth is readily available in the simulation domain. In this article we propose a new framework, ActiveZero++, which is a mixed domain learning solution for active stereovision systems that requires no real world depth annotation. In the simulation domain, we use a combination of supervised disparity loss and self-supervised loss on a shape primitives dataset. By contrast, in the real domain, we only use self-supervised loss on a dataset that is out-of-distribution from either training simulation data or test real data. To improve the robustness and accuracy of our reprojection loss in hard-to-perceive regions, our method introduces a novel self-supervised loss called temporal IR reprojection. Further, we propose the confidence-based depth completion module, which uses the confidence from the stereo network to identify and improve erroneous areas in depth prediction through depth-normal consistency. Extensive qualitative and quantitative evaluations on real-world data demonstrate state-of-the-art results that can even outperform a commercial depth sensor. Furthermore, our method can significantly narrow the Sim2Real domain gap of depth maps for state-of-the-art learning based 6D pose estimation algorithms.},
  archive      = {J_TPAMI},
  author       = {Rui Chen and Isabella Liu and Edward Yang and Jianyu Tao and Xiaoshuai Zhang and Qing Ran and Zhu Liu and Jing Xu and Hao Su},
  doi          = {10.1109/TPAMI.2023.3305399},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14098-14113},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ActiveZero++: Mixed domain learning stereo and confidence-based depth completion with zero annotation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action recognition and benchmark using event cameras.
<em>TPAMI</em>, <em>45</em>(12), 14081–14097. (<a
href="https://doi.org/10.1109/TPAMI.2023.3300741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed remarkable achievements in video-based action recognition. Apart from traditional frame-based cameras, event cameras are bio-inspired vision sensors that only record pixel-wise brightness changes rather than the brightness value. However, little effort has been made in event-based action recognition, and large-scale public datasets are also nearly unavailable. In this paper, we propose an event-based action recognition framework called EV-ACT . The Learnable Multi-Fused Representation (LMFR) is first proposed to integrate multiple event information in a learnable manner. The LMFR with dual temporal granularity is fed into the event-based slow-fast network for the fusion of appearance and motion features. A spatial-temporal attention mechanism is introduced to further enhance the learning capability of action recognition. To prompt research in this direction, we have collected the largest event-based action recognition benchmark named THUE-ACT-50 and the accompanying THUE-ACT-50-CHL dataset under challenging environments, including a total of over 12,830 recordings from 50 action categories, which is over 4 times the size of the previous largest dataset. Experimental results show that our proposed framework could achieve improvements of over 14.5\%, 7.6\%, 11.2\%, and 7.4\% compared to previous works on four benchmarks. We have also deployed our proposed EV-ACT framework on a mobile platform to validate its practicality and efficiency.},
  archive      = {J_TPAMI},
  author       = {Yue Gao and Jiaxuan Lu and Siqi Li and Nan Ma and Shaoyi Du and Yipeng Li and Qionghai Dai},
  doi          = {10.1109/TPAMI.2023.3300741},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14081-14097},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Action recognition and benchmark using event cameras},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of vectorization methods in topological data
analysis. <em>TPAMI</em>, <em>45</em>(12), 14069–14080. (<a
href="https://doi.org/10.1109/TPAMI.2023.3308391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attempts to incorporate topological information in supervised learning tasks have resulted in the creation of several techniques for vectorizing persistent homology barcodes. In this paper, we study thirteen such methods. Besides describing an organizational framework for these methods, we comprehensively benchmark them against three well-known classification tasks. Surprisingly, we discover that the best-performing method is a simple vectorization, which consists only of a few elementary summary statistics. Finally, we provide a convenient web application which has been designed to facilitate exploration and experimentation with various vectorization methods.},
  archive      = {J_TPAMI},
  author       = {Dashti Ali and Aras Asaad and Maria-Jose Jimenez and Vidit Nanda and Eduardo Paluzo-Hidalgo and Manuel Soriano-Trigueros},
  doi          = {10.1109/TPAMI.2023.3308391},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14069-14080},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey of vectorization methods in topological data analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parametrical model for instance-dependent label noise.
<em>TPAMI</em>, <em>45</em>(12), 14055–14068. (<a
href="https://doi.org/10.1109/TPAMI.2023.3301876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In label-noise learning, estimating the transition matrix is a hot topic as the matrix plays an important role in building statistically consistent classifiers . Traditionally, the transition from clean labels to noisy labels (i.e., clean-label transition matrix (CLTM) ) has been widely exploited on class-dependent label-noise (wherein all samples in a clean class share the same label transition matrix). However, the CLTM cannot handle the more common instance-dependent label-noise well (wherein the clean-to-noisy label transition matrix needs to be estimated at the instance level by considering the input quality). Motivated by the fact that classifiers mostly output Bayes optimal labels for prediction, in this paper, we study to directly model the transition from Bayes optimal labels to noisy labels (i.e., Bayes-Label Transition Matrix (BLTM) ) and learn a classifier to predict Bayes optimal labels . Note that given only noisy data, it is ill-posed to estimate either the CLTM or the BLTM . But favorably, Bayes optimal labels have no uncertainty compared with the clean labels, i.e., the class posteriors of Bayes optimal labels are one-hot vectors while those of clean labels are not. This enables two advantages to estimate the BLTM , i.e., (a) a set of examples with theoretically guaranteed Bayes optimal labels can be collected out of noisy data; (b) the feasible solution space is much smaller. By exploiting the advantages, this work proposes a parametrical model for estimating the instance-dependent label-noise transition matrix by employing a deep neural network , leading to better generalization and superior classification performance.},
  archive      = {J_TPAMI},
  author       = {Shuo Yang and Songhua Wu and Erkun Yang and Bo Han and Yang Liu and Min Xu and Gang Niu and Tongliang Liu},
  doi          = {10.1109/TPAMI.2023.3301876},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {14055-14068},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A parametrical model for instance-dependent label noise},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Still an ineffective method with supertrials/ERPs—comments
on “decoding brain representations by multimodal learning of neural
activity and visual features.” <em>TPAMI</em>, <em>45</em>(11),
14052–14054. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paper claims that a newly proposed method classifies EEG data recorded from subjects viewing ImageNet stimuli better than two prior methods. However, the analysis used to support that claim is based on confounded data. We repeat the analysis on a large new dataset that is free from that confound. Training and testing on aggregated supertrials derived by summing trials demonstrates that the two prior methods achieve statistically significant above-chance accuracy while the newly proposed method does not.},
  archive      = {J_TPAMI},
  author       = {Hari M Bharadwaj and Ronnie B. Wilbur and Jeffrey Mark Siskind},
  doi          = {10.1109/TPAMI.2023.3292062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {14052-14054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Still an ineffective method with Supertrials/ERPs—Comments on “Decoding brain representations by multimodal learning of neural activity and visual features”},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric back-propagation in morphological neural networks.
<em>TPAMI</em>, <em>45</em>(11), 14045–14051. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a definition of back-propagation through geometric correspondences for morphological neural networks. In addition, dilation layers are shown to learn probe geometry by erosion of layer inputs and outputs. A proof-of-principle is provided, in which predictions and convergence of morphological networks significantly outperform convolutional networks.},
  archive      = {J_TPAMI},
  author       = {Rick Groenendijk and Leo Dorst and Theo Gevers},
  doi          = {10.1109/TPAMI.2023.3290615},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {14045-14051},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geometric back-propagation in morphological neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Circular silhouette and a fast algorithm. <em>TPAMI</em>,
<em>45</em>(11), 14038–14044. (<a
href="https://doi.org/10.1109/TPAMI.2023.3310495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data clustering has recently been solved exactly in sub-quadratic time. However, the solution requires a given number of clusters; methods for choosing this number on linear data are inapplicable to circular data. To fill this gap, we introduce the circular silhouette to measure cluster quality and a fast algorithm to calculate the average silhouette width. The algorithm runs in linear time to the number of points on sorted data, instead of quadratic time by the silhouette definition. Empirically, it is over 3000 times faster than by silhouette definition on 1,000,000 circular data points in five clusters. On simulated datasets, the algorithm returned correct numbers of clusters. We identified clusters on round genomes of human mitochondria and bacteria. On sunspot activity data, we found changed solar-cycle patterns over the past two centuries. Using the circular silhouette not only eliminates the subjective selection of number of clusters, but is also scalable to big circular and periodic data abundant in science, engineering, and medicine.},
  archive      = {J_TPAMI},
  author       = {Yinong Chen and Tathagata Debnath and Andrew Cai and Mingzhou Song},
  doi          = {10.1109/TPAMI.2023.3310495},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {14038-14044},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Circular silhouette and a fast algorithm},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SODFormer: Streaming object detection with transformer using
events and frames. <em>TPAMI</em>, <em>45</em>(11), 14020–14037. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a novel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect objects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1 k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual streams to improve the detection performance. Finally, an asynchronous attention-based fusion module is proposed to integrate two heterogeneous sensing modalities and take complementary advantages from each end, which can be queried at any time to locate objects and break through the limited output frequency from synchronized frame-based fusion strategies. The results show that the proposed SODFormer outperforms four state-of-the-art methods and our eight baselines by a significant margin. We also show that our unifying framework works well even in cases where the conventional frame-based camera fails, e.g., high-speed motion and low-light conditions. Our dataset and code can be available at https://github.com/dianzl/SODFormer .},
  archive      = {J_TPAMI},
  author       = {Dianze Li and Yonghong Tian and Jianing Li},
  doi          = {10.1109/TPAMI.2023.3298925},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {14020-14037},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SODFormer: Streaming object detection with transformer using events and frames},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STAR-FC: Structure-aware face clustering on
ultra-large-scale graphs. <em>TPAMI</em>, <em>45</em>(11), 14005–14019.
(<a href="https://doi.org/10.1109/TPAMI.2023.3299263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face clustering is a promising method for annotating unlabeled face images. Recent supervised approaches have boosted the face clustering accuracy greatly, however their performance is still far from satisfactory. These methods can be roughly divided into global-based and local-based ones. Global-based methods suffer from the limitation of training data scale, while local-based ones are inefficient for inference due to the use of numerous overlapped subgraphs. Previous approaches fail to tackle these two challenges simultaneously. To address the dilemma of large-scale training and efficient inference, we propose the STructure-AwaRe Face Clustering (STAR-FC) method. Specifically, we design a structure-preserving subgraph sampling strategy to explore the power of large-scale training data, which can increase the training data scale from ${10^{5}}$ to ${10^{7}}$ . On this basis, a novel hierarchical GCN training paradigm is further proposed for better capturing the dynamic local structure. During inference, the STAR-FC performs efficient full-graph clustering with two steps: graph parsing and graph refinement. And the concept of node intimacy is introduced in the second step to mine the local structural information, where a calibration module is further proposed for fairer edge scores. The STAR-FC gets 93.21 pairwise F-score on standard partial MS1M within 312 seconds, which far surpasses the state-of-the-arts while maintaining high inference efficiency. Furthermore, we are the first to train on an ultra-large-scale graph with 20 M nodes, and achieve superior inference results on 12 M testing data. Overall, as a simple and effective method, the proposed STAR-FC provides a strong baseline for large-scale face clustering.},
  archive      = {J_TPAMI},
  author       = {Shuai Shen and Wanhua Li and Zheng Zhu and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3299263},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {14005-14019},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {STAR-FC: Structure-aware face clustering on ultra-large-scale graphs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape from polarization with distant lighting estimation.
<em>TPAMI</em>, <em>45</em>(11), 13991–14004. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new approach for surface normal recovery from polarization images under an unknown distant light. Polarization provides rich cues of object geometry and material, but it is also influenced by different lighting conditions. Different from previous Shape-from-Polarization (SfP) methods, which rely on handcrafted or data-driven priors, we analytically investigate the benefits of estimating distant lighting for resolving the ambiguity in normal estimation from SfP using the polarimetric Bidirectional Reflectance Distribution Function (pBRDF) based image formation model. We then propose a two-stage learning framework that first effectively exploits polarization and shading cues to estimate the reflectance and lighting information and then optimizes the initial normal as the geometric prior. Leveraging the normal prior with the polarization cues from the input images, our network further generates the surface normal with more details in the second stage. We also present a data generation pipeline derived from the pBRDF model enabling model training and create a real dataset for evaluation of SfP approaches. Extensive ablation studies show the effectiveness of our designed architecture, and our approach outperforms existing methods in quantitative and qualitative experiments on real data.},
  archive      = {J_TPAMI},
  author       = {Youwei Lyu and Lingran Zhao and Si Li and Boxin Shi},
  doi          = {10.1109/TPAMI.2023.3298376},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13991-14004},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Shape from polarization with distant lighting estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DIY your EasyNAS for vision: Convolution operation merging,
map channel reducing, and search space to supernet conversion tooling.
<em>TPAMI</em>, <em>45</em>(11), 13974–13990. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its popularity as a one-shot Neural Architecture Search (NAS) approach, the applicability of differentiable architecture search (DARTS) on complex vision tasks is still limited by the high computation and memory costs incurred by the over-parameterized supernet. We propose a new architecture search method called EasyNAS, whose memory and computational efficiency is achieved via our devised operator merging technique which shares and merges the weights of candidate convolution operations into a single convolution, and a dynamic channel refinement strategy. We also introduce a configurable search space-to-supernet conversion tool, leveraging the concept of atomic search components, to enable its application from classification to more complex vision tasks: detection and semantic segmentation. In classification, EasyNAS achieves state-of-the-art performance on the NAS-Bench-201 benchmark, attaining an impressive 76.2\% accuracy on ImageNet. For detection, it achieves a mean average precision (mAP) of 40.1 with 120 frames per second (FPS) on MS-COCO test-dev. Additionally, we transfer the discovered architecture to the rotation detection task, where EasyNAS achieves a remarkable 77.05 mAP $_{50}$ on the DOTA-v1.0 test set, using only 21.1 M parameters. In semantic segmentation, it achieves a competitive mean intersection over union (mIoU) of 72.6\% at 173 FPS on Cityscape, after searching for only 0.7 GPU-day.},
  archive      = {J_TPAMI},
  author       = {Xiaoxing Wang and Zhirui Lian and Jiale Lin and Chao Xue and Junchi Yan},
  doi          = {10.1109/TPAMI.2023.3298296},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13974-13990},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DIY your EasyNAS for vision: Convolution operation merging, map channel reducing, and search space to supernet conversion tooling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Object affinity learning: Towards annotation-free instance
segmentation. <em>TPAMI</em>, <em>45</em>(11), 13959–13973. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of annotation-free instance segmentation in the wild, aiming to relieve the expensive cost of manual mask annotations. Existing approaches utilize appearance cues, such as color, edge, and texture information, to generate pseudo masks for instance segmentation. However, due to the ambiguity of defining an object by visual appearance alone, these methods fail to distinguish objects from the background under complex scenes. Beyond visual cues, objects are one-piece in space and move together over time, which indicates that geometry cues, such as spatial continuity and motion consistency, are also exploitable for this problem. To directly utilize geometry cues, we propose an affinity-based paradigm for annotation-free instance segmentation. The new paradigm is called object affinity learning , a proxy task of annotation-free instance segmentation, which aims to tell whether two pixels come from the same object by learning feature representation from geometry cues. During inference, the learned object affinity could be further converted into instance segmentation masks by some graph partition algorithms. The proposed object affinity learning achieves much better instance segmentation performance than existing pseudo-mask-based methods on the large-scale Waymo Open Dataset and KITTI dataset.},
  archive      = {J_TPAMI},
  author       = {Yuqi Wang and Yuntao Chen and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2023.3298351},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13959-13973},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Object affinity learning: Towards annotation-free instance segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unifying flow, stereo and depth estimation. <em>TPAMI</em>,
<em>45</em>(11), 13941–13958. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unified formulation and model for three motion and 3D perception tasks: optical flow, rectified stereo matching and unrectified stereo depth estimation from posed images. Unlike previous specialized architectures for each specific task, we formulate all three tasks as a unified dense correspondence matching problem, which can be solved with a single model by directly comparing feature similarities. Such a formulation calls for discriminative feature representations, which we achieve using a Transformer, in particular the cross-attention mechanism. We demonstrate that cross-attention enables integration of knowledge from another image via cross-view interactions, which greatly improves the quality of the extracted features. Our unified model naturally enables cross-task transfer since the model architecture and parameters are shared across tasks. We outperform RAFT with our unified model on the challenging Sintel dataset, and our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow, stereo and depth datasets, while being simpler and more efficient in terms of model design and inference speed.},
  archive      = {J_TPAMI},
  author       = {Haofei Xu and Jing Zhang and Jianfei Cai and Hamid Rezatofighi and Fisher Yu and Dacheng Tao and Andreas Geiger},
  doi          = {10.1109/TPAMI.2023.3298645},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13941-13958},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unifying flow, stereo and depth estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fine-grained predicates learning for scene graph
generation. <em>TPAMI</em>, <em>45</em>(11), 13921–13940. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of current Scene Graph Generation (SGG) models is severely hampered by hard-to-distinguish predicates, e.g., “woman-on/standing on/walking on-beach”. As general SGG models tend to predict head predicates and re-balancing strategies prefer tail categories, none of them can appropriately handle hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained image classification, which focuses on differentiating hard-to-distinguish objects, we propose an Adaptive Fine-Grained Predicates Learning (FGPL-A) which aims at differentiating hard-to-distinguish predicates for SGG. First, we introduce an Adaptive Predicate Lattice (PL-A) to figure out hard-to-distinguish predicates, which adaptively explores predicate correlations in keeping with model&#39;s dynamic learning pace. Practically, PL-A is initialized from SGG dataset, and gets refined by exploring model&#39;s predictions of current mini-batch. Utilizing PL-A, we propose an Adaptive Category Discriminating Loss (CDL-A) and an Adaptive Entity Discriminating Loss (EDL-A) , which progressively regularize model&#39;s discriminating process with fine-grained supervision concerning model&#39;s dynamic learning status, ensuring balanced and efficient learning process. Extensive experimental results show that our proposed model-agnostic strategy significantly boosts performance of benchmark models on VG-SGG and GQA-SGG datasets by up to 175\% and 76\% on Mean Recall@100 , achieving new state-of-the-art performance. Moreover, experiments on Sentence-to-Graph Retrieval and Image Captioning tasks further demonstrate practicability of our method.},
  archive      = {J_TPAMI},
  author       = {Xinyu Lyu and Lianli Gao and Pengpeng Zeng and Heng Tao Shen and Jingkuan Song},
  doi          = {10.1109/TPAMI.2023.3298356},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13921-13940},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive fine-grained predicates learning for scene graph generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-aware deep hashing with self-consistency for
large-scale fine-grained image retrieval. <em>TPAMI</em>,
<em>45</em>(11), 13904–13920. (<a
href="https://doi.org/10.1109/TPAMI.2023.3299563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose attribute-aware hashing networks with self-consistency for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. Our models are also equipped with a feature decorrelation constraint upon these attribute vectors to strengthen their representative abilities. Then, driven by preserving original entities’ similarity, the required hash codes can be generated from these attribute-specific vectors and thus become attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we consider the model design from the perspective of the self-consistency principle and propose to further enhance models’ self-consistency by equipping an additional image reconstruction path. Comprehensive quantitative experiments under diverse empirical settings on six fine-grained retrieval datasets and two generic retrieval datasets show the superiority of our models over competing methods. Moreover, qualitative results demonstrate that not only the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects, but also our self-consistency designs can effectively overcome simplicity bias in fine-grained hashing.},
  archive      = {J_TPAMI},
  author       = {Xiu-Shen Wei and Yang Shen and Xuhao Sun and Peng Wang and Yuxin Peng},
  doi          = {10.1109/TPAMI.2023.3299563},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13904-13920},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attribute-aware deep hashing with self-consistency for large-scale fine-grained image retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KitBit: A new AI model for solving intelligence tests and
numerical series. <em>TPAMI</em>, <em>45</em>(11), 13893–13903. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resolution of intelligence tests, in particular numerical sequences, has been of great interest in the evaluation of AI systems. We present a new computational model called KitBit that uses a reduced set of algorithms and their combinations to build a predictive model that finds the underlying pattern in numerical sequences, such as those included in IQ tests and others of much greater complexity. We present the fundamentals of the model and its application in different cases. First, the system is tested on a set of number series used in IQ tests collected from various sources. Next, our model is successfully applied on the series used to evaluate the models reported in the literature. In both cases, the system is capable of solving these types of problems in less than a second using standard computing power. Finally, KitBit&#39;s algorithms have been applied for the first time to the complete set of entire series of the well-known OEIS database. We find a pattern in the form of a list of algorithms and predict the following terms in the largest number of series to date. These results demonstrate the potential of KitBit to solve complex problems that could be represented numerically.},
  archive      = {J_TPAMI},
  author       = {Víctor Corsino and José Manuel Gilpérez and Luis Herrera},
  doi          = {10.1109/TPAMI.2023.3298592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13893-13903},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {KitBit: A new AI model for solving intelligence tests and numerical series},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The equalization losses: Gradient-driven training for
long-tailed object recognition. <em>TPAMI</em>, <em>45</em>(11),
13876–13892. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tail distribution is widely spread in real-world applications. Due to the extremely small ratio of instances, tail categories often show inferior accuracy. In this paper, we find such performance bottleneck is mainly caused by the imbalanced gradients, which can be categorized into two parts: (1) positive part, deriving from the samples of the same category, and (2) negative part, contributed by other categories. Based on comprehensive experiments, it is also observed that the gradient ratio of accumulated positives to negatives is a good indicator to measure how balanced a category is trained. Inspired by this, we come up with a gradient-driven training mechanism to tackle the long-tail problem: re-balancing the positive/negative gradients dynamically according to current accumulative gradients, with a unified goal of achieving balance gradient ratios. Taking advantage of the simple and flexible gradient mechanism, we introduce a new family of gradient-driven loss functions, namely equalization losses. We conduct extensive experiments on a wide spectrum of visual tasks, including two-stage/single-stage long-tailed object detection (LVIS), long-tailed image classification (ImageNet-LT, Places-LT, iNaturalist), and long-tailed semantic segmentation (ADE20 K). Our method consistently outperforms the baseline models, demonstrating the effectiveness and generalization ability of the proposed equalization losses.},
  archive      = {J_TPAMI},
  author       = {Jingru Tan and Bo Li and Xin Lu and Yongqiang Yao and Fengwei Yu and Tong He and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2023.3298433},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13876-13892},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The equalization losses: Gradient-driven training for long-tailed object recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HDGT: Heterogeneous driving graph transformer for
multi-agent trajectory prediction via scene encoding. <em>TPAMI</em>,
<em>45</em>(11), 13860–13875. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g., trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. For the aggregation module in the graph neural network (GNN), we adopt the transformer structure in a hierarchical way to fit the heterogeneous nature of inputs. Experimental results show that HDGT achieves state-of-the-art performance for the task of trajectory prediction, on INTERACTION Prediction Challenge and Waymo Open Motion Challenge.},
  archive      = {J_TPAMI},
  author       = {Xiaosong Jia and Penghao Wu and Li Chen and Yu Liu and Hongyang Li and Junchi Yan},
  doi          = {10.1109/TPAMI.2023.3298301},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13860-13875},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HDGT: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial video-based remote physiological measurement via
self-supervised learning. <em>TPAMI</em>, <em>45</em>(11), 13844–13859.
(<a href="https://doi.org/10.1109/TPAMI.2023.3298650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based remote physiological measurement aims to estimate remote photoplethysmography (rPPG) signals from human facial videos and then measure multiple vital signs (e.g., heart rate, respiration frequency) from rPPG signals. Recent approaches achieve it by training deep neural networks, which normally require abundant facial videos and synchronously recorded photoplethysmography (PPG) signals for supervision. However, the collection of these annotated corpora is not easy in practice. In this paper, we introduce a novel frequency-inspired self-supervised framework that learns to estimate rPPG signals from facial videos without the need of ground truth PPG signals. Given a video sample, we first augment it into multiple positive/negative samples which contain similar/dissimilar signal frequencies to the original one. Specifically, positive samples are generated using spatial augmentation; negative samples are generated via a learnable frequency augmentation module, which performs non-linear signal frequency transformation on the input without excessively changing its visual appearance. Next, we introduce a local rPPG expert aggregation module to estimate rPPG signals from augmented samples. It encodes complementary pulsation information from different face regions and aggregates them into one rPPG prediction. Finally, we propose a series of frequency-inspired losses, i.e., frequency contrastive loss, frequency ratio consistency loss, and cross-video frequency agreement loss, for the optimization of estimated rPPG signals from multiple augmented video samples. We conduct rPPG-based heart rate, heart rate variability, and respiration frequency estimation on five standard benchmarks. The experimental results demonstrate that our method improves the state of the art by a large margin.},
  archive      = {J_TPAMI},
  author       = {Zijie Yue and Miaojing Shi and Shuai Ding},
  doi          = {10.1109/TPAMI.2023.3298650},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13844-13859},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Facial video-based remote physiological measurement via self-supervised learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). TAKDE: Temporal adaptive kernel density estimator for
real-time dynamic density estimation. <em>TPAMI</em>, <em>45</em>(11),
13831–13843. (<a
href="https://doi.org/10.1109/TPAMI.2023.3297950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time density estimation is ubiquitous in many applications, including computer vision and signal processing. Kernel density estimation is arguably one of the most commonly used density estimation techniques, and the use of “sliding window” mechanism adapts kernel density estimators to dynamic processes. In this article, we derive the asymptotic mean integrated squared error (AMISE) upper bound for the “sliding window” kernel density estimator. This upper bound provides a principled guide to devise a novel estimator, which we name the temporal adaptive kernel density estimator (TAKDE). Compared to heuristic approaches for “sliding window” kernel density estimator, TAKDE is theoretically optimal in terms of the worst-case AMISE. We provide numerical experiments using synthetic and real-world datasets, showing that TAKDE outperforms other state-of-the-art dynamic density estimators (including those outside of kernel family). In particular, TAKDE achieves a superior test log-likelihood with a smaller run-time.},
  archive      = {J_TPAMI},
  author       = {Yinsong Wang and Yu Ding and Shahin Shahrampour},
  doi          = {10.1109/TPAMI.2023.3297950},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13831-13843},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TAKDE: Temporal adaptive kernel density estimator for real-time dynamic density estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to immunize images for tamper localization and
self-recovery. <em>TPAMI</em>, <em>45</em>(11), 13814–13830. (<a
href="https://doi.org/10.1109/TPAMI.2023.3301958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital images are vulnerable to nefarious tampering attacks such as content addition or removal that severely alter the original meaning. It is somehow like a person without protection that is open to various kinds of viruses. Image immunization (Imuge) is a technology of protecting the images by introducing trivial perturbation, so that the protected images are immune to the viruses in that the tampered contents can be auto-recovered. This paper presents Imuge+, an enhanced scheme for image immunization. By observing the invertible relationship between image immunization and the corresponding self-recovery, we employ an invertible neural network to jointly learn image immunization and recovery respectively in the forward and backward pass. We also introduce an efficient attack layer that involves both malicious tamper and benign image post-processing, where a novel distillation-based JPEG simulator is proposed for improved JPEG robustness. Our method achieves promising results in real-world tests where experiments show accurate tamper localization as well as high-fidelity content recovery. Additionally, we show superior performance on tamper localization compared to state-of-the-art schemes based on passive forensics.},
  archive      = {J_TPAMI},
  author       = {Qichao Ying and Hang Zhou and Zhenxing Qian and Sheng Li and Xinpeng Zhang},
  doi          = {10.1109/TPAMI.2023.3301958},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13814-13830},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to immunize images for tamper localization and self-recovery},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A heterogeneous graph to abstract syntax tree framework for
text-to-SQL. <em>TPAMI</em>, <em>45</em>(11), 13796–13813. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-SQL is the task of converting a natural language utterance plus the corresponding database schema into a SQL program. The inputs naturally form a heterogeneous graph while the output SQL can be transduced into an abstract syntax tree (AST). Traditional encoder-decoder models ignore higher-order semantics in heterogeneous graph encoding and introduce permutation biases during AST construction, thus incapable of exploiting the refined structure knowledge precisely. In this work, we propose a generic heterogeneous graph to abstract syntax tree (HG2AST) framework to integrate dedicated structure knowledge into statistics-based models. On the encoder side, we leverage a line graph enhanced encoder (LGESQL) to iteratively update both node and edge features through dual graph message passing and aggregation. On the decoder side, a grammar-based decoder first constructs the equivalent SQL AST and then transforms it into the desired SQL via post-processing. To avoid over-fitting permutation biases, we propose a golden tree-oriented learning (GTL) algorithm to adaptively control the expanding order of AST nodes. The graph encoder and tree decoder are combined into a unified framework through two auxiliary modules. Extensive experiments on various text-to-SQL datasets, including single/multi-table, single/cross-domain, and multilingual settings, demonstrate the superiority and broad applicability.},
  archive      = {J_TPAMI},
  author       = {Ruisheng Cao and Lu Chen and Jieyu Li and Hanchong Zhang and Hongshen Xu and Wangyou Zhang and Kai Yu},
  doi          = {10.1109/TPAMI.2023.3298895},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13796-13813},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A heterogeneous graph to abstract syntax tree framework for text-to-SQL},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source free semi-supervised transfer learning for diagnosis
of mental disorders on fMRI scans. <em>TPAMI</em>, <em>45</em>(11),
13778–13795. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high prevalence of mental disorders gradually poses a huge pressure on the public healthcare services. Deep learning-based computer-aided diagnosis (CAD) has emerged to relieve the tension in healthcare institutions by detecting abnormal neuroimaging-derived phenotypes. However, training deep learning models relies on sufficient annotated datasets, which can be costly and laborious. Semi-supervised learning (SSL) and transfer learning (TL) can mitigate this challenge by leveraging unlabeled data within the same institution and advantageous information from source domain, respectively. This work is the first attempt to propose an effective semi-supervised transfer learning (SSTL) framework dubbed S3TL for CAD of mental disorders on fMRI data. Within S3TL, a secure cross-domain feature alignment method is developed to generate target-related source model in SSL. Subsequently, we propose an enhanced dual-stage pseudo-labeling approach to assign pseudo-labels for unlabeled samples in target domain. Finally, an advantageous knowledge transfer method is conducted to improve the generalization capability of the target model. Comprehensive experimental results demonstrate that S3TL achieves competitive accuracies of 69.14\%, 69.65\%, and 72.62\% on ABIDE-I, ABIDE-II, and ADHD-200 datasets, respectively. Furthermore, the simulation experiments also demonstrate the application potential of S3TL through model interpretation analysis and federated learning extension.},
  archive      = {J_TPAMI},
  author       = {Yao Hu and Zhi-An Huang and Rui Liu and Xiaoming Xue and Xiaoyan Sun and Linqi Song and Kay Chen Tan},
  doi          = {10.1109/TPAMI.2023.3298332},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13778-13795},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Source free semi-supervised transfer learning for diagnosis of mental disorders on fMRI scans},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A clustering algorithm for polygonal data applied to
scientific journal profiles. <em>TPAMI</em>, <em>45</em>(11),
13766–13777. (<a
href="https://doi.org/10.1109/TPAMI.2023.3297022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of papers are submitted and published every year, but researchers often do not have much information about the journals that interest them. In this paper, we introduced the first dynamical clustering algorithm for symbolic polygonal data and this was applied to build scientific journals profiles. Dynamic clustering algorithms are a family of iterative two-step relocation algorithms involving the construction of clusters at each iteration and the identification of a suitable representation or prototype (means, axes, probability laws, groups of elements, etc.) for each cluster by locally optimizing an adequacy criterion that measures the fitting between clusters and their corresponding prototypes The application gives a powerful vision to understand the main variables that describe journals. Symbolic polygonal data can represent summarized extensive datasets taking into account variability. In addition, we developed cluster and partition interpretation indices for polygonal data that have the ability to extract insights about clustering results. From these indices, we discovered, e.g., that the number of difficult words in abstract is fundamental to building journal profiles.},
  archive      = {J_TPAMI},
  author       = {Wagner J. F. Silva and Pedro J.C. Souza and Renata M.C.R. Souza and Francisco José A. Cysneiros},
  doi          = {10.1109/TPAMI.2023.3297022},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13766-13777},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A clustering algorithm for polygonal data applied to scientific journal profiles},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surface geometry processing: An efficient normal-based
detail representation. <em>TPAMI</em>, <em>45</em>(11), 13749–13765. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of high-resolution 3D vision applications, the traditional way of manipulating surface detail requires considerable memory and computing time. To address these problems, we introduce an efficient surface detail processing framework in 2D normal domain, which extracts new normal feature representations as the carrier of micro geometry structures that are illustrated both theoretically and empirically in this article. Compared with the existing state of the arts, we verify and demonstrate that the proposed normal-based representation has three important properties, including detail separability , detail transferability and detail idempotence . Finally, three new schemes are further designed for geometric surface detail processing applications, including geometric texture synthesis , geometry detail transfer , and 3D surface super-resolution . Theoretical analysis and experimental results on the latest benchmark dataset verify the effectiveness and versatility of our normal-based representation, which accepts 30 times of the input surface vertices but at the same time only takes 6.5\% memory cost and 14.0\% running time in comparison with existing competing algorithms.},
  archive      = {J_TPAMI},
  author       = {Wuyuan Xie and Miaohui Wang and Di Lin and Boxin Shi and Jianmin Jiang},
  doi          = {10.1109/TPAMI.2023.3296509},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13749-13765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Surface geometry processing: An efficient normal-based detail representation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The importance of expert knowledge for automatic modulation
open set recognition. <em>TPAMI</em>, <em>45</em>(11), 13730–13748. (<a
href="https://doi.org/10.1109/TPAMI.2023.3294505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic modulation classification (AMC) is an important technology for the monitoring, management, and control of communication systems. In recent years, machine learning approaches are becoming popular to improve the effectiveness of AMC for radio signals. However, the automatic modulation open-set recognition (AMOSR) scheme that aims to identify the known modulation types and recognize the unknown modulation signals is not well studied. Therefore, in this paper, we propose a novel multi-modal marginal prototype framework for radio frequency (RF) signals ( MMPRF ) to improve AMOSR performance. First, MMPRF addresses the problem of simultaneous recognition of closed and open sets by partitioning the feature space in the way of one versus other and marginal restrictions. Second, we exploit the wireless signal domain knowledge to extract a series of signal-related features to enhance the AMOSR capability. In addition, we propose a GAN-based unknown sample generation strategy to allow the model to understand the unknown world. Finally, we conduct extensive experiments on several publicly available radio modulation data, and experimental results show that our proposed MMPRF outperforms the state-of-the-art AMOSR methods.},
  archive      = {J_TPAMI},
  author       = {Taotao Li and Zhenyu Wen and Yang Long and Zhen Hong and Shilian Zheng and Li Yu and Bo Chen and Xiaoniu Yang and Ling Shao},
  doi          = {10.1109/TPAMI.2023.3294505},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13730-13748},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The importance of expert knowledge for automatic modulation open set recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FarSeg++: Foreground-aware relation network for geospatial
object segmentation in high spatial resolution remote sensing imagery.
<em>TPAMI</em>, <em>45</em>(11), 13715–13729. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geospatial object segmentation, a fundamental Earth vision task, always suffers from scale variation, the larger intra-class variance of background, and foreground-background imbalance in high spatial resolution (HSR) remote sensing imagery. Generic semantic segmentation methods mainly focus on the scale variation in natural scenarios. However, the other two problems are insufficiently considered in large area Earth observation scenarios. In this paper, we propose a foreground-aware relation network (FarSeg++) from the perspectives of relation-based, optimization-based, and objectness-based foreground modeling, alleviating the above two problems. From the perspective of the relations, the foreground-scene relation module improves the discrimination of the foreground features via the foreground-correlated contexts associated with the object-scene relation. From the perspective of optimization, foreground-aware optimization is proposed to focus on foreground examples and hard examples of the background during training to achieve a balanced optimization. Besides, from the perspective of objectness, a foreground-aware decoder is proposed to improve the objectness representation, alleviating the objectness prediction problem that is the main bottleneck revealed by an empirical upper bound analysis. We also introduce a new large-scale high-resolution urban vehicle segmentation dataset to verify the effectiveness of the proposed method and push the development of objectness prediction further forward. The experimental results suggest that FarSeg++ is superior to the state-of-the-art generic semantic segmentation methods and can achieve a better trade-off between speed and accuracy.},
  archive      = {J_TPAMI},
  author       = {Zhuo Zheng and Yanfei Zhong and Junjue Wang and Ailong Ma and Liangpei Zhang},
  doi          = {10.1109/TPAMI.2023.3296757},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13715-13729},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FarSeg++: Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning symbolic model-agnostic loss functions via
meta-learning. <em>TPAMI</em>, <em>45</em>(11), 13699–13714. (<a
href="https://doi.org/10.1109/TPAMI.2023.3294394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop upon the emerging topic of loss function learning, which aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for learning model-agnostic loss functions via a hybrid neuro-symbolic search approach. The framework first uses evolution-based methods to search the space of primitive mathematical operations to find a set of symbolic loss functions. Second, the set of learned loss functions are subsequently parameterized and optimized via an end-to-end gradient-based training procedure. The versatility of the proposed framework is empirically validated on a diverse set of supervised learning tasks. Results show that the meta-learned loss functions discovered by the newly proposed method outperform both the cross-entropy loss and state-of-the-art loss function learning methods on a diverse range of neural network architectures and datasets. We make our code available at * retracted *.},
  archive      = {J_TPAMI},
  author       = {Christian Raymond and Qi Chen and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TPAMI.2023.3294394},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13699-13714},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning symbolic model-agnostic loss functions via meta-learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DROID: Driver-centric risk object identification.
<em>TPAMI</em>, <em>45</em>(11), 13683–13698. (<a
href="https://doi.org/10.1109/TPAMI.2023.3294305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of high-risk driving situations is generally approached through collision risk estimation or accident pattern recognition. In this work, we approach the problem from the perspective of subjective risk. We operationalize subjective risk assessment by predicting driver behavior changes and identifying the cause of changes. To this end, we introduce a new task called driver-centric risk object identification (DROID), which uses egocentric video to identify object(s) influencing a driver&#39;s behavior, given only the driver&#39;s response as the supervision signal. We formulate the task as a cause-effect problem and present a novel two-stage DROID framework, taking inspiration from models of situation awareness and causal inference. A subset of data constructed from the Honda Research Institute Driving Dataset (HDD) is used to evaluate DROID. We demonstrate state-of-the-art DROID performance, even compared with strong baseline models using this dataset. Additionally, we conduct extensive ablative studies to justify our design choices. Moreover, we demonstrate the applicability of DROID for risk assessment.},
  archive      = {J_TPAMI},
  author       = {Chengxi Li and Stanley H. Chan and Yi-Ting Chen},
  doi          = {10.1109/TPAMI.2023.3294305},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13683-13698},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DROID: Driver-centric risk object identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RayMVSNet++: Learning ray-based 1D implicit fields for
accurate multi-view stereo. <em>TPAMI</em>, <em>45</em>(11),
13666–13682. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based multi-view stereo (MVS) has by far centered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited. Different from most existing works dedicated to adaptive refinement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range (depth) finding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization. In particular, we propose RayMVSNet which learns sequential prediction of a 1D implicit field along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer features, essentially learns the epipolar line search in traditional multi-view stereo. We devise a multi-task learning for better optimization convergence and depth accuracy. We found the monotonicity property of the SDFs along each ray greatly benefits the depth estimation. Our method ranks top on both the DTU and the Tanks &amp; Temples datasets over all previous learning-based methods, achieving an overall reconstruction score of 0.33 mm on DTU and an F-score of 59.48\% on Tanks &amp; Temples. It is able to produce high-quality depth estimation and point cloud reconstruction in challenging scenarios such as objects/scenes with non-textured surface, severe occlusion, and highly varying depth range. Further, we propose RayMVSNet++ to enhance contextual feature aggregation for each ray through designing an attentional gating unit to select semantically relevant neighboring rays within the local frustum around that ray. This improves the performance on datasets with more challenging examples (e.g., low-quality images caused by poor lighting conditions or motion blur). RayMVSNet++ achieves state-of-the-art performance on the ScanNet dataset. In particular, it attains an AbsRel of 0.058m and produces accurate results on the two subsets of textureless regions and large depth variation.},
  archive      = {J_TPAMI},
  author       = {Yifei Shi and Junhua Xi and Dewen Hu and Zhiping Cai and Kai Xu},
  doi          = {10.1109/TPAMI.2023.3296163},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13666-13682},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RayMVSNet++: Learning ray-based 1D implicit fields for accurate multi-view stereo},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Versatile weight attack via flipping limited bits.
<em>TPAMI</em>, <em>45</em>(11), 13653–13665. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many attack paradigms against deep neural networks have been well studied, such as the backdoor attack in the training stage and the adversarial attack in the inference stage. In this article, we study a novel attack paradigm, the bit-flip based weight attack, which directly modifies weight bits of the attacked model in the deployment stage. To meet various attack scenarios, we propose a general formulation including terms to achieve effectiveness and stealthiness goals and a constraint on the number of bit-flips. Furthermore, benefitting from this extensible and flexible formulation, we present two cases with different malicious purposes, i.e., single sample attack (SSA) and triggered samples attack (TSA). SSA which aims at misclassifying a specific sample into a target class is a binary optimization with determining the state of the binary bits (0 or 1); TSA which is to misclassify the samples embedded with a specific trigger is a mixed integer programming (MIP) with flipped bits and a learnable trigger. Utilizing the latest technique in integer programming, we equivalently reformulate them as continuous optimization problems, whose approximate solutions can be effectively and efficiently obtained by the alternating direction method of multipliers (ADMM) method. Extensive experiments demonstrate the superiority of our methods.},
  archive      = {J_TPAMI},
  author       = {Jiawang Bai and Baoyuan Wu and Zhifeng Li and Shu-Tao Xia},
  doi          = {10.1109/TPAMI.2023.3296408},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13653-13665},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Versatile weight attack via flipping limited bits},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransVG++: End-to-end visual grounding with language
conditioned vision transformer. <em>TPAMI</em>, <em>45</em>(11),
13636–13652. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.},
  archive      = {J_TPAMI},
  author       = {Jiajun Deng and Zhengyuan Yang and Daqing Liu and Tianlang Chen and Wengang Zhou and Yanyong Zhang and Houqiang Li and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2023.3296823},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13636-13652},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransVG++: End-to-end visual grounding with language conditioned vision transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D point-voxel correlation fields for scene flow estimation.
<em>TPAMI</em>, <em>45</em>(11), 13621–13635. (<a
href="https://doi.org/10.1109/TPAMI.2023.3294355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Point-Voxel Correlation Fields to explore relations between two consecutive point clouds and estimate scene flow that represents 3D motions. Most existing works only consider local correlations, which are able to handle small movements but fail when there are large displacements. Therefore, it is essential to introduce all-pair correlation volumes that are free from local neighbor restrictions and cover both short- and long-term dependencies. However, it is challenging to efficiently extract correlation features from all-pairs fields in the 3D space, given the irregular and unordered nature of point clouds. To tackle this problem, we present point-voxel correlation fields , proposing distinct point and voxel branches to inquire about local and long-range correlations from all-pair fields respectively. To exploit point-based correlations, we adopt the K-Nearest Neighbors search that preserves fine-grained information in the local region, which guarantees the scene flow estimation precision. By voxelizing point clouds in a multi-scale manner, we construct pyramid correlation voxels to model long-range correspondences, which are utilized to handle fast-moving objects. Integrating these two types of correlations, we propose Point-Voxel Recurrent All-Pairs Field Transforms (PV-RAFT) architecture that employs an iterative scheme to estimate scene flow from point clouds. To adapt to different flow scope conditions and obtain more fine-grained results, we further propose Deformable PV-RAFT (DPV-RAFT), where the Spatial Deformation deforms the voxelized neighborhood, and the Temporal Deformation controls the iterative update process. We evaluate the proposed method on the FlyingThings3D and KITTI Scene Flow 2015 datasets and experimental results show that we outperform state-of-the-art methods by remarkable margins.},
  archive      = {J_TPAMI},
  author       = {Ziyi Wang and Yi Wei and Yongming Rao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3294355},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13621-13635},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D point-voxel correlation fields for scene flow estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank-based decomposable losses in machine learning: A
survey. <em>TPAMI</em>, <em>45</em>(11), 13599–13620. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses versus aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank-based decomposable losses into eight categories. Following these categories, we review the literature on rank-based aggregate losses and rank-based individual losses. We describe general formulas for these losses and connect them with existing research topics. We also suggest future research directions spanning unexplored, remaining, and emerging issues in rank-based decomposable losses.},
  archive      = {J_TPAMI},
  author       = {Shu Hu and Xin Wang and Siwei Lyu},
  doi          = {10.1109/TPAMI.2023.3296062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13599-13620},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rank-based decomposable losses in machine learning: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DifFormer: Multi-resolutional differencing transformer with
dynamic ranging for time series analysis. <em>TPAMI</em>,
<em>45</em>(11), 13586–13598. (<a
href="https://doi.org/10.1109/TPAMI.2023.3293516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis is essential to many far-reaching applications of data science and statistics including economic and financial forecasting, surveillance, and automated business processing. Though being greatly successful of Transformer in computer vision and natural language processing, the potential of employing it as the general backbone in analyzing the ubiquitous times series data has not been fully released yet. Prior Transformer variants on time series highly rely on task-dependent designs and pre-assumed “pattern biases”, revealing its insufficiency in representing nuanced seasonal, cyclic, and outlier patterns which are highly prevalent in time series. As a consequence, they can not generalize well to different time series analysis tasks. To tackle the challenges, we propose DifFormer , an effective and efficient Transformer architecture that can serve as a workhorse for a variety of time-series analysis tasks. DifFormer incorporates a novel multi-resolutional differencing mechanism, which is able to progressively and adaptively make nuanced yet meaningful changes prominent, meanwhile, the periodic or cyclic patterns can be dynamically captured with flexible lagging and dynamic ranging operations. Extensive experiments demonstrate DifFormer significantly outperforms state-of-the-art models on three essential time-series analysis tasks, including classification, regression, and forecasting. In addition to its superior performances, DifFormer also excels in efficiency – a linear time/memory complexity with empirically lower time consumption.},
  archive      = {J_TPAMI},
  author       = {Bing Li and Wei Cui and Le Zhang and Ce Zhu and Wei Wang and Ivor W. Tsang and Joey Tianyi Zhou},
  doi          = {10.1109/TPAMI.2023.3293516},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13586-13598},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DifFormer: Multi-resolutional differencing transformer with dynamic ranging for time series analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A memorizing and generalizing framework for lifelong person
re-identification. <em>TPAMI</em>, <em>45</em>(11), 13567–13585. (<a
href="https://doi.org/10.1109/TPAMI.2023.3297058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a challenging yet practical setting for person re-identification (ReID) task, named lifelong person re-identification (LReID), which aims to continuously train a ReID model across multiple domains and the trained model is required to generalize well on both seen and unseen domains. It is therefore critical to learn a ReID model that can learn a generalized representation without forgetting knowledge of seen domains. In this paper, we propose a new MEmorizing and GEneralizing framework (MEGE) for LReID, which can jointly prevent the model from forgetting and improve its generalization ability. Specifically, our MEGE is composed of two novel modules, i.e. , Adaptive Knowledge Accumulation (AKA) and differentiable Ranking Consistency Distillation (RCD). Taking inspiration from the cognitive processes in the human brain, we endow AKA with two special capacities, knowledge representation and knowledge operation by graph convolution networks. AKA can effectively mitigate catastrophic forgetting on seen domains while improving the generalization ability to unseen domains. By considering the ranking factor that is specifically important in ReID, RCD is designed to distill the ranking knowledge in a differentiable manner, which can further prevent the catastrophic forgetting. To supporting the study of LReID, we build a new and large-scale benchmark with two practical evaluation protocols that consider the metrics of non-forgetting and generalization. Experiments demonstrate that 1) our MEGE framework can effectively improve the performance on seen and unseen domains under the domain-incremental learning constraint, and that 2) the proposed MEGE outperforms state-of-the-art competitors by large margins.},
  archive      = {J_TPAMI},
  author       = {Nan Pu and Zhun Zhong and Nicu Sebe and Michael S. Lew},
  doi          = {10.1109/TPAMI.2023.3297058},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13567-13585},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A memorizing and generalizing framework for lifelong person re-identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MADAv2: Advanced multi-anchor based active domain adaptation
segmentation. <em>TPAMI</em>, <em>45</em>(11), 13553–13566. (<a
href="https://doi.org/10.1109/TPAMI.2023.3293893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaption has been widely adopted in tasks with scarce annotated data. Unfortunately, mapping the target-domain distribution to the source-domain unconditionally may distort the essential structural information of the target-domain data, leading to inferior performance. To address this issue, we first propose to introduce active sample selection to assist domain adaptation regarding the semantic segmentation task. By innovatively adopting multiple anchors instead of a single centroid, both source and target domains can be better characterized as multimodal distributions, in which way more complementary and informative samples are selected from the target domain. With only a little workload to manually annotate these active samples, the distortion of the target-domain distribution can be effectively alleviated, achieving a large performance gain. In addition, a powerful semi-supervised domain adaptation strategy is proposed to alleviate the long-tail distribution problem and further improve the segmentation performance. Extensive experiments are conducted on public datasets, and the results demonstrate that the proposed approach outperforms state-of-the-art methods by large margins and achieves similar performance to the fully-supervised upperbound, i.e., 71.4\% mIoU on GTA5 and 71.8\% mIoU on SYNTHIA. The effectiveness of each component is also verified by thorough ablation studies.},
  archive      = {J_TPAMI},
  author       = {Munan Ning and Donghuan Lu and Yujia Xie and Dongdong Chen and Dong Wei and Yefeng Zheng and Yonghong Tian and Shuicheng Yan and Li Yuan},
  doi          = {10.1109/TPAMI.2023.3293893},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13553-13566},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MADAv2: Advanced multi-anchor based active domain adaptation segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Noisy label learning with provable consistency for a wider
family of losses. <em>TPAMI</em>, <em>45</em>(11), 13536–13552. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models have achieved state-of-the-art performance on a broad range of visual recognition tasks. Nevertheless, the generalization ability of deep models is seriously affected by noisy labels. Though deep learning packages have different losses, this is not transparent for users to choose consistent losses. This paper addresses the problem of how to use abundant loss functions designed for the traditional classification problem in the presence of label noise. We present a dynamic label learning (DLL) algorithm for noisy label learning and then prove that any surrogate loss function can be used for classification with noisy labels by using our proposed algorithm, with a consistency guarantee that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. In addition, we provide a depth theoretical analysis of our algorithm to verify the justifies’ correctness and explain the powerful robustness. Finally, experimental results on synthetic and real datasets confirm the efficiency of our algorithm and the correctness of our justifies and show that our proposed algorithm significantly outperforms or is comparable to current state-of-the-art counterparts.},
  archive      = {J_TPAMI},
  author       = {Defu Liu and Wen Li and Lixin Duan and Ivor W. Tsang and Guowu Yang},
  doi          = {10.1109/TPAMI.2023.3296156},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13536-13552},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Noisy label learning with provable consistency for a wider family of losses},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flow-based spatio-temporal structured prediction of motion
dynamics. <em>TPAMI</em>, <em>45</em>(11), 13523–13535. (<a
href="https://doi.org/10.1109/TPAMI.2023.3296446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional Normalizing Flows (CNFs) are flexible generative models capable of representing complicated distributions with high dimensionality and large interdimensional correlations, making them appealing for structured output learning. Their effectiveness in modelling multivariates spatio-temporal structured data has yet to be completely investigated. We propose MotionFlow as a novel normalizing flows approach that autoregressively conditions the output distributions on the spatio-temporal input features. It combines deterministic and stochastic representations with CNFs to create a probabilistic neural generative approach that can model the variability seen in high-dimensional structured spatio-temporal data. We specifically propose to use conditional priors to factorize the latent space for the time dependent modeling. We also exploit the use of masked convolutions as autoregressive conditionals in CNFs. As a result, our method is able to define arbitrarily expressive output probability distributions under temporal dynamics in multivariate prediction tasks. We apply our method to different tasks, including trajectory prediction, motion prediction, time series forecasting, and binary segmentation, and demonstrate that our model is able to leverage normalizing flows to learn complicated time dependent conditional distributions.},
  archive      = {J_TPAMI},
  author       = {Mohsen Zand and Ali Etemad and Michael Greenspan},
  doi          = {10.1109/TPAMI.2023.3296446},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13523-13535},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Flow-based spatio-temporal structured prediction of motion dynamics},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting field dependencies for learning on categorical
data. <em>TPAMI</em>, <em>45</em>(11), 13509–13522. (<a
href="https://doi.org/10.1109/TPAMI.2023.3298028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional approaches for learning on categorical data underexploit the dependencies between columns (a.k.a. fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the matrix performing projection) and global dependency matrix in a supervised fashion (with the use of labels). Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks. Detailed ablation studies provide additional insights into our method.},
  archive      = {J_TPAMI},
  author       = {Zhibin Li and Piotr Koniusz and Lu Zhang and Daniel Edward Pagendam and Peyman Moghadam},
  doi          = {10.1109/TPAMI.2023.3298028},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13509-13522},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploiting field dependencies for learning on categorical data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MNGNAS: Distilling adaptive combination of multiple searched
networks for one-shot neural architecture search. <em>TPAMI</em>,
<em>45</em>(11), 13489–13508. (<a
href="https://doi.org/10.1109/TPAMI.2023.3293885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently neural architecture (NAS) search has attracted great interest in academia and industry. It remains a challenging problem due to the huge search space and computational costs. Recent studies in NAS mainly focused on the usage of weight sharing to train a SuperNet once. However, the corresponding branch of each subnetwork is not guaranteed to be fully trained. It may not only incur huge computation costs but also affect the architecture ranking in the retraining procedure. We propose a multi-teacher-guided NAS, which proposes to use the adaptive ensemble and perturbation-aware knowledge distillation algorithm in the one-shot-based NAS algorithm. The optimization method aiming to find the optimal descent directions is used to obtain adaptive coefficients for the feature maps of the combined teacher model. Besides, we propose a specific knowledge distillation process for optimal architectures and perturbed ones in each searching process to learn better feature maps for later distillation procedures. Comprehensive experiments verify our approach is flexible and effective. We show improvement in precision and search efficiency in the standard recognition dataset. We also show improvement in correlation between the accuracy of the search algorithm and true accuracy by NAS benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Zhihua Chen and Guhao Qiu and Ping Li and Lei Zhu and Xiaokang Yang and Bin Sheng},
  doi          = {10.1109/TPAMI.2023.3293885},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13489-13508},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MNGNAS: Distilling adaptive combination of multiple searched networks for one-shot neural architecture search},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards large-scale small object detection: Survey and
benchmarks. <em>TPAMI</em>, <em>45</em>(11), 13467–13488. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of deep convolutional neural networks, object detection has achieved prominent advances in past years. However, such prosperity could not camouflage the unsatisfactory situation of Small Object Detection (SOD), one of the notoriously challenging tasks in computer vision, owing to the poor visual appearance and noisy representation caused by the intrinsic structure of small targets. In addition, large-scale dataset for benchmarking small object detection methods remains a bottleneck. In this paper, we first conduct a thorough review of small object detection. Then, to catalyze the development of SOD, we construct two large-scale Small Object Detection dAtasets (SODA), SODA-D and SODA-A, which focus on the Driving and Aerial scenarios respectively. SODA-D includes 24828 high-quality traffic images and 278433 instances of nine categories. For SODA-A, we harvest 2513 high resolution aerial images and annotate 872069 instances over nine classes. The proposed datasets, as we know, are the first-ever attempt to large-scale benchmarks with a vast collection of exhaustively annotated instances tailored for multi-category SOD. Finally, we evaluate the performance of mainstream methods on SODA. We expect the released benchmarks could facilitate the development of SOD and spawn more breakthroughs in this field.},
  archive      = {J_TPAMI},
  author       = {Gong Cheng and Xiang Yuan and Xiwen Yao and Kebing Yan and Qinghua Zeng and Xingxing Xie and Junwei Han},
  doi          = {10.1109/TPAMI.2023.3290594},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13467-13488},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards large-scale small object detection: Survey and benchmarks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing aggregation functions in GNNs: Building high
capacity and robust GNNs via nonlinear aggregation. <em>TPAMI</em>,
<em>45</em>(11), 13454–13466. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aspect powering GNNs is the multi-layer network architecture to learn the nonlinear representation for graph learning task. The core operation in GNNs is the message propagation in which each node updates its information by aggregating the information from its neighbors. Existing GNNs usually adopt either linear neighborhood aggregation (e.g. mean, sum) or max aggregator in their message propagation. 1) For linear aggregators, the whole nonlinearity and network&#39;s capacity of GNNs are generally limited because deeper GNNs usually suffer from the over-smoothing issue due to their inherent information propagation mechanism. Also, linear aggregators are usually vulnerable to the spatial perturbations. 2) For max aggregator, it usually fails to be aware of the detailed information of node representations within neighborhood. To overcome these issues, we re-think the message propagation mechanism in GNNs and develop the new general nonlinear aggregators for neighborhood information aggregation in GNNs. One main aspect of our nonlinear aggregators is that they all provide the optimally balanced aggregator between max and mean/sum aggregators. Thus, they can inherit both i) high nonlinearity that enhances network&#39;s capacity, robustness and ii) detail-sensitivity that is aware of the detailed information of node representations in GNNs’ message propagation. Promising experiments show the effectiveness, high capacity and robustness of the proposed methods.},
  archive      = {J_TPAMI},
  author       = {Beibei Wang and Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2023.3290649},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13454-13466},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalizing aggregation functions in GNNs: Building high capacity and robust GNNs via nonlinear aggregation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-aware adversarial makeup generation for facial privacy
protection. <em>TPAMI</em>, <em>45</em>(11), 13438–13453. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The privacy and security of face data on social media are facing unprecedented challenges as it is vulnerable to unauthorized access and identification. A common practice for solving this problem is to modify the original data so that it could be protected from being recognized by malicious face recognition (FR) systems. However, such “adversarial examples” obtained by existing methods usually suffer from low transferability and poor image quality, which severely limits the application of these methods in real-world scenarios. In this paper, we propose a 3D-Aware Adversarial Makeup Generation GAN (3DAM-GAN). which aims to improve the quality and transferability of synthetic makeup for identity information concealing. Specifically, a UV-based generator consisting of a novel Makeup Adjustment Module (MAM) and Makeup Transfer Module (MTM) is designed to render realistic and robust makeup with the aid of symmetric characteristics of human faces. Moreover, a makeup attack mechanism with an ensemble training strategy is proposed to boost the transferability of black-box models. Extensive experiment results on several benchmark datasets demonstrate that 3DAM-GAN could effectively protect faces against various FR models, including both publicly available state-of-the-art models and commercial face verification APIs, such as Face++, Baidu, and Aliyun.},
  archive      = {J_TPAMI},
  author       = {Yueming Lyu and Yue Jiang and Ziwen He and Bo Peng and Yunfan Liu and Jing Dong},
  doi          = {10.1109/TPAMI.2023.3290175},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13438-13453},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D-aware adversarial makeup generation for facial privacy protection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-efficient learning via minimizing hyperspherical
energy. <em>TPAMI</em>, <em>45</em>(11), 13422–13437. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on large-scale data is currently dominant nowadays. The unprecedented scale of data has been arguably one of the most important driving forces behind its success. However, there still exist scenarios where collecting data or labels could be extremely expensive, e.g., medical imaging and robotics. To fill up this gap, this paper considers the problem of data-efficient learning from scratch using a small amount of representative data. First, we characterize this problem by active learning on homeomorphic tubes of spherical manifolds. This naturally generates feasible hypothesis class. With homologous topological properties, we identify an important connection – finding tube manifolds is equivalent to minimizing hyperspherical energy (MHE) in physical geometry. Inspired by this connection, we propose a MHE-based active learning (MHEAL) algorithm, and provide comprehensive theoretical guarantees for MHEAL, covering convergence and generalization analysis. Finally, we demonstrate the empirical performance of MHEAL in a wide range of applications for data-efficient learning, including deep clustering, distribution matching, version space sampling, and deep active learning.},
  archive      = {J_TPAMI},
  author       = {Xiaofeng Cao and Weiyang Liu and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2023.3290544},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13422-13437},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Data-efficient learning via minimizing hyperspherical energy},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive region-specific loss for improved medical image
segmentation. <em>TPAMI</em>, <em>45</em>(11), 13408–13421. (<a
href="https://doi.org/10.1109/TPAMI.2023.3289667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defining the loss function is an important part of neural network design and critically determines the success of deep learning modeling. A significant shortcoming of the conventional loss functions is that they weight all regions in the input image volume equally, despite the fact that the system is known to be heterogeneous (i.e., some regions can achieve high prediction performance more easily than others). Here, we introduce a region-specific loss to lift the implicit assumption of homogeneous weighting for better learning. We divide the entire volume into multiple sub-regions, each with an individualized loss constructed for optimal local performance. Effectively, this scheme imposes higher weightings on the sub-regions that are more difficult to segment, and vice versa . Furthermore, the regional false positive and false negative errors are computed for each input image during a training step and the regional penalty is adjusted accordingly to enhance the overall accuracy of the prediction. Using different public and in-house medical image datasets, we demonstrate that the proposed regionally adaptive loss paradigm outperforms conventional methods in the multi-organ segmentations, without any modification to the neural network architecture or additional data preparation.},
  archive      = {J_TPAMI},
  author       = {Yizheng Chen and Lequan Yu and Jen-Yeu Wang and Neil Panjwani and Jean-Pierre Obeid and Wu Liu and Lianli Liu and Nataliya Kovalchuk and Michael Francis Gensheimer and Lucas Kas Vitzthum and Beth M. Beadle and Daniel T. Chang and Quynh-Thu Le and Bin Han and Lei Xing},
  doi          = {10.1109/TPAMI.2023.3289667},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13408-13421},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive region-specific loss for improved medical image segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-lingual universal dependency parsing only from one
monolingual treebank. <em>TPAMI</em>, <em>45</em>(11), 13393–13407. (<a
href="https://doi.org/10.1109/TPAMI.2023.3291388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Syntactic parsing is a highly linguistic processing task whose parser requires training on treebanks from the expensive human annotation. As it is unlikely to obtain a treebank for every human language, in this work, we propose an effective cross-lingual UD parsing framework for transferring parser from only one source monolingual treebank to any other target languages without treebank available. To reach satisfactory parsing accuracy among quite different languages, we introduce two language modeling tasks into the training process of dependency parsing as multi-tasking. Assuming only unlabeled data from target languages plus the source treebank can be exploited together, we adopt a self-training strategy for further performance improvement in terms of our multi-task framework. Our proposed cross-lingual parsers are implemented for English, Chinese, and 29 UD treebanks. The empirical study shows that our cross-lingual parsers yield promising results for all target languages, approaching the parser performance which is trained in its own target treebank.},
  archive      = {J_TPAMI},
  author       = {Kailai Sun and Zuchao Li and Hai Zhao},
  doi          = {10.1109/TPAMI.2023.3291388},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13393-13407},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-lingual universal dependency parsing only from one monolingual treebank},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CycMuNet+: Cycle-projected mutual learning for
spatial-temporal video super-resolution. <em>TPAMI</em>,
<em>45</em>(11), 13376–13392. (<a
href="https://doi.org/10.1109/TPAMI.2023.3293522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate high-quality videos with higher resolution (HR) and higher frame rate (HFR). Quite intuitively, pioneering two-stage based methods complete ST-VSR by directly combining two sub-tasks: Spatial Video Super-Resolution (S-VSR) and Temporal Video Super-Resolution (T-VSR) but ignore the reciprocal relations among them. 1) T-VSR to S-VSR: temporal correlations help accurate spatial detail representation; 2) S-VSR to T-VSR: abundant spatial information contributes to the refinement of temporal prediction. To this end, we propose a one-stage based Cycle-projected Mutual learning network (CycMuNet) for ST-VSR, which makes full use of spatial-temporal correlations via the mutual learning between S-VSR and T-VSR. Specifically, we propose to exploit the mutual information among them via iterative up- and down projections, where spatial and temporal features are fully fused and distilled, helping high-quality video reconstruction. In addition, we also show interesting extensions for efficient network design (CycMuNet+), such as parameter sharing and dense connection on projection units and feedback mechanism in CycMuNet. Besides extensive experiments on benchmark datasets, we also compare our proposed CycMuNet (+) with S-VSR and T-VSR tasks, demonstrating that our method significantly outperforms the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Mengshun Hu and Kui Jiang and Zheng Wang and Xiang Bai and Ruimin Hu},
  doi          = {10.1109/TPAMI.2023.3293522},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13376-13392},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CycMuNet+: Cycle-projected mutual learning for spatial-temporal video super-resolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards causality-aware inferring: A sequential
discriminative approach for medical diagnosis. <em>TPAMI</em>,
<em>45</em>(11), 13363–13375. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical diagnosis assistant (MDA) aims to build an interactive diagnostic agent to sequentially inquire about symptoms for discriminating diseases. However, since the dialogue records for building a patient simulator are collected passively, the collected records might be deteriorated by some task-unrelated biases, such as the preference of the collectors. These biases might hinder the diagnostic agent to capture transportable knowledge from the simulator. This work identifies and resolves two representative non-causal biases, i.e., (i) default-answer bias and (ii) distributional inquiry bias. Specifically, Bias (i) originates from the patient simulator which tries to answer the unrecorded inquiries with some biased default answers. To eliminate this bias and improve upon a well-known causal inference technique, i.e., propensity score matching, we propose a novel propensity latent matching in building a patient simulator to effectively answer unrecorded inquiries; Bias (ii) inherently comes along with the passively collected data that the agent might learn by remembering what to inquire within the training data while not able to generalize to the out-of-distribution cases. To this end, we propose a progressive assurance agent, which includes the dual processes accounting for symptom inquiry and disease diagnosis respectively. The diagnosis process pictures the patient mentally and probabilistically by intervention to eliminate the effect of the inquiry behavior. And the inquiry process is driven by the diagnosis process to inquire about symptoms to enhance the diagnostic confidence which alters as the patient distribution changes. In this cooperative manner, our proposed agent can improve upon the out-of-distribution generalization significantly. Extensive experiments demonstrate that our framework achieves new state-of-the-art performance and possesses the advantage of transportability.},
  archive      = {J_TPAMI},
  author       = {Junfan Lin and Keze Wang and Ziliang Chen and Xiaodan Liang and Liang Lin},
  doi          = {10.1109/TPAMI.2023.3292363},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13363-13375},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards causality-aware inferring: A sequential discriminative approach for medical diagnosis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning in deep reinforcement learning: A survey.
<em>TPAMI</em>, <em>45</em>(11), 13344–13362. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.},
  archive      = {J_TPAMI},
  author       = {Zhuangdi Zhu and Kaixiang Lin and Anil K. Jain and Jiayu Zhou},
  doi          = {10.1109/TPAMI.2023.3292075},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13344-13362},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transfer learning in deep reinforcement learning: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous multi-party learning with data-driven network
sampling. <em>TPAMI</em>, <em>45</em>(11), 13328–13343. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-party learning provides an effective approach for training a machine learning model, e.g., deep neural networks (DNNs), over decentralized data by leveraging multiple decentralized computing devices, subjected to legal and practical constraints. Different parties, so-called local participants, usually provide heterogenous data in a decentralized mode, leading to non-IID data distributions across different local participants which pose a notorious challenge for multi-party learning. To address this challenge, we propose a novel heterogeneous differentiable sampling (HDS) framework. Inspired by the dropout strategy in DNNs, a data-driven network sampling strategy is devised in the HDS framework, with differentiable sampling rates which allow each local participant to extract from a common global model the optimal local model that best adapts to its own data properties so that the size of the local model can be significantly reduced to enable more efficient inference. Meanwhile, co-adaptation of the global model via learning such local models allows for achieving better learning performance under non-IID data distributions and speeds up the convergence of the global model. Experiments have demonstrated the superiority of the proposed method over several popular multi-party learning techniques in the multi-party settings with non-IID data distributions.},
  archive      = {J_TPAMI},
  author       = {Maoguo Gong and Yuan Gao and Yue Wu and Yuanqiao Zhang and A. K. Qin and Yew-Soon Ong},
  doi          = {10.1109/TPAMI.2023.3290213},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13328-13343},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Heterogeneous multi-party learning with data-driven network sampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric deep neural network using rigid and non-rigid
transformations for landmark-based human behavior analysis.
<em>TPAMI</em>, <em>45</em>(11), 13314–13327. (<a
href="https://doi.org/10.1109/TPAMI.2023.3291663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this article, we propose a geometric deep learning approach using rigid and non-rigid transformations, named KShapenet, for 2D and 3D landmark-based human motion analysis. Landmark configuration sequences are first modeled as trajectories on Kendall&#39;s shape space and then mapped to a linear tangent space. The resulting structured data are then input to a deep learning architecture, which includes a layer that optimizes over rigid and non-rigid transformations of landmark configurations, followed by a CNN-LSTM network. We apply KShapenet to 3D human landmark sequences for action and gait recognition, and 2D facial landmark sequences for expression recognition, and demonstrate the competitiveness of the proposed approach with respect to state-of-the-art.},
  archive      = {J_TPAMI},
  author       = {Rasha Friji and Faten Chaieb and Hassen Drira and Sebastian Kurtek},
  doi          = {10.1109/TPAMI.2023.3291663},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13314-13327},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geometric deep neural network using rigid and non-rigid transformations for landmark-based human behavior analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative uncertainty benefits multi-agent multi-modal
trajectory forecasting. <em>TPAMI</em>, <em>45</em>(11), 13297–13313.
(<a href="https://doi.org/10.1109/TPAMI.2023.3290823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-modal multi-agent trajectory forecasting, two major challenges have not been fully tackled: 1) how to measure the uncertainty brought by the interaction module that causes correlations among the predicted trajectories of multiple agents; 2) how to rank the multiple predictions and select the optimal predicted trajectory. In order to handle the aforementioned challenges, this work first proposes a novel concept, collaborative uncertainty (CU), which models the uncertainty resulting from interaction modules. Then we build a general CU-aware regression framework with an original permutation-equivariant uncertainty estimator to do both tasks of regression and uncertainty estimation. Furthermore, we apply the proposed framework to current SOTA multi-agent multi-modal forecasting systems as a plugin module, which enables the SOTA systems to: 1) estimate the uncertainty in the multi-agent multi-modal trajectory forecasting task; 2) rank the multiple predictions and select the optimal one based on the estimated uncertainty. We conduct extensive experiments on a synthetic dataset and two public large-scale multi-agent trajectory forecasting benchmarks. Experiments show that: 1) on the synthetic dataset, the CU-aware regression framework allows the model to appropriately approximate the ground-truth Laplace distribution; 2) on the multi-agent trajectory forecasting benchmarks, the CU-aware regression framework steadily helps SOTA systems improve their performances. Especially, the proposed framework helps VectorNet improve by 262 cm regarding the Final Displacement Error of the chosen optimal prediction on the nuScenes dataset; 3) in multi-agent multi-modal trajectory forecasting, prediction uncertainty is proportional to future stochasticity; 4) the estimated CU values are highly related to the interactive information among agents. The proposed framework can guide the development of more reliable and safer forecasting systems in the future.},
  archive      = {J_TPAMI},
  author       = {Bohan Tang and Yiqi Zhong and Chenxin Xu and Wei-Tao Wu and Ulrich Neumann and Ya Zhang and Siheng Chen and Yanfeng Wang},
  doi          = {10.1109/TPAMI.2023.3290823},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13297-13313},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Collaborative uncertainty benefits multi-agent multi-modal trajectory forecasting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ModeRNN: Harnessing spatiotemporal mode collapse in
unsupervised predictive learning. <em>TPAMI</em>, <em>45</em>(11),
13281–13296. (<a
href="https://doi.org/10.1109/TPAMI.2023.3293145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning predictive models for unlabeled spatiotemporal data is challenging in part because visual dynamics can be highly entangled, especially in real scenes. In this paper, we refer to the multi-modal output distribution of predictive learning as spatiotemporal modes . We find an experimental phenomenon named spatiotemporal mode collapse (STMC) on most existing video prediction models, that is, features collapse into invalid representation subspaces due to the ambiguous understanding of mixed physical processes. We propose to quantify STMC and explore its solution for the first time in the context of unsupervised predictive learning. To this end, we present ModeRNN, a decoupling-aggregation framework that has a strong inductive bias of discovering the compositional structures of spatiotemporal modes between recurrent states. We first leverage a set of dynamic slots with independent parameters to extract individual building components of spatiotemporal modes. We then perform a weighted fusion of slot features to adaptively aggregate them into a unified hidden representation for recurrent updates. Through a series of experiments, we show high correlation between STMC and the fuzzy prediction results of future video frames. Besides, ModeRNN is shown to better mitigate STMC and achieve the state of the art on five video prediction datasets.},
  archive      = {J_TPAMI},
  author       = {Zhiyu Yao and Yunbo Wang and Haixu Wu and Jianmin Wang and Mingsheng Long},
  doi          = {10.1109/TPAMI.2023.3293145},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13281-13296},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ModeRNN: Harnessing spatiotemporal mode collapse in unsupervised predictive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive video question answering via video graph
transformer. <em>TPAMI</em>, <em>45</em>(11), 13265–13280. (<a
href="https://doi.org/10.1109/TPAMI.2023.3292266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to perform video question answering (VideoQA) in a Co ntrastive manner via a V ideo G raph T ransformer model (CoVGT). CoVGT&#39;s uniqueness and superiority are three-fold: 1) It proposes a dynamic graph transformer module which encodes video by explicitly capturing the visual objects, their relations and dynamics, for complex spatio-temporal reasoning. 2) It designs separate video and text transformers for contrastive learning between the video and text to perform QA, instead of multi-modal transformer for answer classification. Fine-grained video-text communication is done by additional cross-modal interaction modules. 3) It is optimized by the joint fully- and self-supervised contrastive objectives between the correct and incorrect answers, as well as the relevant and irrelevant questions respectively. With superior video encoding and QA solution, we show that CoVGT can achieve much better performances than previous arts on video reasoning tasks. Its performances even surpass those models that are pretrained with millions of external data. We further show that CoVGT can also benefit from cross-modal pretraining, yet with orders of magnitude smaller data. The results demonstrate the effectiveness and superiority of CoVGT, and additionally reveal its potential for more data-efficient pretraining.},
  archive      = {J_TPAMI},
  author       = {Junbin Xiao and Pan Zhou and Angela Yao and Yicong Li and Richang Hong and Shuicheng Yan and Tat-Seng Chua},
  doi          = {10.1109/TPAMI.2023.3292266},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13265-13280},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive video question answering via video graph transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View synthesis of dynamic scenes based on deep 3D mask
volume. <em>TPAMI</em>, <em>45</em>(11), 13250–13264. (<a
href="https://doi.org/10.1109/TPAMI.2023.3289333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image view synthesis has seen great success in reconstructing photorealistic visuals, thanks to deep learning and various novel representations. The next key step in immersive virtual experiences is view synthesis of dynamic scenes. However, several challenges exist due to the lack of high-quality training datasets, and the additional time dimension for videos of dynamic scenes. To address this issue, we introduce a multi-view video dataset, captured with a custom 10-camera rig in 120FPS. The dataset contains 96 high-quality scenes showing various visual effects and human interactions in outdoor scenes. We develop a new algorithm, Deep 3D Mask Volume, which enables temporally-stable view extrapolation from binocular videos of dynamic scenes, captured by static cameras. Our algorithm addresses the temporal inconsistency of disocclusions by identifying the error-prone areas with a 3D mask volume, and replaces them with static background observed throughout the video. Our method enables manipulation in 3D space as opposed to simple 2D masks, We demonstrate better temporal stability than frame-by-frame static view synthesis methods, or those that use 2D masks. The resulting view synthesis videos show minimal flickering artifacts and allow for larger translational movements.},
  archive      = {J_TPAMI},
  author       = {Kai-En Lin and Guowei Yang and Lei Xiao and Feng Liu and Ravi Ramamoorthi},
  doi          = {10.1109/TPAMI.2023.3289333},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13250-13264},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {View synthesis of dynamic scenes based on deep 3D mask volume},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bipartite ranking fairness through a model agnostic ordering
adjustment. <em>TPAMI</em>, <em>45</em>(11), 13235–13249. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the applications of algorithms in various risky scenarios, algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this article, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. We are interested in whether the learned ranking function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.},
  archive      = {J_TPAMI},
  author       = {Sen Cui and Weishen Pan and Changshui Zhang and Fei Wang},
  doi          = {10.1109/TPAMI.2023.3290949},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13235-13249},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bipartite ranking fairness through a model agnostic ordering adjustment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactual samples synthesizing and training for robust
visual question answering. <em>TPAMI</em>, <em>45</em>(11), 13218–13234.
(<a href="https://doi.org/10.1109/TPAMI.2023.3290012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to the complex model design, ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. Specifically, CSST is composed of two parts: Counterfactual Samples Synthesizing (CSS) and Counterfactual Samples Training (CST). CSS generates counterfactual samples by carefully masking critical objects in images or words in questions and assigning pseudo ground-truth answers. CST not only trains the VQA models with both complementary samples to predict respective ground-truth answers, but also urges the VQA models to further distinguish the original samples and superficially similar counterfactual ones. To facilitate the CST training, we propose two variants of supervised contrastive loss for VQA, and design an effective positive and negative sample selection mechanism based on CSS. Extensive experiments have shown the effectiveness of CSST. Particularly, by building on top of model LMH+SAR (Clark et al. 2019), (Si et al. 2021), we achieve record-breaking performance on all out-of-distribution benchmarks (e.g., VQA-CP v2, VQA-CP v1, and GQA-OOD).},
  archive      = {J_TPAMI},
  author       = {Long Chen and Yuhang Zheng and Yulei Niu and Hanwang Zhang and Jun Xiao},
  doi          = {10.1109/TPAMI.2023.3290012},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13218-13234},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Counterfactual samples synthesizing and training for robust visual question answering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards enabling binary decomposition for partial
multi-label learning. <em>TPAMI</em>, <em>45</em>(11), 13203–13217. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) is an emerging weakly supervised learning framework, where each training example is associated with multiple candidate labels which are only partially valid. To learn the multi-label predictive model from PML training examples, most existing approaches work by identifying valid labels within candidate label set via label confidence estimation. In this paper, a novel strategy towards partial multi-label learning is proposed by enabling binary decomposition for handling PML training examples. Specifically, the widely used error-correcting output codes (ECOC) techniques are adapted to transform the PML learning problem into a number of binary learning problems, which refrains from using the error-prone procedure of estimating labeling confidence of individual candidate label. In the encoding phase, a ternary encoding scheme is utilized to balance the definiteness and adequacy of the derived binary training set. In the decoding phase, a loss weighted scheme is applied to consider the empirical performance and predictive margin of derived binary classifiers. Extensive comparative studies against state-of-the-art PML learning approaches clearly show the performance advantage of the proposed binary decomposition strategy for partial multi-label learning.},
  archive      = {J_TPAMI},
  author       = {Bing-Qing Liu and Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1109/TPAMI.2023.3290797},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13203-13217},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards enabling binary decomposition for partial multi-label learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging symbolic knowledge bases for commonsense natural
language inference using pattern theory. <em>TPAMI</em>,
<em>45</em>(11), 13185–13202. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The commonsense natural language inference (CNLI) tasks aim to select the most likely follow-up statement to a contextual description of ordinary, everyday events and facts. Current approaches to transfer learning of CNLI models across tasks require many labeled data from the new task. This paper presents a way to reduce this need for additional annotated training data from the new task by leveraging symbolic knowledge bases, such as ConceptNet. We formulate a teacher-student framework for mixed symbolic-neural reasoning, with the large-scale symbolic knowledge base serving as the teacher and a trained CNLI model as the student. This hybrid distillation process involves two steps. The first step is a symbolic reasoning process. Given a collection of unlabeled data, we use an abductive reasoning framework based on Grenander&#39;s pattern theory to create weakly labeled data. Pattern theory is an energy-based graphical probabilistic framework for reasoning among random variables with varying dependency structures. In the second step, the weakly labeled data, along with a fraction of the labeled data, is used to transfer-learn the CNLI model into the new task. The goal is to reduce the fraction of labeled data required. We demonstrate the efficacy of our approach by using three publicly available datasets (OpenBookQA, SWAG, and HellaSWAG) and evaluating three CNLI models (BERT, LSTM, and ESIM) that represent different tasks. We show that, on average, we achieve 63\% of the top performance of a fully supervised BERT model with no labeled data. With only 1,000 labeled samples, we can improve this performance to 72\%. Interestingly, without training, the teacher mechanism itself has significant inference power. The pattern theory framework achieves 32.7\% accuracy on OpenBookQA, outperforming transformer-based models such as GPT (26.6\%), GPT-2 (30.2\%), and BERT (27.1\%) by a significant margin. We demonstrate that the framework can be generalized to successfully train neural CNLI models using knowledge distillation under unsupervised and semi-supervised learning settings. Our results show that it outperforms all unsupervised and weakly supervised baselines and some early supervised approaches, while offering competitive performance with fully supervised baselines. Additionally, we show that the abductive learning framework can be adapted for other downstream tasks, such as unsupervised semantic textual similarity, unsupervised sentiment classification, and zero-shot text classification, without significant modification to the framework. Finally, user studies show that the generated interpretations enhance its explainability by providing key insights into its reasoning mechanism.},
  archive      = {J_TPAMI},
  author       = {Sathyanarayanan N. Aakur and Sudeep Sarkar},
  doi          = {10.1109/TPAMI.2023.3287837},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13185-13202},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Leveraging symbolic knowledge bases for commonsense natural language inference using pattern theory},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relational temporal graph reasoning for dual-task dialogue
language understanding. <em>TPAMI</em>, <em>45</em>(11), 13170–13184.
(<a href="https://doi.org/10.1109/TPAMI.2023.3289509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual-task dialog language understanding aims to tackle two correlative dialog language understanding tasks simultaneously via leveraging their inherent correlations. In this paper, we put forward a new framework, whose core is relational temporal graph reasoning. We propose a speaker-aware temporal graph (SATG) and a dual-task relational temporal graph (DRTG) to facilitate relational temporal modeling in dialog understanding and dual-task reasoning. Besides, different from previous works that only achieve implicit semantics-level interactions, we propose to model the explicit dependencies via integrating prediction-level interactions . To implement our framework, we first propose a novel model D ual-t A sk temporal R elational r E current R easoning network ( DARER ), which first generates the context-, speaker- and temporal-sensitive utterance representations through relational temporal modeling of SATG, then conducts recurrent dual-task relational temporal graph reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions. And the relational temporal modeling in DARER is achieved by relational graph convolutional networks (RGCNs). Then we further propose Re lational Te mporal Trans former ( ReTeFormer ), which achieves fine-grained relational temporal modeling via Relation- and Structure-aware Disentangled Multi-head Attention. Accordingly, we propose DARER with R eTeFormer ( DARER$^{\mathbf{2}}$2 ), which adopts two variants of ReTeFormer to achieve the relational temporal modeling of SATG and DTRG, respectively. The extensive experiments on different scenarios verify that our models outperform state-of-the-art models by a large margin. Remarkably, on the dialog sentiment classification task in the Mastodon dataset, DARER and DARER $^{2}$ gain relative improvements of about 28\% and 34\% over the previous best model in terms of F1.},
  archive      = {J_TPAMI},
  author       = {Bowen Xing and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2023.3289509},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13170-13184},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Relational temporal graph reasoning for dual-task dialogue language understanding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards more reliable confidence estimation. <em>TPAMI</em>,
<em>45</em>(11), 13152–13169. (<a
href="https://doi.org/10.1109/TPAMI.2023.3291676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a task that aims to assess the trustworthiness of the model&#39;s prediction output during deployment, confidence estimation has received much research attention recently, due to its importance for the safe deployment of deep models. Previous works have outlined two important characteristics that a reliable confidence estimation model should possess, i.e., the ability to perform well under label imbalance and the ability to handle various out-of-distribution data inputs. In this work, we propose a meta-learning framework that can simultaneously improve upon both characteristics in a confidence estimation model. Specifically, we first construct virtual training and testing sets with some intentionally designed distribution differences between them. Our framework then uses the constructed sets to train the confidence estimation model through a virtual training and testing scheme leading it to learn knowledge that generalizes to diverse distributions. Besides, we also incorporate our framework with a modified meta optimization rule, which converges the confidence estimator to flat meta minima . We show the effectiveness of our framework through extensive experiments on various tasks including monocular depth estimation, image classification, and semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Haoxuan Qu and Lin Geng Foo and Yanchao Li and Jun Liu},
  doi          = {10.1109/TPAMI.2023.3291676},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13152-13169},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards more reliable confidence estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesis of multi-view 3D fingerprints to advance
contactless fingerprint identification. <em>TPAMI</em>, <em>45</em>(11),
13134–13151. (<a
href="https://doi.org/10.1109/TPAMI.2023.3294357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Billions of contact-based fingerprint images have been acquired in large databases. Contactless 2D fingerprint identification systems have emerged to provide more hygienic and secured alternatives and are highly sought under the current pandemic. The success of such an alternative requires high match accuracy, not just for the contactless-to-contactless but also for the contactless-to-contact-based matching, which is currently below expectations for large-scale deployments. We introduce a new approach to advance such expectations on match accuracy and also to address privacy-related concerns, e.g., recent GDPR regulations, in the acquisition of very large databases. This paper introduces a novel approach for accurately synthesizing multi-view contactless 3D fingerprints to develop a very large-scale multi-view fingerprint database, and corresponding contact-based fingerprint database. A unique advantage of our approach is the simultaneous availability of much-needed ground truth labels and alleviation of laborious and often prone to erroneous tasks performed by human labeling. We also introduce a new framework that can not only accurately match contactless to contact-based images but also contactless to contactless images, as both of these capabilities are simultaneously required to advance contactless fingerprint technologies. Our rigorous experimental results presented in this paper, both for within-database and cross-database experiments, illustrate outperforming results to simultaneously meet both of these expectations and validate the effectiveness of the proposed approach.},
  archive      = {J_TPAMI},
  author       = {Chengdong Dong and Ajay Kumar},
  doi          = {10.1109/TPAMI.2023.3294357},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13134-13151},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Synthesis of multi-view 3D fingerprints to advance contactless fingerprint identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entity-graph enhanced cross-modal pretraining for
instance-level product retrieval. <em>TPAMI</em>, <em>45</em>(11),
13117–13133. (<a
href="https://doi.org/10.1109/TPAMI.2023.3291237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal in this research is to study a more realistic environment in which we can conduct weakly-supervised multi-modal instance-level product retrieval for fine-grained product categories. We first contribute the Product1M datasets and define two real practical instance-level retrieval tasks that enable evaluations on price comparison and personalized recommendations. For both instance-level tasks, accurately identifying the intended product target mentioned in visual-linguistic data and mitigating the impact of irrelevant content are quite challenging. To address this, we devise a more effective cross-modal pretraining model capable of adaptively incorporating key concept information from multi-modal data. This is accomplished by utilizing an entity graph, where nodes represented entities and edges denoted the similarity relations between them. Specifically, a novel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed for instance-level commodity retrieval, which explicitly injects entity knowledge in both node-based and subgraph-based ways into the multi-modal networks via a self-supervised hybrid-stream transformer. This could reduce the confusion between different object contents, thereby effectively guiding the network to focus on entities with real semantics. Experimental results sufficiently verify the efficacy and generalizability of our EGE-CMP, outperforming several SOTA cross-modal baselines like CLIP Radford et al. 2021, UNITER Chen et al. 2020 and CAPTURE Zhan et al. 2021.},
  archive      = {J_TPAMI},
  author       = {Xiao Dong and Xunlin Zhan and Yunchao Wei and Xiaoyong Wei and Yaowei Wang and Minlong Lu and Xiaochun Cao and Xiaodan Liang},
  doi          = {10.1109/TPAMI.2023.3291237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13117-13133},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Entity-graph enhanced cross-modal pretraining for instance-level product retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-based prior for generative saliency.
<em>TPAMI</em>, <em>45</em>(11), 13100–13116. (<a
href="https://doi.org/10.1109/TPAMI.2023.3290610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel generative saliency prediction framework that adopts an informative energy-based model as a prior distribution. The energy-based prior model is defined on the latent space of a saliency generator network that generates the saliency map based on a continuous latent variables and an observed image. Both the parameters of saliency generator and the energy-based prior are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. With the generative saliency model, we can obtain a pixel-wise uncertainty map from an image, indicating model confidence in the saliency prediction. Different from existing generative models, which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive in capturing the latent space of the data. With the informative energy-based prior, we extend the Gaussian distribution assumption of generative models to achieve a more representative distribution of the latent space, leading to more reliable uncertainty estimation. We apply the proposed frameworks to both RGB and RGB-D salient object detection tasks with both transformer and convolutional neural network backbones. We further propose an adversarial learning algorithm and a variational inference algorithm as alternatives to train the proposed generative framework. Experimental results show that our generative saliency model with an energy-based prior can achieve not only accurate saliency predictions but also reliable uncertainty maps that are consistent with human perception.},
  archive      = {J_TPAMI},
  author       = {Jing Zhang and Jianwen Xie and Nick Barnes and Ping Li},
  doi          = {10.1109/TPAMI.2023.3290610},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13100-13116},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An energy-based prior for generative saliency},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D visual saliency: An independent perceptual measure or a
derivative of 2D image saliency? <em>TPAMI</em>, <em>45</em>(11),
13083–13099. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While 3D visual saliency aims to predict regional importance of 3D surfaces in agreement with human visual perception and has been well researched in computer vision and graphics, latest work with eye-tracking experiments shows that state-of-the-art 3D visual saliency methods remain poor at predicting human fixations. Cues emerging prominently from these experiments suggest that 3D visual saliency might associate with 2D image saliency. This paper proposes a framework that combines a Generative Adversarial Network and a Conditional Random Field for learning visual saliency of both a single 3D object and a scene composed of multiple 3D objects with image saliency ground truth to 1) investigate whether 3D visual saliency is an independent perceptual measure or just a derivative of image saliency and 2) provide a weakly supervised method for more accurately predicting 3D visual saliency. Through extensive experiments, we not only demonstrate that our method significantly outperforms the state-of-the-art approaches, but also manage to answer the interesting and worthy question proposed within the title of this paper.},
  archive      = {J_TPAMI},
  author       = {Ran Song and Wei Zhang and Yitian Zhao and Yonghuai Liu and Paul L. Rosin},
  doi          = {10.1109/TPAMI.2023.3287356},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13083-13099},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D visual saliency: An independent perceptual measure or a derivative of 2D image saliency?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble multi-quantiles: Adaptively flexible distribution
prediction for uncertainty quantification. <em>TPAMI</em>,
<em>45</em>(11), 13068–13082. (<a
href="https://doi.org/10.1109/TPAMI.2023.3288028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel, succinct, and effective approach for distribution prediction to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction of $\mathbb {P}(\mathbf {y}|\mathbf {X}=x)$ in regression tasks. This conditional distribution&#39;s quantiles of probability levels spreading the interval (0,1) are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\mathbb {P}(\mathbf {y}|\mathbf {X}=x)$ , while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On extensive regression tasks from UCI datasets, we show that EMQ achieves state-of-the-art performance comparing to many recent uncertainty quantification methods. Visualization results further illustrate the necessity and the merits of such an ensemble model.},
  archive      = {J_TPAMI},
  author       = {Xing Yan and Yonghua Su and Wenxuan Ma},
  doi          = {10.1109/TPAMI.2023.3288028},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13068-13082},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ensemble multi-quantiles: Adaptively flexible distribution prediction for uncertainty quantification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpolated joint space adversarial training for robust and
generalizable defenses. <em>TPAMI</em>, <em>45</em>(11), 13054–13067.
(<a href="https://doi.org/10.1109/TPAMI.2023.3286772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) is considered to be one of the most reliable defenses against adversarial attacks. However, models trained with AT sacrifice standard accuracy and do not generalize well to unseen attacks. Recent works show generalization improvement with adversarial samples under unseen threat models such as on-manifold threat model or neural perceptual threat model. However, the former requires exact manifold information while the latter requires algorithm relaxation. Motivated by these considerations, we propose a novel threat model called Joint Space Threat Model (JSTM), which exploits the underlying manifold information with Normalizing Flow, ensuring that the exact manifold assumption holds. Under JSTM, we develop novel adversarial attacks and defenses. Specifically, we propose the Robust Mixup strategy in which we maximize the adversity of the interpolated images and gain robustness and prevent overfitting. Our experiments show that Interpolated Joint Space Adversarial Training (IJSAT) achieves good performance in standard accuracy, robustness, and generalization. IJSAT is also flexible and can be used as a data augmentation method to improve standard accuracy and combined with many existing AT approaches to improve robustness. We demonstrate the effectiveness of our approach on three benchmark datasets, CIFAR-10/100, OM-ImageNet and CIFAR-10-C.},
  archive      = {J_TPAMI},
  author       = {Chun Pong Lau and Jiang Liu and Hossein Souri and Wei-An Lin and Soheil Feizi and Rama Chellappa},
  doi          = {10.1109/TPAMI.2023.3286772},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13054-13067},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpolated joint space adversarial training for robust and generalizable defenses},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hong kong world: Leveraging structural regularity for
line-based SLAM. <em>TPAMI</em>, <em>45</em>(11), 13035–13053. (<a
href="https://doi.org/10.1109/TPAMI.2023.3276204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manhattan and Atlanta worlds hold for the structured scenes with only vertical and horizontal dominant directions (DDs). To describe the scenes with additional sloping DDs, a mixture of independent Manhattan worlds seems plausible, but may lead to unaligned and unrelated DDs. By contrast, we propose a novel structural model called Hong Kong world. It is more general than Manhattan and Atlanta worlds since it can represent the environments with slopes, e.g., a city with hilly terrain, a house with sloping roof, and a loft apartment with staircase. Moreover, it is more compact and accurate than a mixture of independent Manhattan worlds by enforcing the orthogonality constraints between not only vertical and horizontal DDs, but also horizontal and sloping DDs. We further leverage the structural regularity of Hong Kong world for the line-based SLAM. Our SLAM method is reliable thanks to three technical novelties. First, we estimate DDs/vanishing points in Hong Kong world in a semi-searching way. We use a new consensus voting strategy for search, instead of traditional branch and bound. This method is the first one that can simultaneously determine the number of DDs, and achieve quasi-global optimality in terms of the number of inliers. Second, we compute the camera pose by exploiting the spatial relations between DDs in Hong Kong world. This method generates concise polynomials, and thus is more accurate and efficient than existing approaches designed for unstructured scenes. Third, we refine the estimated DDs in Hong Kong world by a novel filter-based method. Then we use these refined DDs to optimize the camera poses and 3D lines, leading to higher accuracy and robustness than existing optimization algorithms. In addition, we establish the first dataset of sequential images in Hong Kong world. Experiments showed that our approach outperforms state-of-the-art methods in terms of accuracy and/or efficiency.},
  archive      = {J_TPAMI},
  author       = {Haoang Li and Ji Zhao and Jean-Charles Bazin and Pyojin Kim and Kyungdon Joo and Zhenjun Zhao and Yun-Hui Liu},
  doi          = {10.1109/TPAMI.2023.3276204},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13035-13053},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hong kong world: Leveraging structural regularity for line-based SLAM},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeeperGCN: Training deeper GCNs with generalized aggregation
functions. <em>TPAMI</em>, <em>45</em>(11), 13024–13034. (<a
href="https://doi.org/10.1109/TPAMI.2023.3306930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been drawing significant attention to representation learning on graphs. Recent works developed frameworks to train very deep GNNs and showed impressive results in tasks like point cloud learning and protein interaction prediction. In this work, we study the performance of such deep models in large-scale graphs. In particular, we look at the effect of adequately choosing an aggregation function on deep models. We find that GNNs are very sensitive to the choice of aggregation functions (e.g. mean , max , and sum ) when applied to different datasets. We systematically study and propose to alleviate this issue by introducing a novel class of aggregation functions named Generalized Aggregation Functions . The proposed functions extend beyond commonly used aggregation functions to a wide range of new permutation-invariant functions. Generalized Aggregation Functions are fully differentiable, where their parameters can be learned in an end-to-end fashion to yield a suitable aggregation function for each task. We show that equipped with the proposed aggregation functions, deep residual GNNs outperform state-of-the-art in several benchmarks from Open Graph Benchmark (OGB) across tasks and domains.},
  archive      = {J_TPAMI},
  author       = {Guohao Li and Chenxin Xiong and Guocheng Qian and Ali Thabet and Bernard Ghanem},
  doi          = {10.1109/TPAMI.2023.3306930},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13024-13034},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeeperGCN: Training deeper GCNs with generalized aggregation functions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What makes for good tokenizers in vision transformer?
<em>TPAMI</em>, <em>45</em>(11), 13011–13023. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architecture of transformers, which recently witness booming applications in vision tasks, has pivoted against the widespread convolutional paradigm. Relying on the tokenization process that splits inputs into multiple tokens, transformers are capable of extracting their pairwise relationships using self-attention. While being the stemming building block of transformers, what makes for a good tokenizer has not been well understood in computer vision. In this work, we investigate this uncharted problem from an information trade-off perspective. In addition to unifying and understanding existing structural modifications, our derivation leads to better design strategies for vision tokenizers. The proposed Modulation across Tokens (MoTo) incorporates inter-token modeling capability through normalization. Furthermore, a regularization objective TokenProp is embraced in the standard training regime. Through extensive experiments on various transformer architectures, we observe both improved performance and intriguing properties of these two plug-and-play designs with negligible computational overhead. These observations further indicate the importance of the commonly-omitted designs of tokenizers in vision transformer.},
  archive      = {J_TPAMI},
  author       = {Shengju Qian and Yi Zhu and Wenbo Li and Mu Li and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3231442},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {13011-13023},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {What makes for good tokenizers in vision transformer?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deconfounded image captioning: A causal retrospect.
<em>TPAMI</em>, <em>45</em>(11), 12996–13010. (<a
href="https://doi.org/10.1109/TPAMI.2021.3121705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset bias in vision-language tasks is becoming one of the main problems which hinders the progress of our community. Existing solutions lack a principled analysis about why modern image captioners easily collapse into dataset bias. In this paper, we present a novel perspective: Deconfounded Image Captioning (DIC), to find out the answer of this question, then retrospect modern neural image captioners, and finally propose a DIC framework: DICv1.0 to alleviate the negative effects brought by dataset bias. DIC is based on causal inference, whose two principles: the backdoor and front-door adjustments, help us review previous studies and design new effective models. In particular, we showcase that DICv1.0 can strengthen two prevailing captioning models and can achieve a single-model 131.1 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split and online split of the challenging MS COCO dataset, respectively. Interestingly, DICv1.0 is a natural derivation from our causal retrospect, which opens promising directions for image captioning.},
  archive      = {J_TPAMI},
  author       = {Xu Yang and Hanwang Zhang and Jianfei Cai},
  doi          = {10.1109/TPAMI.2021.3121705},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12996-13010},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deconfounded image captioning: A causal retrospect},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image de-raining transformer. <em>TPAMI</em>,
<em>45</em>(11), 12978–12995. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning based de-raining approaches have resorted to the convolutional architectures. However, the intrinsic limitations of convolution, including local receptive fields and independence of input content, hinder the model&#39;s ability to capture long-range and complicated rainy artifacts. To overcome these limitations, we propose an effective and efficient transformer-based architecture for the image de-raining. First, we introduce general priors of vision tasks, i.e., locality and hierarchy, into the network architecture so that our model can achieve excellent de-raining performance without costly pre-training. Second, since the geometric appearance of rainy artifacts is complicated and of significant variance in space, it is essential for de-raining models to extract both local and non-local features. Therefore, we design the complementary window-based transformer and spatial transformer to enhance locality while capturing long-range dependencies. Besides, to compensate for the positional blindness of self-attention, we establish a separate representative space for modeling positional relationship, and design a new relative position enhanced multi-head self-attention. In this way, our model enjoys powerful abilities to capture dependencies from both content and position, so as to achieve better image content recovery while removing rainy artifacts. Experiments substantiate that our approach attains more appealing results than state-of-the-art methods quantitatively and qualitatively.},
  archive      = {J_TPAMI},
  author       = {Jie Xiao and Xueyang Fu and Aiping Liu and Feng Wu and Zheng-Jun Zha},
  doi          = {10.1109/TPAMI.2022.3183612},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12978-12995},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image de-raining transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer for image harmonization and beyond.
<em>TPAMI</em>, <em>45</em>(11), 12960–12977. (<a
href="https://doi.org/10.1109/TPAMI.2022.3207091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image harmonization, aiming to make composite images look more realistic, is an important and challenging task. The composite, synthesized by combining foreground from one image with background from another image, inevitably suffers from the issue of inharmonious appearance caused by distinct imaging conditions, i.e., lights. Current solutions mainly adopt an encoder-decoder architecture with convolutional neural network (CNN) to capture the context of composite images, trying to understand what it should look like in the foreground referring to surrounding background. In this work, we seek to solve image harmonization with Transformer, by leveraging its powerful ability of modeling long-range context dependencies, for adjusting foreground light to make it compatible with background light while keeping structure and semantics unchanged. We present the design of our two vision Transformer frameworks and corresponding methods, as well as comprehensive experiments and empirical study, demonstrating the power of Transformer and investigating the Transformer for vision. Our methods achieve state-of-the-art performance on the image harmonization as well as four additional vision and graphics tasks, i.e., image enhancement, image inpainting, white-balance editing, and portrait relighting, indicating the superiority of our work. Code, models, more results and details can be found at the project website http://ouc.ai/project/HarmonyTransformer .},
  archive      = {J_TPAMI},
  author       = {Zonghui Guo and Zhaorui Gu and Bing Zheng and Junyu Dong and Haiyong Zheng},
  doi          = {10.1109/TPAMI.2022.3207091},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12960-12977},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transformer for image harmonization and beyond},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised video-centralised transformer for video face
clustering. <em>TPAMI</em>, <em>45</em>(11), 12944–12959. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel method for face clustering in videos using a video-centralised transformer. Previous works often employed contrastive learning to learn frame-level representation and used average pooling to aggregate the features along the temporal dimension. This approach may not fully capture the complicated video dynamics. In addition, despite the recent progress in video-based contrastive learning, few have attempted to learn a self-supervised clustering-friendly face representation that benefits the video face clustering task. To overcome these limitations, our method employs a transformer to directly learn video-level representations that can better reflect the temporally-varying property of faces in videos, while we also propose a video-centralised self-supervised framework to train the transformer model. We also investigate face clustering in egocentric videos, a fast-emerging field that has not been studied yet in works related to face clustering. To this end, we present and release the first large-scale egocentric video face clustering dataset named EasyCom-Clustering. We evaluate our proposed method on both the widely used Big Bang Theory (BBT) dataset and the new EasyCom-Clustering dataset. Results show the performance of our video-centralised transformer has surpassed all previous state-of-the-art methods on both benchmarks, exhibiting a self-attentive understanding of face videos.},
  archive      = {J_TPAMI},
  author       = {Yujiang Wang and Mingzhi Dong and Jie Shen and Yiming Luo and Yiming Lin and Pingchuan Ma and Stavros Petridis and Maja Pantic},
  doi          = {10.1109/TPAMI.2023.3243812},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12944-12959},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised video-centralised transformer for video face clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video transformers: A survey. <em>TPAMI</em>,
<em>45</em>(11), 12922–12943. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models have shown great success handling long-range interactions, making them a promising tool for modeling video. However, they lack inductive biases and scale quadratically with input length. These limitations are further exacerbated when dealing with the high dimensionality introduced by the temporal dimension. While there are surveys analyzing the advances of Transformers for vision, none focus on an in-depth analysis of video-specific designs. In this survey, we analyze the main contributions and trends of works leveraging Transformers to model video. Specifically, we delve into how videos are handled at the input level first. Then, we study the architectural changes made to deal with video more efficiently, reduce redundancy, re-introduce useful inductive biases, and capture long-term temporal dynamics. In addition, we provide an overview of different training regimes and explore effective self-supervised learning strategies for video. Finally, we conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D ConvNets even with less computational complexity.},
  archive      = {J_TPAMI},
  author       = {Javier Selva and Anders S. Johansen and Sergio Escalera and Kamal Nasrollahi and Thomas B. Moeslund and Albert Clapés},
  doi          = {10.1109/TPAMI.2023.3243465},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12922-12943},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video transformers: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-to-character-to-word transformers for accurate scene
text recognition. <em>TPAMI</em>, <em>45</em>(11), 12908–12921. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging the advances of natural language processing, most recent scene text recognizers adopt an encoder-decoder architecture where text images are first converted to representative features and then a sequence of characters via ‘sequential decoding’. However, scene text images suffer from rich noises of different sources such as complex background and geometric distortions which often confuse the decoder and lead to incorrect alignment of visual features at noisy decoding time steps . This paper presents I2C2W, a novel scene text recognition technique that is tolerant to geometric and photometric degradation by decomposing scene text recognition into two inter-connected tasks. The first task focuses on image-to-character (I2C) mapping which detects a set of character candidates from images based on different alignments of visual features in an non-sequential way . The second task tackles character-to-word (C2W) mapping which recognizes scene text by decoding words from the detected character candidates. The direct learning from character semantics (instead of noisy image features) corrects falsely detected character candidates effectively which improves the final text recognition accuracy greatly. Extensive experiments over nine public datasets show that the proposed I2C2W outperforms the state-of-the-art by large margins for challenging scene text datasets with various curvature and perspective distortions. It also achieves very competitive recognition performance over multiple normal scene text datasets.},
  archive      = {J_TPAMI},
  author       = {Chuhui Xue and Jiaxing Huang and Wenqing Zhang and Shijian Lu and Changhu Wang and Song Bai},
  doi          = {10.1109/TPAMI.2022.3230962},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12908-12921},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image-to-character-to-word transformers for accurate scene text recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SignNet II: A transformer-based two-way sign language
translation model. <em>TPAMI</em>, <em>45</em>(11), 12896–12907. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of a sign interpreting agent is to bridge the communication gap between the hearing-only and Deaf or Hard of Hearing communities by translating both from sign language to text and from text to sign language. Until now, much of the AI work in automated sign language processing has focused primarily on sign language to text translation, which puts the advantage mainly on the side of hearing individuals. In this article, we describe advances in sign language processing based on transformer networks. Specifically, we introduce SignNet II, a sign language processing architecture, a promising step towards facilitating two-way sign language communication. It is comprised of sign-to-text and text-to-sign networks jointly trained using a dual learning mechanism. Furthermore, by exploiting the notion of sign similarity, a metric embedding learning process is introduced to enhance the text-to-sign translation performance. Using a bank of multi-feature transformers, we analyzed several input feature representations and discovered that keypoint-based pose features consistently performed well, irrespective of the quality of the input videos. We demonstrated that the two jointly trained networks outperformed their singly-trained counterparts, showing noteworthy enhancements in BLEU-1 - BLEU-4 scores when tested on the largest available German Sign Language (GSL) benchmark dataset.},
  archive      = {J_TPAMI},
  author       = {Lipisha Chaudhary and Tejaswini Ananthanarayana and Enjamamul Hoq and Ifeoma Nwogu},
  doi          = {10.1109/TPAMI.2022.3232389},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12896-12907},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SignNet II: A transformer-based two-way sign language translation model},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransFuser: Imitation with transformer-based sensor fusion
for autonomous driving. <em>TPAMI</em>, <em>45</em>(11), 12878–12895.
(<a href="https://doi.org/10.1109/TPAMI.2022.3200245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g., object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird&#39;s eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48\%.},
  archive      = {J_TPAMI},
  author       = {Kashyap Chitta and Aditya Prakash and Bernhard Jaeger and Zehao Yu and Katrin Renz and Andreas Geiger},
  doi          = {10.1109/TPAMI.2022.3200245},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12878-12895},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransFuser: Imitation with transformer-based sensor fusion for autonomous driving},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StARformer: Transformer with state-action-reward
representations for robot learning. <em>TPAMI</em>, <em>45</em>(11),
12862–12877. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) can be considered as a sequence modeling task, where an agent employs a sequence of past state-action-reward experiences to predict a sequence of future actions. In this work, we propose St ate- A ction- R eward Transformer ( StAR former), a Transformer architecture for robot learning with image inputs, which explicitly models short-term state-action-reward representations (StAR-representations), essentially introducing a Markovian-like inductive bias to improve long-term modeling. StARformer first extracts StAR-representations using self-attending patches of image states, action, and reward tokens within a short temporal window. These StAR-representations are combined with pure image state representations, extracted as convolutional features, to perform self-attention over the whole sequence. Our experimental results show that StARformer outperforms the state-of-the-art Transformer-based method on image-based Atari and DeepMind Control Suite benchmarks, under both offline-RL and imitation learning settings. We find that models can benefit from our combination of patch-wise and convolutional image embeddings. StARformer is also more compliant with longer sequences of inputs than the baseline method. Finally, we demonstrate how StARformer can be successfully applied to a real-world robot imitation learning setting via a human-following task.},
  archive      = {J_TPAMI},
  author       = {Jinghuan Shang and Xiang Li and Kumara Kahatapitiya and Yu-Cheol Lee and Michael S. Ryoo},
  doi          = {10.1109/TPAMI.2022.3204708},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12862-12877},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StARformer: Transformer with state-action-reward representations for robot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransZero++: Cross attribute-guided transformer for
zero-shot learning. <em>TPAMI</em>, <em>45</em>(11), 12844–12861. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) tackles the novel class recognition problem by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is typically represented by attribute descriptions shared between different classes, which act as strong priors for localizing object attributes that represent discriminative region features, enabling significant and sufficient visual-semantic interaction for advancing ZSL. Existing attention-based models have struggled to learn inferior region features in a single image by solely using unidirectional attention, which ignore the transferable and discriminative attribute localization of visual features for representing the key semantic knowledge for effective knowledge transfer in ZSL. In this paper, we propose a cross attribute-guided Transformer network, termed TransZero++, to refine visual features and learn accurate attribute localization for key semantic knowledge representations in ZSL. Specifically, TransZero++ employs an attribute $\rightarrow$ visual Transformer sub-net (AVT) and a visual $\rightarrow$ attribute Transformer sub-net (VAT) to learn attribute-based visual features and visual-based attribute features, respectively. By further introducing feature-level and prediction-level semantical collaborative losses, the two attribute-guided transformers teach each other to learn semantic-augmented visual embeddings for key semantic knowledge representations via semantical collaborative learning. Finally, the semantic-augmented visual embeddings learned by AVT and VAT are fused to conduct desirable visual-semantic interaction cooperated with class semantic vectors for ZSL classification. Extensive experiments show that TransZero++ achieves the new state-of-the-art results on three golden ZSL benchmarks and on the large-scale ImageNet dataset. The project website is available at: https://shiming-chen.github.io/TransZero-pp/TransZero-pp.html .},
  archive      = {J_TPAMI},
  author       = {Shiming Chen and Ziming Hong and Wenjin Hou and Guo-Sen Xie and Yibing Song and Jian Zhao and Xinge You and Shuicheng Yan and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3229526},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12844-12861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransZero++: Cross attribute-guided transformer for zero-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-DETR: Image-level few-shot detection with inter-class
correlation exploitation. <em>TPAMI</em>, <em>45</em>(11), 12832–12843.
(<a href="https://doi.org/10.1109/TPAMI.2022.3195735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection has been extensively investigated by incorporating meta-learning into region-based detection frameworks. Despite its success, the said paradigm is still constrained by several factors, such as (i) low-quality region proposals for novel classes and (ii) negligence of the inter-class correlation among different classes. Such limitations hinder the generalization of base-class knowledge for the detection of novel-class objects. In this work, we design Meta-DETR, which (i) is the first image-level few-shot detector, and (ii) introduces a novel inter-class correlational meta-learning strategy to capture and leverage the correlation among different classes for robust and accurate few-shot object detection. Meta-DETR works entirely at image level without any region proposals, which circumvents the constraint of inaccurate proposals in prevalent few-shot detection frameworks. In addition, the introduced correlational meta-learning enables Meta-DETR to simultaneously attend to multiple support classes within a single feedforward, which allows to capture the inter-class correlation among different classes, thus significantly reducing the misclassification over similar classes and enhancing knowledge generalization to novel classes. Experiments over multiple few-shot object detection benchmarks show that the proposed Meta-DETR outperforms state-of-the-art methods by large margins. The implementation codes are publicly available at https://github.com/ZhangGongjie/Meta-DETR .},
  archive      = {J_TPAMI},
  author       = {Gongjie Zhang and Zhipeng Luo and Kaiwen Cui and Shijian Lu and Eric P. Xing},
  doi          = {10.1109/TPAMI.2022.3195735},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12832-12843},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Meta-DETR: Image-level few-shot detection with inter-class correlation exploitation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot class-incremental learning by sampling multi-phase
tasks. <em>TPAMI</em>, <em>45</em>(11), 12816–12831. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New classes arise frequently in our ever-changing world, e.g., emerging topics in social media and new types of products in e-commerce. A model should recognize new classes and meanwhile maintain discriminability over old classes. Under severe circumstances, only limited novel instances are available to incrementally update the model. The task of recognizing few-shot new classes without forgetting old classes is called few-shot class-incremental learning (FSCIL). In this work, we propose a new paradigm for FSCIL based on meta-learning by LearnIng Multi-phase Incremental Tasks ( Limit ), which synthesizes fake FSCIL tasks from the base dataset. The data format of fake tasks is consistent with the ‘real’ incremental tasks, and we can build a generalizable feature space for the unseen tasks through meta-learning. Besides, Limit also constructs a calibration module based on transformer, which calibrates the old class classifiers and new class prototypes into the same scale and fills in the semantic gap. The calibration module also adaptively contextualizes the instance-specific embedding with a set-to-set function. Limit efficiently adapts to new classes and meanwhile resists forgetting over old classes. Experiments on three benchmark datasets (CIFAR100, mini ImageNet, and CUB200) and large-scale dataset, i.e., ImageNet ILSVRC2012 validate that Limit achieves state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Da-Wei Zhou and Han-Jia Ye and Liang Ma and Di Xie and Shiliang Pu and De-Chuan Zhan},
  doi          = {10.1109/TPAMI.2022.3200865},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12816-12831},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Few-shot class-incremental learning by sampling multi-phase tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PoseBERT: A generic transformer module for temporal 3D human
modeling. <em>TPAMI</em>, <em>45</em>(11), 12798–12815. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training state-of-the-art models for human pose estimation in videos requires datasets with annotations that are really hard and expensive to obtain. Although transformers have been recently utilized for body pose sequence modeling, related methods rely on pseudo-ground truth to augment the currently limited training data available for learning such models. In this paper, we introduce PoseBERT, a transformer module that is fully trained on 3D Motion Capture (MoCap) data via masked modeling. It is simple, generic and versatile, as it can be plugged on top of any image-based model to transform it in a video-based model leveraging temporal information. We showcase variants of PoseBERT with different inputs varying from 3D skeleton keypoints to rotations of a 3D parametric model for either the full body (SMPL) or just the hands (MANO). Since PoseBERT training is task agnostic, the model can be applied to several tasks such as pose refinement, future pose prediction or motion completion without finetuning . Our experimental results validate that adding PoseBERT on top of various state-of-the-art pose estimation methods consistently improves their performances, while its low computational cost allows us to use it in a real-time demo for smoothly animating a robotic hand via a webcam. Test code and models are available at https://github.com/naver/posebert .},
  archive      = {J_TPAMI},
  author       = {Fabien Baradel and Romain Brégier and Thibault Groueix and Philippe Weinzaepfel and Yannis Kalantidis and Grégory Rogez},
  doi          = {10.1109/TPAMI.2022.3216899},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12798-12815},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PoseBERT: A generic transformer module for temporal 3D human modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Looking beyond two frames: End-to-end multi-object tracking
using spatial and temporal transformers. <em>TPAMI</em>,
<em>45</em>(11), 12783–12797. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking a time-varying indefinite number of objects in a video sequence over time remains a challenge despite recent advances in the field. Most existing approaches are not able to properly handle multi-object tracking challenges such as occlusion, in part because they ignore long-term temporal information. To address these shortcomings, we present MO3TR: a truly end-to-end Transformer-based online multi-object tracking (MOT) framework that learns to handle occlusions, track initiation and termination without the need for an explicit data association module or any heuristics. MO3TR encodes object interactions into long-term temporal embeddings using a combination of spatial and temporal Transformers, and recursively uses the information jointly with the input data to estimate the states of all tracked objects over time. The spatial attention mechanism enables our framework to learn implicit representations between all the objects and the objects to the measurements, while the temporal attention mechanism focuses on specific parts of past information, allowing our approach to resolve occlusions over multiple frames. Our experiments demonstrate the potential of this new approach, achieving results on par with or better than the current state-of-the-art on multiple MOT metrics for several popular multi-object tracking benchmarks.},
  archive      = {J_TPAMI},
  author       = {Tianyu Zhu and Markus Hiller and Mahsa Ehsanpour and Rongkai Ma and Tom Drummond and Ian Reid and Hamid Rezatofighi},
  doi          = {10.1109/TPAMI.2022.3213073},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12783-12797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Looking beyond two frames: End-to-end multi-object tracking using spatial and temporal transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised pre-training for detection transformers.
<em>TPAMI</em>, <em>45</em>(11), 12772–12782. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DEtection TRansformer (DETR) for object detection reaches competitive performance compared with Faster R-CNN via a transformer encoder-decoder architecture. However, trained with scratch transformers, DETR needs large-scale training data and an extreme long training schedule even on COCO dataset. Inspired by the great success of pre-training transformers in natural language processing, we propose a novel pretext task named random query patch detection in Unsupervised Pre-training DETR (UP-DETR). Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the input image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we find that freezing the CNN backbone is the prerequisite for the success of pre-training transformers. (2) To perform multi-query localization, we develop UP-DETR with multi-query patch detection with attention mask. Besides, UP-DETR also provides a unified perspective for fine-tuning object detection and one-shot detection tasks. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr .},
  archive      = {J_TPAMI},
  author       = {Zhigang Dai and Bolun Cai and Yugeng Lin and Junying Chen},
  doi          = {10.1109/TPAMI.2022.3216514},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12772-12782},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised pre-training for detection transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). P2T: Pyramid pooling transformer for scene understanding.
<em>TPAMI</em>, <em>45</em>(11), 12760–12771. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. One of the most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational cost (quadratic complexity). A popular solution to this problem is to use a single pooling operation to reduce the sequence length. This paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation seems less powerful. To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction. However, pyramid pooling has not been explored in backbone network design. To bridge this gap, we propose to adapt pyramid pooling to Multi-Head Self-Attention (MHSA) in the vision transformer, simultaneously reducing the sequence length and capturing powerful contextual features. Plugged with our pooling-based MHSA, we build a universal vision transformer backbone, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various vision tasks such as image classification, semantic segmentation, object detection, and instance segmentation, compared to previous CNN- and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T .},
  archive      = {J_TPAMI},
  author       = {Yu-Huan Wu and Yun Liu and Xin Zhan and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2022.3202765},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12760-12771},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {P2T: Pyramid pooling transformer for scene understanding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic unary convolution in transformers. <em>TPAMI</em>,
<em>45</em>(11), 12747–12759. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is uncertain whether the power of transformer architectures can complement existing convolutional neural networks. A few recent attempts have combined convolution with transformer design through a range of structures in series, where the main contribution of this paper is to explore a parallel design approach. While previous transformed-based approaches need to segment the image into patch-wise tokens, we observe that the multi-head self-attention conducted on convolutional features is mainly sensitive to global correlations and that the performance degrades when these correlations are not exhibited. We propose two parallel modules along with multi-head self-attention to enhance the transformer. For local information, a dynamic local enhancement module leverages convolution to dynamically and explicitly enhance positive local patches and suppress the response to less informative ones. For mid-level structure, a novel unary co-occurrence excitation module utilizes convolution to actively search the local co-occurrence between patches. The parallel-designed Dynamic Unary Convolution in Transformer (DUCT) blocks are aggregated into a deep architecture, which is comprehensively evaluated across essential computer vision tasks in image-based classification, segmentation, retrieval and density estimation. Both qualitative and quantitative results show our parallel convolutional-transformer approach with dynamic and unary convolution outperforms existing series-designed structures.},
  archive      = {J_TPAMI},
  author       = {Haoran Duan and Yang Long and Shidong Wang and Haofeng Zhang and Chris G. Willcocks and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3233482},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12747-12759},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic unary convolution in transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Token selection is a simple booster for vision transformers.
<em>TPAMI</em>, <em>45</em>(11), 12738–12746. (<a
href="https://doi.org/10.1109/TPAMI.2022.3208922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers have recently attained state-of-the-art results in visual recognition tasks. Their success is largely attributed to the self-attention component, which models the global dependencies among the image patches (tokens) and aggregates them into higher-level features. However, self-attention brings significant training difficulties to ViTs. Many recent works thus develop various new self-attention components to alleviate this issue. In this article, instead of developing complicated self-attention mechanism, we aim to explore simple approaches to fully release the potential of the vanilla self-attention. We first study the token selection behavior of self-attention and find that it suffers from a low diversity due to attention over-smoothing, which severely limits its effectiveness in learning discriminative token features. We then develop simple approaches to enhance selectivity and diversity for self-attention in token selection. The resulted token selector module can server as a drop-in module for various ViT backbones and consistently boost their performance. Significantly, they enable ViTs to achieve 84.6\% top-1 classification accuracy on ImageNet with only 25M parameters. When scaled up to 81M parameters, the result can be further improved to 86.1\%. In addition, we also present comprehensive experiments to demonstrate the token selector can be applied to a variety of transformer-based models to boost their performance for image classification, semantic segmentation and NLP tasks. Code is available at https://github.com/zhoudaquan/dvit_repo .},
  archive      = {J_TPAMI},
  author       = {Daquan Zhou and Qibin Hou and Linjie Yang and Xiaojie Jin and Jiashi Feng},
  doi          = {10.1109/TPAMI.2022.3208922},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12738-12746},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Token selection is a simple booster for vision transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear complexity self-attention with <span
class="math inline">3rd</span><!-- -->3 rd order polynomials.
<em>TPAMI</em>, <em>45</em>(11), 12726–12737. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention mechanisms and non-local blocks have become crucial building blocks for state-of-the-art neural architectures thanks to their unparalleled ability in capturing long-range dependencies in the input. However their cost is quadratic with the number of spatial positions hence making their use impractical in many real case applications. In this work, we analyze these methods through a polynomial lens, and we show that self-attention can be seen as a special case of a 3 rd order polynomial. Within this polynomial framework, we are able to design polynomial operators capable of accessing the same data pattern of non-local and self-attention blocks while reducing the complexity from quadratic to linear. As a result, we propose two modules (Poly-NL and Poly-SA) that can be used as ”drop-in” replacements for more-complex non-local and self-attention layers in state-of-the-art CNNs and ViT architectures. Our modules can achieve comparable, if not better, performance across a wide range of computer vision tasks while keeping a complexity equivalent to a standard linear layer.},
  archive      = {J_TPAMI},
  author       = {Francesca Babiloni and Ioannis Marras and Jiankang Deng and Filippos Kokkinos and Matteo Maggioni and Grigorios Chrysos and Philip Torr and Stefanos Zafeiriou},
  doi          = {10.1109/TPAMI.2022.3231971},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12726-12737},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Linear complexity self-attention with $3{\mathrm{rd}}$3 rd order polynomials},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial introduction to the special section on
transformer models in vision. <em>TPAMI</em>, <em>45</em>(11),
12721–12725. (<a
href="https://doi.org/10.1109/TPAMI.2023.3306164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models have achieved outstanding results on a variety of language tasks, such as text classification, ma- chine translation, and question answering. This success in the field of Natural Language Processing (NLP) has sparked interest in the computer vision community to apply these models to vision and multi-modal learning tasks. However, visual data has a unique structure, requiring the need to rethink network designs and training methods. As a result, Transformer models and their variations have been suc- cessfully used for image recognition, object detection, seg- mentation, image super-resolution, video understanding, image generation, text-image synthesis, and visual question answering, among other applications.},
  archive      = {J_TPAMI},
  author       = {Salman Khan and Fahad Shahbaz Khan and Ashish Vaswani and Niki Parmar and Ming-Hsuan Yang and Mubarak Shah},
  doi          = {10.1109/TPAMI.2023.3306164},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {12721-12725},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guest editorial introduction to the special section on transformer models in vision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What makes for hierarchical vision transformer?
<em>TPAMI</em>, <em>45</em>(10), 12714–12720. (<a
href="https://doi.org/10.1109/TPAMI.2023.3282019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies indicate that hierarchical Vision Transformer (ViT) with a macro architecture of interleaved non-overlapped window-based self-attention &amp; shifted-window operation can achieve state-of-the-art performance in various visual recognition tasks, and challenges the ubiquitous convolutional neural networks (CNNs) using densely slid kernels. In most recently proposed hierarchical ViTs, self-attention is the de-facto standard for spatial information aggregation. In this paper, we question whether self-attention is the only choice for hierarchical ViT to attain strong performance, and study the effects of different kinds of cross-window communication methods. To this end, we replace self-attention layers with embarrassingly simple linear mapping layers, and the resulting proof-of-concept architecture termed TransLinear can achieve very strong performance in ImageNet- $\text{1}\,k$ image recognition. Moreover, we find that TransLinear is able to leverage the ImageNet pre-trained weights and demonstrates competitive transfer learning properties on downstream dense prediction tasks such as object detection and instance segmentation. We also experiment with other alternatives to self-attention for content aggregation inside each non-overlapped window under different cross-window communication approaches. Our results reveal that the macro architecture , other than specific aggregation layers or cross-window communication mechanisms, is more responsible for hierarchical ViT&#39;s strong performance and is the real challenger to the ubiquitous CNN&#39;s dense sliding window paradigm.},
  archive      = {J_TPAMI},
  author       = {Yuxin Fang and Xinggang Wang and Rui Wu and Wenyu Liu},
  doi          = {10.1109/TPAMI.2023.3282019},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12714-12720},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {What makes for hierarchical vision transformer?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rainbow UDA: Combining domain adaptive models for semantic
segmentation tasks. <em>TPAMI</em>, <em>45</em>(10), 12707–12713. (<a
href="https://doi.org/10.1109/TPAMI.2023.3289308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Rainbow UDA, a framework designed to address the drawbacks of the previous ensemble-distillation frameworks when combining multiple unsupervised domain adaptation (UDA) models for semantic segmentation tasks. Such drawbacks are mainly caused by overlooking the magnitudes of the output certainties of different members in an ensemble as well as their individual performance in the target domain, causing the distillation process to suffer from certainty inconsistency and performance variation issues. These issues may hinder the effectiveness of an ensemble that includes members with either biased certainty distributions or have poor performance in the target domain. To mitigate such a deficiency, Rainbow UDA introduces two operations: the unification and the channel-wise fusion operations, to address the above two issues. In order to validate the designs of Rainbow UDA, we leverage the GTA5 $\to$ Cityscapes and SYNTHIA $\to$ Cityscapes benchmarks to examine the effectiveness of the two operations, and compare Rainbow UDA against a wide variety of baseline approaches. We also provide a set of analyses to show that Rainbow UDA is effective, robust, and can evolve with time as the ensemble grows.},
  archive      = {J_TPAMI},
  author       = {Chen-Hao Chao and Bo-Wun Cheng and Tzu-Wen Wang and Huang-Ru Liao and Chun-Yi Lee},
  doi          = {10.1109/TPAMI.2023.3289308},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12707-12713},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rainbow UDA: Combining domain adaptive models for semantic segmentation tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnable distribution calibration for few-shot
class-incremental learning. <em>TPAMI</em>, <em>45</em>(10),
12699–12706. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class-incremental learning (FSCIL) faces the challenges of memorizing old class distributions and estimating new class distributions given few training samples. In this study, we propose a learnable distribution calibration (LDC) approach, to systematically solve these two challenges using a unified framework. LDC is built upon a parameterized calibration unit (PCU), which initializes biased distributions for all classes based on classifier vectors (memory-free) and a single covariance matrix. The covariance matrix is shared by all classes, so that the memory costs are fixed. During base training, PCU is endowed with the ability to calibrate biased distributions by recurrently updating sampled features under supervision of real distributions. During incremental learning, PCU recovers distributions for old classes to avoid ‘forgetting’, as well as estimating distributions and augmenting samples for new classes to alleviate ‘over-fitting’ caused by the biased distributions of few-shot samples. LDC is theoretically plausible by formatting a variational inference procedure. It improves FSCIL&#39;s flexibility as the training procedure requires no class similarity priori. Experiments on CUB200, CIFAR100, and mini-ImageNet datasets show that LDC respectively outperforms the state-of-the-arts by 4.64\%, 1.98\%, and 3.97\%. LDC&#39;s effectiveness is also validated on few-shot learning scenarios.},
  archive      = {J_TPAMI},
  author       = {Binghao Liu and Boyu Yang and Lingxi Xie and Ren Wang and Qi Tian and Qixiang Ye},
  doi          = {10.1109/TPAMI.2023.3273291},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12699-12706},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learnable distribution calibration for few-shot class-incremental learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural network meets sparse representation: Graph
sparse neural networks via exclusive group lasso. <em>TPAMI</em>,
<em>45</em>(10), 12692–12698. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing GNNs usually conduct the layer-wise message propagation via the ‘full’ aggregation of all neighborhood information which are usually sensitive to the structural noises existed in the graphs, such as incorrect or undesired redundant edge connections. To overcome this issue, we propose to exploit Sparse Representation (SR) theory into GNNs and propose Graph Sparse Neural Networks (GSNNs) which conduct sparse aggregation to select reliable neighbors for message aggregation. GSNNs problem contains discrete/sparse constraint which is difficult to be optimized. Thus, we then develop a tight continuous relaxation model Exclusive Group Lasso GNNs (EGLassoGNNs) for GSNNs. An effective algorithm is derived to optimize the proposed EGLassoGNNs model. Experimental results on several benchmark datasets demonstrate the better performance and robustness of the proposed EGLassoGNNs model.},
  archive      = {J_TPAMI},
  author       = {Bo Jiang and Beibei Wang and Si Chen and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2023.3285215},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12692-12698},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph neural network meets sparse representation: Graph sparse neural networks via exclusive group lasso},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approach to robust ICP initialization. <em>TPAMI</em>,
<em>45</em>(10), 12685–12691. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we propose an approach to initialize the Iterative Closest Point (ICP) algorithm to match unlabelled point clouds related by rigid transformations. The method is based on matching the ellipsoids defined by the points’ covariance matrices and then testing the various principal half–axes matchings that differ by elements of a finite reflection group. We derive bounds on the robustness of our approach to noise and numerical experiments confirm our theoretical findings.},
  archive      = {J_TPAMI},
  author       = {Alexander Kolpakov and Michael Werman},
  doi          = {10.1109/TPAMI.2023.3287468},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12685-12691},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An approach to robust ICP initialization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ZITS++: Image inpainting by improving the incremental
transformer on structural priors. <em>TPAMI</em>, <em>45</em>(10),
12667–12684. (<a
href="https://doi.org/10.1109/TPAMI.2023.3280222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting involves filling missing areas of a corrupted image. Despite impressive results have been achieved recently, restoring images with both vivid textures and reasonable structures remains a significant challenge. Previous methods have primarily addressed regular textures while disregarding holistic structures due to the limited receptive fields of Convolutional Neural Networks (CNNs). To this end, we study learning a Zero-initialized residual addition based Incremental Transformer on Structural priors (ZITS++), an improved model upon our conference work, ZITS (Dong et al. 2022). Specifically, given one corrupt image, we present the Transformer Structure Restorer (TSR) module to restore holistic structural priors at low image resolution, which are further upsampled by Simple Structure Upsampler (SSU) module to higher image resolution. To recover image texture details, we use the Fourier CNN Texture Restoration (FTR) module, which is strengthened by Fourier and large-kernel attention convolutions. Furthermore, to enhance the FTR, the upsampled structural priors from TSR are further processed by Structure Feature Encoder (SFE) and optimized with the Zero-initialized Residual Addition (ZeroRA) incrementally. Besides, a new masking positional encoding is proposed to encode the large irregular masks. Compared with ZITS, ZITS++ improves the FTR&#39;s stability and inpainting ability with several techniques. More importantly, we comprehensively explore the effects of various image priors for inpainting and investigate how to utilize them to address high-resolution image inpainting with extensive experiments. This investigation is orthogonal to most inpainting approaches and can thus significantly benefit the community.},
  archive      = {J_TPAMI},
  author       = {Chenjie Cao and Qiaole Dong and Yanwei Fu},
  doi          = {10.1109/TPAMI.2023.3280222},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12667-12684},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ZITS++: Image inpainting by improving the incremental transformer on structural priors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot hyperspectral sharpening. <em>TPAMI</em>,
<em>45</em>(10), 12650–12666. (<a
href="https://doi.org/10.1109/TPAMI.2023.3279050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing hyperspectral images (HSIs) with multispectral images (MSIs) of higher spatial resolution has become an effective way to sharpen HSIs. Recently, deep convolutional neural networks (CNNs) have achieved promising fusion performance. However, these methods often suffer from the lack of training data and limited generalization ability. To address the above problems, we present a zero-shot learning (ZSL) method for HSI sharpening. Specifically, we first propose a novel method to quantitatively estimate the spectral and spatial responses of imaging sensors with high accuracy. In the training procedure, we spatially subsample the MSI and HSI based on the estimated spatial response and use the downsampled HSI and MSI to infer the original HSI. In this way, we can not only exploit the inherent information in the HSI and MSI, but the trained CNN can also be well generalized to the test data. In addition, we take the dimension reduction on the HSI, which reduces the model size and storage usage without sacrificing fusion accuracy. Furthermore, we design an imaging model-based loss function for CNN, which further boosts the fusion performance. The experimental results show the significantly high efficiency and accuracy of our approach.},
  archive      = {J_TPAMI},
  author       = {Renwei Dian and Anjing Guo and Shutao Li},
  doi          = {10.1109/TPAMI.2023.3279050},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12650-12666},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Zero-shot hyperspectral sharpening},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vicinity vision transformer. <em>TPAMI</em>,
<em>45</em>(10), 12635–12649. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers have shown great success on numerous computer vision tasks. However, their central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Linear attention was introduced in natural language processing (NLP) which reorders the self-attention mechanism to mitigate a similar issue, but directly applying existing linear attention to vision may not lead to satisfactory results. We investigate this problem and point out that existing linear attention methods ignore an inductive bias in vision tasks, i.e., 2D locality. In this article, we propose Vicinity Attention, which is a type of linear attention that integrates 2D locality. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance from its neighbouring patches. In this case, we achieve 2D locality in a linear complexity where the neighbouring image patches receive stronger attention than far away patches. In addition, we propose a novel Vicinity Attention Block that is comprised of Feature Reduction Attention (FRA) and Feature Preserving Connection (FPC) in order to address the computational bottleneck of linear attention approaches, including our Vicinity Attention, whose complexity grows quadratically with respect to the feature dimension. The Vicinity Attention Block computes attention in a compressed feature space with an extra skip connection to retrieve the original feature distribution. We experimentally validate that the block further reduces computation without degenerating the accuracy. Finally, to validate the proposed methods, we build a linear vision transformer backbone named Vicinity Vision Transformer (VVT). Targeting general vision tasks, we build VVT in a pyramid structure with progressively reduced sequence length. We perform extensive experiments on CIFAR-100, ImageNet-1 k, and ADE20 K datasets to validate the effectiveness of our method. Our method has a slower growth rate in terms of computational overhead than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50\% fewer parameters than previous approaches.},
  archive      = {J_TPAMI},
  author       = {Weixuan Sun and Zhen Qin and Hui Deng and Jianyuan Wang and Yi Zhang and Kaihao Zhang and Nick Barnes and Stan Birchfield and Lingpeng Kong and Yiran Zhong},
  doi          = {10.1109/TPAMI.2023.3285569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12635-12649},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vicinity vision transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational data-free knowledge distillation for continual
learning. <em>TPAMI</em>, <em>45</em>(10), 12618–12634. (<a
href="https://doi.org/10.1109/TPAMI.2023.3271626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks suffer from catastrophic forgetting when trained on sequential tasks in continual learning. Various methods rely on storing data of previous tasks to mitigate catastrophic forgetting, which is prohibited in real-world applications considering privacy and security issues. In this paper, we consider a realistic setting of continual learning, where training data of previous tasks are unavailable and memory resources are limited. We contribute a novel knowledge distillation-based method in an information-theoretic framework by maximizing mutual information between outputs of previously learned and current networks. Due to the intractability of computation of mutual information, we instead maximize its variational lower bound, where the covariance of variational distribution is modeled by a graph convolutional network. The inaccessibility of data of previous tasks is tackled by Taylor expansion, yielding a novel regularizer in network training loss for continual learning. The regularizer relies on compressed gradients of network parameters. It avoids storing previous task data and previously learned networks. Additionally, we employ self-supervised learning technique for learning effective features, which improves the performance of continual learning. We conduct extensive experiments including image classification and semantic segmentation, and the results show that our method achieves state-of-the-art performance on continual learning benchmarks.},
  archive      = {J_TPAMI},
  author       = {Xiaorong Li and Shipeng Wang and Jian Sun and Zongben Xu},
  doi          = {10.1109/TPAMI.2023.3271626},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12618-12634},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational data-free knowledge distillation for continual learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational cross-graph reasoning and adaptive structured
semantics learning for compositional temporal grounding. <em>TPAMI</em>,
<em>45</em>(10), 12601–12617. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal grounding is the task of locating a specific segment from an untrimmed video according to a query sentence. This task has achieved significant momentum in the computer vision community as it enables activity grounding beyond pre-defined activity classes by utilizing the semantic diversity of natural language descriptions. The semantic diversity is rooted in the principle of compositionality in linguistics, where novel semantics can be systematically described by combining known words in novel ways ( compositional generalization ). However, existing temporal grounding datasets are not carefully designed to evaluate the compositional generalizability. To systematically benchmark the compositional generalizability of temporal grounding models, we introduce a new Compositional Temporal Grounding task and construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. We empirically find that they fail to generalize to queries with novel combinations of seen words. We argue that the inherent compositional structure (i.e., composition constituents and their relationships) inside the videos and language is the crucial factor to achieve compositional generalization. Based on this insight, we propose a variational cross-graph reasoning framework that explicitly decomposes video and language into hierarchical semantic graphs, respectively, and learns fine-grained semantic correspondence between the two graphs. Meanwhile, we introduce a novel adaptive structured semantics learning approach to derive the structure-informed and domain-generalizable graph representations, which facilitate the fine-grained semantic correspondence reasoning between the two graphs. To further evaluate the understanding of the compositional structure, we also introduce a more challenging setting, where one of the components in the novel composition is unseen. This requires more sophisticated understanding of the compositional structure to infer the potential semantics of the unseen word based on the other learned composition constituents appearing in both the video and language context, and their relationships. Extensive experiments validate the superior compositional generalizability of our approach, demonstrating its ability to handle queries with novel combinations of seen words as well as novel words in the testing composition.},
  archive      = {J_TPAMI},
  author       = {Juncheng Li and Siliang Tang and Linchao Zhu and Wenqiao Zhang and Yi Yang and Tat-Seng Chua and Fei Wu and Yueting Zhuang},
  doi          = {10.1109/TPAMI.2023.3274139},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12601-12617},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). UniFormer: Unifying convolution and self-attention for
visual recognition. <em>TPAMI</em>, <em>45</em>(10), 12581–12600. (<a
href="https://doi.org/10.1109/TPAMI.2023.3282631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to learn discriminative representation from images and videos, due to large local redundancy and complex global dependency in these visual data. Convolution neural networks (CNNs) and vision transformers (ViTs) have been two dominant frameworks in the past few years. Though CNNs can efficiently decrease local redundancy by convolution within a small neighborhood, the limited receptive field makes it hard to capture global dependency. Alternatively, ViTs can effectively capture long-range dependency via self-attention, while blind similarity comparisons among all the tokens lead to high redundancy. To resolve these problems, we propose a novel Unified transFormer (UniFormer), which can seamlessly integrate the merits of convolution and self-attention in a concise transformer format. Different from the typical transformer blocks, the relation aggregators in our UniFormer block are equipped with local and global token affinity respectively in shallow and deep layers, allowing tackling both redundancy and dependency for efficient and effective representation learning. Finally, we flexibly stack our blocks into a new powerful backbone, and adopt it for various vision tasks from image to video domain, from classification to dense prediction. Without any extra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1 K classification task. With only ImageNet-1 K pre-training, it can simply achieve state-of-the-art performance in a broad range of downstream tasks. It obtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy on Something-Something V1/V2 video classification tasks, 53.8 box AP and 46.4 mask AP on COCO object detection task, 50.8 mIoU on ADE20 K semantic segmentation task, and 77.4 AP on COCO pose estimation task. Moreover, we build an efficient UniFormer with a concise hourglass design of token shrinking and recovering, which achieves 2-4 $\bm {\times }$ higher throughput than the recent lightweight models.},
  archive      = {J_TPAMI},
  author       = {Kunchang Li and Yali Wang and Junhao Zhang and Peng Gao and Guanglu Song and Yu Liu and Hongsheng Li and Yu Qiao},
  doi          = {10.1109/TPAMI.2023.3282631},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12581-12600},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {UniFormer: Unifying convolution and self-attention for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unbiased scene graph generation via two-stage causal
modeling. <em>TPAMI</em>, <em>45</em>(10), 12562–12580. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of recent unbiased Scene Graph Generation (SGG) methods, the current debiasing literature mainly focuses on the long-tailed distribution problem, whereas it overlooks another source of bias, i.e., semantic confusion, which makes the SGG model prone to yield false predictions for similar relationships. In this paper, we explore a debiasing procedure for the SGG task leveraging causal inference. Our central insight is that the Sparse Mechanism Shift (SMS) in causality allows independent intervention on multiple biases, thereby potentially preserving head category performance while pursuing the prediction of high-informative tail relationships. However, the noisy datasets lead to unobserved confounders for the SGG task, and thus the constructed causal models are always causal-insufficient to benefit from SMS. To remedy this, we propose Two-stage Causal Modeling (TsCM) for the SGG task, which takes the long-tailed distribution and semantic confusion as confounders to the Structural Causal Model (SCM) and then decouples the causal intervention into two stages. The first stage is causal representation learning, where we use a novel Population Loss (P-Loss) to intervene in the semantic confusion confounder. The second stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the long-tailed distribution confounder to complete causal calibration learning. These two stages are model agnostic and thus can be used in any SGG model that seeks unbiased predictions. Comprehensive experiments conducted on the popular SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art performance in terms of mean recall rate. Furthermore, TsCM can maintain a higher recall rate than other debiasing methods, which indicates that our method can achieve a better tradeoff between head and tail relationships.},
  archive      = {J_TPAMI},
  author       = {Shuzhou Sun and Shuaifeng Zhi and Qing Liao and Janne Heikkilä and Li Liu},
  doi          = {10.1109/TPAMI.2023.3285009},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12562-12580},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unbiased scene graph generation via two-stage causal modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards trajectory forecasting from detection.
<em>TPAMI</em>, <em>45</em>(10), 12550–12561. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory forecasting for traffic participants (e.g., vehicles) is critical for autonomous platforms to make safe plans. Currently, most trajectory forecasting methods assume that object trajectories have been extracted and directly develop trajectory predictors based on the ground truth trajectories. However, this assumption does not hold in practical situations. Trajectories obtained from object detection and tracking are inevitably noisy, which could cause serious forecasting errors to predictors built on ground truth trajectories. In this paper, we propose to predict trajectories directly based on detection results without relying on explicitly formed trajectories. Different from traditional methods which encode the motion cues of an agent based on its clearly defined trajectory, we extract the motion information only based on the affinity cues among detection results, in which an affinity-aware state update mechanism is designed to manage the state information. In addition, considering that there could be multiple plausible matching candidates, we aggregate the states of them. These designs take the uncertainty of association into account which relax the undesirable effect of noisy trajectory obtained from data association and improve the robustness of the predictor. Extensive experiments validate the effectiveness of our method and its generalization ability to different detectors or forecasting schemes.},
  archive      = {J_TPAMI},
  author       = {Pu Zhang and Lei Bai and Yuning Wang and Jianwu Fang and Jianru Xue and Nanning Zheng and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2023.3274686},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12550-12561},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards trajectory forecasting from detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards deviation-robust agent navigation via
perturbation-aware contrastive learning. <em>TPAMI</em>,
<em>45</em>(10), 12535–12549. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-language navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world navigation scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Pro gressive Per turbation-aware Contrastive Learning (PROPER) to enhance the generalization ability of existing VLN agents to the real world, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction. Since directly enforcing the agent to learn perturbed trajectories may lead to insufficient and inefficient training, a progressively perturbed trajectory augmentation strategy is designed, where the agent can self-adaptively learn to navigate under perturbation with the improvement of its navigation performance for each specific trajectory. For encouraging the agent to well capture the difference brought by perturbation and adapt to both perturbation-free and perturbation-based environments, a perturbation-aware contrastive learning mechanism is further developed by contrasting perturbation-free trajectory encodings and perturbation-based counterparts. Extensive experiments on the standard Room-to-Room (R2R) benchmark show that PROPER can benefit multiple state-of-the-art VLN baselines in perturbation-free scenarios. We further collect the perturbed path data to construct an introspection subset based on the R2R, called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying robustness of popular VLN agents and the capability of PROPER in improving the navigation robustness under deviation.},
  archive      = {J_TPAMI},
  author       = {Bingqian Lin and Yanxin Long and Yi Zhu and Fengda Zhu and Xiaodan Liang and Qixiang Ye and Liang Lin},
  doi          = {10.1109/TPAMI.2023.3273594},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12535-12549},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards deviation-robust agent navigation via perturbation-aware contrastive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward human-like grasp: Functional grasp by dexterous
robotic hand via object-hand semantic representation. <em>TPAMI</em>,
<em>45</em>(10), 12521–12534. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent robotic manipulation is a challenging study of machine intelligence. Although many dexterous robotic hands have been designed to assist or replace human hands in executing various tasks, how to teach them to perform dexterous operations like human hands is still a challenge. This motivates us to conduct an in-depth analysis of human behavior in manipulating objects and propose an object-hand manipulation representation. This representation provides an intuitive and clear semantic indication of how the dexterous hand should touch and manipulate an object based on the object&#39;s own functional areas. At the same time, we propose a functional grasp synthesis framework, which does not require real grasp label supervision, but relies on the guidance of our object-hand manipulation representation. In addition, in order to obtain better functional grasp synthesis results, we propose a network pre-training method that can make full use of easily obtained stable grasp data, and a network training strategy to coordinate the loss functions. We conduct object manipulation experiments on a real robot platform, and evaluate the performance and generalization of our object-hand manipulation representation and grasp synthesis framework.},
  archive      = {J_TPAMI},
  author       = {Tianqiang Zhu and Rina Wu and Jinglue Hang and Xiangbo Lin and Yi Sun},
  doi          = {10.1109/TPAMI.2023.3272571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12521-12534},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward human-like grasp: Functional grasp by dexterous robotic hand via object-hand semantic representation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal perceiver: A general architecture for arbitrary
boundary detection. <em>TPAMI</em>, <em>45</em>(10), 12506–12520. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic Boundary Detection (GBD) aims at locating the general boundaries that divide videos into semantically coherent and taxonomy-free units, and could serve as an important pre-processing step for long-form video understanding. Previous works often separately handle these different types of generic boundaries with specific designs of deep networks from simple CNN to LSTM. Instead, in this paper, we present Temporal Perceiver , a general architecture with Transformer, offering a unified solution to the detection of arbitrary generic boundaries, ranging from shot-level, event-level, to scene-level GBDs. Our core design is to introduce a small set of latent feature queries as anchors to compress the redundant video input into a fixed dimension via cross-attention blocks. Thanks to this fixed number of latent units, it reduces the quadratic complexity of attention operation to a linear form of input frames. Specifically, to explicitly leverage the temporal structure of videos, we construct two types of latent feature queries: boundary queries and context queries, which handle the semantic incoherence and coherence accordingly. Moreover, to guide the learning of latent feature queries, we propose an alignment loss on the cross-attention maps to explicitly encourage the boundary queries to attend on the top boundary candidates. Finally, we present a sparse detection head on the compressed representation, and directly output the final boundary detection results without any post-processing module. We test our Temporal Perceiver on a variety of GBD benchmarks. Our method obtains the state-of-the-art results on all benchmarks with RGB single-stream features: SoccerNet-v2 (81.9 percent average-mAP), Kinetics-GEBD (86.0 percent average-f1), TAPOS (73.2 percent average-f1), MovieScenes (51.9 percent AP and 53.1 percent $M_{iou}$ ) and MovieNet (53.3 percent AP and 53.2 percent $M_{iou}$ ), demonstrating the generalization ability of our Temporal Perceiver. To further pursue a general GBD model, we combine various tasks to train a class-agnostic Temporal perceiver and evaluate its performance across all benchmarks. Results show that the class-agnostic Perceiver achieves comparable detection accuracy but better generalization ability compared to dataset-specific counterparts.},
  archive      = {J_TPAMI},
  author       = {Jing Tan and Yuhong Wang and Gangshan Wu and Limin Wang},
  doi          = {10.1109/TPAMI.2023.3283067},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12506-12520},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Temporal perceiver: A general architecture for arbitrary boundary detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Super sparse 3D object detection. <em>TPAMI</em>,
<em>45</em>(10), 12490–12505. (<a
href="https://doi.org/10.1109/TPAMI.2023.3286409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the perception range of LiDAR expands, LiDAR-based 3D object detection contributes ever-increasingly to the long-range perception in autonomous driving. Mainstream 3D object detectors often build dense feature maps, where the cost is quadratic to the perception range, making them hardly scale up to the long-range settings. To enable efficient long-range detection, we first propose a fully sparse object detector termed FSD. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR groups the points into instances and applies highly-efficient instance-wise feature extraction. The instance-wise grouping sidesteps the issue of the center feature missing, which hinders the design of the fully sparse architecture. To further enjoy the benefit of fully sparse characteristic, we leverage temporal information to remove data redundancy and propose a super sparse detector named FSD++. FSD++ first generates residual points, which indicate the point changes between consecutive frames. The residual points, along with a few previous foreground points, form the super sparse input data, greatly reducing data redundancy and computational overhead. We comprehensively analyze our method on the large-scale Waymo Open Dataset, and state-of-the-art performance is reported. To showcase the superiority of our method in long-range detection, we also conduct experiments on Argoverse 2 Dataset, where the perception range ( $\text{200}$ m) is much larger than Waymo Open Dataset ( $\text{75}$ m).},
  archive      = {J_TPAMI},
  author       = {Lue Fan and Yuxue Yang and Feng Wang and Naiyan Wang and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2023.3286409},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12490-12505},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Super sparse 3D object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPDET: Edge-aware self-supervised panoramic depth estimation
transformer with spherical geometry. <em>TPAMI</em>, <em>45</em>(10),
12474–12489. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoramic depth estimation has become a hot topic in 3D reconstruction techniques with its omnidirectional spatial field of view. However, panoramic RGB-D datasets are difficult to obtain due to the lack of panoramic RGB-D cameras, thus limiting the practicality of supervised panoramic depth estimation. Self-supervised learning based on RGB stereo image pairs has the potential to overcome this limitation due to its low dependence on datasets. In this work, we propose the SPDET, an edge-aware self-supervised panoramic depth estimation network that combines the transformer with a spherical geometry feature. Specifically, we first introduce the panoramic geometry feature to construct our panoramic transformer and reconstruct high-quality depth maps. Furthermore, we introduce the pre-filtered depth-image-based rendering method to synthesize the novel view image for self-supervision. Meanwhile, we design an edge-aware loss function to improve the self-supervised depth estimation for panorama images. Finally, we demonstrate the effectiveness of our SPDET with a series of comparison and ablation experiments while achieving the state-of-the-art self-supervised monocular panoramic depth estimation. Our code and models are available at https://github.com/zcq15/SPDET .},
  archive      = {J_TPAMI},
  author       = {Chuanqing Zhuang and Zhengda Lu and Yiqun Wang and Jun Xiao and Ying Wang},
  doi          = {10.1109/TPAMI.2023.3272949},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12474-12489},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SPDET: Edge-aware self-supervised panoramic depth estimation transformer with spherical geometry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-path bit sharing for automatic loss-aware model
compression. <em>TPAMI</em>, <em>45</em>(10), 12459–12473. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning and quantization are proven to be effective ways for deep model compression. To obtain a highly compact model, most methods first perform network pruning and then conduct quantization based on the pruned model. However, this strategy may ignore that the pruning and quantization would affect each other and thus performing them separately may lead to sub-optimal performance. To address this, performing pruning and quantization jointly is essential. Nevertheless, how to make a trade-off between pruning and quantization is non-trivial. Moreover, existing compression methods often rely on some pre-defined compression configurations (i.e., pruning rates or bitwidths). Some attempts have been made to search for optimal configurations, which however may take unbearable optimization cost. To address these issues, we devise a simple yet effective method named Single-path Bit Sharing (SBS) for automatic loss-aware model compression. To this end, we consider the network pruning as a special case of quantization and provide a unified view for model pruning and quantization. We then introduce a single-path model to encode all candidate compression configurations, where a high bitwidth value will be decomposed into the sum of a lowest bitwidth value and a series of re-assignment offsets. Relying on the single-path model, we introduce learnable binary gates to encode the choice of configurations and learn the binary gates and model parameters jointly. More importantly, the configuration search problem can be transformed into a subset selection problem, which helps to significantly reduce the optimization difficulty and computation cost. In this way, the compression configurations of each layer and the trade-off between pruning and quantization can be automatically determined. Extensive experiments on CIFAR-100 and ImageNet show that SBS significantly reduces computation cost while achieving promising performance. For example, our SBS compressed MobileNetV2 achieves 22.6× Bit-Operation (BOP) reduction with only 0.1\% drop in the Top-1 accuracy.},
  archive      = {J_TPAMI},
  author       = {Jing Liu and Bohan Zhuang and Peng Chen and Chunhua Shen and Jianfei Cai and Mingkui Tan},
  doi          = {10.1109/TPAMI.2023.3275159},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12459-12473},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single-path bit sharing for automatic loss-aware model compression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensing diversity and sparsity models for event generation
and video reconstruction from events. <em>TPAMI</em>, <em>45</em>(10),
12444–12458. (<a
href="https://doi.org/10.1109/TPAMI.2023.3278940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Events-to-video (E2V) reconstruction and video-to-events (V2E) simulation are two fundamental research topics in event-based vision. Current deep neural networks for E2V reconstruction are usually complex and difficult to interpret. Moreover, existing event simulators are designed to generate realistic events, but research on how to improve the event generation process has been so far limited. In this paper, we propose a light, simple model-based deep network for E2V reconstruction, explore the diversity for adjacent pixels in V2E generation, and finally build a video-to-events-to-video (V2E2V) architecture to validate how alternative event generation strategies improve video reconstruction. For the E2V reconstruction, we model the relationship between events and intensity using sparse representation models. A convolutional ISTA network (CISTA) is then designed using the algorithm unfolding strategy. Long short-term temporal consistency (LSTC) constraints are further introduced to enhance the temporal coherence. In the V2E generation, we introduce the idea of having interleaved pixels with different contrast threshold and lowpass bandwidth and conjecture that this can help extract more useful information from intensity. Finally, V2E2V architecture is used to verify the effectiveness of this strategy. Results highlight that our CISTA-LSTC network outperforms state-of-the-art methods and achieves better temporal consistency. Sensing diversity in event generation reveals more fine details and this leads to a significantly improved reconstruction quality.},
  archive      = {J_TPAMI},
  author       = {Siying Liu and Pier Luigi Dragotti},
  doi          = {10.1109/TPAMI.2023.3278940},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12444-12458},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sensing diversity and sparsity models for event generation and video reconstruction from events},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic and temporal contextual correlation learning for
weakly-supervised temporal action localization. <em>TPAMI</em>,
<em>45</em>(10), 12427–12443. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised temporal action localization (WSTAL) aims to automatically identify and localize action instances in untrimmed videos with only video-level labels as supervision. In this task, there exist two challenges: (1) how to accurately discover the action categories in an untrimmed video (what to discover); (2) how to elaborately focus on the integral temporal interval of each action instance (where to focus). Empirically, to discover the action categories, discriminative semantic information should be extracted, while robust temporal contextual information is beneficial for complete action localization. However, most existing WSTAL methods ignore to explicitly and jointly model the semantic and temporal contextual correlation information for the above two challenges. In this article, a S emantic and T emporal Contextual C orrelation L earning Net work (STCL-Net) with the semantic (SCL) and temporal contextual correlation learning (TCL) modules is proposed, which achieves both accurate action discovery and complete action localization by modeling the semantic and temporal contextual correlation information for each snippet in the inter- and intra-video manners respectively. It is noteworthy that the two proposed modules are both designed in a unified dynamic correlation-embedding paradigm. Extensive experiments are performed on different benchmarks. On all the benchmarks, our proposed method exhibits superior or comparable performance in comparison to the existing state-of-the-art models, especially achieving gains as high as 7.2\% in terms of the average mAP on THUMOS-14. In addition, comprehensive ablation studies also verify the effectiveness and robustness of each component in our model.},
  archive      = {J_TPAMI},
  author       = {Jie Fu and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TPAMI.2023.3287208},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12427-12443},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semantic and temporal contextual correlation learning for weakly-supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning from untrimmed videos via
hierarchical consistency. <em>TPAMI</em>, <em>45</em>(10), 12408–12426.
(<a href="https://doi.org/10.1109/TPAMI.2023.3273415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural untrimmed videos provide rich visual content for self-supervised learning. Yet most previous efforts to learn spatio-temporal representations rely on manually trimmed videos, such as Kinetics dataset (Carreira and Zisserman 2017), resulting in limited diversity in visual patterns and limited performance gains. In this work, we aim to improve video representations by leveraging the rich information in natural untrimmed videos. For this purpose, we propose learning a hierarchy of temporal consistencies in videos, i.e., visual consistency and topical consistency, corresponding respectively to clip pairs that tend to be visually similar when separated by a short time span, and clip pairs that share similar topics when separated by a long time span. Specifically, we present a Hi erarchical Co nsistency (HiCo++) learning framework, in which the visually consistent pairs are encouraged to share the same feature representations by contrastive learning, while topically consistent pairs are coupled through a topical classifier that distinguishes whether they are topic-related, i.e., from the same untrimmed video. Additionally, we impose a gradual sampling algorithm for the proposed hierarchical consistency learning, and demonstrate its theoretical superiority. Empirically, we show that HiCo++ can not only generate stronger representations on untrimmed videos, but also improve the representation quality when applied to trimmed videos. This contrasts with standard contrastive learning, which fails to learn powerful representations from untrimmed videos. Source code will be made available here.},
  archive      = {J_TPAMI},
  author       = {Zhiwu Qing and Shiwei Zhang and Ziyuan Huang and Yi Xu and Xiang Wang and Changxin Gao and Rong Jin and Nong Sang},
  doi          = {10.1109/TPAMI.2023.3273415},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12408-12426},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised learning from untrimmed videos via hierarchical consistency},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised arbitrary-scale implicit point clouds
upsampling. <em>TPAMI</em>, <em>45</em>(10), 12394–12407. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds upsampling (PCU), which aims to generate dense and uniform point clouds from the captured sparse input of 3D sensor such as LiDAR, is a practical yet challenging task. It has potential applications in many real-world scenarios, such as autonomous driving, robotics, AR/VR, etc. Deep neural network based methods achieve remarkable success in PCU. However, most existing deep PCU methods either take the end-to-end supervised training, where large amounts of pairs of sparse input and dense ground-truth are required to serve as the supervision; or treat up-scaling of different factors as independent tasks, where multiple networks are required for different scaling factors, leading to significantly increased model complexity and training time. In this article, we propose a novel method that achieves self-supervised and magnification-flexible PCU simultaneously. No longer explicitly learning the mapping between sparse and dense point clouds, we formulate PCU as the task of seeking nearest projection points on the implicit surface for seed points. We then define two implicit neural functions to estimate projection direction and distance respectively, which can be trained by the pretext learning tasks. Moreover, the projection rectification strategy is tailored to remove outliers so as to keep the shape of object clear and sharp. Experimental results demonstrate that our self-supervised learning based scheme achieves competitive or even better performance than state-of-the-art supervised methods.},
  archive      = {J_TPAMI},
  author       = {Wenbo Zhao and Xianming Liu and Deming Zhai and Junjun Jiang and Xiangyang Ji},
  doi          = {10.1109/TPAMI.2023.3287628},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12394-12407},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised arbitrary-scale implicit point clouds upsampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-prior guided pixel adversarial networks for blind image
inpainting. <em>TPAMI</em>, <em>45</em>(10), 12377–12393. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image inpainting involves two critical aspects, i.e., “where to inpaint” and “how to inpaint”. Knowing “where to inpaint” can eliminate the interference arising from corrupted pixel values; a good “how to inpaint” strategy yields high-quality inpainted results robust to various corruptions. In existing methods, these two aspects usually lack explicit and separate consideration. This paper fully explores these two aspects and proposes a self-prior guided inpainting network (SIN). The self-priors are obtained by detecting semantic-discontinuous regions and by predicting global semantic structures of the input image. On the one hand, the self-priors are incorporated into the SIN, which enables the SIN to perceive valid context information from uncorrupted regions and to synthesize semantic-aware textures for corrupted regions. On the other hand, the self-priors are reformulated to provide a pixel-wise adversarial feedback and a high-level semantic structure feedback, which can promote the semantic continuity of inpainted images. Experimental results demonstrate that our method achieves state-of-the-art performance in metric scores and in visual quality. It has an advantage over many existing methods that assume “where to inpaint” is known in advance. Extensive experiments on a series of related image restoration tasks validate the effectiveness of our method in obtaining high-quality inpainting.},
  archive      = {J_TPAMI},
  author       = {Juan Wang and Chunfeng Yuan and Bing Li and Ying Deng and Weiming Hu and Stephen Maybank},
  doi          = {10.1109/TPAMI.2023.3284431},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12377-12393},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-prior guided pixel adversarial networks for blind image inpainting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SC<span
class="math inline"><sup>2</sup></span><!-- -->2-PCR++: Rethinking the
generation and selection for efficient and robust point cloud
registration. <em>TPAMI</em>, <em>45</em>(10), 12358–12376. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier removal is a critical part of feature-based point cloud registration. In this article, we revisit the model generation and selection of the classic RANSAC approach for fast and robust point cloud registration. For the model generation, we propose a second-order spatial compatibility (SC $^{2}$ ) measure to compute the similarity between correspondences. It takes into account global compatibility instead of local consistency, allowing for more distinctive clustering between inliers and outliers at an early stage. The proposed measure promises to find a certain number of outlier-free consensus sets using fewer samplings, making the model generation more efficient. For the model selection, we propose a new Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric for evaluating the generated models. It considers the alignment quality, the feature matching properness, and the spatial consistency constraint simultaneously, enabling the correct model to be selected even when the inlier rate of the putative correspondence set is extremely low. Extensive experiments are carried out to investigate the performance of our method. In addition, we also experimentally prove that the proposed SC $^{2}$ measure and the FS-TCD metric are general and can be easily plugged into deep learning based frameworks.},
  archive      = {J_TPAMI},
  author       = {Zhi Chen and Kun Sun and Fan Yang and Lin Guo and Wenbing Tao},
  doi          = {10.1109/TPAMI.2023.3272557},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12358-12376},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SC$^{2}$2-PCR++: Rethinking the generation and selection for efficient and robust point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Saliency as pseudo-pixel supervision for weakly and
semi-supervised semantic segmentation. <em>TPAMI</em>, <em>45</em>(10),
12341–12357. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies on semantic segmentation using image-level weak supervision have several limitations, including sparse object coverage, inaccurate object boundaries, and co-occurring pixels from non-target objects. To overcome these challenges, we propose a novel framework, an improved version of Explicit Pseudo-pixel Supervision (EPS++), which learns from pixel-level feedback by combining two types of weak supervision. Specifically, the image-level label provides the object identity via the localization map, and the saliency map from an off-the-shelf saliency detection model offers rich object boundaries. We devise a joint training strategy to fully utilize the complementary relationship between disparate information. Notably, we suggest an Inconsistent Region Drop (IRD) strategy, which effectively handles errors in saliency maps using fewer hyper-parameters than EPS. Our method can obtain accurate object boundaries and discard co-occurring pixels, significantly improving the quality of pseudo-masks. Experimental results show that EPS++ effectively resolves the key challenges of semantic segmentation using weak supervision, resulting in new state-of-the-art performances on three benchmark datasets in a weakly supervised semantic segmentation setting. Furthermore, we show that the proposed method can be extended to solve the semi-supervised semantic segmentation problem using image-level weak supervision. Surprisingly, the proposed model also achieves new state-of-the-art performances on two popular benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Minhyun Lee and Seungho Lee and Jongwuk Lee and Hyunjung Shim},
  doi          = {10.1109/TPAMI.2023.3273592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12341-12357},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Saliency as pseudo-pixel supervision for weakly and semi-supervised semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recent advances for quantum neural networks in generative
learning. <em>TPAMI</em>, <em>45</em>(10), 12321–12340. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum variational autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relations and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.},
  archive      = {J_TPAMI},
  author       = {Jinkai Tian and Xiaoyu Sun and Yuxuan Du and Shanshan Zhao and Qing Liu and Kaining Zhang and Wei Yi and Wanrong Huang and Chaoyue Wang and Xingyao Wu and Min-Hsiu Hsieh and Tongliang Liu and Wenjing Yang and Dacheng Tao},
  doi          = {10.1109/TPAMI.2023.3272029},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12321-12340},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recent advances for quantum neural networks in generative learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ranking-based color constancy with limited training samples.
<em>TPAMI</em>, <em>45</em>(10), 12304–12320. (<a
href="https://doi.org/10.1109/TPAMI.2023.3278832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational color constancy is an important component of Image Signal Processors (ISP) for white balancing in many imaging devices. Recently, deep convolutional neural networks (CNN) have been introduced for color constancy. They achieve prominent performance improvements comparing with those statistics or shallow learning-based methods. However, the need for a large number of training samples, a high computational cost and a huge model size make CNN-based methods unsuitable for deployment on low-resource ISPs for real-time applications. In order to overcome these limitations and to achieve comparable performance to CNN-based methods, an efficient method is defined for selecting the optimal simple statistics-based method (SM) for each image. To this end, we propose a novel ranking-based color constancy method (RCC) that formulates the selection of the optimal SM method as a label ranking problem. RCC designs a specific ranking loss function, and uses a low rank constraint to control the model complexity and a grouped sparse constraint for feature selection. Finally, we apply the RCC model to predict the order of the candidate SM methods for a test image, and then estimate its illumination using the predicted optimal SM method (or fusing the results estimated by the top $k$ SM methods). Comprehensive experiment results show that the proposed RCC outperforms nearly all the shallow learning-based methods and achieves comparable performance to (sometimes even better performance than) deep CNN-based methods with only 1/2000 of the model size and training time. RCC also shows good robustness to limited training samples and good generalization crossing cameras. Furthermore, to remove the dependence on the ground truth illumination, we extend RCC to obtain a novel ranking-based method without ground truth illumination (RCC_NO) that learns the ranking model using simple partial binary preference annotations provided by untrained annotators rather than experts. RCC_NO also achieves better performance than the SM methods and most shallow learning-based methods with low costs of sample collection and illumination measurement.},
  archive      = {J_TPAMI},
  author       = {Bing Li and Haina Qin and Weihua Xiong and Yangxi Li and Songhe Feng and Weiming Hu and Stephen Maybank},
  doi          = {10.1109/TPAMI.2023.3278832},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12304-12320},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ranking-based color constancy with limited training samples},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PyMAF-x: Towards well-aligned full-body model regression
from monocular images. <em>TPAMI</em>, <em>45</em>(10), 12287–12303. (<a
href="https://doi.org/10.1109/TPAMI.2023.3271691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PyMAF-X, a regression-based approach to recovering a parametric full-body model from a single image. This task is very challenging since minor parametric deviation may lead to noticeable misalignment between the estimated mesh and the input image. Moreover, when integrating part-specific estimations into the full-body model, existing solutions tend to either degrade the alignment or produce unnatural wrist poses. To address these issues, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for well-aligned human mesh recovery and extend it as PyMAF-X for the recovery of expressive full-body models. The core idea of PyMAF is to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status. Specifically, given the currently predicted parameters, mesh-aligned evidence will be extracted from finer-resolution features accordingly and fed back for parameter rectification. To enhance the alignment perception, an auxiliary dense supervision is employed to provide mesh-image correspondence guidance while spatial alignment attention is introduced to enable the awareness of the global contexts for our network. When extending PyMAF for full-body mesh recovery, an adaptive integration strategy is proposed in PyMAF-X to produce natural wrist poses while maintaining the well-aligned performance of the part-specific estimations. The efficacy of our approach is validated on several benchmark datasets for body, hand, face, and full-body mesh recovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment and achieve new The project page with code and video results can be found at https://www.liuyebin.com/pymaf-x .},
  archive      = {J_TPAMI},
  author       = {Hongwen Zhang and Yating Tian and Yuxiang Zhang and Mengcheng Li and Liang An and Zhenan Sun and Yebin Liu},
  doi          = {10.1109/TPAMI.2023.3271691},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12287-12303},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PyMAF-X: Towards well-aligned full-body model regression from monocular images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PWLU: Learning specialized activation functions with the
piecewise linear unit. <em>TPAMI</em>, <em>45</em>(10), 12269–12286. (<a
href="https://doi.org/10.1109/TPAMI.2023.3286109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The choice of activation functions is crucial to deep neural networks. ReLU is a popular hand-designed activation function. Swish, the automatically searched activation function, outperforms ReLU on many challenging datasets. However, the search method has two main drawbacks. First, the tree-based search space is highly discrete and restricted, which is difficult to search. Second, the sample-based search method is inefficient in finding specialized activation functions for each dataset or neural architecture. To overcome these drawbacks, we propose a new activation function called Piecewise Linear Unit (PWLU), incorporating a carefully designed formulation and learning method. PWLU can learn specialized activation functions for different models, layers, or channels. Besides, we propose a non-uniform version of PWLU, which maintains sufficient flexibility but requires fewer intervals and parameters. Additionally, we generalize PWLU to three-dimensional space to define a piecewise linear surface named 2D-PWLU, which can be treated as a non-linear binary operator. Experimental results show that PWLU achieves SOTA performance on various tasks and models, and 2D-PWLU is better than element-wise addition when aggregating features from different branches. The proposed PWLU and its variation are easy to implement and efficient for inference, which can be widely applied in real-world applications.},
  archive      = {J_TPAMI},
  author       = {Zezhou Zhu and Yucong Zhou and Yuan Dong and Zhao Zhong},
  doi          = {10.1109/TPAMI.2023.3286109},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12269-12286},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PWLU: Learning specialized activation functions with the piecewise linear unit},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prototype completion for few-shot learning. <em>TPAMI</em>,
<em>45</em>(10), 12250–12268. (<a
href="https://doi.org/10.1109/TPAMI.2023.3277881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to recognize novel classes with few examples. Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then fine-tuning it through the nearest centroid based meta-learning. However, results show that the fine-tuning step makes marginal improvements. In this paper, 1) we figure out the reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which implies that fine-tuning feature extractor is less meaningful; 2) instead of fine-tuning feature extractor, we focus on estimating more representative prototypes. Consequently, we propose a novel prototype completion based meta-learning framework. This framework first introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative features for seen attributes as priors. Second, a part/attribute transfer network is designed to learn to infer the representative features for unseen attributes as supplementary priors. Finally, a prototype completion network is devised to learn to complete prototypes with these priors. Moreover, to avoid the prototype completion error, we further develop a Gaussian based prototype fusion strategy that fuses the mean-based and completed prototypes by exploiting the unlabeled samples. At last, we also develop an economic prototype completion version for FSL, which does not need to collect primitive knowledge, for a fair comparison with existing FSL methods without external knowledge. Extensive experiments show that our method: i) obtains more accurate prototypes; ii) achieves superior performance on both inductive and transductive FSL settings.},
  archive      = {J_TPAMI},
  author       = {Baoquan Zhang and Xutao Li and Yunming Ye and Shanshan Feng},
  doi          = {10.1109/TPAMI.2023.3277881},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12250-12268},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Prototype completion for few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prescribed safety performance imitation learning from a
single expert dataset. <em>TPAMI</em>, <em>45</em>(10), 12236–12249. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing safe imitation learning (safe IL) methods mainly focus on learning safe policies that are similar to expert ones, but may fail in applications requiring different safety constraints. In this paper, we propose the Lagrangian Generative Adversarial Imitation Learning (LGAIL) algorithm, which can adaptively learn safe policies from a single expert dataset under diverse prescribed safety constraints. To achieve this, we augment GAIL with safety constraints and then relax it as an unconstrained optimization problem by utilizing a Lagrange multiplier. The Lagrange multiplier enables explicit consideration of the safety and is dynamically adjusted to balance the imitation and safety performance during training. Then, we apply a two-stage optimization framework to solve LGAIL: (1) a discriminator is optimized to measure the similarity between the agent-generated data and the expert ones; (2) forward reinforcement learning is employed to improve the similarity while considering safety concerns enabled by a Lagrange multiplier. Furthermore, theoretical analyses on the convergence and safety of LGAIL demonstrate its capability of adaptively learning a safe policy given prescribed safety constraints. At last, extensive experiments in OpenAI Safety Gym conclude the effectiveness of our approach.},
  archive      = {J_TPAMI},
  author       = {Zhihao Cheng and Li Shen and Miaoxi Zhu and Jiaxian Guo and Meng Fang and Liu Liu and Bo Du and Dacheng Tao},
  doi          = {10.1109/TPAMI.2023.3287908},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12236-12249},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Prescribed safety performance imitation learning from a single expert dataset},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). POVNet: Image-based virtual try-on through accurate warping
and residual. <em>TPAMI</em>, <em>45</em>(10), 12222–12235. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual dressing room applications help online shoppers visualize outfits. Such a system, to be commercially viable, must satisfy a set of performance criteria. The system must produce high quality images that faithfully preserve garment properties, allow users to mix and match garments of various types and support human models varying in skin tone, hair color, body shape, and so on. This paper describes POVNet, a framework that meets all these requirements (except body shapes variations). Our system uses warping methods together with residual data to preserve garment texture at fine scales and high resolution. Our warping procedure adapts to a wide range of garments and allows swapping in and out of individual garments. A learned rendering procedure using an adversarial loss ensures that fine shading, etc. is accurately reflected. A distance transform representation ensures that hems, cuffs, stripes, and so on are correctly placed. We demonstrate improvements in garment rendering over state of the art resulting from these procedures. We demonstrate that the framework is scalable, responds in real-time, and works robustly with a variety of garment categories. Finally, we demonstrate that using this system as a virtual dressing room interface for fashion e-commerce websites has significantly boosted user-engagement rates.},
  archive      = {J_TPAMI},
  author       = {Kedan Li and Jeffrey Zhang and David Forsyth},
  doi          = {10.1109/TPAMI.2023.3283302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12222-12235},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {POVNet: Image-based virtual try-on through accurate warping and residual},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PiGLET: Pixel-level grounding of language expressions with
transformers. <em>TPAMI</em>, <em>45</em>(10), 12206–12221. (<a
href="https://doi.org/10.1109/TPAMI.2023.3286760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes Panoptic Narrative Grounding , a spatially fine and general formulation of the natural language visual grounding problem. We establish an experimental framework for the study of this new task, including new ground truth and metrics. We propose PiGLET, a novel multi-modal Transformer architecture to tackle the Panoptic Narrative Grounding task, and to serve as a stepping stone for future work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level using segmentations. In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. PiGLET achieves a performance of 63.2 absolute Average Recall points. By leveraging the rich language information on the Panoptic Narrative Grounding benchmark on MS COCO, PiGLET obtains an improvement of 0.4 Panoptic Quality points over its base method on the panoptic segmentation task. Finally, we demonstrate the generalizability of our method to other natural language visual grounding problems such as Referring Expression Segmentation. PiGLET is competitive with previous state-of-the-art in RefCOCO, RefCOCO+ and RefCOCOg.},
  archive      = {J_TPAMI},
  author       = {Cristina González and Nicolás Ayobi and Isabela Hernández and Jordi Pont-Tuset and Pablo Arbeláez},
  doi          = {10.1109/TPAMI.2023.3286760},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12206-12221},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PiGLET: Pixel-level grounding of language expressions with transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAR<span
class="math inline"><sup>2</sup></span><!-- -->2Net: End-to-end
panoramic image reflection removal. <em>TPAMI</em>, <em>45</em>(10),
12192–12205. (<a
href="https://doi.org/10.1109/TPAMI.2023.3286429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the problem of panoramic image reflection removal to relieve the content ambiguity between the reflection layer and the transmission scene. Although a partial view of the reflection scene is attainable in the panoramic image and provides additional information for reflection removal, it is not trivial to directly apply this for getting rid of undesired reflections due to its misalignment with the reflection-contaminated image. We propose an end-to-end framework to tackle this problem. By resolving misalignment issues with adaptive modules, the high-fidelity recovery of reflection layer and transmission scenes is accomplished. We further propose a new data generation approach that considers the physics-based formation model of mixture images and the in-camera dynamic range clipping to diminish the domain gap between synthetic and real data. Experimental results demonstrate the effectiveness of the proposed method and its applicability for mobile devices and industrial applications.},
  archive      = {J_TPAMI},
  author       = {Yuchen Hong and Qian Zheng and Lingran Zhao and Xudong Jiang and Alex C. Kot and Boxin Shi},
  doi          = {10.1109/TPAMI.2023.3286429},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12192-12205},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PAR$^{2}$2Net: End-to-end panoramic image reflection removal},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot adaptation of GAN in just one CLIP. <em>TPAMI</em>,
<em>45</em>(10), 12179–12191. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many recent research efforts to fine-tune a pre-trained generator with a few target images to generate images of a novel domain. Unfortunately, these methods often suffer from overfitting or under-fitting when fine-tuned with a single target image. To address this, here we present a novel single-shot GAN adaptation method through unified CLIP space manipulations. Specifically, our model employs a two-step training strategy: reference image search in the source generator using a CLIP-guided latent optimization, followed by generator fine-tuning with a novel loss function that imposes CLIP space consistency between the source and adapted generators. To further improve the adapted model to produce spatially consistent samples with respect to the source generator, we also propose contrastive regularization for patchwise relationships in the CLIP space. Experimental results show that our model generates diverse outputs with the target texture and outperforms the baseline models both qualitatively and quantitatively. Furthermore, we show that our CLIP space manipulation strategy allows more effective attribute editing.},
  archive      = {J_TPAMI},
  author       = {Gihyun Kwon and Jong Chul Ye},
  doi          = {10.1109/TPAMI.2023.3283551},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12179-12191},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {One-shot adaptation of GAN in just one CLIP},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No adversaries to zero-shot learning: Distilling an ensemble
of gaussian feature generators. <em>TPAMI</em>, <em>45</em>(10),
12167–12178. (<a
href="https://doi.org/10.1109/TPAMI.2023.3282971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In zero-shot learning (ZSL), the task of recognizing unseen categories when no data for training is available, state-of-the-art methods generate visual features from semantic auxiliary information (e.g., attributes). In this work, we propose a valid alternative (simpler, yet better scoring) to fulfill the very same task. We observe that, if first- and second-order statistics of the classes to be recognized were known, sampling from Gaussian distributions would synthesize visual features that are almost identical to the real ones as per classification purposes. We propose a novel mathematical framework to estimate first- and second-order statistics, even for unseen classes: our framework builds upon prior compatibility functions for ZSL and does not require additional training. Endowed with such statistics, we take advantage of a pool of class-specific Gaussian distributions to solve the feature generation stage through sampling. We exploit an ensemble mechanism to aggregate a pool of softmax classifiers, each trained in a one-seen-class-out fashion to better balance the performance over seen and unseen classes. Neural distillation is finally applied to fuse the ensemble into a single architecture which can perform inference through one forward pass only. Our method, termed Distilled Ensemble of Gaussian Generators, scores favorably with respect to state-of-the-art works.},
  archive      = {J_TPAMI},
  author       = {Jacopo Cavazza and Vittorio Murino and Alessio Del Bue},
  doi          = {10.1109/TPAMI.2023.3282971},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12167-12178},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {No adversaries to zero-shot learning: Distilling an ensemble of gaussian feature generators},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MURF: Mutually reinforcing multi-modal image registration
and fusion. <em>TPAMI</em>, <em>45</em>(10), 12148–12166. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image fusion methods are typically limited to aligned source images and have to “tolerate” parallaxes when images are unaligned. Simultaneously, the large variances between different modalities pose a significant challenge for multi-modal image registration. This study proposes a novel method called MURF, where for the first time, image registration and fusion are mutually reinforced rather than being treated as separate issues. MURF leverages three modules: shared information extraction module (SIEM), multi-scale coarse registration module (MCRM), and fine registration and fusion module (F2M). The registration is carried out in a coarse-to-fine manner. During coarse registration, SIEM first transforms multi-modal images into mono-modal shared information to eliminate the modal variances. Then, MCRM progressively corrects the global rigid parallaxes. Subsequently, fine registration to repair local non-rigid offsets and image fusion are uniformly implemented in F2M. The fused image provides feedback to improve registration accuracy, and the improved registration result further improves the fusion result. For image fusion, rather than solely preserving the original source information in existing methods, we attempt to incorporate texture enhancement into image fusion. We test on four types of multi-modal data (RGB-IR, RGB-NIR, PET-MRI, and CT-MRI). Extensive registration and fusion results validate the superiority and universality of MURF.},
  archive      = {J_TPAMI},
  author       = {Han Xu and Jiteng Yuan and Jiayi Ma},
  doi          = {10.1109/TPAMI.2023.3283682},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12148-12166},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MURF: Mutually reinforcing multi-modal image registration and fusion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple instance differentiation learning for active object
detection. <em>TPAMI</em>, <em>45</em>(10), 12133–12147. (<a
href="https://doi.org/10.1109/TPAMI.2023.3277738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the substantial progress of active learning for image recognition, there lacks a systematic investigation of instance-level active learning for object detection. In this paper, we propose to unify instance uncertainty calculation with image uncertainty estimation for informative image selection, creating a multiple instance differentiation learning (MIDL) method for instance-level active learning. MIDL consists of a classifier prediction differentiation module and a multiple instance differentiation module. The former leverages two adversarial instance classifiers trained on the labeled and unlabeled sets to estimate instance uncertainty of the unlabeled set. The latter treats unlabeled images as instance bags and re-estimates image-instance uncertainty using the instance classification model in a multiple instance learning fashion. Through weighting the instance uncertainty using instance class probability and instance objectness probability under the total probability formula, MIDL unifies the image uncertainty with instance uncertainty in the Bayesian theory framework. Extensive experiments validate that MIDL sets a solid baseline for instance-level active learning. On commonly used object detection datasets, it outperforms other state-of-the-art methods by significant margins, particularly when the labeled sets are small.},
  archive      = {J_TPAMI},
  author       = {Fang Wan and Qixiang Ye and Tianning Yuan and Songcen Xu and Jianzhuang Liu and Xiangyang Ji and Qingming Huang},
  doi          = {10.1109/TPAMI.2023.3277738},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12133-12147},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiple instance differentiation learning for active object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal learning with transformers: A survey.
<em>TPAMI</em>, <em>45</em>(10), 12113–12132. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
  archive      = {J_TPAMI},
  author       = {Peng Xu and Xiatian Zhu and David A. Clifton},
  doi          = {10.1109/TPAMI.2023.3275156},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12113-12132},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multimodal learning with transformers: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MO-MIX: Multi-objective multi-agent cooperative
decision-making with deep reinforcement learning. <em>TPAMI</em>,
<em>45</em>(10), 12098–12112. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (RL) has been applied extensively to solve complex decision-making problems. In many real-world scenarios, tasks often have several conflicting objectives and may require multiple agents to cooperate, which are the multi-objective multi-agent decision-making problems. However, only few works have been conducted on this intersection. Existing approaches are limited to separate fields and can only handle multi-agent decision-making with a single objective, or multi-objective decision-making with a single agent. In this paper, we propose MO-MIX to solve the multi-objective multi-agent reinforcement learning (MOMARL) problem. Our approach is based on the centralized training with decentralized execution (CTDE) framework. A weight vector representing preference over the objectives is fed into the decentralized agent network as a condition for local action-value function estimation, while a mixing network with parallel architecture is used to estimate the joint action-value function. In addition, an exploration guide approach is applied to improve the uniformity of the final non-dominated solutions. Experiments demonstrate that the proposed method can effectively solve the multi-objective multi-agent cooperative decision-making problem and generate an approximation of the Pareto set. Our approach not only significantly outperforms the baseline method in all four kinds of evaluation metrics, but also requires less computational cost.},
  archive      = {J_TPAMI},
  author       = {Tianmeng Hu and Biao Luo and Chunhua Yang and Tingwen Huang},
  doi          = {10.1109/TPAMI.2023.3283537},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12098-12112},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MO-MIX: Multi-objective multi-agent cooperative decision-making with deep reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MLink: Linking black-box models from multiple domains for
collaborative inference. <em>TPAMI</em>, <em>45</em>(10), 12085–12097.
(<a href="https://doi.org/10.1109/TPAMI.2023.3283780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g., smart city), we need inference results of multiple ML models, but the cost budget (e.g., GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking, which aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. We propose the design of model links which supports linking heterogeneous black-box ML models. Also, in order to address the distribution discrepancy challenge, we present adaptation and aggregation methods of model links. Based on our proposed model links, we developed a scheduling algorithm, named MLink . Through collaborative multi-model inference enabled by model links, MLink can improve the accuracy of obtained inference results under the cost budget. We evaluated MLink on a multi-modal dataset with seven different ML models and two real-world video analytics systems with six ML models and 3,264 hours of video. Experimental results show that our proposed model links can be effectively built among various black-box models. Under the budget of GPU memory, MLink can save 66.7\% inference computations while preserving 94\% inference accuracy, which outperforms multi-task learning, deep reinforcement learning-based scheduler and frame filtering baselines.},
  archive      = {J_TPAMI},
  author       = {Mu Yuan and Lan Zhang and Zimu Zheng and Yi-Nan Zhang and Xiang-Yang Li},
  doi          = {10.1109/TPAMI.2023.3283780},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12085-12097},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MLink: Linking black-box models from multiple domains for collaborative inference},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Making a bird AI expert work for you and me. <em>TPAMI</em>,
<em>45</em>(10), 12068–12084. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As powerful as fine-grained visual classification (FGVC) is, responding your query with a bird name of “Whip-poor-will” or “Mallard” probably does not make much sense. This however commonly accepted in the literature, underlines a fundamental question interfacing AI and human – what constitutes transferable knowledge for human to learn from AI? This paper sets out to answer this very question using FGVC as a test bed. Specifically, we envisage a scenario where a trained FGVC model (the AI expert) functions as a knowledge provider in enabling average people (you and me) to become better domain experts ourselves. Assuming an AI expert trained using expert human labels, we anchor our focus on asking and providing solutions for two questions: (i) what is the best transferable knowledge we can extract from AI, and (ii) what is the most practical means to measure the gains in expertise given that knowledge? We propose to represent knowledge as highly discriminative visual regions that are expert-exclusive and instantiate it via a novel multi-stage learning framework. A human study of 15,000 trials shows our method is able to consistently improve people of divergent bird expertise to recognise once unrecognisable birds. We further propose a crude but benchmarkable metric TEMI and therefore allow future efforts in this direction to be comparable to ours without the need of large-scale human studies.},
  archive      = {J_TPAMI},
  author       = {Dongliang Chang and Kaiyue Pang and Ruoyi Du and Yujun Tong and Yi-Zhe Song and Zhanyu Ma and Jun Guo},
  doi          = {10.1109/TPAMI.2023.3274593},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12068-12084},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Making a bird AI expert work for you and me},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Light field reconstruction via deep adaptive fusion of
hybrid lenses. <em>TPAMI</em>, <em>45</em>(10), 12050–12067. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the problem of reconstructing high-resolution light field (LF) images from hybrid lenses, including a high-resolution camera surrounded by multiple low-resolution cameras. The performance of existing methods is still limited, as they produce either blurry results on plain textured areas or distortions around depth discontinuous boundaries. To tackle this challenge, we propose a novel end-to-end learning-based approach, which can comprehensively utilize the specific characteristics of the input from two complementary and parallel perspectives. Specifically, one module regresses a spatially consistent intermediate estimation by learning a deep multidimensional and cross-domain feature representation, while the other module warps another intermediate estimation, which maintains the high-frequency textures, by propagating the information of the high-resolution view. We finally leverage the advantages of the two intermediate estimations adaptively via the learned confidence maps, leading to the final high-resolution LF image with satisfactory results on both plain textured areas and depth discontinuous boundaries. Besides, to promote the effectiveness of our method trained with simulated hybrid data on real hybrid data captured by a hybrid LF imaging system, we carefully design the network architecture and the training strategy. Extensive experiments on both real and simulated hybrid data demonstrate the significant superiority of our approach over state-of-the-art ones. To the best of our knowledge, this is the first end-to-end deep learning method for LF reconstruction from a real hybrid input. We believe our framework could potentially decrease the cost of high-resolution LF data acquisition and benefit LF data storage and transmission. The code will be publicly available at https://github.com/jingjin25/LFhybridSR-Fusion .},
  archive      = {J_TPAMI},
  author       = {Jing Jin and Mantang Guo and Junhui Hou and Hui Liu and Hongkai Xiong},
  doi          = {10.1109/TPAMI.2023.3287603},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12050-12067},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Light field reconstruction via deep adaptive fusion of hybrid lenses},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging commonsense for object localisation in partial
scenes. <em>TPAMI</em>, <em>45</em>(10), 12038–12049. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end solution to address the problem of object localisation in partial scenes, where we aim to estimate the position of an object in an unknown area given only a partial 3D scan of the scene. We propose a novel scene representation to facilitate the geometric reasoning, Directed Spatial Commonsense Graph (D-SCG), a spatial scene graph that is enriched with additional concept nodes from a commonsense knowledge base. Specifically, the nodes of D-SCG represent the scene objects and the edges are their relative positions. Each object node is then connected via different commonsense relationships to a set of concept nodes. With the proposed graph-based scene representation, we estimate the unknown position of the target object using a Graph Neural Network that implements a sparse attentional message passing mechanism. The network first predicts the relative positions between the target object and each visible object by learning a rich representation of the objects via aggregating both the object nodes and the concept nodes in D-SCG. These relative positions then are merged to obtain the final position. We evaluate our method using Partial ScanNet, improving the state-of-the-art by 5.9\% in terms of the localisation accuracy at a 8x faster training speed.},
  archive      = {J_TPAMI},
  author       = {Francesco Giuliari and Geri Skenderi and Marco Cristani and Alessio Del Bue and Yiming Wang},
  doi          = {10.1109/TPAMI.2023.3272523},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12038-12049},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Leveraging commonsense for object localisation in partial scenes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning to infer unseen single-/ multi-attribute-object
compositions with graph networks. <em>TPAMI</em>, <em>45</em>(10),
12022–12037. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the unseen attribute-object composition is critical to make machines learn to decompose and compose complex concepts like people. Most existing methods are limited to the composition recognition of single-attribute-object, and can hardly learn relations between the attributes and objects. In this paper, we propose an attribute-object semantic association graph model to learn the complex relations and enable knowledge transfer between primitives. With nodes representing attributes and objects, the graph can be constructed flexibly, which realizes both single- and multi-attribute-object composition recognition. In order to reduce mis-classifications of similar compositions (e.g., scratched screen and broken screen ), driven by the contrastive loss, the anchor image feature is pulled closer to the corresponding label feature and pushed away from other negative label features. Specifically, a novel balance loss is proposed to alleviate the domain bias, where a model prefers to predict seen compositions. In addition, we build a large-scale Multi-Attribute Dataset (MAD) with 116,099 images and 8,030 label categories for inferring unseen multi-attribute-object compositions. Along with MAD, we propose two novel metrics Hard and Soft to give a comprehensive evaluation in the multi-attribute setting. Experiments on MAD and two other single-attribute-object benchmarks (MIT-States and UT-Zappos50K) demonstrate the effectiveness of our approach.},
  archive      = {J_TPAMI},
  author       = {Hui Chen and Jingjing Jiang and Nanning Zheng},
  doi          = {10.1109/TPAMI.2023.3273712},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12022-12037},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to infer unseen single-/ multi-attribute-object compositions with graph networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning probabilistic coordinate fields for robust
correspondences. <em>TPAMI</em>, <em>45</em>(10), 12004–12021. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Probabilistic Coordinate Fields (PCFs), a novel geometric-invariant coordinate representation for image correspondence problems. In contrast to standard Cartesian coordinates, PCFs encode coordinates in correspondence-specific barycentric coordinate systems (BCS) with affine invariance. To know when and where to trust the encoded coordinates, we implement PCFs in a probabilistic network termed PCF-Net, which parameterizes the distribution of coordinate fields as Gaussian mixture models. By jointly optimizing coordinate fields and their confidence conditioned on dense flows, PCF-Net can work with various feature descriptors when quantifying the reliability of PCFs by confidence maps. An interesting observation of this work is that the learned confidence map converges to geometrically coherent and semantically consistent regions, which facilitates robust coordinate representation. By delivering the confident coordinates to keypoint/feature descriptors, we show that PCF-Net can be used as a plug-in to existing correspondence-dependent approaches. Extensive experiments on both indoor and outdoor datasets suggest that accurate geometric invariant coordinates help to achieve the state of the art in several correspondence problems, such as sparse feature matching, dense image registration, camera pose estimation, and consistency filtering. Further, the interpretable confidence map predicted by PCF-Net can also be leveraged to other novel applications from texture transfer to multi-homography classification.},
  archive      = {J_TPAMI},
  author       = {Weiyue Zhao and Hao Lu and Xinyi Ye and Zhiguo Cao and Xin Li},
  doi          = {10.1109/TPAMI.2023.3284487},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {12004-12021},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning probabilistic coordinate fields for robust correspondences},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning hidden graphs from samples. <em>TPAMI</em>,
<em>45</em>(10), 11993–12003. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several real-world problems, like molecular biology and chemical reactions, have hidden graphs, and we need to learn the hidden graph using edge-detecting samples. In this problem, the learner receives examples explaining whether a set of vertices induces an edge of the hidden graph. This article examines the learnability of this problem using the PAC and Agnostic PAC learning models. By computing the VC-dimension of hypothesis spaces of hidden graphs, hidden trees, hidden connected graphs, and hidden planar graphs through edge-detecting samples, we also find the sample complexity of learning these spaces. We study the learnability of this space of hidden graphs in two cases, namely for known and unknown vertex sets. We show that the class of hidden graphs is uniformly learnable when the vertex set is known. Furthermore, we prove that the family of hidden graphs is not uniformly learnable but is nonuniformly learnable when the vertex set is unknown.},
  archive      = {J_TPAMI},
  author       = {Ahmad Abniki and Hamid Beigy},
  doi          = {10.1109/TPAMI.2023.3283784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11993-12003},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning hidden graphs from samples},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning an invariant and equivariant network for weakly
supervised object detection. <em>TPAMI</em>, <em>45</em>(10),
11977–11992. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Object Detection (WSOD) is of increasing importance in the community of computer vision as its extensive applications and low manual cost. Most of the advanced WSOD approaches build upon an indefinite and quality-agnostic framework, leading to unstable and incomplete object detectors. This paper attributes these issues to the process of inconsistent learning for object variations and the unawareness of localization quality and constructs a novel end-to-end Invariant and Equivariant Network (IENet). It is implemented with a flexible multi-branch online refinement, to be naturally more comprehensive-perceptive against various objects. Specifically, IENet first performs label propagation from the predicted instances to their transformed ones in a progressive manner, achieving affine-invariant learning. Meanwhile, IENet also naturally utilizes rotation-equivariant learning as a pretext task and derives an instance-level rotation-equivariant branch to be aware of the localization quality. With affine-invariance learning and rotation-equivariant learning, IENet urges consistent and holistic feature learning for WSOD without additional annotations. On the challenging datasets of both natural scenes and aerial scenes, we substantially boost WSOD to new state-of-the-art performance. The codes have been released at: https://github.com/XiaoxFeng/IENet .},
  archive      = {J_TPAMI},
  author       = {Xiaoxu Feng and Xiwen Yao and Hui Shen and Gong Cheng and Bin Xiao and Junwei Han},
  doi          = {10.1109/TPAMI.2023.3275142},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11977-11992},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning an invariant and equivariant network for weakly supervised object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LARNeXt: End-to-end lie algebra residual network for face
recognition. <em>TPAMI</em>, <em>45</em>(10), 11961–11976. (<a
href="https://doi.org/10.1109/TPAMI.2023.3279378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has always been courted in computer vision and is especially amenable to situations with significant variations between frontal and profile faces. Traditional techniques make great strides either by synthesizing frontal faces from sizable datasets or by empirical pose invariant learning. In this paper, we propose a completely integrated embedded end-to-end Lie algebra residual architecture (LARNeXt) to achieve pose robust face recognition. First, we explore how the face rotation in the 3D space affects the deep feature generation process of convolutional neural networks (CNNs), and prove that face rotation in the image space is equivalent to an additive residual component in the feature space of CNNs, which is determined solely by the rotation. Second, on the basis of this theoretical finding, we further design three critical subnets to leverage a soft regression subnet with novel multi-fusion attention feature aggregation for efficient pose estimation, a residual subnet for decoding rotation information from input face images, and a gating subnet to learn rotation magnitude for controlling the strength of the residual component that contributes to the feature learning process. Finally, we conduct a large number of ablation experiments, and our quantitative and visualization results both corroborate the credibility of our theory and corresponding network designs. Our comprehensive experimental evaluations on frontal-profile face datasets, general unconstrained face recognition datasets, and industrial-grade tasks demonstrate that our method consistently outperforms the state-of-the-art ones.},
  archive      = {J_TPAMI},
  author       = {Xiaolong Yang and Xiaohong Jia and Dihong Gong and Dong-Ming Yan and Zhifeng Li and Wei Liu},
  doi          = {10.1109/TPAMI.2023.3279378},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11961-11976},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LARNeXt: End-to-end lie algebra residual network for face recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-based embodied question answering. <em>TPAMI</em>,
<em>45</em>(10), 11948–11960. (<a
href="https://doi.org/10.1109/TPAMI.2023.3277206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Knowledge-based Embodied Question Answering (K-EQA) task, in which the agent intelligently explores the environment to answer various questions with the knowledge. Different from explicitly specifying the target object in the question as existing EQA work, the agent can resort to external knowledge to understand more complicated question such as “Please tell me what are objects used to cut food in the room?”, in which the agent must know the knowledge such as “knife is used for cutting food”. To address this K-EQA problem, a novel framework based on neural program synthesis reasoning is proposed, where the joint reasoning of the external knowledge and 3D scene graph is performed to realize navigation and question answering. Especially, the 3D scene graph can provide the memory to store the visual information of visited scenes, which significantly improves the efficiency for the multi-turn question answering. Experimental results have demonstrated that the proposed framework is capable of answering more complicated and realistic questions in the embodied environment. The proposed method is also applicable to multi-agent scenarios.},
  archive      = {J_TPAMI},
  author       = {Sinan Tan and Mengmeng Ge and Di Guo and Huaping Liu and Fuchun Sun},
  doi          = {10.1109/TPAMI.2023.3277206},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11948-11960},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Knowledge-based embodied question answering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inherit with distillation and evolve with contrast:
Exploring class incremental semantic segmentation without exemplar
memory. <em>TPAMI</em>, <em>45</em>(10), 11932–11947. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a front-burner problem in incremental learning, class incremental semantic segmentation (CISS) is plagued by catastrophic forgetting and semantic drift. Although recent methods have utilized knowledge distillation to transfer knowledge from the old model, they are still unable to avoid pixel confusion, which results in severe misclassification after incremental steps due to the lack of annotations for past and future classes. Meanwhile data-replay-based approaches suffer from storage burdens and privacy concerns. In this paper, we propose to address CISS without exemplar memory and resolve catastrophic forgetting as well as semantic drift synchronously. We present Inherit with Distillation and Evolve with Contrast (IDEC), which consists of a Dense Knowledge Distillation on all Aspects (DADA) manner and an Asymmetric Region-wise Contrastive Learning (ARCL) module. Driven by the devised dynamic class-specific pseudo-labelling strategy, DADA distils intermediate-layer features and output-logits collaboratively with more emphasis on semantic-invariant knowledge inheritance. ARCL implements region-wise contrastive learning in the latent space to resolve semantic drift among known classes, current classes, and unknown classes. We demonstrate the effectiveness of our method on multiple CISS tasks by state-of-the-art performance, including Pascal VOC 2012, ADE20K and ISPRS datasets. Our method also shows superior anti-forgetting ability, particularly in multi-step CISS tasks.},
  archive      = {J_TPAMI},
  author       = {Danpei Zhao and Bo Yuan and Zhenwei Shi},
  doi          = {10.1109/TPAMI.2023.3273574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11932-11947},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Inherit with distillation and evolve with contrast: Exploring class incremental semantic segmentation without exemplar memory},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influence-driven data poisoning for robust recommender
systems. <em>TPAMI</em>, <em>45</em>(10), 11915–11931. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that recommender systems are vulnerable, and it is easy for attackers to inject well-designed malicious profiles into the system, resulting in biased recommendations. We cannot deprive these data&#39;s injection right and deny their existence&#39;s rationality, making it imperative to study recommendation robustness. Despite impressive emerging work, threat assessment of the bi-level poisoning problem and the imperceptibility of poisoning users remain key challenges to be solved. To this end, we propose Infmix, an efficient poisoning attack strategy. Specifically, Infmix consists of an influence-based threat estimator and a user generator, Usermix. First, the influence-based estimator can efficiently evaluate the user&#39;s harm to the recommender system without retraining, which is challenging for existing attacks. Second, Usermix, a distribution-agnostic generator, can generate unnoticeable fake data even with a few known users. Under the guidance of the threat estimator, Infmix can select the users with large attacking impacts from the quasi-real candidates generated by Usermix. Extensive experiments demonstrate Infmix&#39;s superiority by attacking six recommendation systems with four real datasets. Additionally, we propose a novel defense strategy, adversarial poisoning training (APT). It mimics the poisoning process by injecting fake users (ERM users) committed to minimizing empirical risk to build a robust system. Similar to Infmix, we also utilize the influence function to solve the bi-level optimization challenge of generating ERM users. Although the idea of “fighting fire with fire” in APT seems counterintuitive, we prove its effectiveness in improving recommendation robustness through theoretical analysis and empirical experiments.},
  archive      = {J_TPAMI},
  author       = {Chenwang Wu and Defu Lian and Yong Ge and Zhihao Zhu and Enhong Chen},
  doi          = {10.1109/TPAMI.2023.3274759},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11915-11931},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Influence-driven data poisoning for robust recommender systems},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human interaction understanding with consistency-aware
learning. <em>TPAMI</em>, <em>45</em>(10), 11898–11914. (<a
href="https://doi.org/10.1109/TPAMI.2023.3280906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the progress made on human activity classification, much less success has been achieved on human interaction understanding (HIU). Apart from the latter task is much more challenging, the main causation is that recent approaches learn human interactive relations via shallow graphical representations, which are inadequate to model complicated human interactive-relations. This paper proposes a deep consistency-aware framework aiming at tackling the grouping and labelling inconsistencies in HIU. This framework consists of three components, including a backbone CNN to extract image features, a factor graph network to implicitly learn higher-order consistencies among labelling and grouping variables, and a consistency-aware reasoning module to explicitly enforcing consistencies. The last module is inspired by our key observation that the consistency-aware reasoning bias can be embedded into an energy function or a particular loss function, minimizing which delivers consistent predictions. An efficient mean-field inference algorithm is proposed, such that all modules of our network could be trained in an end-to-end fashion. Experimental results demonstrate that the two proposed consistency-learning modules complement each other, and both make considerable contributions in achieving leading performance on three benchmarks of HIU. The effectiveness of the proposed approach is further validated by experiments on detecting human-object interactions.},
  archive      = {J_TPAMI},
  author       = {Jiajun Meng and Zhenhua Wang and Kaining Ying and Jianhua Zhang and Dongyan Guo and Zhen Zhang and Javen Qinfeng Shi and Shengyong Chen},
  doi          = {10.1109/TPAMI.2023.3280906},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11898-11914},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human interaction understanding with consistency-aware learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HRegNet: A hierarchical network for efficient and accurate
outdoor LiDAR point cloud registration. <em>TPAMI</em>, <em>45</em>(10),
11884–11897. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a fundamental problem in 3D computer vision. Outdoor LiDAR point clouds are typically large-scale and complexly distributed, which makes the registration challenging. In this paper, we propose an efficient hierarchical network named HRegNet for large-scale outdoor LiDAR point cloud registration. Instead of using all points in the point clouds, HRegNet performs registration on hierarchically extracted keypoints and descriptors. The overall framework combines the reliable features in deeper layer and the precise position information in shallower layers to achieve robust and precise registration. We present a correspondence network to generate correct and accurate keypoints correspondences. Moreover, bilateral consensus and neighborhood consensus are introduced for keypoints matching, and novel similarity features are designed to incorporate them into the correspondence network, which significantly improves the registration performance. In addition, we design a consistency propagation strategy to effectively incorporate spatial consistency into the registration pipeline. The whole network is also highly efficient since only a small number of keypoints are used for registration. Extensive experiments are conducted on three large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HRegNet. The source code of the proposed HRegNet is available at https://github.com/ispc-lab/HRegNet2 .},
  archive      = {J_TPAMI},
  author       = {Fan Lu and Guang Chen and Yinlong Liu and Lijun Zhang and Sanqing Qu and Shu Liu and Rongqi Gu and Changjun Jiang},
  doi          = {10.1109/TPAMI.2023.3284896},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11884-11897},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HRegNet: A hierarchical network for efficient and accurate outdoor LiDAR point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GP-UNIT: Generative prior for versatile unsupervised
image-to-image translation. <em>TPAMI</em>, <em>45</em>(10),
11869–11883. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have witnessed many successful unsupervised image-to-image translation models that learn correspondences between two visual domains without paired data. However, it is still a great challenge to build robust mappings between various domains especially for those with drastic visual discrepancies. In this paper, we introduce a novel versatile framework, Generative Prior-guided UNsupervised Image-to-image Translation ( GP-UNIT ), that improves the quality, applicability and controllability of the existing translation models. The key idea of GP-UNIT is to distill the generative prior from pre-trained class-conditional GANs to build coarse-level cross-domain correspondences, and to apply the learned prior to adversarial translations to excavate fine-level correspondences. With the learned multi-level content correspondences, GP-UNIT is able to perform valid translations between both close domains and distant domains. For close domains, GP-UNIT can be conditioned on a parameter to determine the intensity of the content correspondences during translation, allowing users to balance between content and style consistency. For distant domains, semi-supervised learning is explored to guide GP-UNIT to discover accurate semantic correspondences that are hard to learn solely from the appearance. We validate the superiority of GP-UNIT over state-of-the-art translation models in robust, high-quality and diversified translations between various domains through extensive experiments.},
  archive      = {J_TPAMI},
  author       = {Shuai Yang and Liming Jiang and Ziwei Liu and Chen Change Loy},
  doi          = {10.1109/TPAMI.2023.3284003},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11869-11883},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GP-UNIT: Generative prior for versatile unsupervised image-to-image translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAIA-universe: Everything is super-netify. <em>TPAMI</em>,
<em>45</em>(10), 11856–11868. (<a
href="https://doi.org/10.1109/TPAMI.2023.3276392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training on large-scale datasets has played an increasingly significant role in computer vision and natural language processing recently. However, as there exist numerous application scenarios that have distinctive demands such as certain latency constraints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. we focus on two fundamental perception tasks (object detection and semantic segmentation) and present a complete and flexible system named GAIA-Universe(GAIA), which could automatically and efficiently give birth to customized solutions according to heterogeneous downstream needs through data union and super-net training. GAIA is capable of providing powerful pre-trained weights and searching models that conform to downstream demands such as hardware constraints, computation constraints, specified data domains, and telling relevant data for practitioners who have very few datapoints on their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open Images, BDD100 k, and UODB which is a collection of datasets including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as an example, GAIA is able to efficiently produce models covering a wide range of latency from 16 ms to 53 ms, and yields AP from 38.2 to 46.5 without whistles and bells. GAIA is released at https://github.com/GAIA-vision .},
  archive      = {J_TPAMI},
  author       = {Junran Peng and Qing Chang and Haoran Yin and Xingyuan Bu and Jiajun Sun and Lingxi Xie and Xiaopeng Zhang and Qi Tian and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2023.3276392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11856-11868},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GAIA-universe: Everything is super-netify},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fixed pattern noise removal based on a semi-calibration
method. <em>TPAMI</em>, <em>45</em>(10), 11842–11855. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the manufacturing imperfections, nonuniformities are ubiquitous in digital sensors, causing the notorious Fixed Pattern Noise (FPN). The ability of modern digital cameras to take images under low-light environments is severely limited by the FPN. This paper proposes a novel semi-calibration-based method for the FPN removal that utilizes a pre-calibrated Noise Pattern . The key observation of this work is that the FPN in each shot is actually a scaled Noise Pattern with an unknown scale parameter, since each pixel in the array generates a characteristic amount of dark current which is fundamentally determined by its physical properties. Given a noised image and the corresponding Noise Pattern , the scale parameter is automatically estimated, and then the FPN is removed by subtracting the scaled Noise Pattern from the noised image. The estimation of the scale parameter is based on an entropy minimization estimator, which is derived from the Maximum Likelihood principle and is further justified by subsequent analysis that minimizing the entropy uniquely identifies the true parameter. Convergence issues, as well as the optimality of the proposed estimator, are also theoretically discussed. Finally, some applications are given, illustrating the performance of the proposed FPN removal method in real-world tasks.},
  archive      = {J_TPAMI},
  author       = {Lingfei Song and Hua Huang},
  doi          = {10.1109/TPAMI.2023.3274826},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11842-11855},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fixed pattern noise removal based on a semi-calibration method},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot partial multi-view learning. <em>TPAMI</em>,
<em>45</em>(10), 11824–11841. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often the case that data are with multiple views in real-world applications. Fully exploring the information of each view is significant for making data more representative. However, due to various limitations and failures in data collection and pre-processing, it is inevitable for real data to suffer from view missing and data scarcity. The coexistence of these two issues makes it more challenging to achieve the pattern classification task. Currently, to our best knowledge, few appropriate methods can well-handle these two issues simultaneously. Aiming to draw more attention from the community to this challenge, we propose a new task in this paper, called few-shot partial multi-view learning, which focuses on overcoming the negative impact of the view-missing issue in the low-data regime. The challenges of this task are twofold: (i) it is difficult to overcome the impact of data scarcity under the interference of missing views; (ii) the limited number of data exacerbates information scarcity, thus making it harder to address the view-missing issue in turn. To address these challenges, we propose a new unified Gaussian dense-anchoring method. The unified dense anchors are learned for the limited partial multi-view data, thereby anchoring them into a unified dense representation space where the influence of data scarcity and view missing can be alleviated. We conduct extensive experiments to evaluate our method. The results on Cub-googlenet-doc2vec, Handwritten, Caltech102, Scene15, Animal, ORL, tieredImagenet, and Birds-200-2011 datasets validate its effectiveness. The codes will be released at https://github.com/zhouyuan888888/UGDA .},
  archive      = {J_TPAMI},
  author       = {Yuan Zhou and Yanrong Guo and Shijie Hao and Richang Hong and Jiebo Luo},
  doi          = {10.1109/TPAMI.2023.3275162},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11824-11841},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Few-shot partial multi-view learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot multi-agent perception with ranking-based feature
learning. <em>TPAMI</em>, <em>45</em>(10), 11810–11823. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on performing few-shot learning (FSL) under multi-agent scenarios in which participating agents only have scarce labeled data and need to collaborate to predict labels of query observations. We aim at designing a coordination and learning framework in which multiple agents, such as drones and robots, can collectively perceive the environment accurately and efficiently under limited communication and computation conditions. We propose a metric-based multi-agent FSL framework which has three main components: an efficient communication mechanism that propagates compact and fine-grained query feature maps from query agents to support agents; an asymmetric attention mechanism that computes region-level attention weights between query and support feature maps; and a metric-learning module which calculates the image-level relevance between query and support data fast and accurately. Furthermore, we propose a specially designed ranking-based feature learning module, which can fully utilize the order information of training data by maximizing the inter-class distance, while minimizing the intra-class distance explicitly. We perform extensive numerical studies and demonstrate that our approach can achieve significantly improved accuracy in visual and acoustic perception tasks such as face identification, semantic segmentation, and sound genre recognition, consistently outperforming the state-of-the-art baselines by 5\%–20\%.},
  archive      = {J_TPAMI},
  author       = {Chenyou Fan and Junjie Hu and Jianwei Huang},
  doi          = {10.1109/TPAMI.2023.3285755},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11810-11823},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Few-shot multi-agent perception with ranking-based feature learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Fast-SNARF: A fast deformer for articulated neural fields.
<em>TPAMI</em>, <em>45</em>(10), 11796–11809. (<a
href="https://doi.org/10.1109/TPAMI.2023.3271569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of rigid scenes. A key challenge in making such methods applicable to articulated objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of $150\times$ . These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans.},
  archive      = {J_TPAMI},
  author       = {Xu Chen and Tianjian Jiang and Jie Song and Max Rietmann and Andreas Geiger and Michael J. Black and Otmar Hilliges},
  doi          = {10.1109/TPAMI.2023.3271569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11796-11809},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast-SNARF: A fast deformer for articulated neural fields},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast rolling shutter correction in the wild. <em>TPAMI</em>,
<em>45</em>(10), 11778–11795. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of rolling shutter correction (RSC) in uncalibrated videos. Existing works remove rolling shutter (RS) distortion by explicitly computing the camera motion and depth as intermediate products, followed by motion compensation. In contrast, we first show that each distorted pixel can be implicitly rectified back to the corresponding global shutter (GS) projection by rescaling its optical flow. Such a point-wise RSC is feasible with both perspective and non-perspective cases without the pre-knowledge of the camera used. Besides, it allows a pixel-wise varying direct RS correction (DRSC) framework that handles locally varying distortion caused by various sources, such as camera motion, moving objects, and even highly varying depth scenes. More importantly, our approach is an efficient CPU-based solution that enables undistorting RS videos in real-time (40fps for 480p). We evaluate our approach across a broad range of cameras and video sequences, including fast motion, dynamic scenes, and non-perspective lenses, demonstrating the superiority of our proposed approach over state-of-the-art methods in both effectiveness and efficiency. We also evaluated the ability of the RSC results to serve for downstream 3D analysis, such as visual odometry and structure-from-motion, which verifies preference for the output of our algorithm over other existing RSC methods.},
  archive      = {J_TPAMI},
  author       = {Delin Qu and Bangyan Liao and Huiqing Zhang and Omar Ait-Aider and Yizhen Lao},
  doi          = {10.1109/TPAMI.2023.3284847},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11778-11795},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast rolling shutter correction in the wild},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental design for overparameterized learning with
application to single shot deep active learning. <em>TPAMI</em>,
<em>45</em>(10), 11766–11777. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impressive performance exhibited by modern machine learning models hinges on the ability to train such models on a very large amounts of labeled data. However, since access to large volumes of labeled data is often limited or expensive, it is desirable to alleviate this bottleneck by carefully curating the training set. Optimal experimental design is a well-established paradigm for selecting data point to be labeled so to maximally inform the learning process. Unfortunately, classical theory on optimal experimental design focuses on selecting examples in order to learn underparameterized (and thus, non-interpolative) models, while modern machine learning models such as deep neural networks are overparameterized, and oftentimes are trained to be interpolative. As such, classical experimental design methods are not applicable in many modern learning setups. Indeed, the predictive performance of underparameterized models tends to be variance dominated, so classical experimental design focuses on variance reduction, while the predictive performance of overparameterized models can also be, as is shown in this paper, bias dominated or of mixed nature. In this paper we propose a design strategy that is well suited for overparameterized regression and interpolation, and we demonstrate the applicability of our method in the context of deep learning by proposing a new algorithm for single shot deep active learning.},
  archive      = {J_TPAMI},
  author       = {Neta Shoham and Haim Avron},
  doi          = {10.1109/TPAMI.2023.3287042},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11766-11777},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Experimental design for overparameterized learning with application to single shot deep active learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Earning extra performance from restrictive feedbacks.
<em>TPAMI</em>, <em>45</em>(10), 11753–11765. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine learning applications encounter situations where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named Earning eXtra PerformancE from restriCTive feEDdbacks (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods where the target data is always ready for calculating model gradients, the model providers in EXPECTED only see some feedbacks which could be as simple as scalars, such as inference accuracy or usage rate. To enable tuning in this restrictive circumstance, we propose to characterize the geometry of the model performance with regard to model parameters through exploring the parameters’ distribution. In particular, for deep models whose parameters distribute across multiple layers, a more query-efficient algorithm is further tailor-designed that conducts layerwise tuning with more attention to those layers which pay off better. Our theoretical analyses justify the proposed algorithms from the aspects of both efficacy and efficiency. Extensive experiments on different applications demonstrate that our work forges a sound solution to the EXPECTED problem, which establishes the foundation for future studies towards this direction.},
  archive      = {J_TPAMI},
  author       = {Jing Li and Yuangang Pan and Yueming Lyu and Yinghua Yao and Yulei Sui and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2023.3273249},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11753-11765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Earning extra performance from restrictive feedbacks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual compensation residual networks for class imbalanced
learning. <em>TPAMI</em>, <em>45</em>(10), 11733–11752. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning generalizable representation and classifier for class-imbalanced data is challenging for data-driven deep models. Most studies attempt to re-balance the data distribution, which is prone to overfitting on tail classes and underfitting on head classes. In this work, we propose Dual Compensation Residual Networks to better fit both tail and head classes. First, we propose dual Feature Compensation Module (FCM) and Logit Compensation Module (LCM) to alleviate the overfitting issue. The design of these two modules is based on the observation: an important factor causing overfitting is that there is severe feature drift between training and test data on tail classes. In details, the test features of a tail category tend to drift towards feature cloud of multiple similar head categories. So FCM estimates a multi-mode feature drift direction for each tail category and compensate for it. Furthermore, LCM translates the deterministic feature drift vector estimated by FCM along intra-class variations, so as to cover a larger effective compensation space, thereby better fitting the test features. Second, we propose a Residual Balanced Multi-Proxies Classifier (RBMC) to alleviate the under-fitting issue. Motivated by the observation that re-balancing strategy hinders the classifier from learning sufficient head knowledge and eventually causes underfitting, RBMC utilizes uniform learning with a residual path to facilitate classifier learning. Comprehensive experiments on Long-tailed and Class-Incremental benchmarks validate the efficacy of our method.},
  archive      = {J_TPAMI},
  author       = {Ruibing Hou and Hong Chang and Bingpeng Ma and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2023.3275585},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11733-11752},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual compensation residual networks for class imbalanced learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual adaptive representation alignment for cross-domain
few-shot learning. <em>TPAMI</em>, <em>45</em>(10), 11720–11732. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel queries with limited support samples by learning from base knowledge. Recent progress in this setting assumes that the base knowledge and novel query samples are distributed in the same domains, which are usually infeasible for realistic applications. Toward this issue, we propose to address the cross-domain few-shot learning problem where only extremely few samples are available in target domains. Under this realistic setting, we focus on the fast adaptation capability of meta-learners by proposing an effective dual adaptive representation alignment approach. In our approach, a prototypical feature alignment is first proposed to recalibrate support instances as prototypes and reproject these prototypes with a differentiable closed-form solution. Therefore feature spaces of learned knowledge can be adaptively transformed to query spaces by the cross-instance and cross-prototype relations. Besides the feature alignment, we further present a normalized distribution alignment module, which exploits prior statistics of query samples for solving the covariant shifts among the support and query samples. With these two modules, a progressive meta-learning framework is constructed to perform the fast adaptation with extremely few-shot samples while maintaining its generalization capabilities. Experimental evidence demonstrates our approach achieves new state-of-the-art results on 4 CDFSL benchmarks and 4 fine-grained cross-domain benchmarks.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhao and Tong Zhang and Jia Li and Yonghong Tian},
  doi          = {10.1109/TPAMI.2023.3272697},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11720-11732},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual adaptive representation alignment for cross-domain few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-scalable unpaired image translation via latent space
anchoring. <em>TPAMI</em>, <em>45</em>(10), 11707–11719. (<a
href="https://doi.org/10.1109/TPAMI.2023.3287774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired image-to-image translation (UNIT) aims to map images between two visual domains without paired training data. However, given a UNIT model trained on certain domains, it is difficult for current methods to incorporate new domains because they often need to train the full model on both existing and new domains. To address this problem, we propose a new domain-scalable UNIT method, termed as latent space anchoring , which can be efficiently extended to new visual domains and does not need to fine-tune encoders and decoders of existing domains. Our method anchors images of different domains to the same latent space of frozen GANs by learning lightweight encoder and regressor models to reconstruct single-domain images. In the inference phase, the learned encoders and decoders of different domains can be arbitrarily combined to translate images between any two domains without fine-tuning. Experiments on various datasets show that the proposed method achieves superior performance on both standard and domain-scalable UNIT tasks in comparison with the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Siyu Huang and Jie An and Donglai Wei and Zudi Lin and Jiebo Luo and Hanspeter Pfister},
  doi          = {10.1109/TPAMI.2023.3287774},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11707-11719},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Domain-scalable unpaired image translation via latent space anchoring},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diverse sample generation: Pushing the limit of generative
data-free quantization. <em>TPAMI</em>, <em>45</em>(10), 11689–11706.
(<a href="https://doi.org/10.1109/TPAMI.2023.3272925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative data-free quantization emerges as a practical compression approach that quantizes deep neural networks to low bit-width without accessing the real data. This approach generates data utilizing batch normalization (BN) statistics of the full-precision networks to quantize the networks. However, it always faces the serious challenges of accuracy degradation in practice. We first give a theoretical analysis that the diversity of synthetic samples is crucial for the data-free quantization, while in existing approaches, the synthetic data completely constrained by BN statistics experimentally exhibit severe homogenization at distribution and sample levels. This paper presents a generic D iverse S ample G eneration ( DSG ) scheme for the generative data-free quantization, to mitigate detrimental homogenization. We first slack the statistics alignment for features in the BN layer to relax the distribution constraint. Then, we strengthen the loss impact of the specific BN layers for different samples and inhibit the correlation among samples in the generation process, to diversify samples from the statistical and spatial perspectives, respectively. Comprehensive experiments show that for large-scale image classification tasks, our DSG can consistently quantization performance on different neural architectures, especially under ultra-low bit-width. And data diversification caused by our DSG brings a general gain to various quantization-aware training and post-training quantization approaches, demonstrating its generality and effectiveness.},
  archive      = {J_TPAMI},
  author       = {Haotong Qin and Yifu Ding and Xiangguo Zhang and Jiakai Wang and Xianglong Liu and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3272925},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11689-11706},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Diverse sample generation: Pushing the limit of generative data-free quantization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discourse-aware graph networks for textual logical
reasoning. <em>TPAMI</em>, <em>45</em>(10), 11668–11688. (<a
href="https://doi.org/10.1109/TPAMI.2023.3280178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonability of the logical structures built in DAGNs and the effectiveness of the learned logic features. Moreover, zero-shot transfer results show the features’ generality to unseen logical texts.},
  archive      = {J_TPAMI},
  author       = {Yinya Huang and Lemao Liu and Kun Xu and Meng Fang and Liang Lin and Xiaodan Liang},
  doi          = {10.1109/TPAMI.2023.3280178},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11668-11688},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discourse-aware graph networks for textual logical reasoning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable logic policy for interpretable deep
reinforcement learning: A study from an optimization perspective.
<em>TPAMI</em>, <em>45</em>(10), 11654–11667. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretability of policies remains an important challenge in Deep Reinforcement Learning (DRL). This paper explores interpretable DRL via representing policy by Differentiable Inductive Logic Programming (DILP) and provides a theoretical and empirical study of DILP-based policy learning from an optimization perspective. We first identified a fundamental fact that DILP-based policy learning should be solved as a constrained policy optimization problem. We then proposed to use Mirror Descent for policy optimization (MDPO) to deal with the constraints of DILP-based policies. We derived the closed-form regret bound of MDPO with function approximation, which is helpful to the design of DRL frameworks. Moreover, we studied the convexity of DILP-based policy to further verify the benefits gained from MDPO. Empirically, we experimented MDPO, its on-policy variant, and 3 mainstream policy learning methods, and the results verified our theoretical analysis.},
  archive      = {J_TPAMI},
  author       = {Xin Li and Haojie Lei and Li Zhang and Mingzhong Wang},
  doi          = {10.1109/TPAMI.2023.3285634},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11654-11667},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentiable logic policy for interpretable deep reinforcement learning: A study from an optimization perspective},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable histogram loss functions for intensity-based
image-to-image translation. <em>TPAMI</em>, <em>45</em>(10),
11642–11653. (<a
href="https://doi.org/10.1109/TPAMI.2023.3278287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the HueNet - a novel deep learning framework for a differentiable construction of intensity (1D) and joint (2D) histograms and present its applicability to paired and unpaired image-to-image translation problems. The key idea is an innovative technique for augmenting a generative neural network by histogram layers appended to the image generator. These histogram layers allow us to define two new histogram-based loss functions for constraining the structural appearance of the synthesized output image and its color distribution. Specifically, the color similarity loss is defined by the Earth Mover&#39;s Distance between the intensity histograms of the network output and a color reference image. The structural similarity loss is determined by the mutual information between the output and a content reference image based on their joint histogram. Although the HueNet can be applied to a variety of image-to-image translation problems, we chose to demonstrate its strength on the tasks of color transfer, exemplar-based image colorization, and edges $\to$ photo, where the colors of the output image are predefined. The code is available at https://github.com/mor-avi-aharon-bgu/HueNet.git .},
  archive      = {J_TPAMI},
  author       = {Mor Avi-Aharon and Assaf Arbelle and Tammy Riklin Raviv},
  doi          = {10.1109/TPAMI.2023.3278287},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11642-11653},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentiable histogram loss functions for intensity-based image-to-image translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Cross-modal causal relational reasoning for event-level
visual question answering. <em>TPAMI</em>, <em>45</em>(10), 11624–11641.
(<a href="https://doi.org/10.1109/TPAMI.2023.3284038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing visual question answering methods often suffer from cross-modal spurious correlations and oversimplified event-level reasoning processes that fail to capture event temporality, causality, and dynamics spanning over the video. In this work, to address the task of event-level visual question answering, we propose a framework for cross-modal causal relational reasoning. In particular, a set of causal intervention operations is introduced to discover the underlying causal structures across visual and linguistic modalities. Our framework, named C ross- M odal C ausal Relat I onal R easoning (CMCIR), involves three modules: i) Causality-aware Visual-Linguistic Reasoning (CVLR) module for collaboratively disentangling the visual and linguistic spurious correlations via front-door and back-door causal interventions; ii) Spatial-Temporal Transformer (STT) module for capturing the fine-grained interactions between visual and linguistic semantics; iii) Visual-Linguistic Feature Fusion (VLFF) module for learning the global semantic-aware visual-linguistic representations adaptively. Extensive experiments on four event-level datasets demonstrate the superiority of our CMCIR in discovering visual-linguistic causal structures and achieving robust event-level visual question answering.},
  archive      = {J_TPAMI},
  author       = {Yang Liu and Guanbin Li and Liang Lin},
  doi          = {10.1109/TPAMI.2023.3284038},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11624-11641},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-modal causal relational reasoning for event-level visual question answering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross domain lifelong learning based on task similarity.
<em>TPAMI</em>, <em>45</em>(10), 11612–11623. (<a
href="https://doi.org/10.1109/TPAMI.2023.3276991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans gradually learn a sequence of cross-domain tasks and seldom experience catastrophic forgetting. In contrast, deep neural networks achieve good performance only in specific tasks within a single domain. To equip the network with lifelong learning capabilities, we propose a Cross-Domain Lifelong Learning (CDLL) framework that fully explores task similarities. Specifically, we employ a Dual Siamese Network (DSN) to learn the essential similarity features of tasks across different domains. To further understand similarity information across domains, we introduce a Domain-Invariant Feature Enhancement Module (DFEM) to better extract domain-invariant features. Moreover, we propose a Spatial Attention Network (SAN) that assigns different weights to various tasks based on the learned similarity features. Ultimately, to maximize the use of model parameters for learning new tasks, we propose a Structural Sparsity Loss (SSL) that can make the SAN as sparse as possible while ensuring accuracy. Experimental results show that our method effectively reduces catastrophic forgetting compared with state-of-the-art methods when continuously learning multiple tasks across different domains. It is worth noting that the proposed method scarcely forgets old knowledge while consistently enhancing the performance of learned tasks, more closely aligning with human learning.},
  archive      = {J_TPAMI},
  author       = {Shuojin Yang and Zhanchuan Cai},
  doi          = {10.1109/TPAMI.2023.3276991},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11612-11623},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross domain lifelong learning based on task similarity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CQ<span class="math inline"><sup>+</sup></span>+ training:
Minimizing accuracy loss in conversion from convolutional neural
networks to spiking neural networks. <em>TPAMI</em>, <em>45</em>(10),
11600–11611. (<a
href="https://doi.org/10.1109/TPAMI.2023.3286121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are attractive for energy-constrained use-cases due to their binarized activation, eliminating the need for weight multiplication. However, its lag in accuracy compared to traditional convolutional network networks (CNNs) has limited its deployment. In this paper, we propose CQ+ training (extended ”clamped” and ”quantized” training), an SNN-compatible CNN training algorithm that achieves state-of-the-art accuracy for both CIFAR-10 and CIFAR-100 datasets. Using a 7-layer modified VGG model (VGG-*), we achieved 95.06\% accuracy on the CIFAR-10 dataset for equivalent SNNs. The accuracy drop from converting the CNN solution to an SNN is only 0.09\% when using a time step of 600. To reduce the latency, we propose a parameterized input encoding method and a threshold training method, which further reduces the time window size to 64 while still achieving an accuracy of 94.09\%. For the CIFAR-100 dataset, we achieved an accuracy of 77.27\% using the same VGG-* structure and a time window of 500. We also demonstrate the transformation of popular CNNs, including ResNet (basic, bottleneck, and shortcut block), MobileNet v1/2, and Densenet, to SNNs with near-zero conversion accuracy loss and a time window size smaller than 60. The framework was developed in PyTorch and is publicly available.},
  archive      = {J_TPAMI},
  author       = {Zhanglu Yan and Jun Zhou and Weng-Fai Wong},
  doi          = {10.1109/TPAMI.2023.3286121},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11600-11611},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CQ$^{+}$+ training: Minimizing accuracy loss in conversion from convolutional neural networks to spiking neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Constrained structure learning for scene graph generation.
<em>TPAMI</em>, <em>45</em>(10), 11588–11599. (<a
href="https://doi.org/10.1109/TPAMI.2023.3282889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a structured prediction task, scene graph generation aims to build a visually-grounded scene graph to explicitly model objects and their relationships in an input image. Currently, the mean field variational Bayesian framework is the de facto methodology used by the existing methods, in which the unconstrained inference step is often implemented by a message passing neural network. However, such formulation fails to explore other inference strategies, and largely ignores the more general constrained optimization models. In this paper, we present a constrained structure learning method, for which an explicit constrained variational inference objective is proposed. Instead of applying the ubiquitous message-passing strategy, a generic constrained optimization method - entropic mirror descent - is utilized to solve the constrained variational inference step. We validate the proposed generic model on various popular scene graph generation benchmarks and show that it outperforms the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Daqi Liu and Miroslaw Bober and Josef Kittler},
  doi          = {10.1109/TPAMI.2023.3282889},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11588-11599},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Constrained structure learning for scene graph generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conformal prediction for time series. <em>TPAMI</em>,
<em>45</em>(10), 11575–11587. (<a
href="https://doi.org/10.1109/TPAMI.2023.3272339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general framework for constructing distribution-free prediction intervals for time series. We establish explicit bounds on the conditional and marginal coverage gaps of estimated prediction intervals, which asymptotically converge to zero under additional assumptions. We also provide similar bounds on the size of set differences between oracle and estimated prediction intervals. To implement this framework, we introduce an efficient algorithm called EnbPI , which utilizes ensemble predictors and is closely related to conformal prediction (CP) but does not require data exchangeability. Unlike other methods, EnbPI avoids data-splitting and is computationally efficient by avoiding retraining, making it scalable for sequentially producing prediction intervals. Extensive simulation and real-data analyses demonstrate the effectiveness of EnbPI compared to existing methods.},
  archive      = {J_TPAMI},
  author       = {Chen Xu and Yao Xie},
  doi          = {10.1109/TPAMI.2023.3272339},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11575-11587},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Conformal prediction for time series},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confluence: A robust non-IoU alternative to non-maxima
suppression in object detection. <em>TPAMI</em>, <em>45</em>(10),
11561–11574. (<a
href="https://doi.org/10.1109/TPAMI.2023.3273210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confluence is a novel non-Intersection over Union (IoU) alternative to Non-Maxima Suppression (NMS) in bounding box post-processing in object detection. It overcomes the inherent limitations of IoU-based NMS variants to provide a more stable, consistent predictor of bounding box clustering by using a normalized Manhattan Distance inspired proximity metric to represent bounding box clustering. Unlike Greedy and Soft NMS, it does not rely solely on classification confidence scores to select optimal bounding boxes, instead selecting the box which is closest to every other box within a given cluster and removing highly confluent neighboring boxes. Confluence is experimentally validated on the MS COCO and CrowdHuman benchmarks, improving Average Precision by 0.2--2.7\% and 1--3.8\% respectively and Average Recall by 1.3--9.3 and 2.4--7.3\% when compared against Greedy and Soft-NMS variants. Quantitative results are supported by extensive qualitative analysis and threshold sensitivity analysis experiments support the conclusion that Confluence is more robust than NMS variants. Confluence represents a paradigm shift in bounding box processing, with potential to replace IoU in bounding box regression processes.},
  archive      = {J_TPAMI},
  author       = {Andrew J. Shepley and Greg Falzon and Paul Kwan and Ljiljana Brankovic},
  doi          = {10.1109/TPAMI.2023.3273210},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11561-11574},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Confluence: A robust non-IoU alternative to non-maxima suppression in object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compositional scene representation learning via
reconstruction: A survey. <em>TPAMI</em>, <em>45</em>(10), 11540–11560.
(<a href="https://doi.org/10.1109/TPAMI.2023.3286184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual scenes are composed of visual concepts and have the property of combinatorial explosion. An important reason for humans to efficiently learn from diverse visual scenes is the ability of compositional perception, and it is desirable for artificial intelligence to have similar abilities. Compositional scene representation learning is a task that enables such abilities. In recent years, various methods have been proposed to apply deep neural networks, which have been proven to be advantageous in representation learning, to learn compositional scene representations via reconstruction, advancing this research direction into the deep learning era. Learning via reconstruction is advantageous because it may utilize massive unlabeled data and avoid costly and laborious data annotation. In this survey, we first outline the current progress on reconstruction-based compositional scene representation learning with deep neural networks, including development history and categorizations of existing methods from the perspectives of the modeling of visual scenes and the inference of scene representations; then provide benchmarks, including an open source toolbox to reproduce the benchmark experiments, of representative methods that consider the most extensively studied problem setting and form the foundation for other methods; and finally discuss the limitations of existing methods and future directions of this research topic.},
  archive      = {J_TPAMI},
  author       = {Jinyang Yuan and Tonglin Chen and Bin Li and Xiangyang Xue},
  doi          = {10.1109/TPAMI.2023.3286184},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11540-11560},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Compositional scene representation learning via reconstruction: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CMW-net: Learning a class-aware sample weighting mapping for
robust deep learning. <em>TPAMI</em>, <em>45</em>(10), 11521–11539. (<a
href="https://doi.org/10.1109/TPAMI.2023.3271451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks can easily overfit to biased training data containing corrupted labels or class imbalance. Sample re-weighting methods are popularly used to alleviate this data bias issue. Most current methods, however, require to manually pre-specify the weighting schemes relying on the characteristics of the investigated problem and training data. This makes them fairly hard to be generally applied in practical scenarios, due to their significant complexities and inter-class variations of data bias. To address this issue, we propose a meta-model capable of adaptively learning an explicit weighting scheme directly from data. Specifically, by seeing each training class as a separate learning task, our method aims to extract an explicit weighting function with sample loss and task/class feature as input, and sample weight as output, expecting to impose adaptively varying weighting schemes to different sample classes based on their own intrinsic bias characteristics. Extensive experiments substantiate the capability of our method on achieving proper weighting schemes in various data bias cases, like class imbalance, feature-independent and dependent label noises, and more complicated bias scenarios beyond conventional cases. Besides, the task-transferability of the learned weighting scheme is also substantiated, by readily deploying the weighting function learned on relatively smaller-scale CIFAR-10 dataset on much larger-scale full WebVision dataset. The general availability of our method for multiple robust deep learning issues, including partial-label learning, semi-supervised learning and selective classification, has also been validated. Code for reproducing our experiments is available at https://github.com/xjtushujun/CMW-Net .},
  archive      = {J_TPAMI},
  author       = {Jun Shu and Xiang Yuan and Deyu Meng and Zongben Xu},
  doi          = {10.1109/TPAMI.2023.3271451},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11521-11539},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CMW-net: Learning a class-aware sample weighting mapping for robust deep learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CIPS-3D++: End-to-end real-time high-resolution 3D-aware
GANs for GAN inversion and stylization. <em>TPAMI</em>, <em>45</em>(10),
11502–11520. (<a
href="https://doi.org/10.1109/TPAMI.2023.3285648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style-based GANs achieve state-of-the-art results for generating high-quality images, but lack explicit and precise control over camera poses. Recently proposed NeRF-based GANs have made great progress towards 3D-aware image generation. However, the methods either rely on convolution operators which are not rotationally invariant, or utilize complex yet suboptimal training procedures to integrate both NeRF and CNN sub-structures, yielding un-robust, low-quality images with a large computational burden. This article presents an upgraded version called CIPS-3D++ , aiming at high-robust, high-resolution and high-efficiency 3D-aware GANs. On the one hand, our basic model CIPS-3D, encapsulated in a style-based architecture, features a shallow NeRF-based 3D shape encoder as well as a deep MLP-based 2D image decoder, achieving robust image generation/editing with rotation-invariance. On the other hand, our proposed CIPS-3D++, inheriting the rotational invariance of CIPS-3D, together with geometric regularization and upsampling operations, encourages high-resolution high-quality image generation/editing with great computational efficiency. Trained on raw single-view images, without any bells and whistles, CIPS-3D++ sets new records for 3D-aware image synthesis, with an impressive FID of 3.2 on FFHQ at the $1024\times 1024$ resolution. In the meantime, CIPS-3D++ runs efficiently and enjoys a low GPU memory footprint so that it can be trained end-to-end on high-resolution images directly, in contrast to previous alternate/progressive methods. Based on the infrastructure of CIPS-3D++, we propose a 3D-aware GAN inversion algorithm named FlipInversion , which can reconstruct the 3D object from a single-view image. We also provide a 3D-aware stylization method for real images based on CIPS-3D++ and FlipInversion. In addition, we analyze the problem of mirror symmetry suffered in training, and solve it by introducing an auxiliary discriminator for the NeRF network. Overall, CIPS-3D++ provides a strong base model that can serve as a testbed for transferring GAN-based image editing methods from 2D to 3D.},
  archive      = {J_TPAMI},
  author       = {Peng Zhou and Lingxi Xie and Bingbing Ni and Qi Tian},
  doi          = {10.1109/TPAMI.2023.3285648},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11502-11520},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CIPS-3D++: End-to-end real-time high-resolution 3D-aware GANs for GAN inversion and stylization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C2F-TCN: A framework for semi- and fully-supervised temporal
action segmentation. <em>TPAMI</em>, <em>45</em>(10), 11484–11501. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action segmentation tags action labels for every frame in an input untrimmed video containing multiple actions in a sequence. For the task of temporal action segmentation, we propose an encoder-decoder style architecture named C2F-TCN featuring a “coarse-to-fine” ensemble of decoder outputs. The C2F-TCN framework is enhanced with a novel model agnostic temporal feature augmentation strategy formed by the computationally inexpensive strategy of the stochastic max-pooling of segments. It produces more accurate and well-calibrated supervised results on three benchmark action segmentation datasets. We show that the architecture is flexible for both supervised and representation learning. In line with this, we present a novel unsupervised way to learn frame-wise representation from C2F-TCN. Our unsupervised learning approach hinges on the clustering capabilities of the input features and the formation of multi-resolution features from the decoder&#39;s implicit structure. Further, we provide first semi-supervised temporal action segmentation results by merging representation learning with conventional supervised learning. Our semi-supervised learning scheme, called “Iterative-Contrastive-Classify (ICC)”, progressively improves in performance with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40\% labeled videos, performs similar to fully supervised counterparts.},
  archive      = {J_TPAMI},
  author       = {Dipika Singhania and Rahul Rahaman and Angela Yao},
  doi          = {10.1109/TPAMI.2023.3284080},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11484-11501},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {C2F-TCN: A framework for semi- and fully-supervised temporal action segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blind image deconvolution using variational deep image
prior. <em>TPAMI</em>, <em>45</em>(10), 11472–11483. (<a
href="https://doi.org/10.1109/TPAMI.2023.3283979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional deconvolution methods utilize hand-crafted image priors to constrain the optimization. While deep-learning-based methods have simplified the optimization by end-to-end training, they fail to generalize well to blurs unseen in the training dataset. Thus, training image-specific models is important for higher generalization. Deep image prior (DIP) provides an approach to optimize the weights of a randomly initialized network with a single degraded image by maximum a posteriori (MAP), which shows that the architecture of a network can serve as the hand-crafted image prior. Unlike conventional hand-crafted image priors, which are obtained through statistical methods, finding a suitable network architecture is challenging due to the unclear relationship between images and their corresponding architectures. As a result, the network architecture cannot provide enough constraint for the latent sharp image. This paper proposes a new variational deep image prior (VDIP) for blind image deconvolution, which exploits additive hand-crafted image priors on latent sharp images and approximates a distribution for each pixel to avoid suboptimal solutions. Our mathematical analysis shows that the proposed method can better constrain the optimization. The experimental results further demonstrate that the generated images have better quality than that of the original DIP on benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Dong Huo and Abbas Masoumzadeh and Rafsanjany Kushol and Yee-Hong Yang},
  doi          = {10.1109/TPAMI.2023.3283979},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11472-11483},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Blind image deconvolution using variational deep image prior},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilateral relation distillation for weakly supervised
temporal action localization. <em>TPAMI</em>, <em>45</em>(10),
11458–11471. (<a
href="https://doi.org/10.1109/TPAMI.2023.3284853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization (WSTAL), which aims to locate the time interval of actions in an untrimmed video with only video-level action labels, has attracted increasing research interest in the past few years. However, a model trained with such labels will tend to focus on segments that contributions most to the video-level classification, leading to inaccurate and incomplete localization results. In this paper, we tackle the problem from a novel perspective of relation modeling and propose a method dubbed Bilateral Relation Distillation (BRD). The core of our method involves learning representations by jointly modeling the relation at the category and sequence levels. Specifically, category-wise latent segment representations are first obtained by different embedding networks, one for each category. We then distill knowledge obtained from a pre-trained language model to capture the category-level relations, which is achieved by performing correlation alignment and category-aware contrast in an intra- and inter-video manner. To model the relations among segments at the sequence-level, we elaborate a gradient-based feature augmentation method and encourage the learned latent representation of the augmented feature to be consistent with that of the original one. Extensive experiments illustrate that our approach achieves state-of-the-art results on THUMOS14 and ActivityNet1.3 datasets.},
  archive      = {J_TPAMI},
  author       = {Zhe Xu and Kun Wei and Erkun Yang and Cheng Deng and Wei Liu},
  doi          = {10.1109/TPAMI.2023.3284853},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11458-11471},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bilateral relation distillation for weakly supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive part mining for robust visual tracking.
<em>TPAMI</em>, <em>45</em>(10), 11443–11457. (<a
href="https://doi.org/10.1109/TPAMI.2023.3275034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking aims to estimate object state in a video sequence, which is challenging when facing drastic appearance changes. Most existing trackers conduct tracking with divided parts to handle appearance variations. However, these trackers commonly divide target objects into regular patches by a hand-designed splitting way, which is too coarse to align object parts well. Besides, a fixed part detector is difficult to partition targets with arbitrary categories and deformations. To address the above issues, we propose a novel adaptive part mining tracker (APMT) for robust tracking via a transformer architecture, including an object representation encoder, an adaptive part mining decoder, and an object state estimation decoder. The proposed APMT enjoys several merits. First, in the object representation encoder, object representation is learned by distinguishing target object from background regions. Second, in the adaptive part mining decoder, we introduce multiple part prototypes to adaptively capture target parts through cross-attention mechanisms for arbitrary categories and deformations. Third, in the object state estimation decoder, we propose two novel strategies to effectively handle appearance variations and distractors. Extensive experimental results demonstrate that our APMT achieves promising results with high FPS. Notably, our tracker is ranked the first place in the VOT-STb2022 challenge.},
  archive      = {J_TPAMI},
  author       = {Yinchao Ma and Jianfeng He and Dawei Yang and Tianzhu Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2023.3275034},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11443-11457},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive part mining for robust visual tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified multimodal de- and re-coupling framework for RGB-d
motion recognition. <em>TPAMI</em>, <em>45</em>(10), 11428–11442. (<a
href="https://doi.org/10.1109/TPAMI.2023.3274783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion recognition is a promising direction in computer vision, but the training of video classification models is much harder than images due to insufficient data and considerable parameters. To get around this, some works strive to explore multimodal cues from RGB-D data. Although improving motion recognition to some extent, these methods still face sub-optimal situations in the following aspects: (i) Data augmentation, i.e., the scale of the RGB-D datasets is still limited, and few efforts have been made to explore novel data augmentation strategies for videos; (ii) Optimization mechanism, i.e., the tightly space-time-entangled network structure brings more challenges to spatiotemporal information modeling; And (iii) cross-modal knowledge fusion, i.e., the high similarity between multimodal representations leads to insufficient late fusion. To alleviate these drawbacks, we propose to improve RGB-D-based motion recognition both from data and algorithm perspectives in this article. In more detail, firstly, we introduce a novel video data augmentation method dubbed ShuffleMix, which acts as a supplement to MixUp, to provide additional temporal regularization for motion recognition. Secondly, a U nified M ultimodal D e-coupling and multi-stage R e-coupling framework, termed UMDR, is proposed for video representation learning. Finally, a novel cross-modal Complement Feature Catcher (CFCer) is explored to mine potential commonalities features in multimodal information as the auxiliary fusion stream, to improve the late fusion results. The seamless combination of these novel designs forms a robust spatiotemporal representation and achieves better performance than state-of-the-art methods on four public motion datasets. Specifically, UMDR achieves unprecedented improvements of $\uparrow 4.5\%$ on the Chalearn IsoGD dataset.},
  archive      = {J_TPAMI},
  author       = {Benjia Zhou and Pichao Wang and Jun Wan and Yanyan Liang and Fan Wang},
  doi          = {10.1109/TPAMI.2023.3274783},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11428-11442},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified multimodal de- and re-coupling framework for RGB-D motion recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on non-autoregressive generation for neural machine
translation and beyond. <em>TPAMI</em>, <em>45</em>(10), 11407–11427.
(<a href="https://doi.org/10.1109/TPAMI.2023.3277122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications.},
  archive      = {J_TPAMI},
  author       = {Yisheng Xiao and Lijun Wu and Junliang Guo and Juntao Li and Min Zhang and Tao Qin and Tie-Yan Liu},
  doi          = {10.1109/TPAMI.2023.3277122},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11407-11427},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey on non-autoregressive generation for neural machine translation and beyond},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A holistically-guided decoder for deep representation
learning with applications to semantic segmentation and object
detection. <em>TPAMI</em>, <em>45</em>(10), 11390–11406. (<a
href="https://doi.org/10.1109/TPAMI.2021.3114342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both high-level and high-resolution feature representations are of great importance in various visual understanding tasks. To acquire high-resolution feature maps with high-level semantic information, one common strategy is to adopt dilated convolutions in the backbone networks to extract high-resolution feature maps, such as the dilatedFCN-based methods for semantic segmentation. However, due to many convolution operations are conducted on the high-resolution feature maps, such methods have large computational complexity and memory consumption. To balance the performance and efficiency, there also exist encoder-decoder structures that gradually recover the spatial information by combining multi-level feature maps from a feature encoder, such as the FPN architecture for object detection and the U-Net for semantic segmentation. Although being more efficient, the performances of existing encoder-decoder methods for semantic segmentation are far from comparable with the dilatedFCN-based methods. In this paper, we propose one novel holistically-guided decoder which is introduced to obtain the high-resolution semantic-rich feature maps via the multi-scale features from the encoder. The decoding is achieved via novel holistic codeword generation and codeword assembly operations, which take advantages of both the high-level and low-level features from the encoder features. With the proposed holistically-guided decoder, we implement the EfficientFCN architecture for semantic segmentation and HGD-FPN for object detection and instance segmentation. The EfficientFCN achieves comparable or even better performance than state-of-the-art methods with only 1/3 of their computational costs for semantic segmentation on PASCAL Context, PASCAL VOC, ADE20K datasets. Meanwhile, the proposed HGD-FPN achieves $&amp;gt;2\%$ higher mean Average Precision (mAP) when integrated into several object detection frameworks with ResNet-50 encoding backbones.},
  archive      = {J_TPAMI},
  author       = {Jianbo Liu and Junjun He and Yuanjie Zheng and Shuai Yi and Xiaogang Wang and Hongsheng Li},
  doi          = {10.1109/TPAMI.2021.3114342},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {11390-11406},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A holistically-guided decoder for deep representation learning with applications to semantic segmentation and object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring human perception to improve open set recognition.
<em>TPAMI</em>, <em>45</em>(9), 11382–11389. (<a
href="https://doi.org/10.1109/TPAMI.2023.3270772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human ability to recognize when an object belongs or does not belong to a particular vision task outperforms all open set recognition algorithms. Human perception as measured by the methods and procedures of visual psychophysics from psychology provides an additional data stream for algorithms that need to manage novelty. For instance, measured reaction time from human subjects can offer insight as to whether a class sample is prone to be confused with a different class — known or novel. In this work, we designed and performed a large-scale behavioral experiment that collected over 200,000 human reaction time measurements associated with object recognition. The data collected indicated reaction time varies meaningfully across objects at the sample-level. We therefore designed a new psychophysical loss function that enforces consistency with human behavior in deep networks which exhibit variable reaction time for different images. As in biological vision, this approach allows us to achieve good open set recognition performance in regimes with limited labeled training data. Through experiments using data from ImageNet, significant improvement is observed when training Multi-Scale DenseNets with this new formulation: it significantly improved top-1 validation accuracy by 6.02\%, top-1 test accuracy on known samples by 9.81\%, and top-1 test accuracy on unknown samples by 33.18\%. We compared our method to 10 open set recognition methods from the literature, which were all outperformed on multiple metrics.},
  archive      = {J_TPAMI},
  author       = {Jin Huang and Derek Prijatelj and Justin Dulay and Walter Scheirer},
  doi          = {10.1109/TPAMI.2023.3270772},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11382-11389},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Measuring human perception to improve open set recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GradMDM: Adversarial attack on dynamic networks.
<em>TPAMI</em>, <em>45</em>(9), 11374–11381. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic neural networks can greatly reduce computation redundancy without compromising accuracy by adapting their structures based on the input. In this paper, we explore the robustness of dynamic neural networks against energy-oriented attacks targeted at reducing their efficiency. Specifically, we attack dynamic models with our novel algorithm GradMDM. GradMDM is a technique that adjusts the direction and the magnitude of the gradients to effectively find a small perturbation for each input, that will activate more computational units of dynamic models during inference. We evaluate GradMDM on multiple datasets and dynamic models, where it outperforms previous energy-oriented attack techniques, significantly increasing computation complexity while reducing the perceptibility of the perturbations https://github.com/lingengfoo/GradMDM .},
  archive      = {J_TPAMI},
  author       = {Jianhong Pan and Lin Geng Foo and Qichen Zheng and Zhipeng Fan and Hossein Rahmani and Qiuhong Ke and Jun Liu},
  doi          = {10.1109/TPAMI.2023.3263619},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11374-11381},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GradMDM: Adversarial attack on dynamic networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated fast hough transform for multidimensional
data. <em>TPAMI</em>, <em>45</em>(9), 11365–11373. (<a
href="https://doi.org/10.1109/TPAMI.2023.3269202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line, plane and hyperplane detection in multidimensional data has many applications in computer vision and artificial intelligence. We propose Integrated Fast Hough Transform (IFHT), a highly-efficient multidimensional Hough transform algorithm based on a new mathematical model. The parameter space of IFHT can be represented with a single k -tree to support hierarchical storage and “coarse-to-fine” search strategy. IFHT essentially changes the least square data-fitting in Li&#39;s Fast Hough transform (FHT) to the total least squares data-fitting, in which observational errors across all dimensions are taken into account, thus more practical and more resistant to data noise. It has practically resolved the problem of decreased precision of FHT for target objects mapped to boundaries between accumulators in the parameter space. In addition, it enables a straightforward visualization of the parameter space which not only provides intuitive insight on the number of objects in the data, but also helps with tuning the parameters and combining multiple instances if needed. In all simulated data with different levels of noise and parameters, IFHT surpasses Li&#39;s Fast Hough transform in terms of robustness and precision significantly.},
  archive      = {J_TPAMI},
  author       = {Yanhui Li and Xiangchao Gan},
  doi          = {10.1109/TPAMI.2023.3269202},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11365-11373},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An integrated fast hough transform for multidimensional data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual reasoning: From state to transformation.
<em>TPAMI</em>, <em>45</em>(9), 11352–11364. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing visual reasoning tasks, such as CLEVR in VQA, ignore an important factor, i.e., transformation. They are solely defined to test how well machines understand concepts and relations within static settings, like one image. Such state driven visual reasoning has limitations in reflecting the ability to infer the dynamics between different states, which has shown to be equally important for human cognition in Piaget&#39;s theory. To tackle this problem, we propose a novel transformation driven visual reasoning (TVR) task. Given both the initial and final states, the target becomes to infer the corresponding intermediate transformation. Following this definition, a new synthetic dataset namely TRANCE is first constructed on the basis of CLEVR, including three levels of settings, i.e., Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Next, we build another real dataset called TRANCO based on COIN, to cover the loss of transformation diversity on TRANCE. Inspired by human reasoning, we propose a three-staged reasoning framework called TranNet, including observing, analyzing, and concluding, to test how recent advanced techniques perform on TVR. Experimental results show that the state-of-the-art visual reasoning models perform well on Basic, but are still far from human-level intelligence on Event, View, and TRANCO. We believe the proposed new paradigm will boost the development of machine visual reasoning. More advanced methods and new problems need to be investigated in this direction.},
  archive      = {J_TPAMI},
  author       = {Xin Hong and Yanyan Lan and Liang Pang and Jiafeng Guo and Xueqi Cheng},
  doi          = {10.1109/TPAMI.2023.3268093},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11352-11364},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual reasoning: From state to transformation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational relational point completion network for robust
3D classification. <em>TPAMI</em>, <em>45</em>(9), 11340–11351. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise, which hampers 3D geometric modeling and perception. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, V ariational R elational point C ompletion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute multi-view partial point cloud datasets (MVP and MVP-40 dataset) containing over 200,000 high-quality scans, which render partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans. Moreover, we can achieve robust 3D classification for partial point clouds with the help of VRCNet, which can highly increase classification accuracy.},
  archive      = {J_TPAMI},
  author       = {Liang Pan and Xinyi Chen and Zhongang Cai and Junzhe Zhang and Haiyu Zhao and Shuai Yi and Ziwei Liu},
  doi          = {10.1109/TPAMI.2023.3268305},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11340-11351},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational relational point completion network for robust 3D classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised point cloud representation learning with deep
neural networks: A survey. <em>TPAMI</em>, <em>45</em>(9), 11321–11339.
(<a href="https://doi.org/10.1109/TPAMI.2023.3262786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant background including widely adopted point cloud datasets and DNN architectures is then briefly presented. This is followed by an extensive discussion of existing unsupervised point cloud representation learning methods according to their technical approaches. We also quantitatively benchmark and discuss the reviewed methods over multiple widely adopted point cloud datasets. Finally, we share our humble opinion about several challenges and problems that could be pursued in the future research in unsupervised point cloud representation learning.},
  archive      = {J_TPAMI},
  author       = {Aoran Xiao and Jiaxing Huang and Dayan Guan and Xiaoqin Zhang and Shijian Lu and Ling Shao},
  doi          = {10.1109/TPAMI.2023.3262786},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11321-11339},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised point cloud representation learning with deep neural networks: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The cluster structure function. <em>TPAMI</em>,
<em>45</em>(9), 11309–11320. (<a
href="https://doi.org/10.1109/TPAMI.2023.3264690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For each partition of a data set into a given number of parts there is a partition such that every part is as much as possible a good model (an “algorithmic sufficient statistic”) for the data in that part. Since this can be done for every number between one and the number of data, the result is a function, the cluster structure function . It maps the number of parts of a partition to values related to the deficiencies of being good models by the parts. Such a function starts with a value at least zero for no partition of the data set and descents to zero for the partition of the data set into singleton parts. The optimal clustering is the one selected by analyzing the cluster structure function. The theory behind the method is expressed in algorithmic information theory (Kolmogorov complexity). In practice the Kolmogorov complexities involved are approximated by a concrete compressor. We give examples using real data sets: the MNIST handwritten digits and the segmentation of real cells as used in stem cell research.},
  archive      = {J_TPAMI},
  author       = {Andrew R. Cohen and Paul M.B. Vitányi},
  doi          = {10.1109/TPAMI.2023.3264690},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11309-11320},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The cluster structure function},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal pixel-level semantic understanding through the VSPW
dataset. <em>TPAMI</em>, <em>45</em>(9), 11297–11308. (<a
href="https://doi.org/10.1109/TPAMI.2023.3266023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene understanding through pixel-level semantic parsing is one of the main problems in computer vision. Till now, image-based methods and datasets for scene parsing have been well explored. However, the real world is naturally dynamic instead of a static state. Thus, learning to perform video scene parsing is more practical for real-world applications. Considering that few datasets cover an extensive range of scenes and object categories with temporal pixel-level annotations, in this work, we present a large-scale video scene parsing dataset, namely VSPW (Video Scene Parsing in the Wild). To be specific, there are a total of 251,633 frames from 3,536 videos with densely pixel-wise annotations in VSPW, including a large variety of 231 scenes and 124 object categories. Besides, VSPW is densely annotated with a high frame rate of 15 f/s, and over 96\% of videos from VSPW have high spatial resolutions from 720P to 4 K. To the best of our knowledge, VSPW is the first attempt to address the challenging video scene parsing task in the wild by considering diverse scenes. Based on our VSPW, we further propose Temporal Attention Blending (TAB) Networks to harness temporal context information for better pixel-level semantic understanding of videos. Extensive experiments on VSPW well demonstrate the superiority of the proposed TAB over other baseline approaches. We hope the new proposed dataset and the explorations in this work can help advance the challenging yet practical video scene parsing task in the future. Both the dataset and the code are available at www.vspwdataset.com .},
  archive      = {J_TPAMI},
  author       = {Jiaxu Miao and Yunchao Wei and Xiaohan Wang and Yi Yang},
  doi          = {10.1109/TPAMI.2023.3266023},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11297-11308},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Temporal pixel-level semantic understanding through the VSPW dataset},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate modeling for bayesian optimization beyond a single
gaussian process. <em>TPAMI</em>, <em>45</em>(9), 11283–11296. (<a
href="https://doi.org/10.1109/TPAMI.2023.3264741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization (BO) has well-documented merits for optimizing black-box functions with an expensive evaluation cost. Such functions emerge in applications as diverse as hyperparameter tuning, drug discovery, and robotics. BO hinges on a Bayesian surrogate model to sequentially select query points so as to balance exploration with exploitation of the search space. Most existing works rely on a single Gaussian process (GP) based surrogate model, where the kernel function form is typically preselected using domain knowledge. To bypass such a design process, this paper leverages an ensemble (E) of GPs to adaptively select the surrogate model fit on-the-fly, yielding a GP mixture posterior with enhanced expressiveness for the sought function. Acquisition of the next evaluation input using this EGP-based function posterior is then enabled by Thompson sampling (TS) that requires no additional design parameters. To endow function sampling with scalability, random feature-based kernel approximation is leveraged per GP model. The novel EGP-TS readily accommodates parallel operation. To further establish convergence of the proposed EGP-TS to the global optimum, analysis is conducted based on the notion of Bayesian regret for both sequential and parallel settings. Tests on synthetic functions and real-world applications showcase the merits of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Qin Lu and Konstantinos D. Polyzos and Bingcong Li and Georgios B. Giannakis},
  doi          = {10.1109/TPAMI.2023.3264741},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11283-11296},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Surrogate modeling for bayesian optimization beyond a single gaussian process},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse-to-dense matching network for large-scale LiDAR point
cloud registration. <em>TPAMI</em>, <em>45</em>(9), 11270–11282. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a fundamental problem in 3D computer vision. Previous learning-based methods for LiDAR point cloud registration can be categorized into two schemes: dense-to-dense matching methods and sparse-to-sparse matching methods. However, for large-scale outdoor LiDAR point clouds, solving dense point correspondences is time-consuming, whereas sparse keypoint matching easily suffers from keypoint detection error. In this paper, we propose SDMNet, a novel Sparse-to-Dense Matching Network for large-scale outdoor LiDAR point cloud registration. Specifically, SDMNet performs registration in two sequential stages: sparse matching stage and local-dense matching stage. In the sparse matching stage, we sample a set of sparse points from the source point cloud and then match them to the dense target point cloud using a spatial consistency enhanced soft matching network and a robust outlier rejection module. Furthermore, a novel neighborhood matching module is developed to incorporate local neighborhood consensus, significantly improving performance. The local-dense matching stage is followed for fine-grained performance, where dense correspondences are efficiently obtained by performing point matching in local spatial neighborhoods of high-confidence sparse correspondences. Extensive experiments on three large-scale outdoor LiDAR point cloud datasets demonstrate that the proposed SDMNet achieves state-of-the-art performance with high efficiency.},
  archive      = {J_TPAMI},
  author       = {Fan Lu and Guang Chen and Yinlong Liu and Yibing Zhan and Zhijun Li and Dacheng Tao and Changjun Jiang},
  doi          = {10.1109/TPAMI.2023.3265531},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11270-11282},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse-to-dense matching network for large-scale LiDAR point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse quadratic approximation for graph learning.
<em>TPAMI</em>, <em>45</em>(9), 11256–11269. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning graphs represented by $M$ -matrices via an $\ell _{1}$ -regularized Gaussian maximum-likelihood method is a popular approach, but also one that poses computational challenges for large scale datasets. Recently proposed methods cast this problem as a constrained optimization variant of precision matrix estimation. In this paper, we build on a state-of-the-art sparse precision matrix estimation method and introduce two algorithms that learn $M$ -matrices, that can be subsequently used for the estimation of graph Laplacian matrices. In the first one, we propose an unconstrained method that follows a post processing approach in order to learn an $M$ -matrix, and in the second one, we implement a constrained approach based on sequential quadratic programming. We also demonstrate the effectiveness, accuracy, and performance of both algorithms. Our numerical examples and comparative results with modern open-source packages reveal that the proposed methods can accelerate the learning of graphs by up to 3 orders of magnitude, while accurately retrieving the latent graphical structure of the data. Furthermore, we conduct large scale case studies for the clustering of COVID-19 daily cases and the classification of image datasets to highlight the applicability in real-world scenarios.},
  archive      = {J_TPAMI},
  author       = {Dimosthenis Pasadakis and Matthias Bollhöfer and Olaf Schenk},
  doi          = {10.1109/TPAMI.2023.3263969},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11256-11269},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse quadratic approximation for graph learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source-free progressive graph learning for open-set domain
adaptation. <em>TPAMI</em>, <em>45</em>(9), 11240–11255. (<a
href="https://doi.org/10.1109/TPAMI.2023.3270288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set domain adaptation (OSDA) aims to transfer knowledge from a label-rich source domain to a label-scarce target domain while addressing disturbances from irrelevant target classes not present in the source data. However, most OSDA approaches are limited due to the lack of essential theoretical analysis of generalization bound, reliance on the coexistence of source and target data during adaptation, and failure to accurately estimate model predictions&#39; uncertainty. To address these limitations, the Progressive Graph Learning (PGL) framework is proposed. PGL decomposes the target hypothesis space into shared and unknown subspaces and progressively pseudo-labels the most confident known samples from the target domain for hypothesis adaptation. PGL guarantees a tight upper bound of the target error by integrating a graph neural network with episodic training and leveraging adversarial learning to close the gap between the source and target distributions. The proposed approach also tackles a more realistic source-free open-set domain adaptation (SF-OSDA) setting that makes no assumptions about the coexistence of source and target domains. In a two-stage framework, the SF-PGL model&#39; uniformly selects the most confident target instances from each category at a fixed ratio, and the confidence thresholds in each class weigh the classification loss in the adaptation step. The proposed methods are evaluated on benchmark image classification and action recognition datasets, where they demonstrate superiority and flexibility in recognizing both shared and unknown categories. Additionally, balanced pseudo-labeling plays a significant role in improving calibration, making the trained model less prone to over- or under-confident predictions on the target data.},
  archive      = {J_TPAMI},
  author       = {Yadan Luo and Zijian Wang and Zhuoxiao Chen and Zi Huang and Mahsa Baktashmotlagh},
  doi          = {10.1109/TPAMI.2023.3270288},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11240-11255},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Source-free progressive graph learning for open-set domain adaptation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SignBERT+: Hand-model-aware self-supervised pre-training
for sign language understanding. <em>TPAMI</em>, <em>45</em>(9),
11221–11239. (<a
href="https://doi.org/10.1109/TPAMI.2023.3269220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.},
  archive      = {J_TPAMI},
  author       = {Hezhen Hu and Weichao Zhao and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TPAMI.2023.3269220},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11221-11239},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SignBERT+: Hand-model-aware self-supervised pre-training for sign language understanding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDV-LOAM: Semi-direct visual–LiDAR odometry and mapping.
<em>TPAMI</em>, <em>45</em>(9), 11203–11220. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-LiDAR odometry and mapping (V-LOAM), which fuses complementary information of a camera and a LiDAR, is an attractive solution for accurate and robust pose estimation and mapping. However, existing systems could suffer nontrivial tracking errors arising from 1) association between 3D LiDAR points and sparse 2D features (i.e., 3D-2D depth association) and 2) obvious drifts in the vertical direction in the 6-degree of freedom (DOF) sweep-to-map optimization. In this paper, we present SDV-LOAM which incorporates a semi-direct visual odometry and an adaptive sweep-to-map LiDAR odometry to effectively avoid the above-mentioned errors and in turn achieve high tracking accuracy. The visual module of our SDV-LOAM directly extracts high-gradient pixels where 3D LiDAR points project on for tracking. To avoid the problem of large scale difference between matching frames in the VO, we design a novel point matching with propagation method to propagate points of a host frame to an intermediate keyframe which is closer to the current frame to reduce scale differences. To reduce the pose estimation drifts in the vertical direction, our LiDAR module employs an adaptive sweep-to-map optimization method which automatically choose to optimize 3 horizontal DOF or 6 full DOF pose according to the richness of geometric constraints in the vertical direction. In addition, we propose a novel sweep reconstruction method which can increase the input frequency of LiDAR point clouds to the same frequency as the camera images, and in turn yield a high frequency output of the LiDAR odometry in theory. Experimental results demonstrate that our SDV-LOAM ranks 8th on the KITTI odometry benchmark which outperforms most LiDAR/visual-LiDAR odometry systems. In addition, our visual module outperforms state-of-the-art visual odometry and our adaptive sweep-to-map optimization can improve the performance of several existing open-sourced LiDAR odometry systems. Moreover, we demonstrate our SDV-LOAM on a custom-built hardware platform in large-scale environments which achieves both a high accuracy and output frequency. We have released the source code of our SDV-LOAM for the development of the community.},
  archive      = {J_TPAMI},
  author       = {Zikang Yuan and Qingjie Wang and Ken Cheng and Tianyu Hao and Xin Yang},
  doi          = {10.1109/TPAMI.2023.3262817},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11203-11220},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SDV-LOAM: Semi-direct Visual–LiDAR odometry and mapping},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Representing multimodal behaviors with mean location for
pedestrian trajectory prediction. <em>TPAMI</em>, <em>45</em>(9),
11184–11202. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing multimodal behaviors is a critical challenge for pedestrian trajectory prediction. Previous methods commonly represent this multimodality with multiple latent variables repeatedly sampled from a latent space, encountering difficulties in interpretable trajectory prediction. Moreover, the latent space is usually built by encoding global interaction into future trajectory, which inevitably introduces superfluous interactions and thus leads to performance reduction. To tackle these issues, we propose a novel Interpretable Multimodality Predictor (IMP) for pedestrian trajectory prediction, whose core is to represent a specific mode by its mean location. We model the distribution of mean location as a Gaussian Mixture Model (GMM) conditioned on sparse spatio-temporal features, and sample multiple mean locations from the decoupled components of GMM to encourage multimodality. Our IMP brings four-fold benefits: 1) Interpretable prediction to provide semantics about the motion behavior of a specific mode; 2) Friendly visualization to present multimodal behaviors; 3) Well theoretical feasibility to estimate the distribution of mean locations supported by the central-limit theorem; 4) Effective sparse spatio-temporal features to reduce superfluous interactions and model temporal continuity of interaction. Extensive experiments validate that our IMP not only outperforms state-of-the-art methods but also can achieve a controllable prediction by customizing the corresponding mean location.},
  archive      = {J_TPAMI},
  author       = {Liushuai Shi and Le Wang and Chengjiang Long and Sanping Zhou and Wei Tang and Nanning Zheng and Gang Hua},
  doi          = {10.1109/TPAMI.2023.3268110},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11184-11202},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Representing multimodal behaviors with mean location for pedestrian trajectory prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RelTR: Relation transformer for scene graph generation.
<em>TPAMI</em>, <em>45</em>(9), 11169–11183. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by Detection Transformer, which excels in object detection, we view scene graph generation as a set prediction problem. In this article, we propose an end-to-end scene graph generation model Relation Transformer (RelTR), which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts sparse scene graphs directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome, Open Images V6, and VRD datasets demonstrate the superior performance and fast inference of our model.},
  archive      = {J_TPAMI},
  author       = {Yuren Cong and Michael Ying Yang and Bodo Rosenhahn},
  doi          = {10.1109/TPAMI.2023.3268066},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11169-11183},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RelTR: Relation transformer for scene graph generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). REDRESS: Generating compressed models for edge inference
using tsetlin machines. <em>TPAMI</em>, <em>45</em>(9), 11152–11168. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference at-the-edge using embedded machine learning models is associated with challenging trade-offs between resource metrics, such as energy and memory footprint, and the performance metrics, such as computation time and accuracy. In this work, we go beyond the conventional Neural Network based approaches to explore Tsetlin Machine (TM), an emerging machine learning algorithm, that uses learning automata to create propositional logic for classification. We use algorithm-hardware co-design to propose a novel methodology for training and inference of TM. The methodology, called REDRESS, comprises independent TM training and inference techniques to reduce the memory footprint of the resulting automata to target low and ultra-low power applications. The array of Tsetlin Automata (TA) holds learned information in the binary form as bits: $\lbrace 0,1\rbrace$ , called excludes and includes, respectively. REDRESS proposes a lossless TA compression method, called the include-encoding, that stores only the information associated with includes to achieve over 99\% compression. This is enabled by a novel computationally minimal training procedure, called the Tsetlin Automata Re-profiling, to improve the accuracy and increase the sparsity of TA to reduce the number of includes, hence, the memory footprint. Finally, REDRESS includes an inherently bit-parallel inference algorithm that operates on the optimally trained TA in the compressed domain, that does not require decompression during runtime, to obtain high speedups when compared with the state-of-the-art Binary Neural Network (BNN) models. In this work, we demonstrate that using REDRESS approach, TM outperforms BNN models on all design metrics for five benchmark datasets viz. MNIST, CIFAR2, KWS6, Fashion-MNIST and Kuzushiji-MNIST. When implemented on an STM32F746G-DISCO microcontroller, REDRESS obtained speedups and energy savings ranging 5-5700× compared with different BNN models.},
  archive      = {J_TPAMI},
  author       = {Sidharth Maheshwari and Tousif Rahman and Rishad Shafik and Alex Yakovlev and Ashur Rafiev and Lei Jiao and Ole-Christoffer Granmo},
  doi          = {10.1109/TPAMI.2023.3268415},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11152-11168},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {REDRESS: Generating compressed models for edge inference using tsetlin machines},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QGORE: Quadratic-time guaranteed outlier removal for point
cloud registration. <em>TPAMI</em>, <em>45</em>(9), 11136–11151. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D matching technology, correspondence-based point cloud registration gains more attention. Unfortunately, 3D keypoint techniques inevitably produce a large number of outliers, i.e., outlier rate is often larger than 95\%. Guaranteed outlier removal (GORE) Bustos and Chin has shown very good robustness to extreme outliers. However, the high computational cost (exponential in the worst case) largely limits its usages in practice. In this paper, we propose the first $O(N^{2})$ time GORE method, called quadratic-time GORE (QGORE), which preserves the globally optimal solution while largely increases the efficiency. QGORE leverages a simple but effective voting idea via geometric consistency for upper bound estimation, which achieves almost the same tightness as the one in GORE. We also present a one-point RANSAC by exploring “rotation correspondence” for lower bound estimation, which largely reduces the number of iterations of traditional 3-point RANSAC. Further, we propose a l$_{p}$p -like adaptive estimator for optimization. Extensive experiments show that QGORE achieves the same robustness and optimality as GORE while being 1 $\sim$ 2 orders faster. The source code will be made publicly available.},
  archive      = {J_TPAMI},
  author       = {Jiayuan Li and Pengcheng Shi and Qingwu Hu and Yongjun Zhang},
  doi          = {10.1109/TPAMI.2023.3262780},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11136-11151},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {QGORE: Quadratic-time guaranteed outlier removal for point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSLT: A light-weight vision transformer with ladder
self-attention and progressive shift. <em>TPAMI</em>, <em>45</em>(9),
11120–11135. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformer (ViT) has shown great potential for various visual tasks due to its ability to model long-range dependency. However, ViT requires a large amount of computing resource to compute the global self-attention. In this work, we propose a ladder self-attention block with multiple branches and a progressive shift mechanism to develop a light-weight transformer backbone that requires less computing resources (e.g., a relatively small number of parameters and FLOPs), termed Progressive Shift Ladder Transformer (PSLT). First, the ladder self-attention block reduces the computational cost by modelling local self-attention in each branch. In the meanwhile, the progressive shift mechanism is proposed to enlarge the receptive field in the ladder self-attention block by modelling diverse local self-attention for each branch and interacting among these branches. Second, the input feature of the ladder self-attention block is split equally along the channel dimension for each branch, which considerably reduces the computational cost in the ladder self-attention block (with nearly $\frac{1}{3}$ the amount of parameters and FLOPs), and the outputs of these branches are then collaborated by a pixel-adaptive fusion. Therefore, the ladder self-attention block with a relatively small number of parameters and FLOPs is capable of modelling long-range interactions. Based on the ladder self-attention block, PSLT performs well on several vision tasks, including image classification, objection detection and person re-identification. On the ImageNet-1 k dataset, PSLT achieves a top-1 accuracy of 79.9\% with 9.2 M parameters and 1.9 G FLOPs, which is comparable to several existing models with more than 20 M parameters and 4 G FLOPs. Code is available at https://isee-ai.cn/wugaojie/PSLT.html .},
  archive      = {J_TPAMI},
  author       = {Gaojie Wu and Wei-Shi Zheng and Yutong Lu and Qi Tian},
  doi          = {10.1109/TPAMI.2023.3265499},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11120-11135},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PSLT: A light-weight vision transformer with ladder self-attention and progressive shift},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Prioritized subnet sampling for resource-adaptive supernet
training. <em>TPAMI</em>, <em>45</em>(9), 11108–11119. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A resource-adaptive supernet adjusts its subnets for inference to fit the dynamically available resources. In this paper, we propose prioritized subnet sampling to train a resource-adaptive supernet, termed PSS-Net. We maintain multiple subnet pools, each of which stores the information of substantial subnets with similar resource consumption. Considering a resource constraint, subnets conditioned on this resource constraint are sampled from a pre-defined subnet structure space and high-quality ones will be inserted into the corresponding subnet pool. Then, the sampling will gradually be prone to sampling subnets from the subnet pools. Moreover, the one with a better performance metric is assigned with higher priority to train our PSS-Net, if sampling is from a subnet pool. At the end of training, our PSS-Net retains the best subnet in each pool to entitle a fast switch of high-quality subnets for inference when the available resources vary. Experiments on ImageNet using MobileNet-V1/V2 and ResNet-50 show that our PSS-Net can well outperform state-of-the-art resource-adaptive supernets. Our project is publicly available at https://github.com/chenbong/PSS-Net .},
  archive      = {J_TPAMI},
  author       = {Bohong Chen and Mingbao Lin and Rongrong Ji and Liujuan Cao},
  doi          = {10.1109/TPAMI.2023.3265198},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11108-11119},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Prioritized subnet sampling for resource-adaptive supernet training},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prior image guided snapshot compressive spectral imaging.
<em>TPAMI</em>, <em>45</em>(9), 11096–11107. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral images with rich spatial and spectral information have wide usage, however, traditional spectral imaging techniques undeniably take a long time to capture scenes. We consider the computational imaging problem of the snapshot spectral spectrometer, i.e., the Coded Aperture Snapshot Spectral Imaging (CASSI) system. For the sake of a fast and generalized reconstruction algorithm, we propose a prior image guidance-based snapshot compressive imaging method. Typically, the prior image denotes the RGB measurement captured by the additional uncoded panchromatic camera of the dual-camera CASSI system. We argue that the RGB image as a prior image can provide valuable semantic information. More importantly, we design the Prior Image Semantic Similarity (PIDS) regularization term to enhance the reconstructed spectral image fidelity. In particular, the PIDS is formulated as the difference between the total variation of the prior image and the recovered spectral image. Then, we solve the PIDS regularized reconstruction problem by the Alternating Direction Method of Multipliers (ADMM) optimization algorithm. Comprehensive experiments on various datasets demonstrate the superior performance of our method.},
  archive      = {J_TPAMI},
  author       = {Yurong Chen and Yaonan Wang and Hui Zhang},
  doi          = {10.1109/TPAMI.2023.3265749},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11096-11107},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Prior image guided snapshot compressive spectral imaging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point cloud scene completion with joint color and semantic
estimation from single RGB-d image. <em>TPAMI</em>, <em>45</em>(9),
11079–11095. (<a
href="https://doi.org/10.1109/TPAMI.2023.3264449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep reinforcement learning method of progressive view inpainting for colored semantic point cloud scene completion under volume guidance, achieving high-quality scene reconstruction from only a single RGB-D image with severe occlusion. Our approach is end-to-end, consisting of three modules: 3D scene volume reconstruction, 2D RGB-D and segmentation image inpainting, and multi-view selection for completion. Given a single RGB-D image, our method first predicts its semantic segmentation map and goes through the 3D volume branch to obtain a volumetric scene reconstruction as a guide to the next view inpainting step, which attempts to make up the missing information; the third step involves projecting the volume under the same view of the input, concatenating them to complete the current view RGB-D and segmentation map, and integrating all RGB-D and segmentation maps into the point cloud. Since the occluded areas are unavailable, we resort to a A3C network to glance around and pick the next best view for large hole completion progressively until a scene is adequately reconstructed while guaranteeing validity. All steps are learned jointly to achieve robust and consistent results. We perform qualitative and quantitative evaluations with extensive experiments on the 3D-FUTURE data, obtaining better results than state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Zhaoxuan Zhang and Xiaoguang Han and Bo Dong and Tong Li and Baocai Yin and Xin Yang},
  doi          = {10.1109/TPAMI.2023.3264449},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11079-11095},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Point cloud scene completion with joint color and semantic estimation from single RGB-D image},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MuxGNN: Multiplex graph neural network for heterogeneous
graphs. <em>TPAMI</em>, <em>45</em>(9), 11067–11078. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have become effective learning techniques for many downstream network mining tasks including node and graph classification, link prediction, and network reconstruction. However, most GNN methods have been developed for homogeneous networks with only a single type of node and edge. In this work we present muxGNN, a multiplex graph neural network for heterogeneous graphs. To model heterogeneity, we represent graphs as multiplex networks consisting of a set of relation layer graphs and a coupling graph that links node instantiations across multiple relations. We parameterize relation-specific representations of nodes and design a novel coupling attention mechanism that models the importance of multi-relational contexts for different types of nodes and edges in heterogeneous graphs. We further develop two complementary coupling structures: node invariant coupling suitable for node- and graph-level tasks, and node equivariant coupling suitable for link-level tasks. Extensive experiments conducted on six real-world datasets for link prediction in both transductive and inductive contexts and graph classification demonstrate the superior performance of muxGNN over state-of-the-art heterogeneous GNNs. In addition, we show that muxGNN&#39;s coupling attention discovers interpretable connections between different relations in heterogeneous networks.},
  archive      = {J_TPAMI},
  author       = {Joshua Melton and Siddharth Krishnan},
  doi          = {10.1109/TPAMI.2023.3263079},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11067-11078},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MuxGNN: Multiplex graph neural network for heterogeneous graphs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Missingness-pattern-adaptive learning with incomplete data.
<em>TPAMI</em>, <em>45</em>(9), 11053–11066. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems deal with collections of data with missing values, e.g., RNA sequential analytics, image completion, video processing, etc. Usually, such missing data is a serious impediment to a good learning achievement. Existing methods tend to use a universal model for all incomplete data, resulting in a suboptimal model for each missingness pattern. In this paper, we present a general model for learning with incomplete data. The proposed model can be appropriately adjusted with different missingness patterns, alleviating competitions between data. Our model is based on observable features only, so it does not incur errors from data imputation. We further introduce a low-rank constraint to promote the generalization ability of our model. Analysis of the generalization error justifies our idea theoretically. In additional, a subgradient method is proposed to optimize our model with a proven convergence rate. Experiments on different types of data show that our method compares favorably with typical imputation strategies and other state-of-the-art models for incomplete data. More importantly, our method can be seamlessly incorporated into the neural networks with the best results achieved. The source code is released at https://github.com/YS-GONG/missingness-patterns .},
  archive      = {J_TPAMI},
  author       = {Yongshun Gong and Zhibin Li and Wei Liu and Xiankai Lu and Xinwang Liu and Ivor W. Tsang and Yilong Yin},
  doi          = {10.1109/TPAMI.2023.3262784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11053-11066},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Missingness-pattern-adaptive learning with incomplete data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LRRNet: A novel representation learning guided fusion
network for infrared and visible images. <em>TPAMI</em>, <em>45</em>(9),
11040–11052. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based fusion methods have been achieving promising performance in image fusion tasks. This is attributed to the network architecture that plays a very important role in the fusion process. However, in general, it is hard to specify a good fusion architecture, and consequently, the design of fusion networks is still a black art, rather than science. To address this problem, we formulate the fusion task mathematically, and establish a connection between its optimal solution and the network architecture that can implement it. This approach leads to a novel method proposed in the paper of constructing a lightweight fusion network. It avoids the time-consuming empirical network design by a trial-and-test strategy. In particular we adopt a learnable representation approach to the fusion task, in which the construction of the fusion network architecture is guided by the optimisation algorithm producing the learnable model. The low-rank representation (LRR) objective is the foundation of our learnable model. The matrix multiplications, which are at the heart of the solution are transformed into convolutional operations, and the iterative process of optimisation is replaced by a special feed-forward network. Based on this novel network architecture, an end-to-end lightweight fusion network is constructed to fuse infrared and visible light images. Its successful training is facilitated by a detail-to-semantic information loss function proposed to preserve the image details and to enhance the salient features of the source images. Our experiments show that the proposed fusion network exhibits better fusion performance than the state-of-the-art fusion methods on public datasets. Interestingly, our network requires a fewer training parameters than other existing methods.},
  archive      = {J_TPAMI},
  author       = {Hui Li and Tianyang Xu and Xiao-Jun Wu and Jiwen Lu and Josef Kittler},
  doi          = {10.1109/TPAMI.2023.3268209},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11040-11052},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LRRNet: A novel representation learning guided fusion network for infrared and visible images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning by restoring broken 3D geometry. <em>TPAMI</em>,
<em>45</em>(9), 11024–11039. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key point for an experienced craftsman to repair broken objects effectively is that he must know about them deeply. Similarly, we believe that a model can capture rich geometry information from a shape/scene and generate discriminative representations if it is able to find distorted parts of shapes/scenes and restore them. Inspired by this observation, we propose a novel self-supervised 3D learning paradigm named learning by restoring broken shapes/scenes (collectively called 3D geometry). We first develop a destroy-method cluster, from which we sample methods to break some local parts of an object. Then the destroyed object and the normal object are both sent into a point cloud network to get representations, which are employed to segment points that belong to distorted parts and further reconstruct/restore them to normal. To perform better in these two associated pretext tasks, the model is constrained to capture useful object features, such as rich geometric and contextual information. The object representations learned by this self-supervised paradigm transfer well to different datasets and perform well on downstream classification, segmentation and detection tasks. Experimental results on shape datasets and scene datasets demonstrate that our method achieves state-of-the-art performance among unsupervised methods. We also show experimentally that pre-training with our framework significantly boosts the performance of supervised models.},
  archive      = {J_TPAMI},
  author       = {Jinxian Liu and Bingbing Ni and Ye Chen and Zhenbo Yu and Hang Wang},
  doi          = {10.1109/TPAMI.2023.3263867},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11024-11039},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning by restoring broken 3D geometry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). High-order correlation-guided slide-level histology
retrieval with self-supervised hashing. <em>TPAMI</em>, <em>45</em>(9),
11008–11023. (<a
href="https://doi.org/10.1109/TPAMI.2023.3269810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathological Whole Slide Images (WSIs) play a crucial role in cancer diagnosis. It is of significant importance for pathologists to search for images sharing similar content with the query WSI, especially in the case-based diagnosis. While slide-level retrieval could be more intuitive and practical in clinical applications, most methods are designed for patch-level retrieval. A few recently unsupervised slide-level methods only focus on integrating patch features directly, without perceiving slide-level information, and thus severely limits the performance of WSI retrieval. To tackle the issue, we propose a H igh-Order Correlation-Guided S elf-Supervised H ashing-Encoding R etrieval (HSHR) method. Specifically, we train an attention-based hash encoder with slide-level representation in a self-supervised manner, enabling it to generate more representative slide-level hash codes of cluster centers and assign weights for each. These optimized and weighted codes are leveraged to establish a similarity-based hypergraph, in which a hypergraph-guided retrieval module is adopted to explore high-order correlations in the multi-pairwise manifold to conduct WSI retrieval. Extensive experiments on multiple TCGA datasets with over 24,000 WSIs spanning 30 cancer subtypes demonstrate that HSHR achieves state-of-the-art performance compared with other unsupervised histology WSI retrieval methods.},
  archive      = {J_TPAMI},
  author       = {Shengrui Li and Yining Zhao and Jun Zhang and Ting Yu and Ji Zhang and Yue Gao},
  doi          = {10.1109/TPAMI.2023.3269810},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {11008-11023},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High-order correlation-guided slide-level histology retrieval with self-supervised hashing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guaranteed tensor recovery fused low-rankness and
smoothness. <em>TPAMI</em>, <em>45</em>(9), 10990–11007. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor recovery is a fundamental problem in tensor research field. It generally requires to explore intrinsic prior structures underlying tensor data, and formulate them as certain forms of regularization terms for guiding a sound estimate of the restored tensor. Recent researches have made significant progress by adopting two insightful tensor priors, i.e., global low-rankness (L) and local smoothness (S), which are always encoded as a sum of two separate regularizers into recovery models. However, unlike the primary theoretical developments on low-rank tensor recovery, these joint “L+S” models have no theoretical exact-recovery guarantees yet, making the methods lack reliability in real practice. To this crucial issue, in this work, we build a unique regularizer termed as tensor correlated total variation (t-CTV), which essentially encodes both L and S priors of a tensor simultaneously. Especially, by equipping t-CTV into the recovery models, we can rigorously prove the exact recovery guarantees for two typical tensor recovery tasks, i.e., tensor completion and tensor robust principal component analysis. To the best of our knowledge, this should be the first exact-recovery results among all related “L+S” methods for tensor recovery. We further propose ADMM algorithms with fine convergence to solve the proposed models. Significant recovery accuracy improvements are observed in extensive experiments. Typically, our method achieves a workable performance when the missing rate is extremely large, e.g., 99.5\%, for the color image inpainting task, while all its peers totally fail in such a challenging case. Code is released at https://github.com/wanghailin97 .},
  archive      = {J_TPAMI},
  author       = {Hailin Wang and Jiangjun Peng and Wenjin Qin and Jianjun Wang and Deyu Meng},
  doi          = {10.1109/TPAMI.2023.3259640},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10990-11007},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guaranteed tensor recovery fused low-rankness and smoothness},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Global aligned structured sparsity learning for efficient
image super-resolution. <em>TPAMI</em>, <em>45</em>(9), 10974–10989. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient image super-resolution (SR) has witnessed rapid progress thanks to novel lightweight architectures or model compression techniques (e.g., neural architecture search and knowledge distillation). Nevertheless, these methods consume considerable resources or/and neglect to squeeze out the network redundancy at a more fine-grained convolution filter level. Network pruning is a promising alternative to overcome these shortcomings. However, structured pruning is known to be tricky when applied to SR networks because the extensive residual blocks demand the pruned indices of different layers to be the same. Besides, the principled determination of proper layerwise sparsities remains challenging too. In this article, we present Global Aligned Structured Sparsity Learning (GASSL) to resolve these problems. GASSL has two major components: Hessian-Aided Regularization (HAIR) and Aligned Structured Sparsity Learning (ASSL). HAIR is a regularization-based sparsity auto-selection algorithm with Hessian considered implicitly. A proven proposition is introduced to justify its design. ASSL is for physically pruning SR networks. Particularly, a new penalty term Sparsity Structure Alignment (SSA) is proposed to align the pruned indices of different layers. With GASSL, we design two new efficient single image SR networks of different architecture genres, pushing the efficiency envelope of SR models one step forward. Extensive results demonstrate the merits of GASSL over other recent counterparts.},
  archive      = {J_TPAMI},
  author       = {Huan Wang and Yulun Zhang and Can Qin and Luc Van Gool and Yun Fu},
  doi          = {10.1109/TPAMI.2023.3268675},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10974-10989},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global aligned structured sparsity learning for efficient image super-resolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GFNet: Global filter networks for visual recognition.
<em>TPAMI</em>, <em>45</em>(9), 10960–10973. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision Transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. Based on this basic design, we develop a series of isotropic models with a Transformer-style simple architecture and CNN-style hierarchical models with better performance. Isotropic GFNet models exhibit favorable accuracy/complexity trade-offs compared to recent vision Transformers and pure MLP models. Hierarchical GFNet models can inherit successful designs in CNNs and be easily scaled up with larger model sizes and more training data, showing strong performance on both image classification (e.g., 85.0\% top-1 accuracy on ImageNet-1 k without any extra data or supervision, and 87.4\% accuracy with ImageNet-21 k pre-training) and dense prediction tasks (e.g., 54.3 mIoU on ADE20 k val). Our results demonstrate that GFNet can be a very competitive alternative to Transformer-based models and CNNs in terms of efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet .},
  archive      = {J_TPAMI},
  author       = {Yongming Rao and Wenliang Zhao and Zheng Zhu and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3263824},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10960-10973},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GFNet: Global filter networks for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry- and accuracy-preserving random forest proximities.
<em>TPAMI</em>, <em>45</em>(9), 10947–10959. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forests are considered one of the best out-of-the-box classification and regression algorithms due to their high level of predictive performance with relatively little tuning. Pairwise proximities can be computed from a trained random forest and measure the similarity between data points relative to the supervised task. Random forest proximities have been used in many applications including the identification of variable importance, data imputation, outlier detection, and data visualization. However, existing definitions of random forest proximities do not accurately reflect the data geometry learned by the random forest. In this paper, we introduce a novel definition of random forest proximities called Random Forest-Geometry- and Accuracy-Preserving proximities (RF-GAP). We prove that the proximity-weighted sum (regression) or majority vote (classification) using RF-GAP exactly matches the out-of-bag random forest prediction, thus capturing the data geometry learned by the random forest. We empirically show that this improved geometric representation outperforms traditional random forest proximities in tasks such as data imputation and provides outlier detection and visualization results consistent with the learned data geometry.},
  archive      = {J_TPAMI},
  author       = {Jake S. Rhodes and Adele Cutler and Kevin R. Moon},
  doi          = {10.1109/TPAMI.2023.3263774},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10947-10959},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geometry- and accuracy-preserving random forest proximities},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCoNet+: A stronger group collaborative co-salient object
detector. <em>TPAMI</em>, <em>45</em>(9), 10929–10946. (<a
href="https://doi.org/10.1109/TPAMI.2023.3264571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel end-to-end group collaborative learning network, termed GCoNet+ , which can effectively and efficiently (250 fps) identify co-salient objects in natural scenes. The proposed GCoNet+ achieves the new state-of-the-art performance for co-salient object detection (CoSOD) through mining consensus representations based on the following two essential criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module (GAM); 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module (GCM) conditioning on the inconsistent consensus. To further improve the accuracy, we design a series of simple yet effective components as follows: i) a recurrent auxiliary classification module (RACM) promoting model learning at the semantic level; ii) a confidence enhancement module (CEM) assisting the model in improving the quality of the final predictions; and iii) a group-based symmetric triplet (GST) loss guiding the model to learn more discriminative features. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been released at https://github.com/ZhengPeng7/GCoNet_plus .},
  archive      = {J_TPAMI},
  author       = {Peng Zheng and Huazhu Fu and Deng-Ping Fan and Qi Fan and Jie Qin and Yu-Wing Tai and Chi-Keung Tang and Luc Van Gool},
  doi          = {10.1109/TPAMI.2023.3264571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10929-10946},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GCoNet+: A stronger group collaborative co-salient object detector},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Gate-shift-fuse for video action recognition.
<em>TPAMI</em>, <em>45</em>(9), 10913–10928. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks are the de facto models for image recognition. However 3D CNNs, the straight forward extension of 2D CNNs for video recognition, have not achieved the same success on standard action recognition benchmarks. One of the main reasons for this reduced performance of 3D CNNs is the increased computational complexity requiring large scale annotated datasets to train them in scale. 3D kernel factorization approaches have been proposed to reduce the complexity of 3D CNNs. Existing kernel factorization approaches follow hand-designed and hard-wired techniques. In this paper we propose Gate-Shift-Fuse (GSF), a novel spatio-temporal feature extraction module which controls interactions in spatio-temporal decomposition and learns to adaptively route features through time and combine them in a data dependent manner. GSF leverages grouped spatial gating to decompose input tensor and channel weighting to fuse the decomposed tensors. GSF can be inserted into existing 2D CNNs to convert them into an efficient and high performing spatio-temporal feature extractor, with negligible parameter and compute overhead. We perform an extensive analysis of GSF using two popular 2D CNN families and achieve state-of-the-art or competitive performance on five standard action recognition benchmarks.},
  archive      = {J_TPAMI},
  author       = {Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz},
  doi          = {10.1109/TPAMI.2023.3268134},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10913-10928},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gate-shift-fuse for video action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient robustness assessment via adversarial
spatial-temporal focus on videos. <em>TPAMI</em>, <em>45</em>(9),
10898–10912. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness assessment for video recognition models has raised concerns owing to their wide applications on safety-critical tasks. Compared with images, videos have much high dimension, which brings huge computational costs when generating adversarial videos. This is especially serious for the query-based black-box attacks where gradient estimation for the threat models is usually utilized, and high dimensions will lead to a large number of queries. To mitigate this issue, we propose to simultaneously eliminate the temporal and spatial redundancy within the video to achieve an effective and efficient gradient estimation on the reduced searching space, and thus query number could decrease. To implement this idea, we design the novel A dversarial s patial- t emporal Focus ( AstFocus ) attack on videos, which performs attacks on the simultaneously focused key frames and key regions from the inter-frames and intra-frames in the video. AstFocus attack is based on the cooperative Multi-Agent Reinforcement Learning (MARL) framework. One agent is responsible for selecting key frames, and another agent is responsible for selecting key regions. These two agents are jointly trained by the common rewards received from the black-box threat models to perform a cooperative prediction. By continuously querying, the reduced searching space composed of key frames and key regions is becoming precise, and the whole query number becomes less than that on the original video. Extensive experiments on four mainstream video recognition models and three widely used action recognition datasets demonstrate that the proposed AstFocus attack outperforms the SOTA methods, which is prevenient in fooling rate, query number, time, and perturbation magnitude at the same time.},
  archive      = {J_TPAMI},
  author       = {Xingxing Wei and Songping Wang and Huanqian Yan},
  doi          = {10.1109/TPAMI.2023.3262592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10898-10912},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient robustness assessment via adversarial spatial-temporal focus on videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic spatial sparsification for efficient vision
transformers and convolutional neural networks. <em>TPAMI</em>,
<em>45</em>(9), 10883–10897. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative regions, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find that the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers as well as more complex dense prediction tasks. To handle structured feature maps, we formulate a generic dynamic spatial sparsification framework with progressive sparsification and asymmetric computation for different spatial locations. By applying lightweight fast paths to less informative features and expressive slow paths to important locations, we can maintain the complete structure of feature maps while significantly reducing the overall computations. Extensive experiments on diverse modern architectures and different visual tasks demonstrate the effectiveness of our proposed framework. By hierarchically pruning 66\% of the input tokens, our method greatly reduces 31\% $\sim$ 35\% FLOPs and improves the throughput by over 40\% while the drop of accuracy is within 0.5\% for various vision Transformers. By introducing asymmetric computation, a similar acceleration can be achieved on modern CNNs and Swin Transformers. Moreover, our method achieves promising results on more complex tasks including semantic segmentation and object detection. Our results clearly demonstrate that dynamic spatial sparsification offers a new and more effective dimension for model acceleration. Code is available at https://github.com/raoyongming/DynamicViT .},
  archive      = {J_TPAMI},
  author       = {Yongming Rao and Zuyan Liu and Wenliang Zhao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3263826},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10883-10897},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual vision transformer. <em>TPAMI</em>, <em>45</em>(9),
10870–10882. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances have presented several strategies to mitigate the computations of self-attention mechanism with high-resolution inputs. Many of these works consider decomposing the global self-attention procedure over image patches into regional and local feature extraction procedures that each incurs a smaller computational complexity. Despite good efficiency, these approaches seldom explore the holistic interactions among all patches, and are thus difficult to fully capture the global semantics. In this paper, we propose a novel Transformer architecture that elegantly exploits the global semantics for self-attention learning, namely Dual Vision Transformer (Dual-ViT). The new architecture incorporates a critical semantic pathway that can more efficiently compress token vectors into global semantics with reduced order of complexity. Such compressed global semantics then serve as useful prior information in learning finer local pixel level details, through another constructed pixel pathway. The semantic pathway and pixel pathway are integrated together and are jointly trained, spreading the enhanced self-attention information in parallel through both of the pathways. Dual-ViT is henceforth able to capitalize on global semantics to boost self-attention learning without compromising much computational complexity. We empirically demonstrate that Dual-ViT provides superior accuracy than SOTA Transformer architectures with comparable training complexity.},
  archive      = {J_TPAMI},
  author       = {Ting Yao and Yehao Li and Yingwei Pan and Yu Wang and Xiao-Ping Zhang and Tao Mei},
  doi          = {10.1109/TPAMI.2023.3268446},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10870-10882},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual vision transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diffusion models in vision: A survey. <em>TPAMI</em>,
<em>45</em>(9), 10850–10869. (<a
href="https://doi.org/10.1109/TPAMI.2023.3261988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
  archive      = {J_TPAMI},
  author       = {Florinel-Alin Croitoru and Vlad Hondru and Radu Tudor Ionescu and Mubarak Shah},
  doi          = {10.1109/TPAMI.2023.3261988},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10850-10869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Diffusion models in vision: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth-guided optimization of neural radiance fields for
indoor multi-view stereo. <em>TPAMI</em>, <em>45</em>(9), 10835–10849.
(<a href="https://doi.org/10.1109/TPAMI.2023.3263464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a new multi-view depth estimation method NerfingMVS that utilizes both conventional reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system first adapts a monocular depth network over the target scene by finetuning on its MVS reconstruction from COLMAP. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. We further present NerfingMVS++, where a coarse-to-fine depth priors training strategy is proposed to directly utilize sparse SfM points and the uniform sampling is replaced by Gaussian sampling to boost the performance. Experiments show that our NerfingMVS and its extension NerfingMVS++ achieve state-of-the-art performances on indoor datasets ScanNet and NYU Depth V2. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS .},
  archive      = {J_TPAMI},
  author       = {Yi Wei and Shaohui Liu and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3263464},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10835-10849},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Depth-guided optimization of neural radiance fields for indoor multi-view stereo},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deformable part region learning and feature aggregation tree
representation for object detection. <em>TPAMI</em>, <em>45</em>(9),
10817–10834. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-based object detection infers object regions for one or more categories in an image. Due to the recent advances in deep learning and region proposal methods, object detectors based on convolutional neural networks (CNNs) have been flourishing and provided promising detection results. However, the accuracy of the convolutional object detectors can be degraded often due to the low feature discriminability caused by geometric variation or transformation of an object. In this article, we propose a deformable part region (DPR) learning in order to allow decomposed part regions to be deformable according to the geometric transformation of an object. Because the ground truth of the part models is not available in many cases, we design part model losses for the detection and segmentation, and learn the geometric parameters by minimizing an integral loss including those part losses. As a result, we can train our DPR network without extra supervision, and make multi-part models deformable according to object geometric variation. Moreover, we propose a novel feature aggregation tree (FAT) so as to learn more discriminative region of interest (RoI) features via bottom-up tree construction. The FAT can learn the stronger semantic features by aggregating part RoI features along the bottom-up pathways of the tree. We also present a spatial and channel attention mechanism for the aggregation between different node features. Based on the proposed DPR and FAT networks, we design a new cascade architecture that can refine detection tasks iteratively. Without bells and whistles, we achieve impressive detection and segmentation results on MSCOCO and PASCAL VOC datasets. Our Cascade D-PRD achieves the 57.9 box AP with the Swin-L backbone. We also provide an extensive ablation study to prove the effectiveness and usefulness of the proposed methods for large-scale object detection.},
  archive      = {J_TPAMI},
  author       = {Seung-Hwan Bae},
  doi          = {10.1109/TPAMI.2023.3268864},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10817-10834},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deformable part region learning and feature aggregation tree representation for object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep long-tailed learning: A survey. <em>TPAMI</em>,
<em>45</em>(9), 10795–10816. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep long-tailed learning, one of the most challenging problems in visual recognition, aims to train well-performing deep models from a large number of images that follow a long-tailed class distribution. In the last decade, deep learning has emerged as a powerful recognition model for learning high-quality image representations and has led to remarkable breakthroughs in generic visual recognition. However, long-tailed class imbalance, a common problem in practical visual recognition tasks, often limits the practicality of deep network based recognition models in real-world applications, since they can be easily biased towards dominant classes and perform poorly on tail classes. To address this problem, a large number of studies have been conducted in recent years, making promising progress in the field of deep long-tailed learning. Considering the rapid evolution of this field, this article aims to provide a comprehensive survey on recent advances in deep long-tailed learning. To be specific, we group existing deep long-tailed learning studies into three main categories (i.e., class re-balancing, information augmentation and module improvement), and review these methods following this taxonomy in detail. Afterward, we empirically analyze several state-of-the-art methods by evaluating to what extent they address the issue of class imbalance via a newly proposed evaluation metric, i.e., relative accuracy. We conclude the survey by highlighting important applications of deep long-tailed learning and identifying several promising directions for future research.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhang and Bingyi Kang and Bryan Hooi and Shuicheng Yan and Jiashi Feng},
  doi          = {10.1109/TPAMI.2023.3268118},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10795-10816},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep long-tailed learning: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep gaussian scale mixture prior for image reconstruction.
<em>TPAMI</em>, <em>45</em>(9), 10778–10794. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image reconstruction from partial observations has attracted increasing attention. Conventional image reconstruction methods with hand-crafted priors often fail to recover fine image details due to the poor representation capability of the hand-crafted priors. Deep learning methods attack this problem by directly learning mapping functions between the observations and the targeted images can achieve much better results. However, most powerful deep networks lack transparency and are nontrivial to design heuristically. This paper proposes a novel image reconstruction method based on the Maximum a Posterior (MAP) estimation framework using learned Gaussian Scale Mixture (GSM) prior. Unlike existing unfolding methods that only estimate the image means (i.e., the denoising prior) but neglected the variances, we propose characterizing images by the GSM models with learned means and variances through a deep network. Furthermore, to learn the long-range dependencies of images, we develop an enhanced variant based on the Swin Transformer for learning GSM models. All parameters of the MAP estimator and the deep network are jointly optimized through end-to-end training. Extensive simulation and real data experimental results on spectral compressive imaging and image super-resolution demonstrate that the proposed method outperforms existing state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Tao Huang and Xin Yuan and Weisheng Dong and Jinjian Wu and Guangming Shi},
  doi          = {10.1109/TPAMI.2023.3265103},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10778-10794},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep gaussian scale mixture prior for image reconstruction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decoding visual neural representations by multimodal
learning of brain-visual-linguistic features. <em>TPAMI</em>,
<em>45</em>(9), 10760–10777. (<a
href="https://doi.org/10.1109/TPAMI.2023.3263181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding human visual neural representations is a challenging task with great scientific significance in revealing vision-processing mechanisms and developing brain-like intelligent machines. Most existing methods are difficult to generalize to novel categories that have no corresponding neural data for training. The two main reasons are 1) the under-exploitation of the multimodal semantic knowledge underlying the neural data and 2) the small number of paired ( stimuli-responses ) training data. To overcome these limitations, this paper presents a generic neural decoding method called BraVL that uses multimodal learning of brain-visual-linguistic features. We focus on modeling the relationships between brain, visual and linguistic features via multimodal deep generative models. Specifically, we leverage the mixture-of-product-of-experts formulation to infer a latent code that enables a coherent joint generation of all three modalities. To learn a more consistent joint representation and improve the data efficiency in the case of limited brain activity data, we exploit both intra- and inter-modality mutual information maximization regularization terms. In particular, our BraVL model can be trained under various semi-supervised scenarios to incorporate the visual and textual features obtained from the extra categories. Finally, we construct three trimodal matching datasets, and the extensive experiments lead to some interesting conclusions and cognitive insights: 1) decoding novel visual categories from human brain activity is practically possible with good accuracy; 2) decoding models using the combination of visual and linguistic features perform much better than those using either of them alone; 3) visual perception may be accompanied by linguistic influences to represent the semantics of visual stimuli.},
  archive      = {J_TPAMI},
  author       = {Changde Du and Kaicheng Fu and Jinpeng Li and Huiguang He},
  doi          = {10.1109/TPAMI.2023.3263181},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10760-10777},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Decoding visual neural representations by multimodal learning of brain-visual-linguistic features},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dawn of the transformer era in speech emotion recognition:
Closing the valence gap. <em>TPAMI</em>, <em>45</em>(9), 10745–10759.
(<a href="https://doi.org/10.1109/TPAMI.2023.3263585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in transformer-based architectures have shown promise in several machine learning tasks. In the audio domain, such architectures have been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation , robustness , fairness , and efficiency . The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of. 638 on MSP-Podcast. Our investigations reveal that transformer-based architectures are more robust compared to a CNN-based baseline and fair with respect to gender groups, but not towards individual speakers. Finally, we show that their success on valence is based on implicit linguistic information, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. To make our findings reproducible, we release the best performing model to the community.},
  archive      = {J_TPAMI},
  author       = {Johannes Wagner and Andreas Triantafyllopoulos and Hagen Wierstorf and Maximilian Schmitt and Felix Burkhardt and Florian Eyben and Björn W. Schuller},
  doi          = {10.1109/TPAMI.2023.3263585},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10745-10759},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dawn of the transformer era in speech emotion recognition: Closing the valence gap},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CRNet: A fast continual learning framework with random
theory. <em>TPAMI</em>, <em>45</em>(9), 10731–10744. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks are prone to suffer from catastrophic forgetting. Networks trained on something new tend to rapidly forget what was learned previously, a common phenomenon within connectionist models. In this work, we propose an effective and efficient continual learning framework using random theory, together with Bayes’ rule, to equip a single model with the ability to learn streaming data. The core idea of our framework is to preserve the performance of old tasks by guiding output weights to stay in a region of low error while encountering new tasks. In contrast to the existing continual learning approaches, our main contributions concern (1) closed-formed solutions with detailed theoretical analysis; (2) training continual learners by one-pass observation of samples; (3) remarkable advantages in terms of easy implementation, efficient parameters, fast convergence, and strong task-order robustness. Comprehensive experiments under popular image classification benchmarks, FashionMNIST, CIFAR-100, and ImageNet, demonstrate that our methods predominately outperform the extensive state-of-the-art methods on training speed while maintaining superior accuracy and the number of parameters, in the class incremental learning scenario. Code is available at https://github.com/toil2sweet/CRNet .},
  archive      = {J_TPAMI},
  author       = {Depeng Li and Zhigang Zeng},
  doi          = {10.1109/TPAMI.2023.3262853},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10731-10744},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CRNet: A fast continual learning framework with random theory},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CaCo: Both positive and negative samples are directly
learnable via cooperative-adversarial contrastive learning.
<em>TPAMI</em>, <em>45</em>(9), 10718–10730. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a representative self-supervised method, contrastive learning has achieved great successes in unsupervised training of representations. It trains an encoder by distinguishing positive samples from negative ones given query anchors. These positive and negative samples play critical roles in defining the objective to learn the discriminative encoder, avoiding it from learning trivial features. While existing methods heuristically choose these samples, we present a principled method where both positive and negative samples are directly learnable end-to-end with the encoder. We show that the positive and negative samples can be cooperatively and adversarially learned by minimizing and maximizing the contrastive loss, respectively. This yields cooperative positives and adversarial negatives with respect to the encoder, which are updated to continuously track the learned representation of the query anchors over mini-batches. The proposed method achieves 71.3\% and 75.3\% in top-1 accuracy respectively over 200 and 800 epochs of pre-training ResNet-50 backbone on ImageNet1K without tricks such as multi-crop or stronger augmentations. With Multi-Crop, it can be further boosted into 75.7\%. The source code and pre-trained model are released in https://github.com/maple-research-lab/caco .},
  archive      = {J_TPAMI},
  author       = {Xiao Wang and Yuhang Huang and Dan Zeng and Guo-Jun Qi},
  doi          = {10.1109/TPAMI.2023.3262608},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10718-10730},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CaCo: Both positive and negative samples are directly learnable via cooperative-adversarial contrastive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Brain-machine coupled learning method for facial emotion
recognition. <em>TPAMI</em>, <em>45</em>(9), 10703–10717. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network models of machine learning have shown promising prospects for visual tasks, such as facial emotion recognition (FER). However, the generalization of the model trained from a dataset with a few samples is limited. Unlike the machine, the human brain can effectively realize the required information from a few samples to complete the visual tasks. To learn the generalization ability of the brain, in this article, we propose a novel brain-machine coupled learning method for facial emotion recognition to let the neural network learn the visual knowledge of the machine and cognitive knowledge of the brain simultaneously. The proposed method utilizes visual images and electroencephalogram (EEG) signals to couple training the models in the visual and cognitive domains. Each domain model consists of two types of interactive channels, common and private. Since the EEG signals can reflect brain activity, the cognitive process of the brain is decoded by a model following reverse engineering. Decoding the EEG signals induced by the facial emotion images, the common channel in the visual domain can approach the cognitive process in the cognitive domain. Moreover, the knowledge specific to each domain is found in each private channel using an adversarial strategy. After learning, without the participation of the EEG signals, only the concatenation of both channels in the visual domain is used to classify facial emotion images based on the visual knowledge of the machine and the cognitive knowledge learned from the brain. Experiments demonstrate that the proposed method can produce excellent performance on several public datasets. Further experiments show that the proposed method trained from the EEG signals has good generalization ability on new datasets and can be applied to other network models, illustrating the potential for practical applications.},
  archive      = {J_TPAMI},
  author       = {Dongjun Liu and Weichen Dai and Hangkui Zhang and Xuanyu Jin and Jianting Cao and Wanzeng Kong},
  doi          = {10.1109/TPAMI.2023.3257846},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10703-10717},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Brain-machine coupled learning method for facial emotion recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bias-compensated integral regression for human pose
estimation. <em>TPAMI</em>, <em>45</em>(9), 10687–10702. (<a
href="https://doi.org/10.1109/TPAMI.2023.3264742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human and hand pose estimation, heatmaps are a crucial intermediate representation for a body or hand keypoint. Two popular methods to decode the heatmap into a final joint coordinate are via an argmax, as done in heatmap detection, or via softmax and expectation, as done in integral regression. Integral regression is learnable end-to-end, but has lower accuracy than detection. This paper uncovers an induced bias from integral regression that results from combining the softmax and the expectation operation. This bias often forces the network to learn degenerately localized heatmaps, obscuring the keypoint&#39;s true underlying distribution and leads to lower accuracies. Training-wise, by investigating the gradients of integral regression, we show that the implicit guidance of integral regression to update the heatmap makes it slower to converge than detection. To counter the above two limitations, we propose Bias Compensated Integral Regression (BCIR), an integral regression-based framework that compensates for the bias. BCIR also incorporates a Gaussian prior loss to speed up training and improve prediction accuracy. Experimental results on both the human body and hand benchmarks show that BCIR is faster to train and more accurate than the original integral regression, making it competitive with state-of-the-art detection methods.},
  archive      = {J_TPAMI},
  author       = {Kerui Gu and Linlin Yang and Michael Bi Mi and Angela Yao},
  doi          = {10.1109/TPAMI.2023.3264742},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10687-10702},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bias-compensated integral regression for human pose estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Base and meta: A new perspective on few-shot segmentation.
<em>TPAMI</em>, <em>45</em>(9), 10669–10686. (<a
href="https://doi.org/10.1109/TPAMI.2023.3265865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the progress made by few-shot segmentation (FSS) in low-data regimes, the generalization capability of most previous works could be fragile when countering hard query samples with seen-class objects. This paper proposes a fresh and powerful scheme to tackle such an intractable bias problem, dubbed base and meta (BAM). Concretely, we apply an auxiliary branch (base learner) to the conventional FSS framework (meta learner) to explicitly identify base-class objects, i.e., the regions that do not need to be segmented. Then, the coarse results output by these two learners in parallel are adaptively integrated to derive accurate segmentation predictions. Considering the sensitivity of meta learner, we further introduce adjustment factors to estimate the scene differences between support and query image pairs from both style and appearance perspectives, so as to facilitate the model ensemble forecasting. The remarkable performance gains on standard benchmarks (PASCAL-5 $^{i}$ , COCO-20 $^{i}$ , and FSS-1000) manifest the effectiveness, and surprisingly, our versatile scheme sets new state-of-the-arts even with two plain learners. Furthermore, in light of its unique nature, we also discuss several more practical but challenging extensions, including generalized FSS, 3D point cloud FSS, class-agnostic FSS, cross-domain FSS, weak-label FSS, and zero-shot segmentation. Our source code is available at https://github.com/chunbolang/BAM .},
  archive      = {J_TPAMI},
  author       = {Chunbo Lang and Gong Cheng and Binfei Tu and Chao Li and Junwei Han},
  doi          = {10.1109/TPAMI.2023.3265865},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10669-10686},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Base and meta: A new perspective on few-shot segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic transformation search against deep leakage from
gradients. <em>TPAMI</em>, <em>45</em>(9), 10650–10668. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative learning has gained great popularity due to its benefit of data privacy protection: participants can jointly train a Deep Learning model without sharing their training sets. However, recent works discovered that an adversary can fully recover the sensitive training samples from the shared gradients. Such reconstruction attacks pose severe threats to collaborative learning. Hence, effective mitigation solutions are urgently desired. In this paper, we systematically analyze existing reconstruction attacks and propose to leverage data augmentation to defeat these attacks: by preprocessing sensitive images with carefully-selected transformation policies, it becomes infeasible for the adversary to extract training samples from the corresponding gradients. We first design two new metrics to quantify the impacts of transformations on data privacy and model usability. With the two metrics, we design a novel search method to automatically discover qualified policies from a given data augmentation library. Our defense method can be further combined with existing collaborative training systems without modifying the training protocols. We conduct comprehensive experiments on various system settings. Evaluation results demonstrate that the policies discovered by our method can defeat state-of-the-art reconstruction attacks in collaborative learning, with high efficiency and negligible impact on the model performance.},
  archive      = {J_TPAMI},
  author       = {Wei Gao and Xu Zhang and Shangwei Guo and Tianwei Zhang and Tao Xiang and Han Qiu and Yonggang Wen and Yang Liu},
  doi          = {10.1109/TPAMI.2023.3262813},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10650-10668},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Automatic transformation search against deep leakage from gradients},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Attention weighted local descriptors. <em>TPAMI</em>,
<em>45</em>(9), 10632–10649. (<a
href="https://doi.org/10.1109/TPAMI.2023.3266728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local features detection and description are widely used in many vision applications with high industrial and commercial demands. With large-scale applications, these tasks raise high expectations for both the accuracy and speed of local features. Most existing studies on local features learning focus on the local descriptions of individual keypoints, which neglect their relationships established from global spatial awareness. In this paper, we present AWDesc with a consistent attention mechanism (CoAM) that opens up the possibility for local descriptors to embrace image-level spatial awareness in both the training and matching stages. For local features detection, we adopt local features detection with feature pyramid to obtain more stable and accurate keypoints localization. For local features description, we provide two versions of AWDesc to cope with different accuracy and speed requirements. On the one hand, we introduce Context Augmentation to address the inherent locality of convolutional neural networks by injecting non-local context information, so that local descriptors can “look wider to describe better”. Specifically, well-designed Adaptive Global Context Augmented Module (AGCA) and Diverse Surrounding Context Augmented Module (DSCA) are proposed to construct robust local descriptors with context information from global to surrounding. On the other hand, we design an extremely lightweight backbone network coupled with the proposed special knowledge distillation strategy to achieve the best trade-off in accuracy and speed. What is more, we perform thorough experiments on image matching, homography estimation, visual localization, and 3D reconstruction tasks, and the results demonstrate that our method surpasses the current state-of-the-art local descriptors. Code is available at: https://github.com/vignywang/AWDesc .},
  archive      = {J_TPAMI},
  author       = {Changwei Wang and Rongtao Xu and Ke Lu and Shibiao Xu and Weiliang Meng and Yuyang Zhang and Bin Fan and Xiaopeng Zhang},
  doi          = {10.1109/TPAMI.2023.3266728},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10632-10649},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attention weighted local descriptors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An information-theoretic method to automatic shortcut
avoidance and domain generalization for dense prediction tasks.
<em>TPAMI</em>, <em>45</em>(9), 10615–10631. (<a
href="https://doi.org/10.1109/TPAMI.2023.3268640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks for dense prediction tasks are commonly optimized using synthetic data, as generating pixel-wise annotations for real-world data is laborious. However, the synthetically trained models do not generalize well to real-world environments. This poor “synthetic to real” (S2R) generalization we address through the lens of shortcut learning. We demonstrate that the learning of feature representations in deep convolutional networks is heavily influenced by synthetic data artifacts (shortcut attributes). To mitigate this issue, we propose an Information-Theoretic Shortcut Avoidance (ITSA) approach to automatically restrict shortcut-related information from being encoded into the feature representations. Specifically, our proposed method minimizes the sensitivity of latent features to input variations: to regularize the learning of robust and shortcut-invariant features in synthetically trained models. To avoid the prohibitive computational cost of direct input sensitivity optimization, we propose a practical yet feasible algorithm to achieve robustness. Our results show that the proposed method can effectively improve S2R generalization in multiple distinct dense prediction tasks, such as stereo matching, optical flow, and semantic segmentation. Importantly, the proposed method enhances the robustness of the synthetically trained networks and outperforms their fine-tuned counterparts (on real data) for challenging out-of-domain applications.},
  archive      = {J_TPAMI},
  author       = {WeiQin Chuah and Ruwan Tennakoon and Reza Hoseinnezhad and David Suter and Alireza Bab-Hadiashar},
  doi          = {10.1109/TPAMI.2023.3268640},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10615-10631},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An information-theoretic method to automatic shortcut avoidance and domain generalization for dense prediction tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A perceptual measure for deep single image camera and lens
calibration. <em>TPAMI</em>, <em>45</em>(9), 10603–10614. (<a
href="https://doi.org/10.1109/TPAMI.2023.3269641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image editing and compositing have become ubiquitous in entertainment, from digital art to AR and VR experiences. To produce beautiful composites, the camera needs to be geometrically calibrated, which can be tedious and requires a physical calibration target. In place of the traditional multi-image calibration process, we propose to infer the camera calibration parameters such as pitch, roll, field of view, and lens distortion directly from a single image using a deep convolutional neural network. We train this network using automatically generated samples from a large-scale panorama dataset, yielding competitive accuracy in terms of standard $\ell ^{2}$ error. However, we argue that minimizing such standard error metrics might not be optimal for many applications. In this work, we investigate human sensitivity to inaccuracies in geometric camera calibration. To this end, we conduct a large-scale human perception study where we ask participants to judge the realism of 3D objects composited with correct and biased camera calibration parameters. Based on this study, we develop a new perceptual measure for camera calibration and demonstrate that our deep calibration network outperforms previous single-image based calibration methods both on standard metrics as well as on this novel perceptual measure. Finally, we demonstrate the use of our calibration network for several applications, including virtual object insertion, image retrieval, and compositing.},
  archive      = {J_TPAMI},
  author       = {Yannick Hold-Geoffroy and Dominique Piché-Meunier and Kalyan Sunkavalli and Jean-Charles Bazin and François Rameau and Jean-François Lalonde},
  doi          = {10.1109/TPAMI.2023.3269641},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {10603-10614},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A perceptual measure for deep single image camera and lens calibration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Properties of standard and sketched kernel fisher
discriminant. <em>TPAMI</em>, <em>45</em>(8), 10596–10602. (<a
href="https://doi.org/10.1109/TPAMI.2023.3242681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel Fisher discriminant (KFD) is a popular tool as a nonlinear extension of Fisher&#39;s linear discriminant, based on the use of the kernel trick. However, its asymptotic properties are still rarely studied. We first present an operator-theoretical formulation of KFD which elucidates the population target of the estimation problem. Convergence of the KFD solution to its population target is then established. However, the complexity of finding the solution poses significant challenges when $n$ is large and we further propose a sketched estimation approach based on a $m\times n$ sketching matrix which possesses the same asymptotic properties (in terms of convergence rate) even when $m$ is much smaller than $n$ . Some numerical results are presented to illustrate the performances of the sketched estimator.},
  archive      = {J_TPAMI},
  author       = {Jiamin Liu and Wangli Xu and Fode Zhang and Heng Lian},
  doi          = {10.1109/TPAMI.2023.3242681},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10596-10602},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Properties of standard and sketched kernel fisher discriminant},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persistent homology with improved locality information for
more effective delineation. <em>TPAMI</em>, <em>45</em>(8), 10588–10595.
(<a href="https://doi.org/10.1109/TPAMI.2023.3246921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent Homology (PH) has been successfully used to train networks to detect curvilinear structures and to improve the topological quality of their results. However, existing methods are very global and ignore the location of topological features. In this paper, we remedy this by introducing a new filtration function that fuses two earlier approaches: thresholding-based filtration, previously used to train deep networks to segment medical images, and filtration with height functions, typically used to compare 2D and 3D shapes. We experimentally demonstrate that deep networks trained using our PH-based loss function yield reconstructions of road networks and neuronal processes that reflect ground-truth connectivity better than networks trained with existing loss functions based on PH.},
  archive      = {J_TPAMI},
  author       = {Doruk Oner and Adélie Garin and Mateusz Koziński and Kathryn Hess and Pascal Fua},
  doi          = {10.1109/TPAMI.2023.3246921},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10588-10595},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Persistent homology with improved locality information for more effective delineation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Augmentation pathways network for visual recognition.
<em>TPAMI</em>, <em>45</em>(8), 10580–10587. (<a
href="https://doi.org/10.1109/TPAMI.2023.3250330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is practically helpful for visual recognition, especially at the time of data scarcity. However, such success is only limited to quite a few light augmentations (e.g., random crop, flip). Heavy augmentations are either unstable or show adverse effects during training, owing to the big gap between the original and augmented images. This paper introduces a novel network design, noted as Augmentation Pathways (AP), to systematically stabilize training on a much wider range of augmentation policies. Notably, AP tames various heavy data augmentations and stably boosts performance without a careful selection among augmentation policies. Unlike traditional single pathway, augmented images are processed in different neural paths. The main pathway handles the light augmentations, while other pathways focus on the heavier augmentations. By interacting with multiple paths in a dependent manner, the backbone network robustly learns from shared visual patterns among augmentations, and suppresses the side effect of heavy augmentations at the same time. Furthermore, we extend AP to high-order versions for high-order scenarios, demonstrating its robustness and flexibility in practical usage. Experimental results on ImageNet demonstrate the compatibility and effectiveness on a much wider range of augmentations, while consuming fewer parameters and lower computational costs at inference time.},
  archive      = {J_TPAMI},
  author       = {Yalong Bai and Mohan Zhou and Wei Zhang and Bowen Zhou and Tao Mei},
  doi          = {10.1109/TPAMI.2023.3250330},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10580-10587},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Augmentation pathways network for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). When object detection meets knowledge distillation: A
survey. <em>TPAMI</em>, <em>45</em>(8), 10555–10579. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection (OD) is a crucial computer vision task that has seen the development of many algorithms and models over the years. While the performance of current OD models has improved, they have also become more complex, making them impractical for industry applications due to their large parameter size. To tackle this problem, knowledge distillation (KD) technology was proposed in 2015 for image classification and subsequently extended to other visual tasks due to its ability to transfer knowledge learned by complex teacher models to lightweight student models. This paper presents a comprehensive survey of KD-based OD models developed in recent years, with the aim of providing researchers with an overview of recent progress in the field. We conduct an in-depth analysis of existing works, highlighting their advantages and limitations, and explore future research directions to inspire the design of models for related tasks. We summarize the basic principles of designing KD-based OD models, describe related KD-based OD tasks, including performance improvements for lightweight models, catastrophic forgetting in incremental OD, small object detection, and weakly/semi-supervised OD. We also analyze novel distillation techniques, i.e. different types of distillation loss, feature interaction between teacher and student models, etc. Additionally, we provide an overview of the extended applications of KD-based OD models on specific datasets, such as remote sensing images and 3D point cloud datasets. We compare and analyze the performance of different models on several common datasets and discuss promising directions for solving specific OD problems.},
  archive      = {J_TPAMI},
  author       = {Zhihui Li and Pengfei Xu and Xiaojun Chang and Luyao Yang and Yuanyuan Zhang and Lina Yao and Xiaojiang Chen},
  doi          = {10.1109/TPAMI.2023.3257546},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10555-10579},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {When object detection meets knowledge distillation: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visible and infrared image fusion using deep learning.
<em>TPAMI</em>, <em>45</em>(8), 10535–10554. (<a
href="https://doi.org/10.1109/TPAMI.2023.3261282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible and infrared image fusion (VIF) has attracted a lot of interest in recent years due to its application in many tasks, such as object detection, object tracking, scene segmentation, and crowd counting. In addition to conventional VIF methods, an increasing number of deep learning-based VIF methods have been proposed in the last five years. Different types of methods, such as CNN-based, autoencoder-based, GAN-based, and transformer-based methods, have been proposed. Deep learning-based methods have undoubtedly become dominant methods for the VIF task. However, while much progress has been made, the field will benefit from a systematic review of these deep learning-based methods. In this paper we present a comprehensive review of deep learning-based VIF methods. We discuss motivation, taxonomy, recent development characteristics, datasets, and performance evaluation methods in detail. We also discuss future prospects of the VIF field. This paper can serve as a reference for VIF researchers and those interested in entering this fast-developing field.},
  archive      = {J_TPAMI},
  author       = {Xingchen Zhang and Yiannis Demiris},
  doi          = {10.1109/TPAMI.2023.3261282},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10535-10554},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visible and infrared image fusion using deep learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational nested dropout. <em>TPAMI</em>, <em>45</em>(8),
10519–10534. (<a
href="https://doi.org/10.1109/TPAMI.2023.3241945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nested dropout is a variant of dropout operation that is able to order network parameters or features based on the pre-defined importance during training. It has been explored for: I. Constructing nested nets Cui et al. 2020, Cui et al. 2021: the nested nets are neural networks whose architectures can be adjusted instantly during testing time, e.g., based on computational constraints. The nested dropout implicitly ranks the network parameters, generating a set of sub-networks such that any smaller sub-network forms the basis of a larger one. II. Learning ordered representation Rippel et al. 2014: the nested dropout applied to the latent representation of a generative model (e.g., auto-encoder) ranks the features, enforcing explicit order of the dense representation over dimensions. However, the dropout rate is fixed as a hyper-parameter during the whole training process. For nested nets, when network parameters are removed, the performance decays in a human-specified trajectory rather than in a trajectory learned from data. For generative models, the importance of features is specified as a constant vector, restraining the flexibility of representation learning. To address the problem, we focus on the probabilistic counterpart of the nested dropout. We propose a variational nested dropout (VND) operation that draws samples of multi-dimensional ordered masks at a low cost, providing useful gradients to the parameters of nested dropout. Based on this approach, we design a Bayesian nested neural network that learns the order knowledge of the parameter distributions. We further exploit the VND under different generative models for learning ordered latent distributions. In experiments, we show that the proposed approach outperforms the nested network in terms of accuracy, calibration, and out-of-domain detection in classification tasks. It also outperforms the related generative models on data generation tasks.},
  archive      = {J_TPAMI},
  author       = {Yufei Cui and Yu Mao and Ziquan Liu and Qiao Li and Antoni B. Chan and Xue Liu and Tei-Wei Kuo and Chun Jason Xue},
  doi          = {10.1109/TPAMI.2023.3241945},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10519-10534},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational nested dropout},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Unsupervised learning of graph matching with mixture of
modes via discrepancy minimization. <em>TPAMI</em>, <em>45</em>(8),
10500–10518. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching (GM) has been a long-standing combinatorial problem due to its NP-hard nature. Recently (deep) learning-based approaches have shown their superiority over the traditional solvers while the methods are almost based on supervised learning which can be expensive or even impractical. We develop a unified unsupervised framework from matching two graphs to multiple graphs, without correspondence ground truth for training. Specifically, a Siamese-style unsupervised learning framework is devised and trained by minimizing the discrepancy of a second-order classic solver and a first-order (differentiable) Sinkhorn net as two branches for matching prediction. The two branches share the same CNN backbone for visual graph matching. Our framework further allows unsupervised learning with graphs from a mixture of modes which is ubiquitous in reality. Specifically, we develop and unify the graduated assignment (GA) strategy for matching two-graph, multi-graph, and graphs from a mixture of modes, whereby two-way constraint and clustering confidence (for mixture case) are modulated by two separate annealing parameters, respectively. Moreover, for partial and outlier matching, an adaptive reweighting technique is developed to suppress the overmatching issue. Experimental results on real-world benchmarks including natural image matching show our unsupervised method performs comparatively and even better against two-graph based supervised approaches.},
  archive      = {J_TPAMI},
  author       = {Runzhong Wang and Junchi Yan and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2023.3257830},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10500-10518},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised learning of graph matching with mixture of modes via discrepancy minimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised 3D pose transfer with cross consistency and
dual reconstruction. <em>TPAMI</em>, <em>45</em>(8), 10488–10499. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of 3D pose transfer is to transfer the pose from the source mesh to the target mesh while preserving the identity information (e.g., face, body shape) of the target mesh. Deep learning-based methods improved the efficiency and performance of 3D pose transfer. However, most of them are trained under the supervision of the ground truth, whose availability is limited in real-world scenarios. In this work, we present X-DualNet , a simple yet effective approach that enables unsupervised 3D pose transfer. In X-DualNet , we introduce a generator $G$ which contains correspondence learning and pose transfer modules to achieve 3D pose transfer. We learn the shape correspondence by solving an optimal transport problem without any key point annotations and generate high-quality meshes with our elastic instance normalization (ElaIN) in the pose transfer module. With $G$ as the basic component, we propose a cross consistency learning scheme and a dual reconstruction objective to learn the pose transfer without supervision. Besides that, we also adopt an as-rigid-as-possible deformer in the training process to fine-tune the body shape of the generated results. Extensive experiments on human and animal data demonstrate that our framework can successfully achieve comparable performance as the state-of-the-art supervised approaches.},
  archive      = {J_TPAMI},
  author       = {Chaoyue Song and Jiacheng Wei and Ruibo Li and Fayao Liu and Guosheng Lin},
  doi          = {10.1109/TPAMI.2023.3259059},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10488-10499},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised 3D pose transfer with cross consistency and dual reconstruction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training compact CNNs for image classification using
dynamic-coded filter fusion. <em>TPAMI</em>, <em>45</em>(8),
10478–10487. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mainstream approach for filter pruning is usually either to force a hard-coded importance estimation upon a computation-heavy pretrained model to select “important” filters, or to impose a hyperparameter-sensitive sparse constraint on the loss objective to regularize the network training. In this paper, we present a novel filter pruning method, dubbed dynamic-coded filter fusion (DCFF), to derive compact CNNs in a computation-economical and regularization-free manner for efficient image classification. Each filter in our DCFF is first given an inter-similarity distribution with a temperature parameter as a filter proxy, on top of which, a fresh Kullback-Leibler divergence based dynamic-coded criterion is proposed to evaluate the filter importance. In contrast to simply keeping high-score filters in other methods, we propose the concept of filter fusion, i.e., the weighted averages using the assigned proxies, as our preserved filters. We obtain a one-hot inter-similarity distribution as the temperature parameter approaches infinity. Thus, the relative importance of each filter can vary along with the training of the compact CNN, leading to dynamically changeable fused filters without both the dependency on the pretrained model and the introduction of sparse constraints. Extensive experiments on classification benchmarks demonstrate the superiority of our DCFF over the compared counterparts. For example, our DCFF derives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while reaching top-1 accuracy of 93.47\% on CIFAR-10. A compact ResNet-50 is obtained with 63.8\% FLOPs and 58.6\% parameter reductions, retaining 75.60\% top-1 accuracy on ILSVRC-2012. Our code, narrower models and training logs are available at https://github.com/lmbxmu/DCFF .},
  archive      = {J_TPAMI},
  author       = {Mingbao Lin and Bohong Chen and Fei Chao and Rongrong Ji},
  doi          = {10.1109/TPAMI.2023.3259402},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10478-10487},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Training compact CNNs for image classification using dynamic-coded filter fusion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tractable maximum likelihood estimation for latent structure
influence models with applications to EEG &amp; ECoG processing.
<em>TPAMI</em>, <em>45</em>(8), 10466–10477. (<a
href="https://doi.org/10.1109/TPAMI.2023.3244130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain signals are nonlinear and nonstationary time series, which provide information about spatiotemporal patterns of electrical activity in the brain. CHMMs are suitable tools for modeling multi-channel time-series dependent on both time and space, but state-space parameters grow exponentially with the number of channels. To cope with this limitation, we consider the influence model as the interaction of hidden Markov chains called Latent Structure Influence Models (LSIMs). LSIMs are capable of detecting nonlinearity and nonstationarity, making them well suited for multi-channel brain signals. We apply LSIMs to capture the spatial and temporal dynamics in multi-channel EEG/ECoG signals. The current manuscript extends the scope of the re-estimation algorithm from HMMs to LSIMs. We prove that the re-estimation algorithm of LSIMs will converge to stationary points corresponding to Kullback-Leibler divergence. We prove convergence by developing a new auxiliary function using the influence model and a mixture of strictly log-concave or elliptically symmetric densities. The theories that support this proof are derived from previous studies by Baum, Liporace, Dempster, and Juang. We then develop a closed-form expression for re-estimation formulas using tractable marginal forward-backward parameters defined in our previous study. Simulated datasets and EEG/ECoG recordings confirm the practical convergence of the derived re-estimation formulas. We also study the use of LSIMs for modeling and classification on simulated and real EEG/ECoG datasets. Based on AIC and BIC, LSIMs perform better than HMMs and CHMMs in modeling embedded Lorenz systems and ECoG recordings. LSIMs are more reliable and better classifiers than HMMs, SVMs and CHMMs in 2-class simulated CHMMs. EEG biometric verification results indicate that the LSIM-based method improves the area under curve (AUC) values by about 6.8\% and decreases the standard deviation of AUC values from 5.4\% to 3.3\% compared to the existing HMM-based method for all conditions on the BED dataset.},
  archive      = {J_TPAMI},
  author       = {Sajjad Karimi and Mohammad Bagher Shamsollahi},
  doi          = {10.1109/TPAMI.2023.3244130},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10466-10477},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tractable maximum likelihood estimation for latent structure influence models with applications to EEG &amp; ECoG processing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Temporal sentence grounding in videos: A survey and future
directions. <em>TPAMI</em>, <em>45</em>(8), 10443–10465. (<a
href="https://doi.org/10.1109/TPAMI.2023.3258628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal sentence grounding in videos (TSGV), a.k.a., natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. Lastly, we discuss issues with the current TSGV research and share our insights about promising research directions.},
  archive      = {J_TPAMI},
  author       = {Hao Zhang and Aixin Sun and Wei Jing and Joey Tianyi Zhou},
  doi          = {10.1109/TPAMI.2023.3258628},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10443-10465},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Temporal sentence grounding in videos: A survey and future directions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SS-TBN: A semi-supervised tri-branch network for COVID-19
screening and lesion segmentation. <em>TPAMI</em>, <em>45</em>(8),
10427–10442. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insufficient annotated data and minor lung lesions pose big challenges for computed tomography (CT)-aided automatic COVID-19 diagnosis at an early outbreak stage. To address this issue, we propose a Semi-Supervised Tri-Branch Network (SS-TBN). First, we develop a joint TBN model for dual-task application scenarios of image segmentation and classification such as CT-based COVID-19 diagnosis, in which pixel-level lesion segmentation and slice-level infection classification branches are simultaneously trained via lesion attention, and individual-level diagnosis branch aggregates slice-level outputs for COVID-19 screening. Second, we propose a novel hybrid semi-supervised learning method to make full use of unlabeled data, combining a new double-threshold pseudo labeling method specifically designed to the joint model and a new inter-slice consistency regularization method specifically tailored to CT images. Besides two publicly available external datasets, we collect internal and our own external datasets including 210,395 images (1,420 cases versus 498 controls) from ten hospitals. Experimental results show that the proposed method achieves state-of-the-art performance in COVID-19 classification with limited annotated data even if lesions are subtle, and that segmentation results promote interpretability for diagnosis, suggesting the potential of the SS-TBN in early screening in insufficient labeled data situations at the early stage of a pandemic outbreak like COVID-19.},
  archive      = {J_TPAMI},
  author       = {Ling-Li Zeng and Kai Gao and Dewen Hu and Zhichao Feng and Chenping Hou and Pengfei Rong and Wei Wang},
  doi          = {10.1109/TPAMI.2023.3240886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10427-10442},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SS-TBN: A semi-supervised tri-branch network for COVID-19 screening and lesion segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SequenceMorph: A unified unsupervised learning framework for
motion tracking on cardiac image sequences. <em>TPAMI</em>,
<em>45</em>(8), 10409–10426. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern medical imaging techniques, such as ultrasound (US) and cardiac magnetic resonance (MR) imaging, have enabled the evaluation of myocardial deformation directly from an image sequence. While many traditional cardiac motion tracking methods have been developed for the automated estimation of the myocardial wall deformation, they are not widely used in clinical diagnosis, due to their lack of accuracy and efficiency. In this paper, we propose a novel deep learning-based fully unsupervised method, SequenceMorph, for in vivo motion tracking in cardiac image sequences. In our method, we introduce the concept of motion decomposition and recomposition. We first estimate the inter-frame (INF) motion field between any two consecutive frames, by a bi-directional generative diffeomorphic registration neural network. Using this result, we then estimate the Lagrangian motion field between the reference frame and any other frame, through a differentiable composition layer. Our framework can be extended to incorporate another registration network, to further reduce the accumulated errors introduced in the INF motion tracking step, and to refine the Lagrangian motion estimation. By utilizing temporal information to perform reasonable estimations of spatio-temporal motion fields, this novel method provides a useful solution for image sequence motion tracking. Our method has been applied to US (echocardiographic) and cardiac MR (untagged and tagged cine) image sequences; the results show that SequenceMorph is significantly superior to conventional motion tracking methods, in terms of the cardiac motion tracking accuracy and inference efficiency.},
  archive      = {J_TPAMI},
  author       = {Meng Ye and Dong Yang and Qiaoying Huang and Mikael Kanski and Leon Axel and Dimitris N. Metaxas},
  doi          = {10.1109/TPAMI.2023.3243040},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10409-10426},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SequenceMorph: A unified unsupervised learning framework for motion tracking on cardiac image sequences},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salvage of supervision in weakly supervised object detection
and segmentation. <em>TPAMI</em>, <em>45</em>(8), 10394–10408. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised vision tasks, including detection and segmentation, have attracted much attention in the vision community recently. However, the lack of detailed and precise annotations in the weakly supervised case leads to a large accuracy gap between weakly- and fully-supervised methods. In this article, we propose a new framework, Salvage of Supervision (SoS), with the key idea being to effectively harness every potentially useful supervisory signal in weakly supervised vision tasks. Starting with weakly supervised object detection (WSOD), we propose SoS-WSOD to shrink the technology gap between WSOD and FSOD, which utilizes the weak image-level labels, the pseudo-labels, and the power of semi-supervised object detection for WSOD. Moreover, SoS-WSOD removes restrictions in traditional WSOD methods, including the reliance on ImageNet pretraining and inability to use modern backbones. The SoS framework also extends to weakly supervised semantic segmentation and instance segmentation. On several weakly supervised vision benchmarks, SoS achieves significant performance boost and generalization ability.},
  archive      = {J_TPAMI},
  author       = {Lin Sui and Chen-Lin Zhang and Jianxin Wu},
  doi          = {10.1109/TPAMI.2023.3243054},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10394-10408},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Salvage of supervision in weakly supervised object detection and segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoReg: Pairwise point cloud registration with oriented
descriptors and local rotations. <em>TPAMI</em>, <em>45</em>(8),
10376–10393. (<a
href="https://doi.org/10.1109/TPAMI.2023.3244951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RoReg, a novel point cloud registration framework that fully exploits oriented descriptors and estimated local rotations in the whole registration pipeline. Previous methods mainly focus on extracting rotation-invariant descriptors for registration but unanimously neglect the orientations of descriptors. In this paper, we show that the oriented descriptors and the estimated local rotations are very useful in the whole registration pipeline, including feature description, feature detection, feature matching, and transformation estimation. Consequently, we design a novel oriented descriptor RoReg-Desc and apply RoReg-Desc to estimate the local rotations. Such estimated local rotations enable us to develop a rotation-guided detector, a rotation coherence matcher, and a one-shot-estimation RANSAC, all of which greatly improve the registration performance. Extensive experiments demonstrate that RoReg achieves state-of-the-art performance on the widely-used 3DMatch and 3DLoMatch datasets, and also generalizes well to the outdoor ETH dataset. In particular, we also provide in-depth analysis on each component of RoReg, validating the improvements brought by oriented descriptors and the estimated local rotations. Source code and supplementary material are available at https://github.com/HpWang-whu/RoReg .},
  archive      = {J_TPAMI},
  author       = {Haiping Wang and Yuan Liu and Qingyong Hu and Bing Wang and Jianguo Chen and Zhen Dong and Yulan Guo and Wenping Wang and Bisheng Yang},
  doi          = {10.1109/TPAMI.2023.3244951},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10376-10393},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RoReg: Pairwise point cloud registration with oriented descriptors and local rotations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust face alignment via inherent relation learning and
uncertainty estimation. <em>TPAMI</em>, <em>45</em>(8), 10358–10375. (<a
href="https://doi.org/10.1109/TPAMI.2023.3260926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human tends to locate the facial landmarks with heavy occlusion by their relative position to the easily identified landmarks. The clue is defined as the landmark inherent relation while it is ignored by most existing methods. In this paper, we present Dynamic Sparse Local Patch Transformer (DSLPT), a novel face alignment framework for the inherent relation learning and uncertainty estimation. Unlike most existing methods that regress facial landmarks directly from global features, the DSLPT first generates a rough representation of each landmark from a local patch cropped from the feature map and then adaptively aggregates them by a case dependent inherent relation. Finally, the DSLPT predicts the coordinate and uncertainty of each landmark by regressing their probability distribution from the output features. Moreover, we introduce a coarse-to-fine framework to incorporate with DSLPT for an improved result. In the framework, the position and size of each patch are determined by the probability distribution of the corresponding landmark predicted in the previous stage. The dynamic patches will ensure a fine-grained landmark representation for inherent relation learning so that a rough prediction result can gradually converge to the target facial landmarks. We integrate the coarse-to-fine model into an end-to-end training pipeline and carry out experiments on the mainstream benchmarks. The results demonstrate that the DSLPT achieves state-of-the-art performance with much less computational complexity. The codes and models are available at https://github.com/Jiahao-UTS/DSLPT .},
  archive      = {J_TPAMI},
  author       = {Jiahao Xia and Min Xu and Haimin Zhang and Jianguo Zhang and Wenjian Huang and Hu Cao and Shiping Wen},
  doi          = {10.1109/TPAMI.2023.3260926},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10358-10375},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust face alignment via inherent relation learning and uncertainty estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Restoring vision in adverse weather conditions with
patch-based denoising diffusion models. <em>TPAMI</em>, <em>45</em>(8),
10346–10357. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.},
  archive      = {J_TPAMI},
  author       = {Ozan Özdenizci and Robert Legenstein},
  doi          = {10.1109/TPAMI.2023.3238179},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10346-10357},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Restoring vision in adverse weather conditions with patch-based denoising diffusion models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random cycle loss and its application to voice conversion.
<em>TPAMI</em>, <em>45</em>(8), 10331–10345. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech disentanglement aims to decompose independent causal factors of speech signals into separate codes. Perfect disentanglement benefits to a broad range of speech processing tasks. This paper presents a simple but effective disentanglement approach based on cycle consistency loss and random factor substitution. This leads to a novel random cycle (RC) loss that enforces analysis-and-resynthesis consistency, a main principle of reductionism. We theoretically demonstrate that the proposed RC loss can achieve independent codes if well optimized, which in turn leads to superior disentanglement when combined with information bottleneck (IB). Extensive simulation experiments were conducted to understand the properties of the RC loss, and experimental results on voice conversion further demonstrate the practical merit of the proposal. Source code and audio samples can be found on the webpage http://rc.cslt.org .},
  archive      = {J_TPAMI},
  author       = {Haoran Sun and Dong Wang and Lantian Li and Chen Chen and Thomas F. Zheng},
  doi          = {10.1109/TPAMI.2023.3257839},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10331-10345},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Random cycle loss and its application to voice conversion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive instance-aware feature learning for
compositional action recognition. <em>TPAMI</em>, <em>45</em>(8),
10317–10330. (<a
href="https://doi.org/10.1109/TPAMI.2023.3261659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to enable the model to generalize to unseen “action-objects” (compositional action), previous methods encode multiple pieces of information (i.e., the appearance, position, and identity of visual instances) independently and concatenate them for classification. However, these methods ignore the potential supervisory role of instance information (i.e., position and identity) in the process of visual perception. To this end, we present a novel framework, namely Progressive Instance-aware Feature Learning (PIFL), to progressively extract, reason, and predict dynamic cues of moving instances from videos for compositional action recognition. Specifically, this framework extracts features from foreground instances that are likely to be relevant to human actions (Position-aware Appearance Feature Extraction in Section III-B1), performs identity-aware reasoning among instance-centric features with semantic-specific interactions (Identity-aware Feature Interaction in Section III-B2), and finally predicts instances’ position from observed states to force the model into perceiving their movement (Semantic-aware Position Prediction in Section III-B3). We evaluate our approach on two compositional action recognition benchmarks, namely, Something-Else and IKEA-Assembly. Our approach achieves consistent accuracy gain beyond off-the-shelf action recognition algorithms in terms of both ground truth and detected position of instances.},
  archive      = {J_TPAMI},
  author       = {Rui Yan and Lingxi Xie and Xiangbo Shu and Liyan Zhang and Jinhui Tang},
  doi          = {10.1109/TPAMI.2023.3261659},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10317-10330},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Progressive instance-aware feature learning for compositional action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics-informed guided disentanglement in generative
networks. <em>TPAMI</em>, <em>45</em>(8), 10300–10316. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation (i2i) networks suffer from entanglement effects in presence of physics-related phenomena in target domain (such as occlusions, fog, etc), lowering altogether the translation quality, controllability and variability. In this paper, we propose a general framework to disentangle visual traits in target images. Primarily, we build upon collection of simple physics models, guiding the disentanglement with a physical model that renders some of the target traits, and learning the remaining ones. Because physics allows explicit and interpretable outputs, our physical models (optimally regressed on target) allows generating unseen scenarios in a controllable manner. Secondarily, we show the versatility of our framework to neural-guided disentanglement where a generative network is used in place of a physical model in case the latter is not directly accessible. Altogether, we introduce three strategies of disentanglement being guided from either a fully differentiable physics model, a (partially) non-differentiable physics model, or a neural network. The results show our disentanglement strategies dramatically increase performances qualitatively and quantitatively in several challenging scenarios for image translation.},
  archive      = {J_TPAMI},
  author       = {Fabio Pizzati and Pietro Cerri and Raoul de Charette},
  doi          = {10.1109/TPAMI.2023.3257486},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10300-10316},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Physics-informed guided disentanglement in generative networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized latent structure learning for recommendation.
<em>TPAMI</em>, <em>45</em>(8), 10285–10299. (<a
href="https://doi.org/10.1109/TPAMI.2023.3247563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommender systems, users’ behavior data are driven by the interactions of user-item latent factors. To improve recommendation effectiveness and robustness, recent advances focus on latent factor disentanglement via variational inference. Despite significant progress, uncovering the underlying interactions, i. e ., dependencies of latent factors, remains largely neglected by the literature. To bridge the gap, we investigate the joint disentanglement of user-item latent factors and the dependencies between them, namely latent structure learning. We propose to analyze the problem from the causal perspective, where a latent structure should ideally reproduce observational interaction data, and satisfy the structure acyclicity and dependency constraints, i. e ., causal prerequisites. We further identify the recommendation-specific challenges for latent structure learning, i. e ., the subjective nature of users’ minds and the inaccessibility of private/sensitive user factors causing universally learned latent structure to be suboptimal for individuals. To address these challenges, we propose the personalized latent structure learning framework for recommendation, namely PlanRec, which incorporates 1) differentiable Reconstruction, Dependency, and Acyclicity regularizations to satisfy the causal prerequisites; 2) Personalized Structure Learning (PSL) which personalizes the universally learned dependencies through probabilistic modeling; and 3) uncertainty estimation which explicitly measures the uncertainty of structure personalization, and adaptively balances personalization and shared knowledge for different users. We conduct extensive experiments on two public benchmark datasets from MovieLens and Amazon, and a large-scale industrial dataset from Alipay. Empirical studies validate that PlanRec discovers effective shared/personalized structures, and successfully balances shared knowledge and personalization via rational uncertainty estimation.},
  archive      = {J_TPAMI},
  author       = {Shengyu Zhang and Fuli Feng and Kun Kuang and Wenqiao Zhang and Zhou Zhao and Hongxia Yang and Tat-Seng Chua and Fei Wu},
  doi          = {10.1109/TPAMI.2023.3247563},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10285-10299},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Personalized latent structure learning for recommendation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance-aware approximation of global channel pruning
for multitask CNNs. <em>TPAMI</em>, <em>45</em>(8), 10267–10284. (<a
href="https://doi.org/10.1109/TPAMI.2023.3260903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global channel pruning (GCP) aims to remove a subset of channels (filters) across different layers from a deep model without hurting the performance. Previous works focus on either single task model pruning or simply adapting it to multitask scenario, and still face the following problems when handling multitask pruning: 1) Due to the task mismatch, a well-pruned backbone for classification task focuses on preserving filters that can extract category-sensitive information, causing filters that may be useful for other tasks to be pruned during the backbone pruning stage; 2) For multitask predictions, different filters within or between layers are more closely related and interacted than that for single task prediction, making multitask pruning more difficult. Therefore, aiming at multitask model compression, we propose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first theoretically present the objective for achieving superior GCP, by considering the joint saliency of filters from intra- and inter-layers. Then a sequentially greedy pruning strategy is proposed to optimize the objective, where a performance-aware oracle criterion is developed to evaluate sensitivity of filters to each task and preserve the globally most task-related filters. Experiments on several multitask datasets show that the proposed PAGCP can reduce the FLOPs and parameters by over 60\% with minor performance drop, and achieves 1.2x $\sim$ 3.3x acceleration on both cloud and mobile platforms. Our code is available at http://www.github.com/HankYe/PAGCP.git .},
  archive      = {J_TPAMI},
  author       = {Hancheng Ye and Bo Zhang and Tao Chen and Jiayuan Fan and Bin Wang},
  doi          = {10.1109/TPAMI.2023.3260903},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10267-10284},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Performance-aware approximation of global channel pruning for multitask CNNs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PDC-net+: Enhanced probabilistic dense correspondence
network. <em>TPAMI</em>, <em>45</em>(8), 10247–10266. (<a
href="https://doi.org/10.1109/TPAMI.2023.3249225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches. We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval.},
  archive      = {J_TPAMI},
  author       = {Prune Truong and Martin Danelljan and Radu Timofte and Luc Van Gool},
  doi          = {10.1109/TPAMI.2023.3249225},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10247-10266},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PDC-net+: Enhanced probabilistic dense correspondence network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing two-way partial AUC with an end-to-end framework.
<em>TPAMI</em>, <em>45</em>(8), 10228–10246. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Area Under the ROC Curve (AUC) is a crucial metric for machine learning, which evaluates the average performance over all possible True Positive Rates (TPRs) and False Positive Rates (FPRs). Based on the knowledge that a skillful classifier should simultaneously embrace a high TPR and a low FPR, we turn to study a more general variant called Two-way Partial AUC (TPAUC), where only the region with $\mathsf {TPR} \geq \alpha, \mathsf {FPR} \leq \beta$ is included in the area. Moreover, a recent work shows that the TPAUC is essentially inconsistent with the existing Partial AUC metrics where only the FPR range is restricted, opening a new problem to seek solutions to leverage high TPAUC. Motivated by this, we present the first trial in this article to optimize this new metric. The critical challenge along this course lies in the difficulty of performing gradient-based optimization with end-to-end stochastic training, even with a proper choice of surrogate loss. To address this issue, we propose a generic framework to construct surrogate optimization problems, which supports efficient end-to-end training with deep learning. Moreover, our theoretical analyses show that: 1) the objective function of the surrogate problems will achieve an upper bound of the original problem under mild conditions, and 2) optimizing the surrogate problems leads to good generalization performance in terms of TPAUC with a high probability. Finally, empirical studies over several benchmark datasets speak to the efficacy of our framework.},
  archive      = {J_TPAMI},
  author       = {Zhiyong Yang and Qianqian Xu and Shilong Bao and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3185311},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10228-10246},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimizing two-way partial AUC with an end-to-end framework},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online knowledge distillation via mutual contrastive
learning for visual recognition. <em>TPAMI</em>, <em>45</em>(8),
10212–10227. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to intermediate layers and perform an adaptive layer-matching mechanism trained by meta-optimization. Experiments on image classification and transfer learning to visual recognition tasks show that layer-wise MCL can lead to consistent performance gains against state-of-the-art online KD approaches. The superiority demonstrates that layer-wise MCL can guide the network to generate better feature representations. Our code is publicly avaliable at https://github.com/winycg/L-MCL .},
  archive      = {J_TPAMI},
  author       = {Chuanguang Yang and Zhulin An and Helong Zhou and Fuzhen Zhuang and Yongjun Xu and Qian Zhang},
  doi          = {10.1109/TPAMI.2023.3257878},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10212-10227},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Online knowledge distillation via mutual contrastive learning for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occlusion-aware instance segmentation via BiLayer network
architectures. <em>TPAMI</em>, <em>45</em>(8), 10197–10211. (<a
href="https://doi.org/10.1109/TPAMI.2023.3246174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting highly-overlapping image objects is challenging, because there is typically no distinction between real object contours and occlusion boundaries on images. Unlike previous instance segmentation methods, we model image formation as a composition of two overlapping layers, and propose B ilayer C onvolutional Net work ( BCNet ), where the top layer detects occluding objects (occluders) and the bottom layer infers partially occluded instances (occludees). The explicit modeling of occlusion relationship with bilayer structure naturally decouples the boundaries of both the occluding and occluded instances, and considers the interaction between them during mask regression. We investigate the efficacy of bilayer structure using two popular convolutional network designs, namely, Fully Convolutional Network (FCN) and Graph Convolutional Network (GCN). Further, we formulate bilayer decoupling using the vision transformer (ViT), by representing instances in the image as separate learnable occluder and occludee queries. Large and consistent improvements using one/two-stage and query-based object detectors with various backbones and network layer choices validate the generalization ability of bilayer decoupling, as shown by extensive experiments on image instance segmentation benchmarks (COCO, KINS, COCOA) and video instance segmentation benchmarks (YTVIS, OVIS, BDD100 K MOTS), especially for heavy occlusion cases.},
  archive      = {J_TPAMI},
  author       = {Lei Ke and Yu-Wing Tai and Chi-Keung Tang},
  doi          = {10.1109/TPAMI.2023.3246174},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10197-10211},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Occlusion-aware instance segmentation via BiLayer network architectures},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Normalization techniques in training DNNs: Methodology,
analysis and application. <em>TPAMI</em>, <em>45</em>(8), 10173–10196.
(<a href="https://doi.org/10.1109/TPAMI.2023.3250241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
  archive      = {J_TPAMI},
  author       = {Lei Huang and Jie Qin and Yi Zhou and Fan Zhu and Li Liu and Ling Shao},
  doi          = {10.1109/TPAMI.2023.3250241},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10173-10196},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Normalization techniques in training DNNs: Methodology, analysis and application},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Neural belief propagation for scene graph generation.
<em>TPAMI</em>, <em>45</em>(8), 10161–10172. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation aims to interpret an input image by explicitly modelling the objects contained therein and their relationships. In existing methods the problem is predominantly solved by message passing neural network models. Unfortunately, in such models, the variational distributions generally ignore the structural dependencies among the output variables, and most of the scoring functions only consider pairwise dependencies. This can lead to inconsistent interpretations. In this article, we propose a novel neural belief propagation method seeking to replace the traditional mean field approximation with a structural Bethe approximation. To find a better bias-variance trade-off, higher-order dependencies among three or more output variables are also incorporated into the relevant scoring function. The proposed method achieves the state-of-the-art performance on various popular scene graph generation benchmarks.},
  archive      = {J_TPAMI},
  author       = {Daqi Liu and Miroslaw Bober and Josef Kittler},
  doi          = {10.1109/TPAMI.2023.3243306},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10161-10172},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural belief propagation for scene graph generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multifractal characterization of texts for pattern
recognition: On the complexity of morphological structures in modern and
ancient languages. <em>TPAMI</em>, <em>45</em>(8), 10143–10160. (<a
href="https://doi.org/10.1109/TPAMI.2023.3245886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of languages’ structure and their organization in a set of well-defined relation schemes is a delicate matter. In the last decades, the convergence of traditional conflicting views by linguists is supported by an interdisciplinary approach that involves not only genetics or bio-archelogy but nowadays even the science of complexity. In light of this new and useful approach, this study proposes an in-depth analysis of the complexity underlying the morphological organization, in terms of multifractality and long-range correlations, of several modern and ancient texts pertaining to various linguistic strains (including ancient Greek, Arabic, Coptic, Neo-Latin and Germanic languages). The methodology is grounded on the mapping procedure between lexical categories belonging to text excerpts and time series, which is based on the rank of the frequency occurrence. Through the well-known MFDFA technique and a specific multifractal formalism, several multifractal indexes are then extracted for characterizing texts and the multifractal signature has been adopted for characterizing several language families, such as Indo-European, Semitic and Hamito-Semitic. The regularities and differences in the linguistic strains are assessed within a multivariate statistical framework and corroborated with a Machine Learning approach that is dedicated, in turn, to investigate the predictive power of the multifractal signature pertinent to text excerpts. The obtained results show a strong presence of persistence, i.e., memory, in the morphological structure of analyzed texts and we claim that this property has a role in characterizing the studied linguistic families. In fact, for example, the proposed analysis framework – grounded on complexity indexes – is able to easily distinguish ancient Greek texts from Arabic ones, as they belong to different language strains, i.e., indo-European and Semitic, respectively. The proposed approach has been proven effective and can be adopted for further comparative studies and for designing new informetrics for further advances in the fields of information retrieval and Artificial Intelligence.},
  archive      = {J_TPAMI},
  author       = {Enrico De Santis and Giovanni De Santis and Antonello Rizzi},
  doi          = {10.1109/TPAMI.2023.3245886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10143-10160},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multifractal characterization of texts for pattern recognition: On the complexity of morphological structures in modern and ancient languages},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MILO: Multi-bounce inverse rendering for indoor scene with
light-emitting objects. <em>TPAMI</em>, <em>45</em>(8), 10129–10142. (<a
href="https://doi.org/10.1109/TPAMI.2023.3244658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many advances in inverse rendering are achieved by high-dimensional lighting representations and differentiable rendering. However, multi-bounce lighting effects can hardly be handled correctly in scene editing using high-dimensional lighting representations, and light source model deviation and ambiguities exist in differentiable rendering methods. These problems limit the applications of inverse rendering. In this paper, we present a multi-bounce inverse rendering method based on Monte Carlo path tracing, to enable correct complex multi-bounce lighting effects rendering in scene editing. We propose a novel light source model that is more suitable for light source editing in indoor scenes, and design a specific neural network with corresponding disambiguation constraints to alleviate ambiguities during the inverse rendering. We evaluate our method on both synthetic and real indoor scenes through virtual object insertion, material editing, relighting tasks, and so on. The results demonstrate that our method achieves better photo-realistic quality.},
  archive      = {J_TPAMI},
  author       = {Bohan Yu and Siqi Yang and Xuanning Cui and Siyan Dong and Baoquan Chen and Boxin Shi},
  doi          = {10.1109/TPAMI.2023.3244658},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10129-10142},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MILO: Multi-bounce inverse rendering for indoor scene with light-emitting objects},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring perceptual color differences of smartphone
photographs. <em>TPAMI</em>, <em>45</em>(8), 10114–10128. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring perceptual color differences (CDs) is of great importance in modern smartphone photography. Despite the long history, most CD measures have been constrained by psychophysical data of homogeneous color patches or a limited number of simplistic natural photographic images. It is thus questionable whether existing CD measures generalize in the age of smartphone photography characterized by greater content complexities and learning-based image signal processors. In this article, we put together so far the largest image dataset for perceptual CD assessment, in which the photographic images are 1) captured by six flagship smartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of the smartphones, and 4) reproduced with incorrect color profiles. We then conduct a large-scale psychophysical experiment to gather perceptual CDs of 30,000 image pairs in a carefully controlled laboratory environment. Based on the newly established dataset, we make one of the first attempts to construct an end-to-end learnable CD formula based on a lightweight neural network, as a generalization of several previous metrics. Extensive experiments demonstrate that the optimized formula outperforms 33 existing CD measures by a large margin, offers reasonable local CD maps without the use of dense supervision, generalizes well to homogeneous color patch data, and empirically behaves as a proper metric in the mathematical sense. Our dataset and code are publicly available at https://github.com/hellooks/CDNet .},
  archive      = {J_TPAMI},
  author       = {Zhihua Wang and Keshuo Xu and Yang Yang and Jianlei Dong and Shuhang Gu and Lihao Xu and Yuming Fang and Kede Ma},
  doi          = {10.1109/TPAMI.2023.3262424},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10114-10128},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Measuring perceptual color differences of smartphone photographs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matrix completion with cross-concentrated sampling: Bridging
uniform sampling and CUR sampling. <em>TPAMI</em>, <em>45</em>(8),
10100–10113. (<a
href="https://doi.org/10.1109/TPAMI.2023.3261185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While uniform sampling has been widely studied in the matrix completion literature, CUR sampling approximates a low-rank matrix via row and column samples. Unfortunately, both sampling models lack flexibility for various circumstances in real-world applications. In this work, we propose a novel and easy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS). By bridging uniform sampling and CUR sampling, CCS provides extra flexibility that can potentially save sampling costs in applications. In addition, we also provide a sufficient condition for CCS-based matrix completion. Moreover, we propose a highly efficient non-convex algorithm, termed Iterative CUR Completion (ICURC), for the proposed CCS model. Numerical experiments verify the empirical advantages of CCS and ICURC against uniform sampling and its baseline algorithms, on both synthetic and real-world datasets.},
  archive      = {J_TPAMI},
  author       = {HanQin Cai and Longxiu Huang and Pengyu Li and Deanna Needell},
  doi          = {10.1109/TPAMI.2023.3261185},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10100-10113},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Matrix completion with cross-concentrated sampling: Bridging uniform sampling and CUR sampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank matrix completion theory via plücker coordinates.
<em>TPAMI</em>, <em>45</em>(8), 10084–10099. (<a
href="https://doi.org/10.1109/TPAMI.2023.3250325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the popularity of low-rank matrix completion, the majority of its theory has been developed under the assumption of random observation patterns, whereas very little is known about the practically relevant case of non-random patterns. Specifically, a fundamental yet largely open question is to describe patterns that allow for unique or finitely many completions. This paper provides three such families of patterns for any rank and any matrix size. A key to achieving this is a novel formulation of low-rank matrix completion in terms of Plücker coordinates, the latter a traditional tool in computer vision. This connection is of potential significance to a wide family of matrix and subspace learning problems with incomplete data.},
  archive      = {J_TPAMI},
  author       = {Manolis C. Tsakiris},
  doi          = {10.1109/TPAMI.2023.3250325},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10084-10099},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Low-rank matrix completion theory via plücker coordinates},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Localization distillation for object detection.
<em>TPAMI</em>, <em>45</em>(8), 10070–10083. (<a
href="https://doi.org/10.1109/TPAMI.2023.3248583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous knowledge distillation (KD) methods for object detection mostly focus on feature imitation instead of mimicking the prediction logits due to its inefficiency in distilling the localization information. In this paper, we investigate whether logit mimicking always lags behind feature imitation. Towards this goal, we first present a novel localization distillation (LD) method which can efficiently transfer the localization knowledge from the teacher to the student. Second, we introduce the concept of valuable localization region that can aid to selectively distill the classification and localization knowledge for a certain region. Combining these two new components, for the first time, we show that logit mimicking can outperform feature imitation and the absence of localization distillation is a critical reason for why logit mimicking under-performs for years. The thorough studies exhibit the great potential of logit mimicking that can significantly alleviate the localization ambiguity, learn robust feature representation, and ease the training difficulty in the early stage. We also provide the theoretical connection between the proposed LD and the classification KD, that they share the equivalent optimization effect. Our distillation scheme is simple as well as effective and can be easily applied to both dense horizontal object detectors and rotated object detectors. Extensive experiments on the MS COCO, PASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve considerable AP improvement without any sacrifice on the inference speed. Our source code and pretrained models are publicly available at https://github.com/HikariTJU/LD .},
  archive      = {J_TPAMI},
  author       = {Zhaohui Zheng and Rongguang Ye and Qibin Hou and Dongwei Ren and Ping Wang and Wangmeng Zuo and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2023.3248583},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10070-10083},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Localization distillation for object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local-global context aware transformer for language-guided
video segmentation. <em>TPAMI</em>, <em>45</em>(8), 10055–10069. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the task of language-guided video segmentation (LVS). Previous algorithms mostly adopt 3D CNNs to learn video representation, struggling to capture long-term context and easily suffering from visual-linguistic misalignment. In light of this, we present Locater ( lo cal-global c ontext a ware T ransform er ), which augments the Transformer architecture with a finite memory so as to query the entire video with the language expression in an efficient manner. The memory is designed to involve two components – one for persistently preserving global video content, and one for dynamically gathering local temporal context and segmentation history. Based on the memorized local-global context and the particular content of each frame, Locater holistically and flexibly comprehends the expression as an adaptive query vector for each frame. The vector is used to query the corresponding frame for mask generation. The memory also allows Locater to process videos with linear time complexity and constant size memory, while Transformer-style self-attention computation scales quadratically with sequence length. To thoroughly examine the visual grounding capability of LVS models, we contribute a new LVS dataset, A2D-S $^+$ , which is built upon A2D-S dataset but poses increased challenges in disambiguating among similar objects. Experiments on three LVS datasets and our A2D-S $^+$ show that Locater outperforms previous state-of-the-arts. Further, we won the 1 st place in the Referring Video Object Segmentation Track of the 3 rd Large-scale Video Object Segmentation Challenge, where Locater served as the foundation for the winning solution.},
  archive      = {J_TPAMI},
  author       = {Chen Liang and Wenguan Wang and Tianfei Zhou and Jiaxu Miao and Yawei Luo and Yi Yang},
  doi          = {10.1109/TPAMI.2023.3262578},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10055-10069},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Local-global context aware transformer for language-guided video segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning with asymmetric kernels: Least squares and feature
interpretation. <em>TPAMI</em>, <em>45</em>(8), 10044–10054. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric kernels naturally exist in real life, e.g., for conditional probability and directed graphs. However, most of the existing kernel-based learning methods require kernels to be symmetric, which prevents the use of asymmetric kernels. This paper addresses the asymmetric kernel-based learning in the framework of the least squares support vector machine named AsK-LS , resulting in the first classification method that can utilize asymmetric kernels directly. We will show that AsK-LS can learn with asymmetric features, namely source and target features, while the kernel trick remains applicable, i.e., the source and target features exist but are not necessarily known. Besides, the computational burden of AsK-LS is as cheap as dealing with symmetric kernels. Experimental results on various tasks, including Corel, PASCAL VOC, Satellite, directed graphs, and UCI database, all show that in the case asymmetric information is crucial, the proposed AsK-LS can learn with asymmetric kernels and performs much better than the existing kernel methods that rely on symmetrization to accommodate asymmetric kernels.},
  archive      = {J_TPAMI},
  author       = {Mingzhen He and Fan He and Lei Shi and Xiaolin Huang and Johan A. K. Suykens},
  doi          = {10.1109/TPAMI.2023.3257351},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10044-10054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning with asymmetric kernels: Least squares and feature interpretation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to super-resolve blurry images with events.
<em>TPAMI</em>, <em>45</em>(8), 10027–10043. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {S uper- R esolution from a single motion B lurred image (SRB) is a severely ill-posed problem due to the joint degradation of motion blurs and low spatial resolution. In this article, we employ events to alleviate the burden of SRB and propose an E vent-enhanced SRB (E-SRB) algorithm, which can generate a sequence of sharp and clear images with H igh R esolution (HR) from a single blurry image with L ow R esolution (LR). To achieve this end, we formulate an event-enhanced degeneration model to consider the low spatial resolution, motion blurs, and event noises simultaneously. We then build an e vent-enhanced S parse L earning Net work ( eSL-Net++ ) upon a dual sparse learning scheme where both events and intensity frames are modeled with sparse representations. Furthermore, we propose an event shuffle-and-merge scheme to extend the single-frame SRB to the sequence-frame SRB without any additional training process. Experimental results on synthetic and real-world datasets show that the proposed eSL-Net++ outperforms state-of-the-art methods by a large margin. Datasets, codes, and more results are available at https://github.com/ShinyWang33/eSL-Net-Plusplus .},
  archive      = {J_TPAMI},
  author       = {Lei Yu and Bishan Wang and Xiang Zhang and Haijian Zhang and Wen Yang and Jianzhuang Liu and Gui-Song Xia},
  doi          = {10.1109/TPAMI.2023.3240397},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10027-10043},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to super-resolve blurry images with events},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to augment poses for 3D human pose estimation in
images and videos. <em>TPAMI</em>, <em>45</em>(8), 10012–10026. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing 3D human pose estimation methods often suffer inferior generalization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data. To address this problem, we present PoseAug, a novel auto-augmentation framework that learns to augment the available training poses towards greater diversity and thus enhances the generalization power of the trained 2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose augmentor that learns to adjust various geometry factors of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to generate more diverse and harder poses in an online manner. PoseAug is generic and handy to be applied to various 3D pose estimation models. It is also extendable to aid pose estimation from video frames. To demonstrate this, we introduce PoseAug-V, a simple yet effective method that decomposes video pose augmentation into end pose augmentation and conditioned intermediate pose generation. Extensive experiments demonstrate that PoseAug and its extension PoseAug-V bring clear improvements for frame-based and video-based 3D pose estimation on several out-of-domain 3D human pose benchmarks.},
  archive      = {J_TPAMI},
  author       = {Jianfeng Zhang and Kehong Gong and Xinchao Wang and Jiashi Feng},
  doi          = {10.1109/TPAMI.2023.3243400},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {10012-10026},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to augment poses for 3D human pose estimation in images and videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rates for nonconvex pairwise learning.
<em>TPAMI</em>, <em>45</em>(8), 9996–10011. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise learning is receiving increasing attention since it covers many important machine learning tasks, e.g., metric learning, AUC maximization, and ranking. Investigating the generalization behavior of pairwise learning is thus of great significance. However, existing generalization analysis mainly focuses on the convex objective functions, leaving the nonconvex pairwise learning far less explored. Moreover, the current learning rates of pairwise learning are mostly of slower order. Motivated by these problems, we study the generalization performance of nonconvex pairwise learning and provide improved learning rates. Specifically, we develop different uniform convergence of gradients for pairwise learning under different assumptions, based on which we characterize empirical risk minimizer, gradient descent, and stochastic gradient descent. We first establish learning rates for these algorithms in a general nonconvex setting, where the analysis sheds insights on the trade-off between optimization and generalization and the role of early-stopping. We then derive faster learning rates of order $\mathcal {O}(1/n)$ for nonconvex pairwise learning with a gradient dominance curvature condition, where $n$ is the sample size. Provided that the optimal population risk is small, we further improve the learning rates to $\mathcal {O}(1/n^{2})$ , which, to the best of our knowledge, are the first $\mathcal {O}(1/n^{2})$ rates for pairwise learning.},
  archive      = {J_TPAMI},
  author       = {Shaojie Li and Yong Liu},
  doi          = {10.1109/TPAMI.2023.3259324},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9996-10011},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning rates for nonconvex pairwise learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning good features to transfer across tasks and domains.
<em>TPAMI</em>, <em>45</em>(8), 9981–9995. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Availability of labelled data is the major obstacle to the deployment of deep learning algorithms for computer vision tasks in new domains. The fact that many frameworks adopted to solve different tasks share the same architecture suggests that there should be a way of reusing the knowledge learned in a specific setting to solve novel tasks with limited or no additional supervision. In this work, we first show that such knowledge can be shared across tasks by learning a mapping between task-specific deep features in a given domain. Then, we show that this mapping function, implemented by a neural network, is able to generalize to novel unseen domains. Besides, we propose a set of strategies to constrain the learned feature spaces, to ease learning and increase the generalization capability of the mapping network, thereby considerably improving the final performance of our framework. Our proposal obtains compelling results in challenging synthetic-to-real adaptation scenarios by transferring knowledge between monocular depth estimation and semantic segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Pierluigi Zama Ramirez and Adriano Cardace and Luca De Luigi and Alessio Tonioni and Samuele Salti and Luigi Di Stefano},
  doi          = {10.1109/TPAMI.2023.3240316},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9981-9995},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning good features to transfer across tasks and domains},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent class-conditional noise model. <em>TPAMI</em>,
<em>45</em>(8), 9964–9980. (<a
href="https://doi.org/10.1109/TPAMI.2023.3247629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with noisy labels has become imperative in the Big Data era, which saves expensive human labors on accurate annotations. Previous noise-transition-based methods have achieved theoretically-grounded performance under the Class-Conditional Noise model (CCN). However, these approaches builds upon an ideal but impractical anchor set available to pre-estimate the noise transition. Even though subsequent works adapt the estimation as a neural layer, the ill-posed stochastic learning of its parameters in back-propagation easily falls into undesired local minimums. We solve this problem by introducing a Latent Class-Conditional Noise model (LCCN) to parameterize the noise transition under a Bayesian framework. By projecting the noise transition into the Dirichlet space, the learning is constrained on a simplex characterized by the complete dataset, instead of some ad-hoc parametric space wrapped by the neural layer. We then deduce a dynamic label regression method for LCCN, whose Gibbs sampler allows us efficiently infer the latent true labels to train the classifier and to model the noise. Our approach safeguards the stable update of the noise transition, which avoids previous arbitrarily tuning from a mini-batch of samples. We further generalize LCCN to different counterparts compatible with open-set noisy labels, semi-supervised learning as well as cross-model training. A range of experiments demonstrate the advantages of LCCN and its variants over the current state-of-the-art methods. The code is available at here .},
  archive      = {J_TPAMI},
  author       = {Jiangchao Yao and Bo Han and Zhihan Zhou and Ya Zhang and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2023.3247629},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9964-9980},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Latent class-conditional noise model},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale clustering with structured optimal bipartite
graph. <em>TPAMI</em>, <em>45</em>(8), 9950–9963. (<a
href="https://doi.org/10.1109/TPAMI.2023.3277532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread arising of data size gives rise to the necessity of undertaking large-scale data clustering tasks. To do so, the bipartite graph theory is frequently applied to design a scalable algorithm, which depicts the relations between samples and a few anchors, instead of binding pairwise samples. However, the bipartite graphs and existing spectral embedding methods ignore the explicit cluster structure learning. They have to obtain cluster labels by using post-processing like K -Means. More than that, existing anchor-based approaches always acquire anchors by using centroids of K -Means or a few random samples, both of which are time-saving but performance-unstable. In this paper, we investigate the scalability, stableness and integration in large-scale graph clustering. We propose a cluster-structured graph learning model, thus obtaining a $c$ -connected ( $c$ is the cluster number) bipartite graph and also getting discrete labels straightforward. Taking data feature or pairwise relation as a start point, we further design an initialization-independent anchor selection strategy. Experimental results reported for synthetic and real-world datasets demonstrate the proposed method outperforms its peers.},
  archive      = {J_TPAMI},
  author       = {Han Zhang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TPAMI.2023.3277532},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9950-9963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large-scale clustering with structured optimal bipartite graph},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large scale visual food recognition. <em>TPAMI</em>,
<em>45</em>(8), 9932–9949. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recognition plays an important role in food choice and intake, which is essential to the health and well‐being of humans. It is thus of importance to the computer vision community, and can further support many food-oriented vision and multimodal tasks, e.g., food detection and segmentation, cross-modal recipe retrieval and generation. Unfortunately, we have witnessed remarkable advancements in generic visual recognition for released large-scale datasets, yet largely lags in the food domain. In this paper, we introduce Food2K, which is the largest food recognition dataset with 2,000 categories and over 1 million images. Compared with existing food recognition datasets, Food2K bypasses them in both categories and images by one order of magnitude, and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning. Furthermore, we propose a deep progressive region enhancement network for food recognition, which mainly consists of two components, namely progressive local feature learning and region feature enhancement. The former adopts improved progressive training to learn diverse and complementary local features, while the latter utilizes self-attention to incorporate richer context with multiple scales into local features for further local feature enhancement. Extensive experiments on Food2K demonstrate the effectiveness of our proposed method. More importantly, we have verified better generalization ability of Food2K in various tasks, including food image recognition, food image retrieval, cross-modal recipe retrieval, food detection and segmentation. Food2K can be further explored to benefit more food-relevant tasks including emerging and more complex ones (e.g., nutritional understanding of food), and the trained models on Food2K can be expected as backbones to improve the performance of more food-relevant tasks. We also hope Food2K can serve as a large scale fine-grained visual recognition benchmark, and contributes to the development of large scale fine-grained visual analysis.},
  archive      = {J_TPAMI},
  author       = {Weiqing Min and Zhiling Wang and Yuxin Liu and Mengjiang Luo and Liping Kang and Xiaoming Wei and Xiaolin Wei and Shuqiang Jiang},
  doi          = {10.1109/TPAMI.2023.3237871},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9932-9949},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large scale visual food recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Jointly defending DeepFake manipulation and adversarial
attack using decoy mechanism. <em>TPAMI</em>, <em>45</em>(8), 9922–9931.
(<a href="https://doi.org/10.1109/TPAMI.2023.3253390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly realistic imaging and video synthesis have become possible and relatively simple tasks with the rapid growth of generative adversarial networks (GANs). GAN-related applications, such as DeepFake image and video manipulation and adversarial attacks, have been used to disrupt and confound the truth in images and videos over social media. DeepFake technology aims to synthesize high visual quality image content that can mislead the human vision system, while the adversarial perturbation attempts to mislead the deep neural networks to a wrong prediction. Defense strategy becomes difficult when adversarial perturbation and DeepFake are combined. This study examined a novel deceptive mechanism based on statistical hypothesis testing against DeepFake manipulation and adversarial attacks. First, a deceptive model based on two isolated sub-networks was designed to generate two-dimensional random variables with a specific distribution for detecting the DeepFake image and video. This research proposes a maximum likelihood loss for training the deceptive model with two isolated sub-networks. Afterward, a novel hypothesis was proposed for a testing scheme to detect the DeepFake video and images with a well-trained deceptive model. The comprehensive experiments demonstrated that the proposed decoy mechanism could be generalized to compressed and unseen manipulation methods for both DeepFake and attack detection.},
  archive      = {J_TPAMI},
  author       = {Guan-Lin Chen and Chih-Chung Hsu},
  doi          = {10.1109/TPAMI.2023.3253390},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9922-9931},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Jointly defending DeepFake manipulation and adversarial attack using decoy mechanism},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Insights from generative modeling for neural video
compression. <em>TPAMI</em>, <em>45</em>(8), 9908–9921. (<a
href="https://doi.org/10.1109/TPAMI.2023.3260684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent machine learning research has revealed connections between deep generative models such as VAEs and rate-distortion losses used in learned compression, most of this work has focused on images. In a similar spirit, we view recently proposed neural video coding algorithms through the lens of deep autoregressive and latent variable modeling. We present these codecs as instances of a generalized stochastic temporal autoregressive transform, and propose new avenues for further improvements inspired by normalizing flows and structured priors. We propose several architectures that yield state-of-the-art video compression performance on high-resolution video and discuss their tradeoffs and ablations. In particular, we propose (i) improved temporal autoregressive transforms, (ii) improved entropy models with structured and temporal dependencies, and (iii) variable bitrate versions of our algorithms. Since our improvements are compatible with a large class of existing models, we provide further evidence that the generative modeling viewpoint can advance the neural video coding field.},
  archive      = {J_TPAMI},
  author       = {Ruihan Yang and Yibo Yang and Joseph Marino and Stephan Mandt},
  doi          = {10.1109/TPAMI.2023.3260684},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9908-9921},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Insights from generative modeling for neural video compression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit neural representations with structured latent codes
for human body modeling. <em>TPAMI</em>, <em>45</em>(8), 9895–9907. (<a
href="https://doi.org/10.1109/TPAMI.2023.3245815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. Furthermore, we combine Neural Body with implicit surface models to improve the learned geometry. To evaluate our approach, we perform experiments on both synthetic and real-world data, which show that our approach outperforms prior works by a large margin on novel view synthesis and 3D reconstruction. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset.},
  archive      = {J_TPAMI},
  author       = {Sida Peng and Chen Geng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Xiaowei Zhou and Hujun Bao},
  doi          = {10.1109/TPAMI.2023.3245815},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9895-9907},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Implicit neural representations with structured latent codes for human body modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Image intensity variation information for interest point
detection. <em>TPAMI</em>, <em>45</em>(8), 9883–9894. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest point detection methods are gaining more attention and are widely applied in computer vision tasks such as image retrieval and 3D reconstruction. However, there still exist two main problems to be solved: (1) from the perspective of mathematical representations, the differences among edges, corners, and blobs have not been convincingly explained and the relationships among the amplitude response, scale factor, and filtering orientation for interest points have not been thoroughly explained; (2) the existing design mechanism for interest point detection does not show how to accurately obtain intensity variation information on corners and blobs. In this paper, the first- and second-order Gaussian directional derivative representations of a step edge, four common genres of corners, an anisotropic-type blob, and an isotropic-type blob are analyzed and derived. Multiple interest point characteristics are discovered. The characteristics for interest points that we obtained help us describe the differences among edges, corners, and blobs, explain why the existing interest point detection methods with multiple scales cannot properly obtain interest points from images, and present novel corner and blob detection methods. Extensive experiments demonstrate the superiority of our proposed methods in terms of detection performance, robustness to affine transformations, noise, image matching, and 3D reconstruction.},
  archive      = {J_TPAMI},
  author       = {Weichuan Zhang and Changming Sun and Yongsheng Gao},
  doi          = {10.1109/TPAMI.2023.3240129},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9883-9894},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image intensity variation information for interest point detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hitchhiker’s guide to super-resolution: Introduction and
recent advances. <em>TPAMI</em>, <em>45</em>(8), 9862–9882. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of Deep Learning (DL), Super-Resolution (SR) has also become a thriving research area. However, despite promising results, the field still faces challenges that require further research, e.g., allowing flexible upsampling, more effective loss functions, and better evaluation metrics. We review the domain of SR in light of recent advances and examine state-of-the-art models such as diffusion (DDPM) and transformer-based SR models. We critically discuss contemporary strategies used in SR and identify promising yet unexplored research directions. We complement previous surveys by incorporating the latest developments in the field, such as uncertainty-driven losses, wavelet networks, neural architecture search, novel normalization methods, and the latest evaluation techniques. We also include several visualizations for the models and methods throughout each chapter to facilitate a global understanding of the trends in the field. This review ultimately aims at helping researchers to push the boundaries of DL applied to SR.},
  archive      = {J_TPAMI},
  author       = {Brian B. Moser and Federico Raue and Stanislav Frolov and Sebastian Palacio and Jörn Hees and Andreas Dengel},
  doi          = {10.1109/TPAMI.2023.3243794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9862-9882},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hitchhiker&#39;s guide to super-resolution: Introduction and recent advances},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling open-set noise and novel target recognition in
domain adaptive semantic segmentation. <em>TPAMI</em>, <em>45</em>(8),
9846–9861. (<a
href="https://doi.org/10.1109/TPAMI.2023.3246392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a practical domain adaptive (DA) semantic segmentation problem where only pseudo-labeled target data is accessible through a black-box model. Due to the domain gap and label shift between two domains, pseudo-labeled target data contains mixed closed-set and open-set label noises. In this paper, we propose a simplex noise transition matrix (SimT) to model the mixed noise distributions in DA semantic segmentation, and leverage SimT to handle open-set label noise and enable novel target recognition . When handling open-set noises, we formulate the problem as estimation of SimT. By exploiting computational geometry analysis and properties of segmentation, we design four complementary regularizers, i.e., volume regularization, anchor guidance, convex guarantee, and semantic constraint, to approximate the true SimT. Specifically, volume regularization minimizes the volume of simplex formed by rows of the non-square SimT, ensuring outputs of model to fit into the ground truth label distribution. To compensate for the lack of open-set knowledge, anchor guidance, convex guarantee, and semantic constraint are devised to enable the modeling of open-set noise distribution. The estimated SimT is utilized to correct noise issues in pseudo labels and promote the generalization ability of segmentation model on target domain data. In the task of novel target recognition, we first propose closed-to-open label correction (C2OLC) to explicitly derive the supervision signal for open-set classes by exploiting the estimated SimT, and then advance a semantic relation (SR) loss that harnesses the inter-class relation to facilitate the open-set class sample recognition in target domain. Extensive experimental results demonstrate that the proposed SimT can be flexibly plugged into existing DA methods to boost both closed-set and open-set class performance.},
  archive      = {J_TPAMI},
  author       = {Xiaoqing Guo and Jie Liu and Tongliang Liu and Yixuan Yuan},
  doi          = {10.1109/TPAMI.2023.3246392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9846-9861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Handling open-set noise and novel target recognition in domain adaptive semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph theory based large-scale machine learning with
multi-dimensional constrained optimization approaches for exact
epidemiological modeling of pandemic diseases. <em>TPAMI</em>,
<em>45</em>(8), 9836–9845. (<a
href="https://doi.org/10.1109/TPAMI.2023.3256421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional prediction models of the pandemic diseases should be constructed in a way to reflect their peculiar epidemiological characters. In this paper, a graph theory-based constrained multi-dimensional (CM) mathematical and meta-heuristic algorithms (MA) are formed to learn the unknown parameters of a large-scale epidemiological model. The specified parameter signs and the coupling parameters of the sub-models constitute the constraints of the optimization problem. In addition, magnitude constraints on the unknown parameters are imposed to proportionally weight the input-output data importance. To learn these parameters, a gradient-based CM recursive least square (CM-RLS) algorithm, and three search-based MAs; namely, the CM particle swarm optimization (CM-PSO), the CM success history-based adaptive differential evolution (CM-SHADE), and the CM-SHADEWO enriched with the whale optimization (WO) algorithms are constructed. The traditional SHADE algorithm was the winner of the 2018 IEEE congress on evolutionary computation (CEC) and its versions in this paper are modified to create more certain parameter search spaces. The results obtained under the equal conditions show that the mathematical optimization algorithm CM-RLS outperforms the MA algorithms, which is expected since it uses the available gradient information. However, the search-based CM-SHADEWO algorithm is able to capture the dominant character of the CM optimization solution and produce satisfactory estimates in the presence of the hard constraints, uncertainties and lack of gradient information.},
  archive      = {J_TPAMI},
  author       = {Onder Tutsoy},
  doi          = {10.1109/TPAMI.2023.3256421},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9836-9845},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph theory based large-scale machine learning with multi-dimensional constrained optimization approaches for exact epidemiological modeling of pandemic diseases},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural network and spatiotemporal transformer
attention for 3D video object detection from point clouds.
<em>TPAMI</em>, <em>45</em>(8), 9822–9835. (<a
href="https://doi.org/10.1109/TPAMI.2021.3125981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works for LiDAR-based 3D object detection mainly focus on the single-frame paradigm. In this paper, we propose to detect 3D objects by exploiting temporal information in multiple frames, i.e., point cloud videos . We empirically categorize the temporal information into short-term and long-term patterns. To encode the short-term data, we present a Grid Message Passing Network (GMPNet), which considers each grid (i.e., the grouped points) as a node and constructs a $k$ -NN graph with the neighbor grids. To update features for a grid, GMPNet iteratively collects information from its neighbors, thus mining the motion cues in grids from nearby frames. To further aggregate long-term frames, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module. STA and TTA enhance the vanilla GRU to focus on small objects and better align moving objects. Our overall framework supports both online and offline video object detection in point clouds. We implement our algorithm based on prevalent anchor-based and anchor-free detectors. Evaluation results on the challenging nuScenes benchmark show superior performance of our method, achieving first on the leaderboard (at the time of paper submission) without any “bells and whistles.” Our source code is available at https://github.com/shenjianbing/GMP3D .},
  archive      = {J_TPAMI},
  author       = {Junbo Yin and Jianbing Shen and Xin Gao and David J. Crandall and Ruigang Yang},
  doi          = {10.1109/TPAMI.2021.3125981},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9822-9835},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph neural network and spatiotemporal transformer attention for 3D video object detection from point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GeoTransformer: Fast and robust point cloud registration
with geometric transformer. <em>TPAMI</em>, <em>45</em>(8), 9806–9821.
(<a href="https://doi.org/10.1109/TPAMI.2023.3259038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods have shown great potential through bypassing the detection of repeatable keypoints which is difficult to do especially in low-overlap scenarios. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer, or GeoTransformer for short, to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it invariant to rigid transformation and robust in low-overlap cases. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to 100 times acceleration. Extensive experiments on rich benchmarks encompassing indoor, outdoor, synthetic, multiway and non-rigid demonstrate the efficacy of GeoTransformer. Notably, our method improves the inlier ratio by $18{\sim }31$ percentage points and the registration recall by over 7 points on the challenging 3DLoMatch benchmark.},
  archive      = {J_TPAMI},
  author       = {Zheng Qin and Hao Yu and Changjian Wang and Yulan Guo and Yuxing Peng and Slobodan Ilic and Dewen Hu and Kai Xu},
  doi          = {10.1109/TPAMI.2023.3259038},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9806-9821},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GeoTransformer: Fast and robust point cloud registration with geometric transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). General greedy de-bias learning. <em>TPAMI</em>,
<em>45</em>(8), 9789–9805. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks often make predictions relying on the spurious correlations from the datasets rather than the intrinsic properties of the task of interest, facing with sharp degradation on out-of-distribution (OOD) test data. Existing de-bias learning frameworks try to capture specific dataset bias by annotations but they fail to handle complicated OOD scenarios. Others implicitly identify the dataset bias by special design low capability biased models or losses, but they degrade when the training and testing data are from the same distribution. In this paper, we propose a General Greedy De-bias learning framework (GGD), which greedily trains the biased models and base model. The base model is encouraged to focus on examples that are hard to solve with biased models, thus remaining robust against spurious correlations in the test stage. GGD largely improves models’ OOD generalization ability on various tasks, but sometimes over-estimates the bias level and degrades on the in-distribution test. We further re-analyze the ensemble process of GGD and introduce the Curriculum Regularization inspired by curriculum learning, which achieves a good trade-off between in-distribution (ID) and out-of-distribution performance. Extensive experiments on image classification, adversarial question answering, and visual question answering demonstrate the effectiveness of our method. GGD can learn a more robust base model under the settings of both task-specific biased models with prior knowledge and self-ensemble biased model without prior knowledge. Codes are available at https://github.com/GeraldHan/GGD .},
  archive      = {J_TPAMI},
  author       = {Xinzhe Han and Shuhui Wang and Chi Su and Qingming Huang and Qi Tian},
  doi          = {10.1109/TPAMI.2023.3240337},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9789-9805},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {General greedy de-bias learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully convolutional change detection framework with
generative adversarial network for unsupervised, weakly supervised and
regional supervised change detection. <em>TPAMI</em>, <em>45</em>(8),
9774–9788. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning for change detection is one of the current hot topics in the field of remote sensing. However, most end-to-end networks are proposed for supervised change detection, and unsupervised change detection models depend on traditional pre-detection methods. Therefore, we proposed a fully convolutional change detection framework with generative adversarial network, to unify unsupervised, weakly supervised, regional supervised, and fully supervised change detection tasks into one end-to-end framework. A basic Unet segmentor is used to obtain change detection map, an image-to-image generator is implemented to model the spectral and spatial variation between multi-temporal images, and a discriminator for changed and unchanged is proposed for modeling the semantic changes in weakly and regional supervised change detection task. The iterative optimization of segmentor and generator can build an end-to-end network for unsupervised change detection, the adversarial process between segmentor and discriminator can provide the solutions for weakly and regional supervised change detection, the segmentor itself can be trained for fully supervised task. The experiments indicate the effectiveness of the propsed framework in unsupervised, weakly supervised and regional supervised change detection. This article provides new theorical definitions for unsupervised, weakly supervised and regional supervised change detection tasks with the proposed framework, and shows great potentials in exploring end-to-end network for remote sensing change detection ( https://github.com/Cwuwhu/FCD-GAN-pytorch ).},
  archive      = {J_TPAMI},
  author       = {Chen Wu and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TPAMI.2023.3237896},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9774-9788},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fully convolutional change detection framework with generative adversarial network for unsupervised, weakly supervised and regional supervised change detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From instance to metric calibration: A unified framework for
open-world few-shot learning. <em>TPAMI</em>, <em>45</em>(8), 9757–9773.
(<a href="https://doi.org/10.1109/TPAMI.2023.3244023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust few-shot learning (RFSL), which aims to address noisy labels in few-shot learning, has recently gained considerable attention. Existing RFSL methods are based on the assumption that the noise comes from known classes (in-domain), which is inconsistent with many real-world scenarios where the noise does not belong to any known classes (out-of-domain). We refer to this more complex scenario as open-world few-shot learning (OFSL), where in-domain and out-of-domain noise simultaneously exists in few-shot datasets. To address the challenging problem, we propose a unified framework to implement comprehensive calibration from instance to metric. Specifically, we design a dual-networks structure composed of a contrastive network and a meta network to respectively extract feature-related intra-class information and enlarged inter-class variations. For instance-wise calibration, we present a novel prototype modification strategy to aggregate prototypes with intra-class and inter-class instance reweighting. For metric-wise calibration, we present a novel metric to implicitly scale the per-class prediction by fusing two spatial metrics respectively constructed by the two networks. In this way, the impact of noise in OFSL can be effectively mitigated from both feature space and label space. Extensive experiments on various OFSL settings demonstrate the robustness and superiority of our method. Our source codes is available at https://github.com/anyuexuan/IDEAL .},
  archive      = {J_TPAMI},
  author       = {Yuexuan An and Hui Xue and Xingyu Zhao and Jing Wang},
  doi          = {10.1109/TPAMI.2023.3244023},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9757-9773},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From instance to metric calibration: A unified framework for open-world few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Free-HeadGAN: Neural talking head synthesis with explicit
gaze control. <em>TPAMI</em>, <em>45</em>(8), 9743–9756. (<a
href="https://doi.org/10.1109/TPAMI.2023.3253243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Free-HeadGAN, a person-generic neural talking head synthesis system. We show that modeling faces with sparse 3D facial landmarks is sufficient for achieving state-of-the-art generative performance, without relying on strong statistical priors of the face, such as 3D Morphable Models. Apart from 3D pose and facial expressions, our method is capable of fully transferring the eye gaze, from a driving actor to a source identity. Our complete pipeline consists of three components: a canonical 3D key-point estimator that regresses 3D pose and expression-related deformations, a gaze estimation network and a generator that is built upon the architecture of HeadGAN. We further experiment with an extension of our generator to accommodate few-shot learning using an attention mechanism, in case multiple source images are available. Compared to recent methods for reenactment and motion transfer, our system achieves higher photo-realism combined with superior identity preservation, while offering explicit gaze control.},
  archive      = {J_TPAMI},
  author       = {Michail Christos Doukas and Evangelos Ververas and Viktoriia Sharmanska and Stefanos Zafeiriou},
  doi          = {10.1109/TPAMI.2023.3253243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9743-9756},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Free-HeadGAN: Neural talking head synthesis with explicit gaze control},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flattening-net: Deep regular 2D representation for 3D point
cloud analysis. <em>TPAMI</em>, <em>45</em>(8), 9726–9742. (<a
href="https://doi.org/10.1109/TPAMI.2023.3244828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are characterized by irregularity and unstructuredness, which pose challenges in efficient data exploitation and discriminative feature extraction. In this paper, we present an unsupervised deep neural architecture called Flattening-Net to represent irregular 3D point clouds of arbitrary geometry and topology as a completely regular 2D point geometry image (PGI) structure, in which coordinates of spatial points are captured in colors of image pixels. Intuitively, Flattening-Net implicitly approximates a locally smooth 3D-to-2D surface flattening process while effectively preserving neighborhood consistency. As a generic representation modality, PGI inherently encodes the intrinsic property of the underlying manifold structure and facilitates surface-style point feature aggregation. To demonstrate its potential, we construct a unified learning framework directly operating on PGIs to achieve diverse types of high-level and low-level downstream applications driven by specific task networks, including classification, segmentation, reconstruction, and upsampling. Extensive experiments demonstrate that our methods perform favorably against the current state-of-the-art competitors.},
  archive      = {J_TPAMI},
  author       = {Qijian Zhang and Junhui Hou and Yue Qian and Yiming Zeng and Juyong Zhang and Ying He},
  doi          = {10.1109/TPAMI.2023.3244828},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9726-9742},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Flattening-net: Deep regular 2D representation for 3D point cloud analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot drug synergy prediction with a prior-guided
hypernetwork architecture. <em>TPAMI</em>, <em>45</em>(8), 9709–9725.
(<a href="https://doi.org/10.1109/TPAMI.2023.3248041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug synergy is critical to tailoring feasible drug combination treatment regimens for cancer patients. However, most of the existing computational methods only focus on data-rich cell lines, and hardly work on data-poor cell lines. To this end, here we proposed a novel few-shot drug synergy prediction method (called HyperSynergy) for data-poor cell lines by designing a prior-guided Hypernetwork architecture, in which the meta-generative network based on the task embedding of each cell line generates cell line dependent parameters for the drug synergy prediction network. In HyperSynergy model, we designed a deep Bayesian variational inference model to infer the prior distribution over the task embedding to quickly update the task embedding with a few labeled drug synergy samples, and presented a three-stage learning strategy to train HyperSynergy for quickly updating the prior distribution by a few labeled drug synergy samples of each data-poor cell line. Moreover, we proved theoretically that HyperSynergy aims to maximize the lower bound of log-likelihood of the marginal distribution over each data-poor cell line. The experimental results show that our HyperSynergy outperforms other state-of-the-art methods not only on data-poor cell lines with a few samples (e.g., 10, 5, 0), but also on data-rich cell lines.},
  archive      = {J_TPAMI},
  author       = {Qing-Qing Zhang and Shao-Wu Zhang and Yue-Hua Feng and Jian-Yu Shi},
  doi          = {10.1109/TPAMI.2023.3248041},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9709-9725},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Few-shot drug synergy prediction with a prior-guided hypernetwork architecture},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated learning via inexact ADMM. <em>TPAMI</em>,
<em>45</em>(8), 9699–9708. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the crucial issues in federated learning is how to develop efficient optimization algorithms. Most of the current ones require full device participation and/or impose strong assumptions for convergence. Different from the widely-used gradient descent-based algorithms, in this article, we develop an inexact alternating direction method of multipliers (ADMM), which is both computation- and communication-efficient, capable of combating the stragglers’ effect, and convergent under mild conditions. Furthermore, it has high numerical performance compared with several state-of-the-art algorithms for federated learning.},
  archive      = {J_TPAMI},
  author       = {Shenglong Zhou and Geoffrey Ye Li},
  doi          = {10.1109/TPAMI.2023.3243080},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9699-9708},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Federated learning via inexact ADMM},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and robust non-rigid registration using accelerated
majorization-minimization. <em>TPAMI</em>, <em>45</em>(8), 9681–9698.
(<a href="https://doi.org/10.1109/TPAMI.2023.3247603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid 3D registration, which deforms a source 3D shape in a non-rigid way to align with a target 3D shape, is a classical problem in computer vision. Such problems can be challenging because of imperfect data (noise, outliers and partial overlap) and high degrees of freedom. Existing methods typically adopt the $\ell _{p}$ type robust norm to measure the alignment error and regularize the smoothness of deformation, and use a proximal algorithm to solve the resulting non-smooth optimization problem. However, the slow convergence of such algorithms limits their wide applications. In this paper, we propose a formulation for robust non-rigid registration based on a globally smooth robust norm for alignment and regularization, which can effectively handle outliers and partial overlaps. The problem is solved using the majorization-minimization algorithm, which reduces each iteration to a convex quadratic problem with a closed-form solution. We further apply Anderson acceleration to speed up the convergence of the solver, enabling the solver to run efficiently on devices with limited compute capability. Extensive experiments demonstrate the effectiveness of our method for non-rigid alignment between two shapes with outliers and partial overlaps, with quantitative evaluation showing that it outperforms state-of-the-art methods in terms of registration accuracy and computational speed. The source code is available at https://github.com/yaoyx689/AMM_NRR .},
  archive      = {J_TPAMI},
  author       = {Yuxin Yao and Bailin Deng and Weiwei Xu and Juyong Zhang},
  doi          = {10.1109/TPAMI.2023.3247603},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9681-9698},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast and robust non-rigid registration using accelerated majorization-minimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and informative model selection using learning curve
cross-validation. <em>TPAMI</em>, <em>45</em>(8), 9669–9680. (<a
href="https://doi.org/10.1109/TPAMI.2023.3251957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common cross-validation (CV) methods like k-fold cross-validation or Monte Carlo cross-validation estimate the predictive performance of a learner by repeatedly training it on a large portion of the given data and testing it on the remaining data. These techniques have two major drawbacks. First, they can be unnecessarily slow on large datasets. Second, beyond an estimation of the final performance, they give almost no insights into the learning process of the validated algorithm. In this article, we present a new approach for validation based on learning curves (LCCV). Instead of creating train-test splits with a large portion of training data, LCCV iteratively increases the number of instances used for training. In the context of model selection, it discards models that are unlikely to become competitive. In a series of experiments on 75 datasets, we could show that in over 90\% of the cases using LCCV leads to the same performance as using 5/10-fold CV while substantially reducing the runtime (median runtime reductions of over 50\%); the performance using LCCV never deviated from CV by more than 2.5\%. We also compare it to a racing-based method and successive halving, a multi-armed bandit method. Additionally, it provides important insights, which for example allows assessing the benefits of acquiring more data.},
  archive      = {J_TPAMI},
  author       = {Felix Mohr and Jan N. van Rijn},
  doi          = {10.1109/TPAMI.2023.3251957},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9669-9680},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast and informative model selection using learning curve cross-validation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extracting semantic knowledge from GANs with unsupervised
learning. <em>TPAMI</em>, <em>45</em>(8), 9654–9668. (<a
href="https://doi.org/10.1109/TPAMI.2023.3262140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised learning has made impressive progress on various tasks. Despite the dominance of discriminative models, increasing attention is drawn to representations learned by generative models and in particular, Generative Adversarial Networks (GANs). Previous works on the interpretation of GANs reveal that GANs encode semantics in feature maps in a linearly separable form. In this work, we further find that GAN&#39;s features can be well clustered with the linear separability assumption. We propose a novel clustering algorithm, named KLiSH, which leverages the linear separability to cluster GAN&#39;s features. KLiSH succeeds in extracting fine-grained semantics of GANs trained on datasets of various objects, e.g., car, portrait, animals, and so on. With KLiSH, we can sample images from GANs along with their segmentation masks and synthesize paired image-segmentation datasets. Using the synthesized datasets, we enable two downstream applications. First, we train semantic segmentation networks on these datasets and test them on real images, realizing unsupervised semantic segmentation. Second, we train image-to-image translation networks on the synthesized datasets, enabling semantic-conditional image synthesis without human annotations.},
  archive      = {J_TPAMI},
  author       = {Jianjin Xu and Zhaoxiang Zhang and Xiaolin Hu},
  doi          = {10.1109/TPAMI.2023.3262140},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9654-9668},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Extracting semantic knowledge from GANs with unsupervised learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating classification model against bayes error rate.
<em>TPAMI</em>, <em>45</em>(8), 9639–9653. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a classification task, we usually select an appropriate classifier via model selection. How to evaluate whether the chosen classifier is optimal? One can answer this question via Bayes error rate (BER). Unfortunately, estimating BER is a fundamental conundrum. Most existing BER estimators focus on giving the upper and lower bounds of the BER. However, evaluating whether the selected classifier is optimal based on these bounds is hard. In this article, we aim to learn the exact BER instead of bounds on BER. The core of our method is to transform the BER calculation problem into a noise recognition problem. Specifically, we define a type of noise called Bayes noise and prove that the proportion of Bayes noisy samples in a data set is statistically consistent with the BER of the data set. To recognize the Bayes noisy samples, we present a method consisting of two parts: selecting reliable samples based on percolation theory and then employing a label propagation algorithm to recognize the Bayes noisy samples based on the selected reliable samples. The superiority of the proposed method compared to the existing BER estimators is verified on extensive synthetic, benchmark, and image data sets.},
  archive      = {J_TPAMI},
  author       = {Qingqiang Chen and Fuyuan Cao and Ying Xing and Jiye Liang},
  doi          = {10.1109/TPAMI.2023.3240194},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9639-9653},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evaluating classification model against bayes error rate},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DeepEIT: Deep image prior enabled electrical impedance
tomography. <em>TPAMI</em>, <em>45</em>(8), 9627–9638. (<a
href="https://doi.org/10.1109/TPAMI.2023.3240565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have been widely applied in tomographic imaging through data-driven training and image processing. One of the main challenges in using NNs in real medical imaging is the requirement of massive amounts of training data – which are not always available in clinical practice. In this article, we demonstrate that, on the contrary, one can directly execute image reconstruction using NNs without training data. The key idea is to bring in the recently introduced deep image prior (DIP) and merge it with electrical impedance tomography (EIT) reconstruction. DIP provides a novel approach to the regularization of EIT reconstruction problems by compelling the recovered image to be synthesized from a given NN architecture. Then, by relying on the NN&#39;s built-in back-propagation and the finite element solver, the conductivity distribution is optimized. Quantitative results based on simulation and experimental data show that the proposed method is an effective unsupervised approach capable of outperforming state-of-the-art alternatives.},
  archive      = {J_TPAMI},
  author       = {Dong Liu and Junwu Wang and Qianxue Shan and Danny Smyl and Jiansong Deng and Jiangfeng Du},
  doi          = {10.1109/TPAMI.2023.3240565},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9627-9638},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepEIT: Deep image prior enabled electrical impedance tomography},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAQE: Enhancing the quality of compressed images by
exploiting the inherent characteristic of defocus. <em>TPAMI</em>,
<em>45</em>(8), 9611–9626. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image defocus is inherent in the physics of image formation caused by the optical aberration of lenses, providing plentiful information on image quality. Unfortunately, existing quality enhancement approaches for compressed images neglect the inherent characteristic of defocus, resulting in inferior performance. This paper finds that in compressed images, significantly defocused regions have better compression quality, and two regions with different defocus values possess diverse texture patterns. These observations motivate our defocus-aware quality enhancement (DAQE) approach. Specifically, we propose a novel dynamic region-based deep learning architecture of the DAQE approach, which considers the regionwise defocus difference of compressed images in two aspects. (1) The DAQE approach employs fewer computational resources to enhance the quality of significantly defocused regions and more resources to enhance the quality of other regions; (2) The DAQE approach learns to separately enhance diverse texture patterns for regions with different defocus values, such that texture-specific enhancement can be achieved. Extensive experiments validate the superiority of our DAQE approach over state-of-the-art approaches in terms of quality enhancement and resource savings.},
  archive      = {J_TPAMI},
  author       = {Qunliang Xing and Mai Xu and Xin Deng and Yichen Guo},
  doi          = {10.1109/TPAMI.2023.3257888},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9611-9626},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DAQE: Enhancing the quality of compressed images by exploiting the inherent characteristic of defocus},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal retrieval with partially mismatched pairs.
<em>TPAMI</em>, <em>45</em>(8), 9595–9610. (<a
href="https://doi.org/10.1109/TPAMI.2023.3247939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a challenging but less-touched problem in cross-modal retrieval, i.e., partially mismatched pairs (PMPs). Specifically, in real-world scenarios, a huge number of multimedia data (e.g., the Conceptual Captions dataset) are collected from the Internet, and thus it is inevitable to wrongly treat some irrelevant cross-modal pairs as matched. Undoubtedly, such a PMP problem will remarkably degrade the cross-modal retrieval performance. To tackle this problem, we derive a unified theoretical Robust Cross-modal Learning framework (RCL) with an unbiased estimator of the cross-modal retrieval risk, which aims to endow the cross-modal retrieval methods with robustness against PMPs. In detail, our RCL adopts a novel complementary contrastive learning paradigm to address the following two challenges, i.e., the overfitting and underfitting issues. On the one hand, our method only utilizes the negative information which is much less likely false compared with the positive information, thus avoiding the overfitting issue to PMPs. However, these robust strategies could induce underfitting issues, thus making training models more difficult. On the other hand, to address the underfitting issue brought by weak supervision, we present to leverage of all available negative pairs to enhance the supervision contained in the negative information. Moreover, to further improve the performance, we propose to minimize the upper bounds of the risk to pay more attention to hard samples. To verify the effectiveness and robustness of the proposed method, we carry out comprehensive experiments on five widely-used benchmark datasets compared with nine state-of-the-art approaches w.r.t. the image-text and video-text retrieval tasks. The code is available at https://github.com/penghu-cs/RCL .},
  archive      = {J_TPAMI},
  author       = {Peng Hu and Zhenyu Huang and Dezhong Peng and Xu Wang and Xi Peng},
  doi          = {10.1109/TPAMI.2023.3247939},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9595-9610},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-modal retrieval with partially mismatched pairs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CP3: Unifying point cloud completion by
pretrain-prompt-predict paradigm. <em>TPAMI</em>, <em>45</em>(8),
9583–9594. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to predict complete shape from its partial observation. Current approaches mainly consist of generation and refinement stages in a coarse-to-fine style. However, the generation stage often lacks robustness to tackle different incomplete variations, while the refinement stage blindly recovers point clouds without the semantic awareness. To tackle these challenges, we unify point cloud C ompletion by a generic P retrain- P rompt- P redict paradigm, namely CP3 . Inspired by prompting approaches from NLP, we creatively reinterpret point cloud generation and refinement as the prompting and predicting stages, respectively. Then, we introduce a concise self-supervised pretraining stage before prompting. It can effectively increase robustness of point cloud generation, by an Incompletion-Of-Incompletion (IOI) pretext task. Moreover, we develop a novel Semantic Conditional Refinement (SCR) network at the predicting stage. It can discriminatively modulate multi-scale refinement with the guidance of semantics. Finally, extensive experiments demonstrate that our CP3 outperforms the state-of-the-art methods with a large margin. code will be available at https://github.com/MingyeXu/cp3 .},
  archive      = {J_TPAMI},
  author       = {Mingye Xu and Yali Wang and Yihao Liu and Tong He and Yu Qiao},
  doi          = {10.1109/TPAMI.2023.3257026},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9583-9594},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CP3: Unifying point cloud completion by pretrain-prompt-predict paradigm},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoReS: Compatible representations via stationarity.
<em>TPAMI</em>, <em>45</em>(8), 9567–9582. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compatible features enable the direct comparison of old and new learned features allowing to use them interchangeably over time. In visual search systems, this eliminates the need to extract new features from the gallery-set when the representation model is upgraded with novel data. This has a big value in real applications as re-indexing the gallery-set can be computationally expensive when the gallery-set is large, or even infeasible due to privacy or other concerns of the application. In this paper, we propose CoReS, a new training procedure to learn representations that are compatible with those previously learned, grounding on the stationarity of the features as provided by fixed classifiers based on polytopes. With this solution, classes are maximally separated in the representation space and maintain their spatial configuration stationary as new classes are added, so that there is no need to learn any mappings between representations nor to impose pairwise training with the previously learned model. We demonstrate that our training procedure largely outperforms the current state of the art and is particularly effective in the case of multiple upgrades of the training-set, which is the typical case in real applications.},
  archive      = {J_TPAMI},
  author       = {Niccolò Biondi and Federico Pernici and Matteo Bruni and Alberto Del Bimbo},
  doi          = {10.1109/TPAMI.2023.3259542},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9567-9582},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CoReS: Compatible representations via stationarity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive multi-view kernel learning. <em>TPAMI</em>,
<em>45</em>(8), 9552–9566. (<a
href="https://doi.org/10.1109/TPAMI.2023.3253211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel method is a proven technique in multi-view learning. It implicitly defines a Hilbert space where samples can be linearly separated. Most kernel-based multi-view learning algorithms compute a kernel function aggregating and compressing the views into a single kernel. However, existing approaches compute the kernels independently for each view. This ignores complementary information across views and thus may result in a bad kernel choice. In contrast, we propose the Contrastive Multi-view Kernel — a novel kernel function based on the emerging contrastive learning framework. The Contrastive Multi-view Kernel implicitly embeds the views into a joint semantic space where all of them resemble each other while promoting to learn diverse views. We validate the method&#39;s effectiveness in a large empirical study. It is worth noting that the proposed kernel functions share the types and parameters with traditional ones, making them fully compatible with existing kernel theory and application. On this basis, we also propose a contrastive multi-view clustering framework and instantiate it with multiple kernel $k$ -means, achieving a promising performance. To the best of our knowledge, this is the first attempt to explore kernel generation in multi-view setting and the first approach to use contrastive learning for a multi-view kernel learning.},
  archive      = {J_TPAMI},
  author       = {Jiyuan Liu and Xinwang Liu and Yuexiang Yang and Qing Liao and Yuanqing Xia},
  doi          = {10.1109/TPAMI.2023.3253211},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9552-9566},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive multi-view kernel learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual image deraining with hypergraph convolutional
networks. <em>TPAMI</em>, <em>45</em>(8), 9534–9551. (<a
href="https://doi.org/10.1109/TPAMI.2023.3241756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deraining is a challenging task since rain streaks have the characteristics of a spatially long structure and have a complex diversity. Existing deep learning-based methods mainly construct the deraining networks by stacking vanilla convolutional layers with local relations, and can only handle a single dataset due to catastrophic forgetting, resulting in a limited performance and insufficient adaptability. To address these issues, we propose a new image deraining framework to effectively explore nonlocal similarity, and to continuously learn on multiple datasets. Specifically, we first design a patchwise hypergraph convolutional module, which aims to better extract the nonlocal properties with higher-order constraints on the data, to construct a new backbone and to improve the deraining performance. Then, to achieve better generalizability and adaptability in real-world scenarios, we propose a biological brain-inspired continual learning algorithm. By imitating the plasticity mechanism of brain synapses during the learning and memory process, our continual learning process allows the network to achieve a subtle stability-plasticity tradeoff. This it can effectively alleviate catastrophic forgetting and enables a single network to handle multiple datasets. Compared with the competitors, our new deraining network with unified parameters attains a state-of-the-art performance on seen synthetic datasets and has a significantly improved generalizability on unseen real rainy images.},
  archive      = {J_TPAMI},
  author       = {Xueyang Fu and Jie Xiao and Yurui Zhu and Aiping Liu and Feng Wu and Zheng-Jun Zha},
  doi          = {10.1109/TPAMI.2023.3241756},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9534-9551},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Continual image deraining with hypergraph convolutional networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual instance decoupling for instance-level human
analysis. <em>TPAMI</em>, <em>45</em>(8), 9520–9533. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental challenge of instance-level human analysis is to decouple instances in crowded scenes, where multiple persons are overlapped with each other. This paper proposes the Contextual Instance Decoupling (CID), which presents a new pipeline of decoupling persons for multi-person instance-level analysis. Instead of relying on person bounding boxes to spatially differentiate persons, CID decouples persons in an image into multiple instance-aware feature maps. Each of those feature maps is hence adopted to infer instance-level cues for a specific person, e.g., keypoints, instance mask or part segmentation masks. Compared with bounding box detection, CID is differentiable and robust to detection errors. Decoupling persons into different feature maps also allows to isolate distractions from other persons, and explore context cues at scales larger than the bounding box size. Extensive experiments on various tasks including multi-person pose estimation, person foreground segmentation, and part segmentation, show that CID consistently outperforms previous methods in both accuracy and efficiency. For instance, it achieves 71.3\% AP on CrowdPose in multi-person pose estimation, outperforming the recent single-stage DEKR by 5.6\%, the bottom-up CenterAttention by 3.7\%, and the top-down JC-SPPE by 5.3\%. This advantage sustains on multi-person segmentation and part segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Dongkai Wang and Shiliang Zhang},
  doi          = {10.1109/TPAMI.2023.3243223},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9520-9533},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contextual instance decoupling for instance-level human analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ContextLoc++: A unified context model for temporal action
localization. <em>TPAMI</em>, <em>45</em>(8), 9504–9519. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching the local, global and multi-scale contexts in the popular two-stage temporal localization framework. Our proposed model, dubbed ContextLoc++, can be divided into three sub-networks: L-Net, G-Net, and M-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. Furthermore, the spatial and temporal snippet-level features, functioning as keys and values, are fused by temporal gating. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. M-Net further fuses the local and global contexts with multi-scale proposal features. Specially, proposal-level features from multi-scale video snippets can focus on different action characteristics. Short-term snippets with fewer frames pay attention to the action details while long-term snippets with more frames focus on the action variations. Experiments on the THUMOS14 and ActivityNet v1.3 datasets validate the efficacy of our method against existing state-of-the-art TAL algorithms.},
  archive      = {J_TPAMI},
  author       = {Zixin Zhu and Le Wang and Wei Tang and Nanning Zheng and Gang Hua},
  doi          = {10.1109/TPAMI.2023.3237597},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9504-9519},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ContextLoc++: A unified context model for temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Content-aware warping for view synthesis. <em>TPAMI</em>,
<em>45</em>(8), 9486–9503. (<a
href="https://doi.org/10.1109/TPAMI.2023.3242709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image-based rendering methods usually adopt depth-based image warping operation to synthesize novel views. In this paper, we reason the essential limitations of the traditional warping operation to be the limited neighborhood and only distance-based interpolation weights. To this end, we propose content-aware warping , which adaptively learns the interpolation weights for pixels of a relatively large neighborhood from their contextual information via a lightweight neural network. Based on this learnable warping module, we propose a new end-to-end learning-based framework for novel view synthesis from a set of input source views, in which two additional modules, namely confidence-based blending and feature-assistant spatial refinement, are naturally proposed to handle the occlusion issue and capture the spatial correlation among pixels of the synthesized view, respectively. Besides, we also propose a weight-smoothness loss term to regularize the network. Experimental results on light field datasets with wide baselines and multi-view datasets show that the proposed method significantly outperforms state-of-the-art methods both quantitatively and visually. The source code is publicly available at https://github.com/MantangGuo/CW4VS .},
  archive      = {J_TPAMI},
  author       = {Mantang Guo and Junhui Hou and Jing Jin and Hui Liu and Huanqiang Zeng and Jiwen Lu},
  doi          = {10.1109/TPAMI.2023.3242709},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9486-9503},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Content-aware warping for view synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistent 3D hand reconstruction in video via
self-supervised learning. <em>TPAMI</em>, <em>45</em>(8), 9469–9485. (<a
href="https://doi.org/10.1109/TPAMI.2023.3247907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that the detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Accordingly, in this work, we propose $\mathrm{{S}^{2}HAND}$ , a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and explore $\mathrm{{S}^{2}HAND(V)}$ , which uses a set of weights shared $\mathrm{{S}^{2}HAND}$ to process each frame and exploits additional motion, texture, and shape consistency constrains to obtain more accurate hand poses, and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised method produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using the video training data.},
  archive      = {J_TPAMI},
  author       = {Zhigang Tu and Zhisheng Huang and Yujin Chen and Di Kang and Linchao Bao and Bisheng Yang and Junsong Yuan},
  doi          = {10.1109/TPAMI.2023.3247907},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9469-9485},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Consistent 3D hand reconstruction in video via self-supervised learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conformer: Local features coupling global representations
for recognition and detection. <em>TPAMI</em>, <em>45</em>(8),
9454–9468. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With convolution operations, Convolutional Neural Networks (CNNs) are good at extracting local features but experience difficulty to capture global representations. With cascaded self-attention modules, vision transformers can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take both advantages of convolution operations and self-attention mechanisms for enhanced representation learning. Conformer roots in feature coupling of CNN local features and transformer global representations under different resolutions in an interactive fashion. Conformer adopts a dual structure so that local details and global dependencies are retained to the maximum extent. We also propose a Conformer-based detector (ConformerDet), which learns to predict and refine object proposals, by performing region-level feature coupling in an augmented cross-attention fashion. Experiments on ImageNet and MS COCO datasets validate Conformer&#39;s superiority for visual recognition and object detection, demonstrating its potential to be a general backbone network.},
  archive      = {J_TPAMI},
  author       = {Zhiliang Peng and Zonghao Guo and Wei Huang and Yaowei Wang and Lingxi Xie and Jianbin Jiao and Qi Tian and Qixiang Ye},
  doi          = {10.1109/TPAMI.2023.3243048},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9454-9468},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Conformer: Local features coupling global representations for recognition and detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coarse-to-fine disentangling demoiréing framework for
recaptured screen images. <em>TPAMI</em>, <em>45</em>(8), 9439–9453. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing the undesired moiré patterns from images capturing the contents displayed on screens is of increasing research interest, as the need for recording and sharing the instant information conveyed by the screens is growing. Previous demoiréing methods provide limited investigations into the formation process of moiré patterns to exploit moiré-specific priors for guiding the learning of demoiréing models. In this paper, we investigate the moiré pattern formation process from the perspective of signal aliasing, and correspondingly propose a coarse-to-fine disentangling demoiréing framework. In this framework, we first disentangle the moiré pattern layer and the clean image with alleviated ill-posedness based on the derivation of our moiré image formation model. Then we refine the demoiréing results exploiting both the frequency domain features and edge attention, considering moiré patterns’ property on spectrum distribution and edge intensity revealed in our aliasing based analysis. Experiments on several datasets show that the proposed method performs favorably against state-of-the-art methods. Besides, the proposed method is validated to adapt well to different data sources and scales, especially on the high-resolution moiré images.},
  archive      = {J_TPAMI},
  author       = {Ce Wang and Bin He and Shengsen Wu and Renjie Wan and Boxin Shi and Ling-Yu Duan},
  doi          = {10.1109/TPAMI.2023.3243310},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9439-9453},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Coarse-to-fine disentangling demoiréing framework for recaptured screen images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustered task-aware meta-learning by learning from learning
paths. <em>TPAMI</em>, <em>45</em>(8), 9426–9438. (<a
href="https://doi.org/10.1109/TPAMI.2023.3250323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable effective learning of new tasks with only a few examples, meta-learning acquires common knowledge from the existing tasks with a globally shared meta-learner. To further address the problem of task heterogeneity, recent developments balance between customization and generalization by incorporating task clustering to generate task-aware modulation to be applied to the global meta-learner. However, these methods learn task representation mostly from the features ofinput data, while the task-specific optimization process with respect to the base-learner is often neglected. In this work, we propose a C lustered T ask-Aware M eta- L earning (CTML) framework with task representation learned from both features and learning paths. We first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes this learning path. By inputting this set of values into a meta path learner, we automatically abstract path representation optimized for downstream clustering and modulation. Aggregating the path and feature representations results in an improved task representation. To further improve inference efficiency, we devise a shortcut tunnel to bypass the rehearsed learning process at a meta-testing time. Extensive experiments on two real-world application domains: few-shot image classification and cold-start recommendation demonstrate the superiority of CTML compared to state-of-the-art methods. We provide our code at https://github.com/didiya0825 .},
  archive      = {J_TPAMI},
  author       = {Danni Peng and Sinno Jialin Pan},
  doi          = {10.1109/TPAMI.2023.3250323},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9426-9438},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Clustered task-aware meta-learning by learning from learning paths},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded deep video deblurring using temporal sharpness
prior and non-local spatial-temporal similarity. <em>TPAMI</em>,
<em>45</em>(8), 9411–9425. (<a
href="https://doi.org/10.1109/TPAMI.2023.3243059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present compact and effective deep convolutional neural networks (CNNs) by exploring properties of videos for video deblurring. Motivated by the non-uniform blur property that not all the pixels of the frames are blurry, we develop a CNN to integrate a temporal sharpness prior (TSP) for removing blur in videos. The TSP exploits sharp pixels from adjacent frames to facilitate the CNN for better frame restoration. Observing that the motion field is related to latent frames instead of blurry ones in the image formation model, we develop an effective cascaded training approach to solve the proposed CNN in an end-to-end manner. As videos usually contain similar contents within and across frames, we propose a non-local similarity mining approach based on a self-attention method with the propagation of global features to constrain CNNs for frame restoration. We show that exploring the domain knowledge of videos can make CNNs more compact and efficient, where the CNN with the non-local spatial-temporal similarity is $3\times$ smaller than the state-of-the-art methods in terms of model parameters while its performance gains are at least 1 dB higher in terms of PSNRs. Extensive experimental results show that our method performs favorably against state-of-the-art approaches on benchmarks and real-world videos.},
  archive      = {J_TPAMI},
  author       = {Jinshan Pan and Boming Xu and Haoran Bai and Jinhui Tang and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2023.3243059},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9411-9425},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cascaded deep video deblurring using temporal sharpness prior and non-local spatial-temporal similarity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention spiking neural networks. <em>TPAMI</em>,
<em>45</em>(8), 9393–9410. (<a
href="https://doi.org/10.1109/TPAMI.2023.3241201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired spiking neural networks (SNNs) are becoming a promising energy-efficient alternative to traditional artificial neural networks (ANNs). However, the performance gap between SNNs and ANNs has been a significant hindrance to deploying SNNs ubiquitously. To leverage the full potential of SNNs, in this paper we study the attention mechanisms, which can help human focus on important information. We present our idea of attention in SNNs with a multi-dimensional attention module, which infers attention weights along the temporal, channel, as well as spatial dimension separately or simultaneously. Based on the existing neuroscience theories, we exploit the attention weights to optimize membrane potentials, which in turn regulate the spiking response. Extensive experimental results on event-based action recognition and image classification datasets demonstrate that attention facilitates vanilla SNNs to achieve sparser spiking firing, better performance, and energy efficiency concurrently. In particular, we achieve top-1 accuracy of 75.92\% and 77.08\% on ImageNet-1 K with single/4-step Res-SNN-104, which are state-of-the-art results in SNNs. Compared with counterpart Res-ANN-104, the performance gap becomes -0.95/+0.21 percent and the energy efficiency is 31.8×/7.4×. To analyze the effectiveness of attention SNNs, we theoretically prove that the spiking degradation or the gradient vanishing, which usually holds in general SNNs, can be resolved by introducing the block dynamical isometry theory. We also analyze the efficiency of attention SNNs based on our proposed spiking response visualization method. Our work lights up SNN&#39;s potential as a general backbone to support various applications in the field of SNN research, with a great balance between effectiveness and energy efficiency.},
  archive      = {J_TPAMI},
  author       = {Man Yao and Guangshe Zhao and Hengyu Zhang and Yifan Hu and Lei Deng and Yonghong Tian and Bo Xu and Guoqi Li},
  doi          = {10.1109/TPAMI.2023.3241201},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9393-9410},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attention spiking neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGConv: Adaptive graph convolution on 3D point clouds.
<em>TPAMI</em>, <em>45</em>(8), 9374–9392. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution on 3D point clouds is widely researched yet far from perfect in geometric deep learning. The traditional wisdom of convolution characterises feature correspondences indistinguishably among 3D points, arising an intrinsic limitation of poor distinctive feature learning. In this article, we propose Adaptive Graph Convolution (AGConv) for wide applications of point cloud analysis. AGConv generates adaptive kernels for points according to their dynamically learned features. Compared with the solution of using fixed/isotropic kernels, AGConv improves the flexibility of point cloud convolutions, effectively and precisely capturing the diverse relations between points from different semantic parts. Unlike the popular attentional weight schemes, AGConv implements the adaptiveness inside the convolution operation instead of simply assigning different weights to the neighboring points. Extensive evaluations clearly show that our method outperforms state-of-the-arts of point cloud classification and segmentation on various benchmark datasets. Meanwhile, AGConv can flexibly serve more point cloud analysis approaches to boost their performance. To validate its flexibility and effectiveness, we explore AGConv-based paradigms of completion, denoising, upsampling, registration and circle extraction, which are comparable or even superior to their competitors.},
  archive      = {J_TPAMI},
  author       = {Mingqiang Wei and Zeyong Wei and Haoran Zhou and Fei Hu and Huajian Si and Zhilei Chen and Zhe Zhu and Jingbo Qiu and Xuefeng Yan and Yanwen Guo and Jun Wang and Jing Qin},
  doi          = {10.1109/TPAMI.2023.3238516},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9374-9392},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AGConv: Adaptive graph convolution on 3D point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affine subspace robust low-rank self-representation: From
matrix to tensor. <em>TPAMI</em>, <em>45</em>(8), 9357–9373. (<a
href="https://doi.org/10.1109/TPAMI.2023.3257407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank self-representation based subspace learning has confirmed its great effectiveness in a broad range of applications. Nevertheless, existing studies mainly focus on exploring the global linear subspace structure, and cannot commendably handle the case where the samples approximately (i.e., the samples contain data errors) lie in several more general affine subspaces. To overcome this drawback, in this paper, we innovatively propose to introduce affine and nonnegative constraints into low-rank self-representation learning. While simple enough, we provide their underlying theoretical insight from a geometric perspective. The union of two constraints geometrically restricts each sample to be expressed as a convex combination of other samples in the same subspace. In this way, when exploring the global affine subspace structure, we can also consider the specific local distribution of data in each subspace. To comprehensively demonstrate the benefits of introducing two constraints, we instantiate three low-rank self-representation methods ranging from single-view low-rank matrix learning to multi-view low-rank tensor learning. We carefully design the solution algorithms to efficiently optimize the proposed three approaches. Extensive experiments are conducted on three typical tasks, including single-view subspace clustering, multi-view subspace clustering, and multi-view semi-supervised classification. The notably superior experimental results powerfully verify the effectiveness of our proposals.},
  archive      = {J_TPAMI},
  author       = {Yongqiang Tang and Yuan Xie and Wensheng Zhang},
  doi          = {10.1109/TPAMI.2023.3257407},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9357-9373},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Affine subspace robust low-rank self-representation: From matrix to tensor},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ADPL: Adaptive dual path learning for domain adaptation of
semantic segmentation. <em>TPAMI</em>, <em>45</em>(8), 9339–9356. (<a
href="https://doi.org/10.1109/TPAMI.2023.3248294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the need for large-scale pixel-wise annotations, domain adaptation for semantic segmentation trains segmentation models on synthetic data (source) with computer-generated annotations, which can be then generalized to segment realistic images (target). Recently, self-supervised learning (SSL) with a combination of image-to-image translation shows great effectiveness in adaptive segmentation. The most common practice is to perform SSL along with image translation to well align a single domain (source or target). However, in this single-domain paradigm, unavoidable visual inconsistency raised by image translation may affect subsequent learning. In addition, pseudo labels generated by a single segmentation model aligned in either the source or target domain may be not accurate enough for SSL. In this paper, based on the observation that domain adaptation frameworks performed in the source and target domain are almost complementary, we propose a novel adaptive dual path learning (ADPL) framework to alleviate visual inconsistency and promote pseudo-labeling by introducing two interactive single-domain adaptation paths aligned in source and target domain respectively. To fully explore the potential of this dual-path design, novel technologies such as dual path image translation (DPIT), dual path adaptive segmentation (DPAS), dual path pseudo label generation (DPPLG) and Adaptive ClassMix are proposed. The inference of ADPL is extremely simple, only one segmentation model in the target domain is employed. Our ADPL outperforms the state-of-the-art methods by large margins on GTA5 $\rightarrow$ Cityscapes, SYNTHIA $\rightarrow$ Cityscapes and GTA5 $\rightarrow$ BDD100K scenarios. Code and models are available at https://github.com/royee182/DPL .},
  archive      = {J_TPAMI},
  author       = {Yiting Cheng and Fangyun Wei and Jianmin Bao and Dong Chen and Wenqiang Zhang},
  doi          = {10.1109/TPAMI.2023.3248294},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9339-9356},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ADPL: Adaptive dual path learning for domain adaptation of semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive search-and-training for robust and efficient
network pruning. <em>TPAMI</em>, <em>45</em>(8), 9325–9338. (<a
href="https://doi.org/10.1109/TPAMI.2023.3248612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both network pruning and neural architecture search (NAS) can be interpreted as techniques to automate the design and optimization of artificial neural networks. In this paper, we challenge the conventional wisdom of training before pruning by proposing a joint search-and-training approach to learn a compact network directly from scratch. Using pruning as a search strategy, we advocate three new insights for network engineering: 1) to formulate adaptive search as a cold start strategy to find a compact subnetwork on the coarse scale; and 2) to automatically learn the threshold for network pruning; 3) to offer flexibility to choose between efficiency and robustness . More specifically, we propose an adaptive search algorithm in the cold start by exploiting the randomness and flexibility of filter pruning. The weights associated with the network filters will be updated by ThreshNet, a flexible coarse-to-fine pruning method inspired by reinforcement learning. In addition, we introduce a robust pruning strategy leveraging the technique of knowledge distillation through a teacher-student network. Extensive experiments on ResNet and VGGNet have shown that our proposed method can achieve a better balance in terms of efficiency and accuracy and notable advantages over current state-of-the-art pruning methods in several popular datasets, including CIFAR10, CIFAR100, and ImageNet. The code associate with this paper is available at: https://see.xidian.edu.cn/faculty/wsdong/Projects/AST-NP.htm .},
  archive      = {J_TPAMI},
  author       = {Xiaotong Lu and Weisheng Dong and Xin Li and Jinjian Wu and Leida Li and Guangming Shi},
  doi          = {10.1109/TPAMI.2023.3248612},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9325-9338},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive search-and-training for robust and efficient network pruning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive feature selection with augmented attributes.
<em>TPAMI</em>, <em>45</em>(8), 9306–9324. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many dynamic environment applications, with the evolution of data collection ways, the data attributes are incremental and the samples are stored with accumulated feature spaces gradually. For instance, in the neuroimaging-based diagnosis of neuropsychiatric disorders, with emerging of diverse testing ways, we get more brain image features over time. The accumulation of different types of features will unavoidably bring difficulties in manipulating the high-dimensional data. It is challenging to design an algorithm to select valuable features in this feature incremental scenario. To address this important but rarely studied problem, we propose a novel Adaptive Feature Selection method (AFS). It enables the reusability of the feature selection model trained on previous features and adapts it to fit the feature selection requirements on all features automatically. Besides, an ideal $\ell _{0}$ -norm sparse constraint for feature selection is imposed with a proposed effective solving strategy. We present the theoretical analyses about the generalization bound and convergence behavior. After tackling this problem in a one-shot case, we extend it to the multi-shot scenario. Plenty of experimental results demonstrate the effectiveness of reusing previous features and the superior of $\ell _{0}$ -norm constraint in various aspects, together with its effectiveness in discriminating schizophrenic patients from healthy controls.},
  archive      = {J_TPAMI},
  author       = {Chenping Hou and Ruidong Fan and Ling-Li Zeng and Dewen Hu},
  doi          = {10.1109/TPAMI.2023.3238011},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9306-9324},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive feature selection with augmented attributes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on label-efficient deep image segmentation:
Bridging the gap between weak supervision and dense prediction.
<em>TPAMI</em>, <em>45</em>(8), 9284–9305. (<a
href="https://doi.org/10.1109/TPAMI.2023.3246102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning has made a great progress in image segmentation, one of the fundamental tasks of computer vision. However, the current segmentation algorithms mostly rely on the availability of pixel-level annotations, which are often expensive, tedious, and laborious. To alleviate this burden, the past years have witnessed an increasing attention in building label-efficient, deep-learning-based image segmentation algorithms. This paper offers a comprehensive review on label-efficient image segmentation methods. To this end, we first develop a taxonomy to organize these methods according to the supervision provided by different types of weak labels (including no supervision, inexact supervision, incomplete supervision and inaccurate supervision) and supplemented by the types of segmentation problems (including semantic segmentation, instance segmentation and panoptic segmentation). Next, we summarize the existing label-efficient image segmentation methods from a unified perspective that discusses an important question: how to bridge the gap between weak supervision and dense prediction – the current methods are mostly based on heuristic priors, such as cross-pixel similarity, cross-label constraint, cross-view consistency, and cross-image relation. Finally, we share our opinions about the future research directions for label-efficient deep image segmentation.},
  archive      = {J_TPAMI},
  author       = {Wei Shen and Zelin Peng and Xuehui Wang and Huayu Wang and Jiazhong Cen and Dongsheng Jiang and Lingxi Xie and Xiaokang Yang and Qi Tian},
  doi          = {10.1109/TPAMI.2023.3246102},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9284-9305},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized explanation framework for visualization of
deep learning model predictions. <em>TPAMI</em>, <em>45</em>(8),
9265–9283. (<a
href="https://doi.org/10.1109/TPAMI.2023.3241106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribution-based explanations are popular in computer vision but of limited use for fine-grained classification problems typical of expert domains, where classes differ by subtle details. In these domains, users also seek understanding of “why” a class was chosen and “why not” an alternative class. A new GenerAlized expLanatiOn fRamEwork (GALORE) is proposed to satisfy all these requirements, by unifying attributive explanations with explanations of two other types. The first is a new class of explanations, denoted deliberative , proposed to address the “why” question, by exposing the network insecurities about a prediction. The second is the class of counterfactual explanations, which have been shown to address the “why not” question but are now more efficiently computed. GALORE unifies these explanations by defining them as combinations of attribution maps with respect to various classifier predictions and a confidence score. An evaluation protocol that leverages object recognition (CUB200) and scene classification (ADE20 K) datasets combining part and attribute annotations is also proposed. Experiments show that confidence scores can improve explanation accuracy, deliberative explanations provide insight into the network deliberation process, the latter correlates with that performed by humans, and counterfactual explanations enhance the performance of human students in machine teaching experiments.},
  archive      = {J_TPAMI},
  author       = {Pei Wang and Nuno Vasconcelos},
  doi          = {10.1109/TPAMI.2023.3241106},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {9265-9283},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A generalized explanation framework for visualization of deep learning model predictions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Split-GCN: Effective interactive annotation for segmentation
of disconnected instance. <em>TPAMI</em>, <em>45</em>(7), 9256–9263. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating object boundaries by humans demands high costs. Recently, polygon-based annotation methods with human interaction have shown successful performance. However, given the connected vertex topology, these methods exhibit difficulty predicting the disconnected components in an object. This article introduces Split-GCN, a novel architecture based on the polygon approach and self-attention mechanism. By offering the direction information, Split-GCN enables the polygon&#39;s vertices to move more precisely to the object boundary. Our model successfully predicts disconnected components of an object by transforming the initial topology using the context exchange about the dependencies of vertices. Split-GCN demonstrates competitive performance with the state-of-the-art models on Cityscapes and even higher performance with the baseline models. On four cross-domain datasets, we confirm our model&#39;s generalization ability.},
  archive      = {J_TPAMI},
  author       = {Namgil Kim and Barom Kang and Yeonok Cho},
  doi          = {10.1109/TPAMI.2022.3229091},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9256-9263},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Split-GCN: Effective interactive annotation for segmentation of disconnected instance},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing spatial labeling redundancy for active
semi-supervised crowd counting. <em>TPAMI</em>, <em>45</em>(7),
9248–9255. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeling is onerous for crowd counting as it should annotate each individual in crowd images. Recently, several methods have been proposed for semi-supervised crowd counting to reduce the labeling efforts. Given a limited labeling budget, they typically select a few crowd images and densely label all individuals in each of them. Despite the promising results, we argue the None-or-All labeling strategy is suboptimal as the densely labeled individuals in each crowd image usually appear similar while the massive unlabeled crowd images may contain entirely diverse individuals. To this end, we propose to break the labeling chain of previous methods and make the first attempt to reduce spatial labeling redundancy for semi-supervised crowd counting. First, instead of annotating all the regions in each crowd image, we propose to annotate the representative ones only. We analyze the region representativeness from both vertical and horizontal directions of initially estimated density maps, and formulate them as cluster centers of Gaussian Mixture Models. Additionally, to leverage the rich unlabeled regions, we exploit the similarities among individuals in each crowd image to directly supervise the unlabeled regions via feature propagation instead of the error-prone label propagation employed in the previous methods. In this way, we can transfer the original spatial labeling redundancy caused by individual similarities to effective supervision signals on the unlabeled regions. Extensive experiments on the widely-used benchmarks demonstrate that our method can outperform previous best approaches by a large margin.},
  archive      = {J_TPAMI},
  author       = {Yongtuo Liu and Sucheng Ren and Liangyu Chai and Hanjie Wu and Dan Xu and Jing Qin and Shengfeng He},
  doi          = {10.1109/TPAMI.2022.3232712},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9248-9255},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reducing spatial labeling redundancy for active semi-supervised crowd counting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Patch-based separable transformer for visual recognition.
<em>TPAMI</em>, <em>45</em>(7), 9241–9247. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational complexity of transformers limits it to be widely deployed onto frameworks for visual recognition. Recent work Dosovitskiy et al. 2021 significantly accelerates the network processing speed by reducing the resolution at the beginning of the network, however, it is still hard to be directly generalized onto other downstream tasks e.g.object detection and segmentation like CNN. In this paper, we present a transformer-based architecture retaining both the local and global interactions within the network, and can be transferable to other downstream tasks. The proposed architecture reforms the original full spatial self-attention into pixel-wise local attention and patch-wise global attention. Such factorization saves the computational cost while retaining the information of different granularities, which helps generate multi-scale features required by different tasks. By exploiting the factorized attention, we construct a Separable Transformer (SeT) for visual modeling. Experimental results show that SeT outperforms the previous state-of-the-art transformer-based approaches and its CNN counterparts on three major tasks including image classification, object detection and instance segmentation. 1},
  archive      = {J_TPAMI},
  author       = {Shuyang Sun and Xiaoyu Yue and Hengshuang Zhao and Philip H.S. Torr and Song Bai},
  doi          = {10.1109/TPAMI.2022.3231725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9241-9247},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Patch-based separable transformer for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fisher’s linear discriminant analysis with space-folding
operations. <em>TPAMI</em>, <em>45</em>(7), 9233–9240. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher&#39;s linear discriminant analysis (LDA) is an easy-to-use supervised dimensionality reduction method. However, LDA may be ineffective against complicated class distributions. It is well-known that deep feedforward neural networks with rectified linear units as activation functions can map many input neighborhoods to similar outputs by a succession of space-folding operations. This short paper shows that the space-folding operation can reveal to LDA classification information in the subspace where LDA cannot find any. A composition of LDA with the space-folding operation can find classification information more than LDA can do. End-to-end fine-tuning can improve that composition further. Experimental results on artificial and open data sets have shown the feasibility of the proposed approach.},
  archive      = {J_TPAMI},
  author       = {Chin-Chun Chang},
  doi          = {10.1109/TPAMI.2022.3233572},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9233-9240},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fisher&#39;s linear discriminant analysis with space-folding operations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BNET: Batch normalization with enhanced linear
transformation. <em>TPAMI</em>, <em>45</em>(7), 9225–9232. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch normalization (BN) is a fundamental unit in modern deep neural networks. However, BN and its variants focus on normalization statistics but neglect the recovery step that uses linear transformation to improve the capacity of fitting complex data distributions. In this paper, we demonstrate that the recovery step can be improved by aggregating the neighborhood of each neuron rather than just considering a single neuron. Specifically, we propose a simple yet effective method named batch normalization with enhanced linear transformation (BNET) to embed spatial contextual information and improve representation ability. BNET can be easily implemented using the depth-wise convolution and seamlessly transplanted into existing architectures with BN. To our best knowledge, BNET is the first attempt to enhance the recovery step for BN. Furthermore, BN is interpreted as a special case of BNET from both spatial and spectral views. Experimental results demonstrate that BNET achieves consistent performance gains based on various backbones in a wide range of visual tasks. Moreover, BNET can accelerate the convergence of network training and enhance spatial information by assigning important neurons with large weights accordingly.},
  archive      = {J_TPAMI},
  author       = {Yuhui Xu and Lingxi Xie and Cihang Xie and Wenrui Dai and Jieru Mei and Siyuan Qiao and Wei Shen and Hongkai Xiong and Alan Yuille},
  doi          = {10.1109/TPAMI.2023.3235369},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9225-9232},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BNET: Batch normalization with enhanced linear transformation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math inline">𝒳</span>-metric: An n-dimensional
information-theoretic framework for groupwise registration and deep
combined computing. <em>TPAMI</em>, <em>45</em>(7), 9206–9224. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a generic probabilistic framework for estimating the statistical dependency and finding the anatomical correspondences among an arbitrary number of medical images. The method builds on a novel formulation of the $N$ -dimensional joint intensity distribution by representing the common anatomy as latent variables and estimating the appearance model with nonparametric estimators. Through connection to maximum likelihood and the expectation-maximization algorithm, an information-theoretic metric called $\mathcal {X}$ -metric and a co-registration algorithm named $\mathcal {X}$ -CoReg are induced, allowing groupwise registration of the $N$ observed images with computational complexity of $\mathcal {O}(N)$ . Moreover, the method naturally extends for a weakly-supervised scenario where anatomical labels of certain images are provided. This leads to a combined-computing framework implemented with deep learning, which performs registration and segmentation simultaneously and collaboratively in an end-to-end fashion. Extensive experiments were conducted to demonstrate the versatility and applicability of our model, including multimodal groupwise registration, motion correction for dynamic contrast enhanced magnetic resonance images, and deep combined computing for multimodal medical images. Results show the superiority of our method in various applications in terms of both accuracy and efficiency, highlighting the advantage of the proposed representation of the imaging process. Code is available from https://zmiclab.github.io/projects.html .},
  archive      = {J_TPAMI},
  author       = {Xinzhe Luo and Xiahai Zhuang},
  doi          = {10.1109/TPAMI.2022.3225418},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9206-9224},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {$\mathcal {X}$-metric: An N-dimensional information-theoretic framework for groupwise registration and deep combined computing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WebUAV-3M: A benchmark for unveiling the power of
million-scale deep UAV tracking. <em>TPAMI</em>, <em>45</em>(7),
9186–9205. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) tracking is of great significance for a wide range of applications, such as delivery and agriculture. Previous benchmarks in this area mainly focused on small-scale tracking problems while ignoring the amounts of data, types of data modalities, diversities of target categories and scenarios, and evaluation protocols involved, greatly hiding the massive power of deep UAV tracking. In this article, we propose WebUAV-3M, the largest public UAV tracking benchmark to date, to facilitate both the development and evaluation of deep UAV trackers. WebUAV-3M contains over 3.3 million frames across 4,500 videos and offers 223 highly diverse target categories. Each video is densely annotated with bounding boxes by an efficient and scalable semi-automatic target annotation (SATA) pipeline. Importantly, to take advantage of the complementary superiority of language and audio, we enrich WebUAV-3M by innovatively providing both natural language specifications and audio descriptions. We believe that such additions will greatly boost future research in terms of exploring language features and audio cues for multi-modal UAV tracking. In addition, a fine-grained UAV tracking-under-scenario constraint (UTUSC) evaluation protocol and seven challenging scenario subtest sets are constructed to enable the community to develop, adapt and evaluate various types of advanced trackers. We provide extensive evaluations and detailed analyses of 43 representative trackers and envision future research directions in the field of deep UAV tracking and beyond. The dataset, toolkits, and baseline results are available at https://github.com/983632847/WebUAV-3M .},
  archive      = {J_TPAMI},
  author       = {Chunhui Zhang and Guanjie Huang and Li Liu and Shan Huang and Yinan Yang and Xiang Wan and Shiming Ge and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3232854},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9186-9205},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {WebUAV-3M: A benchmark for unveiling the power of million-scale deep UAV tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Universal multimodal representation for language
understanding. <em>TPAMI</em>, <em>45</em>(7), 9169–9185. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is the foundation of natural language processing (NLP). This work presents new methods to employ visual information as assistant signals to general NLP tasks. For each sentence, we first retrieve a flexible number of images either from a light topic-image lookup table extracted over the existing sentence-image pairs or a shared cross-modal embedding space that is pre-trained on out-of-shelf text-image pairs. Then, the text and images are encoded by a Transformer encoder and convolutional neural network, respectively. The two sequences of representations are further fused by an attention layer for the interaction of the two modalities. In this study, the retrieval process is controllable and flexible. The universal visual representation overcomes the lack of large-scale bilingual sentence-image pairs. Our method can be easily applied to text-only tasks without manually annotated multimodal parallel corpora. We apply the proposed method to a wide range of natural language generation and understanding tasks, including neural machine translation, natural language inference, and semantic similarity. Experimental results show that our method is generally effective for different tasks and languages. Analysis indicates that the visual signals enrich textual representations of content words, provide fine-grained grounding information about the relationship between concepts and events, and potentially conduce to disambiguation.},
  archive      = {J_TPAMI},
  author       = {Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},
  doi          = {10.1109/TPAMI.2023.3234170},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9169-9185},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Universal multimodal representation for language understanding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transforming complex problems into k-means solutions.
<em>TPAMI</em>, <em>45</em>(7), 9149–9168. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is a fundamental clustering algorithm widely used in both academic and industrial applications. Its popularity can be attributed to its simplicity and efficiency. Studies show the equivalence of K-means to principal component analysis, non-negative matrix factorization, and spectral clustering. However, these studies focus on standard K-means with squared euclidean distance. In this review paper, we unify the available approaches in generalizing K-means to solve challenging and complex problems. We show that these generalizations can be seen from four aspects: data representation, distance measure, label assignment, and centroid updating. As concrete applications of transforming problems into modified K-means formulation, we review the following applications: iterative subspace projection and clustering, consensus clustering, constrained clustering, domain adaptation, and outlier detection.},
  archive      = {J_TPAMI},
  author       = {Hongfu Liu and Junxiang Chen and Jennifer Dy and Yun Fu},
  doi          = {10.1109/TPAMI.2023.3237667},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9149-9168},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transforming complex problems into K-means solutions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards lightweight pixel-wise hallucination for
heterogeneous face recognition. <em>TPAMI</em>, <em>45</em>(7),
9135–9148. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-spectral face hallucination is an intuitive way to mitigate the modality discrepancy in Heterogeneous Face Recognition (HFR). However, due to imaging differences, the hallucination inevitably suffers from a shape misalignment between paired heterogeneous images. Rather than building complicated architectures to circumvent the problem like previous works, we propose a simple yet effective method called Shape Alignment FacE (SAFE). Specifically, given an image, we align its shape to that of the paired one under the assistance of a 3D face model. The produced aligned pair enables us to train a lightweight generator that solely concentrates on spectrum translation with a pixel-wise supervision. However, since the 3D face model is powerless to attributes like the hair and glasses, there are still pixel discrepancies between the aligned pair. Given that, in the image space, we introduce a probabilistic pixel-wise loss that incorporates the discrepancies into a probabilistic distribution. Moreover, in order to alleviate the influence of the shape misalignment on spectrum translation, a spectrum optimal transport is performed in a shape-irrelevant latent space. Note that, in the final inference phase, except the lightweight generator, all other auxiliary modules are discarded. In addition to superior performance in qualitative synthesis and quantitative recognition, extensive experiments on 6 datasets demonstrate that our method also gains other two distinct advantages over existing state-of-the-art counterparts. The first is using a more lightweight generator. Compared with the state-of-the-art method, our method can achieve higher recognition results with 128x fewer parameters and 63x fewer FLOPs with only 4.58 ms latency on a single TITAN-XP. The second is training on low-shot datasets such as Oulu-CASIA NIR-VIS that just contains 1,920 images from 20 identities. To the best of our knowledge, we are the first that can perform well on such a small-scale dataset. These advantages make our method more practical in the real world and further push boundaries of heterogeneous face recognition.},
  archive      = {J_TPAMI},
  author       = {Chaoyou Fu and Xiaoqiang Zhou and Weizan He and Ran He},
  doi          = {10.1109/TPAMI.2022.3227180},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9135-9148},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards lightweight pixel-wise hallucination for heterogeneous face recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TextStyleBrush: Transfer of text aesthetics from a single
example. <em>TPAMI</em>, <em>45</em>(7), 9122–9134. (<a
href="https://doi.org/10.1109/TPAMI.2023.3239736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for disentangling the content of a text image from all aspects of its appearance. The appearance representation we derive can then be applied to new content, for one-shot transfer of the source style to new content. We learn this disentanglement in a self-supervised manner. Our method processes entire word boxes, without requiring segmentation of text from background, per-character processing, or making assumptions on string lengths. We show results in different text domains which were previously handled by specialized methods, e.g., scene text, handwritten text. To these ends, we make a number of technical contributions: (1) We disentangle the style and content of a textual image into a non-parametric, fixed-dimensional vector. (2) We propose a novel approach inspired by StyleGAN but conditioned over the example style at different resolution and content. (3) We present novel self-supervised training criteria which preserve both source style and target content using a pre-trained font classifier and text recognizer. Finally, (4) we also introduce Imgur5K, a new challenging dataset for handwritten word images. We offer numerous qualitative photo-realistic results of our method. We further show that our method surpasses previous work in quantitative tests on scene text and handwriting datasets, as well as in a user study.},
  archive      = {J_TPAMI},
  author       = {Praveen Krishnan and Rama Kovvuri and Guan Pang and Boris Vassilev and Tal Hassner},
  doi          = {10.1109/TPAMI.2023.3239736},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9122-9134},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TextStyleBrush: Transfer of text aesthetics from a single example},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-aware weakly supervised object localization with
transformer. <em>TPAMI</em>, <em>45</em>(7), 9109–9121. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) aims to predict both object locations and categories with only image-level class labels. However, most existing methods rely on class-specific image regions for localization, resulting in incomplete object localization. To alleviate this problem, we propose a novel end-to-end task-aware framework with a transformer encoder-decoder architecture (TAFormer) to learn class-agnostic foreground maps, including a representation encoder, a localization decoder, and a classification decoder. The proposed TAFormer enjoys several merits. First, the designed three modules can effectively perform class-agnostic localization and classification in a task-aware manner, achieving remarkable performance for both tasks. Second, an optimal transport algorithm is proposed to provide pixel-level pseudo labels to online refine foreground maps. To the best of our knowledge, this is the first work by exploring a task-aware framework with a transformer architecture and an optimal transport algorithm to achieve accurate object localization for WSOL. Extensive experiments with four backbones on two standard benchmarks demonstrate that our TAFormer achieves favorable performance against state-of-the-art methods. Furthermore, we show that the proposed TAFormer provides higher robustness against adversarial attacks and noisy labels.},
  archive      = {J_TPAMI},
  author       = {Meng Meng and Tianzhu Zhang and Zhe Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2022.3230902},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9109-9121},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Task-aware weakly supervised object localization with transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey: Leakage and privacy at inference time.
<em>TPAMI</em>, <em>45</em>(7), 9090–9108. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leakage of data from publicly available Machine Learning (ML) models is an area of growing significance since commercial and government applications of ML can draw on multiple sources of data, potentially including users’ and clients’ sensitive data. We provide a comprehensive survey of contemporary advances on several fronts, covering involuntary data leakage which is natural to ML models, potential malicious leakage which is caused by privacy attacks, and currently available defence mechanisms. We focus on inference-time leakage, as the most likely scenario for publicly available models. We first discuss what leakage is in the context of different data, tasks, and model architectures. We then propose a taxonomy across involuntary and malicious leakage, followed by description of currently available defences, assessment metrics, and applications. We conclude with outstanding challenges and open questions, outlining some promising directions for future research.},
  archive      = {J_TPAMI},
  author       = {Marija Jegorova and Chaitanya Kaul and Charlie Mayor and Alison Q. O&#39;Neil and Alexander Weir and Roderick Murray-Smith and Sotirios A. Tsaftaris},
  doi          = {10.1109/TPAMI.2022.3229593},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9090-9108},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Survey: Leakage and privacy at inference time},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial-temporal transformer for video snapshot compressive
imaging. <em>TPAMI</em>, <em>45</em>(7), 9072–9089. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video snapshot compressive imaging (SCI) captures multiple sequential video frames by a single measurement using the idea of computational imaging. The underlying principle is to modulate high-speed frames through different masks and these modulated frames are summed to a single measurement captured by a low-speed 2D sensor (dubbed optical encoder); following this, algorithms are employed to reconstruct the desired high-speed frames (dubbed software decoder) if needed. In this article, we consider the reconstruction algorithm in video SCI, i.e., recovering a series of video frames from a compressed measurement. Specifically, we propose a Spatial-Temporal transFormer (STFormer) to exploit the correlation in both spatial and temporal domains. STFormer network is composed of a token generation block, a video reconstruction block, and these two blocks are connected by a series of STFormer blocks. Each STFormer block consists of a spatial self-attention branch, a temporal self-attention branch and the outputs of these two branches are integrated by a fusion network. Extensive results on both simulated and real data demonstrate the state-of-the-art performance of STFormer. The code and models are publicly available at https://github.com/ucaswangls/STFormer .},
  archive      = {J_TPAMI},
  author       = {Lishun Wang and Miao Cao and Yong Zhong and Xin Yuan},
  doi          = {10.1109/TPAMI.2022.3225382},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9072-9089},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spatial-temporal transformer for video snapshot compressive imaging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Sparse tensor-based multiscale representation for point
cloud geometry compression. <em>TPAMI</em>, <em>45</em>(7), 9055–9071.
(<a href="https://doi.org/10.1109/TPAMI.2022.3225816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a unified Point Cloud Geometry (PCG) compression method through the processing of multiscale sparse tensor-based voxelized PCG. We call this compression method SparsePCGC. The proposed SparsePCGC is a low complexity solution because it only performs the convolutions on sparsely-distributed Most-Probable Positively-Occupied Voxels (MP-POV). The multiscale representation also allows us to compress scale-wise MP-POVs by exploiting cross-scale and same-scale correlations extensively and flexibly. The overall compression efficiency highly depends on the accuracy of estimated occupancy probability for each MP-POV. Thus, we first design the Sparse Convolution-based Neural Network (SparseCNN) which stacks sparse convolutions and voxel sampling to best characterize and embed spatial correlations. We then develop the SparseCNN-based Occupancy Probability Approximation (SOPA) model to estimate the occupancy probability either in a single-stage manner only using the cross-scale correlation, or in a multi-stage manner by exploiting stage-wise correlation among same-scale neighbors. Besides, we also suggest the SparseCNN based Local Neighborhood Embedding (SLNE) to aggregate local variations as spatial priors in feature attribute to improve the SOPA. Our unified approach not only shows state-of-the-art performance in both lossless and lossy compression modes across a variety of datasets including the dense object PCGs (8iVFB, Owlii, MUVB) and sparse LiDAR PCGs (KITTI, Ford) when compared with standardized MPEG G-PCC and other prevalent learning-based schemes, but also has low complexity which is attractive to practical applications.},
  archive      = {J_TPAMI},
  author       = {Jianqiang Wang and Dandan Ding and Zhu Li and Xiaoxing Feng and Chuntong Cao and Zhan Ma},
  doi          = {10.1109/TPAMI.2022.3225816},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9055-9071},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse tensor-based multiscale representation for point cloud geometry compression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneously optimizing perturbations and positions for
black-box adversarial patch attacks. <em>TPAMI</em>, <em>45</em>(7),
9041–9054. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial patch is an important form of real-world adversarial attack that brings serious risks to the robustness of deep neural networks. Previous methods generate adversarial patches by either optimizing their perturbation values while fixing the pasting position or manipulating the position while fixing the patch&#39;s content. This reveals that the positions and perturbations are both important to the adversarial attack. For that, in this article, we propose a novel method to simultaneously optimize the position and perturbation for an adversarial patch, and thus obtain a high attack success rate in the black-box setting. Technically, we regard the patch&#39;s position, the pre-designed hyper-parameters to determine the patch&#39;s perturbations as the variables, and utilize the reinforcement learning framework to simultaneously solve for the optimal solution based on the rewards obtained from the target model with a small number of queries. Extensive experiments are conducted on the Face Recognition (FR) task, and results on four representative FR models show that our method can significantly improve the attack success rate and query efficiency. Besides, experiments on the commercial FR service and physical environments confirm its practical application value. We also extend our method to the traffic sign recognition task to verify its generalization ability.},
  archive      = {J_TPAMI},
  author       = {Xingxing Wei and Ying Guo and Jie Yu and Bo Zhang},
  doi          = {10.1109/TPAMI.2022.3231886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9041-9054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Simultaneously optimizing perturbations and positions for black-box adversarial patch attacks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SIGMA++: Improved semantic-complete graph matching for
domain adaptive object detection. <em>TPAMI</em>, <em>45</em>(7),
9022–9040. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptive Object Detection (DAOD) generalizes the object detector from an annotated domain to a label-free novel one. Recent works estimate prototypes (class centers) and minimize the corresponding distances to adapt the cross-domain class conditional distribution. However, this prototype-based paradigm 1) fails to capture the class variance with agnostic structural dependencies, and 2) ignores the domain-mismatched classes with a sub-optimal adaptation. To address these two challenges, we propose an improved SemantIc-complete Graph MAtching framework, dubbed SIGMA++, for DAOD, completing mismatched semantics and reformulating adaptation with hypergraph matching. Specifically, we propose a Hypergraphical Semantic Completion (HSC) module to generate hallucination graph nodes in mismatched classes. HSC builds a cross-image hypergraph to model class conditional distribution with high-order dependencies and learns a graph-guided memory bank to generate missing semantics. After representing the source and target batch with hypergraphs, we reformulate domain adaptation with a hypergraph matching problem, i.e., discovering well-matched nodes with homogeneous semantics to reduce the domain gap, which is solved with a Bipartite Hypergraph Matching (BHM) module. Graph nodes are used to estimate semantic-aware affinity, while edges serve as high-order structural constraints in a structure-aware matching loss, achieving fine-grained adaptation with hypergraph matching. The applicability of various object detectors verifies the generalization of SIGMA++, and extensive experiments on nine benchmarks show its state-of-the-art performance on both AP $_{50}$ and adaptation gains.},
  archive      = {J_TPAMI},
  author       = {Wuyang Li and Xinyu Liu and Yixuan Yuan},
  doi          = {10.1109/TPAMI.2023.3235367},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9022-9040},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SIGMA++: Improved semantic-complete graph matching for domain adaptive object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SePiCo: Semantic-guided pixel contrast for domain adaptive
semantic segmentation. <em>TPAMI</em>, <em>45</em>(7), 9004–9021. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the supervised model trained on a labeled source domain. One popular solution is self-training, which retrains the model with pseudo labels on target instances. Plenty of approaches tend to alleviate noisy pseudo labels, however, they ignore the intrinsic connection of the training data, i.e., intra-class compactness and inter-class dispersion between pixel representations across and within domains. In consequence, they struggle to handle cross-domain semantic variations and fail to build a well-structured embedding space, leading to less discrimination and poor generalization. In this work, we propose Se mantic-Guided Pi xel Co ntrast (SePiCo) , a novel one-stage adaptation framework that highlights the semantic concepts of individual pixels to promote learning of class-discriminative and class-balanced pixel representations across domains, eventually boosting the performance of self-training methods. Specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. Considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast , in which we approximate the true distribution of each semantic category from the statistics of labeled source data. Moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs, making it computationally efficient. Extensive experiments show that SePiCo not only helps stabilize training but also yields discriminative representations, making significant progress on both synthetic-to-real and daytime-to-nighttime adaptation scenarios. The code and models are available at https://github.com/BIT-DA/SePiCo .},
  archive      = {J_TPAMI},
  author       = {Binhui Xie and Shuang Li and Mingjia Li and Chi Harold Liu and Gao Huang and Guoren Wang},
  doi          = {10.1109/TPAMI.2023.3237740},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {9004-9021},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SePiCo: Semantic-guided pixel contrast for domain adaptive semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-blindly enhancing extremely noisy videos with recurrent
spatio-temporal large-span network. <em>TPAMI</em>, <em>45</em>(7),
8984–9003. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing videos under the extremely dark environment is quite challenging for the extremely large and complex noise. To accurately represent the complex noise distribution, the physics-based noise modeling and learning-based blind noise modeling methods are proposed. However, these methods suffer from either the requirement of complex calibration procedure or performance degradation in practice. In this paper, we propose a semi-blind noise modeling and enhancing method, which incorporates the physics-based noise model with a learning-based Noise Analysis Module (NAM). With NAM, self-calibration of model parameters can be realized, which enables the denoising process to be adaptive to various noise distributions of either different cameras or camera settings. Besides, we develop a recurrent Spatio-Temporal Large-span Network (STLNet), constructed with a Slow-Fast Dual-branch (SFDB) architecture and an Interframe Non-local Correlation Guidance (INCG) mechanism, to fully investigate the spatio-temporal correlation in a large span. The effectiveness and superiority of the proposed method are demonstrated with extensive experiments, both qualitatively and quantitatively.},
  archive      = {J_TPAMI},
  author       = {Xin Chen and Xuemei Hu and Tao Yue},
  doi          = {10.1109/TPAMI.2023.3234026},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8984-9003},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-blindly enhancing extremely noisy videos with recurrent spatio-temporal large-span network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised 3D representation learning of dressed humans
from social media videos. <em>TPAMI</em>, <em>45</em>(7), 8969–8983. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge of learning a visual representation for the 3D high fidelity geometry of dressed humans lies in the limited availability of the ground truth data (e.g., 3D scanned models), which results in the performance degradation of 3D human reconstruction when applying to real-world imagery. We address this challenge by leveraging a new data resource: a number of social media dance videos that span diverse appearance, clothing styles, performances, and identities. Each video depicts dynamic movements of the body and clothes of a single person while lacking the 3D ground truth geometry. To learn a visual representation from these videos, we present a new self-supervised learning method to use the local transformation that warps the predicted local geometry of the person from an image to that of another image at a different time instant. This allows self-supervision by enforcing a temporal coherence over the predictions. In addition, we jointly learn the depths along with the surface normals that are highly responsive to local texture, wrinkle, and shade by maximizing their geometric consistency. Our method is end-to-end trainable, resulting in high fidelity depth estimation that predicts fine geometry faithful to the input real image. We further provide a theoretical bound of self-supervised learning via an uncertainty analysis that characterizes the performance of the self-supervised learning without training. We demonstrate that our method outperforms the state-of-the-art human depth estimation and human shape recovery approaches on both real and rendered images.},
  archive      = {J_TPAMI},
  author       = {Yasamin Jafarian and Hyun Soo Park},
  doi          = {10.1109/TPAMI.2022.3231558},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8969-8983},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised 3D representation learning of dressed humans from social media videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-adversarial disentangling for specific domain
adaptation. <em>TPAMI</em>, <em>45</em>(7), 8954–8968. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to bridge the domain shifts between the source and the target domain. These shifts may span different dimensions such as fog, rainfall, etc. However, recent methods typically do not consider explicit prior knowledge about the domain shifts on a specific dimension, thus leading to less desired adaptation performance. In this article, we study a practical setting called Specific Domain Adaptation (SDA) that aligns the source and target domains in a demanded-specific dimension. Within this setting, we observe the intra-domain gap induced by different domainness ( i.e., numerical magnitudes of domain shifts in this dimension) is crucial when adapting to a specific domain. To address the problem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In particular, given a specific dimension, we first enrich the source domain by introducing a domainness creator with providing additional supervisory signals. Guided by the created domainness, we design a self-adversarial regularizer and two loss functions to jointly disentangle the latent representations into domainness-specific and domainness-invariant features, thus mitigating the intra-domain gap. Our method can be easily taken as a plug-and-play framework and does not introduce any extra costs in the inference time. We achieve consistent improvements over state-of-the-art methods in both object detection and semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Qianyu Zhou and Qiqi Gu and Jiangmiao Pang and Xuequan Lu and Lizhuang Ma},
  doi          = {10.1109/TPAMI.2023.3238727},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8954-8968},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-adversarial disentangling for specific domain adaptation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searching for network width with bilaterally coupled
network. <em>TPAMI</em>, <em>45</em>(7), 8936–8953. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfil the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance w.r.t. different network widths. However, current methods mainly follow a unilaterally augmented (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this article, we introduce a new supernet called Bilaterally Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we propose to reduce the redundant search space and present the BCNetV2 as the enhanced supernet to ensure rigorous training fairness over channels. Furthermore, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. We also propose a new open-source width search benchmark on macro structures named Channel-Bench-Macro for the better comparisons of the width search algorithms with MobileNet- and ResNet-like architectures. Extensive experiments on the benchmark datasets demonstrate that our method can achieve state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Xiu Su and Shan You and Jiyang Xie and Fei Wang and Chen Qian and Changshui Zhang and Chang Xu},
  doi          = {10.1109/TPAMI.2022.3226777},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8936-8953},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Searching for network width with bilaterally coupled network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScoreMix: A scalable augmentation strategy for training GANs
with limited data. <em>TPAMI</em>, <em>45</em>(7), 8920–8935. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) typically suffer from overfitting when limited training data is available. To facilitate GAN training, current methods propose to use data-specific augmentation techniques. Despite the effectiveness, it is difficult for these methods to scale to practical applications. In this article, we present ScoreMix, a novel and scalable data augmentation approach for various image synthesis tasks. We first produce augmented samples using the convex combinations of the real samples. Then, we optimize the augmented samples by minimizing the norms of the data scores, i.e., the gradients of the log-density functions. This procedure enforces the augmented samples close to the data manifold. To estimate the scores, we train a deep estimation network with multi-scale score matching. For different image synthesis tasks, we train the score estimation network using different data. We do not require the tuning of the hyperparameters or modifications to the network architecture. The ScoreMix method effectively increases the diversity of data and reduces the overfitting problem. Moreover, it can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ScoreMix method achieve significant improvements.},
  archive      = {J_TPAMI},
  author       = {Jie Cao and Mandi Luo and Junchi Yu and Ming-Hsuan Yang and Ran He},
  doi          = {10.1109/TPAMI.2022.3231649},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8920-8935},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ScoreMix: A scalable augmentation strategy for training GANs with limited data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SceneHGN: Hierarchical graph networks for 3D indoor scene
generation with fine-grained geometry. <em>TPAMI</em>, <em>45</em>(7),
8902–8919. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D indoor scenes are widely used in computer graphics, with applications ranging from interior design to gaming to virtual and augmented reality. They also contain rich information, including room layout, as well as furniture type, geometry, and placement. High-quality 3D indoor scenes are highly demanded while it requires expertise and is time-consuming to design high-quality 3D indoor scenes manually. Existing research only addresses partial problems: some works learn to generate room layout, and other works focus on generating detailed structure and geometry of individual furniture objects. However, these partial steps are related and should be addressed together for optimal synthesis. We propose Scene HGN, a hierarchical graph network for 3D indoor scenes that takes into account the full hierarchy from the room level to the object level, then finally to the object part level. Therefore for the first time, our method is able to directly generate plausible 3D room content, including furniture objects with fine-grained geometry, and their layout. To address the challenge, we introduce functional regions as intermediate proxies between the room and object levels to make learning more manageable. To ensure plausibility, our graph-based representation incorporates both vertical edges connecting child nodes with parent nodes from different levels, and horizontal edges encoding relationships between nodes at the same level. Our generation network is a conditional recursive neural network (RvNN) based variational autoencoder (VAE) that learns to generate detailed content with fine-grained geometry for a room, given the room boundary as the condition. Extensive experiments demonstrate that our method produces superior generation results, even when comparing results of partial steps with alternative methods that can only achieve these. We also demonstrate that our method is effective for various applications such as part-level room editing, room interpolation, and room generation by arbitrary room boundaries.},
  archive      = {J_TPAMI},
  author       = {Lin Gao and Jia-Mu Sun and Kaichun Mo and Yu-Kun Lai and Leonidas J. Guibas and Jie Yang},
  doi          = {10.1109/TPAMI.2023.3237577},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8902-8919},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SceneHGN: Hierarchical graph networks for 3D indoor scene generation with fine-grained geometry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Revealing the distributional vulnerability of
discriminators by implicit generators. <em>TPAMI</em>, <em>45</em>(7),
8888–8901. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep neural learning, a discriminator trained on in-distribution (ID) samples may make high-confidence predictions on out-of-distribution (OOD) samples. This triggers a significant matter for robust, trustworthy and safe deep learning. The issue is primarily caused by the limited ID samples observable in training the discriminator when OOD samples are unavailable. We propose a general approach for fine-tuning discriminators by implicit generators (FIG). FIG is grounded on information theory and applicable to standard discriminators without retraining. It improves the ability of a standard discriminator in distinguishing ID and OOD samples by generating and penalizing its specific OOD samples. According to the Shannon entropy, an energy-based implicit generator is inferred from a discriminator without extra training costs. Then, a Langevin dynamic sampler draws specific OOD samples for the implicit generator. Lastly, we design a regularizer fitting the design principle of the implicit generator to induce high entropy on those generated OOD samples. The experiments on different networks and datasets demonstrate that FIG achieves the state-of-the-art OOD detection performance.},
  archive      = {J_TPAMI},
  author       = {Zhilin Zhao and Longbing Cao and Kun-Yu Lin},
  doi          = {10.1109/TPAMI.2022.3229318},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8888-8901},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revealing the distributional vulnerability of discriminators by implicit generators},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reference-based image and video super-resolution via <span
class="math inline"><em>C</em><sup>2</sup></span>-matching.
<em>TPAMI</em>, <em>45</em>(7), 8874–8887. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image or video by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g., scale and rotation) and the resolution gap (e.g., HR and LR). To tackle these challenges, we propose $C^{2}$ -Matching in this work, which performs explicit robust matching crossing transformation and resolution. 1) To bridge the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) To address the resolution gap, we adopt teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue between input images and reference images. In addition, to faithfully evaluate the performance of Reference-based Image Super-Resolution (Ref Image SR) under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. We also extend $C^{2}$ -Matching to Reference-based Video Super-Resolution (Ref VSR) task, where an image taken in a similar scene serves as the HR reference image. Extensive experiments demonstrate that our proposed $C^{2}$ -Matching significantly outperforms state of the arts by up to 0.7 dB on the standard CUFED5 benchmark and also boosts the performance of video super-resolution by incorporating the $C^{2}$ -Matching component into Video SR pipelines. Notably, $C^{2}$ -Matching also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation transformations. Codes and datasets are available at https://github.com/yumingj/C2-Matching .},
  archive      = {J_TPAMI},
  author       = {Yuming Jiang and Kelvin C.K. Chan and Xintao Wang and Chen Change Loy and Ziwei Liu},
  doi          = {10.1109/TPAMI.2022.3231089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8874-8887},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reference-based image and video super-resolution via $C^{2}$-matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Recognizing object by components with human prior knowledge
enhances adversarial robustness of deep neural networks. <em>TPAMI</em>,
<em>45</em>(7), 8861–8873. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks can easily fool object recognition systems based on deep neural networks (DNNs). Although many defense methods have been proposed in recent years, most of them can still be adaptively evaded. One reason for the weak adversarial robustness may be that DNNs are only supervised by category labels and do not have part-based inductive bias like the recognition process of humans. Inspired by a well-known theory in cognitive psychology – recognition-by-components, we propose a novel object recognition model ROCK (Recognizing Object by Components with human prior Knowledge). It first segments parts of objects from images, then scores part segmentation results with predefined human prior knowledge, and finally outputs prediction based on the scores. The first stage of ROCK corresponds to the process of decomposing objects into parts in human vision. The second stage corresponds to the decision process of the human brain. ROCK shows better robustness than classical recognition models across various attack settings. These results encourage researchers to rethink the rationality of currently widely-used DNN-based object recognition models and explore the potential of part-based models, once important but recently ignored, for improving robustness.},
  archive      = {J_TPAMI},
  author       = {Xiao Li and Ziqi Wang and Bo Zhang and Fuchun Sun and Xiaolin Hu},
  doi          = {10.1109/TPAMI.2023.3237935},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8861-8873},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recognizing object by components with human prior knowledge enhances adversarial robustness of deep neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank-one prior: Real-time scene recovery. <em>TPAMI</em>,
<em>45</em>(7), 8845–8860. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recovery is a fundamental imaging task with several practical applications, including video surveillance and autonomous vehicles, etc. In this article, we provide a new real-time scene recovery framework to restore degraded images under different weather/imaging conditions, such as underwater, sand dust and haze. A degraded image can actually be seen as a superimposition of a clear image with the same color imaging environment (underwater, sand or haze, etc.). Mathematically, we can introduce a rank-one matrix to characterize this phenomenon, i.e., rank-one prior (ROP). Using the prior, a direct method with the complexity $O(N)$ is derived for real-time recovery. For general cases, we develop ROP $^+$ to further improve the recovery performance. Comprehensive experiments of the scene recovery illustrate that our method outperforms competitively several state-of-the-art imaging methods in terms of efficiency and robustness.},
  archive      = {J_TPAMI},
  author       = {Jun Liu and Ryan Wen Liu and Jianing Sun and Tieyong Zeng},
  doi          = {10.1109/TPAMI.2022.3226276},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8845-8860},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rank-one prior: Real-time scene recovery},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Querying labeled for unlabeled: Cross-image semantic
consistency guided semi-supervised semantic segmentation.
<em>TPAMI</em>, <em>45</em>(7), 8827–8844. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised semantic segmentation aims to learn a semantic segmentation model via limited labeled images and adequate unlabeled images. The key to this task is generating reliable pseudo labels for unlabeled images. Existing methods mainly focus on producing reliable pseudo labels based on the confidence scores of unlabeled images while largely ignoring the use of labeled images with accurate annotations. In this paper, we propose a Cross-Image Semantic Consistency guided Rectifying (CISC-R) approach for semi-supervised semantic segmentation, which explicitly leverages the labeled images to rectify the generated pseudo labels. Our CISC-R is inspired by the fact that images belonging to the same class have a high pixel-level correspondence. Specifically, given an unlabeled image and its initial pseudo labels, we first query a guiding labeled image that shares the same semantic information with the unlabeled image. Then, we estimate the pixel-level similarity between the unlabeled image and the queried labeled image to form a CISC map, which guides us to achieve a reliable pixel-level rectification for the pseudo labels. Extensive experiments on the PASCAL VOC 2012, Cityscapes, and COCO datasets demonstrate that the proposed CISC-R can significantly improve the quality of the pseudo labels and outperform the state-of-the-art methods. Code is available at https://github.com/Luffy03/CISC-R .},
  archive      = {J_TPAMI},
  author       = {Linshan Wu and Leyuan Fang and Xingxin He and Min He and Jiayi Ma and Zhun Zhong},
  doi          = {10.1109/TPAMI.2022.3233584},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8827-8844},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Querying labeled for unlabeled: Cross-image semantic consistency guided semi-supervised semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Quantformer: Learning extremely low-precision vision
transformers. <em>TPAMI</em>, <em>45</em>(7), 8813–8826. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose extremely low-precision vision transformers called Quantformer for efficient inference. Conventional network quantization methods directly quantize weights and activations of fully-connected layers without considering properties of transformer architectures. Quantization sizably deviates the self-attention compared with full-precision counterparts, and the shared quantization strategy for diversely distributed patch features causes severe quantization errors. To address these issues, we enforce the self-attention rank in quantized transformers to mimic that in full-precision counterparts with capacity-aware distribution for information retention, and quantize patch features with group-wise discretization strategy for quantization error minimization. Specifically, we efficiently preserve the self-attention rank consistency by minimizing the distance between the self-attention in quantized and real-valued transformers with adaptive concentration degree, where the optimal concentration degree is selected according to the self-attention entropy for model capacity adaptation. Moreover, we partition patch features in different dimensions with differentiable group assignment, so that features in different groups leverage various discretization strategies with minimal rounding and clipping errors. Experimental results show that our Quantformer outperforms the state-of-the-art network quantization methods by a sizable margin across various vision transformer architectures in image classification and object detection. We also integrate our Quantformer with mixed-precision quantization to further enhance the performance of the vanilla models.},
  archive      = {J_TPAMI},
  author       = {Ziwei Wang and Changyuan Wang and Xiuwei Xu and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2022.3229313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8813-8826},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Quantformer: Learning extremely low-precision vision transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Polarimetric multi-view inverse rendering. <em>TPAMI</em>,
<em>45</em>(7), 8798–8812. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A polarization camera has great potential for 3D reconstruction since the angle of polarization (AoP) and the degree of polarization (DoP) of reflected light are related to an object&#39;s surface normal. In this paper, we propose a novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that effectively exploits geometric, photometric, and polarimetric cues extracted from input multi-view color-polarization images. We first estimate camera poses and an initial 3D model by geometric reconstruction with a standard structure-from-motion and multi-view stereo pipeline. We then refine the initial model by optimizing photometric rendering errors and polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose a novel polarimetric cost function that enables an effective constraint on the estimated surface normal of each vertex, while considering four possible ambiguous azimuth angles revealed from the AoP measurement. The weight for the polarimetric cost is effectively determined based on the DoP measurement, which is regarded as the reliability of polarimetric information. Experimental results using both synthetic and real data demonstrate that our Polarimetric MVIR can reconstruct a detailed 3D shape without assuming a specific surface material and lighting condition.},
  archive      = {J_TPAMI},
  author       = {Jinyu Zhao and Yusuke Monno and Masatoshi Okutomi},
  doi          = {10.1109/TPAMI.2022.3232211},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8798-8812},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Polarimetric multi-view inverse rendering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial domain adaptation without domain alignment.
<em>TPAMI</em>, <em>45</em>(7), 8787–8797. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to transfer knowledge from a well-labeled source domain to a related and unlabeled target domain with identical label space. The main workhorse in UDA is domain alignment and has proven successful. However, it is practically difficult to find an appropriate source domain with identical label space. A more practical scenario is partial domain adaptation (PDA) where the source label space subsumes the target one. Unfortunately, due to the non-identity between label spaces, it is extremely hard to obtain an ideal alignment, conversely, easier resulting in mode collapse and negative transfer. These motivate us to find a relatively simpler alternative to solve PDA. To achieve this, we first explore a theoretical analysis, which says that the target risk is bounded by both model smoothness and between-domain discrepancy. Then, we instantiate the model smoothness as an intra-domain structure preserving (IDSP) while giving up possibly riskier domain alignment. To our best knowledge, this is the first naive attempt for PDA without alignment. Finally, our empirical results on benchmarks demonstrate that IDSP is not only superior to the PDA SOTAs (e.g., $\sim$ +10\% on Cl $\rightarrow$ Rw and $\sim$ +8\% on Ar $\rightarrow$ Rw), but also complementary to domain alignment in the standard UDA.},
  archive      = {J_TPAMI},
  author       = {Weikai Li and Songcan Chen},
  doi          = {10.1109/TPAMI.2022.3228937},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8787-8797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partial domain adaptation without domain alignment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Orthogonal SVD covariance conditioning and latent
disentanglement. <em>TPAMI</em>, <em>45</em>(7), 8773–8786. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inserting an SVD meta-layer into neural networks is prone to make the covariance ill-conditioned, which could harm the model in the training stability and generalization abilities. In this article, we systematically study how to improve the covariance conditioning by enforcing orthogonality to the Pre-SVD layer. Existing orthogonal treatments on the weights are first investigated. However, these techniques can improve the conditioning but would hurt the performance. To avoid such a side effect, we propose the Nearest Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of our methods is validated in two applications: decorrelated Batch Normalization (BN) and Global Covariance Pooling (GCP). Extensive experiments on visual recognition demonstrate that our methods can simultaneously improve covariance conditioning and generalization. The combinations with orthogonal weight can further boost the performance. Moreover, we show that our orthogonality techniques can benefit generative models for better latent disentanglement through a series of experiments on various benchmarks. Code is available at: https://github.com/KingJamesSong/OrthoImproveCond .},
  archive      = {J_TPAMI},
  author       = {Yue Song and Nicu Sebe and Wei Wang},
  doi          = {10.1109/TPAMI.2022.3228979},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8773-8786},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Orthogonal SVD covariance conditioning and latent disentanglement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orientational distribution learning with hierarchical
spatial attention for open set recognition. <em>TPAMI</em>,
<em>45</em>(7), 8757–8772. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition (OSR) aims to correctly recognize the known classes and reject the unknown classes for increasing the reliability of the recognition system. The distance-based loss is often employed in deep neural networks-based OSR methods to constrain the latent representation of known classes. However, the optimization is usually conducted using the nondirectional euclidean distance in a single feature space without considering the potential impact of spatial distribution. To address this problem, we propose orientational distribution learning (ODL) with hierarchical spatial attention for OSR. In ODL, the spatial distribution of feature representation is optimized orientationally to increase the discriminability of decision boundaries for open set recognition. Then, a hierarchical spatial attention mechanism is proposed to assist ODL to capture the global distribution dependencies in the feature space based on spatial relationships. Moreover, a composite feature space is constructed to integrate the features from different layers and different mapping approaches, and it can well enrich the representation information. Finally, a decision-level fusion method is developed to combine the composite feature space and the naive feature space for producing a more comprehensive classification result. The effectiveness of ODL has been demonstrated on various benchmark datasets, and ODL achieves state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Zhun-ga Liu and Yi-min Fu and Quan Pan and Zuo-wei Zhang},
  doi          = {10.1109/TPAMI.2022.3227913},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8757-8772},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Orientational distribution learning with hierarchical spatial attention for open set recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open world entity segmentation. <em>TPAMI</em>,
<em>45</em>(7), 8743–8756. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new image segmentation task, called Entity Segmentation (ES), which aims to segment all visual entities (objects and stuffs) in an image without predicting their semantic labels. By removing the need of class label prediction, the models trained for such task can focus more on improving segmentation quality. It has many practical applications such as image manipulation and editing where the quality of segmentation masks is crucial but class labels are less important. We conduct the first-ever study to investigate the feasibility of convolutional center-based representation to segment things and stuffs in a unified manner, and show that such representation fits exceptionally well in the context of ES. More specifically, we propose a CondInst-like fully-convolutional architecture with two novel modules specifically designed to exploit the class-agnostic and non-overlapping requirements of ES. Experiments show that the models designed and trained for ES significantly outperforms popular class-specific panoptic segmentation models in terms of segmentation quality. Moreover, an ES model can be easily trained on a combination of multiple datasets without the need to resolve label conflicts in dataset merging, and the model trained for ES on one or more datasets can generalize very well to other test datasets of unseen domains. The code has been released at https://github.com/dvlab-research/Entity .},
  archive      = {J_TPAMI},
  author       = {Lu Qi and Jason Kuen and Yi Wang and Jiuxiang Gu and Hengshuang Zhao and Philip Torr and Zhe Lin and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3227513},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8743-8756},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Open world entity segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-graph data clustering via <span
class="math inline">𝒪(<em>n</em>)</span>o(n) bipartite graph
convolution. <em>TPAMI</em>, <em>45</em>(7), 8729–8742. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the representative capacity of graph-based clustering methods is usually limited by the graph constructed on the original features, it is attractive to find whether graph neural networks ( GNNs ), a strong extension of neural networks to graphs, can be applied to augment the capacity of graph-based clustering methods. The core problems mainly come from two aspects. On the one hand, the graph is unavailable in the most general clustering scenes so that how to construct graph on the non-graph data and the quality of graph is usually the most important part. On the other hand, given $n$ samples, the graph-based clustering methods usually consume at least $\mathcal {O}(n^{2})$ time to build graphs and the graph convolution requires nearly $\mathcal {O}(n^{2})$ for a dense graph and $\mathcal {O}(|\mathcal {E}|)$ for a sparse one with $|\mathcal {E}|$ edges. Accordingly, both graph-based clustering and GNNs suffer from the severe inefficiency problem. To tackle these problems, we propose a novel clustering method, AnchorGAE , with the self-supervised estimation of graph and efficient graph convolution. We first show how to convert a non-graph dataset into a graph dataset, by introducing the generative graph model and anchors. A bipartite graph is built via generating anchors and estimating the connectivity distributions of original points and anchors. We then show that the constructed bipartite graph can reduce the computational complexity of graph convolution from $\mathcal {O}(n^{2})$ and $\mathcal {O}(|\mathcal {E}|)$ to $\mathcal {O}(n)$ . The succeeding steps for clustering can be easily designed as $\mathcal {O}(n)$ operations. Interestingly, the anchors naturally lead to siamese architecture with the help of the Markov process. Furthermore, the estimated bipartite graph is updated dynamically according to the features extracted by GNN modules, to promote the quality of the graph by exploiting the high-level information by GNNs. However, we theoretically prove that the self-supervised paradigm frequently results in a collapse that often occurs after 2-3 update iterations in experiments, especially when the model is well-trained. A specific strategy is accordingly designed to prevent the collapse. The experiments support the theoretical analysis and show the superiority of AnchorGAE.},
  archive      = {J_TPAMI},
  author       = {Hongyuan Zhang and Jiankun Shi and Rui Zhang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3231470},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8729-8742},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Non-graph data clustering via $\mathcal {O}(n)$O(n) bipartite graph convolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural radiance fields from sparse RGB-d images for
high-quality view synthesis. <em>TPAMI</em>, <em>45</em>(7), 8713–8728.
(<a href="https://doi.org/10.1109/TPAMI.2022.3232502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed neural radiance fields (NeRF) use a continuous function formulated as a multi-layer perceptron (MLP) to model the appearance and geometry of a 3D scene. This enables realistic synthesis of novel views, even for scenes with view dependent appearance. Many follow-up works have since extended NeRFs in different ways. However, a fundamental restriction of the method remains that it requires a large number of images captured from densely placed viewpoints for high-quality synthesis and the quality of the results quickly degrades when the number of captured views is insufficient. To address this problem, we propose a novel NeRF-based framework capable of high-quality view synthesis using only a sparse set of RGB-D images, which can be easily captured using cameras and LiDAR sensors on current consumer devices. First, a geometric proxy of the scene is reconstructed from the captured RGB-D images. Renderings of the reconstructed scene along with precise camera parameters can then be used to pre-train a network. Finally, the network is fine-tuned with a small number of real captured images. We further introduce a patch discriminator to supervise the network under novel views during fine-tuning, as well as a 3D color prior to improve synthesis quality. We demonstrate that our method can generate arbitrary novel views of a 3D scene from as few as 6 RGB-D images. Extensive experiments show the improvements of our method compared with the existing NeRF-based methods, including approaches that also aim to reduce the number of input images.},
  archive      = {J_TPAMI},
  author       = {Yu-Jie Yuan and Yu-Kun Lai and Yi-Hua Huang and Leif Kobbelt and Lin Gao},
  doi          = {10.1109/TPAMI.2022.3232502},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8713-8728},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural radiance fields from sparse RGB-D images for high-quality view synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label classification via adaptive resonance
theory-based clustering. <em>TPAMI</em>, <em>45</em>(7), 8696–8712. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a multi-label classification algorithm capable of continual learning by applying an Adaptive Resonance Theory (ART)-based clustering algorithm and the Bayesian approach for label probability computation. The ART-based clustering algorithm adaptively and continually generates prototype nodes corresponding to given data, and the generated nodes are used as classifiers. The label probability computation independently counts the number of label appearances for each class and calculates the Bayesian probabilities. Thus, the label probability computation can cope with an increase in the number of labels. Experimental results with synthetic and real-world multi-label datasets show that the proposed algorithm has competitive classification performance to other well-known algorithms while realizing continual learning.},
  archive      = {J_TPAMI},
  author       = {Naoki Masuyama and Yusuke Nojima and Chu Kiong Loo and Hisao Ishibuchi},
  doi          = {10.1109/TPAMI.2022.3230414},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8696-8712},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-label classification via adaptive resonance theory-based clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular 3D fingerprint reconstruction and unwarping.
<em>TPAMI</em>, <em>45</em>(7), 8679–8695. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with contact-based fingerprint acquisition techniques, contactless acquisition has the advantages of less skin distortion, more complete fingerprint area, and hygienic acquisition. However, perspective distortion is a challenge in contactless fingerprint recognition, which changes the ridge frequency and relative minutiae location, and thus degrades the recognition accuracy. We propose a learning-based shape-from-texture algorithm to reconstruct a 3-D finger shape from a single image and unwarp the raw image to suppress the perspective distortion. Our experimental results for 3-D reconstruction on contactless fingerprint databases show that the proposed method has high 3-D reconstruction accuracy. Experimental results for contactless-to-contactless and contactless-to-contact-based fingerprint matching indicate that the proposed method can improve the matching accuracy.},
  archive      = {J_TPAMI},
  author       = {Zhe Cui and Jianjiang Feng and Jie Zhou},
  doi          = {10.1109/TPAMI.2022.3233898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8679-8695},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Monocular 3D fingerprint reconstruction and unwarping},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to see through with events. <em>TPAMI</em>,
<em>45</em>(7), 8660–8678. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although synthetic aperture imaging (SAI) can achieve the seeing-through effect by blurring out off-focus foreground occlusions while recovering in-focus occluded scenes from multi-view images, its performance is often deteriorated by dense occlusions and extreme lighting conditions. To address the problem, this paper presents an Event-based SAI (E-SAI) method by relying on the asynchronous events with extremely low latency and high dynamic range acquired by an event camera. Specifically, the collected events are first refocused by a Refocus-Net module to align in-focus events while scattering out off-focus ones. Following that, a hybrid network composed of spiking neural networks (SNNs) and convolutional neural networks (CNNs) is proposed to encode the spatio-temporal information from the refocused events and reconstruct a visual image of the occluded targets. Extensive experiments demonstrate that our proposed E-SAI method can achieve remarkable performance in dealing with very dense occlusions and extreme lighting conditions and produce high-quality images from pure events. Codes and datasets are available at https://dvs-whu.cn/projects/esai/ .},
  archive      = {J_TPAMI},
  author       = {Lei Yu and Xiang Zhang and Wei Liao and Wen Yang and Gui-Song Xia},
  doi          = {10.1109/TPAMI.2022.3227448},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8660-8678},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to see through with events},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Language-aware spatial-temporal collaboration for referring
video segmentation. <em>TPAMI</em>, <em>45</em>(7), 8646–8659. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a natural language referring expression, the goal of referring video segmentation task is to predict the segmentation mask of the referred object in the video. Previous methods only adopt 3D CNNs upon the video clip as a single encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolutions are able to recognize which object is performing the described actions, they still introduce misaligned spatial information from adjacent frames, which inevitably confuses features of the target frame and leads to inaccurate segmentation. To tackle this issue, we propose a language-aware spatial-temporal collaboration framework that contains a 3D temporal encoder upon the video clip to recognize the described actions, and a 2D spatial encoder upon the target frame to provide undisturbed spatial features of the referred object. For multimodal features extraction, we propose a Cross-Modal Adaptive Modulation (CMAM) module and its improved version CMAM+ to conduct adaptive cross-modal interaction in the encoders with spatial- or temporal-relevant language features which are also updated progressively to enrich linguistic global context. In addition, we also propose a Language-Aware Semantic Propagation (LASP) module in the decoder to propagate semantic information from deep stages to the shallow stages with language-aware sampling and assignment, which is able to highlight language-compatible foreground visual features and suppress language-incompatible background visual features for better facilitating the spatial-temporal collaboration. Extensive experiments on four popular referring video segmentation benchmarks demonstrate the superiority of our method over the previous state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Tianrui Hui and Si Liu and Zihan Ding and Shaofei Huang and Guanbin Li and Wenguan Wang and Luoqi Liu and Jizhong Han},
  doi          = {10.1109/TPAMI.2023.3235720},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8646-8659},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Language-aware spatial-temporal collaboration for referring video segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Knowledge-enriched attention network with group-wise
semantic for visual storytelling. <em>TPAMI</em>, <em>45</em>(7),
8634–8645. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a technically challenging topic, visual storytelling aims at generating an imaginary and coherent story with narrative multi-sentences from a group of relevant images. Existing methods often generate direct and rigid descriptions of apparent image-based contents, because they are not capable of exploring implicit information beyond images. Hence, these schemes could not capture consistent dependencies from holistic representation, impairing the generation of reasonable and fluent stories. To address these problems, a novel knowledge-enriched attention network with group-wise semantic model is proposed. Three main novel components are designed and supported by substantial experiments to reveal practical advantages. First, a knowledge-enriched attention network is designed to extract implicit concepts from external knowledge system, and these concepts are followed by a cascade cross-modal attention mechanism to characterize imaginative and concrete representations. Second, a group-wise semantic module with second-order pooling is developed to explore the globally consistent guidance. Third, a unified one-stage story generation model with encoder-decoder structure is proposed to simultaneously train and infer the knowledge-enriched attention network, group-wise semantic module and multi-modal story generation decoder in an end-to-end fashion. Substantial experiments on the visual storytelling datasets with both objective and subjective evaluation metrics demonstrate the superior performance of the proposed scheme as compared with other state-of-the-art methods. The source code of this work can be found in https://mic.tongji.edu.cn .},
  archive      = {J_TPAMI},
  author       = {Tengpeng Li and Hanli Wang and Bin He and Chang Wen Chen},
  doi          = {10.1109/TPAMI.2022.3230934},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8634-8645},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Knowledge-enriched attention network with group-wise semantic for visual storytelling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-aware global reasoning for situation recognition.
<em>TPAMI</em>, <em>45</em>(7), 8621–8633. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of situation recognition aims to solve the visual reasoning problem with the ability to predict the activity happening (salient action) in an image and the nouns of all associated semantic roles playing in the activity. This poses severe challenges due to long-tailed data distributions and local class ambiguities. Prior works only propagate the local noun-level features on one single image without utilizing global information. We propose a K nowledge-aware G lobal R easoning (KGR) framework to endow neural networks with the capability of adaptive global reasoning over nouns by exploiting diverse statistical knowledge. Our KGR is a local-global architecture, which consists of a local encoder to generate noun features using local relations and a global encoder to enhance the noun features via global reasoning supervised by an external global knowledge pool. The global knowledge pool is created by counting the pairwise relationships of nouns in the dataset. In this paper, we design an action-guided pairwise knowledge as the global knowledge pool based on the characteristic of the situation recognition task. Extensive experiments have shown that our KGR not only achieves state-of-the-art results on a large-scale situation recognition benchmark, but also effectively solves the long-tailed problem of noun classification by our global knowledge.},
  archive      = {J_TPAMI},
  author       = {Weijiang Yu and Haofan Wang and Guohao Li and Nong Xiao and Bernard Ghanem},
  doi          = {10.1109/TPAMI.2023.3238699},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8621-8633},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Knowledge-aware global reasoning for situation recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Invariant policy learning: A causal perspective.
<em>TPAMI</em>, <em>45</em>(7), 8606–8620. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over different environments. In many real-world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we take a step toward tackling the problem of environmental shifts considering the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved variables are present and show that, in that case, an optimal invariant policy is guaranteed to generalize across environments under suitable assumptions.},
  archive      = {J_TPAMI},
  author       = {Sorawit Saengkyongam and Nikolaj Thams and Jonas Peters and Niklas Pfister},
  doi          = {10.1109/TPAMI.2022.3232363},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8606-8620},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Invariant policy learning: A causal perspective},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive object segmentation with inside-outside
guidance. <em>TPAMI</em>, <em>45</em>(7), 8594–8605. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores how to harvest precise object segmentation masks while minimizing the human interaction cost. To achieve this, we propose a simple yet effective interaction scheme, named Inside-Outside Guidance (IOG). Concretely, we leverage an inside point that is clicked near the object center and two outside points at the symmetrical corner locations (top-left and bottom-right or top-right and bottom-left) of an almost-tight bounding box that encloses the target object. The interaction results in a total of one foreground click and four background clicks for segmentation. The advantages of our IOG are four-fold: 1) the two outside points can help remove distractions from other objects or background; 2) the inside point can help eliminate the unrelated regions inside the bounding box; 3) the inside and outside points are easily identified, reducing the confusion raised by the state-of-the-art DEXTR Maninis et al. 2018, in labeling some extreme samples; 4) it naturally supports additional click annotations for further correction. Despite its simplicity, our IOG not only achieves state-of-the-art performance on several popular benchmarks such as GrabCut Rother et al. 2004, PASCAL Everingham et al. 2010 and MS COCO Russakovsky et al. 2015, but also demonstrates strong generalization capability across different domains such as street scenes (Cityscapes Cordts et al. 2016), aerial imagery (Rooftop Sun et al. 2014 and Agriculture-Vision Chiu et al. 2020) and medical images (ssTEM Gerhard et al. 2013). Code is available at https://github.com/shiyinzhang/Inside-Outside-Guidance https://github.com/shiyinzhang/Inside-Outside-Guidance .},
  archive      = {J_TPAMI},
  author       = {Shiyin Zhang and Shikui Wei and Jun Hao Liew and Kunyang Han and Yao Zhao and Yunchao Wei},
  doi          = {10.1109/TPAMI.2022.3227116},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8594-8605},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interactive object segmentation with inside-outside guidance},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IC9600: A benchmark dataset for automatic image complexity
assessment. <em>TPAMI</em>, <em>45</em>(7), 8577–8593. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image complexity (IC) is an essential visual perception for human beings to understand an image. However, explicitly evaluating the IC is challenging, and has long been overlooked since, on the one hand, the evaluation of IC is relatively subjective due to its dependence on human perception, and on the other hand, the IC is semantic-dependent while real-world images are diverse. To facilitate the research of IC assessment in this deep learning era, we built the first, to our best knowledge, large-scale IC dataset with 9,600 well-annotated images. The images are of diverse areas such as abstract, paintings and real-world scenes, each of which is elaborately annotated by 17 human contributors. Powered by this high-quality dataset, we further provide a base model to predict the IC scores and estimate the complexity density maps in a weakly supervised way. The model is verified to be effective, and correlates well with human perception (with the Pearson correlation coefficient being 0.949). Last but not the least, we have empirically validated that the exploration of IC can provide auxiliary information and boost the performance of a wide range of computer vision tasks. The dataset and source code can be found at https://github.com/tinglyfeng/IC9600 .},
  archive      = {J_TPAMI},
  author       = {Tinglei Feng and Yingjie Zhai and Jufeng Yang and Jie Liang and Deng-Ping Fan and Jing Zhang and Ling Shao and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3232328},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8577-8593},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IC9600: A benchmark dataset for automatic image complexity assessment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hyperparameter-free localized simple multiple kernel
k-means with global optimum. <em>TPAMI</em>, <em>45</em>(7), 8566–8576.
(<a href="https://doi.org/10.1109/TPAMI.2022.3233635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The newly proposed localized simple multiple kernel k-means (SimpleMKKM) provides an elegant clustering framework which sufficiently considers the potential variation among samples. Although achieving superior clustering performance in some applications, we observe that it is required to pre-specify an extra hyperparameter, which determines the size of the localization. This greatly limits its availability in practical applications since there is a little guideline to set a suitable hyperparameter in clustering tasks. To overcome this issue, we firstly parameterize a neighborhood mask matrix as a quadratic combination of a set of pre-computed base neighborhood mask matrices, which corresponds to a group of hyperparameters. We then propose to jointly learn the optimal coefficient of these neighborhood mask matrices together with the clustering tasks. By this way, we obtain the proposed hyperparameter-free localized SimpleMKKM , which corresponds to a more intractable minimization-minimization-maximization optimization problem. We rewrite the resultant optimization as a minimization of an optimal value function, prove its differentiability, and develop a gradient based algorithm to solve it. Furthermore, we theoretically prove that the obtained optimum is the global one. Comprehensive experimental study on several benchmark datasets verifies its effectiveness, comparing with several state-of-the-art counterparts in the recent literature. The source code for hyperparameter-free localized SimpleMKKM is available at https://github.com/xinwangliu/SimpleMKKMcodes/ .},
  archive      = {J_TPAMI},
  author       = {Xinwang Liu},
  doi          = {10.1109/TPAMI.2022.3233635},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8566-8576},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hyperparameter-free localized simple multiple kernel K-means with global optimum},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid high dynamic range imaging fusing neuromorphic and
conventional images. <em>TPAMI</em>, <em>45</em>(7), 8553–8565. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstruction of high dynamic range image from a single low dynamic range image captured by a conventional RGB camera, which suffers from over- or under-exposure, is an ill-posed problem. In contrast, recent neuromorphic cameras like event camera and spike camera can record high dynamic range scenes in the form of intensity maps, but with much lower spatial resolution and no color information. In this article, we propose a hybrid imaging system (denoted as NeurImg) that captures and fuses the visual information from a neuromorphic camera and ordinary images from an RGB camera to reconstruct high-quality high dynamic range images and videos. The proposed NeurImg-HDR+ network consists of specially designed modules, which bridges the domain gaps on resolution, dynamic range, and color representation between two types of sensors and images to reconstruct high-resolution, high dynamic range images and videos. We capture a test dataset of hybrid signals on various HDR scenes using the hybrid camera, and analyze the advantages of the proposed fusing strategy by comparing it to state-of-the-art inverse tone mapping methods and merging two low dynamic range images approaches. Quantitative and qualitative experiments on both synthetic data and real-world scenarios demonstrate the effectiveness of the proposed hybrid high dynamic range imaging system. Code and dataset can be found at: https://github.com/hjynwa/NeurImg-HDR},
  archive      = {J_TPAMI},
  author       = {Jin Han and Yixin Yang and Peiqi Duan and Chu Zhou and Lei Ma and Chao Xu and Tiejun Huang and Imari Sato and Boxin Shi},
  doi          = {10.1109/TPAMI.2022.3231334},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8553-8565},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hybrid high dynamic range imaging fusing neuromorphic and conventional images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How trustworthy are performance evaluations for basic vision
tasks? <em>TPAMI</em>, <em>45</em>(7), 8538–8552. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines performance evaluation criteria for basic vision tasks involving sets of objects namely, object detection, instance-level segmentation and multi-object tracking. The rankings of algorithms by a criterion can fluctuate with different choices of parameters, e.g. Intersection over Union (IoU) threshold, making their evaluations unreliable. More importantly, there is no means to verify whether we can trust the evaluations of a criterion. This work suggests a notion of trustworthiness for performance criteria, which requires (i) robustness to parameters for reliability, (ii) contextual meaningfulness in sanity tests, and (iii) consistency with mathematical requirements such as the metric properties. We observe that these requirements were overlooked by many widely-used criteria, and explore alternative criteria using metrics for sets of shapes. We also assess all these criteria based on the suggested requirements for trustworthiness.},
  archive      = {J_TPAMI},
  author       = {Tran Thien Dat Nguyen and Hamid Rezatofighi and Ba-Ngu Vo and Ba-Tuong Vo and Silvio Savarese and Ian Reid},
  doi          = {10.1109/TPAMI.2022.3227571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8538-8552},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {How trustworthy are performance evaluations for basic vision tasks?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HOP+: History-enhanced and order-aware pre-training for
vision-and-language navigation. <em>TPAMI</em>, <em>45</em>(7),
8524–8537. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works attempt to employ pre-training in Vision-and-Language Navigation (VLN). However, these methods neglect the importance of historical contexts or ignore predicting future actions during pre-training, limiting the learning of visual-textual correspondence and the capability of decision-making. To address these problems, we present a history-enhanced and order-aware pre-training with the complementing fine-tuning paradigm (HOP+) for VLN. Specifically, besides the common Masked Language Modeling (MLM) and Trajectory-Instruction Matching (TIM) tasks, we design three novel VLN-specific proxy tasks: Action Prediction with History (APH) task, Trajectory Order Modeling (TOM) task and Group Order Modeling (GOM) task. APH task takes into account the visual perception trajectory to enhance the learning of historical knowledge as well as action prediction. The two temporal visual-textual alignment tasks, TOM and GOM further improve the agent&#39;s ability to order reasoning. Moreover, we design a memory network to address the representation inconsistency of history context between the pre-training and the fine-tuning stages. The memory network effectively selects and summarizes historical information for action prediction during fine-tuning, without costing huge extra computation consumption for downstream VLN tasks. HOP+ achieves new state-of-the-art performance on four downstream VLN tasks (R2R, REVERIE, RxR, and NDH), which demonstrates the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Yanyuan Qiao and Yuankai Qi and Yicong Hong and Zheng Yu and Peng Wang and Qi Wu},
  doi          = {10.1109/TPAMI.2023.3234243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8524-8537},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HOP+: History-enhanced and order-aware pre-training for vision-and-language navigation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-performance transformer tracking. <em>TPAMI</em>,
<em>45</em>(7), 8507–8523. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation has a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion method that considers the similarity between the template and the search region. However, the correlation operation is a local linear matching process, losing semantic information and easily falling into a local optimum, which may be the bottleneck in designing high-accuracy tracking algorithms. In this work, to determine whether a better feature fusion method exists than correlation, a novel attention-based feature fusion network, inspired by the transformer, is presented. This network effectively combines the template and search region features using attention mechanism. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. First, we present a transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression heads. Based on the TransT baseline, we also design a segmentation branch to generate the accurate mask. Finally, we propose a stronger version of TransT by extending it with a multi-template scheme and an IoU prediction head, named TransT-M. Experiments show that our TransT and TransT-M methods achieve promising results on seven popular benchmarks. Code and models are available at https://github.com/chenxin-dlut/TransT-M .},
  archive      = {J_TPAMI},
  author       = {Xin Chen and Bin Yan and Jiawen Zhu and Huchuan Lu and Xiang Ruan and Dong Wang},
  doi          = {10.1109/TPAMI.2022.3232535},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8507-8523},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High-performance transformer tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAKE: A knowledge engine foundation for human activity
understanding. <em>TPAMI</em>, <em>45</em>(7), 8494–8506. (<a
href="https://doi.org/10.1109/TPAMI.2022.3232797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity understanding is of widespread interest in artificial intelligence and spans diverse applications like health care and behavior analysis. Although there have been advances with deep learning, it remains challenging. The object recognition-like solutions usually try to map pixels to semantics directly, but activity patterns are much different from object patterns, thus hindering another success. In this article, we propose a novel paradigm to reformulate this task in two-stage: first mapping pixels to an intermediate space spanned by atomic activity primitives, then programming detected primitives with interpretable logic rules to infer semantics. To afford a representative primitive space, we build a knowledge base including 26+ M primitive labels and logic rules from human priors or automatic discovering. Our framework, H uman A ctivity K nowledge E ngine ( HAKE ), exhibits superior generalization ability and performance upon canonical methods on challenging benchmarks. Code and data are available at http://hake-mvig.cn/ .},
  archive      = {J_TPAMI},
  author       = {Yong-Lu Li and Xinpeng Liu and Xiaoqian Wu and Yizhuo Li and Zuoyu Qiu and Liang Xu and Yue Xu and Hao-Shu Fang and Cewu Lu},
  doi          = {10.1109/TPAMI.2022.3232797},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8494-8506},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HAKE: A knowledge engine foundation for human activity understanding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Graph diffusion convolutional network for skeleton based
semantic recognition of two-person actions. <em>TPAMI</em>,
<em>45</em>(7), 8477–8493. (<a
href="https://doi.org/10.1109/TPAMI.2023.3238411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have successfully boosted skeleton-based human action recognition. However, existing GCN-based methods mostly cast the problem as separated person&#39;s action recognition while ignoring the interaction between the action initiator and the action responder, especially for the fundamental two-person interactive action recognition. It is still challenging to effectively take into account the intrinsic local-global clues of the two-person activity. Additionally, message passing in GCN depends on adjacency matrix, but skeleton-based human action recognition methods tend to calculate the adjacency matrix with the fixed natural skeleton connectivity. It means that messages can only travel along a fixed path at different layers of the network or in different actions, which greatly reduces the flexibility of the network. To this end, we propose a novel graph diffusion convolutional network for skeleton based semantic recognition of two-person actions by embedding the graph diffusion into GCNs. At technical fronts, we dynamically construct the adjacency matrix based on practical action information, so that we can guide the message propagation in a more meaningful way. Simultaneously, we introduce the frame importance calculation module to conduct dynamic convolution, so that we can avoid the negative effect caused by the traditional convolution, wherein the shared weights may fail to capture key frames or be affected by noisy frames. Besides, we comprehensively leverage the multidimensional features related to joints’ local visual appearances, global spatial relationship and temporal coherency, and for different features, different metrics are designed to measure the similarity underlying the corresponding real physical law of the motions. Moreover, extensive experiments and comprehensive evaluations on four public large-scale datasets (NTU-RGB+D 60, NTU-RGB+D 120, Kinetics-Skeleton 400, and SBU-Interaction) demonstrate that our method outperforms the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Shuai Li and Xinxue He and Wenfeng Song and Aimin Hao and Hong Qin},
  doi          = {10.1109/TPAMI.2023.3238411},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8477-8493},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph diffusion convolutional network for skeleton based semantic recognition of two-person actions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient descent ascent for minimax problems on riemannian
manifolds. <em>TPAMI</em>, <em>45</em>(7), 8466–8476. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of $O(\kappa ^{2}\epsilon ^{-2})$ for finding an $\epsilon$ -stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where $\kappa$ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of $O(\kappa ^{4}\epsilon ^{-4})$ for finding an $\epsilon$ -stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of $\tilde{O}(\kappa ^{4}\epsilon ^{-3})$ in searching for an $\epsilon$ -stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.},
  archive      = {J_TPAMI},
  author       = {Feihu Huang and Shangqian Gao},
  doi          = {10.1109/TPAMI.2023.3234160},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8466-8476},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gradient descent ascent for minimax problems on riemannian manifolds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global learnable attention for single image
super-resolution. <em>TPAMI</em>, <em>45</em>(7), 8453–8465. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity is valuable to the exploration of non-local textures in single image super-resolution (SISR). Researchers usually assume that the importance of non-local textures is positively related to their similarity scores. In this paper, we surprisingly found that when repairing severely damaged query textures, some non-local textures with low-similarity which are closer to the target can provide more accurate and richer details than the high-similarity ones. In these cases, low-similarity does not mean inferior but is usually caused by different scales or orientations. Utilizing this finding, we proposed a Global Learnable Attention (GLA) to adaptively modify similarity scores of non-local textures during training instead of only using a fixed similarity scoring function such as the dot product. The proposed GLA can explore non-local textures with low-similarity but more accurate details to repair severely damaged textures. Furthermore, we propose to adopt Super-Bit Locality-Sensitive Hashing (SB-LSH) as a preprocessing method for our GLA. With the SB-LSH, the computational complexity of our GLA is reduced from quadratic to asymptotic linear with respect to the image size. In addition, the proposed GLA can be integrated into existing deep SISR models as an efficient general building block. Based on the GLA, we constructed a Deep Learnable Similarity Network (DLSN), which achieves state-of-the-art performance for SISR tasks of different degradation types (e.g., blur and noise). Our code and a pre-trained DLSN have been uploaded to GitHub † for validation.},
  archive      = {J_TPAMI},
  author       = {Jian-Nan Su and Min Gan and Guang-Yong Chen and Jia-Li Yin and C. L. Philip Chen},
  doi          = {10.1109/TPAMI.2022.3229689},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8453-8465},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global learnable attention for single image super-resolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geodesic models with convexity shape prior. <em>TPAMI</em>,
<em>45</em>(7), 8433–8452. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimal geodesic models established upon the eikonal equation framework are capable of finding suitable solutions in various image segmentation scenarios. Existing geodesic-based segmentation approaches usually exploit image features in conjunction with geometric regularization terms, such as euclidean curve length or curvature-penalized length, for computing geodesic curves. In this paper, we take into account a more complicated problem: finding curvature-penalized geodesic paths with a convexity shape prior. We establish new geodesic models relying on the strategy of orientation-lifting, by which a planar curve can be mapped to an high-dimensional orientation-dependent space. The convexity shape prior serves as a constraint for the construction of local geodesic metrics encoding a particular curvature constraint. Then the geodesic distances and the corresponding closed geodesic paths in the orientation-lifted space can be efficiently computed through state-of-the-art Hamiltonian fast marching method. In addition, we apply the proposed geodesic models to the active contours, leading to efficient interactive image segmentation algorithms that preserve the advantages of convexity shape prior and curvature penalization.},
  archive      = {J_TPAMI},
  author       = {Da Chen and Jean-Marie Mirebeau and Minglei Shu and Xuecheng Tai and Laurent D. Cohen},
  doi          = {10.1109/TPAMI.2022.3225192},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8433-8452},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geodesic models with convexity shape prior},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCNet: Graph completion network for incomplete multimodal
learning in conversation. <em>TPAMI</em>, <em>45</em>(7), 8419–8432. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversations have become a critical data format on social media platforms. Understanding conversation from emotion, content and other aspects also attracts increasing attention from researchers due to its widespread application in human-computer interaction. In real-world environments, we often encounter the problem of incomplete modalities, which has become a core issue of conversation understanding. To address this problem, researchers propose various methods. However, existing approaches are mainly designed for individual utterances rather than conversational data, which cannot fully exploit temporal and speaker information in conversations. To this end, we propose a novel framework for incomplete multimodal learning in conversations, called “Graph Complete Network (GCNet),” filling the gap of existing works. Our GCNet contains two well-designed graph neural network-based modules, “Speaker GNN” and “Temporal GNN,” to capture temporal and speaker dependencies. To make full use of complete and incomplete data, we jointly optimize classification and reconstruction tasks in an end-to-end manner. To verify the effectiveness of our method, we conduct experiments on three benchmark conversational datasets. Experimental results demonstrate that our GCNet is superior to existing state-of-the-art approaches in incomplete multimodal learning.},
  archive      = {J_TPAMI},
  author       = {Zheng Lian and Lan Chen and Licai Sun and Bin Liu and Jianhua Tao},
  doi          = {10.1109/TPAMI.2023.3234553},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8419-8432},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GCNet: Graph completion network for incomplete multimodal learning in conversation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-volume 3D fluid flow reconstruction with light field
PIV. <em>TPAMI</em>, <em>45</em>(7), 8405–8418. (<a
href="https://doi.org/10.1109/TPAMI.2023.3236344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Imaging Velocimetry (PIV) is a classical method that estimates fluid flow by analyzing the motion of injected particles. To reconstruct and track the swirling particles is a difficult computer vision problem, as the particles are dense in the fluid volume and have similar appearances. Further, tracking a large number of particles is particularly challenging due to heavy occlusion. Here we present a low-cost PIV solution that uses compact lenslet-based light field cameras as imaging device. We develop novel optimization algorithms for dense particle 3D reconstruction and tracking. As a single light field camera has limited capacity in resolving depth (z-dimension measurement), the resolution of 3D reconstruction on the x-y plane is much higher than along the $z$ -axis. To compensate for the imbalanced resolution in 3D, we use two light field cameras positioned at an orthogonal angle to capture particle images. In this way, we can achieve high-resolution 3D particle reconstruction in the full fluid volume. For each time frame, we first estimate particle depths under a single viewpoint by exploiting the focal stack symmetry of light field. We then fuse the recovered 3D particles in two views by solving a linear assignment problem (LAP). Specifically, we propose an anisotropic point-to-ray distance as matching cost to handle the resolution mismatch. Finally, given a sequence of 3D particle reconstructions over time, we recover the full-volume 3D fluid flow with a physically-constrained optical flow, which enforces local motion rigidity and fluid incompressibility. We perform comprehensive experiments on synthetic and real data for ablation and evaluation. We show that our method recovers full-volume 3D fluid flows of various types. Two-view reconstruction results achieves higher accuracy than those with one view only.},
  archive      = {J_TPAMI},
  author       = {Yuqi Ding and Zhong Li and Zhang Chen and Yu Ji and Jingyi Yu and Jinwei Ye},
  doi          = {10.1109/TPAMI.2023.3236344},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8405-8418},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Full-volume 3D fluid flow reconstruction with light field PIV},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From keypoints to object landmarks via self-training
correspondence: A novel approach to unsupervised landmark discovery.
<em>TPAMI</em>, <em>45</em>(7), 8390–8404. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel paradigm for the unsupervised learning of object landmark detectors. Contrary to existing methods that build on auxiliary tasks such as image generation or equivariance, we propose a self-training approach where, departing from generic keypoints, a landmark detector and descriptor is trained to improve itself, tuning the keypoints into distinctive landmarks. To this end, we propose an iterative algorithm that alternates between producing new pseudo-labels through feature clustering and learning distinctive features for each pseudo-class through contrastive learning. With a shared backbone for the landmark detector and descriptor, the keypoint locations progressively converge to stable landmarks, filtering those less stable. Compared to previous works, our approach can learn points that are more flexible in terms of capturing large viewpoint changes. We validate our method on a variety of difficult datasets, including LS3D, BBCPose, Human3.6M and PennAction, achieving new state of the art results. Code and models can be found at https://github.com/dimitrismallis/KeypointsToLandmarks/ .},
  archive      = {J_TPAMI},
  author       = {Dimitrios Mallis and Enrique Sanchez and Matt Bell and Georgios Tzimiropoulos},
  doi          = {10.1109/TPAMI.2023.3234212},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8390-8404},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From keypoints to object landmarks via self-training correspondence: A novel approach to unsupervised landmark discovery},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Formulating event-based image reconstruction as a linear
inverse problem with deep regularization using optical flow.
<em>TPAMI</em>, <em>45</em>(7), 8372–8389. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras are novel bio-inspired sensors that measure per-pixel brightness differences asynchronously. Recovering brightness from events is appealing since the reconstructed images inherit the high dynamic range (HDR) and high-speed properties of events; hence they can be used in many robotic vision applications and to generate slow-motion HDR videos. However, state-of-the-art methods tackle this problem by training an event-to-image Recurrent Neural Network (RNN), which lacks explainability and is difficult to tune. In this work we show, for the first time, how tackling the combined problem of motion and brightness estimation leads us to formulate event-based image reconstruction as a linear inverse problem that can be solved without training an image reconstruction RNN. Instead, classical and learning-based regularizers are used to solve the problem and remove artifacts from the reconstructed images. The experiments show that the proposed approach generates images with visual quality on par with state-of-the-art methods despite only using data from a short time interval. State-of-the-art results are achieved using an image denoising Convolutional Neural Network (CNN) as the regularization function. The proposed regularized formulation and solvers have a unifying character because they can be applied also to reconstruct brightness from the second derivative. Additionally, the formulation is attractive because it can be naturally combined with super-resolution, motion-segmentation and color demosaicing. Code is available at https://github.com/tub-rip/event_based_image_rec_inverse_problem},
  archive      = {J_TPAMI},
  author       = {Zelin Zhang and Anthony J. Yezzi and Guillermo Gallego},
  doi          = {10.1109/TPAMI.2022.3230727},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8372-8389},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Formulating event-based image reconstruction as a linear inverse problem with deep regularization using optical flow},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FingerGAN: A constrained fingerprint generation scheme for
latent fingerprint enhancement. <em>TPAMI</em>, <em>45</em>(7),
8358–8371. (<a
href="https://doi.org/10.1109/TPAMI.2023.3236876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent fingerprint enhancement is an essential preprocessing step for latent fingerprint identification. Most latent fingerprint enhancement methods try to restore corrupted gray ridges/valleys. In this paper, we propose a new method that formulates latent fingerprint enhancement as a constrained fingerprint generation problem within a generative adversarial network (GAN) framework. We name the proposed network FingerGAN. It can enforce its generated fingerprint (i.e, enhanced latent fingerprint) indistinguishable from the corresponding ground truth instance in terms of the fingerprint skeleton map weighted by minutia locations and the orientation field regularized by the FOMFE model. Because minutia is the primary feature for fingerprint recognition and minutia can be retrieved directly from the fingerprint skeleton map, we offer a holistic framework that can perform latent fingerprint enhancement in the context of directly optimizing minutia information. This will help improve latent fingerprint identification performance significantly. Experimental results on two public latent fingerprint databases demonstrate that our method outperforms the state of the arts significantly. The codes will be available for non-commercial purposes from https://github.com/HubYZ/LatentEnhancement .},
  archive      = {J_TPAMI},
  author       = {Yanming Zhu and Xuefei Yin and Jiankun Hu},
  doi          = {10.1109/TPAMI.2023.3236876},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8358-8371},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FingerGAN: A constrained fingerprint generation scheme for latent fingerprint enhancement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face forgery detection by 3D decomposition and composition
search. <em>TPAMI</em>, <em>45</em>(7), 8342–8357. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting digital face manipulation has attracted extensive attention due to fake media&#39;s potential risks to the public. However, recent advances have been able to reduce the forgery signals to a low magnitude. Decomposition, which reversibly decomposes an image into several constituent elements, is a promising way to highlight the hidden forgery details. In this paper, we investigate a novel 3D decomposition based method that considers a face image as the production of the interaction between 3D geometry and lighting environment. Specifically, we disentangle a face image into four graphics components including 3D shape, lighting, common texture, and identity texture, which are respectively constrained by 3D morphable model, harmonic reflectance illumination, and PCA texture model. Meanwhile, we build a fine-grained morphing network to predict 3D shapes with pixel-level accuracy to reduce the noise in the decomposed elements. Moreover, we propose a composition search strategy that enables an automatic construction of an architecture to mine forgery clues from forgery-relevant components. Extensive experiments validate that the decomposed components highlight forgery artifacts, and the searched architecture extracts discriminative forgery features. Thus, our method achieves the state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Xiangyu Zhu and Hongyan Fei and Bin Zhang and Tianshuo Zhang and Xiaoyu Zhang and Stan Z. Li and Zhen Lei},
  doi          = {10.1109/TPAMI.2022.3233586},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8342-8357},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Face forgery detection by 3D decomposition and composition search},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). EPNet++: Cascade bi-directional fusion for multi-modal 3D
object detection. <em>TPAMI</em>, <em>45</em>(7), 8324–8341. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fusing the LiDAR point cloud and camera image to improve the performance and robustness of 3D object detection has received more and more attention, as these two modalities naturally possess strong complementarity. In this paper, we propose EPNet++ for multi-modal 3D object detection by introducing a novel Cascade Bi-directional Fusion (CB-Fusion) module and a Multi-Modal Consistency (MC) loss. More concretely, the proposed CB-Fusion module enhances point features with plentiful semantic information absorbed from the image features in a cascade bi-directional interaction fusion manner, leading to more powerful and discriminative feature representations. The MC loss explicitly guarantees the consistency between predicted scores from two modalities to obtain more comprehensive and reliable confidence scores. The experimental results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the superiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize a critical but easily overlooked problem, which is to explore the performance and robustness of a 3D detector in a sparser scene. Extensive experiments present that EPNet++ outperforms the existing SOTA methods with remarkable margins in highly sparse point cloud cases, which might be an available direction to reduce the expensive cost of LiDAR sensors.},
  archive      = {J_TPAMI},
  author       = {Zhe Liu and Tengteng Huang and Bingling Li and Xiwu Chen and Xi Wang and Xiang Bai},
  doi          = {10.1109/TPAMI.2022.3228806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8324-8341},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EPNet++: Cascade bi-directional fusion for multi-modal 3D object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E2E-FS: An end-to-end feature selection method for neural
networks. <em>TPAMI</em>, <em>45</em>(7), 8311–8323. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic embedded feature selection algorithms are often divided in two large groups: tree-based algorithms and LASSO variants. Both approaches are focused in different aspects: while the tree-based algorithms provide a clear explanation about which variables are being used to trigger a certain output, LASSO-like approaches sacrifice a detailed explanation in favor of increasing its accuracy. In this paper, we present a novel embedded feature selection algorithm, called End-to-End Feature Selection (E2E-FS), that aims to provide both accuracy and explainability in a clever way. Despite having non-convex regularization terms, our algorithm, similar to the LASSO approach, is solved with gradient descent techniques, introducing some restrictions that force the model to specifically select a maximum number of features that are going to be used subsequently by the classifier. Although these are hard restrictions, the experimental results obtained show that this algorithm can be used with any learning model that is trained using a gradient descent algorithm.},
  archive      = {J_TPAMI},
  author       = {Brais Cancela and Verónica Bolón-Canedo and Amparo Alonso-Betanzos},
  doi          = {10.1109/TPAMI.2022.3228824},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8311-8323},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {E2E-FS: An end-to-end feature selection method for neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable multi-granularity human parsing.
<em>TPAMI</em>, <em>45</em>(7), 8296–8310. (<a
href="https://doi.org/10.1109/TPAMI.2023.3239194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the challenging problem of instance-aware human body part parsing. We introduce a new bottom-up regime which achieves the task through learning category-level human semantic segmentation as well as multi-person pose estimation in a joint and end-to-end manner. The output is a compact, efficient and powerful framework that exploits structural information over different human granularities and eases the difficulty of person partitioning. Specifically, a dense-to-sparse projection field, which allows explicitly associating dense human semantics with sparse keypoints, is learnt and progressively improved over the network feature pyramid for robustness. Then, the difficult pixel grouping problem is cast as an easier, multi-person joint assembling task. By formulating joint association as maximum-weight bipartite matching, we develop two novel algorithms based on projected gradient descent and unbalanced optimal transport, respectively, to solve the matching problem differentiablly. These algorithms make our method end-to-end trainable and allow back-propagating the grouping error to directly supervise multi-granularity human representation learning. This is significantly distinguished from current bottom-up human parsers or pose estimators which require sophisticated post-processing or heuristic greedy algorithms. Extensive experiments on three instance-aware human parsing datasets ( i.e ., MHP-v2, DensePose-COCO, PASCAL-Person-Part) demonstrate that our approach outperforms most existing human parsers with much more efficient inference. Our code is available at https://github.com/tfzhou/MG-HumanParsing .},
  archive      = {J_TPAMI},
  author       = {Tianfei Zhou and Yi Yang and Wenguan Wang},
  doi          = {10.1109/TPAMI.2023.3239194},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8296-8310},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentiable multi-granularity human parsing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection-friendly dehazing: Object detection in real-world
hazy scenes. <em>TPAMI</em>, <em>45</em>(7), 8284–8295. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse weather conditions in real-world scenarios lead to performance degradation of deep learning-based detection models. A well-known method is to use image restoration methods to enhance degraded images before object detection. However, how to build a positive correlation between these two tasks is still technically challenging. The restoration labels are also unavailable in practice. To this end, taking the hazy scene as an example, we propose a union architecture BAD-Net that connects the dehazing module and detection module in an end-to-end manner. Specifically, we design a two-branch structure with an attention fusion module for fully combining hazy and dehazing features. This reduces bad impacts on the detection module when the dehazing module performs poorly. Besides, we introduce a self-supervised haze robust loss that enables the detection module to deal with different degrees of haze. Most importantly, an interval iterative data refinement training strategy is proposed to guide the dehazing module learning with weak supervision. BAD-Net improves further detection performance through detection-friendly dehazing. Extensive experiments on RTTS and VOChaze datasets show that BAD-Net achieves higher accuracy compared to the recent state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection.},
  archive      = {J_TPAMI},
  author       = {Chengyang Li and Heng Zhou and Yang Liu and Caidong Yang and Yongqiang Xie and Zhongbo Li and Liping Zhu},
  doi          = {10.1109/TPAMI.2023.3234976},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8284-8295},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Detection-friendly dehazing: Object detection in real-world hazy scenes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep metric learning with adaptively composite dynamic
constraints. <em>TPAMI</em>, <em>45</em>(7), 8265–8283. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a deep metric learning with adaptively composite dynamic constraints (DML-DC) method for image retrieval and clustering. Most existing deep metric learning methods impose pre-defined constraints on the training samples, which might not be optimal at all stages of training. To address this, we propose a learnable constraint generator to adaptively produce dynamic constraints to train the metric towards good generalization. We formulate the objective of deep metric learning under a proxy Collection, pair Sampling, tuple Construction, and tuple Weighting (CSCW) paradigm. For proxy collection, we progressively update a set of proxies using a cross-attention mechanism to integrate information from the current batch of samples. For pair sampling, we employ a graph neural network to model the structural relations between sample-proxy pairs to produce the preservation probabilities for each pair. Having constructed a set of tuples based on the sampled pairs, we further re-weight each training tuple to adaptively adjust its effect on the metric. We formulate the learning of the constraint generator as a meta-learning problem, where we employ an episode-based training scheme and update the generator at each iteration to adapt to the current model status. We construct each episode by sampling two subsets of disjoint labels to simulate the procedure of training and testing and use the performance of the one-gradient-updated metric on the validation subset as the meta-objective of the assessor. We conducted extensive experiments on five widely used benchmarks under two evaluation protocols to demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TPAMI},
  author       = {Wenzhao Zheng and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2023.3234536},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8265-8283},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep metric learning with adaptively composite dynamic constraints},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep depth completion from extremely sparse data: A survey.
<em>TPAMI</em>, <em>45</em>(7), 8244–8264. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims at predicting dense pixel-wise depth from an extremely sparse map captured from a depth sensor, e.g., LiDARs. It plays an essential role in various applications such as autonomous driving, 3D reconstruction, augmented reality, and robot navigation. Recent successes on the task have been demonstrated and dominated by deep learning based solutions. In this article, for the first time, we provide a comprehensive literature review that helps readers better grasp the research trends and clearly understand the current advances. We investigate the related studies from the design aspects of network architectures, loss functions, benchmark datasets, and learning strategies with a proposal of a novel taxonomy that categorizes existing methods. Besides, we present a quantitative comparison of model performance on three widely used benchmarks, including indoor and outdoor datasets. Finally, we discuss the challenges of prior works and provide readers with some insights for future research directions.},
  archive      = {J_TPAMI},
  author       = {Junjie Hu and Chenyu Bao and Mete Ozay and Chenyou Fan and Qing Gao and Honghai Liu and Tin Lun Lam},
  doi          = {10.1109/TPAMI.2022.3229090},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8244-8264},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep depth completion from extremely sparse data: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DAN: A segmentation-free document attention network for
handwritten document recognition. <em>TPAMI</em>, <em>45</em>(7),
8227–8243. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unconstrained handwritten text recognition is a challenging computer vision task. It is traditionally handled by a two-step approach, combining line segmentation followed by text line recognition. For the first time, we propose an end-to-end segmentation-free architecture for the task of handwritten document recognition: the Document Attention Network. In addition to text recognition, the model is trained to label text parts using begin and end tags in an XML-like fashion. This model is made up of an FCN encoder for feature extraction and a stack of transformer decoder layers for a recurrent token-by-token prediction process. It takes whole text documents as input and sequentially outputs characters, as well as logical layout tokens. Contrary to the existing segmentation-based approaches, the model is trained without using any segmentation label. We achieve competitive results on the READ 2016 dataset at page level, as well as double-page level with a CER of 3.43\% and 3.70\%, respectively. We also provide results for the RIMES 2009 dataset at page level, reaching 4.54\% of CER. We provide all source code and pre-trained model weights at https://github.com/FactoDeepLearning/DAN .},
  archive      = {J_TPAMI},
  author       = {Denis Coquenet and Clément Chatelain and Thierry Paquet},
  doi          = {10.1109/TPAMI.2023.3235826},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8227-8243},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DAN: A segmentation-free document attention network for handwritten document recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DaisyRec 2.0: Benchmarking recommendation for rigorous
evaluation. <em>TPAMI</em>, <em>45</em>(7), 8206–8226. (<a
href="https://doi.org/10.1109/TPAMI.2022.3231891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, one critical issue looms large in the field of recommender systems – there are no effective benchmarks for rigorous evaluation – which consequently leads to unreproducible evaluation and unfair comparison. We, therefore, conduct studies from the perspectives of practical theory and experiments, aiming at benchmarking recommendation for rigorous evaluation. Regarding the theoretical study, a series of hyper-factors affecting recommendation performance throughout the whole evaluation chain are systematically summarized and analyzed via an exhaustive review on 141 papers published at eight top-tier conferences within 2017-2020. We then classify them into model-independent and model-dependent hyper-factors, and different modes of rigorous evaluation are defined and discussed in-depth accordingly. For the experimental study, we release DaisyRec 2.0 library by integrating these hyper-factors to perform rigorous evaluation, whereby a holistic empirical study is conducted to unveil the impacts of different hyper-factors on recommendation performance. Supported by the theoretical and experimental studies, we finally create benchmarks for rigorous evaluation by proposing standardized procedures and providing performance of ten state-of-the-arts across six evaluation metrics on six datasets as a reference for later study. Overall, our work sheds light on the issues in recommendation evaluation, provides potential solutions for rigorous evaluation, and lays foundation for further investigation.},
  archive      = {J_TPAMI},
  author       = {Zhu Sun and Hui Fang and Jie Yang and Xinghua Qu and Hongyang Liu and Di Yu and Yew-Soon Ong and Jie Zhang},
  doi          = {10.1109/TPAMI.2022.3231891},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8206-8226},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DaisyRec 2.0: Benchmarking recommendation for rigorous evaluation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-salient object detection with co-representation
purification. <em>TPAMI</em>, <em>45</em>(7), 8193–8205. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-salient object detection (Co-SOD) aims at discovering the common objects in a group of relevant images. Mining a co-representation is essential for locating co-salient objects. Unfortunately, the current Co-SOD method does not pay enough attention that the information not related to the co-salient object is included in the co-representation. Such irrelevant information in the co-representation interferes with its locating of co-salient objects. In this paper, we propose a Co-Representation Purification (CoRP) method aiming at searching noise-free co-representation. We search a few pixel-wise embeddings probably belonging to co-salient regions. These embeddings constitute our co-representation and guide our prediction. For obtaining purer co-representation, we use the prediction to iteratively reduce irrelevant embeddings in our co-representation. Experiments on three datasets demonstrate that our CoRP achieves state-of-the-art performances on the benchmark datasets. Our source code is available at https://github.com/ZZY816/CoRP .},
  archive      = {J_TPAMI},
  author       = {Ziyue Zhu and Zhao Zhang and Zheng Lin and Xing Sun and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2023.3234586},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8193-8205},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Co-salient object detection with co-representation purification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolution-enhanced evolving attention networks.
<em>TPAMI</em>, <em>45</em>(7), 8176–8192. (<a
href="https://doi.org/10.1109/TPAMI.2023.3236725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend among attention maps at different abstraction levels, so it is beneficial to exploit a dedicated convolution-based module to capture this process. Equipped with the proposed mechanism, the convolution-enhanced evolving attention networks achieve superior performance in various applications, including time-series representation, natural language understanding, machine translation, and image classification. Especially on time-series representation tasks, Evolving Attention-enhanced Dilated Convolutional (EA-DC-) Transformer outperforms state-of-the-art models significantly, achieving an average of 17\% improvement compared to the best SOTA. To the best of our knowledge, this is the first work that explicitly models the layer-wise evolution of attention maps. Our implementation is available at https://github.com/pkuyym/EvolvingAttention .},
  archive      = {J_TPAMI},
  author       = {Yujing Wang and Yaming Yang and Zhuo Li and Jiangang Bai and Mingliang Zhang and Xiangtai Li and Jing Yu and Ce Zhang and Gao Huang and Yunhai Tong},
  doi          = {10.1109/TPAMI.2023.3236725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8176-8192},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Convolution-enhanced evolving attention networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional hough matching networks for robust and
efficient visual correspondence. <em>TPAMI</em>, <em>45</em>(7),
8159–8175. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advances in feature representation, leveraging geometric relations is crucial for establishing reliable visual correspondences under large variations of images. In this work we introduce a Hough transform perspective on convolutional matching and propose an effective geometric matching algorithm, dubbed Convolutional Hough Matching (CHM). The method distributes similarities of candidate matches over a geometric transformation space and evaluates them in a convolutional manner. We cast it into a trainable neural layer with a semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters. To further improve the efficiency of high-dimensional voting, we also propose to use an efficient kernel decomposition with center-pivot neighbors, which significantly sparsifies the proposed semi-isotropic kernels without performance degradation. To validate the proposed techniques, we develop the neural network with CHM layers that perform convolutional matching in the space of translation and scaling. Our method sets a new state of the art on standard benchmarks for semantic visual correspondence, proving its strong robustness to challenging intra-class variations.},
  archive      = {J_TPAMI},
  author       = {Juhong Min and Seungwook Kim and Minsu Cho},
  doi          = {10.1109/TPAMI.2022.3233884},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8159-8175},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Convolutional hough matching networks for robust and efficient visual correspondence},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous conditional generative adversarial networks:
Novel empirical losses and label input mechanisms. <em>TPAMI</em>,
<em>45</em>(7), 8143–8158. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on conditional generative modeling (CGM) for image data with continuous, scalar conditions (termed regression labels). We propose the first model for this task which is called continuous conditional generative adversarial network (CcGAN). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels). Conditioning on regression labels is mathematically distinct and raises two fundamental problems: (P1) since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; and (P2) since regression labels are scalar and infinitely many, conventional label input mechanisms (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. We solve these problems by: (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) mechanism and an improved label input (ILI) mechanism to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. Hence, we propose four versions of CcGAN employing different proposed losses and label input mechanisms. The error bounds of the discriminator trained with HVDL and SVDL, respectively, are derived under mild assumptions. To evaluate the performance of CcGANs, two new benchmark datasets (RC-49 and Cell-200) are created. A novel evaluation metric ( Sliding Fréchet Inception Distance ) is also proposed to replace Intra-FID when Intra-FID is not applicable. Our extensive experiments on several benchmark datasets (i.e., RC-49, UTKFace, Cell-200, and Steering Angle with both low and high resolutions) support the following findings: the proposed CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label; and CcGAN substantially outperforms cGAN both visually and quantitatively.},
  archive      = {J_TPAMI},
  author       = {Xin Ding and Yongwei Wang and Zuheng Xu and William J. Welch and Z. Jane Wang},
  doi          = {10.1109/TPAMI.2022.3228915},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8143-8158},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Continuous conditional generative adversarial networks: Novel empirical losses and label input mechanisms},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capture the moment: High-speed imaging with spiking cameras
through short-term plasticity. <em>TPAMI</em>, <em>45</em>(7),
8127–8142. (<a
href="https://doi.org/10.1109/TPAMI.2023.3237856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-speed imaging can help us understand some phenomena that are too fast to be captured by our eyes. Although ultra-fast frame-based cameras (e.g., Phantom) can record millions of fps at reduced resolution, they are too expensive to be widely used. Recently, a retina-inspired vision sensor, spiking camera, has been developed to record external information at 40, 000 Hz. The spiking camera uses the asynchronous binary spike streams to represent visual information. Despite this, how to reconstruct dynamic scenes from asynchronous spikes remains challenging. In this paper, we introduce novel high-speed image reconstruction models based on the short-term plasticity (STP) mechanism of the brain, termed TFSTP and TFMDSTP. We first derive the relationship between states of STP and spike patterns. Then, in TFSTP, by setting up the STP model at each pixel, the scene radiance can be inferred by the states of the models. In TFMDSTP, we use the STP to distinguish the moving and stationary regions, and then use two sets of STP models to reconstruct them respectively. In addition, we present a strategy for correcting error spikes. Experimental results show that the STP-based reconstruction methods can effectively reduce noise with less computing time, and achieve the best performances on both real-world and simulated datasets.},
  archive      = {J_TPAMI},
  author       = {Yajing Zheng and Lingxiao Zheng and Zhaofei Yu and Tiejun Huang and Song Wang},
  doi          = {10.1109/TPAMI.2023.3237856},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8127-8142},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Capture the moment: High-speed imaging with spiking cameras through short-term plasticity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BoostTree and BoostForest for ensemble learning.
<em>TPAMI</em>, <em>45</em>(7), 8110–8126. (<a
href="https://doi.org/10.1109/TPAMI.2022.3227370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrap aggregating (Bagging) and boosting are two popular ensemble learning approaches, which combine multiple base learners to generate a composite model for more accurate and more reliable performance. They have been widely used in biology, engineering, healthcare, etc. This article proposes BoostForest, which is an ensemble learning approach using BoostTree as base learners and can be used for both classification and regression. BoostTree constructs a tree model by gradient boosting. It increases the randomness (diversity) by drawing the cut-points randomly at node splitting. BoostForest further increases the randomness by bootstrapping the training data in constructing different BoostTrees. BoostForest generally outperformed four classical ensemble learning approaches (Random Forest, Extra-Trees, XGBoost and LightGBM) on 35 classification and regression datasets. Remarkably, BoostForest tunes its parameters by simply sampling them randomly from a parameter pool, which can be easily specified, and its ensemble learning framework can also be used to combine many other base learners.},
  archive      = {J_TPAMI},
  author       = {Changming Zhao and Dongrui Wu and Jian Huang and Ye Yuan and Hai-Tao Zhang and Ruimin Peng and Zhenhua Shi},
  doi          = {10.1109/TPAMI.2022.3227370},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8110-8126},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BoostTree and BoostForest for ensemble learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymmetric loss functions for noise-tolerant learning:
Theory and applications. <em>TPAMI</em>, <em>45</em>(7), 8094–8109. (<a
href="https://doi.org/10.1109/TPAMI.2023.3236459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning has achieved tremendous success in many computer vision tasks, which however is prone to overfit noisy labels. To mitigate the undesirable influence of noisy labels, robust loss functions offer a feasible approach to achieve noise-tolerant learning. In this work, we systematically study the problem of noise-tolerant learning with respect to both classification and regression. Specifically, we propose a new class of loss function, namely asymmetric loss functions (ALFs), which are tailored to satisfy the Bayes-optimal condition and thus are robust to noisy labels. For classification, we investigate general theoretical properties of ALFs on categorical noisy labels, and introduce the asymmetry ratio to measure the asymmetry of a loss function. We extend several commonly-used loss functions, and establish the necessary and sufficient conditions to make them asymmetric and thus noise-tolerant. For regression, we extend the concept of noise-tolerant learning for image restoration with continuous noisy labels. We theoretically prove that $\ell _{p}$ loss ( $p&amp;gt;0$ ) is noise-tolerant for targets with the additive white Gaussian noise. For targets with general noise, we introduce two losses as surrogates of $\ell _{0}$ loss that seeks the mode when clean pixels keep dominant. Experimental results demonstrate that ALFs can achieve better or comparative performance compared with the state-of-the-arts. The source code of our method is available at: https://github.com/hitcszx/ALFs .},
  archive      = {J_TPAMI},
  author       = {Xiong Zhou and Xianming Liu and Deming Zhai and Junjun Jiang and Xiangyang Ji},
  doi          = {10.1109/TPAMI.2023.3236459},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8094-8109},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Asymmetric loss functions for noise-tolerant learning: Theory and applications},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarially-regularized mixed effects deep learning
(ARMED) models improve interpretability, performance, and generalization
on clustered (non-iid) data. <em>TPAMI</em>, <em>45</em>(7), 8081–8093.
(<a href="https://doi.org/10.1109/TPAMI.2023.3234291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural science datasets frequently violate assumptions of independence. Samples may be clustered (e.g., by study site, subject, or experimental batch), leading to spurious associations, poor model fitting, and confounded analyses. While largely unaddressed in deep learning, this problem has been handled in the statistics community through mixed effects models, which separate cluster-invariant fixed effects from cluster-specific random effects. We propose a general-purpose framework for Adversarially-Regularized Mixed Effects Deep learning (ARMED) models through non-intrusive additions to existing neural networks: 1) an adversarial classifier constraining the original model to learn only cluster-invariant features, 2) a random effects subnetwork capturing cluster-specific features, and 3) an approach to apply random effects to clusters unseen during training. We apply ARMED to dense, convolutional, and autoencoder neural networks on 4 datasets including simulated nonlinear data, dementia prognosis and diagnosis, and live-cell image analysis. Compared to prior techniques, ARMED models better distinguish confounded from true associations in simulations and learn more biologically plausible features in clinical applications. They can also quantify inter-cluster variance and visualize cluster effects in data. Finally, ARMED matches or improves performance on data from clusters seen during training (5-28\% relative improvement) and generalization to unseen clusters (2-9\% relative improvement) versus conventional models.},
  archive      = {J_TPAMI},
  author       = {Kevin P. Nguyen and Alex H. Treacher and Albert A. Montillo},
  doi          = {10.1109/TPAMI.2023.3234291},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8081-8093},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarially-regularized mixed effects deep learning (ARMED) models improve interpretability, performance, and generalization on clustered (non-iid) data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive subgraph neural network with reinforced critical
structure mining. <em>TPAMI</em>, <em>45</em>(7), 8063–8080. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While graph representation learning methods have shown success in various graph mining tasks, what knowledge is exploited for predictions is less discussed. This paper proposes a novel A daptive S ubgraph N eural N etwork named AdaSNN to find critical structures in graph data, i.e., subgraphs that are dominant to the prediction results. To detect critical subgraphs of arbitrary size and shape in the absence of explicit subgraph-level annotations, AdaSNN designs a Reinforced Subgraph Detection Module to search subgraphs adaptively without heuristic assumptions or predefined rules. To encourage the subgraph to be predictive at the global scale, we design a Bi-Level Mutual Information Enhancement Mechanism including both global-aware and label-aware mutual information maximization to further enhance the subgraph representations in the perspective of information theory. By mining critical subgraphs that reflect the intrinsic property of a graph, AdaSNN can provide sufficient interpretability to the learned results. Comprehensive experimental results on seven typical graph datasets demonstrate that AdaSNN has a significant and consistent performance improvement and provides insightful results.},
  archive      = {J_TPAMI},
  author       = {Jianxin Li and Qingyun Sun and Hao Peng and Beining Yang and Jia Wu and Philip S. Yu},
  doi          = {10.1109/TPAMI.2023.3235931},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8063-8080},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive subgraph neural network with reinforced critical structure mining},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive siamese tracking with a compact latent network.
<em>TPAMI</em>, <em>45</em>(7), 8049–8062. (<a
href="https://doi.org/10.1109/TPAMI.2022.3230064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide an intuitive viewing to simplify the Siamese-based trackers by converting the tracking task to a classification. Under this viewing, we perform an in-depth analysis for them through visual simulations and real tracking examples, and find that the failure cases in some challenging situations can be regarded as the issue of missing decisive samples in offline training. Since the samples in the initial (first) frame contain rich sequence-specific information, we can regard them as the decisive samples to represent the whole sequence. To quickly adapt the base model to new scenes, a compact latent network is presented via fully using these decisive samples. Specifically, we present a statistics-based compact latent feature for fast adjustment by efficiently extracting the sequence-specific information. Furthermore, a new diverse sample mining strategy is designed for training to further improve the discrimination ability of the proposed compact latent network. Finally, a conditional updating strategy is proposed to efficiently update the basic models to handle scene variation during the tracking phase. To evaluate the generalization ability and effectiveness and of our method, we apply it to adjust three classical Siamese-based trackers, namely SiamRPN++, SiamFC, and SiamBAN. Extensive experimental results on six recent datasets demonstrate that all three adjusted trackers obtain the superior performance in terms of the accuracy, while having high running speed.},
  archive      = {J_TPAMI},
  author       = {Xingping Dong and Jianbing Shen and Fatih Porikli and Jiebo Luo and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3230064},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8049-8062},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive siamese tracking with a compact latent network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unifying probabilistic framework for partially labeled
data learning. <em>TPAMI</em>, <em>45</em>(7), 8036–8048. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially labeled data learning (PLDL), including partial label learning (PLL) and partial multi-label learning (PML), has been widely used in nowadays data science. Researchers attempt to construct different specific models to deal with the different classification tasks for PLL and PML scenarios respectively. The main challenge in training classifiers for PLL and PML is how to deal with ambiguities caused by the noisy false-positive labels in the candidate label set. The state-of-the-art strategy for both scenarios is to perform disambiguation by identifying the ground-truth label(s) directly from the candidate label set, which can be summarized into two categories: ‘the identifying method’ and ‘the embedding method’. However, both kinds of methods are constructed by hand-designed heuristic modeling under considerations like feature/label correlations with no theoretical interpretation. Instead of adopting heuristic or specific modeling, we propose a novel unifying framework called A Unifying Probabilistic Framework for Partially Labeled Data Learning (UPF-PLDL), which is derived from a clear probabilistic formulation, and brings existing research on PLL and PML under one theoretical interpretation with respect to information theory. Furthermore, the proposed UPF-PLDL also unifies ‘the identifying method’ and ‘the embedding method’ into one integrated framework, which naturally incorporates the feature and label correlation considerations. Comprehensive experiments on synthetic and real-world datasets for both PLL and PML scenarios clearly demonstrate the superiorities of the derived framework.},
  archive      = {J_TPAMI},
  author       = {Xiuwen Gong and Dong Yuan and Wei Bao and Fulin Luo},
  doi          = {10.1109/TPAMI.2022.3228755},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8036-8048},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unifying probabilistic framework for partially labeled data learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified visual information preservation framework for
self-supervised pre-training in medical image analysis. <em>TPAMI</em>,
<em>45</em>(7), 8020–8035. (<a
href="https://doi.org/10.1109/TPAMI.2023.3234002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-supervised learning (SSL) in computer vision are primarily comparative, whose goal is to preserve invariant and discriminative semantics in latent representations by comparing siamese image views. However, the preserved high-level semantics do not contain enough local information, which is vital in medical image analysis (e.g., image-based diagnosis and tumor segmentation). To mitigate the locality problem of comparative SSL, we propose to incorporate the task of pixel restoration for explicitly encoding more pixel-level information into high-level semantics. We also address the preservation of scale information, a powerful tool in aiding image understanding but has not drawn much attention in SSL. The resulting framework can be formulated as a multi-task optimization problem on the feature pyramid. Specifically, we conduct multi-scale pixel restoration and siamese feature comparison in the pyramid. In addition, we propose non-skip U-Net to build the feature pyramid and develop sub-crop to replace multi-crop in 3D medical imaging. The proposed unified SSL framework (PCRLv2) surpasses its self-supervised counterparts on various tasks, including brain tumor segmentation (BraTS 2018), chest pathology identification (ChestX-ray, CheXpert), pulmonary nodule detection (LUNA), and abdominal organ segmentation (LiTS), sometimes outperforming them by large margins with limited annotations. Codes and models are available at https://github.com/RL4M/PCRLv2 .},
  archive      = {J_TPAMI},
  author       = {Hong-Yu Zhou and Chixiang Lu and Chaoqi Chen and Sibei Yang and Yizhou Yu},
  doi          = {10.1109/TPAMI.2023.3234002},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8020-8035},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified visual information preservation framework for self-supervised pre-training in medical image analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A thorough benchmark and a new model for light field
saliency detection. <em>TPAMI</em>, <em>45</em>(7), 8003–8019. (<a
href="https://doi.org/10.1109/TPAMI.2023.3235415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with current RGB or RGB-D saliency detection datasets, those for light field saliency detection often suffer from many defects, e.g., insufficient data amount and diversity, incomplete data formats, and rough annotations, thus impeding the prosperity of this field. To settle these issues, we elaborately build a large-scale light field dataset, dubbed PKU-LF , comprising 5,000 light fields and covering diverse indoor and outdoor scenes. Our PKU-LF provides all-inclusive representation formats of light fields and offers a unified platform for comparing algorithms utilizing different input formats. For sparking new vitality in saliency detection tasks, we present many unexplored scenarios (such as underwater and high-resolution scenes) and the richest annotations (such as scribble annotations, bounding boxes, object-/instance-level annotations, and edge annotations), on which many potential attention modeling tasks can be investigated. To facilitate the development of saliency detection, we systematically evaluate and analyze 16 representative 2D, 3D, and 4D methods on four existing datasets and the proposed dataset, furnishing a thorough benchmark. Furthermore, tailored to the distinct structural characteristics of light fields, a novel symmetric two-stream architecture ( STSA ) network is proposed to predict the saliency of light fields more accurately. Specifically, our STSA incorporates a focalness interweavement module (FIM) and three partial decoder modules (PDM). The former is designed to efficiently establish long-range dependencies across focal slices, while the latter aims to effectively aggregate the extracted coadjutant features in a mutual-enhancement way. Extensive experiments demonstrate that our method can significantly outperform the competitors.},
  archive      = {J_TPAMI},
  author       = {Wei Gao and Songlin Fan and Ge Li and Weisi Lin},
  doi          = {10.1109/TPAMI.2023.3235415},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {8003-8019},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A thorough benchmark and a new model for light field saliency detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new outlier removal strategy based on reliability of
correspondence graph for fast point cloud registration. <em>TPAMI</em>,
<em>45</em>(7), 7986–8002. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Registration is a basic yet crucial task in point cloud processing. In correspondence-based point cloud registration, matching correspondences by point feature techniques may lead to an extremely high outlier (false correspondence) ratio. Current outlier removal methods still suffer from low efficiency, accuracy, and recall rate. We use an intuitive method to describe the 6-DOF (degree of freedom) curtailment process in point cloud registration and propose an outlier removal strategy based on the reliability of the correspondence graph. The method constructs the corresponding graph according to the given correspondences and designs the concept of the reliability degree of the graph node for optimal candidate selection and the reliability degree of the graph edge to obtain the global maximum consensus set. The presented method achieves fast and accurate outliers removal along with gradual aligning parameters estimation. Extensive experiments on simulations and challenging real-world datasets demonstrate that the proposed method can still perform effective point cloud registration even the correspondence outlier ratio is over 99\%, and the efficiency is better than the state-of-the-art. Code is available at https://github.com/WPC-WHU/GROR .},
  archive      = {J_TPAMI},
  author       = {Li Yan and Pengcheng Wei and Hong Xie and Jicheng Dai and Hao Wu and Ming Huang},
  doi          = {10.1109/TPAMI.2022.3226498},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {7986-8002},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A new outlier removal strategy based on reliability of correspondence graph for fast point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-task multi-stage transitional training framework for
neural chat translation. <em>TPAMI</em>, <em>45</em>(7), 7970–7985. (<a
href="https://doi.org/10.1109/TPAMI.2022.3233226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural chat translation (NCT) aims to translate a cross-lingual chat between speakers of different languages. Existing context-aware NMT models cannot achieve satisfactory performances due to the following inherent problems: 1) limited resources of annotated bilingual dialogues; 2) the neglect of modelling conversational properties; 3) training discrepancy between different stages. To address these issues, in this paper, we propose a m ulti-task m ulti-stage t ransitional (MMT) training framework, where an NCT model is trained using the bilingual chat translation dataset and additional monolingual dialogues. We elaborately design two auxiliary tasks, namely utterance discrimination and speaker discrimination, to introduce the modelling of dialogue coherence and speaker characteristic into the NCT model. The training process consists of three stages: 1) sentence-level pre-training on large-scale parallel corpus; 2) intermediate training with auxiliary tasks using additional monolingual dialogues; 3) context-aware fine-tuning with gradual transition. Particularly, the second stage serves as an intermediate phase that alleviates the training discrepancy between the pre-training and fine-tuning stages. Moreover, to make the stage transition smoother, we train the NCT model using a gradual transition strategy, i.e., gradually transiting from using monolingual to bilingual dialogues. Extensive experiments on two language pairs demonstrate the effectiveness and superiority of our proposed training framework.},
  archive      = {J_TPAMI},
  author       = {Chulun Zhou and Yunlong Liang and Fandong Meng and Jie Zhou and Jinan Xu and Hongji Wang and Min Zhang and Jinsong Su},
  doi          = {10.1109/TPAMI.2022.3233226},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {7970-7985},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A multi-task multi-stage transitional training framework for neural chat translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic graph-based neural architecture encoding scheme
with multifaceted information. <em>TPAMI</em>, <em>45</em>(7),
7955–7969. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) can automatically discover well-performing architectures in a large search space and has been shown to bring improvements to various applications. However, the computational burden of NAS is huge, since exploring a large search space can need evaluating more than thousands of architecture samples. To improve the sample efficiency of search space exploration, predictor-based NAS methods learn a performance predictor of architectures, and utilize the predictor to sample worth-evaluating architectures. The encoding scheme of NN architectures is crucial to the predictor&#39;s generalization ability, and thus crucial to the efficacy of the NAS process. To this end, we have designed a generic Graph-based neural ArchiTecture Encoding Scheme (GATES), a more reasonable modeling of NN architectures that mimics their data processing. Nevertheless, GATES is unaware of the concrete computing semantic of NN operations or architectures. Thus, the learning of operation embeddings and weights in GATES can only exploit the information in architectures-performance pairs. We propose GATES++, which incorporates multifaceted information about NN&#39;s operation-level and architecture-level computing semantics into its construction and training, respectively. Experiments on benchmark search spaces show that both the operation-level and architecture-level information can bring improvements alone, and GATES++ can discover better architectures after evaluating the same number of architectures.},
  archive      = {J_TPAMI},
  author       = {Xuefei Ning and Yin Zheng and Zixuan Zhou and Tianchen Zhao and Huazhong Yang and Yu Wang},
  doi          = {10.1109/TPAMI.2022.3228604},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {7955-7969},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A generic graph-based neural architecture encoding scheme with multifaceted information},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep framework for hyperspectral image fusion between
different satellites. <em>TPAMI</em>, <em>45</em>(7), 7939–7954. (<a
href="https://doi.org/10.1109/TPAMI.2022.3229433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fusing a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) of different satellites has become an effective way to improve the resolution of an HSI. However, due to different imaging satellites, different illumination, and adjacent imaging time, the LR-HSI and HR-MSI may not satisfy the observation models established by existing works, and the LR-HSI and HR-MSI are hard to be registered. To solve the above problems, we establish new observation models for LR-HSIs and HR-MSIs from different satellites, then a deep-learning-based framework is proposed to solve the key steps in multi-satellite HSI fusion, including image registration, blur kernel learning, and image fusion. Specifically, we first construct a convolutional neural network (CNN), called RegNet, to produce pixel-wise offsets between LR-HSI and HR-MSI, which are utilized to register the LR-HSI. Next, according to the new observation models, a tiny network, called BKLNet, is built to learn the spectral and spatial blur kernels, where the BKLNet and RegNet can be trained jointly. In the fusion part, we further train a FusNet by downsampling the registered data with the learned spatial blur kernel. Extensive experiments demonstrate the superiority of the proposed framework in HSI registration and fusion accuracy.},
  archive      = {J_TPAMI},
  author       = {Anjing Guo and Renwei Dian and Shutao Li},
  doi          = {10.1109/TPAMI.2022.3229433},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {7939-7954},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A deep framework for hyperspectral image fusion between different satellites},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-hot graph encoder embedding. <em>TPAMI</em>,
<em>45</em>(6), 7933–7938. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we propose a lightning fast graph embedding method called one-hot graph encoder embedding. It has a linear computational complexity and the capacity to process billions of edges within minutes on standard PC — making it an ideal candidate for huge graph processing. It is applicable to either adjacency matrix or graph Laplacian, and can be viewed as a transformation of the spectral embedding. Under random graph models, the graph encoder embedding is approximately normally distributed per vertex, and asymptotically converges to its mean. We showcase three applications: vertex classification, vertex clustering, and graph bootstrap. In every case, the graph encoder embedding exhibits unrivalled computational advantages.},
  archive      = {J_TPAMI},
  author       = {Cencheng Shen and Qizhe Wang and Carey E. Priebe},
  doi          = {10.1109/TPAMI.2022.3225073},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7933-7938},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {One-hot graph encoder embedding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When age-invariant face recognition meets face age
synthesis: A multi-task learning framework and a new benchmark.
<em>TPAMI</em>, <em>45</em>(6), 7917–7932. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To minimize the impact of age variation on face recognition, age-invariant face recognition (AIFR) extracts identity-related discriminative features by minimizing the correlation between identity- and age-related features while face age synthesis (FAS) eliminates age variation by converting the faces in different age groups to the same group. However, AIFR lacks visual results for model interpretation and FAS compromises downstream recognition due to artifacts. Therefore, we propose a unified, multi-task framework to jointly handle these two tasks, termed MTLFace, which can learn the age-invariant identity-related representation for face recognition while achieving pleasing face synthesis for model interpretation. Specifically, we propose an attention-based feature decomposition to decompose the mixed face features into two uncorrelated components—identity- and age-related features—in a spatially constrained way. Unlike the conventional one-hot encoding that achieves group-level FAS, we propose a novel identity conditional module to achieve identity-level FAS, which can improve the age smoothness of synthesized faces through a weight-sharing strategy. Benefiting from the proposed multi-task framework, we then leverage those high-quality synthesized faces from FAS to further boost AIFR via a novel selective fine-tuning strategy. Furthermore, to advance both AIFR and FAS, we collect and release a large cross-age face dataset with age and gender annotations, and a new benchmark specifically designed for tracing long-missing children. Extensive experimental results on five benchmark cross-age datasets demonstrate that MTLFace yields superior performance than state-of-the-art methods for both AIFR and FAS. We further validate MTLFace on two popular general face recognition datasets, obtaining competitive performance on face recognition in the wild. The source code and datasets are available at http://hzzone.github.io/MTLFace .},
  archive      = {J_TPAMI},
  author       = {Zhizhong Huang and Junping Zhang and Hongming Shan},
  doi          = {10.1109/TPAMI.2022.3217882},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7917-7932},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {When age-invariant face recognition meets face age synthesis: A multi-task learning framework and a new benchmark},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VLT: Vision-language transformer and query generation for
referring segmentation. <em>TPAMI</em>, <em>45</em>(6), 7900–7916. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Vision-Language Transformer (VLT) framework for referring segmentation to facilitate deep interactions among multi-modal information and enhance the holistic understanding to vision-language features. There are different ways to understand the dynamic emphasis of a language expression, especially when interacting with the image. However, the learned queries in existing transformer works are fixed after training, which cannot cope with the randomness and huge diversity of the language expressions. To address this issue, we propose a Query Generation Module, which dynamically produces multiple sets of input-specific queries to represent the diverse comprehensions of language expression. To find the best among these diverse comprehensions, so as to generate a better mask, we propose a Query Balance Module to selectively fuse the corresponding responses of the set of queries. Furthermore, to enhance the model&#39;s ability in dealing with diverse language expressions, we consider inter-sample learning to explicitly endow the model with knowledge of understanding different language expressions to the same object. We introduce masked contrastive learning to narrow down the features of different expressions for the same target object while distinguishing the features of different objects. The proposed approach is lightweight and achieves new state-of-the-art referring segmentation results consistently on five datasets.},
  archive      = {J_TPAMI},
  author       = {Henghui Ding and Chang Liu and Suchen Wang and Xudong Jiang},
  doi          = {10.1109/TPAMI.2022.3217852},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7900-7916},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VLT: Vision-language transformer and query generation for referring segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised global and local homography estimation with
motion basis learning. <em>TPAMI</em>, <em>45</em>(6), 7885–7899. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new framework for unsupervised deep homography estimation. Our contributions are 3 folds. First, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. Second, considering a homography contains 8 Degree-of-Freedoms (DOFs) that is much less than the rank of the network features, we propose a Low Rank Representation (LRR) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. Last, we propose a Feature Identity Loss (FIL) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. With this constraint, the unsupervised optimization can be more effective and the learned features are more stable. With global-to-local homography flow refinement, we also naturally generalize the proposed method to local mesh-grid homography estimation, which can go beyond the constraint of a single homography. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark dataset both qualitatively and quantitatively. Code is available at https://github.com/megvii-research/BasesHomo .},
  archive      = {J_TPAMI},
  author       = {Shuaicheng Liu and Yuhang Lu and Hai Jiang and Nianjin Ye and Chuan Wang and Bing Zeng},
  doi          = {10.1109/TPAMI.2022.3223789},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7885-7899},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised global and local homography estimation with motion basis learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trifocal relative pose from lines at points. <em>TPAMI</em>,
<em>45</em>(6), 7870–7884. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for solving two minimal problems for relative camera pose estimation from three views, which are based on three view correspondences of ( i ) three points and one line and the novel case of ( ii ) three points and two lines through two of the points. These problems are too difficult to be efficiently solved by the state of the art Gröbner basis methods. Our method is based on a new efficient homotopy continuation (HC) solver framework MINUS, which dramatically speeds up previous HC solving by specializing hc methods to generic cases of our problems. We characterize their number of solutions and show with simulated experiments that our solvers are numerically robust and stable under image noise, a key contribution given the borderline intractable degree of nonlinearity of trinocular constraints. We show in real experiments that ( i ) sift feature location and orientation provide good enough point-and-line correspondences for three-view reconstruction and ( ii ) that we can solve difficult cases with too few or too noisy tentative matches, where the state of the art structure from motion initialization fails.},
  archive      = {J_TPAMI},
  author       = {Ricardo Fabbri and Timothy Duff and Hongyi Fan and Margaret Regan and David da Costa de Pinho and Elias Tsigaridas and Charles Wampler and Jonathan Hauenstein and Peter J. Giblin and Benjamin B. Kimia and Anton Leykin and Tomas Pajdla},
  doi          = {10.1109/TPAMI.2022.3226165},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7870-7884},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Trifocal relative pose from lines at points},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransVOD: End-to-end video object detection with
spatial-temporal transformers. <em>TPAMI</em>, <em>45</em>(6),
7853–7869. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD , the first end-to-end video object detection system based on simple yet effective spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of current VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3\%-4\% mAP) on the ImageNet VID dataset. TransVOD yields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0\% mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7\% mAP while running at around 30 FPS on a single V100 GPU device. Code and models are available at https://github.com/SJTU-LuHe/TransVOD .},
  archive      = {J_TPAMI},
  author       = {Qianyu Zhou and Xiangtai Li and Lu He and Yibo Yang and Guangliang Cheng and Yunhai Tong and Lizhuang Ma and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3223955},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7853-7869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransVOD: End-to-end video object detection with spatial-temporal transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transferring knowledge from text to video: Zero-shot
anticipation for procedural actions. <em>TPAMI</em>, <em>45</em>(6),
7836–7852. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we teach a robot to recognize and make predictions for activities that it has never seen before? We tackle this problem by learning models for video from text. This paper presents a hierarchical model that generalizes instructional knowledge from large-scale text corpora and transfers the knowledge to video. Given a portion of an instructional video, our model recognizes and predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the capabilities of our model, we introduce the Tasty Videos Dataset V2 , a collection of 4022 recipes for zero-shot learning, recognition and anticipation. Extensive experiments with various evaluation metrics demonstrate the potential of our method for generalization, given limited video data for training models.},
  archive      = {J_TPAMI},
  author       = {Fadime Sener and Rishabh Saraf and Angela Yao},
  doi          = {10.1109/TPAMI.2022.3218596},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7836-7852},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transferring knowledge from text to video: Zero-shot anticipation for procedural actions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransCenter: Transformers with dense representations for
multiple-object tracking. <em>TPAMI</em>, <em>45</em>(6), 7820–7835. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have proven superior performance for a wide variety of tasks since they were introduced. In recent years, they have drawn attention from the vision community in tasks such as image classification and object detection. Despite this wave, an accurate and efficient multiple-object tracking (MOT) method based on transformers is yet to be designed. We argue that the direct application of a transformer architecture with quadratic complexity and insufficient noise-initialized sparse queries – is not optimal for MOT. We propose TransCenter, a transformer-based MOT architecture with dense representations for accurately tracking all the objects while keeping a reasonable runtime. Methodologically, we propose the use of image-related dense detection queries and efficient sparse tracking queries produced by our carefully designed query learning networks (QLN). On one hand, the dense image-related detection queries allow us to infer targets’ locations globally and robustly through dense heatmap outputs. On the other hand, the set of sparse tracking queries efficiently interacts with image features in our TransCenter Decoder to associate object positions through time. As a result, TransCenterexhibits remarkable performance improvements and outperforms by a large margin the current state-of-the-art methods in two standard MOT benchmarks with two tracking settings (public/private). TransCenter is also proven efficient and accurate by an extensive ablation study and, comparisons to more naive alternatives and concurrent works. The code is made publicly available at https://github.com/yihongxu/transcenter .},
  archive      = {J_TPAMI},
  author       = {Yihong Xu and Yutong Ban and Guillaume Delorme and Chuang Gan and Daniela Rus and Xavier Alameda-Pineda},
  doi          = {10.1109/TPAMI.2022.3225078},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7820-7835},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransCenter: Transformers with dense representations for multiple-object tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The shape of learning curves: A review. <em>TPAMI</em>,
<em>45</em>(6), 7799–7819. (<a
href="https://doi.org/10.1109/TPAMI.2022.3220744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning curves provide insight into the dependence of a learner&#39;s generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
  archive      = {J_TPAMI},
  author       = {Tom Viering and Marco Loog},
  doi          = {10.1109/TPAMI.2022.3220744},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7799-7819},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The shape of learning curves: A review},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised anomaly detection via conditional generative
adversarial network and ensemble active learning. <em>TPAMI</em>,
<em>45</em>(6), 7781–7798. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has wide applications in machine intelligence but is still a difficult unsolved problem. Major challenges include the rarity of labeled anomalies and it is a class highly imbalanced problem. Traditional unsupervised anomaly detectors are suboptimal while supervised models can easily make biased predictions towards normal data. In this paper, we present a new supervised anomaly detector through introducing the novel Ensemble Active Learning Generative Adversarial Network (EAL-GAN). EAL-GAN is a conditional GAN having a unique one generator versus multiple discriminators architecture where anomaly detection is implemented by an auxiliary classifier of the discriminator. In addition to using the conditional GAN to generate class balanced supplementary training data, an innovative ensemble learning loss function ensuring each discriminator makes up for the deficiencies of the others is designed to overcome the class imbalanced problem, and an active learning algorithm is introduced to significantly reduce the cost of labeling real-world data. We present extensive experimental results to demonstrate that the new anomaly detector consistently outperforms a variety of SOTA methods by significant margins.},
  archive      = {J_TPAMI},
  author       = {Zhi Chen and Jiang Duan and Li Kang and Guoping Qiu},
  doi          = {10.1109/TPAMI.2022.3225476},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7781-7798},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Supervised anomaly detection via conditional generative adversarial network and ensemble active learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SuperFast: 200× video frame interpolation via event camera.
<em>TPAMI</em>, <em>45</em>(6), 7764–7780. (<a
href="https://doi.org/10.1109/TPAMI.2022.3224051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional frame-based video frame interpolation (VFI) methods rely on the linear motion assumption and brightness invariance assumption, which may lead to fatal errors confronting the scenarios with high-speed motions. To tackle the above challenge, inspired by the advantages of event cameras on asynchronously recording brightness changes at each pixel, we propose a Fast-Slow joint synthesis framework for event-enhanced high-speed video frame interpolation, named SuperFast , in this paper, which can generate high frame rate (5000 FPS, 200× faster) video from the input low frame rate (25 FPS) video and the corresponding event stream. In our framework, the task is divided into two sub-tasks, i.e., video frame interpolation for the contents with and without high-speed motions, which are tackled by two corresponding branches, i.e., the fast synthesis pathway and the slow synthesis pathway. The fast synthesis pathway leverages a spiking neural network to encode the input event stream, and combines boundary frames to generate intermediate results through synthesis and refinement, targeting on contents with high-speed motions. The slow synthesis pathway stacks the two input boundary frames and the event stream to synthesize intermediate results, focusing on relatively slow-motion contents. Finally, a fusion module with a comparison loss is utilized to generate the final video frame interpolation results. We also build a hybrid visual acquisition system containing an event camera and a high frame rate camera, and collect the first 5000 FPS H igh- S peed E vent-enhanced V ideo frame I nterpolation (THU $^{\text{HSEVI}}$ ) dataset. To evaluate the performance of our proposed framework, we have conducted experiments on our THU $^{\text{HSEVI}}$ dataset and the existing HS-ERGB dataset. Experimental results demonstrate that our proposed framework can achieve state-of-the-art 200× video frame interpolation performance under high-speed motion scenarios.},
  archive      = {J_TPAMI},
  author       = {Yue Gao and Siqi Li and Yipeng Li and Yandong Guo and Qionghai Dai},
  doi          = {10.1109/TPAMI.2022.3224051},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7764-7780},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SuperFast: 200× video frame interpolation via event camera},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure evolution on manifold for graph learning.
<em>TPAMI</em>, <em>45</em>(6), 7751–7763. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph has been widely used in various applications, while how to optimize the graph is still an open question. In this paper, we propose a framework to optimize the graph structure via structure evolution on graph manifold. We first define the graph manifold and search the best graph structure on this manifold. Concretely, associated with the data features and the prediction results of a given task, we define a graph energy to measure how the graph fits the graph manifold from an initial graph structure. The graph structure then evolves by minimizing the graph energy. In this process, the graph structure can be evolved on the graph manifold corresponding to the update of the prediction results. Alternatively iterating these two processes, both the graph structure and the prediction results can be updated until converge. It achieves the suitable structure for graph learning without searching all hyperparameters. To evaluate the performance of the proposed method, we have conducted experiments on eight datasets and compared with the recent state-of-the-art methods. Experiment results demonstrate that our method outperforms the state-of-the-art methods in both transductive and inductive settings.},
  archive      = {J_TPAMI},
  author       = {Hai Wan and Xinwei Zhang and Yubo Zhang and Xibin Zhao and Shihui Ying and Yue Gao},
  doi          = {10.1109/TPAMI.2022.3225572},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7751-7763},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structure evolution on manifold for graph learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State-regularized recurrent neural networks to extract
automata and explain predictions. <em>TPAMI</em>, <em>45</em>(6),
7739–7750. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks are a widely used class of neural architectures. They have, however, two shortcomings. First, they are often treated as black-box models and as such it is difficult to understand what exactly they learn as well as how they arrive at a particular prediction. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the purpose of automata extraction; (2) non-regular languages such as balanced parentheses and palindromes where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition and text categorisation. We show that state-regularization (a) simplifies the extraction of finite state automata that display an RNN&#39;s state transition dynamic; (b) forces RNNs to operate more like automata with external memory and less like finite state machines, which potentiality leads to a more structural memory; (c) leads to better interpretability and explainability of RNNs by leveraging the probabilistic finite state transition mechanism over time steps.},
  archive      = {J_TPAMI},
  author       = {Cheng Wang and Carolin Lawrence and Mathias Niepert},
  doi          = {10.1109/TPAMI.2022.3225334},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7739-7750},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {State-regularized recurrent neural networks to extract automata and explain predictions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-dense feature matching with transformers and its
applications in multiple-view geometry. <em>TPAMI</em>, <em>45</em>(6),
7726–7738. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. We further adapt LoFTR to modern SfM systems and illustrate its application in multiple-view geometry. The proposed method demonstrates superior performance in Image Matching Challenge 2021 and ranks first on two public benchmarks of visual localization among the published methods. The code is available at https://zju3dv.github.io/loftr .},
  archive      = {J_TPAMI},
  author       = {Zehong Shen and Jiaming Sun and Yuang Wang and Xingyi He and Hujun Bao and Xiaowei Zhou},
  doi          = {10.1109/TPAMI.2022.3223530},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7726-7738},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-dense feature matching with transformers and its applications in multiple-view geometry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic and relation modulation for audio-visual event
localization. <em>TPAMI</em>, <em>45</em>(6), 7711–7725. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of localizing audio-visual events that are both audible and visible in a video. Existing works focus on encoding and aligning audio and visual features at the segment level while neglecting informative correlation between segments of the two modalities and between multi-scale event proposals. We propose a novel Semantic and Relation Modulation Network (SRMN) to learn the above correlation and leverage it to modulate the related auditory, visual, and fused features. In particular, for semantic modulation, we propose intra-modal normalization and cross-modal normalization. The former modulates features of a single modality with the event-relevant semantic guidance of the same modality. The latter modulates features of two modalities by establishing and exploiting the cross-modal relationship. For relation modulation, we propose a multi-scale proposal modulating module and a multi-alignment segment modulating module to introduce multi-scale event proposals and enable dense matching between cross-modal segments, which strengthen correlations between successive segments within one proposal and between all segments. With the features modulated by the correlation information regarding audio-visual events, SRMN performs accurate event localization. Extensive experiments conducted on the public AVE dataset demonstrate that our method outperforms the state-of-the-art methods in both supervised event localization and cross-modality localization tasks.},
  archive      = {J_TPAMI},
  author       = {Hao Wang and Zheng-Jun Zha and Liang Li and Xuejin Chen and Jiebo Luo},
  doi          = {10.1109/TPAMI.2022.3226328},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7711-7725},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semantic and relation modulation for audio-visual event localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust point cloud segmentation with noisy annotations.
<em>TPAMI</em>, <em>45</em>(6), 7696–7710. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud segmentation is a fundamental task in 3D. Despite recent progress on point cloud segmentation with the power of deep networks, current learning methods based on the clean label assumptions may fail with noisy labels. Yet, class labels are often mislabeled at both instance-level and boundary-level in real-world datasets. In this work, we take the lead in solving the instance-level label noise by proposing a Point Noise-Adaptive Learning (PNAL) framework. Compared to noise-robust methods on image tasks, our framework is noise-rate blind, to cope with the spatially variant noise rate specific to point clouds. Specifically, we propose a point-wise confidence selection to obtain reliable labels from the historical predictions of each point. A cluster-wise label correction is proposed with a voting strategy to generate the best possible label by considering the neighbor correlations. To handle boundary-level label noise, we also propose a variant “PNAL-boundary ” with a progressive boundary label cleaning strategy. Extensive experiments demonstrate its effectiveness on both synthetic and real-world noisy datasets. Even with $60\%$ symmetric noise and high-level boundary noise, our framework significantly outperforms its baselines, and is comparable to the upper bound trained on completely clean data. Moreover, we cleaned the popular real-world dataset ScanNetV2 for rigorous experiment. Our code and data is available at https://github.com/pleaseconnectwifi/PNAL .},
  archive      = {J_TPAMI},
  author       = {Shuquan Ye and Dongdong Chen and Songfang Han and Jing Liao},
  doi          = {10.1109/TPAMI.2022.3225323},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7696-7710},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust point cloud segmentation with noisy annotations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Reward-adaptive reinforcement learning: Dynamic policy
gradient optimization for bipedal locomotion. <em>TPAMI</em>,
<em>45</em>(6), 7686–7695. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling a non-statically bipedal robot is challenging due to the complex dynamics and multi-criterion optimization involved. Recent works have demonstrated the effectiveness of deep reinforcement learning (DRL) for simulation and physical robots. In these methods, the rewards from different criteria are normally summed to learn a scalar function. However, a scalar is less informative and may be insufficient to derive effective information for each reward channel from the complex hybrid rewards. In this work, we propose a novel reward-adaptive reinforcement learning method for biped locomotion, allowing the control policy to be simultaneously optimized by multiple criteria using a dynamic mechanism. The proposed method applies a multi-head critic to learn a separate value function for each reward component, leading to hybrid policy gradients. We further propose dynamic weight, allowing each component to optimize the policy with different priorities. This hybrid and dynamic policy gradient (HDPG) design makes the agent learn more efficiently. We show that the proposed method outperforms summed-up-reward approaches and is able to transfer to physical robots. The MuJoCo results further demonstrate the effectiveness and generalization of HDPG.},
  archive      = {J_TPAMI},
  author       = {Changxin Huang and Guangrun Wang and Zhibo Zhou and Ronghui Zhang and Liang Lin},
  doi          = {10.1109/TPAMI.2022.3223407},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7686-7695},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reward-adaptive reinforcement learning: Dynamic policy gradient optimization for bipedal locomotion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking label flipping attack: From sample masking to
sample thresholding. <em>TPAMI</em>, <em>45</em>(6), 7668–7685. (<a
href="https://doi.org/10.1109/TPAMI.2022.3220849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, machine learning (ML) and deep learning (DL) methods have become fundamental building blocks for a wide range of AI applications. The popularity of these methods also makes them widely exposed to malicious attacks, which may cause severe security concerns. To understand the security properties of the ML/DL methods, researchers have recently started to turn their focus to adversarial attack algorithms that could successfully corrupt the model or clean data owned by the victim with imperceptible perturbations. In this paper, we study the Label Flipping Attack (LFA) problem, where the attacker expects to corrupt an ML/DL model&#39;s performance by flipping a small fraction of the labels in the training data. Prior art along this direction adopts combinatorial optimization problems, leading to limited scalability toward deep learning models. To this end, we propose a novel minimax problem which provides an efficient reformulation of the sample selection process in LFA. In the new optimization problem, the sample selection operation could be implemented with a single thresholding parameter. This leads to a novel training algorithm called Sample Thresholding . Since the objective function is differentiable and the model complexity does not depend on the sample size, we can apply Sample Thresholding to attack deep learning models. Moreover, since the victim&#39;s behavior is not predictable in a poisonous attack setting, we have to employ surrogate models to simulate the true model employed by the victim model. Seeing the problem, we provide a theoretical analysis of such a surrogate paradigm. Specifically, we show that the performance gap between the true model employed by the victim and the surrogate model is small under mild conditions. On top of this paradigm, we extend Sample Thresholding to the crowdsourced ranking task, where labels collected from the annotators are vulnerable to adversarial attacks. Finally, experimental analyses on three real-world datasets speak to the efficacy of our method.},
  archive      = {J_TPAMI},
  author       = {Qianqian Xu and Zhiyong Yang and Yunrui Zhao and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3220849},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7668-7685},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rethinking label flipping attack: From sample masking to sample thresholding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Referring segmentation via encoder-fused cross-modal
attention network. <em>TPAMI</em>, <em>45</em>(6), 7654–7667. (<a
href="https://doi.org/10.1109/TPAMI.2022.3221387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on referring segmentation, which aims to selectively segment the corresponding visual region in an image (or video) according to the referring expression. However, the existing methods usually consider the interaction between multi-modal features at the decoding end of the network. Specifically, they interact the visual features of each scale with language respectively, thus ignoring the correlation between multi-scale features. In this work, we present an encoder fusion network (EFN), which transfers the multi-modal feature learning process from the decoding end to the encoding end and realizes the gradual refinement of multi-modal features by the language. In EFN, we also adopt a co-attention mechanism to promote the mutual alignment of language and visual information in feature space. In the decoding stage, a boundary enhancement module (BEM) is proposed to enhance the network&#39;s attention to the details of the target. For video data, we introduce an asymmetric cross-frame attention module (ACFM) to effectively capture the temporal information from the video frames by computing the relationship between each pixel of the current frame and each pooled sub-region of the reference frames. Extensive experiments on referring image/video segmentation datasets show that our method outperforms the state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Guang Feng and Lihe Zhang and Jiayu Sun and Zhiwei Hu and Huchuan Lu},
  doi          = {10.1109/TPAMI.2022.3221387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7654-7667},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Referring segmentation via encoder-fused cross-modal attention network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PatchMix augmentation to identify causal features in
few-shot learning. <em>TPAMI</em>, <em>45</em>(6), 7639–7653. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of Few-shot learning (FSL) aims to transfer the knowledge learned from base categories with sufficient labelled data to novel categories with scarce known information. It is currently an important research question and has great practical values in the real-world applications. Despite extensive previous efforts are made on few-shot learning tasks, we emphasize that most existing methods did not take into account the distributional shift caused by sample selection bias in the FSL scenario. Such a selection bias can induce spurious correlation between the semantic causal features, that are causally and semantically related to the class label, and the other non-causal features. Critically, the former ones should be invariant across changes in distributions, highly related to the classes of interest, and thus well generalizable to novel classes, while the latter ones are not stable to changes in the distribution. To resolve this problem, we propose a novel data augmentation strategy dubbed as PatchMix that can break this spurious dependency by replacing the patch-level information and supervision of the query images with random gallery images from different classes from the query ones. We theoretically show that such an augmentation mechanism, different from existing ones, is able to identify the causal features. To further make these features to be discriminative enough for classification, we propose Correlation-guided Reconstruction (CGR) and Hardness-Aware module for instance discrimination and easier discrimination between similar classes. Moreover, such a framework can be adapted to the unsupervised FSL scenario. The utility of our method is demonstrated on the state-of-the-art results consistently achieved on several benchmarks including mini ImageNet, tiered ImageNet, CIFAR-FS, CUB, Cars, Places and Plantae, in all settings of single-domain, cross-domain and unsupervised FSL. By studying the intra-variance property of learned features and visualizing the learned features, we further quantitatively and qualitatively show that such a promising result is due to the effectiveness in learning causal features.},
  archive      = {J_TPAMI},
  author       = {Chengming Xu and Chen Liu and Xinwei Sun and Siqian Yang and Yabiao Wang and Chengjie Wang and Yanwei Fu},
  doi          = {10.1109/TPAMI.2022.3223784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7639-7653},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PatchMix augmentation to identify causal features in few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimising for interpretability: Convolutional dynamic
alignment networks. <em>TPAMI</em>, <em>45</em>(6), 7625–7638. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. As a result, CoDA Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly, CoDA Nets can be combined with conventional neural network models to yield powerful classifiers that more easily scale to complex datasets such as Imagenet whilst exhibiting an increased interpretable depth , i.e., the output can be explained well in terms of contributions from intermediate layers within the network.},
  archive      = {J_TPAMI},
  author       = {Moritz Böhle and Mario Fritz and Bernt Schiele},
  doi          = {10.1109/TPAMI.2022.3226041},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7625-7638},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimising for interpretability: Convolutional dynamic alignment networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeX360: Real-time all-around view synthesis with neural
basis expansion. <em>TPAMI</em>, <em>45</em>(6), 7611–7624. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present NeX, a new approach to novel view synthesis based on enhancements of multiplane images (MPI) that can reproduce view-dependent effects in real time. Unlike traditional MPI, our technique parameterizes each pixel as a linear combination of spherical basis functions learned from a neural network to model view-dependent effects and uses a hybrid implicit-explicit modeling strategy to improve fine detail. Moreover, we also present an extension to NeX, which leverages knowledge distillation to train multiple MPIs for unbounded 360 $^\circ$ scenes. Our method is evaluated on several benchmark datasets: NeRF-Synthetic dataset, Light Field dataset, Real Forward-Facing dataset, Space dataset, as well as Shiny , our new dataset that contains significantly more challenging view-dependent effects, such as the rainbow reflections on the CD. Our method outperforms other real-time rendering approaches on PSNR, SSIM, and LPIPS and can render unbounded 360 $^\circ$ scenes in real time.},
  archive      = {J_TPAMI},
  author       = {Pakkapon Phongthawee and Suttisak Wizadwongsa and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
  doi          = {10.1109/TPAMI.2022.3217957},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7611-7624},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NeX360: Real-time all-around view synthesis with neural basis expansion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Neural architecture search via proxy validation.
<em>TPAMI</em>, <em>45</em>(6), 7595–7610. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper searches for the optimal neural architecture by minimizing a proxy of validation loss. Existing neural architecture search (NAS) methods used to discover the optimal neural architecture that best fits the validation examples given the up-to-date network weights. These intermediate validation results are invaluable but have not been fully explored. We propose to approximate the validation loss landscape by learning a mapping from neural architectures to their corresponding validate losses. The optimal neural architecture thus can be easily identified as the minimum of this proxy validation loss landscape. To improve the efficiency, a novel architecture sampling strategy is developed for the approximation of the proxy validation loss landscape. We also propose an operation importance weight (OIW) to balance the randomness and certainty of architecture sampling. The representation of neural architecture is learned through a graph autoencoder (GAE) over both architectures sampled during search and randomly generated architectures. We provide theoretical analyses on the validation loss estimator learned with our sampling strategy. Experimental results demonstrate that the proposed proxy validation loss landscape can be effective in both the differentiable NAS and the evolutionary-algorithm-based (EA-based) NAS.},
  archive      = {J_TPAMI},
  author       = {Yanxi Li and Minjing Dong and Yunhe Wang and Chang Xu},
  doi          = {10.1109/TPAMI.2022.3217648},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7595-7610},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural architecture search via proxy validation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual-assistance learning for standalone mono-modality
survival analysis of human cancers. <em>TPAMI</em>, <em>45</em>(6),
7577–7594. (<a
href="https://doi.org/10.1109/TPAMI.2022.3222732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current survival analysis of cancers confronts two key issues. While comprehensive perspectives provided by data from multiple modalities often promote the performance of survival models, data with inadequate modalities at the testing phase are more ubiquitous in clinical scenarios, which makes multi-modality approaches not applicable. Additionally, incomplete observations (i.e., censored instances) bring a unique challenge for survival analysis, to tackle which, some models have been proposed based on certain strict assumptions or attribute distributions that, however, may limit their applicability. In this paper, we present a mutual-assistance learning paradigm for standalone mono-modality survival analysis of cancers. The mutual assistance implies the cooperation of multiple components and embodies three aspects: 1) it leverages the knowledge of multi-modality data to guide the representation learning of an individual modality via mutual-assistance similarity and geometry constraints; 2) it formulates mutual-assistance regression and ranking functions independent of strong hypotheses to estimate the relative risk, in which a bias vector is introduced to efficiently cope with the censoring problem; 3) it integrates representation learning and survival modeling into a unified mutual-assistance framework for alleviating the requirement of attribute distributions. Extensive experiments on several datasets demonstrate our method can significantly improve the performance of mono-modality survival model.},
  archive      = {J_TPAMI},
  author       = {Zhenyuan Ning and Zhangxin Zhao and Qianjin Feng and Wufan Chen and Qing Xiao and Yu Zhang},
  doi          = {10.1109/TPAMI.2022.3222732},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7577-7594},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mutual-assistance learning for standalone mono-modality survival analysis of human cancers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-granularity anchor-contrastive representation learning
for semi-supervised skeleton-based action recognition. <em>TPAMI</em>,
<em>45</em>(6), 7559–7576. (<a
href="https://doi.org/10.1109/TPAMI.2022.3222871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the semi-supervised skeleton-based action recognition task, obtaining more discriminative information from both labeled and unlabeled data is a challenging problem. As the current mainstream approach, contrastive learning can learn more representations of augmented data, which can be considered as the pretext task of action recognition. However, such a method still confronts three main limitations: 1) It usually learns global-granularity features that cannot well reflect the local motion information. 2) The positive/negative pairs are usually pre-defined, some of which are ambiguous. 3) It generally measures the distance between positive/negative pairs only within the same granularity, which neglects the contrasting between the cross-granularity positive and negative pairs. Toward these limitations, we propose a novel Multi-granularity Anchor-Contrastive representation Learning (dubbed as MAC-Learning) to learn multi-granularity representations by conducting inter- and intra-granularity contrastive pretext tasks on the learnable and structural-link skeletons among three types of granularities covering local, context, and global views. To avoid the disturbance of ambiguous pairs from noisy and outlier samples, we design a more reliable Multi-granularity Anchor-Contrastive Loss (dubbed as MAC-Loss) that measures the agreement/disagreement between high-confidence soft-positive/negative pairs based on the anchor graph instead of the hard-positive/negative pairs in the conventional contrastive loss. Extensive experiments on both NTU RGB+D and Northwestern-UCLA datasets show that the proposed MAC-Learning outperforms existing competitive methods in semi-supervised skeleton-based action recognition tasks.},
  archive      = {J_TPAMI},
  author       = {Xiangbo Shu and Binqian Xu and Liyan Zhang and Jinhui Tang},
  doi          = {10.1109/TPAMI.2022.3222871},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7559-7576},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-granularity anchor-contrastive representation learning for semi-supervised skeleton-based action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Micro-supervised disturbance learning: A perspective of
representation probability distribution. <em>TPAMI</em>, <em>45</em>(6),
7542–7558. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The instability is shown in the existing methods of representation learning based on Euclidean distance under a broad set of conditions. Furthermore, the scarcity and high cost of labels prompt us to explore more expressive representation learning methods which depends on as few labels as possible. To address above issues, the small-perturbation ideology is firstly introduced on the representation learning model based on the representation probability distribution. The positive small-perturbation information (SPI) which only depend on two labels of each cluster is used to stimulate the representation probability distribution and then two variant models are proposed to fine-tune the expected representation distribution of Restricted Boltzmann Machine (RBM), namely, Micro-supervised Disturbance Gaussian-binary RBM (Micro-DGRBM) and Micro-supervised Disturbance RBM (Micro-DRBM) models. The Kullback-Leibler (KL) divergence of SPI is minimized in the same cluster to promote the representation probability distributions to become more similar in Contrastive Divergence (CD) learning. In contrast, the KL divergence of SPI is maximized in the different clusters to enforce the representation probability distributions to become more dissimilar in CD learning. To explore the representation learning capability under the continuous stimulation of the SPI, we present a deep Micro-supervised Disturbance Learning (Micro-DL) framework based on the Micro-DGRBM and Micro-DRBM models and compare it with a similar deep structure which has no external stimulation. Experimental results demonstrate that the proposed deep Micro-DL architecture shows better performance in comparison to the baseline method, the most related shallow models and deep frameworks for clustering.},
  archive      = {J_TPAMI},
  author       = {Jielei Chu and Jing Liu and Hongjun Wang and Hua Meng and Zhiguo Gong and Tianrui Li},
  doi          = {10.1109/TPAMI.2022.3225461},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7542-7558},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Micro-supervised disturbance learning: A perspective of representation probability distribution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning view-based graph convolutional network for
multi-view 3D shape analysis. <em>TPAMI</em>, <em>45</em>(6), 7525–7541.
(<a href="https://doi.org/10.1109/TPAMI.2022.3221785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View-based approach that recognizes 3D shape through its projected 2D images has achieved state-of-the-art results for 3D shape recognition. The major challenges are how to aggregate multi-view features and deal with 3D shapes in arbitrary poses. We propose two versions of a novel view-based Graph Convolutional Network, dubbed view-GCN and view-GCN++, to recognize 3D shape based on graph representation of multiple views. We first construct view-graph with multiple views as graph nodes, then design two graph convolutional networks over the view-graph to hierarchically learn discriminative shape descriptor considering relations of multiple views. Specifically, view-GCN is a hierarchical network based on two pivotal operations, i.e., feature transform based on local positional and non-local graph convolution, and graph coarsening based on a selective view-sampling operation. To deal with rotation sensitivity, we further propose view-GCN++ with local attentional graph convolution operation and rotation robust view-sampling operation for graph coarsening. By these designs, view-GCN++ achieves invariance to transformations under the finite subgroup of rotation group SO(3). Extensive experiments on benchmark datasets (i.e., ModelNet40, ScanObjectNN, RGBD and ShapeNet Core55) show that view-GCN and view-GCN++ achieve state-of-the-art results for 3D shape classification and retrieval tasks under aligned and rotated settings.},
  archive      = {J_TPAMI},
  author       = {Xin Wei and Ruixuan Yu and Jian Sun},
  doi          = {10.1109/TPAMI.2022.3221785},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7525-7541},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning view-based graph convolutional network for multi-view 3D shape analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning representation for clustering via prototype
scattering and positive sampling. <em>TPAMI</em>, <em>45</em>(6),
7509–7524. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep clustering methods rely on either contrastive or non-contrastive representation learning for downstream clustering task. Contrastive-based methods thanks to negative pairs learn uniform representations for clustering, in which negative pairs, however, may inevitably lead to the class collision issue and consequently compromise the clustering performance. Non-contrastive-based methods, on the other hand, avoid class collision issue, but the resulting non-uniform representations may cause the collapse of clustering. To enjoy the strengths of both worlds, this paper presents a novel end-to-end deep clustering method with prototype scattering and positive sampling, termed ProPos. Specifically, we first maximize the distance between prototypical representations, named prototype scattering loss, which improves the uniformity of representations. Second, we align one augmented view of instance with the sampled neighbors of another view—assumed to be truly positive pair in the embedding space—to improve the within-cluster compactness, termed positive sampling alignment. The strengths of ProPos are avoidable class collision issue, uniform representations, well-separated clusters, and within-cluster compactness. By optimizing ProPos in an end-to-end expectation-maximization framework, extensive experimental results demonstrate that ProPos achieves competing performance on moderate-scale clustering benchmark datasets and establishes new state-of-the-art performance on large-scale datasets. Source code is available at https://github.com/Hzzone/ProPos .},
  archive      = {J_TPAMI},
  author       = {Zhizhong Huang and Jie Chen and Junping Zhang and Hongming Shan},
  doi          = {10.1109/TPAMI.2022.3216454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7509-7524},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning representation for clustering via prototype scattering and positive sampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning invariance from generated variance for unsupervised
person re-identification. <em>TPAMI</em>, <em>45</em>(6), 7494–7508. (<a
href="https://doi.org/10.1109/TPAMI.2022.3226866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on unsupervised representation learning in person re-identification (ReID). Recent self-supervised contrastive learning methods learn invariance by maximizing the representation similarity between two augmented views of a same image. However, traditional data augmentation may bring to the fore undesirable distortions on identity features, which is not always favorable in id-sensitive ReID tasks. In this article, we propose to replace traditional data augmentation with a generative adversarial network (GAN) that is targeted to generate augmented views for contrastive learning. A 3D mesh guided person image generator is proposed to disentangle a person image into id-related and id-unrelated features. Deviating from previous GAN-based ReID methods that only work in id-unrelated space (pose and camera style), we conduct GAN-based augmentation on both id-unrelated and id-related features. We further propose specific contrastive losses to help our network learn invariance from id-unrelated and id-related augmentations. By jointly training the generative and the contrastive modules, our method achieves new state-of-the-art unsupervised person ReID performance on mainstream large-scale benchmarks.},
  archive      = {J_TPAMI},
  author       = {Hao Chen and Yaohui Wang and Benoit Lagadec and Antitza Dantcheva and Francois Bremond},
  doi          = {10.1109/TPAMI.2022.3226866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7494-7508},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning invariance from generated variance for unsupervised person re-identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning by seeing more classes. <em>TPAMI</em>,
<em>45</em>(6), 7477–7493. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional pattern recognition models usually assume a fixed and identical number of classes during both training and inference stages. In this paper, we study an interesting but ignored question: can increasing the number of classes during training improve the generalization and reliability performance? For a $k$ -class problem, instead of training with only these $k$ classes, we propose to learn with $k+m$ classes, where the additional $m$ classes can be either real classes from other datasets or synthesized from known classes. Specifically, we propose two strategies for constructing new classes from known classes. By making the model see more classes during training, we can obtain several advantages. First, the added $m$ classes serve as a regularization which is helpful to improve the generalization accuracy on the original $k$ classes. Second, this will alleviate the overconfident phenomenon and produce more reliable confidence estimation for different tasks like misclassification detection, confidence calibration, and out-of-distribution detection. Lastly, the additional classes can also improve the learned feature representation, which is beneficial for new classes generalization in few-shot learning and class-incremental learning. Compared with the widely proved concept of data augmentation (dataAug), our method is driven from another dimension of augmentation based on additional classes (classAug). Comprehensive experiments demonstrated the superiority of our classAug under various open-environment metrics on benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Fei Zhu and Xu-Yao Zhang and Rui-Qi Wang and Cheng-Lin Liu},
  doi          = {10.1109/TPAMI.2022.3225117},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7477-7493},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning by seeing more classes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale unsupervised semantic segmentation.
<em>TPAMI</em>, <em>45</em>(6), 7457–7476. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowered by large datasets, e.g., ImageNet and MS COCO, unsupervised learning on large-scale data has enabled significant advances for classification tasks. However, whether the large-scale unsupervised semantic segmentation can be achieved remains unknown. There are two major challenges: i) we need a large-scale benchmark for assessing algorithms; ii) we need to develop methods to simultaneously learn category and shape representation in an unsupervised manner. In this work, we propose a new problem of l arge-scale u nsupervised s emantic s egmentation (LUSS) with a newly created benchmark dataset to help the research progress. Building on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS. The benchmark and source code is publicly available at https://github.com/LUSSeg .},
  archive      = {J_TPAMI},
  author       = {Shanghua Gao and Zhong-Yu Li and Ming-Hsuan Yang and Ming-Ming Cheng and Junwei Han and Philip Torr},
  doi          = {10.1109/TPAMI.2022.3218275},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7457-7476},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large-scale unsupervised semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic image transfer for illumination manipulation.
<em>TPAMI</em>, <em>45</em>(6), 7444–7456. (<a
href="https://doi.org/10.1109/TPAMI.2022.3224253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel intrinsic image transfer (IIT) algorithm for image illumination manipulation, which creates a local image translation between two illumination surfaces. This model is built on an optimization-based framework composed of illumination, reflectance and content photo-realistic losses, respectively. Each loss is first defined on the corresponding sub-layers factorized by an intrinsic image decomposition and then reduced under the well-known spatial-varying illumination illumination-invariant reflectance prior knowledge. We illustrate that all losses, with the aid of an “exemplar” image, can be directly defined on images without the necessity of taking an intrinsic image decomposition, thereby giving a closed-form solution to image illumination manipulation. We also demonstrate its versatility and benefits to several illumination-related tasks: illumination compensation, image enhancement and tone mapping, and high dynamic range (HDR) image compression, and show their high-quality results on natural image datasets.},
  archive      = {J_TPAMI},
  author       = {Junqing Huang and Michael Ruzhansky and Qianying Zhang and Haihui Wang},
  doi          = {10.1109/TPAMI.2022.3224253},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7444-7456},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Intrinsic image transfer for illumination manipulation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable by design: Learning predictors by composing
interpretable queries. <em>TPAMI</em>, <em>45</em>(6), 7430–7443. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing concern about typically opaque decision-making with high-performance machine learning algorithms. Providing an explanation of the reasoning process in domain-specific terms can be crucial for adoption in risk-sensitive domains such as healthcare. We argue that machine learning algorithms should be interpretable by design and that the language in which these interpretations are expressed should be domain- and task-dependent. Consequently, we base our model&#39;s prediction on a family of user-defined and task-specific binary functions of the data, each having a clear interpretation to the end-user. We then minimize the expected number of queries needed for accurate prediction on any given input. As the solution is generally intractable, following prior work, we choose the queries sequentially based on information gain. However, in contrast to previous work, we need not assume the queries are conditionally independent. Instead, we leverage a stochastic generative model (VAE) and an MCMC algorithm (Unadjusted Langevin) to select the most informative query about the input based on previous query-answers. This enables the online determination of a query chain of whatever depth is required to resolve prediction ambiguities. Finally, experiments on vision and NLP tasks demonstrate the efficacy of our approach and its superiority over post-hoc explanations.},
  archive      = {J_TPAMI},
  author       = {Aditya Chattopadhyay and Stewart Slocum and Benjamin D. Haeffele and René Vidal and Donald Geman},
  doi          = {10.1109/TPAMI.2022.3225162},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7430-7443},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpretable by design: Learning predictors by composing interpretable queries},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human collective intelligence inspired multi-view
representation learning — enabling view communication by simulating
human communication mechanism. <em>TPAMI</em>, <em>45</em>(6),
7412–7429. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, we often encounter multi-view learning tasks where we need to learn from multiple sources of data or use multiple sources of data to make decisions. Multi-view representation learning, which can learn a unified representation from multiple data sources, is a key pre-task of multi-view learning and plays a significant role in real-world applications. Accordingly, how to improve the performance of multi-view representation learning is an important issue. In this work, inspired by human collective intelligence shown in group decision making, we introduce the concept of view communication into multi-view representation learning. Furthermore, by simulating human communication mechanism, we propose a novel multi-view representation learning approach that can fulfill multi-round view communication. Thus, each view of our approach can exploit the complementary information from other views to help with modeling its own representation, and mutual help between views is achieved. Extensive experiment results on six datasets from three significant fields indicate that our approach substantially improves the average classification accuracy by 4.536\% in medicine and bioinformatics fields as well as 4.115\% in machine learning field.},
  archive      = {J_TPAMI},
  author       = {Xiaodong Jia and Xiao-Yuan Jing and Qixing Sun and Songcan Chen and Bo Du and David Zhang},
  doi          = {10.1109/TPAMI.2022.3218605},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7412-7429},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human collective intelligence inspired multi-view representation learning — enabling view communication by simulating human communication mechanism},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GH-feat: Learning versatile generative hierarchical features
from GANs. <em>TPAMI</em>, <em>45</em>(6), 7395–7411. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years witness the tremendous success of generative adversarial networks (GANs) in synthesizing photo-realistic images. GAN generator learns to compose realistic images and reproduce the real data distribution. Through that, a hierarchical visual feature with multi-level semantics spontaneously emerges. In this work we investigate that such a generative feature learned from image synthesis exhibits great potentials in solving a wide range of computer vision tasks, including both generative ones and more importantly discriminative ones. We first train an encoder by considering the pre-trained StyleGAN generator as a learned loss function. The visual features produced by our encoder, termed as Generative Hierarchical Features (GH-Feat) , highly align with the layer-wise GAN representations, and hence describe the input image adequately from the reconstruction perspective. Extensive experiments support the versatile transferability of GH-Feat across a range of applications, such as image editing, image processing, image harmonization, face verification, landmark detection, layout prediction, image retrieval, etc. We further show that, through a proper spatial expansion, our developed GH-Feat can also facilitate fine-grained semantic segmentation using only a few annotations. Both qualitative and quantitative results demonstrate the appealing performance of GH-Feat. Code and models are available at https://genforce.github.io/ghfeat/ .},
  archive      = {J_TPAMI},
  author       = {Yinghao Xu and Yujun Shen and Jiapeng Zhu and Ceyuan Yang and Bolei Zhou},
  doi          = {10.1109/TPAMI.2022.3225788},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7395-7411},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GH-feat: Learning versatile generative hierarchical features from GANs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry regularized autoencoders. <em>TPAMI</em>,
<em>45</em>(6), 7381–7394. (<a
href="https://doi.org/10.1109/TPAMI.2022.3222104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental task in data exploration is to extract low dimensional representations that capture intrinsic geometry in data, especially for faithfully visualizing data in two or three dimensions. Common approaches use kernel methods for manifold learning. However, these methods typically only provide an embedding of the input data and cannot extend naturally to new data points. Autoencoders have also become popular for representation learning. While they naturally compute feature extractors that are extendable to new data and invertible (i.e., reconstructing original features from latent representation), they often fail at representing the intrinsic data geometry compared to kernel-based manifold learning. We present a new method for integrating both approaches by incorporating a geometric regularization term in the bottleneck of the autoencoder. This regularization encourages the learned latent representation to follow the intrinsic data geometry, similar to manifold learning algorithms, while still enabling faithful extension to new data and preserving invertibility. We compare our approach to autoencoder models for manifold learning to provide qualitative and quantitative evidence of our advantages in preserving intrinsic structure, out of sample extension, and reconstruction. Our method is easily implemented for big-data applications, whereas other methods are limited in this regard.},
  archive      = {J_TPAMI},
  author       = {Andres F. Duque and Sacha Morin and Guy Wolf and Kevin R. Moon},
  doi          = {10.1109/TPAMI.2022.3222104},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7381-7394},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geometry regularized autoencoders},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fast differentiable matrix square root and inverse square
root. <em>TPAMI</em>, <em>45</em>(6), 7367–7380. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the matrix square root and its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient enough in either the forward pass or the backward pass. In this paper, we propose two more efficient variants to compute the differentiable matrix square root and the inverse square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Padé Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation using the matrix sign function. A series of numerical tests show that both methods yield considerable speed-up compared with the SVD or the NS iteration. Moreover, we validate the effectiveness of our methods in several real-world applications, including de-correlated batch normalization, second-order vision transformer, global covariance pooling for large-scale and fine-grained recognition, attentive covariance pooling for video recognition, and neural style transfer. The experiments demonstrate that our methods can also achieve competitive and even slightly better performances. Code is available at https://github.com/KingJamesSong/FastDifferentiableMatSqrt .},
  archive      = {J_TPAMI},
  author       = {Yue Song and Nicu Sebe and Wei Wang},
  doi          = {10.1109/TPAMI.2022.3216339},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7367-7380},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast differentiable matrix square root and inverse square root},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic time warping based adversarial framework for
time-series domain. <em>TPAMI</em>, <em>45</em>(6), 7353–7366. (<a
href="https://doi.org/10.1109/TPAMI.2022.3224754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rapid progress on research in adversarial robustness of deep neural networks (DNNs), there is little principled work for the time-series domain. Since time-series data arises in diverse applications including mobile health, finance, and smart grid, it is important to verify and improve the robustness of DNNs for the time-series domain. In this paper, we propose a novel framework for the time-series domain referred as Dynamic Time Warping for Adversarial Robustness (DTW-AR) using the dynamic time warping measure. Theoretical and empirical evidence is provided to demonstrate the effectiveness of DTW over the standard euclidean distance metric employed in prior methods for the image domain. We develop a principled algorithm justified by theoretical analysis to efficiently create diverse adversarial examples using random alignment paths. Experiments on diverse real-world benchmarks show the effectiveness of DTW-AR to fool DNNs for time-series data and to improve their robustness using adversarial training.},
  archive      = {J_TPAMI},
  author       = {Taha Belkhouja and Yan Yan and Janardhan Rao Doppa},
  doi          = {10.1109/TPAMI.2022.3224754},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7353-7366},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic time warping based adversarial framework for time-series domain},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual instance-consistent network for cross-domain object
detection. <em>TPAMI</em>, <em>45</em>(6), 7338–7352. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain object detection aims to transfer knowledge from a labeled dataset to an unlabeled dataset. Most existing methods apply a unified embedding model to generate the tightly coupled source and target descriptions for domain alignment, leading to the destroyed feature distribution of the target domain because the embedding model is mainly controlled by the source domain. To reduce the representation bias of the target domain, we apply two independent networks to extract two types of discriminative descriptions with mutual consistency, i.e., a novel Dual Instance-Consistent Network (DICN) is proposed for cross-domain object detection. Especially, Dual Instance-Consistent Module containing the instance mutual consistency between Primary Network and Auxiliary Network is applied to align two domains, where Primary and Auxiliary Networks are used to obtain the source-specific and target-specific information, respectively. The instance mutual consistency consists of two terms: feature consistency and detection consistency, which is applied to align the instance feature and the output of detection head, respectively. With the instance mutual consistency, optimizing the Primary (Auxiliary) Network only with source (target) images by fixing the Auxiliary (Primary) Network can generate the source(target)-specific description. Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed DICN, e.g., obtaining mAP of 44.10\% for Cityscapes $\rightarrow$ Foggy Cityscapes, AP on car of 76.50\% for Cityscapes $\rightarrow$ KITTI, MR $^{-2}$ of 8.87\%, 12.66\%, 22.27\%, and 42.06\% for COCOPersons $\rightarrow$ Caltech, CityPersons $\rightarrow$ Caltech, COCOPersons $\rightarrow$ CityPersons, and Caltech $\rightarrow$ CityPersons, respectively.},
  archive      = {J_TPAMI},
  author       = {Yifan Jiao and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TPAMI.2022.3218569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7338-7352},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual instance-consistent network for cross-domain object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DMRNet++: Learning discriminative features with decoupled
networks and enriched pairs for one-step person search. <em>TPAMI</em>,
<em>45</em>(6), 7319–7337. (<a
href="https://doi.org/10.1109/TPAMI.2022.3221079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search aims at localizing and recognizing query persons from raw video frames, which is a combination of two sub-tasks, i.e., pedestrian detection and person re-identification. The dominant fashion is termed as the one-step person search that jointly optimizes detection and identification in a unified network, exhibiting higher efficiency. However, there remain major challenges: (i) conflicting objectives of multiple sub-tasks under the shared feature space, (ii) inconsistent memory bank caused by the limited batch size, (iii) underutilized unlabeled identities during the identification learning. To address these issues, we develop an enhanced d ecoupled and m emory- r einforced network (DMRNet++). First, we simplify the standard tightly coupled pipelines and establish a task-decoupled framework (TDF). Second, we build a memory-reinforced mechanism (MRM), with a slow-moving average of the network to better encode the consistency of the memorized features. Third, considering the potential of unlabeled samples, we model the recognition process as semi-supervised learning. An unlabeled-aided contrastive loss (UCL) is developed to boost the identification feature learning by exploiting the aggregation of unlabeled identities. Experimentally, the proposed DMRNet++ obtains the mAP of 94.5\% and 52.1\% on CUHK-SYSU and PRW datasets, which exceeds most existing methods.},
  archive      = {J_TPAMI},
  author       = {Chuchu Han and Zhedong Zheng and Kai Su and Dongdong Yu and Zehuan Yuan and Changxin Gao and Nong Sang and Yi Yang},
  doi          = {10.1109/TPAMI.2022.3221079},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7319-7337},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DMRNet++: Learning discriminative features with decoupled networks and enriched pairs for one-step person search},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private graph neural networks for whole-graph
classification. <em>TPAMI</em>, <em>45</em>(6), 7308–7318. (<a
href="https://doi.org/10.1109/TPAMI.2022.3228315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have established themselves as state-of-the-art for many machine learning applications such as the analysis of social and medical networks. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce a framework for differential private graph-level classification. Our method is applicable to graph deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification, as well as the scalability of the method on a large medical dataset. Our experiments show that DP-SGD can be applied to graph classification tasks with reasonable utility losses. Furthermore, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings. Our results can also function as robust baselines for future work in this area.},
  archive      = {J_TPAMI},
  author       = {Tamara T. Mueller and Johannes C. Paetzold and Chinmay Prabhakar and Dmitrii Usynin and Daniel Rueckert and Georgios Kaissis},
  doi          = {10.1109/TPAMI.2022.3228315},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7308-7318},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentially private graph neural networks for whole-graph classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable hierarchical optimal transport for robust
multi-view learning. <em>TPAMI</em>, <em>45</em>(6), 7293–7307. (<a
href="https://doi.org/10.1109/TPAMI.2022.3222569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional multi-view learning methods often rely on two assumptions: ( $i$ ) the samples in different views are well-aligned, and ( $ii$ ) their representations obey the same distribution in a latent space. Unfortunately, these two assumptions may be questionable in practice, which limits the application of multi-view learning. In this work, we propose a differentiable hierarchical optimal transport (DHOT) method to mitigate the dependency of multi-view learning on these two assumptions. Given arbitrary two views of unaligned multi-view data, the DHOT method calculates the sliced Wasserstein distance between their latent distributions. Based on these sliced Wasserstein distances, the DHOT method further calculates the entropic optimal transport across different views and explicitly indicates the clustering structure of the views. Accordingly, the entropic optimal transport, together with the underlying sliced Wasserstein distances, leads to a hierarchical optimal transport distance defined for unaligned multi-view data, which works as the objective function of multi-view learning and leads to a bi-level optimization task. Moreover, our DHOT method treats the entropic optimal transport as a differentiable operator of model parameters. It considers the gradient of the entropic optimal transport in the backpropagation step and thus helps improve the descent direction for the model in the training phase. We demonstrate the superiority of our bi-level optimization strategy by comparing it to the traditional alternating optimization strategy. The DHOT method is applicable for both unsupervised and semi-supervised learning. Experimental results show that our DHOT method is at least comparable to state-of-the-art multi-view learning methods on both synthetic and real-world tasks, especially for challenging scenarios with unaligned multi-view data.},
  archive      = {J_TPAMI},
  author       = {Dixin Luo and Hongteng Xu and Lawrence Carin},
  doi          = {10.1109/TPAMI.2022.3222569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7293-7307},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentiable hierarchical optimal transport for robust multi-view learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for instance retrieval: A survey.
<em>TPAMI</em>, <em>45</em>(6), 7270–7292. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years a vast amount of visual content has been generated and shared from many fields, such as social media platforms, medical imaging, and robotics. This abundance of content creation and sharing has introduced new challenges, particularly that of searching databases for similar content — Content Based Image Retrieval (CBIR) — a long-established research area in which improved efficiency and accuracy are needed for real-time retrieval. Artificial intelligence has made progress in CBIR and has significantly facilitated the process of instance search. In this survey we review recent instance retrieval works that are developed based on deep learning algorithms and techniques, with the survey organized by deep feature extraction, feature embedding and aggregation methods, and network fine-tuning strategies. Our survey considers a wide variety of recent methods, whereby we identify milestone work, reveal connections among various methods and present the commonly used benchmarks, evaluation results, common challenges, and propose promising future directions.},
  archive      = {J_TPAMI},
  author       = {Wei Chen and Yu Liu and Weiping Wang and Erwin M. Bakker and Theodoros Georgiou and Paul Fieguth and Li Liu and Michael S. Lew},
  doi          = {10.1109/TPAMI.2022.3218591},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7270-7292},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning for instance retrieval: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curriculum-based asymmetric multi-task reinforcement
learning. <em>TPAMI</em>, <em>45</em>(6), 7258–7269. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce CAMRL, the first curriculum-based asymmetric multi-task learning (AMTL) algorithm for dealing with multiple reinforcement learning (RL) tasks altogether. To mitigate the negative influence of customizing the one-off training order in curriculum-based AMTL, CAMRL switches its training mode between parallel single-task RL and asymmetric multi-task RL (MTRL), according to an indicator regarding the training time, the overall performance, and the performance gap among tasks. To leverage the multi-sourced prior knowledge flexibly and to reduce negative transfer in AMTL, we customize a composite loss with multiple differentiable ranking functions and optimize the loss through alternating optimization and the Frank-Wolfe algorithm. The uncertainty-based automatic adjustment of hyper-parameters is also applied to eliminate the need of laborious hyper-parameter analysis during optimization. By optimizing the composite loss, CAMRL predicts the next training task and continuously revisits the transfer matrix and network weights. We have conducted experiments on a wide range of benchmarks in multi-task RL, covering Gym-minigrid, Meta-world, Atari video games, vision-based PyBullet tasks, and RLBench, to show the improvements of CAMRL over the corresponding single-task RL algorithm and state-of-the-art MTRL algorithms. The code is available at: https://github.com/huanghanchi/CAMRL .},
  archive      = {J_TPAMI},
  author       = {Hanchi Huang and Deheng Ye and Li Shen and Wei Liu},
  doi          = {10.1109/TPAMI.2022.3223872},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7258-7269},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Curriculum-based asymmetric multi-task reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive positive sample propagation along the
audio-visual event line. <em>TPAMI</em>, <em>45</em>(6), 7239–7257. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual and audio signals often coexist in natural environments, forming audio-visual events (AVEs). Given a video, we aim to localize video segments containing an AVE and identify its category. It is pivotal to learn the discriminative features for each video segment. Unlike existing work focusing on audio-visual feature fusion, in this paper, we propose a new contrastive positive sample propagation (CPSP) method for better deep feature representation learning. The contribution of CPSP is to introduce the available full or weak label as a prior that constructs the exact positive-negative samples for contrastive learning. Specifically, the CPSP involves comprehensive contrastive constraints: pair-level positive sample propagation (PSP), segment-level and video-level positive sample activation (PSA $_{S}$ and PSA $_{V}$ ). Three new contrastive objectives are proposed (i.e., $\mathcal {L}_{\text{avpsp}}$ , $\mathcal {L}_ \text{spsa}$ , and $\mathcal {L}_\text{vpsa}$ ) and introduced into both the fully and weakly supervised AVE localization. To draw a complete picture of the contrastive learning in AVE localization, we also study the self-supervised positive sample propagation (SSPSP). As a result, CPSP is more helpful to obtain the refined audio-visual features that are distinguishable from the negatives, thus benefiting the classifier prediction. Extensive experiments on the AVE and the newly collected VGGSound-AVEL100k datasets verify the effectiveness and generalization ability of our method.},
  archive      = {J_TPAMI},
  author       = {Jinxing Zhou and Dan Guo and Meng Wang},
  doi          = {10.1109/TPAMI.2022.3223688},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7239-7257},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive positive sample propagation along the audio-visual event line},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive bayesian analysis for deep metric learning.
<em>TPAMI</em>, <em>45</em>(6), 7220–7238. (<a
href="https://doi.org/10.1109/TPAMI.2022.3221486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods for deep metric learning have been focusing on designing different contrastive loss functions between positive and negative pairs of samples so that the learned feature embedding is able to pull positive samples of the same class closer and push negative samples from different classes away from each other. In this work, we recognize that there is a significant semantic gap between features at the intermediate feature layer and class labels at the final output layer. To bridge this gap, we develop a contrastive Bayesian analysis to characterize and model the posterior probabilities of image labels conditioned by their features similarity in a contrastive learning setting. This contrastive Bayesian analysis leads to a new loss function for deep metric learning. To improve the generalization capability of the proposed method onto new classes, we further extend the contrastive Bayesian loss with a metric variance constraint. Our experimental results and ablation studies demonstrate that the proposed contrastive Bayesian metric learning method significantly improves the performance of deep metric learning in both supervised and pseudo-supervised scenarios, outperforming existing methods by a large margin.},
  archive      = {J_TPAMI},
  author       = {Shichao Kan and Zhiquan He and Yigang Cen and Yang Li and Vladimir Mladenovic and Zhihai He},
  doi          = {10.1109/TPAMI.2022.3221486},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7220-7238},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive bayesian analysis for deep metric learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional wasserstein generator. <em>TPAMI</em>,
<em>45</em>(6), 7208–7219. (<a
href="https://doi.org/10.1109/TPAMI.2022.3220965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical distance of conditional distributions is an essential element of generating target data given some data as in video prediction. We establish how the statistical distances between two joint distributions are related to those between two conditional distributions for three popular statistical distances: f-divergence, Wasserstein distance, and integral probability metrics. Such characterization plays a crucial role in deriving a tractable form of the objective function to learn a conditional generator. For Wasserstein distance, we show that the distance between joint distributions is an upper bound of the expected distance between conditional distributions, and derive a tractable representation of the upper bound. Based on this theoretical result, we propose a new conditional generator, the conditional Wasserstein generator. Our proposed algorithm can be viewed as an extension of Wasserstein autoencoders (Tolstikhin et al. 2018) to conditional generation or as a Wasserstein counterpart of stochastic video generation (SVG) model by Denton and Fergus (Denton et al. 2018). We apply our algorithm to video prediction and video interpolation. Our experiments demonstrate that the proposed algorithm performs well on benchmark video datasets and produces sharper videos than state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Young-geun Kim and Kyungbok Lee and Myunghee Cho Paik},
  doi          = {10.1109/TPAMI.2022.3220965},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7208-7219},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Conditional wasserstein generator},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cell multi-bernoulli (cell-MB) sensor control for
multi-object search-while-tracking (SWT). <em>TPAMI</em>,
<em>45</em>(6), 7195–7207. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information-driven control can be used to develop intelligent sensors that can optimize their measurement value based on environmental feedback. In object tracking applications, sensor actions are chosen based on the expected reduction in uncertainty also known as information gain. Random finite set (RFS) theory provides a formalism for quantifying and estimating information gain in multi-object tracking problems. However, estimating information gain in these applications remains computationally challenging. This paper presents a new tractable approximation of the RFS expected information gain applicable to sensor control for multi-object search and tracking. Unlike existing RFS approaches, the information gain approximation presented in this paper considers the contributions of non-ideal noisy measurements, missed detections, false alarms, and object appearance/disappearance. The effectiveness of the information-driven sensor control is demonstrated through two multi-vehicle search-while-tracking experiments using real video data from remote terrestrial and satellite sensors.},
  archive      = {J_TPAMI},
  author       = {Keith A. LeGrand and Pingping Zhu and Silvia Ferrari},
  doi          = {10.1109/TPAMI.2022.3223856},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7195-7207},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cell multi-bernoulli (Cell-MB) sensor control for multi-object search-while-tracking (SWT)},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CATs++: Boosting cost aggregation with convolutions and
transformers. <em>TPAMI</em>, <em>45</em>(6), 7174–7194. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost aggregation is a process in image matching tasks that aims to disambiguate the noisy matching scores. Existing methods generally tackle this by hand-crafted or CNN-based methods, which either lack robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields and inadaptability. In this paper, we introduce Cost Aggregation with Transformers (CATs) to tackle this by exploring global consensus among initial correlation map with the help of some architectural designs that allow us to benefit from global receptive fields of self-attention mechanism. To this end, we include appearance affinity modeling, which helps to disambiguate the noisy initial correlation maps. Furthermore, we introduce some techniques, including multi-level aggregation to exploit rich semantics prevalent at different feature levels and swapping self-attention to obtain reciprocal matching scores to act as a regularization. Although CATs can attain competitive performance, it may face some limitations, i.e., high computational costs, which may restrict its applicability only at limited resolution and hurt performance. To overcome this, we propose CATs++, an extension of CATs. Concretely, we introduce early convolutions prior to cost aggregation with a transformer to control the number of tokens and inject some convolutional inductive bias, then propose a novel transformer architecture for both efficient and effective cost aggregation, which results in apparent performance boost and cost reduction. With the reduced costs, we are able to compose our network with a hierarchical structure to process higher-resolution inputs. We show that the proposed method with these integrated outperforms the previous state-of-the-art methods by large margins. Codes and pretrained weights are available at: https://ku-cvlab.github.io/CATs-PlusPlus-Project-Page/ .},
  archive      = {J_TPAMI},
  author       = {Seokju Cho and Sunghwan Hong and Seungryong Kim},
  doi          = {10.1109/TPAMI.2022.3218727},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7174-7194},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CATs++: Boosting cost aggregation with convolutions and transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AlphaPose: Whole-body regional multi-person pose estimation
and tracking in real-time. <em>TPAMI</em>, <em>45</em>(6), 7157–7173.
(<a href="https://doi.org/10.1109/TPAMI.2022.3222784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate whole-body multi-person pose estimation and tracking is an important yet challenging topic in computer vision. To capture the subtle actions of humans for complex behavior analysis, whole-body pose estimation including the face, body, hand and foot is essential over conventional body-only pose estimation. In this article, we present AlphaPose, a system that can perform accurate whole-body pose estimation and tracking jointly while running in realtime. To this end, we propose several new techniques: Symmetric Integral Keypoint Regression (SIKR) for fast and fine localization, Parametric Pose Non-Maximum-Suppression (P-NMS) for eliminating redundant human detections and Pose Aware Identity Embedding for jointly pose estimation and tracking. During training, we resort to Part-Guided Proposal Generator (PGPG) and multi-domain knowledge distillation to further improve the accuracy. Our method is able to localize whole-body keypoints accurately and tracks humans simultaneously given inaccurate bounding boxes and redundant detections. We show a significant improvement over current state-of-the-art methods in both speed and accuracy on COCO-wholebody, COCO, PoseTrack, and our proposed Halpe-FullBody pose estimation dataset. Our model, source codes and dataset are made publicly available at https://github.com/MVIG-SJTU/AlphaPose.},
  archive      = {J_TPAMI},
  author       = {Hao-Shu Fang and Jiefeng Li and Hongyang Tang and Chao Xu and Haoyi Zhu and Yuliang Xiu and Yong-Lu Li and Cewu Lu},
  doi          = {10.1109/TPAMI.2022.3222784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7157-7173},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AlphaPose: Whole-body regional multi-person pose estimation and tracking in real-time},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive transfer kernel learning for transfer gaussian
process regression. <em>TPAMI</em>, <em>45</em>(6), 7142–7156. (<a
href="https://doi.org/10.1109/TPAMI.2022.3219121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer regression is a practical and challenging problem with important applications in various domains, such as engineering design and localization. Capturing the relatedness of different domains is the key of adaptive knowledge transfer. In this paper, we investigate an effective way of explicitly modelling domain relatedness through transfer kernel, a transfer-specified kernel that considers domain information in the covariance calculation. Specifically, we first give the formal definition of transfer kernel, and introduce three basic general forms that well cover existing related works. To cope with the limitations of the basic forms in handling complex real-world data, we further propose two advanced forms. Corresponding instantiations of the two forms are developed, namely ${Trk}_{\alpha \beta }$ and ${Trk}_{\omega }$ based on multiple kernel learning and neural networks, respectively. For each instantiation, we present a condition with which the positive semi-definiteness is guaranteed and a semantic meaning is interpreted to the learned domain relatedness. Moreover, the condition can be easily used in the learning of TrGP $_{\alpha \beta }$ and TrGP $_{\omega }$ that are the Gaussian process models with the transfer kernels ${Trk}_{\alpha \beta }$ and ${Trk}_{\omega }$ respectively. Extensive empirical studies show the effectiveness of TrGP $_{\alpha \beta }$ and TrGP $_{\omega }$ on domain relatedness modelling and transfer adaptiveness.},
  archive      = {J_TPAMI},
  author       = {Pengfei Wei and Yiping Ke and Yew-Soon Ong and Zejun Ma},
  doi          = {10.1109/TPAMI.2022.3219121},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7142-7156},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive transfer kernel learning for transfer gaussian process regression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ABINet++: Autonomous, bidirectional and iterative language
modeling for scene text spotting. <em>TPAMI</em>, <em>45</em>(6),
7123–7141. (<a
href="https://doi.org/10.1109/TPAMI.2022.3223908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text spotting is of great importance to the computer vision community due to its wide variety of applications. Recent methods attempt to introduce linguistic knowledge for challenging recognition rather than pure visual classification. However, how to effectively model the linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited capacity of language models comes from 1) implicit language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we propose an autonomous, bidirectional and iterative ABINet++ for scene text spotting. First, the autonomous suggests enforcing explicitly language modeling by decoupling the recognizer into vision model and language model and blocking gradient flow between both models. Second, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Third, we propose an execution manner of iterative correction for the language model which can effectively alleviate the impact of noise input. Additionally, based on an ensemble of the iterative predictions, a self-training method is developed which can learn from unlabeled images effectively. Finally, to polish ABINet++ in long text recognition, we propose to aggregate horizontal features by embedding Transformer units inside a U-Net, and design a position and content attention module which integrates character order and content to attend to character features precisely. ABINet++ achieves state-of-the-art performance on both scene text recognition and scene text spotting benchmarks, which consistently demonstrates the superiority of our method in various environments especially on low-quality images. Besides, extensive experiments including in English and Chinese also prove that, a text spotter that incorporates our language modeling method can significantly improve its performance both in accuracy and speed compared with commonly used attention-based recognizers. Code is available at https://github.com/FangShancheng/ABINet-PP .},
  archive      = {J_TPAMI},
  author       = {Shancheng Fang and Zhendong Mao and Hongtao Xie and Yuxin Wang and Chenggang Yan and Yongdong Zhang},
  doi          = {10.1109/TPAMI.2022.3223908},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7123-7141},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ABINet++: Autonomous, bidirectional and iterative language modeling for scene text spotting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on deep learning technique for video segmentation.
<em>TPAMI</em>, <em>45</em>(6), 7099–7122. (<a
href="https://doi.org/10.1109/TPAMI.2022.3225573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video segmentation—partitioning video frames into multiple segments or objects—plays a critical role in a broad range of practical applications, from enhancing visual effects in movie, to understanding scenes in autonomous driving, to creating virtual background in video conferencing. Recently, with the renaissance of connectionism in computer vision, there has been an influx of deep learning based approaches for video segmentation that have delivered compelling performance. In this survey, we comprehensively review two basic lines of research — generic object segmentation (of unknown categories) in videos, and video semantic segmentation — by introducing their respective task settings, background concepts, perceived need, development history, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out open issues in this field, and suggest opportunities for further research. We also provide a public website to continuously track developments in this fast advancing field: https://github.com/tfzhou/VS-Survey .},
  archive      = {J_TPAMI},
  author       = {Tianfei Zhou and Fatih Porikli and David J. Crandall and Luc Van Gool and Wenguan Wang},
  doi          = {10.1109/TPAMI.2022.3225573},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7099-7122},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey on deep learning technique for video segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A differentiable perspective for multi-view spectral
clustering with flexible extension. <em>TPAMI</em>, <em>45</em>(6),
7087–7098. (<a
href="https://doi.org/10.1109/TPAMI.2022.3224978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims to discover common patterns from multi-source data, whose generality is remarkable. Compared with traditional methods, deep learning methods are data-driven and have a larger search space for solutions, which may find a better solution to the problem. In addition, more considerations can be introduced by loss functions, so deep models are highly reusable. However, compared with deep learning methods, traditional methods have better interpretability, whose optimization is relatively stable. In this paper, we propose a multi-view spectral clustering model, combining the advantages of traditional methods and deep learning methods. Specifically, we start with the objective function of traditional spectral clustering, perform multi-view extension, and then obtain the traditional optimization process. By partially parameterizing this process, we further design corresponding differentiable modules, and finally construct a complete network structure. The model is interpretable and extensible to a certain extent. Experiments show that the model performs better than other multi-view clustering algorithms, and its semi-supervised classification extension also has excellent performance compared to other algorithms. Further experiments also show the stability and fewer iterations of the model training.},
  archive      = {J_TPAMI},
  author       = {Zhoumin Lu and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3224978},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7087-7098},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A differentiable perspective for multi-view spectral clustering with flexible extension},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-embedding of nodes and edges with graph neural networks.
<em>TPAMI</em>, <em>45</em>(6), 7075–7086. (<a
href="https://doi.org/10.1109/TPAMI.2020.3029762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph, as an important data representation, is ubiquitous in many real world applications ranging from social network analysis to biology. How to correctly and effectively learn and extract information from graph is essential for a large number of machine learning tasks. Graph embedding is a way to transform and encode the data structure in high dimensional and non-euclidean feature space to a low dimensional and structural space, which is easily exploited by other machine learning algorithms. We have witnessed a huge surge of such embedding methods, from statistical approaches to recent deep learning methods such as the graph convolutional networks (GCN). Deep learning approaches usually outperform the traditional methods in most graph learning benchmarks by building an end-to-end learning framework to optimize the loss function directly. However, most of the existing GCN methods can only perform convolution operations with node features, while ignoring the handy information in edge features, such as relations in knowledge graphs. To address this problem, we present CensNet , C onvolution with E dge- N ode S witching graph neural network, for learning tasks in graph-structured data with both node and edge features. CensNet is a general graph embedding framework, which embeds both nodes and edges to a latent feature space. By using line graph of the original undirected graph, the role of nodes and edges are switched, and two novel graph convolution operations are proposed for feature propagation. Experimental results on real-world academic citation networks and quantum chemistry graphs show that our approach achieves or matches the state-of-the-art performance in four graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction.},
  archive      = {J_TPAMI},
  author       = {Xiaodong Jiang and Ronghang Zhu and Pengsheng Ji and Sheng Li},
  doi          = {10.1109/TPAMI.2020.3029762},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7075-7086},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Co-embedding of nodes and edges with graph neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fourier-based and rational graph filters for spectral
processing. <em>TPAMI</em>, <em>45</em>(6), 7063–7074. (<a
href="https://doi.org/10.1109/TPAMI.2022.3177075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data are represented as graphs in a wide range of applications, such as Computer Vision (e.g., images) and Graphics (e.g., 3D meshes), network analysis (e.g., social networks), and bio-informatics (e.g., molecules). In this context, our overall goal is the definition of novel Fourier-based and graph filters induced by rational polynomials for graph processing, which generalise polynomial filters and the Fourier transform to non-euclidean domains. For the efficient evaluation of discrete spectral Fourier-based and wavelet operators, we introduce a spectrum-free approach, which requires the solution of a small set of sparse, symmetric, well-conditioned linear systems and is oblivious of the evaluation of the Laplacian or kernel spectrum. Approximating arbitrary graph filters with rational polynomials provides a more accurate and numerically stable alternative with respect to polynomials. To achieve these goals, we also study the link between spectral operators, wavelets, and filtered convolution with integral operators induced by spectral kernels. According to our tests, main advantages of the proposed approach are (i) its generality with respect to the input data (e.g., graphs, 3D shapes), applications (e.g., signal reconstruction and smoothing, shape correspondence), and filters (e.g., polynomial, rational polynomial), and (ii) a spectrum-free computation with a generally low computational cost and storage overhead.},
  archive      = {J_TPAMI},
  author       = {Giuseppe Patanè},
  doi          = {10.1109/TPAMI.2022.3177075},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7063-7074},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fourier-based and rational graph filters for spectral processing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LogicENN: A neural based knowledge graphs embedding model
with logical rules. <em>TPAMI</em>, <em>45</em>(6), 7050–7062. (<a
href="https://doi.org/10.1109/TPAMI.2021.3121646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding models have gained significant attention in AI research. The aim of knowledge graph embedding is to embed the graphs into a vector space in which the structure of the graph is preserved. Recent works have shown that the inclusion of background knowledge, such as logical rules, can improve the performance of embeddings in downstream machine learning tasks. However, so far, most existing models do not allow the inclusion of rules. We address the challenge of including rules and present a new neural based embedding model (LogicENN). We prove that LogicENN can learn every ground truth of encoded rules in a knowledge graph. To the best of our knowledge, this has not been proved so far for the neural based family of embedding models. Moreover, we derive formulae for the inclusion of various rules, including (anti-)symmetric, inverse, irreflexive and transitive, implication, composition, equivalence and negation. Our formulation allows to avoid grounding for implication and equivalence relations. Our experiments show that LogicENN outperforms the existing models in link prediction.},
  archive      = {J_TPAMI},
  author       = {Mojtaba Nayyeri and Chengjin Xu and Mirza Mohtashim Alam and Jens Lehmann and Hamed Shariat Yazdi},
  doi          = {10.1109/TPAMI.2021.3121646},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7050-7062},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LogicENN: A neural based knowledge graphs embedding model with logical rules},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured knowledge distillation for dense prediction.
<em>TPAMI</em>, <em>45</em>(6), 7035–7049. (<a
href="https://doi.org/10.1109/TPAMI.2020.3001940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider transferring the structure information from large networks to compact ones for dense prediction tasks in computer vision. Previous knowledge distillation strategies used for dense prediction tasks often directly borrow the distillation scheme for image classification and perform knowledge distillation for each pixel separately , leading to sub-optimal performance. Here we propose to distill structured knowledge from large networks to compact networks, taking into account the fact that dense prediction is a structured prediction problem. Specifically, we study two structured distillation schemes: i ) pair-wise distillation that distills the pair-wise similarities by building a static graph; and ii ) holistic distillation that uses adversarial training to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by experiments on three dense prediction tasks: semantic segmentation, depth estimation and object detection. Code is available at https://git.io/StructKD .},
  archive      = {J_TPAMI},
  author       = {Yifan Liu and Changyong Shu and Jingdong Wang and Chunhua Shen},
  doi          = {10.1109/TPAMI.2020.3001940},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7035-7049},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structured knowledge distillation for dense prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fashion retrieval via graph reasoning networks on a
similarity pyramid. <em>TPAMI</em>, <em>45</em>(6), 7019–7034. (<a
href="https://doi.org/10.1109/TPAMI.2020.3025062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching clothing images from customers and online shopping stores has rich applications in e-commerce. Existing algorithms mostly encode an image as a global feature vector and perform retrieval via global representation matching. However, distinctive local information on clothing is immersed in this global representation, resulting in sub-optimized performance. To address this issue, we propose a novel graph reasoning network (GRNet) on a similarity pyramid, which learns similarities between a query and a gallery cloth by using both initial pairwise multi-scale feature representations and matching propagation for unaligned representations. The query local representations at each scale are aligned with those of the gallery via an adaptive window pooling module. The similarity pyramid is represented by a similarity graph, where nodes represent similarities between clothing components at different scales, and the final matching score is obtained by message propagation along edges. In GRNet, graph reasoning is solved by training a graph convolutional network, enabling the alignment of salient clothing components to improve clothing retrieval. To facilitate future research, we introduce a new benchmark, i.e. FindFashion, containing rich annotations of bounding boxes, views, occlusions, and cropping. Extensive experiments show that GRNet obtains new state-of-the-art results on three challenging benchmarks, e.g. pushing the accuracy of top-1, top-20, and top-50 on DeepFashion to 27, 66, and 75 percent (i.e. 6, 12, and 10 percent absolute improvements), outperforming competitors with large margins. On FindFashion, GRNet achieves considerable improvements on all empirical settings.},
  archive      = {J_TPAMI},
  author       = {Yiming Gao and Zhanghui Kuang and Guanbin Li and Ping Luo and Yimin Chen and Liang Lin and Wayne Zhang},
  doi          = {10.1109/TPAMI.2020.3025062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7019-7034},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fashion retrieval via graph reasoning networks on a similarity pyramid},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning multi-attention context graph for group-based
re-identification. <em>TPAMI</em>, <em>45</em>(6), 7001–7018. (<a
href="https://doi.org/10.1109/TPAMI.2020.3032542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to re-identify or retrieve a group of people across non-overlapped camera systems has important applications in video surveillance. However, most existing methods focus on (single) person re-identification (re-id), ignoring the fact that people often walk in groups in real scenarios. In this work, we take a step further and consider employing context information for identifying groups of people, i.e., group re-id. On the one hand, group re-id is more challenging than single person re-id, since it requires both a robust modeling of local individual person appearance (with different illumination conditions, pose/viewpoint variations, and occlusions), as well as full awareness of global group structures (with group layout and group member variations). On the other hand, we believe that person re-id can be greatly enhanced by incorporating additional visual context from neighboring group members, a task which we formulate as group-aware (single) person re-id. In this paper, we propose a novel unified framework based on graph neural networks to simultaneously address the above two group-based re-id tasks, i.e., group re-id and group-aware person re-id. Specifically, we construct a context graph with group members as its nodes to exploit dependencies among different people. A multi-level attention mechanism is developed to formulate both intra-group and inter-group context, with an additional self-attention module for robust graph-level representations by attentively aggregating node-level features. The proposed model can be directly generalized to tackle group-aware person re-id using node-level representations. Meanwhile, to facilitate the deployment of deep learning models on these tasks, we build a new group re-id dataset which contains more than $3.8K$ images with $1.5K$ annotated groups, an order of magnitude larger than existing group re-id datasets. Extensive experiments on the novel dataset as well as three existing datasets clearly demonstrate the effectiveness of the proposed framework for both group-based re-id tasks.},
  archive      = {J_TPAMI},
  author       = {Yichao Yan and Jie Qin and Bingbing Ni and Jiaxin Chen and Li Liu and Fan Zhu and Wei-Shi Zheng and Xiaokang Yang and Ling Shao},
  doi          = {10.1109/TPAMI.2020.3032542},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {7001-7018},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning multi-attention context graph for group-based re-identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Combinatorial learning of robust deep graph matching: An
embedding based approach. <em>TPAMI</em>, <em>45</em>(6), 6984–7000. (<a
href="https://doi.org/10.1109/TPAMI.2020.3005590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching aims to establish node correspondence between two graphs, which has been a fundamental problem for its NP-hard nature. One practical consideration is the effective modeling of the affinity function in the presence of noise, such that the mathematically optimal matching result is also physically meaningful. This paper resorts to deep neural networks to learn the node and edge feature, as well as the affinity model for graph matching in an end-to-end fashion. The learning is supervised by combinatorial permutation loss over nodes. Specifically, the parameters belong to convolutional neural networks for image feature extraction, graph neural networks for node embedding that convert the structural (beyond second-order) information into node-wise features that leads to a linear assignment problem, as well as the affinity kernel between two graphs. Our approach enjoys flexibility in that the permutation loss is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network can deal with varying numbers of nodes for both training and inference. Moreover, our network is class-agnostic. Experimental results on extensive benchmarks show its state-of-the-art performance. It bears some generalization capability across categories and datasets, and is capable for robust matching against outliers.},
  archive      = {J_TPAMI},
  author       = {Runzhong Wang and Junchi Yan and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2020.3005590},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6984-7000},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Combinatorial learning of robust deep graph matching: An embedding based approach},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning graph convolutional networks for multi-label
recognition and applications. <em>TPAMI</em>, <em>45</em>(6), 6969–6983.
(<a href="https://doi.org/10.1109/TPAMI.2021.3063496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important information, we propose graph convolutional networks (GCNs) based models for multi-label image recognition, where directed graphs are constructed over classes and information is propagated between classes to learn inter-dependent class-level representations. Following this idea, we design two particular models that approach multi-label classification from different views. In our first model, the prior knowledge about the class dependencies is integrated into classifier learning. Specifically, we propose Classifier Learning GCN (C-GCN) to map class-level semantic representations (e.g., word embeddings) into classifiers that maintain the inter-class topology. In our second model, we decompose the visual representation of an image into a set of label-aware features and propose prediction learning GCN (P-GCN) to encode such features into inter-dependent image-level prediction scores. Furthermore, we also present an effective correlation matrix construction approach to capture inter-class relationships and consequently guide information propagation among classes. Empirical results on generic multi-label image recognition demonstrate that both of the proposed models can obviously outperform other existing state-of-the-arts. Moreover, the proposed methods also show advantages in some other multi-label classification related applications.},
  archive      = {J_TPAMI},
  author       = {Zhao-Min Chen and Xiu-Shen Wei and Peng Wang and Yanwen Guo},
  doi          = {10.1109/TPAMI.2021.3063496},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6969-6983},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning graph convolutional networks for multi-label recognition and applications},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HiGCIN: Hierarchical graph-based cross inference network for
group activity recognition. <em>TPAMI</em>, <em>45</em>(6), 6955–6968.
(<a href="https://doi.org/10.1109/TPAMI.2020.3034233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group activity recognition (GAR) is a challenging task aimed at recognizing the behavior of a group of people. It is a complex inference process in which visual cues collected from individuals are integrated into the final prediction, being aware of the interaction between them. This paper goes one step further beyond the existing approaches by designing a Hierarchical Graph-based Cross Inference Network (HiGCIN), in which three levels of information, i.e., the body-region level, person level, and group-activity level, are constructed, learned, and inferred in an end-to-end manner. Primarily, we present a generic Cross Inference Block (CIB), which is able to concurrently capture the latent spatiotemporal dependencies among body regions and persons. Based on the CIB, two modules are designed to extract and refine features for group activities at each level. Experiments on two popular benchmarks verify the effectiveness of our approach, particularly in the ability to infer with multilevel visual cues. In addition, training our approach does not require individual action labels to be provided, which greatly reduces the amount of labor required in data annotation.},
  archive      = {J_TPAMI},
  author       = {Rui Yan and Lingxi Xie and Jinhui Tang and Xiangbo Shu and Qi Tian},
  doi          = {10.1109/TPAMI.2020.3034233},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6955-6968},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HiGCIN: Hierarchical graph-based cross inference network for group activity recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning multi-view interactional skeleton graph for action
recognition. <em>TPAMI</em>, <em>45</em>(6), 6940–6954. (<a
href="https://doi.org/10.1109/TPAMI.2020.3032738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the interactions of human articulations lies in the center of skeleton-based action recognition. Recent graph-based methods are inherently limited in the weak spatial context modeling capability due to fixed interaction pattern and inflexible shared weights of GCN. To address above problems, we propose the multi-view interactional graph network (MV-IGNet) which can construct, learn and infer multi-level spatial skeleton context, including view-level (global), group-level, joint-level (local) context, in a unified way. MV-IGNet leverages different skeleton topologies as multi-views to cooperatively generate complementary action features. For each view, separable parametric graph convolution (SPG-Conv) enables multiple parameterized graphs to enrich local interaction patterns, which provides strong graph-adaption ability to handle irregular skeleton topologies. We also partition the skeleton into several groups and then the higher-level group contexts including inter-group and intra-group, are hierarchically captured by above SPG-Conv layers. A simple yet effective global context adaption (GCA) module facilitates representative feature extraction by learning the input-dependent skeleton topologies. Compared to the mainstream works, MV-IGNet can be readily implemented while with smaller model size and faster inference. Experimental results show the proposed MV-IGNet achieves impressive performance on large-scale benchmarks: NTU-RGB+D and NTU-RGB+D 120.},
  archive      = {J_TPAMI},
  author       = {Minsi Wang and Bingbing Ni and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2020.3032738},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6940-6954},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning multi-view interactional skeleton graph for action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepGCNs: Making GCNs go as deep as CNNs. <em>TPAMI</em>,
<em>45</em>(6), 6923–6939. (<a
href="https://doi.org/10.1109/TPAMI.2021.3074057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been very successful at solving a variety of computer vision tasks such as object classification and detection, semantic segmentation, activity understanding, to name just a few. One key enabling factor for their great performance has been the ability to train very deep networks. Despite their huge success in many tasks, CNNs do not work well with non-euclidean data, which is prevalent in many real-world applications. Graph Convolutional Networks (GCNs) offer an alternative that allows for non-Eucledian data input to a neural network. While GCNs already achieve encouraging results, they are currently limited to architectures with a relatively small number of layers, primarily due to vanishing gradients during training. This work transfers concepts such as residual/dense connections and dilated convolutions from CNNs to GCNs in order to successfully train very deep GCNs. We show the benefit of using deep GCNs (with as many as 112 layers) experimentally across various datasets and tasks. Specifically, we achieve very promising performance in part segmentation and semantic segmentation on point clouds and in node classification of protein functions across biological protein-protein interaction (PPI) graphs. We believe that the insights in this work will open avenues for future research on GCNs and their application to further tasks not explored in this paper. The source code for this work is available at https://github.com/lightaime/deep_gcns_torch and https://github.com/lightaime/deep_gcns for PyTorch and TensorFlow implementations respectively.},
  archive      = {J_TPAMI},
  author       = {Guohao Li and Matthias Müller and Guocheng Qian and Itzel C. Delgadillo and Abdulellah Abualshour and Ali Thabet and Bernard Ghanem},
  doi          = {10.1109/TPAMI.2021.3074057},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6923-6939},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepGCNs: Making GCNs go as deep as CNNs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting 2D convolutional neural networks for graph-based
applications. <em>TPAMI</em>, <em>45</em>(6), 6909–6922. (<a
href="https://doi.org/10.1109/TPAMI.2021.3083614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are widely used in graph-based applications such as graph classification and segmentation. However, current GCNs have limitations on implementation such as network architectures due to their irregular inputs. In contrast, convolutional neural networks (CNNs) are capable of extracting rich features from large-scale input data, but they do not support general graph inputs. To bridge the gap between GCNs and CNNs, in this paper we study the problem of how to effectively and efficiently map general graphs to 2D grids that CNNs can be directly applied to, while preserving graph topology as much as possible. We therefore propose two novel graph-to-grid mapping schemes, namely, graph-preserving grid layout (GPGL) and its extension Hierarchical GPGL (H-GPGL) for computational efficiency. We formulate the GPGL problem as integer programming and further propose an approximate yet efficient solver based on a penalized Kamada-Kawai method, a well-known optimization algorithm in 2D graph drawing. We propose a novel vertex separation penalty that encourages graph vertices to lay on the grid without any overlap. Along with this image representation, even extra 2D maxpooling layers contribute to the PointNet, a widely applied point-based neural network. We demonstrate the empirical success of GPGL on general graph classification with small graphs and H-GPGL on 3D point cloud segmentation with large graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout (MSM) CNN.},
  archive      = {J_TPAMI},
  author       = {Yecheng Lyu and Xinming Huang and Ziming Zhang},
  doi          = {10.1109/TPAMI.2021.3083614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6909-6922},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting 2D convolutional neural networks for graph-based applications},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). CCNet: Criss-cross attention for semantic segmentation.
<em>TPAMI</em>, <em>45</em>(6), 6896–6908. (<a
href="https://doi.org/10.1109/TPAMI.2020.3007032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a criss-cross network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires $11\times$ less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85 percent of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9, 45.76 and 55.47 percent on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at https://github.com/speedinghzl/CCNethttps://github.com/speedinghzl/CCNet .},
  archive      = {J_TPAMI},
  author       = {Zilong Huang and Xinggang Wang and Yunchao Wei and Lichao Huang and Humphrey Shi and Wenyu Liu and Thomas S. Huang},
  doi          = {10.1109/TPAMI.2020.3007032},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6896-6908},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CCNet: Criss-cross attention for semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global context networks. <em>TPAMI</em>, <em>45</em>(6),
6881–6895. (<a
href="https://doi.org/10.1109/TPAMI.2020.3047209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-local network (NLNet) presents a pioneering approach for capturing long-range dependencies within an image, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by the non-local network are almost the same for different query positions. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further replace the one-layer transformation function of the non-local block by a two-layer bottleneck, which further reduces the parameter number considerably. The resulting network element, called the global context (GC) block, effectively models global context in a lightweight manner, allowing it to be applied at multiple layers of a backbone network to form a global context network (GCNet). Experiments show that GCNet generally outperforms NLNet on major benchmarks for various recognition tasks. The code and network configurations are available at https://github.com/xvjiarui/GCNet .},
  archive      = {J_TPAMI},
  author       = {Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu},
  doi          = {10.1109/TPAMI.2020.3047209},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6881-6895},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global context networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Second-order pooling for graph neural networks.
<em>TPAMI</em>, <em>45</em>(6), 6870–6880. (<a
href="https://doi.org/10.1109/TPAMI.2020.2999032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have achieved great success in learning node representations for graph tasks such as node classification and link prediction. Graph representation learning requires graph pooling to obtain graph representations from node representations. It is challenging to develop graph pooling methods due to the variable sizes and isomorphic structures of graphs. In this work, we propose to use second-order pooling as graph pooling, which naturally solves the above challenges. In addition, compared to existing graph pooling methods, second-order pooling is able to use information from all nodes and collect second-order statistics, making it more powerful. We show that direct use of second-order pooling with graph neural networks leads to practical problems. To overcome these problems, we propose two novel global graph pooling methods based on second-order pooling; namely, bilinear mapping and attentional second-order pooling. In addition, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. We perform thorough experiments on graph classification tasks to demonstrate the effectiveness and superiority of our proposed methods. Experimental results show that our methods improve the performance significantly and consistently.},
  archive      = {J_TPAMI},
  author       = {Zhengyang Wang and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2020.2999032},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6870-6880},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Second-order pooling for graph neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Introduction to the special section on
graphs in vision and pattern analysis. <em>TPAMI</em>, <em>45</em>(6),
6867–6869. (<a
href="https://doi.org/10.1109/TPAMI.2023.3259779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this special section is to provide a platform to summarize what we have achieved and where we are moving toward this methodological research direction. It gathers the latest advances in learning with graph-structured data in computer vision, as well as interdisciplinary efforts on graph-based pattern analysis in sociology, physics, chemistry, finance, biology, etc.},
  archive      = {J_TPAMI},
  author       = {Song Bai and Philip H.S. Torr and Ranjay Krishna and Fei-Fei Li and Abhinav Gupta and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2023.3259779},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6867-6869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guest editorial: Introduction to the special section on graphs in vision and pattern analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of the hands in egocentric vision: A survey.
<em>TPAMI</em>, <em>45</em>(6), 6846–6866. (<a
href="https://doi.org/10.1109/TPAMI.2020.2986648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric vision (a.k.a. first-person vision–FPV) applications have thrived over the past few years, thanks to the availability of affordable wearable cameras and large annotated datasets. The position of the wearable camera (usually mounted on the head) allows recording exactly what the camera wearers have in front of them, in particular hands and manipulated objects. This intrinsic advantage enables the study of the hands from multiple perspectives: localizing hands and their parts within the images; understanding what actions and activities the hands are involved in; and developing human-computer interfaces that rely on hand gestures. In this survey, we review the literature that focuses on the hands using egocentric vision, categorizing the existing approaches into: localization (where are the hands or parts of them?); interpretation (what are the hands doing?); and application (e.g., systems that used egocentric hand cues for solving a specific problem). Moreover, a list of the most prominent datasets with hand-based annotations is provided.},
  archive      = {J_TPAMI},
  author       = {Andrea Bandini and José Zariffa},
  doi          = {10.1109/TPAMI.2020.2986648},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6846-6866},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Analysis of the hands in egocentric vision: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating personalized summaries of day long egocentric
videos. <em>TPAMI</em>, <em>45</em>(6), 6832–6845. (<a
href="https://doi.org/10.1109/TPAMI.2021.3118077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of egocentric cameras and their always-on nature has lead to the abundance of day long first-person videos. The highly redundant nature of these videos and extreme camera-shakes make them difficult to watch from beginning to end. These videos require efficient summarization tools for consumption. However, traditional summarization techniques developed for static surveillance videos or highly curated sports videos and movies are either not suitable or simply do not scale for such hours long videos in the wild. On the other hand, specialized summarization techniques developed for egocentric videos limit their focus to important objects and people. This paper presents a novel unsupervised reinforcement learning framework to summarize egocentric videos both in terms of length and the content. The proposed framework facilitates incorporating various prior preferences such as faces, places, or scene diversity and interactive user choice in terms of including or excluding the particular type of content. This approach can also be adapted to generate summaries of various lengths, making it possible to view even 1-minute summaries of one’s entire day. When using the facial saliency-based reward, we show that our approach generates summaries focusing on social interactions, similar to the current state-of-the-art (SOTA). The quantitative comparisons on the benchmark Disney dataset show that our method achieves significant improvement in Relaxed F-Score (RFS) (29.60 compared to 19.21 from SOTA), BLEU score (0.68 compared to 0.67 from SOTA), Average Human Ranking (AHR), and unique events covered. Finally, we show that our technique can be applied to summarize traditional, short, hand-held videos as well, where we improve the SOTA F-score on benchmark SumMe and TVSum datasets from 41.4 to 46.40 and 57.6 to 58.3 respectively. We also provide a Pytorch implementation and a web demo at https://pravin74.github.io/Int-sum/index.html .},
  archive      = {J_TPAMI},
  author       = {Pravin Nagar and Anuj Rathore and C. V. Jawahar and Chetan Arora},
  doi          = {10.1109/TPAMI.2021.3118077},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6832-6845},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generating personalized summaries of day long egocentric videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging hand-object interactions in assistive egocentric
vision. <em>TPAMI</em>, <em>45</em>(6), 6820–6831. (<a
href="https://doi.org/10.1109/TPAMI.2021.3123303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric vision holds great promise for increasing access to visual information and improving the quality of life for blind people. While we strive to improve recognition performance, it remains difficult to identify which object is of interest to the user; the object may not even be included in the frame due to challenges in camera aiming without visual feedback. Also, gaze information, commonly used to infer the area of interest in egocentric vision, is often not dependable. However, blind users tend to include their hand either interacting with the object they wish to recognize or simply placing it in proximity for better camera aiming. We propose a method that leverages the hand as the contextual information for recognizing an object of interest. In our method, the output of a pre-trained hand segmentation model is infused to later convolutional layers of our object recognition network with separate output layers for localization and classification. Using egocentric datasets from sighted and blind individuals, we show that the hand-priming achieves more accurate localization than other approaches that encode hand information. Given only object centers along with labels, our method achieves comparable classification performance to the state-of-the-art method that uses bounding boxes with labels.},
  archive      = {J_TPAMI},
  author       = {Kyungjun Lee and Abhinav Shrivastava and Hernisa Kacorri},
  doi          = {10.1109/TPAMI.2021.3123303},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6820-6831},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Leveraging hand-object interactions in assistive egocentric vision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth and video segmentation based visual attention for
embodied question answering. <em>TPAMI</em>, <em>45</em>(6), 6807–6819.
(<a href="https://doi.org/10.1109/TPAMI.2021.3139957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied Question Answering (EQA) is a newly defined research area where an agent is required to answer the user&#39;s questions by exploring the real-world environment. It has attracted increasing research interests due to its broad applications in personal assistants and in-home robots. Most of the existing methods perform poorly in terms of answering and navigation accuracy due to the absence of fine-level semantic information, stability to the ambiguity, and 3D spatial information of the virtual environment. To tackle these problems, we propose a depth and segmentation based visual attention mechanism for Embodied Question Answering. First, we extract local semantic features by introducing a novel high-speed video segmentation framework. Then guided by the extracted semantic features, a depth and segmentation based visual attention mechanism is proposed for the Visual Question Answering (VQA) sub-task. Further, a feature fusion strategy is designed to guide the navigator&#39;s training process without much additional computational cost. The ablation experiments show that our method effectively boosts the performance of the VQA module and navigation module, leading to 4.9 $\%$ and 5.6 $\%$ overall improvement in EQA accuracy on House3D and Matterport3D datasets respectively.},
  archive      = {J_TPAMI},
  author       = {Haonan Luo and Guosheng Lin and Yazhou Yao and Fayao Liu and Zichuan Liu and Zhenmin Tang},
  doi          = {10.1109/TPAMI.2021.3139957},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6807-6819},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Depth and video segmentation based visual attention for embodied question answering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SelfPose: 3D egocentric pose estimation from a headset
mounted camera. <em>TPAMI</em>, <em>45</em>(6), 6794–6806. (<a
href="https://doi.org/10.1109/TPAMI.2020.3029700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. We propose a new encoder-decoder architecture with a novel multi-branch decoder designed specifically to account for the varying uncertainty in 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. To tackle the severe lack of labelled training data for egocentric 3D pose estimation we also introduced a large-scale photo-realistic synthetic dataset. $\boldsymbol{x}$ R-EgoPose offers 383K frames of high quality renderings of people with diverse skin tones, body shapes and clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.},
  archive      = {J_TPAMI},
  author       = {Denis Tome and Thiemo Alldieck and Patrick Peluse and Gerard Pons-Moll and Lourdes Agapito and Hernan Badino and Fernando de la Torre},
  doi          = {10.1109/TPAMI.2020.3029700},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6794-6806},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SelfPose: 3D egocentric pose estimation from a headset mounted camera},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EgoCom: A multi-person multi-modal egocentric communications
dataset. <em>TPAMI</em>, <em>45</em>(6), 6783–6793. (<a
href="https://doi.org/10.1109/TPAMI.2020.3025105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal datasets in artificial intelligence (AI) often capture a third-person perspective, but our embodied human intelligence evolved with sensory input from the egocentric, first-person perspective. Towards embodied AI, we introduce the Egocentric Communications (EgoCom) dataset to advance the state-of-the-art in conversational AI, natural language, audio speech analysis, computer vision, and machine learning. EgoCom is a first-of-its-kind natural conversations dataset containing multi-modal human communication data captured simultaneously from the participants’ egocentric perspectives. EgoCom includes 38.5 hours of synchronized embodied stereo audio, egocentric video with 240,000 ground-truth, time-stamped word-level transcriptions and speaker labels from 34 diverse speakers. We study baseline performance on two novel applications that benefit from embodied data: (1) predicting turn-taking in conversations and (2) multi-speaker transcription. For (1), we investigate Bayesian baselines to predict turn-taking within 5 percent of human performance. For (2), we use simultaneous egocentric capture to combine Google speech-to-text outputs, improving global transcription by 79 percent relative to a single perspective. Both applications exploit EgoCom&#39;s synchronous multi-perspective data to augment performance of embodied AI tasks.},
  archive      = {J_TPAMI},
  author       = {Curtis G. Northcutt and Shengxin Zha and Steven Lovegrove and Richard Newcombe},
  doi          = {10.1109/TPAMI.2020.3025105},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6783-6793},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EgoCom: A multi-person multi-modal egocentric communications dataset},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A large-scale virtual dataset and egocentric localization
for disaster responses. <em>TPAMI</em>, <em>45</em>(6), 6766–6782. (<a
href="https://doi.org/10.1109/TPAMI.2021.3094531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing social demands of disaster response, methods of visual observation for rescue and safety have become increasingly important. However, because of the shortage of datasets for disaster scenarios, there has been little progress in computer vision and robotics in this field. With this in mind, we present the first large-scale synthetic dataset of egocentric viewpoints for disaster scenarios. We simulate pre- and post-disaster cases with drastic changes in appearance, such as buildings on fire and earthquakes. The dataset consists of more than 300K high-resolution stereo image pairs, all annotated with ground-truth data for the semantic label, depth in metric scale, optical flow with sub-pixel precision, and surface normal as well as their corresponding camera poses. To create realistic disaster scenes, we manually augment the effects with 3D models using physically-based graphics tools. We train various state-of-the-art methods to perform computer vision tasks using our dataset, evaluate how well these methods recognize the disaster situations, and produce reliable results of virtual scenes as well as real-world images. We also present a convolutional neural network-based egocentric localization method that is robust to drastic appearance changes, such as the texture changes in a fire, and layout changes from a collapse. To address these key challenges, we propose a new model that learns a shape-based representation by training on stylized images, and incorporate the dominant planes of query images as approximate scene coordinates. We evaluate the proposed method using various scenes including a simulated disaster dataset to demonstrate the effectiveness of our method when confronted with significant changes in scene layout. Experimental results show that our method provides reliable camera pose predictions despite vastly changed conditions.},
  archive      = {J_TPAMI},
  author       = {Hae-Gon Jeon and Sunghoon Im and Byeong-Uk Lee and François Rameau and Dong-Geol Choi and Jean Oh and In So Kweon and Martial Hebert},
  doi          = {10.1109/TPAMI.2021.3094531},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6766-6782},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A large-scale virtual dataset and egocentric localization for disaster responses},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JRDB: A dataset and benchmark of egocentric robot visual
perception of humans in built environments. <em>TPAMI</em>,
<em>45</em>(6), 6748–6765. (<a
href="https://doi.org/10.1109/TPAMI.2021.3070543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present JRDB, a novel egocentric dataset collected from our social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 ${}^\circ$ RGB video at 15 fps, 3D point clouds from two 16 planar rays Velodyne LiDARs, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 ${}^\circ$ spherical image from a fisheye camera and encoder values from the robot&#39;s wheels. Our dataset incorporates data from traditionally underrepresented scenes such as indoor environments and pedestrian areas, all from the ego-perspective of the robot, both stationary and navigating. The dataset has been annotated with over 2.4 million bounding boxes spread over five individual cameras and 1.8 million associated 3D cuboids around all people in the scenes totaling over 3500 time consistent trajectories. Together with our dataset and the annotations, we launch a benchmark and metrics for 2D and 3D person detection and tracking. With this dataset, which we plan on extending with further types of annotation in the future, we hope to provide a new source of data and a test-bench for research in the areas of egocentric robot vision, autonomous navigation, and all perceptual tasks around social robotics in human environments.},
  archive      = {J_TPAMI},
  author       = {Roberto Martín-Martín and Mihir Patel and Hamid Rezatofighi and Abhijeet Shenoi and JunYoung Gwak and Eric Frankel and Amir Sadeghian and Silvio Savarese},
  doi          = {10.1109/TPAMI.2021.3070543},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6748-6765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {JRDB: A dataset and benchmark of egocentric robot visual perception of humans in built environments},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). In the eye of the beholder: Gaze and actions in first
person video. <em>TPAMI</em>, <em>45</em>(6), 6731–6747. (<a
href="https://doi.org/10.1109/TPAMI.2021.3051319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant&#39;s gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset—EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition.},
  archive      = {J_TPAMI},
  author       = {Yin Li and Miao Liu and James M. Rehg},
  doi          = {10.1109/TPAMI.2021.3051319},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6731-6747},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {In the eye of the beholder: Gaze and actions in first person video},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-regulated learning for egocentric video activity
anticipation. <em>TPAMI</em>, <em>45</em>(6), 6715–6730. (<a
href="https://doi.org/10.1109/TPAMI.2021.3059923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future activity anticipation is a challenging problem in egocentric vision. As a standard future activity anticipation paradigm, recursive sequence prediction suffers from the accumulation of errors. To address this problem, we propose a simple and effective Self-Regulated Learning framework, which aims to regulate the intermediate representation consecutively to produce representation that (a) emphasizes the novel information in the frame of the current time-stamp in contrast to previously observed content, and (b) reflects its correlation with previously observed frames. The former is achieved by minimizing a contrastive loss, and the latter can be achieved by a dynamic reweighing mechanism to attend to informative frames in the observed content with a similarity comparison between feature of the current frame and observed frames. The learned final video representation can be further enhanced by multi-task learning which performs joint feature learning on the target activity labels and the automatically detected action and object class tokens. SRL sharply outperforms existing state-of-the-art in most cases on two egocentric video datasets and two third-person video datasets. Its effectiveness is also verified by the experimental fact that the action and object concepts that support the activity semantics can be accurately identified.},
  archive      = {J_TPAMI},
  author       = {Zhaobo Qi and Shuhui Wang and Chi Su and Li Su and Qingming Huang and Qi Tian},
  doi          = {10.1109/TPAMI.2021.3059923},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6715-6730},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-regulated learning for egocentric video activity anticipation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting action through contact representations from
first person video. <em>TPAMI</em>, <em>45</em>(6), 6703–6714. (<a
href="https://doi.org/10.1109/TPAMI.2021.3055233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions involving hand manipulations are structured according to the making and breaking of hand-object contact, and human visual understanding of action is reliant on anticipation of contact as is demonstrated by pioneering work in cognitive science. Taking inspiration from this, we introduce representations and models centered on contact, which we then use in action prediction and anticipation. We annotate a subset of the EPIC Kitchens dataset to include time-to-contact between hands and objects, as well as segmentations of hands and objects. Using these annotations we train the Anticipation Module , a module producing Contact Anticipation Maps and Next Active Object Segmentations - novel low-level representations providing temporal and spatial characteristics of anticipated near future action. On top of the Anticipation Module we apply Egocentric Object Manipulation Graphs (Ego-OMG), a framework for action anticipation and prediction. Ego-OMG models longer term temporal semantic relations through the use of a graph modeling transitions between contact delineated action states. Use of the Anticipation Module within Ego-OMG produces state-of-the-art results, achieving 1st and 2 place on the unseen and seen test sets, respectively, of the EPIC Kitchens Action Anticipation Challenge, and achieving state-of-the-art results on the tasks of action anticipation and action prediction over EPIC Kitchens. We perform ablation studies over characteristics of the Anticipation Module to evaluate their utility.},
  archive      = {J_TPAMI},
  author       = {Eadom Dessalene and Chinmaya Devaraj and Michael Maynord and Cornelia Fermüller and Yiannis Aloimonos},
  doi          = {10.1109/TPAMI.2021.3055233},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6703-6714},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Forecasting action through contact representations from first person video},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple trajectory prediction of moving agents with memory
augmented networks. <em>TPAMI</em>, <em>45</em>(6), 6688–6702. (<a
href="https://doi.org/10.1109/TPAMI.2020.3008558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrians and drivers are expected to safely navigate complex urban environments along with several non cooperating agents. Autonomous vehicles will soon replicate this capability. Each agent acquires a representation of the world from an egocentric perspective and must make decisions ensuring safety for itself and others. This requires to predict motion patterns of observed agents for a far enough future. In this paper we propose MANTRA, a model that exploits memory augmented networks to effectively predict multiple trajectories of other agents, observed from an egocentric perspective. Our model stores observations in memory and uses trained controllers to write meaningful pattern encodings and read trajectories that are most likely to occur in future. We show that our method is able to natively perform multi-modal trajectory prediction obtaining state-of-the art results on four datasets. Moreover, thanks to the non-parametric nature of the memory module, we show how once trained our system can continuously improve by ingesting novel patterns.},
  archive      = {J_TPAMI},
  author       = {Francesco Marchetti and Federico Becattini and Lorenzo Seidenari and Alberto Del Bimbo},
  doi          = {10.1109/TPAMI.2020.3008558},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6688-6702},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiple trajectory prediction of moving agents with memory augmented networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning to recognize actions on objects in egocentric
video with attention dictionaries. <em>TPAMI</em>, <em>45</em>(6),
6674–6687. (<a
href="https://doi.org/10.1109/TPAMI.2021.3058649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present EgoACO, a deep neural architecture for video action recognition that learns to pool action-context-object descriptors from frame level features by leveraging the verb-noun structure of action labels in egocentric video datasets. The core component is class activation pooling (CAP), a differentiable pooling layer that combines ideas from bilinear pooling for fine-grained recognition and from feature learning for discriminative localization. CAP uses self-attention with a dictionary of learnable weights to pool from the most relevant feature regions. Through CAP, EgoACO learns to decode object and scene context descriptors from video frame features. For temporal modeling we design a recurrent version of class activation pooling termed Long Short-Term Attention (LSTA). LSTA extends convolutional gated LSTM with built-in spatial attention and a re-designed output gate. Action, object and context descriptors are fused by a multi-head prediction that accounts for the inter-dependencies between noun-verb-action structured labels in egocentric video datasets. EgoACO features built-in visual explanations, helping learning and interpretation of discriminative information in video. Results on the two largest egocentric action recognition datasets currently available, EPIC-KITCHENS and EGTEA Gaze+, show that by decoding action-context-object descriptors, the model achieves state-of-the-art recognition performance.},
  archive      = {J_TPAMI},
  author       = {Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz},
  doi          = {10.1109/TPAMI.2021.3058649},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6674-6687},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to recognize actions on objects in egocentric video with attention dictionaries},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific priors and meta learning for few-shot
first-person action recognition. <em>TPAMI</em>, <em>45</em>(6),
6659–6673. (<a
href="https://doi.org/10.1109/TPAMI.2021.3058606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of large-scale real datasets with annotations makes transfer learning a necessity for video activity understanding. We aim to develop an effective method for few-shot transfer learning for first-person action classification. We leverage independently trained local visual cues to learn representations that can be transferred from a source domain, which provides primitive action labels, to a different target domain – using only a handful of examples. Visual cues we employ include object-object interactions, hand grasps and motion within regions that are a function of hand locations. We employ a framework based on meta-learning to extract the distinctive and domain invariant components of the deployed visual cues. This enables transfer of action classification models across public datasets captured with diverse scene and action configurations. We present comparative results of our transfer learning methodology and report superior results over state-of-the-art action classification approaches for both inter-class and inter-dataset transfer.},
  archive      = {J_TPAMI},
  author       = {Huseyin Coskun and M. Zeeshan Zia and Bugra Tekin and Federica Bogo and Nassir Navab and Federico Tombari and Harpreet S. Sawhney},
  doi          = {10.1109/TPAMI.2021.3058606},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6659-6673},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Domain-specific priors and meta learning for few-shot first-person action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MS-TCN++: Multi-stage temporal convolutional network for
action segmentation. <em>TPAMI</em>, <em>45</em>(6), 6647–6658. (<a
href="https://doi.org/10.1109/TPAMI.2020.3021756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.},
  archive      = {J_TPAMI},
  author       = {Shijie Li and Yazan Abu Farha and Yun Liu and Ming-Ming Cheng and Juergen Gall},
  doi          = {10.1109/TPAMI.2020.3021756},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6647-6658},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MS-TCN++: Multi-stage temporal convolutional network for action segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). First- and third-person video co-analysis by learning
spatial-temporal joint attention. <em>TPAMI</em>, <em>45</em>(6),
6631–6646. (<a
href="https://doi.org/10.1109/TPAMI.2020.3030048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a tremendous increase of first-person videos captured by wearable devices. Such videos record information from different perspectives than the traditional third-person view, and thus show a wide range of potential usages. However, techniques for analyzing videos from different views can be fundamentally different, not to mention co-analyzing on both views to explore the shared information. In this paper, we take the challenge of cross-view video co-analysis and deliver a novel learning-based method. At the core of our method is the notion of “joint attention”, indicating the shared attention regions that link the corresponding views, and eventually guide the shared representation learning across views. To this end, we propose a multi-branch deep network, which extracts cross-view joint attention and shared representation from static frames with spatial constraints, in a self-supervised and simultaneous manner. In addition, by incorporating the temporal transition model of the joint attention, we obtain spatial-temporal joint attention that can robustly capture the essential information extending through time. Our method outperforms the state-of-the-art on the standard cross-view video matching tasks on public datasets. Furthermore, we demonstrate how the learnt joint information can benefit various applications through a set of qualitative and quantitative experiments.},
  archive      = {J_TPAMI},
  author       = {Huangyue Yu and Minjie Cai and Yunfei Liu and Feng Lu},
  doi          = {10.1109/TPAMI.2020.3030048},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6631-6646},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {First- and third-person video co-analysis by learning spatial-temporal joint attention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-dataset, multitask learning of egocentric vision
tasks. <em>TPAMI</em>, <em>45</em>(6), 6618–6630. (<a
href="https://doi.org/10.1109/TPAMI.2021.3061479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For egocentric vision tasks such as action recognition, there is a relative scarcity of labeled data. This increases the risk of overfitting during training. In this paper, we address this issue by introducing a multitask learning scheme that employs related tasks as well as related datasets in the training process. Related tasks are indicative of the performed action, such as the presence of objects and the position of the hands. By including related tasks as additional outputs to be optimized, action recognition performance typically increases because the network focuses on relevant aspects in the video. Still, the training data is limited to a single dataset because the set of action labels usually differs across datasets. To mitigate this issue, we extend the multitask paradigm to include datasets with different label sets. During training, we effectively mix batches with samples from multiple datasets. Our experiments on egocentric action recognition in the EPIC-Kitchens, EGTEA Gaze+, ADL and Charades-EGO datasets demonstrate the improvements of our approach over single-dataset baselines. On EGTEA we surpass the current state-of-the-art by 2.47 percent. We further illustrate the cross-dataset task correlations that emerge automatically with our novel training scheme.},
  archive      = {J_TPAMI},
  author       = {Georgios Kapidis and Ronald Poppe and Remco C. Veltkamp},
  doi          = {10.1109/TPAMI.2021.3061479},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6618-6630},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-dataset, multitask learning of egocentric vision tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Symbiotic attention for egocentric action recognition with
object-centric alignment. <em>TPAMI</em>, <em>45</em>(6), 6605–6617. (<a
href="https://doi.org/10.1109/TPAMI.2020.3015894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to tackle egocentric action recognition by suppressing background distractors and enhancing action-relevant interactions. The existing approaches usually utilize two independent branches to recognize egocentric actions, i.e., a verb branch and a noun branch. However, the mechanism to suppress distracting objects and exploit local human-object correlations is missing. To this end, we introduce two extra sources of information, i.e., the candidate objects spatial location and their discriminative features, to enable concentration on the occurring interactions. We design a S ymbiotic A ttention with O bject-centric feature A lignment framework (SAOA) to provide meticulous reasoning between the actor and the environment. First, we introduce an object-centric feature alignment method to inject the local object features to the verb branch and noun branch. Second, we propose a symbiotic attention mechanism to encourage the mutual interaction between the two branches and select the most action-relevant candidates for classification. The framework benefits from the communication among the verb branch, the noun branch, and the local object information. Experiments based on different backbones and modalities demonstrate the effectiveness of our method. Notably, our framework achieves the state-of-the-art on the largest egocentric video dataset.},
  archive      = {J_TPAMI},
  author       = {Xiaohan Wang and Linchao Zhu and Yu Wu and Yi Yang},
  doi          = {10.1109/TPAMI.2020.3015894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6605-6617},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Symbiotic attention for egocentric action recognition with object-centric alignment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Special section on egocentric perception.
<em>TPAMI</em>, <em>45</em>(6), 6602–6604. (<a
href="https://doi.org/10.1109/TPAMI.2023.3256679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special issue focus on egocentric perception. It gathers recent advances in this field that brings together multiple communities including computer vision, machine learning, and multimedia.},
  archive      = {J_TPAMI},
  author       = {Antonino Furnari and David Crandall and Dima Damen and Kristen Grauman and Giovanni Maria Farinella},
  doi          = {10.1109/TPAMI.2023.3256679},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {6602-6604},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Editorial: Special section on egocentric perception},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Improving video instance segmentation via temporal pyramid
routing. <em>TPAMI</em>, <em>45</em>(5), 6594–6601. (<a
href="https://doi.org/10.1109/TPAMI.2022.3211612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Instance Segmentation (VIS) is a new and inherently multi-task problem, which aims to detect, segment, and track each instance in a video sequence. Existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where either temporal information or multi-scale information is ignored. To incorporate both temporal and scale information, we propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. Specifically, TPR contains two novel components, including Dynamic Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is designed for aligning and gating pyramid features across temporal dimension, while CPR transfers temporally aggregated features across scale dimension. Moreover, our approach is a light-weight and plug-and-play module and can be easily applied to existing instance segmentation methods. Extensive experiments on three datasets including YouTube-VIS (2019, 2021) and Cityscapes-VPS demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art instance and panoptic segmentation methods. Codes will be publicly available at https://github.com/lxtGH/TemporalPyramidRouting .},
  archive      = {J_TPAMI},
  author       = {Xiangtai Li and Hao He and Yibo Yang and Henghui Ding and Kuiyuan Yang and Guangliang Cheng and Yunhai Tong and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3211612},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6594-6601},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving video instance segmentation via temporal pyramid routing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian RBF centered kernel alignment (CKA) in the
large-bandwidth limit. <em>TPAMI</em>, <em>45</em>(5), 6587–6593. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centered kernel alignment (CKA), also known as centered kernel-target alignment, is useful as a similarity measure between kernels and as a kernel-based similarity measure between feature representations. We prove that CKA based on a Gaussian RBF kernel converges to linear CKA in the large-bandwidth limit. The result relies on mean-centering of the feature maps and on a Hilbert-Schmidt Independence Criterion (HSIC) identity. We show that convergence onset is sensitive to the geometry of the feature representations, and that a notion of representation eccentricity, $\rho$ , constrains the bandwidth range for which Gaussian CKA can differ noticeably from linear CKA. Our experimental results suggest that Gaussian bandwidths less than $\rho$ should be selected in order to enable nonlinear modeling.},
  archive      = {J_TPAMI},
  author       = {Sergio A. Alvarez},
  doi          = {10.1109/TPAMI.2022.3216518},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6587-6593},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gaussian RBF centered kernel alignment (CKA) in the large-bandwidth limit},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VOLO: Vision outlooker for visual recognition.
<em>TPAMI</em>, <em>45</em>(5), 6575–6586. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Vision Transformers (ViTs) have been broadly explored in visual recognition. With low efficiency in encoding fine-level features, the performance of ViTs is still inferior to the state-of-the-art CNNs when trained from scratch on a midsize dataset like ImageNet. Through experimental analysis, we find it is because of two reasons: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines, leading to low training sample efficiency; 2) the redundant attention backbone design of ViTs leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we present a new simple and generic architecture, termed Vision Outlooker (VOLO), which implements a novel outlook attention operation that dynamically conduct the local feature aggregation mechanism in a sliding window manner across the input image. Unlike self-attention that focuses on modeling global dependencies of local features at a coarse level, our outlook attention targets at encoding finer-level features, which is critical for recognition but ignored by self-attention. Outlook attention breaks the bottleneck of self-attention whose computation cost scales quadratically with the input spatial dimension, and thus is much more memory efficient. Compared to our Tokens-To-Token Vision Transformer (T2T-ViT), VOLO can more efficiently encode fine-level features that are essential for high-performance visual recognition. Experiments show that with only 26.6 M learnable parameters, VOLO achieves 84.2\% top-1 accuracy on ImageNet-1 K without using extra training data, 2.7\% better than T2T-ViT with a comparable number of parameters. When the model size is scaled up to 296 M parameters, its performance can be further improved to 87.1\%, setting a new record for ImageNet-1 K classification. In addition, we also take the proposed VOLO as pretrained models and report superior performance on downstream tasks, such as semantic segmentation. Code is available at https://github.com/sail-sg/volo .},
  archive      = {J_TPAMI},
  author       = {Li Yuan and Qibin Hou and Zihang Jiang and Jiashi Feng and Shuicheng Yan},
  doi          = {10.1109/TPAMI.2022.3206108},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6575-6586},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VOLO: Vision outlooker for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual object tracking with discriminative filters and
siamese networks: A survey and outlook. <em>TPAMI</em>, <em>45</em>(5),
6552–6574. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and robust visual object tracking is one of the most challenging and fundamental computer vision problems. It entails estimating the trajectory of the target in an image sequence, given only its initial location, and segmentation, or its rough approximation in the form of a bounding box. Discriminative Correlation Filters (DCFs) and deep Siamese Networks (SNs) have emerged as dominating tracking paradigms, which have led to significant progress. Following the rapid evolution of visual object tracking in the last decade, this survey presents a systematic and thorough review of more than 90 DCFs and Siamese trackers, based on results in nine tracking benchmarks. First, we present the background theory of both the DCF and Siamese tracking core formulations. Then, we distinguish and comprehensively review the shared as well as specific open research challenges in both these tracking paradigms. Furthermore, we thoroughly analyze the performance of DCF and Siamese trackers on nine benchmarks, covering different experimental aspects of visual tracking: datasets, evaluation metrics, performance, and speed comparisons. We finish the survey by presenting recommendations and suggestions for distinguished open challenges based on our analysis.},
  archive      = {J_TPAMI},
  author       = {Sajid Javed and Martin Danelljan and Fahad Shahbaz Khan and Muhammad Haris Khan and Michael Felsberg and Jiri Matas},
  doi          = {10.1109/TPAMI.2022.3212594},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6552-6574},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual object tracking with discriminative filters and siamese networks: A survey and outlook},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational label enhancement. <em>TPAMI</em>,
<em>45</em>(5), 6537–6551. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning focuses on the ambiguity at the label side, i.e., one instance is associated with multiple class labels, where the logical labels are always adopted to partition class labels into relevant labels and irrelevant labels rigidly. However, the relevance or irrelevance of each label corresponding to one instance is essentially relative in real-world tasks and the label distribution is more fine-grained than the logical labels by denoting one instance with a certain number of the description degrees of all class labels. As the label distribution is not explicitly available in most training sets, a process named label enhancement emerges to recover the label distributions in training datasets. By inducing the generative model of the label distribution and adopting the variational inference technique, the approximate posterior density of the label distributions should maximize the variational lower bound. Following the above consideration, LEVI is proposed to recover the label distributions from the training examples. In addition, the multi-label predictive model is induced for multi-label learning by leveraging the recovered label distributions along with a specialized objective function. The recovery experiments on fourteen label distribution datasets and the predictive experiments on fourteen multi-label learning datasets validate the advantage of our approach over the state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Ning Xu and Jun Shu and Renyi Zheng and Xin Geng and Deyu Meng and Min-Ling Zhang},
  doi          = {10.1109/TPAMI.2022.3203678},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6537-6551},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational label enhancement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Untrained neural network priors for inverse imaging
problems: A survey. <em>TPAMI</em>, <em>45</em>(5), 6511–6536. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, advancements in machine learning (ML) techniques, in particular, deep learning (DL) methods have gained a lot of momentum in solving inverse imaging problems, often surpassing the performance provided by hand-crafted approaches. Traditionally, analytical methods have been used to solve inverse imaging problems such as image restoration, inpainting, and superresolution. Unlike analytical methods for which the problem is explicitly defined and the domain knowledge is carefully engineered into the solution, DL models do not benefit from such prior knowledge and instead make use of large datasets to predict an unknown solution to the inverse problem. Recently, a new paradigm of training deep models using a single image, named untrained neural network prior (UNNP) has been proposed to solve a variety of inverse tasks, e.g., restoration and inpainting. Since then, many researchers have proposed various applications and variants of UNNP. In this paper, we present a comprehensive review of such studies and various UNNP applications for different tasks and highlight various open research problems which require further research.},
  archive      = {J_TPAMI},
  author       = {Adnan Qayyum and Inaam Ilahi and Fahad Shamshad and Farid Boussaid and Mohammed Bennamoun and Junaid Qadir},
  doi          = {10.1109/TPAMI.2022.3204527},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6511-6536},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Untrained neural network priors for inverse imaging problems: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Transitional learning: Exploring the transition states of
degradation for blind super-resolution. <em>TPAMI</em>, <em>45</em>(5),
6495–6510. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being extremely dependent on iterative estimation of the degradation prior or optimization of the model from scratch, the existing blind super-resolution (SR) methods are generally time-consuming and less effective, as the estimation of degradation proceeds from a blind initialization and lacks interpretable representation of degradations. To address it, this article proposes a transitional learning method for blind SR using an end-to-end network without any additional iterations in inference, and explores an effective representation for unknown degradation. To begin with, we analyze and demonstrate the transitionality of degradations as interpretable prior information to indirectly infer the unknown degradation model, including the widely used additive and convolutive degradations. We then propose a novel Transitional Learning method for blind Super-Resolution (TLSR), by adaptively inferring a transitional transformation function to solve the unknown degradations without any iterative operations in inference. Specifically, the end-to-end TLSR network consists of a degree of transitionality (DoT) estimation network, a homogeneous feature extraction network, and a transitional learning module. Quantitative and qualitative evaluations on blind SR tasks demonstrate that the proposed TLSR achieves superior performances and costs fewer complexities against the state-of-the-art blind SR methods. The code is available at github.com/YuanfeiHuang/TLSR .},
  archive      = {J_TPAMI},
  author       = {Yuanfei Huang and Jie Li and Yanting Hu and Xinbo Gao and Hua Huang},
  doi          = {10.1109/TPAMI.2022.3206870},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6495-6510},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transitional learning: Exploring the transition states of degradation for blind super-resolution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards accurate reconstruction of 3D scene shape from a
single monocular image. <em>TPAMI</em>, <em>45</em>(5), 6480–6494. (<a
href="https://doi.org/10.1109/TPAMI.2022.3209968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant progress made in the past few years, challenges remain for depth estimation using a single monocular image. First, it is nontrivial to train a metric-depth prediction model that can generalize well to diverse scenes mainly due to limited training data. Thus, researchers have built large-scale relative depth datasets that are much easier to collect. However, existing relative depth estimation models often fail to recover accurate 3D scene shapes due to the unknown depth shift caused by training with the relative depth data. We tackle this problem here and attempt to estimate accurate scene shapes by training on large-scale relative depth data, and estimating the depth shift. To do so, we propose a two-stage framework that first predicts depth up to an unknown scale and shift from a single monocular image, and then exploits 3D point cloud data to predict the depth shift and the camera&#39;s focal length that allow us to recover 3D scene shapes. As the two modules are trained separately, we do not need strictly paired training data. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to improve training with relative depth annotation. We test our depth model on nine unseen datasets and achieve state-of-the-art performance on zero-shot evaluation. Code is available at: https://github.com/aim-uofa/depth/ .},
  archive      = {J_TPAMI},
  author       = {Wei Yin and Jianming Zhang and Oliver Wang and Simon Niklaus and Simon Chen and Yifan Liu and Chunhua Shen},
  doi          = {10.1109/TPAMI.2022.3209968},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6480-6494},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards accurate reconstruction of 3D scene shape from a single monocular image},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards accurate and robust domain adaptation under multiple
noisy environments. <em>TPAMI</em>, <em>45</em>(5), 6460–6479. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many non-stationary environments, machine learning algorithms usually confront the distribution shift scenarios. Previous domain adaptation methods have achieved great success. However, they would lose algorithm robustness in multiple noisy environments where the examples of source domain become corrupted by label noise, feature noise, or open-set noise. In this paper, we report our attempt toward achieving noise-robust domain adaptation. We first give a theoretical analysis and find that different noises have disparate impacts on the expected target risk. To eliminate the effect of source noises, we propose offline curriculum learning minimizing a newly-defined empirical source risk. We suggest a proxy distribution-based margin discrepancy to gradually decrease the noisy distribution distance to reduce the impact of source noises. We propose an energy estimator for assessing the outlier degree of open-set-noise examples to defeat the harmful influence. We also suggest robust parameter learning to mitigate the negative effect further and learn domain-invariant feature representations. Finally, we seamlessly transform these components into an adversarial network that performs efficient joint optimization for them. A series of empirical studies on the benchmark datasets and the COVID-19 screening task show that our algorithm remarkably outperforms the state-of-the-art, with over 10\% accuracy improvements in some transfer tasks.},
  archive      = {J_TPAMI},
  author       = {Zhongyi Han and Xian-Jin Gui and Haoliang Sun and Yilong Yin and Shuo Li},
  doi          = {10.1109/TPAMI.2022.3215150},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6460-6479},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards accurate and robust domain adaptation under multiple noisy environments},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theoretical analysis of null foley-sammon transform and its
implications. <em>TPAMI</em>, <em>45</em>(5), 6445–6459. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Null Foley-Sammon Transform (NFST) has received increasing attention in the machine learning and pattern recognition literature. NFST finds a discriminative nullspace where all samples of the same class get mapped into a single point. It has a closed form solution and is free of parameters to tune. NFST has been leveraged in many areas including novelty detection, person or vehicle re-identification and achieved state-of-the-art results. Motivated from its attractive properties and its effectiveness in wide range of applications, in this paper we focus on the theoretical analysis of NFST. In previous literature, NFST was shown to exist in small sample size (SSS) case. We first prove that NFST can exist in non-SSS case also, under certain conditions. Thereby, we extend the domain of applicability of NFST to a more general case. Secondly, we perform analysis of the singular points of NFST, revealing important insights on their identities and existence. Thirdly, we show the theoretical relation between NFST of SSS data and NFST of the non-SSS data obtained by PCA. Fourthly, we show that this theoretical relation can be exploited to obtain an efficient algorithm for computing NFST on high dimensional SSS data. Finally, we perform extensive experiments to validate our theoretical analysis.},
  archive      = {J_TPAMI},
  author       = {T M Feroz Ali and Subhasis Chaudhuri},
  doi          = {10.1109/TPAMI.2022.3213069},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6445-6459},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Theoretical analysis of null foley-sammon transform and its implications},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The proxy step-size technique for regularized optimization
on the sphere manifold. <em>TPAMI</em>, <em>45</em>(5), 6428–6444. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give an effective solution to the regularized optimization problem $g (\boldsymbol{x}) + h (\boldsymbol{x})$ , where $\boldsymbol{x}$ is constrained on the unit sphere $\Vert \boldsymbol{x} \Vert _{2} = 1$ . Here $g (\cdot)$ is a smooth cost with Lipschitz continuous gradient within the unit ball $\lbrace \boldsymbol{x} : \Vert \boldsymbol{x} \Vert _{2} \leq 1 \rbrace$ whereas $h (\cdot)$ is typically non-smooth but convex and absolutely homogeneous, e.g., norm regularizers and their combinations. Our solution is based on the Riemannian proximal gradient, using an idea we call proxy step-size – a scalar variable which we prove is monotone with respect to the actual step-size within an interval. The proxy step-size exists ubiquitously for convex and absolutely homogeneous $h(\cdot)$ , and decides the actual step-size and the tangent update in closed-form, thus the complete proximal gradient iteration. Based on these insights, we design a Riemannian proximal gradient method using the proxy step-size. We prove that our method converges to a critical point, guided by a line-search technique based on the $g(\cdot)$ cost only. The proposed method can be implemented in a couple of lines of code. We show its usefulness by applying nuclear norm, $\ell _{1}$ norm, and nuclear-spectral norm regularization to three classical computer vision problems. The improvements are consistent and backed by numerical experiments. available at https://bitbucket.org/FangBai/proxystepsize-pgs .},
  archive      = {J_TPAMI},
  author       = {Fang Bai and Adrien Bartoli},
  doi          = {10.1109/TPAMI.2022.3215914},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6428-6444},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The proxy step-size technique for regularized optimization on the sphere manifold},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal representation learning on monocular videos for 3D
human pose estimation. <em>TPAMI</em>, <em>45</em>(5), 6415–6427. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we propose an unsupervised feature extraction method to capture temporal information on monocular videos, where we detect and encode subject of interest in each frame and leverage contrastive self-supervised (CSS) learning to extract rich latent vectors. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly disentangle each latent vector into a time-variant component and a time-invariant one. We then show that applying contrastive loss only to the time-variant features and encouraging a gradual transition on them between nearby and away frames while also reconstructing the input, extract rich temporal features, well-suited for human pose estimation. Our approach reduces error by about 50\% compared to the standard CSS strategies, outperforms other unsupervised single-view methods and matches the performance of multi-view techniques. When 2D pose is available, our approach can extract even richer latent features and improve the 3D pose estimation accuracy, outperforming other state-of-the-art weakly supervised methods.},
  archive      = {J_TPAMI},
  author       = {Sina Honari and Victor Constantin and Helge Rhodin and Mathieu Salzmann and Pascal Fua},
  doi          = {10.1109/TPAMI.2022.3215307},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6415-6427},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Temporal representation learning on monocular videos for 3D human pose estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stylized adversarial defense. <em>TPAMI</em>,
<em>45</em>(5), 6403–6414. (<a
href="https://doi.org/10.1109/TPAMI.2022.3207917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolution Neural Networks (CNNs) can easily be fooled by subtle, imperceptible changes to the input images. To address this vulnerability, adversarial training creates perturbation patterns and includes them in the training set to robustify the model. In contrast to existing adversarial training methods that only use class-boundary information (e.g., using a cross-entropy loss), we propose to exploit additional information from the feature space to craft stronger adversaries that are in turn used to learn a robust model. Specifically, we use the style and content information of the target sample from another class, alongside its class-boundary information to create adversarial perturbations. We apply our proposed multi-task objective in a deeply supervised manner, extracting multi-scale feature knowledge to create maximally separating adversaries. Subsequently, we propose a max-margin adversarial training approach that minimizes the distance between source image and its adversary and maximizes the distance between the adversary and the target image. Our adversarial training approach demonstrates strong robustness compared to state-of-the-art defenses, generalizes well to naturally occurring corruptions and data distributional shifts, and retains the model&#39;s accuracy on clean examples.},
  archive      = {J_TPAMI},
  author       = {Muzammal Naseer and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Fatih Porikli},
  doi          = {10.1109/TPAMI.2022.3207917},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6403-6414},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Stylized adversarial defense},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured sparsity optimization with non-convex surrogates
of <span class="math inline"><em>ℓ</em><sub>2, 0</sub></span>ℓ2,0-norm:
A unified algorithmic framework. <em>TPAMI</em>, <em>45</em>(5),
6386–6402. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a general optimization framework that leverages structured sparsity to achieve superior recovery results. The traditional method for solving the structured sparse objectives based on $\ell _{2,0}$ -norm is to use the $\ell _{2,1}$ -norm as a convex surrogate. However, such an approximation often yields a large performance gap. To tackle this issue, we first provide a framework that allows for a wide range of surrogate functions (including non-convex surrogates), which exhibits better performance in harnessing structured sparsity. Moreover, we develop a fixed point algorithm that solves a key underlying non-convex structured sparse recovery optimization problem to global optimality with a guaranteed super-linear convergence rate. Building on this, we consider three specific applications, i.e., outlier pursuit, supervised feature selection, and structured dictionary learning, which can benefit from the proposed structured sparsity optimization framework. In each application, how the optimization problem can be formulated and thus be relaxed under a generic surrogate function is explained in detail. We conduct extensive experiments on both synthetic and real-world data and demonstrate the effectiveness and efficiency of the proposed framework.},
  archive      = {J_TPAMI},
  author       = {Xiaoqin Zhang and Jingjing Zheng and Di Wang and Guiying Tang and Zhengyuan Zhou and Zhouchen Lin},
  doi          = {10.1109/TPAMI.2022.3213716},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6386-6402},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structured sparsity optimization with non-convex surrogates of $\ell _{2,0}$ℓ2,0-norm: A unified algorithmic framework},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stereo confidence estimation via locally adaptive fusion and
knowledge distillation. <em>TPAMI</em>, <em>45</em>(5), 6372–6385. (<a
href="https://doi.org/10.1109/TPAMI.2022.3207286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo confidence estimation aims to estimate the reliability of the estimated disparity by stereo matching. Different from the previous methods that exploit the limited input modality, we present a novel method that estimates confidence map of an initial disparity by making full use of tri-modal input, including matching cost, disparity, and color image through deep networks. The proposed network, termed as Locally Adaptive Fusion Networks (LAF-Net), learns locally-varying attention and scale maps to fuse the tri-modal confidence features. Moreover, we propose a knowledge distillation framework to learn more compact confidence estimation networks as student networks. By transferring the knowledge from LAF-Net as teacher networks, the student networks that solely take as input a disparity can achieve comparable performance. To transfer more informative knowledge, we also propose a module to learn the locally-varying temperature in a softmax function. We further extend this framework to a multiview scenario. Experimental results show that LAF-Net and its variations outperform the state-of-the-art stereo confidence methods on various benchmarks.},
  archive      = {J_TPAMI},
  author       = {Sunok Kim and Seungryong Kim and Dongbo Min and Pascal Frossard and Kwanghoon Sohn},
  doi          = {10.1109/TPAMI.2022.3207286},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6372-6385},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Stereo confidence estimation via locally adaptive fusion and knowledge distillation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ST3D++: Denoised self-training for unsupervised domain
adaptation on 3D object detection. <em>TPAMI</em>, <em>45</em>(5),
6354–6371. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a self-training method, named ST3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. ST3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, ST3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling (ROS) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) for three common categories (i.e., car, pedestrian and bicycle). ST3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6\% $\sim$ 38.16\% on Waymo $\rightarrow$ KITTI in terms of AP $_{\text{3D}}$ ), and even surpasses the fully supervised oracle results on the KITTI 3D object detection benchmark with target prior. Code is available at https://github.com/CVMI-Lab/ST3D .},
  archive      = {J_TPAMI},
  author       = {Jihan Yang and Shaoshuai Shi and Zhe Wang and Hongsheng Li and Xiaojuan Qi},
  doi          = {10.1109/TPAMI.2022.3216606},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6354-6371},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ST3D++: Denoised self-training for unsupervised domain adaptation on 3D object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spherical image generation from a few normal-field-of-view
images by considering scene symmetry. <em>TPAMI</em>, <em>45</em>(5),
6339–6353. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical images taken in all directions (360 degrees by 180 degrees) can represent an entire space including the subject, providing free direction viewing and an immersive experience to viewers. It is convenient and expands the usage scenarios to generate a spherical image from a few normal-field-of-view (NFOV) images, which are partial observations. The primary challenge is generating a plausible image and controlling the high degree of freedom involved in generating a wide area that includes all directions. We focus on scene symmetry, which is a basic property of the global structure of spherical images, such as the rotational and plane symmetries. We propose a method for generating a spherical image from a few NFOV images and controlling the generated regions using scene symmetry. We incorporate the intensity of the symmetry as a latent variable into conditional variational autoencoders to estimate the possible range of symmetry and decode a spherical image whose features are represented through a combination of symmetric transformations of the NFOV image features. Our experiments show that the proposed method can generate various plausible spherical images controlled from asymmetrically to symmetrically, and can reduce the reconstruction errors of the generated images based on the estimated symmetry.},
  archive      = {J_TPAMI},
  author       = {Takayuki Hara and Yusuke Mukuta and Tatsuya Harada},
  doi          = {10.1109/TPAMI.2022.3215933},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6339-6353},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spherical image generation from a few normal-field-of-view images by considering scene symmetry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Snowflake point deconvolution for point cloud completion and
generation with skip-transformer. <em>TPAMI</em>, <em>45</em>(5),
6320–6338. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing point cloud completion methods suffer from the discrete nature of point clouds and the unstructured prediction of points in local regions, which makes it difficult to reveal fine local geometric details. To resolve this issue, we propose SnowflakeNet with snowflake point deconvolution (SPD) to generate complete point clouds. SPD models the generation of point clouds as the snowflake-like growth of points, where child points are generated progressively by splitting their parent points after each SPD. Our insight into the detailed geometry is to introduce a skip-transformer in the SPD to learn the point splitting patterns that can best fit the local regions. The skip-transformer leverages attention mechanism to summarize the splitting patterns used in the previous SPD layer to produce the splitting in the current layer. The locally compact and structured point clouds generated by SPD precisely reveal the structural characteristics of the 3D shape in local patches, which enables us to predict highly detailed geometries. Moreover, since SPD is a general operation that is not limited to completion, we explore its applications in other generative tasks, including point cloud auto-encoding, generation, single image reconstruction, and upsampling. Our experimental results outperform state-of-the-art methods under widely used benchmarks.},
  archive      = {J_TPAMI},
  author       = {Peng Xiang and Xin Wen and Yu-Shen Liu and Yan-Pei Cao and Pengfei Wan and Wen Zheng and Zhizhong Han},
  doi          = {10.1109/TPAMI.2022.3217161},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6320-6338},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Snowflake point deconvolution for point cloud completion and generation with skip-transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMMP: A stable-membership-based auto-tuning multi-peak
clustering algorithm. <em>TPAMI</em>, <em>45</em>(5), 6307–6319. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most existing single-prototype clustering algorithms are unsuitable for complex-shaped clusters, many multi-prototype clustering algorithms have been proposed. Nevertheless, the automatic estimation of the number of clusters and the detection of complex shapes are still challenging, and to solve such problems usually relies on user-specified parameters and may be prohibitively time-consuming. Herein, a stable-membership-based auto-tuning multi-peak clustering algorithm (SMMP) is proposed, which can achieve fast, automatic, and effective multi-prototype clustering without iteration. A dynamic association-transfer method is designed to learn the representativeness of points to sub-cluster centers during the generation of sub-clusters by applying the density peak clustering technique. According to the learned representativeness, a border-link-based connectivity measure is used to achieve high-fidelity similarity evaluation of sub-clusters. Meanwhile, based on the assumption that a reasonable clustering should have a relatively stable membership state upon the change of clustering thresholds, SMMP can automatically identify the number of sub-clusters and clusters, respectively. Also, SMMP is designed for large datasets. Experimental results on both synthetic and real datasets demonstrated the effectiveness of SMMP.},
  archive      = {J_TPAMI},
  author       = {Junyi Guan and Sheng Li and Xiongxiong He and Jinhui Zhu and Jiajia Chen and Peng Si},
  doi          = {10.1109/TPAMI.2022.3213574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6307-6319},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SMMP: A stable-membership-based auto-tuning multi-peak clustering algorithm},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small-object sensitive segmentation using across feature map
attention. <em>TPAMI</em>, <em>45</em>(5), 6289–6306. (<a
href="https://doi.org/10.1109/TPAMI.2022.3211171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is an important step in understanding the scene for many practical applications such as autonomous driving. Although Deep Convolutional Neural Networks-based methods have significantly improved segmentation accuracy, small/thin objects remain challenging to segment due to convolutional and pooling operations that result in information loss, especially for small objects. This article presents a novel attention-based method called Across Feature Map Attention (AFMA) to address this challenge. It quantifies the inner-relationship between small and large objects belonging to the same category by utilizing the different feature levels of the original image. The AFMA could compensate for the loss of high-level feature information of small objects and improve the small/thin object segmentation. Our method can be used as an efficient plug-in for a wide range of existing architectures and produces much more interpretable feature representation than former studies. Extensive experiments on eight widely used segmentation methods and other existing small-object segmentation models on CamVid and Cityscapes demonstrate that our method substantially and consistently improves the segmentation of small/thin objects.},
  archive      = {J_TPAMI},
  author       = {Shengtian Sang and Yuyin Zhou and Md Tauhidul Islam and Lei Xing},
  doi          = {10.1109/TPAMI.2022.3211171},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6289-6306},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Small-object sensitive segmentation using across feature map attention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiMaN: Sign-to-magnitude network binarization.
<em>TPAMI</em>, <em>45</em>(5), 6277–6288. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary neural networks (BNNs) have attracted broad research interest due to their efficient storage and computational ability. Nevertheless, a significant challenge of BNNs lies in handling discrete constraints while ensuring bit entropy maximization, which typically makes their weight optimization very difficult. Existing methods relax the learning using the sign function, which simply encodes positive weights into $+1$ s, and $-1$ s otherwise. Alternatively, we formulate an angle alignment objective to constrain the weight binarization to $\lbrace 0,+1\rbrace$ to solve the challenge. In this article, we show that our weight binarization provides an analytical solution by encoding high-magnitude weights into $+1$ s, and 0s otherwise. Therefore, a high-quality discrete solution is established in a computationally efficient manner without the sign function. We prove that the learned weights of binarized networks roughly follow a Laplacian distribution that does not allow entropy maximization, and further demonstrate that it can be effectively solved by simply removing the $\ell _{2}$ regularization during network training. Our method, dubbed sign-to-magnitude network binarization (SiMaN), is evaluated on CIFAR-10 and ImageNet, demonstrating its superiority over the sign-based state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/SiMaN .},
  archive      = {J_TPAMI},
  author       = {Mingbao Lin and Rongrong Ji and Zihan Xu and Baochang Zhang and Fei Chao and Chia-Wen Lin and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3212615},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6277-6288},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SiMaN: Sign-to-magnitude network binarization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semi-supervised hierarchical graph classification.
<em>TPAMI</em>, <em>45</em>(5), 6265–6276. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification and graph classification are two graph learning problems that predict the class label of a node and the class label of a graph respectively. A node of a graph usually represents a real-world entity, e.g., a user in a social network, or a document in a document citation network. In this work, we consider a more challenging but practically useful setting, in which a node itself is a graph instance. This leads to a hierarchical graph perspective which arises in many domains such as social network, biological network and document collection. We study the node classification problem in the hierarchical graph where a “node” is a graph instance. As labels are usually limited, we design a novel semi-supervised solution named SEAL-CI. SEAL-CI adopts an iterative framework that takes turns to update two modules, one working at the graph instance level and the other at the hierarchical graph level. To enforce a consistency among different levels of hierarchical graph, we propose the Hierarchical Graph Mutual Information (HGMI) and further present a way to compute HGMI with theoretical guarantee. We demonstrate the effectiveness of this hierarchical graph modeling and the proposed SEAL-CI method on text and social network data.},
  archive      = {J_TPAMI},
  author       = {Jia Li and Yongfeng Huang and Heng Chang and Yu Rong},
  doi          = {10.1109/TPAMI.2022.3203703},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6265-6276},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised hierarchical graph classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic probability distribution modeling for diverse
semantic image synthesis. <em>TPAMI</em>, <em>45</em>(5), 6247–6264. (<a
href="https://doi.org/10.1109/TPAMI.2022.3210085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level or even instance-level multimodal results, still remains a challenge. In this article, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at both semantics and instance level. We achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. To further extend the user interaction function of the proposed method, we also introduce sketches into the network. In addition, specially designed generator modules, Progressive Growing Module and Multi-Scale Refinement Module, can be used as a general module to improve the performance of complex scene generation. Extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. Codes are available at https://github.com/tzt101/INADE.git .},
  archive      = {J_TPAMI},
  author       = {Zhentao Tan and Qi Chu and Menglei Chai and Dongdong Chen and Jing Liao and Qiankun Liu and Bin Liu and Gang Hua and Nenghai Yu},
  doi          = {10.1109/TPAMI.2022.3210085},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6247-6264},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semantic probability distribution modeling for diverse semantic image synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Searching a high performance feature extractor for text
recognition network. <em>TPAMI</em>, <em>45</em>(5), 6231–6246. (<a
href="https://doi.org/10.1109/TPAMI.2022.3205748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extractor plays a critical role in text recognition (TR), but customizing its architecture is relatively less explored due to expensive manual tweaking. In this article, inspired by the success of neural architecture search (NAS), we propose to search for suitable feature extractors. We design a domain-specific search space by exploring principles for having good feature extractors. The space includes a 3D-structured space for the spatial model and a transformed-based space for the sequential model. As the space is huge and complexly structured, no existing NAS algorithms can be applied. We propose a two-stage algorithm to effectively search in the space. In the first stage, we cut the space into several blocks and progressively train each block with the help of an auxiliary head. We introduce the latency constrain into the second stage and search sub-network from the trained supernet via natural gradient descent. In experiments, a series of ablation studies are performed to better understand the designed space, search algorithm, and searched architectures. We also compare the proposed method with various state-of-the-art ones on both hand-written and scene TR tasks. Extensive results show that our approach can achieve better recognition performance with less latency. Code is avaliable at https://github.com/AutoML-Research/TREFE .},
  archive      = {J_TPAMI},
  author       = {Hui Zhang and Quanming Yao and James T. Kwok and Xiang Bai},
  doi          = {10.1109/TPAMI.2022.3205748},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6231-6246},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Searching a high performance feature extractor for text recognition network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rolling shutter inversion: Bring rolling shutter images to
high framerate global shutter video. <em>TPAMI</em>, <em>45</em>(5),
6214–6230. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single rolling-shutter (RS) image may be viewed as a row-wise combination of a sequence of global-shutter (GS) images captured by a (virtual) moving GS camera within the exposure duration. Although rolling-shutter cameras are widely used, the RS effect causes obvious image distortion especially in the presence of fast camera motion, hindering downstream computer vision tasks. In this paper, we propose to invert the rolling-shutter image capture mechanism, i.e., recovering a continuous high framerate global-shutter video from two time-consecutive RS frames. We call this task the RS temporal super-resolution (RSSR) problem. The RSSR is a very challenging task, and to our knowledge, no practical solution exists to date. This paper presents a novel deep-learning based solution. By leveraging the multi-view geometry relationship of the RS imaging process, our learning based framework successfully achieves high framerate GS generation. Specifically, three novel contributions can be identified: (i) novel formulations for bidirectional RS undistortion flows under constant velocity as well as constant acceleration motion model. (ii) a simple linear scaling operation, which bridges the RS undistortion flow and regular optical flow. (iii) a new mutual conversion scheme between varying RS undistortion flows that correspond to different scanlines. Our method also exploits the underlying spatial-temporal geometric relationships within a deep learning framework, where no additional supervision is required beyond the necessary middle-scanline GS image. Building upon these contributions, this paper represents the very first rolling-shutter temporal super-resolution deep-network that is able to recover high framerate global-shutter videos from just two RS frames. Extensive experimental results on both synthetic and real data show that our proposed method can produce high-quality GS image sequences with rich details, outperforming the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Bin Fan and Yuchao Dai and Hongdong Li},
  doi          = {10.1109/TPAMI.2022.3212912},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6214-6230},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rolling shutter inversion: Bring rolling shutter images to high framerate global shutter video},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RobustFusion: Robust volumetric performance reconstruction
under human-object interactions from monocular RGBD stream.
<em>TPAMI</em>, <em>45</em>(5), 6196–6213. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality 4D reconstruction of human performance with complex interactions to various objects is essential in real-world scenarios, which enables numerous immersive VR/AR applications. However, recent advances still fail to provide reliable performance reconstruction, suffering from challenging interaction patterns and severe occlusions, especially for the monocular setting. To fill this gap, in this paper, we propose RobustFusion, a robust volumetric performance reconstruction system for human-object interaction scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex interaction patterns and severe occlusions. We propose a semantic-aware scene decoupling scheme to model the occlusions explicitly, with a segmentation refinement and robust object tracking to prevent disentanglement uncertainty and maintain temporal consistency. We further introduce a robust performance capture scheme with the aid of various data-driven cues, which not only enables re-initialization ability, but also models the complex human-object interaction patterns in a data-driven manner. To this end, we introduce a spatial relation prior to prevent implausible intersections, as well as data-driven interaction cues to maintain natural motions, especially for those regions under severe human-object occlusions. We also adopt an adaptive fusion scheme for temporally coherent human-object reconstruction with occlusion analysis and human parsing cue. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality 4D human performance reconstruction under complex human-object interactions whilst still maintaining the lightweight monocular setting.},
  archive      = {J_TPAMI},
  author       = {Zhuo Su and Lan Xu and Dawei Zhong and Zhong Li and Fan Deng and Shuxue Quan and Lu Fang},
  doi          = {10.1109/TPAMI.2022.3215746},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6196-6213},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RobustFusion: Robust volumetric performance reconstruction under human-object interactions from monocular RGBD stream},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust point cloud registration framework based on deep
graph matching. <em>TPAMI</em>, <em>45</em>(5), 6183–6195. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud registration is a fundamental problem in computer vision and robotics. Recently, learning-based point cloud registration methods have made great progress. However, these methods are sensitive to outliers, which lead to more incorrect correspondences. In this paper, we propose a novel deep graph matching-based framework for point cloud registration. Specifically, we first transform point clouds into graphs and extract deep features for each point. Then, we develop a module based on deep graph matching to calculate a soft correspondence matrix. By using graph matching, not only the local geometry of each point but also its structure and topology in a larger range are considered in establishing correspondences, so that more correct correspondences are found. We train the network with a loss directly defined on the correspondences, and in the test stage the soft correspondences are transformed into hard one-to-one correspondences so that registration can be performed by a correspondence-based solver. Furthermore, we introduce a transformer-based method to generate edges for graph construction, which further improves the quality of the correspondences. Extensive experiments on object-level and scene-level benchmark datasets show that the proposed method achieves state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Kexue Fu and Jiazheng Luo and Xiaoyuan Luo and Shaolei Liu and Chenxi Zhang and Manning Wang},
  doi          = {10.1109/TPAMI.2022.3204713},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6183-6195},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust point cloud registration framework based on deep graph matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust online tracking with meta-updater. <em>TPAMI</em>,
<em>45</em>(5), 6168–6182. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a sequence, the appearance of both the target and background often changes dramatically. Offline-trained models may not handle huge appearance variations well, causing tracking failures. Most discriminative trackers address this issue by introducing an online update scheme, making the model dynamically adapt the changes of the target and background. Although the online update scheme plays an important role in improving the tracker&#39;s accuracy, it inevitably pollutes the model with noisy observation samples. It is necessary to reduce the risk of the online update scheme for better tracking. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed module can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Moreover, we strengthen the effect of appearance information on the module, i.e., the additional local outlier factor is introduced to integrate into a newly designed network. We integrate our meta-updater into eight different types of online update trackers. Extensive experiments on four long-term and two short-term tracking benchmarks demonstrate that our meta-updater is effective and has strong generalization ability.},
  archive      = {J_TPAMI},
  author       = {Jie Zhao and Kenan Dai and Pengyu Zhang and Dong Wang and Huchuan Lu},
  doi          = {10.1109/TPAMI.2022.3202785},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6168-6182},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust online tracking with meta-updater},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust losses for learning value functions. <em>TPAMI</em>,
<em>45</em>(5), 6157–6167. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses—like the Huber loss—they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. We characterize the solutions of the robust losses, providing insight into the problem settings where the robust losses define notably better solutions than the mean squared Bellman error. Finally, we show that the resulting gradient-based algorithms are more stable, for both prediction and control, with less sensitivity to meta-parameters.},
  archive      = {J_TPAMI},
  author       = {Andrew Patterson and Victor Liao and Martha White},
  doi          = {10.1109/TPAMI.2022.3213503},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6157-6167},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust losses for learning value functions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Regularized multi-output gaussian convolution process with
domain adaptation. <em>TPAMI</em>, <em>45</em>(5), 6142–6156. (<a
href="https://doi.org/10.1109/TPAMI.2022.3205036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-output Gaussian process (MGP) has been attracting increasing attention as a transfer learning method to model multiple outputs. Despite its high flexibility and generality, MGP still faces two critical challenges when applied to transfer learning. The first one is negative transfer, which occurs when there exists no shared information among the outputs. The second challenge is the input domain inconsistency, which is commonly studied in transfer learning yet not explored in MGP. In this paper, we propose a regularized MGP modeling framework with domain adaptation to overcome these challenges. More specifically, a sparse covariance matrix of MGP is constructed using convolution process, where penalization terms are added to adaptively select the most informative outputs for knowledge transfer. To deal with the domain inconsistency, a domain adaptation method is proposed by marginalizing inconsistent features and expanding missing features to align the input domains among different outputs. Statistical properties of the proposed method are provided to guarantee the performance practically and asymptotically. The proposed framework outperforms state-of-the-art benchmarks in comprehensive simulation studies and one real case study of a ceramic manufacturing process. The results demonstrate the effectiveness of our method in dealing with both the negative transfer and the domain inconsistency.},
  archive      = {J_TPAMI},
  author       = {Xinming Wang and Chao Wang and Xuan Song and Levi Kirby and Jianguo Wu},
  doi          = {10.1109/TPAMI.2022.3205036},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6142-6156},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Regularized multi-output gaussian convolution process with domain adaptation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reformulating optical flow to solve image-based inverse
problems and quantify uncertainty. <em>TPAMI</em>, <em>45</em>(5),
6125–6141. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From meteorology to medical imaging and cell mechanics, many scientific domains use inverse problems (IPs) to extract physical measurements from image movement. To this end, motion estimation methods such as optical flow (OF) pre-process images into motion data to feed the IP, which then inverts for the measurements through a physical model. However, this combined OFIP pipeline exacerbates the ill-posedness inherent to each technique, propagating errors and preventing uncertainty quantification. We introduce a Bayesian PDE-constrained framework that transforms visual information directly into physical measurements in the context of probability distributions. The posterior mean is a constrained IP that tracks brightness while satisfying the physical model, thereby translating the aperture problem from the motion to the underlying physics; whereas the posterior covariance derives measurement error out of image noise. As we illustrate with traction force microscopy, our approach offers several advantages: more accurate reconstructions; unprecedented flexibility in experiment design (e.g., arbitrary boundary conditions); and the exclusivity of measurement error, central to empirical science, yet still unavailable under the OFIP strategy.},
  archive      = {J_TPAMI},
  author       = {Aleix Boquet-Pujadas and Jean-Christophe Olivo-Marin},
  doi          = {10.1109/TPAMI.2022.3202855},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6125-6141},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reformulating optical flow to solve image-based inverse problems and quantify uncertainty},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PrintsGAN: Synthetic fingerprint generator. <em>TPAMI</em>,
<em>45</em>(5), 6111–6124. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major impediment to researchers working in the area of fingerprint recognition is the lack of publicly available, large-scale, fingerprint datasets. The publicly available datasets that do exist contain very few identities and impressions per finger. This limits research on a number of topics, including e.g., using deep networks to learn fixed length fingerprint embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator capable of generating unique fingerprints along with multiple impressions for a given fingerprint. Using PrintsGAN, we synthesize a database of 525k fingerprints (35K distinct fingers, each with 15 impressions). Next, we show the utility of the PrintsGAN generated dataset by training a deep network to extract a fixed-length embedding from a fingerprint. In particular, an embedding model trained on our synthetic fingerprints and fine-tuned on a small number of publicly available real fingerprints (25K prints from NIST SD 302) obtains a TAR of 87.03\% @ FAR=0.01\% on the NIST SD4 database (a boost from TAR=73.37\% when only trained on NIST SD 302). Prevailing synthetic fingerprint generation methods do not enable such performance gains due to i) lack of realism or ii) inability to generate multiple impressions per finger. Our dataset is released to the public: https://biometrics.cse.msu.edu/Publications/Databases/MSU_PrintsGAN/ .},
  archive      = {J_TPAMI},
  author       = {Joshua James Engelsma and Steven Grosz and Anil K. Jain},
  doi          = {10.1109/TPAMI.2022.3204591},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6111-6124},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PrintsGAN: Synthetic fingerprint generator},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial convolution for padding, inpainting, and image
synthesis. <em>TPAMI</em>, <em>45</em>(5), 6096–6110. (<a
href="https://doi.org/10.1109/TPAMI.2022.3209702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial convolution weights convolutions with binary masks and renormalizes on valid pixels. It was originally proposed for image inpainting task because a corrupted image processed by a standard convolutional often leads to artifacts. Therefore, binary masks are constructed that define the valid and corrupted pixels, so that partial convolution results are only calculated based on valid pixels. It has been also used for conditional image synthesis task, so that when a scene is generated, convolution results of an instance depend only on the feature values that belong to the same instance. One of the unexplored applications for partial convolution is padding which is a critical component of modern convolutional networks. Common padding schemes make strong assumptions about how the padded data should be extrapolated. We show that these padding schemes impair model accuracy, whereas partial convolution based padding provides consistent improvements across a range of tasks. In this article, we review partial convolution applications under one framework. We conduct a comprehensive study of the partial convolution based padding on a variety of computer vision tasks, including image classification, 3D-convolution-based action recognition, and semantic segmentation. Our results suggest that partial convolution-based padding shows promising improvements over strong baselines.},
  archive      = {J_TPAMI},
  author       = {Guilin Liu and Aysegul Dundar and Kevin J. Shih and Ting-Chun Wang and Fitsum A. Reda and Karan Sapra and Zhiding Yu and Xiaodong Yang and Andrew Tao and Bryan Catanzaro},
  doi          = {10.1109/TPAMI.2022.3209702},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6096-6110},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partial convolution for padding, inpainting, and image synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameterized hamiltonian learning with quantum circuit.
<em>TPAMI</em>, <em>45</em>(5), 6086–6095. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamiltonian learning, as an important quantum machine learning technique, provides a significant approach for determining an accurate quantum system. This paper establishes parameterized Hamiltonian learning (PHL) and explores its application and implementation on quantum computers. A parameterized quantum circuit for Hamiltonian learning is first created by decomposing unitary operators to excite the system evolution. Then, a PHL algorithm is developed to prepare a specific Hamiltonian system by iteratively updating the gradient of the loss function about circuit parameters. Finally, the experiments are conducted on Origin Pilot, and it demonstrates that the PHL algorithm can deal with the image segmentation problem and provide a segmentation solution accurately. Compared with the classical Grabcut algorithm, the PHL algorithm eliminates the requirement of early manual intervention. It provides a new possibility for solving practical application problems with quantum devices, which also assists in solving increasingly complicated problems and supports a much wider range of application possibilities in the future.},
  archive      = {J_TPAMI},
  author       = {Jinjing Shi and Wenxuan Wang and Xiaoping Lou and Shichao Zhang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3203157},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6086-6095},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parameterized hamiltonian learning with quantum circuit},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the convergence of tsetlin machines for the XOR operator.
<em>TPAMI</em>, <em>45</em>(5), 6072–6085. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tsetlin Machine (TM) is a novel machine learning algorithm with several distinct properties, including transparent inference and learning using hardware-near building blocks. Although numerous papers explore the TM empirically, many of its properties have not yet been analyzed mathematically. In this article, we analyze the convergence of the TM when input is non-linearly related to output by the XOR-operator. Our analysis reveals that the TM, with just two conjunctive clauses, can converge almost surely to reproducing XOR, learning from training data over an infinite time horizon. Furthermore, the analysis shows how the hyper-parameter $T$ guides clause construction so that the clauses capture the distinct sub-patterns in the data. Our analysis of convergence for XOR thus lays the foundation for analyzing other more complex logical expressions. These analyses altogether, from a mathematical perspective, provide new insights on why TMs have obtained the state-of-the-art performance on several pattern recognition problems.},
  archive      = {J_TPAMI},
  author       = {Lei Jiao and Xuan Zhang and Ole-Christoffer Granmo and Kuruge Darshana Abeyrathna},
  doi          = {10.1109/TPAMI.2022.3203150},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6072-6085},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the convergence of tsetlin machines for the XOR operator},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-channel attention selection GANs for guided
image-to-image translation. <em>TPAMI</em>, <em>45</em>(5), 6055–6071.
(<a href="https://doi.org/10.1109/TPAMI.2022.3212915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel model named Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN) for guided image-to-image translation, where we translate an input image into another while respecting an external semantic guidance. The proposed SelectionGAN explicitly utilizes the semantic guidance information and consists of two stages. In the first stage, the input image and the conditional semantic guidance are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using the proposed multi-scale spatial pooling &amp; channel selection module and the multi-channel attention selection module. Moreover, uncertainty maps automatically learned from attention maps are used to guide the pixel loss for better network optimization. Exhaustive experiments on four challenging guided image-to-image translation tasks (face, hand, body, and street view) demonstrate that our SelectionGAN is able to generate significantly better results than the state-of-the-art methods. Meanwhile, the proposed framework and modules are unified solutions and can be applied to solve other generation tasks such as semantic image synthesis. The code is available at https://github.com/Ha0Tang/SelectionGAN .},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Philip H.S. Torr and Nicu Sebe},
  doi          = {10.1109/TPAMI.2022.3212915},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6055-6071},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-channel attention selection GANs for guided image-to-image translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MPED: Quantifying point cloud distortion based on multiscale
potential energy discrepancy. <em>TPAMI</em>, <em>45</em>(5), 6037–6054.
(<a href="https://doi.org/10.1109/TPAMI.2022.3213831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, in human vision tasks, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception task parameters, such as dense point cloud compression and enhancement. In machine vision tasks, a distortion quantification method usually serves as loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., sparse point cloud reconstruction, completion, and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable, and have low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show that the proposed MPED is superior to current methods on both human and machine perception tasks. Our code is available at https://github.com/Qi-Yangsjtu/MPED .},
  archive      = {J_TPAMI},
  author       = {Qi Yang and Yujie Zhang and Siheng Chen and Yiling Xu and Jun Sun and Zhan Ma},
  doi          = {10.1109/TPAMI.2022.3213831},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6037-6054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MPED: Quantifying point cloud distortion based on multiscale potential energy discrepancy},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing estimated risks on unlabeled data: A new
formulation for semi-supervised medical image segmentation.
<em>TPAMI</em>, <em>45</em>(5), 6021–6036. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised segmentation can be costly, particularly in applications of biomedical image analysis where large scale manual annotations from experts are generally too expensive to be available. Semi-supervised segmentation, able to learn from both the labeled and unlabeled images, could be an efficient and effective alternative for such scenarios. In this work, we propose a new formulation based on risk minimization, which makes full use of the unlabeled images. Different from most of the existing approaches which solely explicitly guarantee the minimization of prediction risks from the labeled training images, the new formulation also considers the risks on unlabeled images. Particularly, this is achieved via an unbiased estimator, based on which we develop a general framework for semi-supervised image segmentation. We validate this framework on three medical image segmentation tasks, namely cardiac segmentation on ACDC2017, optic cup and disc segmentation on REFUGE dataset and 3D whole heart segmentation on MM-WHS dataset. Results show that the proposed estimator is effective, and the segmentation method achieves superior performance and demonstrates great potential compared to the other state-of-the-art approaches. Our code and data will be released via https://zmiclab.github.io/projects.html , once the manuscript is accepted for publication.},
  archive      = {J_TPAMI},
  author       = {Fuping Wu and Xiahai Zhuang},
  doi          = {10.1109/TPAMI.2022.3215186},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6021-6036},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Minimizing estimated risks on unlabeled data: A new formulation for semi-supervised medical image segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-based cross-image contexts for weakly supervised
semantic segmentation. <em>TPAMI</em>, <em>45</em>(5), 6006–6020. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) trains segmentation models by only weak labels, aiming to save the burden of expensive pixel-level annotations. This paper tackles the WSSS problem of utilizing image-level labels as the weak supervision. Previous approaches address this problem by focusing on generating better pseudo-masks from weak labels to train the segmentation model. However, they generally only consider every single image and overlook the potential cross-image contexts. We emphasize that the cross-image contexts among a group of images can provide complementary information for each other to obtain better pseudo-masks. To effectively employ cross-image contexts, we develop an end-to-end cross-image context module containing a memory bank mechanism and a transformer-based cross-image attention module. The former extracts cross-image contexts online from the feature encodings of input images and stores them as the memory. The latter mines useful information from the memorized contexts to provide the original queries with additional information for better pseudo-mask generation. We conduct detailed experiments on the Pascal VOC 2012 and the COCO dataset to demonstrate the advantage of utilizing cross-image contexts. Besides, state-of-the-art performance is also achieved. Codes are available at https://github.com/js-fan/MCIC.git .},
  archive      = {J_TPAMI},
  author       = {Junsong Fan and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2022.3203402},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {6006-6020},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Memory-based cross-image contexts for weakly supervised semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCIBI++: Soft mining contextual information beyond image for
semantic segmentation. <em>TPAMI</em>, <em>45</em>(5), 5988–6005. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-occurrent visual pattern makes context aggregation become an essential paradigm for semantic segmentation. The existing studies focus on modeling the contexts within image while neglecting the valuable semantics of the corresponding category beyond image. To this end, we propose a novel soft mining contextual information beyond image paradigm named MCIBI++ to further boost the pixel-level representations. Specifically, we first set up a dynamically updated memory module to store the dataset-level distribution information of various categories and then leverage the information to yield the dataset-level category representations during network forward. After that, we generate a class probability distribution for each pixel representation and conduct the dataset-level context aggregation with the class probability distribution as weights. Finally, the original pixel representations are augmented with the aggregated dataset-level and the conventional image-level contextual information. Moreover, in the inference phase, we additionally design a coarse-to-fine iterative inference strategy to further boost the segmentation results. MCIBI++ can be effortlessly incorporated into the existing segmentation frameworks and bring consistent performance improvements. Also, MCIBI++ can be extended into the video semantic segmentation framework with considerable improvements over the baseline. Equipped with MCIBI++, we achieved the state-of-the-art performance on seven challenging image or video semantic segmentation benchmarks.},
  archive      = {J_TPAMI},
  author       = {Zhenchao Jin and Dongdong Yu and Zehuan Yuan and Lequan Yu},
  doi          = {10.1109/TPAMI.2022.3206106},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5988-6005},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MCIBI++: Soft mining contextual information beyond image for semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). MaxMatch: Semi-supervised learning with worst-case
consistency. <em>TPAMI</em>, <em>45</em>(5), 5970–5987. (<a
href="https://doi.org/10.1109/TPAMI.2022.3208419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, great progress has been made to incorporate unlabeled data to overcome the inefficiently supervised problem via semi-supervised learning (SSL). Most state-of-the-art models are based on the idea of pursuing consistent model predictions over unlabeled data toward the input noise, which is called consistency regularization . Nonetheless, there is a lack of theoretical insights into the reason behind its success. To bridge the gap between theoretical and practical results, we propose a worst-case consistency regularization technique for SSL in this article. Specifically, we first present a generalization bound for SSL consisting of the empirical loss terms observed on labeled and unlabeled training data separately. Motivated by this bound, we derive an SSL objective that minimizes the largest inconsistency between an original unlabeled sample and its multiple augmented variants. We then provide a simple but effective algorithm to solve the proposed minimax problem, and theoretically prove that it converges to a stationary point. Experiments on five popular benchmark datasets validate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Yangbangyan Jiang and Xiaodan Li and Yuefeng Chen and Yuan He and Qianqian Xu and Zhiyong Yang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3208419},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5970-5987},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MaxMatch: Semi-supervised learning with worst-case consistency},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning with nested scene modeling and cooperative
architecture search for low-light vision. <em>TPAMI</em>,
<em>45</em>(5), 5953–5969. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured from low-light scenes often suffer from severe degradations, including low visibility, color casts, intensive noises, etc. These factors not only degrade image qualities, but also affect the performance of downstream Low-Light Vision (LLV) applications. A variety of deep networks have been proposed to enhance the visual quality of low-light images. However, they mostly rely on significant architecture engineering and often suffer from the high computational burden. More importantly, it still lacks an efficient paradigm to uniformly handle various tasks in the LLV scenarios. To partially address the above issues, we establish Retinex-inspired Unrolling with Architecture Search (RUAS), a general learning framework, that can address low-light enhancement task, and has the flexibility to handle other challenging downstream vision tasks. Specifically, we first establish a nested optimization formulation, together with an unrolling strategy, to explore underlying principles of a series of LLV tasks. Furthermore, we design a differentiable strategy to cooperatively search specific scene and task architectures for RUAS. Last but not least, we demonstrate how to apply RUAS for both low- and high-level LLV applications (e.g., enhancement, detection and segmentation). Extensive experiments verify the flexibility, effectiveness, and efficiency of RUAS.},
  archive      = {J_TPAMI},
  author       = {Risheng Liu and Long Ma and Tengyu Ma and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TPAMI.2022.3212995},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5953-5969},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning with nested scene modeling and cooperative architecture search for low-light vision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to optimize on riemannian manifolds.
<em>TPAMI</em>, <em>45</em>(5), 5935–5952. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many learning tasks are modeled as optimization problems with nonlinear constraints, such as principal component analysis and fitting a Gaussian mixture model. A popular way to solve such problems is resorting to Riemannian optimization algorithms, which yet heavily rely on both human involvement and expert knowledge about Riemannian manifolds. In this paper, we propose a Riemannian meta-optimization method to automatically learn a Riemannian optimizer. We parameterize the Riemannian optimizer by a novel recurrent network and utilize Riemannian operations to ensure that our method is faithful to the geometry of manifolds. The proposed method explores the distribution of the underlying data by minimizing the objective of updated parameters, and hence is capable of learning task-specific optimizations. We introduce a Riemannian implicit differentiation training scheme to achieve efficient training in terms of numerical stability and computational cost. Unlike conventional meta-optimization training schemes that need to differentiate through the whole optimization trajectory, our training scheme is only related to the final two optimization steps. In this way, our training scheme avoids the exploding gradient problem, and significantly reduces the computational load and memory footprint. We discuss experimental results across various constrained problems, including principal component analysis on Grassmann manifolds, face recognition, person re-identification, and texture image classification on Stiefel manifolds, clustering and similarity learning on symmetric positive definite manifolds, and few-shot learning on hyperbolic manifolds.},
  archive      = {J_TPAMI},
  author       = {Zhi Gao and Yuwei Wu and Xiaomeng Fan and Mehrtash Harandi and Yunde Jia},
  doi          = {10.1109/TPAMI.2022.3215702},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5935-5952},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to optimize on riemannian manifolds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to discriminate information for online action
detection: Analysis and application. <em>TPAMI</em>, <em>45</em>(5),
5918–5934. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online action detection, which aims to identify an ongoing action from a streaming video, is an important subject in real-world applications. For this task, previous methods use recurrent neural networks for modeling temporal relations in an input sequence. However, these methods overlook the fact that the input image sequence includes not only the action of interest but background and irrelevant actions. This would induce recurrent units to accumulate unnecessary information for encoding features on the action of interest. To overcome this problem, we propose a novel recurrent unit, named Information Discrimination Unit (IDU), which explicitly discriminates the information relevancy between an ongoing action and others to decide whether to accumulate the input information. This enables learning more discriminative representations for identifying an ongoing action. In this paper, we further present a new recurrent unit, called Information Integration Unit (IIU), for action anticipation. Our IIU exploits the outputs from IDN as pseudo action labels as well as RGB frames to learn enriched features of observed actions effectively. In experiments on TVSeries and THUMOS-14, the proposed methods outperform state-of-the-art methods by a significant margin in online action detection and action anticipation. Moreover, we demonstrate the effectiveness of the proposed units by conducting comprehensive ablation studies.},
  archive      = {J_TPAMI},
  author       = {Sumin Lee and Hyunjun Eun and Jinyoung Moon and Seokeon Choi and Yoonhyung Kim and Chanho Jung and Changick Kim},
  doi          = {10.1109/TPAMI.2022.3204808},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5918-5934},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to discriminate information for online action detection: Analysis and application},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning dual memory dictionaries for blind face
restoration. <em>TPAMI</em>, <em>45</em>(5), 5904–5917. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind face restoration is a challenging task due to the unknown, unsynthesizable and complex degradation, yet is valuable in many practical applications. To improve the performance of blind face restoration, recent works mainly treat the two aspects, i.e., generic and specific restoration, separately. In particular, generic restoration attempts to restore the results through general facial structure prior, while on the one hand, cannot generalize to real-world degraded observations due to the limited capability of direct CNNs’ mappings in learning blind restoration, and on the other hand, fails to exploit the identity-specific details. On the contrary, specific restoration aims to incorporate the identity features from the reference of the same identity, in which the requirement of proper reference severely limits the application scenarios. Generally, it is a challenging and intractable task to improve the photo-realistic performance of blind restoration and adaptively handle the generic and specific restoration scenarios with a single unified model. Instead of implicitly learning the mapping from a low-quality image to its high-quality counterpart, this paper suggests a DMDNet by explicitly memorizing the generic and specific features through dual dictionaries. First, the generic dictionary learns the general facial priors from high-quality images of any identity, while the specific dictionary stores the identity-belonging features for each person individually. Second, to handle the degraded input with or without specific reference, dictionary transform module is suggested to read the relevant details from the dual dictionaries which are subsequently fused into the input features. Finally, multi-scale dictionaries are leveraged to benefit the coarse-to-fine restoration. The whole framework including the generic and specific dictionaries is optimized in an end-to-end manner and can be flexibly plugged into different application scenarios. Moreover, a new high-quality dataset, termed CelebRef-HQ, is constructed to promote the exploration of specific face restoration in the high-resolution space. Experimental results demonstrate that the proposed DMDNet performs favorably against the state of the arts in both quantitative and qualitative evaluation, and generates more photo-realistic results on the real-world low-quality images. The codes, models and the CelebRef-HQ dataset will be publicly available at https://github.com/csxmli2016/DMDNet .},
  archive      = {J_TPAMI},
  author       = {Xiaoming Li and Shiguang Zhang and Shangchen Zhou and Lei Zhang and Wangmeng Zuo},
  doi          = {10.1109/TPAMI.2022.3215251},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5904-5917},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning dual memory dictionaries for blind face restoration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learn from unpaired data for image restoration: A
variational bayes approach. <em>TPAMI</em>, <em>45</em>(5), 5889–5903.
(<a href="https://doi.org/10.1109/TPAMI.2022.3215571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising, super-resolution, and low-light image enhancement tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other approaches.},
  archive      = {J_TPAMI},
  author       = {Dihan Zheng and Xiaowen Zhang and Kaisheng Ma and Chenglong Bao},
  doi          = {10.1109/TPAMI.2022.3215571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5889-5903},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learn from unpaired data for image restoration: A variational bayes approach},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel-based generalized median computation for consensus
learning. <em>TPAMI</em>, <em>45</em>(5), 5872–5888. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing a consensus object from a set of given objects is a core problem in machine learning and pattern recognition. One popular approach is to formulate it as an optimization problem using the generalized median. Previous methods like the Prototype and Distance-Preserving Embedding methods transform objects into a vector space, solve the generalized median problem in this space, and inversely transform back into the original space. Both of these methods have been successfully applied to a wide range of object domains, where the generalized median problem has inherent high computational complexity (typically $\mathcal {NP}$ -hard) and therefore approximate solutions are required. Previously, explicit embedding methods were used in the computation, which often do not reflect the spatial relationship between objects exactly. In this work we introduce a kernel-based generalized median framework that is applicable to both positive definite and indefinite kernels. This framework computes the relationship between objects and its generalized median in kernel space, without the need of an explicit embedding. We show that the spatial relationship between objects is more accurately represented in kernel space than in an explicit vector space using easy-to-compute kernels, and demonstrate superior performance of generalized median computation on datasets of three different domains. A software toolbox resulting from our work is made publicly available to encourage other researchers to explore the generalized median computation and applications.},
  archive      = {J_TPAMI},
  author       = {Andreas Nienkötter and Xiaoyi Jiang},
  doi          = {10.1109/TPAMI.2022.3202565},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5872-5888},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Kernel-based generalized median computation for consensus learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit annealing in kernel spaces: A strongly consistent
clustering approach. <em>TPAMI</em>, <em>45</em>(5), 5862–5871. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel $k$ -means clustering is a powerful tool for unsupervised learning of non-linearly separable data. Its merits are thoroughly validated on a suite of simulated datasets and real data benchmarks that feature nonlinear and multi-view separation. Since the earliest attempts, researchers have noted that such algorithms often become trapped by local minima arising from the non-convexity of the underlying objective function. In this paper, we generalize recent results leveraging a general family of means to combat sub-optimal local solutions to the kernel and multi-kernel settings. Called Kernel Power $k$ -Means, our algorithm uses majorization-minimization (MM) to better solve this non-convex problem. We show that the method implicitly performs annealing in kernel feature space while retaining efficient, closed-form updates. We rigorously characterize its convergence properties both from computational and statistical points of view. In particular, we characterize the large sample behavior of the proposed method by establishing strong consistency guarantees as well as finite-sample bounds on the excess risk of the estimates through modern tools in learning theory. The proposal&#39;s efficacy is demonstrated through an array of simulated and real data experiments.},
  archive      = {J_TPAMI},
  author       = {Debolina Paul and Saptarshi Chakraborty and Swagatam Das and Jason Xu},
  doi          = {10.1109/TPAMI.2022.3217137},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5862-5871},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Implicit annealing in kernel spaces: A strongly consistent clustering approach},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HydraMarker: Efficient, flexible, and multifold marker field
generation. <em>TPAMI</em>, <em>45</em>(5), 5849–5861. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An n -order marker field is a special binary matrix whose n × n subregions are all distinct from each other in four orientations. It is commonly used to guide the composing process of position-sensing markers, which can be detected and identified in a camera image with very limited scope or severe visibility problems. Despite the advantages, position-sensing markers are rare and overlooked because generating marker fields is difficult. In this article, we broaden the definition of marker field, making it more powerful and flexible. Then, we propose bWFC (binary wave function collapse) and its high-speed version, fast-bWFC, to solve the generation problem. The methods are packaged into an open-sourced toolkit named HydraMarker, with which, users not only can generate marker fields on laptops within a short period of time, but also can highly customize them: preset values; fields and subregions in any shape; multifold local uniqueness. Comparative results indicate that the proposed method has superior efficiency, quality, and capability. It makes marker field generation accessible to common marker designers, opening up more possibilities for fiducial markers.},
  archive      = {J_TPAMI},
  author       = {Mingzhu Zhu and Bingwei He and Junzhi Yu and Fusong Yuan and Jiantao Liu},
  doi          = {10.1109/TPAMI.2022.3212862},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5849-5861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HydraMarker: Efficient, flexible, and multifold marker field generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural networks in network neuroscience.
<em>TPAMI</em>, <em>45</em>(5), 5833–5848. (<a
href="https://doi.org/10.1109/TPAMI.2022.3209686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain –namely brain graph. Relying on its non-euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience .},
  archive      = {J_TPAMI},
  author       = {Alaa Bessadok and Mohamed Ali Mahjoub and Islem Rekik},
  doi          = {10.1109/TPAMI.2022.3209686},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5833-5848},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph neural networks in network neuroscience},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geodesic-based bayesian coherent point drift.
<em>TPAMI</em>, <em>45</em>(5), 5816–5832. (<a
href="https://doi.org/10.1109/TPAMI.2022.3214191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherent point drift is a well-known algorithm for non-rigid registration, i.e., a procedure for deforming a shape to match another shape. Despite its prevalence, the algorithm has a major drawback that remains unsolved: It unnaturally deforms the different parts of a shape, e.g., human legs, when they are neighboring each other. The inappropriate deformations originate from a proximity-based deformation constraint, called motion coherence. This study proposes a non-rigid registration method that addresses the drawback. The key to solving the problem is to redefine the motion coherence using a geodesic, i.e., the shortest route between points on a shape&#39;s surface. We also propose the accelerated variant of the registration method. In numerical studies, we demonstrate that the algorithms can circumvent the drawback of coherent point drift. We also show that the accelerated algorithm can be applied to shapes comprising several millions of points.},
  archive      = {J_TPAMI},
  author       = {Osamu Hirose},
  doi          = {10.1109/TPAMI.2022.3214191},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5816-5832},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geodesic-based bayesian coherent point drift},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating hypergraph-based high-order representations of
whole-slide histopathological images for survival prediction.
<em>TPAMI</em>, <em>45</em>(5), 5800–5815. (<a
href="https://doi.org/10.1109/TPAMI.2022.3209652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient survival prediction based on gigapixel whole-slide histopathological images (WSIs) has become increasingly prevalent in recent years. A key challenge of this task is achieving an informative survival-specific global representation from those WSIs with highly complicated data correlation. This article proposes a multi-hypergraph based learning framework, called “HGSurvNet,” to tackle this challenge. HGSurvNet achieves an effective high-order global representation of WSIs via multilateral correlation modeling in multiple spaces and a general hypergraph convolution network. It has the ability to alleviate over-fitting issues caused by the lack of training data by using a new convolution structure called hypergraph max-mask convolution. Extensive validation experiments were conducted on three widely-used carcinoma datasets: Lung Squamous Cell Carcinoma (LUSC), Glioblastoma Multiforme (GBM), and National Lung Screening Trial (NLST). Quantitative analysis demonstrated that the proposed method consistently outperforms state-of-the-art methods, coupled with the Bayesian Concordance Readjust loss. We also demonstrate the individual effectiveness of each module of the proposed framework and its application potential for pathology diagnosis and reporting empowered by its interpretability potential.},
  archive      = {J_TPAMI},
  author       = {Donglin Di and Changqing Zou and Yifan Feng and Haiyan Zhou and Rongrong Ji and Qionghai Dai and Yue Gao},
  doi          = {10.1109/TPAMI.2022.3209652},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5800-5815},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generating hypergraph-based high-order representations of whole-slide histopathological images for survival prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainability in graph neural networks: A taxonomic survey.
<em>TPAMI</em>, <em>45</em>(5), 5782–5799. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.},
  archive      = {J_TPAMI},
  author       = {Hao Yuan and Haiyang Yu and Shurui Gui and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2022.3204236},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5782-5799},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Explainability in graph neural networks: A taxonomic survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact decomposition of joint low rankness and local
smoothness plus sparse matrices. <em>TPAMI</em>, <em>45</em>(5),
5766–5781. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that the decomposition in low-rank and sparse matrices ( L+S for short) can be achieved by several Robust PCA techniques. Besides the low rankness, the local smoothness ( LSS ) is a vitally essential prior for many real-world matrix data such as hyperspectral images and surveillance videos, which makes such matrices have low-rankness and local smoothness property at the same time. This poses an interesting question: Can we make a matrix decomposition in terms of L&amp;amp;LSS +S form exactly? To address this issue, we propose in this paper a new RPCA model based on three-dimensional correlated total variation regularization (3DCTV-RPCA for short) by fully exploiting and encoding the prior expression underlying such joint low-rank and local smoothness matrices. Specifically, using a modification of Golfing scheme, we prove that under some mild assumptions, the proposed 3DCTV-RPCA model can decompose both components exactly, which should be the first theoretical guarantee among all such related methods combining low rankness and local smoothness. In addition, by utilizing Fast Fourier Transform (FFT), we propose an efficient ADMM algorithm with a solid convergence guarantee for solving the resulting optimization problem. Finally, a series of experiments on both simulations and real applications are carried out to demonstrate the general validity of the proposed 3DCTV-RPCA model.},
  archive      = {J_TPAMI},
  author       = {Jiangjun Peng and Yao Wang and Hongying Zhang and Jianjun Wang and Deyu Meng},
  doi          = {10.1109/TPAMI.2022.3204203},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5766-5781},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exact decomposition of joint low rankness and local smoothness plus sparse matrices},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient 3D deep LiDAR odometry. <em>TPAMI</em>,
<em>45</em>(5), 5749–5765. (<a
href="https://doi.org/10.1109/TPAMI.2022.3207015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient 3D point cloud learning architecture, named EfficientLO-Net, for LiDAR odometry is first proposed in this article. In this architecture, the projection-aware representation of the 3D point cloud is proposed to organize the raw 3D point cloud into an ordered data form to achieve efficiency. The Pyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task is built to estimate and refine the pose in a coarse-to-fine approach. A projection-aware attentive cost volume is built to directly associate two discrete point clouds and obtain embedding motion patterns. Then, a trainable embedding mask is proposed to weigh the local motion patterns to regress the overall pose and filter outlier points. The trainable pose warp-refinement module is iteratively used with embedding mask optimized hierarchically to make the pose estimation more robust for outliers. The entire architecture is holistically optimized end-to-end to achieve adaptive learning of cost volume and mask, and all operations involving point cloud sampling and grouping are accelerated by projection-aware 3D feature learning methods. The superior performance and effectiveness of our LiDAR odometry architecture are demonstrated on KITTI, M2DGR, and Argoverse datasets. Our method outperforms all recent learning-based methods and even the geometry-based approach, LOAM with mapping optimization, on most sequences of KITTI odometry dataset. We open sourced our codes at: https://github.com/IRMVLab/EfficientLO-Net .},
  archive      = {J_TPAMI},
  author       = {Guangming Wang and Xinrui Wu and Shuyang Jiang and Zhe Liu and Hesheng Wang},
  doi          = {10.1109/TPAMI.2022.3207015},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5749-5765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient 3D deep LiDAR odometry},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic self-supervised teacher-student network learning.
<em>TPAMI</em>, <em>45</em>(5), 5731–5748. (<a
href="https://doi.org/10.1109/TPAMI.2022.3220928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong learning (LLL) represents the ability of an artificial intelligence system to learn successively a sequence of different databases. In this paper we introduce the Dynamic Self-Supervised Teacher-Student Network (D-TS), representing a more general LLL framework, where the Teacher is implemented as a dynamically expanding mixture model which automatically increases its capacity to deal with a growing number of tasks. We propose the Knowledge Discrepancy Score (KDS) criterion for measuring the relevance of the incoming information characterizing a new task when compared to the existing knowledge accumulated by the Teacher module from its previous training. The KDS ensures a light Teacher architecture while also enabling to reuse the learned knowledge whenever appropriate, accelerating the learning of given tasks. The Student module is implemented as a lightweight probabilistic generative model. We introduce a novel self-supervised learning procedure for the Student that allows to capture cross-domain latent representations from the entire knowledge accumulated by the Teacher as well as from novel data. We perform several experiments which show that D-TS can achieve the state of the art results in LLL while requiring fewer parameters than other methods.},
  archive      = {J_TPAMI},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1109/TPAMI.2022.3220928},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5731-5748},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic self-supervised teacher-student network learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Dynamic graph message passing networks. <em>TPAMI</em>,
<em>45</em>(5), 5712–5730. (<a
href="https://doi.org/10.1109/TPAMI.2022.3207500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling long-range dependencies is critical for scene understanding tasks in computer vision. Although convolution neural networks (CNNs) have excelled in many vision tasks, they are still limited in capturing long-range structured relationships as they typically consist of layers of local kernels. A fully-connected graph, such as the self-attention operation in Transformers, is beneficial for such modelling, however, its computational overhead is prohibitive. In this paper, we propose a dynamic graph message passing network, that significantly reduces the computational complexity compared to related works modelling a fully-connected graph. This is achieved by adaptively sampling nodes in the graph, conditioned on the input, for message passing. Based on the sampled nodes, we dynamically predict node-dependent filter weights and the affinity matrix for propagating information between them. This formulation allows us to design a self-attention module, and more importantly a new Transformer-based backbone network, that we use for both image classification pretraining, and for addressing various downstream tasks (e.g. object detection, instance and semantic segmentation). Using this model, we show significant improvements with respect to strong, state-of-the-art baselines on four different tasks. Our approach also outperforms fully-connected graphs while using substantially fewer floating-point operations and parameters. Code and models will be made publicly available at https://github.com/fudan-zvg/DGMN2 .},
  archive      = {J_TPAMI},
  author       = {Li Zhang and Mohan Chen and Anurag Arnab and Xiangyang Xue and Philip H. S. Torr},
  doi          = {10.1109/TPAMI.2022.3207500},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5712-5730},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic graph message passing networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic convolution for 3D point cloud instance
segmentation. <em>TPAMI</em>, <em>45</em>(5), 5697–5711. (<a
href="https://doi.org/10.1109/TPAMI.2022.3216926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we come up with a simple yet effective approach for instance segmentation on 3D point cloud with strong robustness. Previous top-performing methods for this task adopt a bottom-up strategy, which often involves various inefficient operations or complex pipelines, such as grouping over-segmented components, introducing heuristic post-processing steps, and designing complex loss functions. As a result, the inevitable variations of the instances sizes make it vulnerable and sensitive to the values of pre-defined hyper-parameters. To this end, we instead propose a novel pipeline that applies dynamic convolution to generate instance-aware parameters in response to the characteristics of the instances. The representation capability of the parameters is greatly improved by gathering homogeneous points that have identical semantic categories and close votes for the geometric centroids. Instances are then decoded via several simple convolution layers, where the parameters are generated depending on the input. In addition, to introduce a large context and maintain limited computational overheads, a light-weight transformer is built upon the bottleneck layer to capture the long-range dependencies. With the only post-processing step, non-maximum suppression (NMS), we demonstrate a simpler and more robust approach that achieves promising performance on various datasets: ScanNetV2, S3DIS, and PartNet. The consistent improvements on both voxel- and point-based architectures imply the effectiveness of the proposed method. Code is available at: https://git.io/DyCo3D .},
  archive      = {J_TPAMI},
  author       = {Tong He and Chunhua Shen and Anton van den Hengel},
  doi          = {10.1109/TPAMI.2022.3216926},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5697-5711},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic convolution for 3D point cloud instance segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Drinking from a firehose: Continual learning with web-scale
natural language. <em>TPAMI</em>, <em>45</em>(5), 5684–5696. (<a
href="https://doi.org/10.1109/TPAMI.2022.3218265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning systems will interact with humans, with each other, and with the physical world through time – and continue to learn and adapt as they do. An important open problem for continual learning is a large-scale benchmark which enables realistic evaluation of algorithms. In this paper, we study a natural setting for continual learning on a massive scale. We introduce the problem of personalized online language learning (POLL), which involves fitting personalized language models to a population of users that evolves over time. To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, Firehose10 M and Firehose100 M, comprise 100 million tweets, posted by one million users over six years. Enabled by the Firehose datasets, we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. Based on this analysis, we develop a simple algorithm for continual gradient descent (ConGraD) that outperforms prior continual learning methods on the Firehose datasets as well as earlier benchmarks. Collectively, the POLL problem setting, the Firehose datasets, and the ConGraD algorithm enable a complete benchmark for reproducible research on web-scale continual learning.},
  archive      = {J_TPAMI},
  author       = {Hexiang Hu and Ozan Sener and Fei Sha and Vladlen Koltun},
  doi          = {10.1109/TPAMI.2022.3218265},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5684-5696},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Drinking from a firehose: Continual learning with web-scale natural language},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth restoration in under-display time-of-flight imaging.
<em>TPAMI</em>, <em>45</em>(5), 5668–5683. (<a
href="https://doi.org/10.1109/TPAMI.2022.3209905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under-display imaging has recently received considerable attention in both academia and industry. As a variation of this technique, under-display ToF (UD-ToF) cameras enable depth sensing for full-screen devices. However, it also brings problems of image blurring, signal-to-noise ratio and ranging accuracy reduction. To address these issues, we propose a cascaded deep network to improve the quality of UD-ToF depth maps. The network comprises two subnets, with the first using a complex-valued network in raw domain to perform denoising, deblurring and raw measurements enhancement jointly, while the second refining depth maps in depth domain based on the proposed multi-scale depth enhancement block (MSDEB). To enable training, we establish a data acquisition device and construct a real UD-ToF dataset by collecting real paired ToF raw data. Besides, we also build a large-scale synthetic UD-ToF dataset through noise analysis. The quantitative and qualitative evaluation results on public datasets and ours demonstrate that the presented network outperforms state-of-the-art algorithms and can further promote full-screen devices in practical applications.},
  archive      = {J_TPAMI},
  author       = {Xin Qiao and Chenyang Ge and Pengchao Deng and Hao Wei and Matteo Poggi and Stefano Mattoccia},
  doi          = {10.1109/TPAMI.2022.3209905},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5668-5683},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Depth restoration in under-display time-of-flight imaging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Defensive few-shot learning. <em>TPAMI</em>,
<em>45</em>(5), 5649–5667. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a new challenging problem called defensive few-shot learning in order to learn a robust few-shot model against adversarial attacks. Simply applying the existing adversarial defense methods to few-shot learning cannot effectively solve this problem. This is because the commonly assumed sample-level distribution consistency between the training and test sets can no longer be met in the few-shot setting. To address this situation, we develop a general defensive few-shot learning (DFSL) framework to answer the following two key questions: (1) how to transfer adversarial defense knowledge from one sample distribution to another? (2) how to narrow the distribution gap between clean and adversarial examples under the few-shot setting? To answer the first question, we propose an episode-based adversarial training mechanism by assuming a task-level distribution consistency to better transfer the adversarial defense knowledge. As for the second question, within each few-shot task, we design two kinds of distribution consistency criteria to narrow the distribution gap between clean and adversarial examples from the feature-wise and prediction-wise perspectives, respectively. Extensive experiments demonstrate that the proposed framework can effectively make the existing few-shot models robust against adversarial attacks. Code is available at https://github.com/WenbinLee/DefensiveFSL.git .},
  archive      = {J_TPAMI},
  author       = {Wenbin Li and Lei Wang and Xingxing Zhang and Lei Qi and Jing Huo and Yang Gao and Jiebo Luo},
  doi          = {10.1109/TPAMI.2022.3213755},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5649-5667},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Defensive few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepEMD: Differentiable earth mover’s distance for few-shot
learning. <em>TPAMI</em>, <em>45</em>(5), 5632–5648. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover&#39;s Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement $k$ -shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experiments validate the effectiveness of our algorithm which outperforms state-of-the-art methods by a significant margin on five widely used few-shot classification benchmarks, namely, miniImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB), and CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our method on the image retrieval task in our experiments.},
  archive      = {J_TPAMI},
  author       = {Chi Zhang and Yujun Cai and Guosheng Lin and Chunhua Shen},
  doi          = {10.1109/TPAMI.2022.3217373},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5632-5648},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepEMD: Differentiable earth mover&#39;s distance for few-shot learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for face anti-spoofing: A survey.
<em>TPAMI</em>, <em>45</em>(5), 5609–5631. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) has lately attracted increasing attention due to its vital role in securing face recognition systems from presentation attacks (PAs). As more and more realistic PAs with novel types spring up, early-stage FAS methods based on handcrafted features become unreliable due to their limited representation capacity. With the emergence of large-scale academic datasets in the recent decade, deep learning based FAS achieves remarkable performance and dominates this area. However, existing reviews in this field mainly focus on the handcrafted features, which are outdated and uninspiring for the progress of FAS community. In this paper, to stimulate future research, we present the first comprehensive review of recent advances in deep learning based FAS. It covers several novel and insightful components: 1) besides supervision with binary label (e.g., ‘0’ for bonafide versus ‘1’ for PAs), we also investigate recent methods with pixel-wise supervision (e.g., pseudo depth map); 2) in addition to traditional intra-dataset evaluation, we collect and analyze the latest methods specially designed for domain generalization and open-set FAS; and 3) besides commercial RGB camera, we summarize the deep learning applications under multi-modal (e.g., depth and infrared) or specialized (e.g., light field and flash) sensors. We conclude this survey by emphasizing current open issues and highlighting potential prospects.},
  archive      = {J_TPAMI},
  author       = {Zitong Yu and Yunxiao Qin and Xiaobai Li and Chenxu Zhao and Zhen Lei and Guoying Zhao},
  doi          = {10.1109/TPAMI.2022.3215850},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5609-5631},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning for face anti-spoofing: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep discriminative feature models (DDFMs) for set based
face recognition and distance metric learning. <em>TPAMI</em>,
<em>45</em>(5), 5594–5608. (<a
href="https://doi.org/10.1109/TPAMI.2022.3205939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces two methods that find compact deep feature models for approximating images in set based face recognition problems. The proposed method treats each image set as a nonlinear face manifold that is composed of linear components. To find linear components of the face manifold, we first split image sets into subsets containing face images which share similar appearances. Then, our first proposed method approximates each subset by using the center of the deep feature representations of images in those subsets. Centers modeling the subsets are learned by using distance metric learning. The second proposed method uses discriminative common vectors to represent image features in the subsets, and entire subset is approximated with an affine hull in this approach. Discriminative common vectors are subset centers that are projected onto a new feature space where the combined within-class variances coming from all subsets are removed. Our proposed methods can also be considered as distance metric learning methods using triplet loss function where the learned subcluster centers are the selected anchors. This procedure yields to applying distance metric learning to quantized data and brings many advantages over using classical distance metric learning methods. We tested proposed methods on various face recognition problems using image sets and some visual object classification problems. Experimental results show that the proposed methods achieve the state-of-the-art accuracies on the most of the tested image datasets.},
  archive      = {J_TPAMI},
  author       = {Bedirhan Uzun and Hakan Cevikalp and Hasan Saribas},
  doi          = {10.1109/TPAMI.2022.3205939},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5594-5608},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep discriminative feature models (DDFMs) for set based face recognition and distance metric learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cycle registration in persistent homology with applications
in topological bootstrap. <em>TPAMI</em>, <em>45</em>(5), 5579–5593. (<a
href="https://doi.org/10.1109/TPAMI.2022.3217443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for comparing the persistent homology representations of two spaces (or filtrations). Commonly used methods are based on numerical summaries such as persistence diagrams and persistence landscapes, along with suitable metrics (e.g., Wasserstein). These summaries are useful for computational purposes, but they are merely a marginal of the actual topological information that persistent homology can provide. Instead, our approach compares between two topological representations directly in the data space. We do so by defining a correspondence relation between individual persistent cycles of two different spaces, and devising a method for computing this correspondence. Our matching of cycles is based on both the persistence intervals and the spatial placement of each feature. We demonstrate our new framework in the context of topological inference, where we use statistical bootstrap methods in order to differentiate between real features and noise in point cloud data.},
  archive      = {J_TPAMI},
  author       = {Yohai Reani and Omer Bobrowski},
  doi          = {10.1109/TPAMI.2022.3217443},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5579-5593},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cycle registration in persistent homology with applications in topological bootstrap},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CRIC: A VQA dataset for compositional reasoning on vision
and commonsense. <em>TPAMI</em>, <em>45</em>(5), 5561–5578. (<a
href="https://doi.org/10.1109/TPAMI.2022.3210780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternatively inferring on the visual facts and commonsense is fundamental for an advanced visual question answering (VQA) system. This ability requires models to go beyond the literal understanding of commonsense. The system should not just treat objects as the entrance to query background knowledge, but fully ground commonsense to the visual world and imagine the possible relationships between objects, e.g., “fork, can lift, food”. To comprehensively evaluate such abilities, we propose a VQA benchmark, C ompositional R easoning on v I sion and C ommonsense(CRIC), which introduces new types of questions about CRIC, and an evaluation metric integrating the correctness of answering and commonsense grounding. To collect such questions and rich additional annotations to support the metric, we also propose an automatic algorithm to generate question samples from the scene graph associated with the images and the relevant knowledge graph. We further analyze several representative types of VQA models on the CRIC dataset. Experimental results show that grounding the commonsense to the image region and joint reasoning on vision and commonsense are still challenging for current approaches. The dataset is available at https://cricvqa.github.io .},
  archive      = {J_TPAMI},
  author       = {Difei Gao and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2022.3210780},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5561-5578},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CRIC: A VQA dataset for compositional reasoning on vision and commonsense},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning with stronger augmentations.
<em>TPAMI</em>, <em>45</em>(5), 5549–5560. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning has significantly been developed with the advance of contrastive learning methods. Most of those methods are benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. However, those carefully designed transformations limited us to further explore the novel patterns exposed by other transformations. Meanwhile, as shown in our experiments, direct contrastive learning for stronger augmented images can not learn representations effectively. Thus, we propose a general framework called Contrastive Learning with Stronger Augmentations (CLSA) to complement current contrastive learning approaches. Here, the distribution divergence between the weakly and strongly augmented images over the representation bank is adopted to supervise the retrieval of strongly augmented queries from a pool of instances. Experiments on the ImageNet dataset and downstream datasets showed the information from the strongly augmented images can significantly boost the performance. For example, CLSA achieves top-1 accuracy of 76.2\% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, which is almost the same level as 76.5\% of supervised results.},
  archive      = {J_TPAMI},
  author       = {Xiao Wang and Guo-Jun Qi},
  doi          = {10.1109/TPAMI.2022.3203630},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5549-5560},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive learning with stronger augmentations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous-time fitted value iteration for robust policies.
<em>TPAMI</em>, <em>45</em>(5), 5534–5548. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the Hamilton-Jacobi-Bellman equation is important in many domains including control, robotics and economics. Especially for continuous control, solving this differential equation and its extension the Hamilton-Jacobi-Isaacs equation, is important as it yields the optimal policy that achieves the maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs equation, which includes an adversary controlling the environment and minimizing the reward, the obtained policy is also robust to perturbations of the dynamics. In this paper we propose continuous fitted value iteration (cFVI) and robust fitted value iteration (rFVI). These algorithms leverage the non-linear control-affine dynamics and separable state and action reward of many continuous control problems to derive the optimal policy and optimal adversary in closed form. This analytic expression simplifies the differential equations and enables us to solve for the optimal value function using value iteration for continuous actions and states as well as the adversarial case. Notably, the resulting algorithms do not require discretization of states or actions. We apply the resulting algorithms to the Furuta pendulum and cartpole. We show that both algorithms obtain the optimal policy. The robustness Sim2Real experiments on the physical systems show that the policies successfully achieve the task in the real-world. When changing the masses of the pendulum, we observe that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at https://sites.google.com/view/rfvi .},
  archive      = {J_TPAMI},
  author       = {Michael Lutter and Boris Belousov and Shie Mannor and Dieter Fox and Animesh Garg and Jan Peters},
  doi          = {10.1109/TPAMI.2022.3215769},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5534-5548},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Continuous-time fitted value iteration for robust policies},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-incremental learning: Survey and performance
evaluation on image classification. <em>TPAMI</em>, <em>45</em>(5),
5513–5533. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For future learning systems, incremental learning is desirable because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored – also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular, we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, an investigation into small and large domain shifts, and a comparison of various network architectures.},
  archive      = {J_TPAMI},
  author       = {Marc Masana and Xialei Liu and Bartłomiej Twardowski and Mikel Menta and Andrew D. Bagdanov and Joost van de Weijer},
  doi          = {10.1109/TPAMI.2022.3213473},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5513-5533},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Class-incremental learning: Survey and performance evaluation on image classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-incremental continual learning into the eXtended
DER-verse. <em>TPAMI</em>, <em>45</em>(5), 5497–5512. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method – termed eXtended-DER (X-DER) – outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and mini ImageNet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g., the value of Knowledge Distillation and flatter minima in continual learning setups). We make our results fully reproducible; the codebase is available at https://github.com/aimagelab/mammoth .},
  archive      = {J_TPAMI},
  author       = {Matteo Boschini and Lorenzo Bonicelli and Pietro Buzzega and Angelo Porrello and Simone Calderara},
  doi          = {10.1109/TPAMI.2022.3206549},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5497-5512},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Class-incremental continual learning into the eXtended DER-verse},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Channel exchanging networks for multimodal and multitask
dense image prediction. <em>TPAMI</em>, <em>45</em>(5), 5481–5496. (<a
href="https://doi.org/10.1109/TPAMI.2022.3211086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion and multitask learning are two vital topics in machine learning. Despite the fruitful progress, existing methods for both problems are still brittle to the same challenge—it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task). Besides, while they are actually closely related to each other, multimodal fusion and multitask learning are rarely explored within the same methodological framework before. In this paper, we propose Channel-Exchanging-Network (CEN) which is self-adaptive, parameter-free, and more importantly, applicable for multimodal and multitask dense image prediction. At its core, CEN adaptively exchanges channels between subnetworks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. For the application of dense image prediction, the validity of CEN is tested by four different scenarios: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of CEN compared to state-of-the-art methods. Detailed ablation studies have also been carried out, which demonstrate the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN .},
  archive      = {J_TPAMI},
  author       = {Yikai Wang and Fuchun Sun and Wenbing Huang and Fengxiang He and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3211086},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5481-5496},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Channel exchanging networks for multimodal and multitask dense image prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blind image super-resolution: A survey and beyond.
<em>TPAMI</em>, <em>45</em>(5), 5461–5480. (<a
href="https://doi.org/10.1109/TPAMI.2022.3203009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used to solve the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as revealing novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images.},
  archive      = {J_TPAMI},
  author       = {Anran Liu and Yihao Liu and Jinjin Gu and Yu Qiao and Chao Dong},
  doi          = {10.1109/TPAMI.2022.3203009},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5461-5480},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Blind image super-resolution: A survey and beyond},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BiFuse++: Self-supervised and efficient bi-projection fusion
for 360° depth estimation. <em>TPAMI</em>, <em>45</em>(5), 5448–5460.
(<a href="https://doi.org/10.1109/TPAMI.2022.3203516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rise of spherical cameras, monocular 360 $^\circ$ depth estimation becomes an important technique for many applications (e.g., autonomous systems). Thus, state-of-the-art frameworks for monocular 360 $^\circ$ depth estimation such as bi-projection fusion in BiFuse are proposed. To train such a framework, a large number of panoramas along with the corresponding depth ground truths captured by laser sensors are required, which highly increases the cost of data collection. Moreover, since such a data collection procedure is time-consuming, the scalability of extending these methods to different scenes becomes a challenge. To this end, self-training a network for monocular depth estimation from 360 $^\circ$ videos is one way to alleviate this issue. However, there are no existing frameworks that incorporate bi-projection fusion into the self-training scheme, which highly limits the self-supervised performance since bi-projection fusion can leverage information from different projection types. In this paper, we propose BiFuse++ to explore the combination of bi-projection fusion and the self-training scenario. To be specific, we propose a new fusion module and Contrast-Aware Photometric Loss to improve the performance of BiFuse and increase the stability of self-training on real-world videos. We conduct both supervised and self-supervised experiments on benchmark datasets and achieve state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Fu-En Wang and Yu-Hsuan Yeh and Yi-Hsuan Tsai and Wei-Chen Chiu and Min Sun},
  doi          = {10.1109/TPAMI.2022.3203516},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5448-5460},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BiFuse++: Self-supervised and efficient bi-projection fusion for 360° depth estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond self-attention: External attention using two linear
layers for visual tasks. <em>TPAMI</em>, <em>45</em>(5), 5436–5447. (<a
href="https://doi.org/10.1109/TPAMI.2022.3211006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This article proposes a novel attention mechanism which we call external attention , based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.},
  archive      = {J_TPAMI},
  author       = {Meng-Hao Guo and Zheng-Ning Liu and Tai-Jiang Mu and Shi-Min Hu},
  doi          = {10.1109/TPAMI.2022.3211006},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5436-5447},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Beyond self-attention: External attention using two linear layers for visual tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). ASH: A modern framework for parallel spatial hashing in 3D
perception. <em>TPAMI</em>, <em>45</em>(5), 5417–5435. (<a
href="https://doi.org/10.1109/TPAMI.2022.3214347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ASH, a modern and high-performance framework for parallel spatial hashing on GPU. Compared to existing GPU hash map implementations, ASH achieves higher performance, supports richer functionality, and requires fewer lines of code (LoC) when used for implementing spatially varying operations from volumetric geometry reconstruction to differentiable appearance reconstruction. Unlike existing GPU hash maps, the ASH framework provides a versatile tensor interface, hiding low-level details from the users. In addition, by decoupling the internal hashing data structures and key-value data in buffers, we offer direct access to spatially varying data via indices, enabling seamless integration to modern libraries such as PyTorch. To achieve this, we 1) detach stored key-value data from the low-level hash map implementation; 2) bridge the pointer-first low level data structures to index-first high-level tensor interfaces via an index heap; 3) adapt both generic and non-generic integer-only hash map implementations as backends to operate on multi-dimensional keys. We first profile our hash map against state-of-the-art hash maps on synthetic data to show the performance gain from this architecture. We then show that ASH can consistently achieve higher performance on various large-scale 3D perception tasks with fewer LoC by showcasing several applications, including 1) point cloud voxelization, 2) retargetable volumetric scene reconstruction, 3) non-rigid point cloud registration and volumetric deformation, and 4) spatially varying geometry and appearance refinement. ASH and its example applications are open sourced in Open3D ( http://www.open3d.org ).},
  archive      = {J_TPAMI},
  author       = {Wei Dong and Yixing Lao and Michael Kaess and Vladlen Koltun},
  doi          = {10.1109/TPAMI.2022.3214347},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5417-5435},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ASH: A modern framework for parallel spatial hashing in 3D perception},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analytical tensor voting in ND space and its properties.
<em>TPAMI</em>, <em>45</em>(5), 5404–5416. (<a
href="https://doi.org/10.1109/TPAMI.2022.3215475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to propose a novel Analytical Tensor Voting (ATV) mechanism, which enables robust perceptual grouping and salient information extraction for noisy $N-$ dimensional (ND) data. Firstly, the approximation of the decaying function is investigated and adopted based on the idea of penalizing the $1-$ tensor votes by distance and curvature, respectively, followed by the derivation of analytical solution to the $1-$ tensor voting in ND space from the geometric view. Secondly, a novel spherical representation mechanism is proposed to facilitate the representation of the elementary tensors in various dimensional spaces, where the high dimensional spherical coordinate system is utilized to construct the controllable unit vectors and corresponding $1-$ tensors. Accordingly, any elementary $K-$ tensor is represented by the surface integration of the constructed $1-$ tensors over the unit $K-$ sphere. Thirdly, the ATV mechanism is constructed using the adopted decaying function and proposed spherical representation mechanism, where the analytical solution to tensor voting in ND space is derived, which enables the robust and accurate salient information extraction from noisy ND data. Finally, several interesting properties of the proposed ATV mechanism are investigated. Experimental results on synthetic and real data validate the effectiveness, efficiency and robustness of the proposed method in perceptual grouping tasks in 3D,10D or higher dimensional spaces.},
  archive      = {J_TPAMI},
  author       = {Hongbin Lin and Dan Guo and Jianing Wei and Boran Guan and Zeyu Chen and Xiuping Peng},
  doi          = {10.1109/TPAMI.2022.3215475},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5404-5416},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Analytical tensor voting in ND space and its properties},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient fisher matrix approximation method for
large-scale neural network optimization. <em>TPAMI</em>, <em>45</em>(5),
5391–5403. (<a
href="https://doi.org/10.1109/TPAMI.2022.3213654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the shapes of the parameters are not crucial for designing first-order optimization methods in large scale empirical risk minimization problems, they have important impact on the size of the matrix to be inverted when developing second-order type methods. In this article, we propose an efficient and novel second-order method based on the parameters in the real matrix space $\mathbb {R}^{m\times n}$ and a matrix-product approximate Fisher matrix (MatFisher) by using the products of gradients. The size of the matrix to be inverted is much smaller than that of the Fisher information matrix in the real vector space $\mathbb {R}^{d}$ . Moreover, by utilizing the matrix delayed update and the block diagonal approximation techniques, the computational cost can be controlled and is comparable with first-order methods. A global convergence and a superlinear local convergence analysis are established under mild conditions. Numerical results on image classification with ResNet50, quantum chemistry modeling with SchNet, and data-driven partial differential equations solution with PINN illustrate that our method is quite competitive to the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Minghan Yang and Dong Xu and Qiwen Cui and Zaiwen Wen and Pengxiang Xu},
  doi          = {10.1109/TPAMI.2022.3213654},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5391-5403},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An efficient fisher matrix approximation method for large-scale neural network optimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic survey on deep generative models for graph
generation. <em>TPAMI</em>, <em>45</em>(5), 5370–5390. (<a
href="https://doi.org/10.1109/TPAMI.2022.3214832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are important data representations for describing objects and their relationships, which appear in a wide diversity of real-world scenarios. As one of a critical problem in this area, graph generation considers learning the distributions of given graphs and generating more novel graphs. Owing to their wide range of applications, generative models for graphs, which have a rich history, however, are traditionally hand-crafted and only capable of modeling a few statistical properties of graphs. Recent advances in deep generative models for graph generation is an important step towards improving the fidelity of generated graphs and paves the way for new kinds of applications. This article provides an extensive overview of the literature in the field of deep generative models for graph generation. First, the formal definition of deep generative models for the graph generation and the preliminary knowledge are provided. Second, taxonomies of deep generative models for both unconditional and conditional graph generation are proposed respectively; the existing works of each are compared and analyzed. After that, an overview of the evaluation metrics in this specific domain is provided. Finally, the applications that deep graph generation enables are summarized and five promising future research directions are highlighted.},
  archive      = {J_TPAMI},
  author       = {Xiaojie Guo and Liang Zhao},
  doi          = {10.1109/TPAMI.2022.3214832},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5370-5390},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A systematic survey on deep generative models for graph generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A progressive hierarchical alternating least squares method
for symmetric nonnegative matrix factorization. <em>TPAMI</em>,
<em>45</em>(5), 5355–5369. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the symmetric nonnegative matrix factorization (SNMF) which is a powerful tool in data mining for dimension reduction and clustering. The main contributions of the present work include: (i) a new descent direction for the rank-one SNMF is derived and a strategy for choosing the step size along this descent direction is established; (ii) a progressive hierarchical alternating least squares (PHALS) method for SNMF is developed, which is parameter-free and updates the variables column by column. Moreover, every column is updated by solving a rank-one SNMF subproblem; and (iii) the convergence to the Karush-Kuhn-Tucker (KKT) point set (or the stationary point set) is proved for PHALS. Several synthetical and real data sets are tested to demonstrate the effectiveness and efficiency of the proposed method. Our PHALS provides better performance in terms of the computational accuracy, the optimality gap, and the CPU time, compared with a number of state-of-the-art SNMF methods.},
  archive      = {J_TPAMI},
  author       = {Liangshao Hou and Delin Chu and Li-Zhi Liao},
  doi          = {10.1109/TPAMI.2022.3206465},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5355-5369},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A progressive hierarchical alternating least squares method for symmetric nonnegative matrix factorization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A principled design of image representation: Towards
forensic tasks. <em>TPAMI</em>, <em>45</em>(5), 5337–5354. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forensics is a rising topic as the trustworthy multimedia content is critical for modern society. Like other vision-related applications, forensic analysis relies heavily on the proper image representation. Despite the importance, current theoretical understanding for such representation remains limited, with varying degrees of neglect for its key role. For this gap, we attempt to investigate the forensic-oriented image representation as a distinct problem, from the perspectives of theory, implementation, and application. Our work starts from the abstraction of basic principles that the representation for forensics should satisfy, especially revealing the criticality of robustness, interpretability, and coverage. At the theoretical level, we propose a new representation framework for forensics, called dense invariant representation (DIR), which is characterized by stable description with mathematical guarantees. At the implementation level, the discrete calculation problems of DIR are discussed, and the corresponding accurate and fast solutions are designed with generic nature and constant complexity. We demonstrate the above arguments on the dense-domain pattern detection and matching experiments, providing comparison results with state-of-the-art descriptors. Also, at the application level, the proposed DIR is initially explored in passive and active forensics, namely copy-move forgery detection and perceptual hashing, exhibiting the benefits in fulfilling the requirements of such forensic tasks.},
  archive      = {J_TPAMI},
  author       = {Shuren Qi and Yushu Zhang and Chao Wang and Jiantao Zhou and Xiaochun Cao},
  doi          = {10.1109/TPAMI.2022.3204971},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {5337-5354},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A principled design of image representation: Towards forensic tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised graph embedding via adaptive graph learning.
<em>TPAMI</em>, <em>45</em>(4), 5329–5336. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph autoencoders (GAEs) are powerful tools in representation learning for graph embedding. However, the performance of GAEs is very dependent on the quality of the graph structure, i.e., of the adjacency matrix. In other words, GAEs would perform poorly when the adjacency matrix is incomplete or be disturbed. In this paper, two novel unsupervised graph embedding methods, unsupervised graph embedding via adaptive graph learning (BAGE) and unsupervised graph embedding via variational adaptive graph learning (VBAGE) are proposed. The proposed methods expand the application range of GAEs on graph embedding, i.e, on the general datasets without graph structure. Meanwhile, the adaptive learning mechanism can initialize the adjacency matrix without being affected by the parameter. Besides that, the latent representations are embedded with the Laplacian graph structure to preserve the topology structure of the graph in the vector space. Moreover, the adjacency matrix can be self-learned for better embedding performance when the original graph structure is incomplete. With adaptive learning, the proposed method is much more robust to the graph structure. Experimental studies on several datasets validate our design and demonstrate that our methods outperform baselines by a wide margin in node clustering, node classification, link prediction, and graph visualization tasks.},
  archive      = {J_TPAMI},
  author       = {Rui Zhang and Yunxing Zhang and Chengjun Lu and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3202158},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5329-5336},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised graph embedding via adaptive graph learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse PCA via <span
class="math inline"><em>ℓ</em><sub>2, <em>p</em></sub></span>ℓ2,p-norm
regularization for unsupervised feature selection. <em>TPAMI</em>,
<em>45</em>(4), 5322–5328. (<a
href="https://doi.org/10.1109/TPAMI.2021.3121329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of data mining, how to deal with high-dimensional data is an inevitable topic. Since it does not rely on labels, unsupervised feature selection has attracted a lot of attention. The performance of spectral-based unsupervised methods depends on the quality of the constructed similarity matrix, which is used to depict the intrinsic structure of data. However, real-world data often contain plenty of noise features, making the similarity matrix constructed by original data cannot be completely reliable. Worse still, the size of a similarity matrix expands rapidly as the number of samples rises, making the computational cost increase significantly. To solve this problem, a simple and efficient unsupervised model is proposed to perform feature selection. We formulate PCA as a reconstruction error minimization problem, and incorporate a $\ell _{2,p}$ -norm regularization term to make the projection matrix sparse. The learned row-sparse and orthogonal projection matrix is used to select discriminative features. Then, we present an efficient optimization algorithm to solve the proposed unsupervised model, and analyse the convergence and computational complexity of the algorithm theoretically. Finally, experiments on both synthetic and real-world data sets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Zhengxin Li and Feiping Nie and Jintang Bian and Danyang Wu and Xuelong Li},
  doi          = {10.1109/TPAMI.2021.3121329},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5322-5328},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sparse PCA via $\ell _{2,p}$ℓ2,p-norm regularization for unsupervised feature selection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ResMLP: Feedforward networks for image classification with
data-efficient training. <em>TPAMI</em>, <em>45</em>(4), 5314–5321. (<a
href="https://doi.org/10.1109/TPAMI.2022.3206148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.},
  archive      = {J_TPAMI},
  author       = {Hugo Touvron and Piotr Bojanowski and Mathilde Caron and Matthieu Cord and Alaaeldin El-Nouby and Edouard Grave and Gautier Izacard and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Hervé Jégou},
  doi          = {10.1109/TPAMI.2022.3206148},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5314-5321},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ResMLP: Feedforward networks for image classification with data-efficient training},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ZoomNAS: Searching for whole-body human pose estimation in
the wild. <em>TPAMI</em>, <em>45</em>(4), 5296–5313. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the task of 2D whole-body human pose estimation, which aims to localize dense landmarks on the entire human body including body, feet, face, and hands. We propose a single-network approach, termed ZoomNet, to take into account the hierarchical structure of the full human body and solve the scale variation of different body parts. We further propose a neural architecture search framework, termed ZoomNAS, to promote both the accuracy and efficiency of whole-body pose estimation. ZoomNAS jointly searches the model architecture and the connections between different sub-modules, and automatically allocates computational complexity for searched sub-modules. To train and evaluate ZoomNAS, we introduce the first large-scale 2D human whole-body dataset, namely COCO-WholeBody V1.0, which annotates 133 keypoints for in-the-wild images. Extensive experiments demonstrate the effectiveness of ZoomNAS and the significance of COCO-WholeBody V1.0.},
  archive      = {J_TPAMI},
  author       = {Lumin Xu and Sheng Jin and Wentao Liu and Chen Qian and Wanli Ouyang and Ping Luo and Xiaogang Wang},
  doi          = {10.1109/TPAMI.2022.3197352},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5296-5313},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ZoomNAS: Searching for whole-body human pose estimation in the wild},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised person re-identification with wireless
positioning under weak scene labeling. <em>TPAMI</em>, <em>45</em>(4),
5282–5295. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing unsupervised person re-identification methods only rely on visual clues to match pedestrians under different cameras. Since visual data is essentially susceptible to occlusion, blur, clothing changes, etc., a promising solution is to introduce heterogeneous data to make up for the defect of visual data. Some works based on full-scene labeling introduce wireless positioning to assist cross-domain person re-identification, but their GPS labeling of entire monitoring scenes is laborious. To this end, we propose to explore unsupervised person re-identification with both visual data and wireless positioning trajectories under weak scene labeling, in which we only need to know the locations of the cameras. Specifically, we propose a novel unsupervised multimodal training framework (UMTF), which models the complementarity of visual data and wireless information. Our UMTF contains a multimodal data association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA explores potential data associations in unlabeled multimodal data, while MMGN propagates multimodal messages in the video graph based on the adjacency matrix learned from histogram statistics of wireless data. Thanks to the robustness of the wireless data to visual noise and the collaboration of various modules, UMTF is capable of learning a model free of the human label on data. Extensive experimental results conducted on two challenging datasets, i.e., WP-ReID and Campus4K demonstrate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Yiheng Liu and Wengang Zhou and Qiaokang Xie and Houqiang Li},
  doi          = {10.1109/TPAMI.2022.3196364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5282-5295},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised person re-identification with wireless positioning under weak scene labeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of probably symmetric deformable 3D
objects from images in the wild (invited paper). <em>TPAMI</em>,
<em>45</em>(4), 5268–5281. (<a
href="https://doi.org/10.1109/TPAMI.2021.3076536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least approximately, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.},
  archive      = {J_TPAMI},
  author       = {Shangzhe Wu and Christian Rupprecht and Andrea Vedaldi},
  doi          = {10.1109/TPAMI.2021.3076536},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5268-5281},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised learning of probably symmetric deformable 3D objects from images in the wild (Invited paper)},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty guided collaborative training for weakly
supervised and unsupervised temporal action localization.
<em>TPAMI</em>, <em>45</em>(4), 5252–5267. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In weakly supervised (WSAL) and unsupervised temporal action localization (UAL), the target is to simultaneously localize temporal boundaries and identify category labels of actions with only video-level category labels (WSAL) or category numbers in a dataset (UAL) during training. Among existing methods, attention based methods have achieved superior performance in both tasks by highlighting action segments with foreground attention weights. However, without the segment-level supervision on the attention weight learning, the quality of the attention weight hinders the performance of these methods. In this paper, we propose a novel Uncertainty Guided Collaborative Training (UGCT) strategy to alleviate this problem, which mainly includes two key designs: (1) The first design is an online pseudo label generation module, in which the RGB and FLOW streams work collaboratively to learn from each other. (2) The second design is an uncertainty aware learning module, which can mitigate the noise in the generated pseudo labels. These two designs work together to promote the model performance effectively and efficiently by exchanging information between RGB and FLOW streams. Extensive experimental results on two benchmark datasets with three attention based methods demonstrate the effectiveness of the proposed method, e.g, more than 7.0\% performance gain for mAP@IoU=0.5 on THUMOS14 dataset.},
  archive      = {J_TPAMI},
  author       = {Wenfei Yang and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2022.3200399},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5252-5267},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uncertainty guided collaborative training for weakly supervised and unsupervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransCL: Transformer makes strong and flexible compressive
learning. <em>TPAMI</em>, <em>45</em>(4), 5236–5251. (<a
href="https://doi.org/10.1109/TPAMI.2022.3194001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive learning (CL) is an emerging framework that integrates signal acquisition via compressed sensing (CS) and machine learning for inference tasks directly on a small number of measurements. It can be a promising alternative to classical image-domain methods and enjoys great advantages in memory saving and computational efficiency. However, previous attempts on CL are not only limited to a fixed CS ratio, which lacks flexibility, but also limited to MNIST/CIFAR-like datasets and do not scale to complex real-world high-resolution (HR) data or vision tasks. In this article, a novel transformer-based compressive learning framework on large-scale images with arbitrary CS ratios, dubbed TransCL, is proposed. Specifically, TransCL first utilizes the strategy of learnable block-based compressed sensing and proposes a flexible linear projection strategy to enable CL to be performed on large-scale images in an efficient block-by-block manner with arbitrary CS ratios. Then, regarding CS measurements from all blocks as a sequence, a pure transformer-based backbone is deployed to perform vision tasks with various task-oriented heads. Our sufficient analysis presents that TransCL exhibits strong resistance to interference and robust adaptability to arbitrary CS ratios. Extensive experiments for complex HR data demonstrate that the proposed TransCL can achieve state-of-the-art performance in image classification and semantic segmentation tasks. In particular, TransCL with a CS ratio of 10\% can obtain almost the same performance as when operating directly on the original data and can still obtain satisfying performance even with an extremely low CS ratio of 1\%. The source codes of our proposed TransCL is available at https://github.com/MC-E/TransCL/ .},
  archive      = {J_TPAMI},
  author       = {Chong Mou and Jian Zhang},
  doi          = {10.1109/TPAMI.2022.3194001},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5236-5251},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TransCL: Transformer makes strong and flexible compressive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust person re-identification by defending against
universal attackers. <em>TPAMI</em>, <em>45</em>(4), 5218–5235. (<a
href="https://doi.org/10.1109/TPAMI.2022.3199013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies show that deep person re-identification (re-ID) models are vulnerable to adversarial examples, so it is critical to improving the robustness of re-ID models against attacks. To achieve this goal, we explore the strengths and weaknesses of existing re-ID models, i.e., designing learning-based attacks and training robust models by defending against the learned attacks. The contributions of this paper are three-fold: First, we build a holistic attack-defense framework to study the relationship between the attack and defense for person re-ID. Second, we introduce a combinatorial adversarial attack that is adaptive to unseen domains and unseen model types. It consists of distortions in pixel and color space (i.e., mimicking camera shifts). Third, we propose a novel virtual-guided meta-learning algorithm for our attack-defense system. We leverage a virtual dataset to conduct experiments under our meta-learning framework, which can explore the cross-domain constraints for enhancing the generalization of the attack and the robustness of the re-ID model. Comprehensive experiments on three large-scale re-ID benchmarks demonstrate that: 1) Our combinatorial attack is effective and highly universal in cross-model and cross-dataset scenarios; 2) Our meta-learning algorithm can be readily applied to different attack and defense approaches, which can reach consistent improvement; 3) The defense model trained on the learning-to-learn framework is robust to recent SOTA attacks that are not even used during training.},
  archive      = {J_TPAMI},
  author       = {Fengxiang Yang and Juanjuan Weng and Zhun Zhong and Hong Liu and Zheng Wang and Zhiming Luo and Donglin Cao and Shaozi Li and Shin&#39;ichi Satoh and Nicu Sebe},
  doi          = {10.1109/TPAMI.2022.3199013},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5218-5235},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards robust person re-identification by defending against universal attackers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The geometry of nonlinear embeddings in kernel discriminant
analysis. <em>TPAMI</em>, <em>45</em>(4), 5203–5217. (<a
href="https://doi.org/10.1109/TPAMI.2022.3192726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher&#39;s linear discriminant analysis is a classical method for classification, yet it is limited to capturing linear features only. Kernel discriminant analysis as an extension is known to successfully alleviate the limitation through a nonlinear feature mapping. We study the geometry of nonlinear embeddings in discriminant analysis with polynomial kernels and Gaussian kernel by identifying the population-level discriminant function that depends on the data distribution and the kernel. In order to obtain the discriminant function, we solve a generalized eigenvalue problem with between-class and within-class covariance operators. The polynomial discriminants are shown to capture the class difference through the population moments explicitly. For approximation of the Gaussian discriminant, we use a particular representation of the Gaussian kernel by utilizing the exponential generating function for Hermite polynomials. We also show that the Gaussian discriminant can be approximated using randomized projections of the data. Our results illuminate how the data distribution and the kernel interact in determination of the nonlinear embedding for discrimination, and provide a guideline for choice of the kernel and its parameters.},
  archive      = {J_TPAMI},
  author       = {Jiae Kim and Yoonkyung Lee and Zhiyu Liang},
  doi          = {10.1109/TPAMI.2022.3192726},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5203-5217},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The geometry of nonlinear embeddings in kernel discriminant analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensorized bipartite graph learning for multi-view
clustering. <em>TPAMI</em>, <em>45</em>(4), 5187–5202. (<a
href="https://doi.org/10.1109/TPAMI.2022.3187976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive clustering performance and efficiency in characterizing both the relationship between the data and cluster structure, most existing graph-based multi-view clustering methods still have the following drawbacks. They suffer from the expensive time burden due to both the construction of graphs and eigen-decomposition of Laplacian matrix. Moreover, none of them simultaneously considers the similarity of inter-view and similarity of intra-view. In this article, we propose a variance-based de-correlation anchor selection strategy for bipartite construction. The selected anchors not only cover the whole classes but also characterize the intrinsic structure of data. Following that, we present a tensorized bipartite graph learning for multi-view clustering (TBGL). Specifically, TBGL exploits the similarity of inter-view by minimizing the tensor Schatten p -norm, which well exploits both the spatial structure and complementary information embedded in the bipartite graphs of views. We exploit the similarity of intra-view by using the $\ell _{\text {1,2}}$ -norm minimization regularization and connectivity constraint on each bipartite graph. So the learned graph not only well encodes discriminative information but also has the exact connected components which directly indicates the clusters of data. Moreover, we solve TBGL by an efficient algorithm which is time-economical and has good convergence. Extensive experimental results demonstrate that TBGL is superior to the state-of-the-art methods. Codes and datasets are available: https://github.com/xdweixia/TBGL-MVC .},
  archive      = {J_TPAMI},
  author       = {Wei Xia and Quanxue Gao and Qianqian Wang and Xinbo Gao and Chris Ding and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3187976},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5187-5202},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tensorized bipartite graph learning for multi-view clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). SimpleMKKM: Simple multiple kernel k-means. <em>TPAMI</em>,
<em>45</em>(4), 5174–5186. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple yet effective multiple kernel clustering algorithm, termed simple multiple kernel k-means (SimpleMKKM). It extends the widely used supervised kernel alignment criterion to multi-kernel clustering. Our criterion is given by an intractable minimization-maximization problem in the kernel coefficient and clustering partition matrix. To optimize it, we equivalently rewrite the minimization-maximization formulation as a minimization of an optimal value function, prove its differenentiablity, and design a reduced gradient descent algorithm to decrease it. Furthermore, we prove that the resultant solution of SimpleMKKM is the global optimum . We theoretically analyze the performance of SimpleMKKM in terms of its clustering generalization error. After that, we develop extensive experiments to investigate the proposed SimpleMKKM from the perspectives of clustering accuracy, advantage on the formulation and optimization, variation of the learned consensus clustering matrix with iterations, clustering performance with varied number of samples and base kernels, analysis of the learned kernel weight, the running time and the global convergence. The experimental study demonstrates the effectiveness of the proposed SimpleMKKM by considerably and consistently outperforming state of the art multiple kernel clustering alternatives. In addition, the ablation study shows that the improved clustering performance is contributed by both the novel formulation and new optimization. Our work provides a more effective approach to integrate multi-view data for clustering, and this could trigger novel research on multiple kernel clustering. The source code and data for SimpleMKKM are available at https://github.com/xinwangliu/SimpleMKKMcodes/ .},
  archive      = {J_TPAMI},
  author       = {Xinwang Liu},
  doi          = {10.1109/TPAMI.2022.3198638},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5174-5186},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SimpleMKKM: Simple multiple kernel K-means},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamBAN: Target-aware tracking with siamese box adaptive
network. <em>TPAMI</em>, <em>45</em>(4), 5158–5173. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variation of scales or aspect ratios has been one of the main challenges for tracking. To overcome this challenge, most existing methods adopt either multi-scale search or anchor-based schemes, which use a predefined search space in a handcrafted way and therefore limit their performance in complicated scenes. To address this problem, recent anchor-free based trackers have been proposed without using prior scale or anchor information. However, an inconsistency problem between classification and regression degrades the tracking performance. To address the above issues, we propose a simple yet effective tracker (named Siamese Box Adaptive Network, SiamBAN) to learn a target-aware scale handling schema in a data-driven manner. Our basic idea is to predict the target boxes in a per-pixel fashion through a fully convolutional network, which is anchor-free. Specifically, SiamBAN divides the tracking problem into classification and regression tasks, which directly predict objectiveness and regress bounding boxes, respectively. A no-prior box design is proposed to avoid tuning hyper-parameters related to candidate boxes, which makes SiamBAN more flexible. SiamBAN further uses a target-aware branch to address the inconsistency problem. Experiments on benchmarks including VOT2018, VOT2019, OTB100, UAV123, LaSOT and TrackingNet show that SiamBAN achieves promising performance and runs at 35 FPS.},
  archive      = {J_TPAMI},
  author       = {Zedu Chen and Bineng Zhong and Guorong Li and Shengping Zhang and Rongrong Ji and Zhenjun Tang and Xianxian Li},
  doi          = {10.1109/TPAMI.2022.3195759},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5158-5173},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SiamBAN: Target-aware tracking with siamese box adaptive network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-guided belief propagation – a homotopy continuation
method. <em>TPAMI</em>, <em>45</em>(4), 5139–5157. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belief propagation (BP) is a popular method for performing probabilistic inference on graphical models. In this work, we enhance BP and propose self-guided belief propagation (SBP) that incorporates the pairwise potentials only gradually. This homotopy continuation method converges to a unique solution and increases the accuracy without increasing the computational burden. We provide a formal analysis to demonstrate that SBP finds the global optimum of the Bethe approximation for attractive models where all variables favor the same state. Moreover, we apply SBP to various graphs with random potentials and empirically show that: (i) SBP is superior in terms of accuracy whenever BP converges, and (ii) SBP obtains a unique, stable, and accurate solution whenever BP does not converge.},
  archive      = {J_TPAMI},
  author       = {Christian Knoll and Adrian Weller and Franz Pernkopf},
  doi          = {10.1109/TPAMI.2022.3196140},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5139-5157},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-guided belief propagation – a homotopy continuation method},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-constrained spectral clustering. <em>TPAMI</em>,
<em>45</em>(4), 5126–5138. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a leading graph clustering technique, spectral clustering is one of the most widely used clustering methods to capture complex clusters in data. Some additional prior information can help it to further reduce the difference between its clustering results and users’ expectations. However, it is hard to get the prior information under unsupervised scene to guide the clustering process. To solve this problem, we propose a self-constrained spectral clustering algorithm. In this algorithm, we extend the objective function of spectral clustering by adding pairwise and label self-constrained terms to it. We provide the theoretical analysis to show the roles of the self-constrained terms and the extensibility of the proposed algorithm. Based on the new objective function, we build an optimization model for self-constrained spectral clustering so that we can simultaneously learn the clustering results and constraints. Furthermore, we propose an iterative method to solve the new optimization problem. Compared to other existing versions of spectral clustering algorithms, the new algorithm can discover a high-quality cluster structure of a data set without prior information. Extensive experiments on benchmark data sets illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_TPAMI},
  author       = {Liang Bai and Jiye Liang and Yunxiao Zhao},
  doi          = {10.1109/TPAMI.2022.3188160},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5126-5138},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-constrained spectral clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Repurposing GANs for one-shot semantic part segmentation.
<em>TPAMI</em>, <em>45</em>(4), 5114–5125. (<a
href="https://doi.org/10.1109/TPAMI.2022.3201285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? And can image synthesis serve as an “upstream” representation learning task? In this work, we test these hypotheses and propose a simple and effective approach based on GANs for fundamental vision tasks: semantic part segmentation and landmark detection. With our approach, these tasks only require as few as one labeled example along with an unlabeled dataset, rather than thousands of examples. Our key idea is to leverage a trained GAN to extract a pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that this GAN-derived representation is “readily discriminative” and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning, which can generalize to many other tasks. More results are available at https://RepurposeGANs.github.io/ .},
  archive      = {J_TPAMI},
  author       = {Pitchaporn Rewatbowornwong and Nontawat Tritrong and Supasorn Suwajanakorn},
  doi          = {10.1109/TPAMI.2022.3201285},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5114-5125},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Repurposing GANs for one-shot semantic part segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying the knowledge in a DNN to explain knowledge
distillation for classification. <em>TPAMI</em>, <em>45</em>(4),
5099–5113. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to traditional learning from scratch, knowledge distillation sometimes makes the DNN achieve superior performance. In this paper, we provide a new perspective to explain the success of knowledge distillation based on the information theory, i.e., quantifying knowledge points encoded in intermediate layers of a DNN for classification. To this end, we consider the signal processing in a DNN as a layer-wise process of discarding information. A knowledge point is referred to as an input unit, the information of which is discarded much less than that of other input units. Thus, we propose three hypotheses for knowledge distillation based on the quantification of knowledge points. 1. The DNN learning from knowledge distillation encodes more knowledge points than the DNN learning from scratch. 2. Knowledge distillation makes the DNN more likely to learn different knowledge points simultaneously. In comparison, the DNN learning from scratch tends to encode various knowledge points sequentially. 3. The DNN learning from knowledge distillation is often more stably optimized than the DNN learning from scratch. To verify the above hypotheses, we design three types of metrics with annotations of foreground objects to analyze feature representations of the DNN, i.e., the quantity and the quality of knowledge points, the learning speed of different knowledge points, and the stability of optimization directions. In experiments, we diagnosed various DNNs on different classification tasks, including image classification, 3D point cloud classification, binary sentiment classification, and question answering, which verified the above hypotheses.},
  archive      = {J_TPAMI},
  author       = {Quanshi Zhang and Xu Cheng and Yilan Chen and Zhefan Rao},
  doi          = {10.1109/TPAMI.2022.3200344},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5099-5113},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Quantifying the knowledge in a DNN to explain knowledge distillation for classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Permute me softly: Learning soft permutations for graph
representations. <em>TPAMI</em>, <em>45</em>(4), 5087–5098. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have recently emerged as a dominant paradigm for machine learning with graphs. Research on GNNs has mainly focused on the family of message passing neural networks (MPNNs). Similar to the Weisfeiler-Leman (WL) test of isomorphism, these models follow an iterative neighborhood aggregation procedure to update vertex representations, and they next compute graph representations by aggregating the representations of the vertices. Although very successful, MPNNs have been studied intensively in the past few years. Thus, there is a need for novel architectures which will allow research in the field to break away from MPNNs. In this paper, we propose a new graph neural network model, so-called $\pi$ -GNN which learns a “soft” permutation (i. e., doubly stochastic) matrix for each graph, and thus projects all graphs into a common vector space. The learned matrices impose a “soft” ordering on the vertices of the input graphs, and based on this ordering, the adjacency matrices are mapped into vectors. These vectors can be fed into fully-connected or convolutional layers to deal with supervised learning tasks. In case of large graphs, to make the model more efficient in terms of running time and memory, we further relax the doubly stochastic matrices to row stochastic matrices. We empirically evaluate the model on graph classification and graph regression datasets and show that it achieves performance competitive with state-of-the-art models.},
  archive      = {J_TPAMI},
  author       = {Giannis Nikolentzos and George Dasoulas and Michalis Vazirgiannis},
  doi          = {10.1109/TPAMI.2022.3188911},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5087-5098},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Permute me softly: Learning soft permutations for graph representations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Out-of-domain human mesh reconstruction via dynamic bilevel
online adaptation. <em>TPAMI</em>, <em>45</em>(4), 5070–5086. (<a
href="https://doi.org/10.1109/TPAMI.2022.3194167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a new problem of adapting a human mesh reconstruction model to out-of-domain streaming videos, where the performance of existing SMPL-based models is significantly affected by the distribution shift represented by different camera parameters, bone lengths, backgrounds, and occlusions. We tackle this problem through online adaptation, gradually correcting the model bias during testing. There are two main challenges: First, the lack of 3D annotations increases the training difficulty and results in 3D ambiguities. Second, non-stationary data distribution makes it difficult to strike a balance between fitting regular frames and hard samples with severe occlusions or dramatic changes. To this end, we propose the Dynamic Bilevel Online Adaptation algorithm (DynaBOA). It first introduces the temporal constraints to compensate for the unavailable 3D annotations and leverages a bilevel optimization procedure to address the conflicts between multi-objectives. DynaBOA provides additional 3D guidance by co-training with similar source examples retrieved efficiently despite the distribution shift. Furthermore, it can adaptively adjust the number of optimization steps on individual frames to fully fit hard samples and avoid overfitting regular frames. DynaBOA achieves state-of-the-art results on three out-of-domain human mesh reconstruction benchmarks.},
  archive      = {J_TPAMI},
  author       = {Shanyan Guan and Jingwei Xu and Michelle Zhang He and Yunbo Wang and Bingbing Ni and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2022.3194167},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5070-5086},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Out-of-domain human mesh reconstruction via dynamic bilevel online adaptation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing partial area under the top-k curve: Theory and
practice. <em>TPAMI</em>, <em>45</em>(4), 5053–5069. (<a
href="https://doi.org/10.1109/TPAMI.2022.3199970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top- $k$ error has become a popular metric for large-scale classification benchmarks due to the inevitable semantic ambiguity among classes. Existing literature on top- $k$ optimization generally focuses on the optimization method of the top- $k$ objective, while ignoring the limitations of the metric itself. In this paper, we point out that the top- $k$ objective lacks enough discrimination such that the induced predictions may give a totally irrelevant label a top rank. To fix this issue, we develop a novel metric named partial Area Under the top- $k$ Curve (AUTKC). Theoretical analysis shows that AUTKC has a better discrimination ability, and its Bayes optimal score function could give a correct top- $K$ ranking with respect to the conditional probability. This shows that AUTKC does not allow irrelevant labels to appear in the top list. Furthermore, we present an empirical surrogate risk minimization framework to optimize the proposed metric. Theoretically, we present (1) a sufficient condition for Fisher consistency of the Bayes optimal score function; (2) a generalization upper bound which is insensitive to the number of classes under a simple hyperparameter setting. Finally, the experimental results on four benchmark datasets validate the effectiveness of our proposed framework.},
  archive      = {J_TPAMI},
  author       = {Zitai Wang and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3199970},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5053-5069},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimizing partial area under the top-k curve: Theory and practice},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the minimal adversarial perturbation for deep neural
networks with provable estimation error. <em>TPAMI</em>, <em>45</em>(4),
5038–5052. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Deep Neural Networks (DNNs) have shown incredible performance in perceptive and control tasks, several trustworthy issues are still open. One of the most discussed topics is the existence of adversarial perturbations, which has opened an interesting research line on provable techniques capable of quantifying the robustness of a given input. In this regard, the euclidean distance of the input from the classification boundary denotes a well-proved robustness assessment as the minimal affordable adversarial perturbation. Unfortunately, computing such a distance is highly complex due the non-convex nature of DNNs. Despite several methods have been proposed to address this issue, to the best of our knowledge, no provable results have been presented to estimate and bound the error committed. This paper addresses this issue by proposing two lightweight strategies to find the minimal adversarial perturbation. Differently from the state-of-the-art, the proposed approach allows formulating an error estimation theory of the approximate distance with respect to the theoretical one. Finally, a substantial set of experiments is reported to evaluate the performance of the algorithms and support the theoretical findings. The obtained results show that the proposed strategies approximate the theoretical distance for samples close to the classification boundary, leading to provable robustness guarantees against any adversarial attacks.},
  archive      = {J_TPAMI},
  author       = {Fabio Brau and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1109/TPAMI.2022.3195616},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5038-5052},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the minimal adversarial perturbation for deep neural networks with provable estimation error},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the decision boundaries of neural networks: A tropical
geometry perspective. <em>TPAMI</em>, <em>45</em>(4), 5027–5037. (<a
href="https://doi.org/10.1109/TPAMI.2022.3201490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piecewise linear non-linearity activations. We use tropical geometry, a new development in the area of algebraic geometry, to characterize the decision boundaries of a simple network of the form (Affine, ReLU, Affine). Our main finding is that the decision boundaries are a subset of a tropical hypersurface, which is intimately related to a polytope formed by the convex hull of two zonotopes. The generators of these zonotopes are functions of the network parameters. This geometric characterization provides new perspectives to three tasks. ( i ) We propose a new tropical perspective to the lottery ticket hypothesis, where we view the effect of different initializations on the tropical geometric representation of a network&#39;s decision boundaries. ( ii ) Moreover, we propose new tropical based optimization reformulations that directly influence the decision boundaries of the network for the task of network pruning. ( iii ) At last, we discuss the reformulation of the generation of adversarial attacks in a tropical sense. We demonstrate that one can construct adversaries in a new tropical setting by perturbing a specific set of decision boundaries by perturbing a set of parameters in the network.},
  archive      = {J_TPAMI},
  author       = {Motasem Alfarra and Adel Bibi and Hasan Hammoud and Mohamed Gaafar and Bernard Ghanem},
  doi          = {10.1109/TPAMI.2022.3201490},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5027-5037},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the decision boundaries of neural networks: A tropical geometry perspective},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object-occluded human shape and pose estimation with
probabilistic latent consistency. <em>TPAMI</em>, <em>45</em>(4),
5010–5026. (<a
href="https://doi.org/10.1109/TPAMI.2022.3199449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusions between human and objects, especially for the activities of human-object interactions, are very common in practical applications. However, most of the existing approaches for 3D human shape and pose estimation require that human bodies are well captured without occlusions or with minor self-occlusions. In this paper, we focus on the problem of directly estimating the object-occluded human shape and pose from single color images. Our key idea is to utilize a partial UV map to represent an object-occluded human body, and the full 3D human shape estimation is ultimately converted as an image inpainting problem. We propose a novel two-branch network architecture to train an end-to-end regressor via a latent distribution consistency, which also includes a novel visible feature sub-net to extract the human information from object-occluded color images. To supervise the network training, we further build a novel dataset named as 3DOH50K . Several experiments are conducted to reveal the effectiveness of the proposed method. Experimental results demonstrate that the proposed method achieves state-of-the-art compared with previous methods. The dataset and codes are publicly available at https://www.yangangwang.com/papers/ZHANG-OOH-2020-03.html .},
  archive      = {J_TPAMI},
  author       = {Buzhen Huang and Tianshu Zhang and Yangang Wang},
  doi          = {10.1109/TPAMI.2022.3199449},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {5010-5026},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Object-occluded human shape and pose estimation with probabilistic latent consistency},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NAAQA: A neural architecture for acoustic question
answering. <em>TPAMI</em>, <em>45</em>(4), 4997–5009. (<a
href="https://doi.org/10.1109/TPAMI.2022.3194311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by $\sim$ 17 percentage points. On the other hand, frequency coordinate maps have little influence on this task. NAAQA achieves 79.5\% of accuracy on the AQA task with $\sim$ four times fewer parameters than the previously explored VQA model. We evaluate the performance of NAAQA on an independent data set reconstructed from DAQA. We also test the addition of a MALiMo module in our model on both CLEAR2 and DAQA. We provide a detailed analysis of the results for the different question types. We release the code to produce CLEAR2 as well as NAAQA to foster research in this newly emerging machine learning task.},
  archive      = {J_TPAMI},
  author       = {Jérôme Abdelnour and Jean Rouat and Giampiero Salvi},
  doi          = {10.1109/TPAMI.2022.3194311},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4997-5009},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NAAQA: A neural architecture for acoustic question answering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview unsupervised shapelet learning for multivariate
time series clustering. <em>TPAMI</em>, <em>45</em>(4), 4981–4996. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series clustering has become an important research topic in the time series learning task, which aims to discover the correlation among multiple sequences and partition multivariate time series data into several subsets. Although there are currently some methods that can handle this task, most of them fail to discover informative subsequences from multivariate time series instances. In this paper, we first propose a novel unsupervised shapelet learning with adaptive neighbors (USLA) model for learning salient multivariate subsequences (i.e., multivariate shapelets), where the importance of each variate can be auto-determined when given a candidate multivariate shapelet. USLA performs multivariate shapelet-transformed representation learning and local structure learning simultaneously, but the performance of USLA with multivariate shapelets of different lengths is comparable to that of isometric multivariate shapelets. In fact, the shapelet-transformed representations learned from multivariate shapelets of different lengths can all represent multivariate time series instances separately and often contain complementary information to each other. Therefore, we develop a novel multiview USLA (MUSLA) model which treats shapelet-transformed representations learned from shapelets of different lengths as different views. In this way, MUSLA learns the importance of each view and the neighbor graph matrix among multiview representations when candidate multivariate shapelets of different lengths are determined. Experimental results show that MUSLA outperforms other state-of-the-art multivariate time series algorithms on real-world multivariate time series datasets.},
  archive      = {J_TPAMI},
  author       = {Nan Zhang and Shiliang Sun},
  doi          = {10.1109/TPAMI.2022.3198411},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4981-4996},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiview unsupervised shapelet learning for multivariate time series clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-target markov boundary discovery: Theory, algorithm,
and application. <em>TPAMI</em>, <em>45</em>(4), 4964–4980. (<a
href="https://doi.org/10.1109/TPAMI.2022.3199784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov boundary (MB) has been widely studied in single-target scenarios. Relatively few works focus on the MB discovery for variable set due to the complex variable relationships, where an MB variable might contain predictive information about several targets. This paper investigates the multi-target MB discovery, aiming to distinguish the common MB variables (shared by multiple targets) and the target-specific MB variables (associated with single targets). Considering the multiplicity of MB, the relation between common MB variables and equivalent information is studied. We find that common MB variables are determined by equivalent information through different mechanisms, which is relevant to the existence of the target correlation. Based on the analysis of these mechanisms, we propose a multi-target MB discovery algorithm to identify these two types of variables, whose variant also achieves superiority and interpretability in feature selection tasks. Extensive experiments demonstrate the efficacy of these contributions.},
  archive      = {J_TPAMI},
  author       = {Xingyu Wu and Bingbing Jiang and Yan Zhong and Huanhuan Chen},
  doi          = {10.1109/TPAMI.2022.3199784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4964-4980},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-target markov boundary discovery: Theory, algorithm, and application},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale geometric consistency guided and planar prior
assisted multi-view stereo. <em>TPAMI</em>, <em>45</em>(4), 4945–4963.
(<a href="https://doi.org/10.1109/TPAMI.2022.3200074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose some efficient multi-view stereo methods for accurate and complete depth map estimation. We first present our basic methods with Adaptive Checkerboard sampling and Multi-Hypothesis joint view selection (ACMH $\&amp;amp;$ ACMH+). Based on our basic models, we develop two frameworks to deal with the depth estimation of ambiguous regions (especially low-textured areas) from two different perspectives: multi-scale information fusion and planar geometric clue assistance. For the former one, we propose a multi-scale geometric consistency guidance framework (ACMM) to obtain the reliable depth estimates for low-textured areas at coarser scales and guarantee that they can be propagated to finer scales. For the latter one, we propose a planar prior assisted framework (ACMP). We utilize a probabilistic graphical model to contribute a novel multi-view aggregated matching cost. At last, by taking advantage of the above frameworks, we further design a multi-scale geometric consistency guided and planar prior assisted multi-view stereo (ACMMP). This greatly enhances the discrimination of ambiguous regions and helps their depth sensing. Experiments on extensive datasets show our methods achieve state-of-the-art performance, recovering the depth estimation not only in low-textured areas but also in details. Related codes are available at https://github.com/GhiXu .},
  archive      = {J_TPAMI},
  author       = {Qingshan Xu and Weihang Kong and Wenbing Tao and Marc Pollefeys},
  doi          = {10.1109/TPAMI.2022.3200074},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4945-4963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-scale geometric consistency guided and planar prior assisted multi-view stereo},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-oriented object detection in aerial images with double
horizontal rectangles. <em>TPAMI</em>, <em>45</em>(4), 4932–4944. (<a
href="https://doi.org/10.1109/TPAMI.2022.3191753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods adopt the quadrilateral or rotated rectangle representation to detect multi-oriented objects. Yet, the same oriented object may correspond to several different representations, due to different vertex ordering, or angular periodicity and edge exchangeability. To ensure the uniqueness of the representation, some engineered rules are usually added. This makes these methods suffer from discontinuity problem, resulting in degraded performance for objects around some orientation. In this article, we propose to encode the multi-oriented object with double horizontal rectangles (DHRec) to solve the discontinuity problem. Specifically, for an oriented object, we arrange the horizontal and vertical coordinates of its four vertices in left-right and top-down order, respectively. The first ( resp. second) horizontal box is given by two diagonal points with smallest ( resp. second) and third ( resp. largest) coordinates in both horizontal and vertical dimensions. We then regress three factors given by area ratios between different regions, helping to guide the oriented object decoding from the predicted DHRec. Inherited from the uniqueness of horizontal rectangle representation, the proposed method is free of discontinuity issue, and can accurately detect objects of arbitrary orientation. Extensive experimental results show that the proposed method significantly improves the existing baseline representation, and outperforms state-of-the-art methods. The code is available at: https://github.com/lightbillow/DHRec .},
  archive      = {J_TPAMI},
  author       = {Guangtao Nie and Hua Huang},
  doi          = {10.1109/TPAMI.2022.3191753},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4932-4944},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-oriented object detection in aerial images with double horizontal rectangles},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Momentum-net: Fast and convergent iterative neural network
for inverse problems. <em>TPAMI</em>, <em>45</em>(4), 4915–4931. (<a
href="https://doi.org/10.1109/TPAMI.2020.3012955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative neural networks (INN) are rapidly gaining attention for solving inverse problems in imaging, image processing, and computer vision. INNs combine regression NNs and an iterative model-based image reconstruction (MBIR) algorithm, often leading to both good generalization capability and outperforming reconstruction quality over existing MBIR optimization models. This paper proposes the first fast and convergent INN architecture, Momentum-Net , by generalizing a block-wise MBIR algorithm that uses momentum and majorizers with regression NNs. For fast MBIR, Momentum-Net uses momentum terms in extrapolation modules, and noniterative MBIR modules at each iteration by using majorizers , where each iteration of Momentum-Net consists of three core modules: image refining, extrapolation, and MBIR. Momentum-Net guarantees convergence to a fixed-point for general differentiable (non)convex MBIR functions (or data-fit terms) and convex feasible sets, under two asymptomatic conditions. To consider data-fit variations across training and testing samples, we also propose a regularization parameter selection scheme based on the “spectral spread” of majorization matrices. Numerical experiments for light-field photography using a focal stack and sparse-view computational tomography demonstrate that, given identical regression NN architectures, Momentum-Net significantly improves MBIR speed and accuracy over several existing INNs; it significantly improves reconstruction quality compared to a state-of-the-art MBIR method in each application.},
  archive      = {J_TPAMI},
  author       = {Il Yong Chun and Zhengyu Huang and Hongki Lim and Jeffrey A. Fessler},
  doi          = {10.1109/TPAMI.2020.3012955},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4915-4931},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Momentum-net: Fast and convergent iterative neural network for inverse problems},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to overcome noise in weak caption supervision for
object detection. <em>TPAMI</em>, <em>45</em>(4), 4897–4914. (<a
href="https://doi.org/10.1109/TPAMI.2022.3187350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the first mechanism to train object detection models from weak supervision in the form of captions at the image level. Language-based supervision for detection is appealing and inexpensive: many blogs with images and descriptive text written by human users exist. However, there is significant noise in this supervision: captions do not mention all objects that are shown, and may mention extraneous concepts. We first propose a technique to determine which image-caption pairs provide suitable signal for supervision. We further propose several complementary mechanisms to extract image-level pseudo labels for training from the caption. Finally, we train an iterative weakly-supervised object detection model from these image-level pseudo labels. We use captions from four datasets (COCO, Flickr30K, MIRFlickr1M, and Conceptual Captions) whose level of noise varies. We evaluate our approach on two object detection datasets. Weighting the labels extracted from different captions provides a boost over treating all captions equally. Further, our primary proposed technique for inferring pseudo labels for training at the image level, outperforms alternative techniques under a wide variety of settings. Both techniques generalize to datasets beyond the one they were trained on.},
  archive      = {J_TPAMI},
  author       = {Mesut Erhan Unal and Keren Ye and Mingda Zhang and Christopher Thomas and Adriana Kovashka and Wei Li and Danfeng Qin and Jesse Berent},
  doi          = {10.1109/TPAMI.2022.3187350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4897-4914},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to overcome noise in weak caption supervision for object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to detect 3D symmetry from single-view RGB-d images
with weak supervision. <em>TPAMI</em>, <em>45</em>(4), 4882–4896. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D symmetry detection is a fundamental problem in computer vision and graphics. Most prior works detect symmetry when the object model is fully known, few studies symmetry detection on objects with partial observation, such as single RGB-D images. Recent work addresses the problem of detecting symmetries from incomplete data with a deep neural network by leveraging the dense and accurate symmetry annotations. However, due to the tedious labeling process, full symmetry annotations are not always practically available. In this work, we present a 3D symmetry detection approach to detect symmetry from single-view RGB-D images without using symmetry supervision. The key idea is to train the network in a weakly-supervised learning manner to complete the shape based on the predicted symmetry such that the completed shape be similar to existing plausible shapes. To achieve this, we first propose a discriminative variational autoencoder to learn the shape prior in order to determine whether a 3D shape is plausible or not. Based on the learned shape prior, a symmetry detection network is present to predict symmetries that produce shapes with high shape plausibility when completed based on those symmetries. Moreover, to facilitate end-to-end network training and multiple symmetry detection, we introduce a new symmetry parametrization for the learning-based symmetry estimation of both reflectional and rotational symmetry. The proposed approach, coupled symmetry detection with shape completion, essentially learns the symmetry-aware shape prior, facilitating more accurate and robust symmetry detection. Experiments demonstrate that the proposed method is capable of detecting reflectional and rotational symmetries accurately, and shows good generality in challenging scenarios, such as objects with heavy occlusion and scanning noise. Moreover, it achieves state-of-the-art performance, improving the F1-score over the existing supervised learning method by 2\%-11\% on the ShapeNet and ScanNet datasets.},
  archive      = {J_TPAMI},
  author       = {Yifei Shi and Xin Xu and Junhua Xi and Xiaochang Hu and Dewen Hu and Kai Xu},
  doi          = {10.1109/TPAMI.2022.3186876},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4882-4896},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to detect 3D symmetry from single-view RGB-D images with weak supervision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning mesh representations via binary space partitioning
tree networks. <em>TPAMI</em>, <em>45</em>(4), 4870–4881. (<a
href="https://doi.org/10.1109/TPAMI.2021.3093440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygonal meshes are ubiquitous, but have only played a relatively minor role in the deep learning revolution. State-of-the-art neural generative models for 3D shapes learn implicit functions and generate meshes via expensive iso-surfacing. We overcome these challenges by employing a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core operation of BSP involves recursive subdivision of 3D space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition without supervision . The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built over a set of planes, where the planes and convexes are both defined by learned network weights. BSP-Net directly outputs polygonal meshes from the inferred convexes. The generated meshes are watertight, compact (i.e., low-poly), and well suited to represent sharp geometry. We show that the reconstruction quality by BSP-Net is competitive with those from state-of-the-art methods while using much fewer primitives.We also explore variations to BSP-Net including using a more generic decoder for reconstruction, more general primitives than planes, as well as training a generative model with variational auto-encoders.},
  archive      = {J_TPAMI},
  author       = {Zhiqin Chen and Andrea Tagliasacchi and Hao Zhang},
  doi          = {10.1109/TPAMI.2021.3093440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4870-4881},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning mesh representations via binary space partitioning tree networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning feature-sparse principal subspace. <em>TPAMI</em>,
<em>45</em>(4), 4858–4869. (<a
href="https://doi.org/10.1109/TPAMI.2022.3212646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principal subspace estimation is directly connected to dimension reduction and is important when there is more than one principal component of interest. In this article, we introduce two new algorithms to solve the feature-sparsity constrained PCA problem (FSPCA) for the principal subspace estimation task, which performs feature selection and PCA simultaneously. Existing optimization methods for FSPCA require data distribution assumptions and are lack of global convergence guarantee. Though the general FSPCA problem is NP-hard, we show that, for a low-rank covariance, FSPCA can be solved globally (Algorithm 1). Then, we propose another strategy (Algorithm 2) to solve FSPCA for the general covariance by iteratively building a carefully designed proxy. We prove (data-dependent) approximation bound and regular stationary convergence guarantees for the new algorithms. For the spectrum of covariance with exponential/Zipf&#39;s distribution, we provide exponential/posynomial approximation bounds. Constructive examples and numerical results are provided to demonstrate the tightness of our results. Experimental results show the promising performance and efficiency of the new algorithms compared with the state-of-the-arts on both synthetic and real-world datasets.},
  archive      = {J_TPAMI},
  author       = {Feiping Nie and Lai Tian and Rong Wang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3212646},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4858-4869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning feature-sparse principal subspace},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learn-explain-reinforce: Counterfactual reasoning and its
guidance to reinforce an alzheimer’s disease diagnosis model.
<em>TPAMI</em>, <em>45</em>(4), 4843–4857. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies on disease diagnostic models focus either on diagnostic model learning for performance improvement or on the visual explanation of a trained diagnostic model. We propose a novel learn-explain-reinforce (LEAR) framework that unifies diagnostic model learning, visual explanation generation (explanation unit), and trained diagnostic model reinforcement (reinforcement unit) guided by the visual explanation. For the visual explanation, we generate a counterfactual map that transforms an input sample to be identified as an intended target label. For example, a counterfactual map can localize hypothetical abnormalities within a normal brain image that may cause it to be diagnosed with Alzheimer&#39;s disease (AD). We believe that the generated counterfactual maps represent data-driven knowledge about a target task, i.e., AD diagnosis using structural MRI, which can be a vital source of information to reinforce the generalization of the trained diagnostic model. To this end, we devise an attention-based feature refinement module with the guidance of the counterfactual maps. The explanation and reinforcement units are reciprocal and can be operated iteratively. Our proposed approach was validated via qualitative and quantitative analysis on the ADNI dataset. Its comprehensibility and fidelity were demonstrated through ablation studies and comparisons with existing methods.},
  archive      = {J_TPAMI},
  author       = {Kwanseok Oh and Jee Seok Yoon and Heung-Il Suk},
  doi          = {10.1109/TPAMI.2022.3197845},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4843-4857},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learn-explain-reinforce: Counterfactual reasoning and its guidance to reinforce an alzheimer&#39;s disease diagnosis model},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lattice network for lightweight image restoration.
<em>TPAMI</em>, <em>45</em>(4), 4826–4842. (<a
href="https://doi.org/10.1109/TPAMI.2022.3194090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has made unprecedented progress in image restoration (IR), where residual block (RB) is popularly used and has a significant effect on promising performance. However, the massive stacked RBs bring about burdensome memory and computation cost. To tackle this issue, we aim to design an economical structure for adaptively connecting pair-wise RBs, thereby enhancing the model representation. Inspired by the topological structure of lattice filter in signal processing theory, we elaborately propose the lattice block (LB), where couple butterfly-style topological structures are utilized to bridge pair-wise RBs. Specifically, each candidate structure of LB relies on the combination coefficients learned through adaptive channel reweighting. As a basic mapping block, LB can be plugged into various IR models, such as image super-resolution, image denoising, image deraining, etc. It can avail the construction of lightweight IR models accompanying half parameter amount reduced, while keeping the considerable reconstruction accuracy compared with RBs. Moreover, a novel contrastive loss is exploited as a regularization constraint, which can further enhance the model representation without increasing the inference expenses. Experiments on several IR tasks illustrate that our method can achieve more favorable performance than other state-of-the-art models with lower storage and computation.},
  archive      = {J_TPAMI},
  author       = {Xiaotong Luo and Yanyun Qu and Yuan Xie and Yulun Zhang and Cuihua Li and Yun Fu},
  doi          = {10.1109/TPAMI.2022.3194090},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4826-4842},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lattice network for lightweight image restoration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Key point sensitive loss for long-tailed visual recognition.
<em>TPAMI</em>, <em>45</em>(4), 4812–4825. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For long-tailed distributed data, existing classification models often learn overwhelmingly on the head classes while ignoring the tail classes, resulting in poor generalization capability. To address this problem, we thereby propose a new approach in this paper, in which a key point sensitive (KPS) loss is presented to regularize the key points strongly to improve the generalization performance of the classification model. Meanwhile, in order to improve the performance on tail classes, the proposed KPS loss also assigns relatively large margins on tail classes. Furthermore, we propose a gradient adjustment (GA) optimization strategy to re-balance the gradients of positive and negative samples for each class. By virtue of the gradient analysis of the loss function, it is found that the tail classes always receive negative signals during training, which misleads the tail prediction to be biased towards the head. The proposed GA strategy can circumvent excessive negative signals on tail classes and further improve the overall classification accuracy. Extensive experiments conducted on long-tailed benchmarks show that the proposed method is capable of significantly improving the classification accuracy of the model in tail classes while maintaining competent performance in head classes.},
  archive      = {J_TPAMI},
  author       = {Mengke Li and Yiu-Ming Cheung and Zhikai Hu},
  doi          = {10.1109/TPAMI.2022.3196044},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4812-4825},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Key point sensitive loss for long-tailed visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating multi-label contrastive learning with dual
adversarial graph neural networks for cross-modal retrieval.
<em>TPAMI</em>, <em>45</em>(4), 4794–4811. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing amount of multimodal data, cross-modal retrieval has attracted more and more attention and become a hot research topic. To date, most of the existing techniques mainly convert multimodal data into a common representation space where similarities in semantics between samples can be easily measured across multiple modalities. However, these approaches may suffer from the following limitations: 1) They overcome the modality gap by introducing loss in the common representation space, which may not be sufficient to eliminate the heterogeneity of various modalities; 2) They treat labels as independent entities and ignore label relationships, which is not conducive to establishing semantic connections across multimodal data; 3) They ignore the non-binary values of label similarity in multi-label scenarios, which may lead to inefficient alignment of representation similarity with label similarity. To tackle these problems, in this article, we propose two models to learn discriminative and modality-invariant representations for cross-modal retrieval. First, the dual generative adversarial networks are built to project multimodal data into a common representation space. Second, to model label relation dependencies and develop inter-dependent classifiers, we employ multi-hop graph neural networks (consisting of Probabilistic GNN and Iterative GNN), where the layer aggregation mechanism is suggested for using propagation information of various hops. Third, we propose a novel soft multi-label contrastive loss for cross-modal retrieval, with the soft positive sampling probability, which can align the representation similarity and the label similarity. Additionally, to adapt to incomplete-modal learning, which can have wider applications, we propose a modal reconstruction mechanism to generate missing features. Extensive experiments on three widely used benchmark datasets, i.e., NUS-WIDE, MIRFlickr, and MS-COCO, show the superiority of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Shengsheng Qian and Dizhan Xue and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TPAMI.2022.3188547},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4794-4811},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Integrating multi-label contrastive learning with dual adversarial graph neural networks for cross-modal retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information optimization and transferable state abstractions
in deep reinforcement learning. <em>TPAMI</em>, <em>45</em>(4),
4782–4793. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While humans and animals learn incrementally during their lifetimes and exploit their experience to solve new tasks, standard deep reinforcement learning methods specialize to solve only one task at a time. As a result, the information they acquire is hardly reusable in new situations. Here, we introduce a new perspective on the problem of leveraging prior knowledge to solve future tasks. We show that learning discrete representations of sensory inputs can provide a high-level abstraction that is common across multiple tasks, thus facilitating the transference of information. In particular, we show that it is possible to learn such representations by self-supervision, following an information theoretic approach. Our method is able to learn abstractions in locomotive and optimal control tasks that increase the sample efficiency in both known and unknown tasks, opening a new path to endow artificial agents with generalization abilities.},
  archive      = {J_TPAMI},
  author       = {Diego Gomez and Nicanor Quijano and Luis Felipe Giraldo},
  doi          = {10.1109/TPAMI.2022.3200726},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4782-4793},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Information optimization and transferable state abstractions in deep reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incidents1M: A large-scale dataset of images with natural
disasters, damage, and incidents. <em>TPAMI</em>, <em>45</em>(4),
4768–4781. (<a
href="https://doi.org/10.1109/TPAMI.2022.3191996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural disasters, such as floods, tornadoes, or wildfires, are increasingly pervasive as the Earth undergoes global warming. It is difficult to predict when and where an incident will occur, so timely emergency response is critical to saving the lives of those endangered by destructive events. Fortunately, technology can play a role in these situations. Social media posts can be used as a low-latency data source to understand the progression and aftermath of a disaster, yet parsing this data is tedious without automated methods. Prior work has mostly focused on text-based filtering, yet image and video-based filtering remains largely unexplored. In this work, we present the Incidents1M Dataset, a large-scale multi-label dataset which contains 977,088 images, with 43 incident and 49 place categories. We provide details of the dataset construction, statistics and potential biases; introduce and train a model for incident detection; and perform image-filtering experiments on millions of images on Flickr and Twitter. We also present some applications on incident analysis to encourage and enable future work in computer vision for humanitarian aid. Code, data, and models are available at http://incidentsdataset.csail.mit.edu .},
  archive      = {J_TPAMI},
  author       = {Ethan Weber and Dim P. Papadopoulos and Agata Lapedriza and Ferda Ofli and Muhammad Imran and Antonio Torralba},
  doi          = {10.1109/TPAMI.2022.3191996},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4768-4781},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Incidents1M: A large-scale dataset of images with natural disasters, damage, and incidents},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved generalization in semi-supervised learning: A
survey of theoretical results. <em>TPAMI</em>, <em>45</em>(4),
4747–4767. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning is the learning setting in which we have both labeled and unlabeled data at our disposal. This survey covers theoretical results for this setting and maps out the benefits of unlabeled data in classification and regression tasks. Most methods that use unlabeled data rely on certain assumptions about the data distribution. When those assumptions are not met, including unlabeled data may actually decrease performance. For all practical purposes, it is therefore instructive to have an understanding of the underlying theory and the possible learning behavior that comes with it. This survey gathers results about the possible gains one can achieve when using semi-supervised learning as well as results about the limits of such methods. Specifically, it aims to answer the following questions: what are, in terms of improving supervised methods, the limits of semi-supervised learning? What are the assumptions of different methods? What can we achieve if the assumptions are true? As, indeed, the precise assumptions made are of the essence, this is where the survey&#39;s particular attention goes out to.},
  archive      = {J_TPAMI},
  author       = {Alexander Mey and Marco Loog},
  doi          = {10.1109/TPAMI.2022.3198175},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4747-4767},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improved generalization in semi-supervised learning: A survey of theoretical results},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imperceptible transfer attack and defense on 3D point cloud
classification. <em>TPAMI</em>, <em>45</em>(4), 4727–4746. (<a
href="https://doi.org/10.1109/TPAMI.2022.3193449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many efforts have been made into attack and defense on the 2D image domain in recent years, few methods explore the vulnerability of 3D models. Existing 3D attackers generally perform point-wise perturbation over point clouds, resulting in deformed structures or outliers, which is easily perceivable by humans. Moreover, their adversarial examples are generated under the white-box setting, which frequently suffers from low success rates when transferred to attack remote black-box models. In this article, we study 3D point cloud attacks from two new and challenging perspectives by proposing a novel Imperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the perturbation direction of each point along its normal vector of the neighborhood surface, leading to generated examples with similar geometric properties and thus enhancing the imperceptibility. 2) Transferability: we develop an adversarial transformation model to generate the most harmful distortions and enforce the adversarial examples to resist it, improving their transferability to unknown black-box models. Further, we propose to train more robust black-box 3D models to defend against such ITA attacks by learning more discriminative point cloud representations. Extensive evaluations demonstrate that our ITA attack is more imperceptible and transferable than state-of-the-arts and validate the superiority of our defense strategy.},
  archive      = {J_TPAMI},
  author       = {Daizong Liu and Wei Hu},
  doi          = {10.1109/TPAMI.2022.3193449},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4727-4746},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Imperceptible transfer attack and defense on 3D point cloud classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image super-resolution via iterative refinement.
<em>TPAMI</em>, <em>45</em>(4), 4713–4726. (<a
href="https://doi.org/10.1109/TPAMI.2022.3204461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34\%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.},
  archive      = {J_TPAMI},
  author       = {Chitwan Saharia and Jonathan Ho and William Chan and Tim Salimans and David J. Fleet and Mohammad Norouzi},
  doi          = {10.1109/TPAMI.2022.3204461},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4713-4726},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image super-resolution via iterative refinement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image feature information extraction for interest point
detection: A comprehensive review. <em>TPAMI</em>, <em>45</em>(4),
4694–4712. (<a
href="https://doi.org/10.1109/TPAMI.2022.3201185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest point detection is one of the most fundamental and critical problems in computer vision and image processing. In this paper, we carry out a comprehensive review on image feature information (IFI) extraction techniques for interest point detection. To systematically introduce how the existing interest point detection methods extract IFI from an input image, we propose a taxonomy of the IFI extraction techniques for interest point detection. According to this taxonomy, we discuss different types of IFI extraction techniques for interest point detection. Furthermore, we identify the main unresolved issues related to the existing IFI extraction techniques for interest point detection and any interest point detection methods that have not been discussed before. The existing popular datasets and evaluation standards are provided and the performances for fifteen state-of-the-art approaches are evaluated and discussed. Moreover, future research directions on IFI extraction techniques for interest point detection are elaborated.},
  archive      = {J_TPAMI},
  author       = {Junfeng Jing and Tian Gao and Weichuan Zhang and Yongsheng Gao and Changming Sun},
  doi          = {10.1109/TPAMI.2022.3201185},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4694-4712},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image feature information extraction for interest point detection: A comprehensive review},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human motion transfer with 3D constraints and detail
enhancement. <em>TPAMI</em>, <em>45</em>(4), 4682–4693. (<a
href="https://doi.org/10.1109/TPAMI.2022.3201904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for realistic human motion transfer using a generative adversarial network (GAN), which generates a motion video of a target character imitating actions of a source character, while maintaining high authenticity of the generated results. We tackle the problem by decoupling and recombining the posture information and appearance information of both the source and target characters. The innovation of our approach lies in the use of the projection of a reconstructed 3D human model as the condition of GAN to better maintain the structural integrity of transfer results in different poses. We further introduce a detail enhancement net to enhance the details of transfer results by exploiting the details in real source frames. Extensive experiments show that our approach yields better results both qualitatively and quantitatively than the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Yang-Tian Sun and Qian-Cheng Fu and Yue-Ren Jiang and Zitao Liu and Yu-Kun Lai and Hongbo Fu and Lin Gao},
  doi          = {10.1109/TPAMI.2022.3201904},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4682-4693},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human motion transfer with 3D constraints and detail enhancement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HoughNet: Integrating near and long-range evidence for
visual detection. <em>TPAMI</em>, <em>45</em>(4), 4667–4681. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet&#39;s best model achieves 46.4 $AP$ (and 65.1 $AP_{50}$ ), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in other visual detection tasks, namely, video object detection, instance segmentation, 3D object detection and keypoint detection for human pose estimation, and an additional “labels to photo’’ image generation task, where the integration of our voting module consistently improves performance in all cases.},
  archive      = {J_TPAMI},
  author       = {Nermin Samet and Samet Hicsonmez and Emre Akbas},
  doi          = {10.1109/TPAMI.2022.3200413},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4667-4681},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HoughNet: Integrating near and long-range evidence for visual detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistic prototype activation for few-shot segmentation.
<em>TPAMI</em>, <em>45</em>(4), 4650–4666. (<a
href="https://doi.org/10.1109/TPAMI.2022.3193587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional deep CNN-based segmentation approaches have achieved satisfactory performance in recent years, however, they are essentially Big Data-driven technologies and are difficult to generalize to unseen categories. Few-shot segmentation is subsequently developed to perform pertinent operations in a low-data regime. Unfortunately, due to the training paradigm and network architecture factors, existing methods are prone to overfit the targets of base categories and yield inaccurate segmentation boundaries, which impedes the research progress to some extent. In this paper, we propose a Holistic Prototype Activation (HPA) network to alleviate these problems. Its novel designs can be summarized in three aspects: 1) A training-free scheme to derive the prior representations of base categories. 2) Prototype Activation Module (PAM) that generates reliable activation maps and well-matched query features by filtering the objects of irrelevant classes with high confidence. 3) Cross-Referenced Decoder (CRD) for interacted feature reweighting and multi-level feature aggregation. Extensive experiments on standard few-shot segmentation benchmarks (PASCAL-5 $^{i}$ and COCO-20 $^{i}$ ) verify the effectiveness of our method. On top of that, the superior performance on multiple extended tasks, such as weak-label segmentation, zero-shot segmentation, and video object segmentation, also illustrates its flexibility and versatility. Our code is publicly available at https://github.com/chunbolang/HPA .},
  archive      = {J_TPAMI},
  author       = {Gong Cheng and Chunbo Lang and Junwei Han},
  doi          = {10.1109/TPAMI.2022.3193587},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4650-4666},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Holistic prototype activation for few-shot segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High dimensional mode hunting using pettiest components
analysis. <em>TPAMI</em>, <em>45</em>(4), 4637–4649. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal components analysis has been used to reduce the dimensionality of datasets for a long time. In this paper, we will demonstrate that in mode detection the components of smallest variance, the pettiest components, are more important. We prove that for a multivariate normal or Laplace distribution, we obtain boxes of optimal volume by implementing “pettiest component analysis,” in the sense that their volume is minimal over all possible boxes with the same number of dimensions and fixed probability. This reduction in volume produces an information gain that is measured using active information. We illustrate our results with a simulation and a search for modal patterns of digitized images of hand-written numbers using the famous MNIST database; in both cases pettiest components work better than their competitors. In fact, we show that modes obtained with pettiest components generate better written digits for MNIST than principal components.},
  archive      = {J_TPAMI},
  author       = {Tianhao Liu and Daniel Andrés Díaz-Pachón and J. Sunil Rao and Jean-Eudes Dazard},
  doi          = {10.1109/TPAMI.2022.3195462},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4637-4649},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High dimensional mode hunting using pettiest components analysis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical prototype networks for continual graph
representation learning. <em>TPAMI</em>, <em>45</em>(4), 4622–4636. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advances in graph representation learning, little attention has been paid to the more practical continual learning scenario in which new categories of nodes (e.g., new research areas in citation networks, or new types of products in co-purchasing networks) and their associated edges are continuously emerging, causing catastrophic forgetting on previous categories. Existing methods either ignore the rich topological information or sacrifice plasticity for stability. To this end, we present Hierarchical Prototype Networks (HPNs) which extract different levels of abstract knowledge in the form of prototypes to represent the continuously expanded graphs. Specifically, we first leverage a set of Atomic Feature Extractors (AFEs) to encode both the elemental attribute information and the topological structure of the target node. Next, we develop HPNs to adaptively select relevant AFEs and represent each node with three levels of prototypes. In this way, whenever a new category of nodes is given, only the relevant AFEs and prototypes at each level will be activated and refined, while others remain uninterrupted to maintain the performance over existing nodes. Theoretically, we first demonstrate that the memory consumption of HPNs is bounded regardless of how many tasks are encountered. Then, we prove that under mild constraints, learning new tasks will not alter the prototypes matched to previous data, thereby eliminating the forgetting problem. The theoretical results are supported by experiments on five datasets, showing that HPNs not only outperform state-of-the-art baseline techniques but also consume relatively less memory. Code and datasets are available at https://github.com/QueuQ/HPNs .},
  archive      = {J_TPAMI},
  author       = {Xikun Zhang and Dongjin Song and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3186909},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4622-4636},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical prototype networks for continual graph representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Glance and focus networks for dynamic visual recognition.
<em>TPAMI</em>, <em>45</em>(4), 4605–4621. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial redundancy widely exists in visual recognition tasks, i.e., discriminative features in an image or video frame usually correspond to only a subset of pixels, while the remaining regions are irrelevant to the task at hand. Therefore, static models which process all the pixels with an equal amount of computation result in considerable redundancy in terms of time and space consumption. In this paper, we formulate the image recognition problem as a sequential coarse-to-fine feature learning process, mimicking the human visual system. Specifically, the proposed Glance and Focus Network (GFNet) first extracts a quick global representation of the input image at a low resolution scale, and then strategically attends to a series of salient (small) regions to learn finer features. The sequential process naturally facilitates adaptive inference at test time, as it can be terminated once the model is sufficiently confident about its prediction, avoiding further redundant computation. It is worth noting that the problem of locating discriminant regions in our model is formulated as a reinforcement learning task, thus requiring no additional manual annotations other than classification labels. GFNet is general and flexible as it is compatible with any off-the-shelf backbone models (such as MobileNets, EfficientNets and TSM), which can be conveniently deployed as the feature extractor. Extensive experiments on a variety of image classification and video recognition tasks and with various backbone models demonstrate the remarkable efficiency of our method. For example, it reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 1.3x without sacrificing accuracy. Code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch .},
  archive      = {J_TPAMI},
  author       = {Gao Huang and Yulin Wang and Kangchen Lv and Haojun Jiang and Wenhui Huang and Pengfei Qi and Shiji Song},
  doi          = {10.1109/TPAMI.2022.3196959},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4605-4621},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Glance and focus networks for dynamic visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Generative text convolutional neural network for
hierarchical document representation learning. <em>TPAMI</em>,
<em>45</em>(4), 4586–4604. (<a
href="https://doi.org/10.1109/TPAMI.2022.3192319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For document analysis, existing methods often resort to the document representation that either discards the word order information or projects each word into a low-dimensional dense embedding vector. However, confined by the data&#39;s sparsity and high-dimensionality, limited effort has been made to explore the semantic structures underlying the document representation that formulates each document as a sequence of one-hot vectors, especially in the probabilistic modeling literature. To construct a probabilistic generative model for this type of document representation, we first develop convolutional Poisson factor analysis (CPFA) that not only utilizes the sparse property of data but also enables model parallelism. Through interleaving probabilistic Dirichlet-gamma pooling layers with learnable parameters, we extend the shallow CPFA into a generative text convolutional neural network (GTCNN), which captures richer semantic information with multiple probabilistic convolutional layers and can be coupled with existing deep topic models to alleviate their loss of word order. For efficient and scalable model inference, we not only develop both a parallel upward-downward Gibbs sampler and SG-MCMC based algorithm for training GTCNN, but also construct a hierarchical Weibull convolutional inference network for fast out-of-sample prediction. Experimental results on document representation learning tasks demonstrate the effectiveness of the proposed methods.},
  archive      = {J_TPAMI},
  author       = {Chaojie Wang and Bo Chen and Zhibin Duan and Wenchao Chen and Hao Zhang and Mingyuan Zhou},
  doi          = {10.1109/TPAMI.2022.3192319},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4586-4604},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generative text convolutional neural network for hierarchical document representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FVC: An end-to-end framework towards deep video compression
in feature space. <em>TPAMI</em>, <em>45</em>(4), 4569–4585. (<a
href="https://doi.org/10.1109/TPAMI.2022.3210652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep video compression is attracting increasing attention from both deep learning and video processing community. Recent learning-based approaches follow the hybrid coding paradigm to perform pixel space operations for reducing redundancy along both spatial and temporal dimentions, which leads to inaccurate motion estimation or less effective motion compensation. In this work, we propose a feature-space video coding framework (FVC), which performs all major operations (i.e., motion estimation, motion compression, motion compensation and residual compression) in the feature space. Specifically, a new deformable compensation module, which consists of motion estimation, motion compression and motion compensation, is proposed for more effective motion compensation. In our deformable compensation module, we first perform motion estimation in the feature space to produce the motion information (i.e., the offset maps). Then the motion information is compressed by using the auto-encoder style network. After that, we use the deformable convolution operation to generate the predicted feature for motion compensation. Finally, the residual information between the feature from the current frame and the predicted feature from the deformable compensation module is also compressed in the feature space. Motivated by the conventional codecs, in which the blocks with different sizes are used for motion estimation, we additionally propose two new modules called resolution-adaptive motion coding (RaMC) and resolution-adaptive residual coding (RaRC) to automatically cope with different types of motion and residual patterns at different spatial locations. Comprehensive experimental results demonstrate that our proposed framework achieves the state-of-the-art performance on three benchmark datasets including HEVC, UVG and MCL-JCV.},
  archive      = {J_TPAMI},
  author       = {Zhihao Hu and Dong Xu and Guo Lu and Wei Jiang and Wei Wang and Shan Liu},
  doi          = {10.1109/TPAMI.2022.3210652},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4569-4585},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FVC: An end-to-end framework towards deep video compression in feature space},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Fully convolutional networks for panoptic segmentation with
point-based supervision. <em>TPAMI</em>, <em>45</em>(4), 4552–4568. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a conceptually simple, strong, and efficient framework for fully- and weakly-supervised panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline, which can be optimized with point-based fully or weak supervision. In particular, Panoptic FCN encodes each object instance or stuff category with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-then-segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms the previous box-based and -free models with high efficiency. Furthermore, we propose a new form of point-based annotation for weakly-supervised panoptic segmentation. It only needs several random points for both things and stuff, which dramatically reduces the annotation cost of human. The proposed Panoptic FCN is also proved to have much superior performance in this weakly-supervised setting, which achieves 82\% of the fully-supervised performance with only 20 randomly annotated points per instance. Extensive experiments demonstrate the effectiveness and efficiency of Panoptic FCN on COCO, VOC 2012, Cityscapes, and Mapillary Vistas datasets. And it sets up a new leading benchmark for both fully- and weakly-supervised panoptic segmentation.},
  archive      = {J_TPAMI},
  author       = {Yanwei Li and Hengshuang Zhao and Xiaojuan Qi and Yukang Chen and Lu Qi and Liwei Wang and Zeming Li and Jian Sun and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3200416},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4552-4568},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fully convolutional networks for panoptic segmentation with point-based supervision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fourier series expansion based filter parametrization for
equivariant convolutions. <em>TPAMI</em>, <em>45</em>(4), 4537–4551. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that equivariant convolution is very helpful for many types of computer vision tasks. Recently, the 2D filter parametrization technique has played an important role for designing equivariant convolutions, and has achieved success in making use of rotation symmetry of images. However, the current filter parametrization strategy still has its evident drawbacks, where the most critical one lies in the accuracy problem of filter representation. To address this issue, in this paper we explore an ameliorated Fourier series expansion for 2D filters, and propose a new filter parametrization method based on it. The proposed filter parametrization method not only finely represents 2D filters with zero error when the filter is not rotated (similar as the classical Fourier series expansion), but also substantially alleviates the aliasing-effect-caused quality degradation when the filter is rotated (which usually arises in classical Fourier series expansion method). Accordingly, we construct a new equivariant convolution method based on the proposed filter parametrization method, named F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the continuous domain, which becomes approximate only after discretization. Moreover, we provide theoretical error analysis for the case when the equivariance is approximate, showing that the approximation error is related to the mesh size and filter size. Extensive experiments show the superiority of the proposed method. Particularly, we adopt rotation equivariant convolution methods to a typical low-level image processing task, image super-resolution. It can be substantiated that the proposed F-Conv based method evidently outperforms classical convolution based methods. Compared with pervious filter parametrization based methods, the F-Conv performs more accurately on this low-level image processing task, reflecting its intrinsic capability of faithfully preserving rotation symmetries in local image features.},
  archive      = {J_TPAMI},
  author       = {Qi Xie and Qian Zhao and Zongben Xu and Deyu Meng},
  doi          = {10.1109/TPAMI.2022.3196652},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4537-4551},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fourier series expansion based filter parametrization for equivariant convolutions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedIPR: Ownership verification for federated deep neural
network models. <em>TPAMI</em>, <em>45</em>(4), 4521–4536. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning models are collaboratively developed upon valuable training data owned by multiple parties. During the development and deployment of federated models, they are exposed to risks including illegal copying, re-distribution, misuse and/or free-riding. To address these risks, the ownership verification of federated learning models is a prerequisite that protects federated learning model intellectual property rights (IPR) i.e., FedIPR. We propose a novel federated deep neural network (FedDNN) ownership verification scheme that allows private watermarks to be embedded and verified to claim legitimate IPR of FedDNN models. In the proposed scheme, each client independently verifies the existence of the model watermarks and claims respective ownership of the federated model without disclosing neither private training data nor private watermark information. The effectiveness of embedded watermarks is theoretically justified by the rigorous analysis of conditions under which watermarks can be privately embedded and detected by multiple clients. Moreover, extensive experimental results on computer vision and natural language processing tasks demonstrate that varying bit-length watermarks can be embedded and reliably detected without compromising original model performances. Our watermarking scheme is also resilient to various federated training settings and robust against removal attacks.},
  archive      = {J_TPAMI},
  author       = {Bowen Li and Lixin Fan and Hanlin Gu and Jie Li and Qiang Yang},
  doi          = {10.1109/TPAMI.2022.3195956},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4521-4536},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FedIPR: Ownership verification for federated deep neural network models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast quaternion product units for learning disentangled
representations in <span class="math inline">𝕊𝕆(3)</span>.
<em>TPAMI</em>, <em>45</em>(4), 4504–4520. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world 3D structured data like point clouds and skeletons often can be represented as data in a 3D rotation group (denoted as $\mathbb {SO}(3)$ ). However, most existing neural networks are tailored for the data in the euclidean space, which makes the 3D rotation data not closed under their algebraic operations and leads to sub-optimal performance in 3D-related learning tasks. To resolve the issues caused by the above mismatching between data and model, we propose a novel non-real neuron model called quaternion product unit (QPU) to represent data on 3D rotation groups. The proposed QPU leverages quaternion algebra and the law of the 3D rotation group, representing 3D rotation data as quaternions and merging them via a weighted chain of Hamilton products. We demonstrate that the QPU mathematically maintains the $\mathbb {SO}(3)$ structure of the 3D rotation data during the inference process and disentangles the 3D representations into “rotation-invariant” features and “rotation-equivariant” features, respectively. Moreover, we design a fast QPU to accelerate the computation of QPU. The fast QPU applies a tree-structured data indexing process, and accordingly, leverages the power of parallel computing, which reduces the computational complexity of QPU in a single thread from $\mathcal {O}(N)$ to $\mathcal {O}(\log N)$ . Taking the fast QPU as a basic module, we develop a series of quaternion neural networks (QNNs), including quaternion multi-layer perceptron (QMLP), quaternion message passing (QMP), and so on. In addition, we make the QNNs compatible with conventional real-valued neural networks and applicable for both skeletons and point clouds. Experiments on synthetic and real-world 3D tasks show that the QNNs based on our fast QPUs are superior to state-of-the-art real-valued models, especially in the scenarios requiring the robustness to random rotations. The code of this work is available at https://github.com/SuferQin/Fast-QPU .},
  archive      = {J_TPAMI},
  author       = {Shaofei Qin and Xuan Zhang and Hongteng Xu and Yi Xu},
  doi          = {10.1109/TPAMI.2022.3202217},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4504-4520},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast quaternion product units for learning disentangled representations in $\mathbb {SO}(3)$},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast hierarchical games for image explanations.
<em>TPAMI</em>, <em>45</em>(4), 4494–4503. (<a
href="https://doi.org/10.1109/TPAMI.2022.3189849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As modern complex neural networks keep breaking records and solving harder problems, their predictions also become less and less intelligible. The current lack of interpretability often undermines the deployment of accurate machine learning tools in sensitive settings. In this work, we present a model-agnostic explanation method for image classification based on a hierarchical extension of Shapley coefficients– Hierarchical Shap (h-Shap) –that resolves some of the limitations of current approaches. Unlike other Shapley-based explanation methods, h-Shap is scalable and can be computed without the need of approximation. Under certain distributional assumptions, such as those common in multiple instance learning, h-Shap retrieves the exact Shapley coefficients with an exponential improvement in computational complexity. We compare our hierarchical approach with popular Shapley-based and non-Shapley-based methods on a synthetic dataset, a medical imaging scenario, and a general computer vision problem, showing that h-Shap outperforms the state-of-the-art in both accuracy and runtime. Code and experiments are made publicly available.},
  archive      = {J_TPAMI},
  author       = {Jacopo Teneggi and Alexandre Luster and Jeremias Sulam},
  doi          = {10.1109/TPAMI.2022.3189849},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4494-4503},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast hierarchical games for image explanations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring fine-grained sparsity in convolutional neural
networks for efficient inference. <em>TPAMI</em>, <em>45</em>(4),
4474–4493. (<a
href="https://doi.org/10.1109/TPAMI.2022.3193925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks contain considerable redundant computation, which drags down the inference efficiency and hinders the deployment on resource-limited devices. In this paper, we study the sparsity in convolutional neural networks and propose a generic sparse mask mechanism to improve the inference efficiency of networks. Specifically, sparse masks are learned in both data and channel dimensions to dynamically localize and skip redundant computation at a fine-grained level. Based on our sparse mask mechanism, we develop SMPointSeg, SMSR, and SMStereo for point cloud semantic segmentation, single image super-resolution, and stereo matching tasks, respectively. It is demonstrated that our sparse masks are well compatible to different model components and network architectures to accurately localize redundant computation, with computational cost being significantly reduced for practical speedup. Extensive experiments show that our SMPointSeg, SMSR, and SMStereo achieve state-of-the-art performance on benchmark datasets in terms of both accuracy and efficiency.},
  archive      = {J_TPAMI},
  author       = {Longguang Wang and Yulan Guo and Xiaoyu Dong and Yingqian Wang and Xinyi Ying and Zaiping Lin and Wei An},
  doi          = {10.1109/TPAMI.2022.3193925},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4474-4493},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploring fine-grained sparsity in convolutional neural networks for efficient inference},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EM-driven unsupervised learning for efficient motion
segmentation. <em>TPAMI</em>, <em>45</em>(4), 4462–4473. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a CNN-based fully unsupervised method for motion segmentation from optical flow. We assume that the input optical flow can be represented as a piecewise set of parametric motion models, typically, affine or quadratic motion models. The core idea of our work is to leverage the Expectation-Maximization (EM) framework in order to design in a well-founded manner a loss function and a training procedure of our motion segmentation neural network that does not require either ground-truth or manual annotation. However, in contrast to the classical iterative EM, once the network is trained, we can provide a segmentation for any unseen optical flow field in a single inference step and without estimating any motion models. We investigate different loss functions including robust ones and propose a novel efficient data augmentation technique on the optical flow field, applicable to any network taking optical flow as input. In addition, our method is able by design to segment multiple motions. Our motion segmentation network was tested on four benchmarks, DAVIS2016, SegTrackV2, FBMS59, and MoCA, and performed very well, while being fast at test time.},
  archive      = {J_TPAMI},
  author       = {Etienne Meunier and Anaïs Badoual and Patrick Bouthemy},
  doi          = {10.1109/TPAMI.2022.3198480},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4462-4473},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EM-driven unsupervised learning for efficient motion segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual contrastive prediction for incomplete multi-view
representation learning. <em>TPAMI</em>, <em>45</em>(4), 4447–4461. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a unified framework to solve the following two challenging problems in incomplete multi-view representation learning: i) how to learn a consistent representation unifying different views, and ii) how to recover the missing views. To address the challenges, we provide an information theoretical framework under which the consistency learning and data recovery are treated as a whole. With the theoretical framework, we propose a novel objective function which jointly solves the aforementioned two problems and achieves a provable sufficient and minimal representation. In detail, the consistency learning is performed by maximizing the mutual information of different views through contrastive learning, and the missing views are recovered by minimizing the conditional entropy through dual prediction. To the best of our knowledge, this is one of the first works to theoretically unify the cross-view consistency learning and data recovery for representation learning. Extensive experimental results show that the proposed method remarkably outperforms 20 competitive multi-view learning methods on six datasets in terms of clustering, classification, and human action recognition. The code could be accessed from https://pengxi.me .},
  archive      = {J_TPAMI},
  author       = {Yijie Lin and Yuanbiao Gou and Xiaotian Liu and Jinfeng Bai and Jiancheng Lv and Xi Peng},
  doi          = {10.1109/TPAMI.2022.3197238},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4447-4461},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual contrastive prediction for incomplete multi-view representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DS-net++: Dynamic weight slicing for efficient inference in
CNNs and vision transformers. <em>TPAMI</em>, <em>45</em>(4), 4430–4446.
(<a href="https://doi.org/10.1109/TPAMI.2022.3194044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks have shown their promising capability in reducing theoretical computation complexity by adapting their architectures to the input during inference. However, their practical runtime usually lags behind the theoretical acceleration due to inefficient sparsity. In this paper, we explore a hardware-efficient dynamic inference regime, named dynamic weight slicing, that can generalized well on multiple dimensions in both CNNs and transformers (e.g. kernel size, embedding dimension, number of heads, etc .). Instead of adaptively selecting important weight elements in a sparse way, we pre-define dense weight slices with different importance level by nested residual learning. During inference, weights are progressively sliced beginning with the most important elements to less important ones to achieve different model capacity for inputs with diverse difficulty levels. Based on this conception, we present DS-CNN++ and DS-ViT++, by carefully designing the double headed dynamic gate and the overall network architecture. We further propose dynamic idle slicing to address the drastic reduction of embedding dimension in DS-ViT++. To ensure sub-network generality and routing fairness, we propose a disentangled two-stage optimization scheme. In Stage I, in-place bootstrapping (IB) and multi-view consistency (MvCo) are proposed to stablize and improve the training of DS-CNN++ and DS-ViT++ supernet, respectively. In Stage II, sandwich gate sparsification (SGS) is proposed to assist the gate training. Extensive experiments on 4 datasets and 3 different network architectures demonstrate our methods consistently outperform the state-of-the-art static and dynamic model compression methods by a large margin (up to 6.6\%). Typically, we achieves 2-4× computation reduction and up to 61.5\% real-world acceleration on MobileNet, ResNet-50 and Vision Transformer, with minimal accuracy drops on ImageNet. Code release: https://github.com/changlin31/DS-Net .},
  archive      = {J_TPAMI},
  author       = {Changlin Li and Guangrun Wang and Bing Wang and Xiaodan Liang and Zhihui Li and Xiaojun Chang},
  doi          = {10.1109/TPAMI.2022.3194044},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4430-4446},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DS-net++: Dynamic weight slicing for efficient inference in CNNs and vision transformers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSGN++: Exploiting visual-spatial relation for stereo-based
3D detectors. <em>TPAMI</em>, <em>45</em>(4), 4416–4429. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera-based 3D object detectors are welcome due to their wider deployment and lower price than LiDAR sensors. We first revisit the prior stereo detector DSGN for its stereo volume construction ways for representing both 3D geometry and semantics. We polish the stereo modeling and propose the advanced version, DSGN++, aiming to enhance effective information flow throughout the 2D-to-3D pipeline in three main aspects. First, to effectively lift the 2D information to stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser connections and extracts depth-guided features. Second, for grasping differently spaced features, we present a novel stereo volume – Dual-view Stereo Volume (DSV) that integrates front-view and top-view features and reconstructs sub-voxel depth in the camera frustum. Third, as the foreground region becomes less dominant in 3D space, we propose a multi-modal data editing strategy – Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and improves data efficiency. Without bells and whistles, extensive experiments in various modality setups on the popular KITTI benchmark show that our method consistently outperforms other camera-based 3D detectors for all categories. Code is available at https://github.com/chenyilun95/DSGN2 .},
  archive      = {J_TPAMI},
  author       = {Yilun Chen and Shijia Huang and Shu Liu and Bei Yu and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3197236},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4416-4429},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DSGN++: Exploiting visual-spatial relation for stereo-based 3D detectors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain generalization: A survey. <em>TPAMI</em>,
<em>45</em>(4), 4396–4415. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
  archive      = {J_TPAMI},
  author       = {Kaiyang Zhou and Ziwei Liu and Yu Qiao and Tao Xiang and Chen Change Loy},
  doi          = {10.1109/TPAMI.2022.3195549},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4396-4415},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Domain generalization: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do the math: Making mathematics in wikipedia computable.
<em>TPAMI</em>, <em>45</em>(4), 4384–4395. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikipedia combines the power of AI solutions and human reviewers to safeguard article quality. Quality control objectives include detecting malicious edits, fixing typos, and spotting inconsistent formatting. However, no automated quality control mechanisms currently exist for mathematical formulae. Spell checkers are widely used to highlight textual errors, yet no equivalent tool exists to detect algebraically incorrect formulae. Our paper addresses this shortcoming by making mathematical formulae computable. We present a method that (1) gathers the semantic information surrounding the context of each mathematical formulae, (2) provides access to the information in a graph-structured dependency hierarchy, and (3) performs automatic plausibility checks on equations. We evaluate the performance of our approach on 6,337 mathematical expressions contained in 104 Wikipedia articles on the topic of orthogonal polynomials and special functions. Our system, $\text{L}{A}\text{C}{\scriptsize{\text{AS}}}\text{T}$ , verified 358 out of 1,516 equations as error-free. $\text{L}{A}\text{C}{\scriptsize\text{AS}}\text{T}$ successfully translated 27\% of the mathematical expressions and outperformed existing translation approaches by 16\%. Additionally, $\text{L}{A}\text{C}{\scriptsize\text{AS}}\text{T}$ achieved an F1 score of .495 for annotating mathematical expressions with relevant textual descriptions, which is a significant step towards advancing searchability, readability, and accessibility of mathematical formulae in Wikipedia. A prototype of $\text{L}{A}\text{C}{\scriptsize\text{AS}}\text{T}$ and the semantically enhanced Wikipedia articles are available at: https://tpami.wmflabs.org .},
  archive      = {J_TPAMI},
  author       = {André Greiner-Petter and Moritz Schubotz and Corinna Breitinger and Philipp Scharpf and Akiko Aizawa and Bela Gipp},
  doi          = {10.1109/TPAMI.2022.3195261},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4384-4395},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Do the math: Making mathematics in wikipedia computable},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative self-paced group-metric adaptation for online
visual identification. <em>TPAMI</em>, <em>45</em>(4), 4368–4383. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing solutions to instance-level visual identification usually aim to learn faithful and discriminative feature extractors from offline training data and directly use them for the unseen online testing data. However, their performance is largely limited due to the severe distribution shifting issue between training and testing samples. Therefore, we propose a novel online group-metric adaptation model to adapt the offline learned identification models for the online data by learning a series of metrics for all sharing-subsets. Each sharing-subset is obtained from the proposed novel frequent sharing-subset mining module and contains a group of testing samples that share strong visual similarity relationships to each other. Furthermore, to handle potentially large-scale testing samples, we introduce self-paced learning (SPL) to gradually include samples into adaptation from easy to difficult which elaborately simulates the learning principle of humans. Unlike existing online visual identification methods, our model simultaneously takes both the sample-specific discriminant and the set-based visual similarity among testing samples into consideration. Our method is generally suitable to any off-the-shelf offline learned visual identification baselines for online performance improvement which can be verified by extensive experiments on several widely-used visual identification benchmarks.},
  archive      = {J_TPAMI},
  author       = {Jiahuan Zhou and Bing Su and Ying Wu},
  doi          = {10.1109/TPAMI.2022.3200036},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4368-4383},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discriminative self-paced group-metric adaptation for online visual identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete search photometric stereo for fast and accurate
shape estimation. <em>TPAMI</em>, <em>45</em>(4), 4355–4367. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating surface normals of a scene with spatially varying, general bidirectional reflectance distribution functions (BRDFs) observed by a static camera under varying distant illuminations. Unlike previous approaches that rely on continuous optimization of surface normals, we cast the problem as a discrete search problem over a set of finely discretized surface normals. In this setting, we show that the expensive processes can be precomputed in a scene-independent manner, resulting in accelerated inference. We discuss two variants of our discrete search photometric stereo (DSPS), one working with continuous linear combinations of BRDF bases and the other working with discrete BRDFs sampled from a BRDF space. Experiments show that DSPS has comparable accuracy to state-of-the-art exemplar-based photometric stereo methods while achieving 10–100x acceleration.},
  archive      = {J_TPAMI},
  author       = {Kenji Enomoto and Michael Waechter and Fumio Okura and Kiriakos N. Kutulakos and Yasuyuki Matsushita},
  doi          = {10.1109/TPAMI.2022.3198729},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4355-4367},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discrete search photometric stereo for fast and accurate shape estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting rotated objects as gaussian distributions and its
3-d generalization. <em>TPAMI</em>, <em>45</em>(4), 4335–4354. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing detection methods commonly use a parameterized bounding box (BBox) to model and detect (horizontal) objects and an additional rotation angle parameter is used for rotated objects. We argue that such a mechanism has fundamental limitations in building an effective regression loss for rotation detection, especially for high-precision detection with high IoU (e.g., 0.75). Instead, we propose to model the rotated objects as Gaussian distributions. A direct advantage is that our new regression loss regarding the distance between two Gaussians e.g., Kullback-Leibler Divergence (KLD), can well align the actual detection performance metric, which is not well addressed in existing methods. Moreover, the two bottlenecks i.e., boundary discontinuity and square-like problem also disappear. We also propose an efficient Gaussian metric-based label assignment strategy to further boost the performance. Interestingly, by analyzing the BBox parameters’ gradients under our Gaussian-based KLD loss, we show that these parameters are dynamically updated with interpretable physical meaning, which help explain the effectiveness of our approach, especially for high-precision detection. We extend our approach from 2-D to 3-D with a tailored algorithm design to handle the heading estimation, and experimental results on twelve public datasets (2-D/3-D, aerial/text/face images) with various base detectors show its superiority.},
  archive      = {J_TPAMI},
  author       = {Xue Yang and Gefan Zhang and Xiaojiang Yang and Yue Zhou and Wentao Wang and Jin Tang and Tao He and Junchi Yan},
  doi          = {10.1109/TPAMI.2022.3197152},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4335-4354},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Detecting rotated objects as gaussian distributions and its 3-D generalization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepLogic: Joint learning of neural perception and logical
reasoning. <em>TPAMI</em>, <em>45</em>(4), 4321–4334. (<a
href="https://doi.org/10.1109/TPAMI.2022.3191093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-symbolic learning, aiming to combine the perceiving power of neural perception and the reasoning power of symbolic logic together, has drawn increasing research attention. However, existing works simply cascade the two components together and optimize them isolatedly, failing to utilize the mutual enhancing information between them. To address this problem, we propose DeepLogic , a framework with joint learning of neural perception and logical reasoning, such that these two components are jointly optimized through mutual supervision signals. In particular, the proposed DeepLogic framework contains a deep-logic module that is capable of representing complex first-order-logic formulas in a tree structure with basic logic operators. We then theoretically quantify the mutual supervision signals and propose the deep&amp;logic optimization algorithm for joint optimization. We further prove the convergence of DeepLogic and conduct extensive experiments on model performance, convergence, and generalization, as well as its extension to the continuous domain. The experimental results show that through jointly learning both perceptual ability and logic formulas in a weakly supervised manner, our proposed DeepLogic framework can significantly outperform DNN-based baselines by a great margin and beat other strong baselines without out-of-box tools .},
  archive      = {J_TPAMI},
  author       = {Xuguang Duan and Xin Wang and Peilin Zhao and Guangyao Shen and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2022.3191093},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4321-4334},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepLogic: Joint learning of neural perception and logical reasoning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based action detection in untrimmed videos: A
survey. <em>TPAMI</em>, <em>45</em>(4), 4302–4320. (<a
href="https://doi.org/10.1109/TPAMI.2022.3193611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human behavior and activity facilitates advancement of numerous real-world applications, and is critical for video analysis. Despite the progress of action recognition algorithms in trimmed videos, the majority of real-world videos are lengthy and untrimmed with sparse segments of interest. The task of temporal activity detection in untrimmed videos aims to localize the temporal boundary of actions and classify the action categories. Temporal activity detection task has been investigated in full and limited supervision settings depending on the availability of action annotations. This article provides an extensive overview of deep learning-based algorithms to tackle temporal action detection in untrimmed videos with different supervision levels including fully-supervised, weakly-supervised, unsupervised, self-supervised, and semi-supervised. In addition, this article reviews advances in spatio-temporal action detection where actions are localized in both temporal and spatial dimensions. Action detection in online setting is also reviewed where the goal is to detect actions in each frame without considering any future context in a live video stream. Moreover, the commonly used action detection benchmark datasets and evaluation metrics are described, and the performance of the state-of-the-art methods are compared. Finally, real-world applications of temporal action detection in untrimmed videos and a set of future directions are discussed.},
  archive      = {J_TPAMI},
  author       = {Elahe Vahdani and Yingli Tian},
  doi          = {10.1109/TPAMI.2022.3193611},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4302-4320},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning-based action detection in untrimmed videos: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized federated averaging. <em>TPAMI</em>,
<em>45</em>(4), 4289–4301. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated averaging (FedAvg) is a communication-efficient algorithm for distributed training with an enormous number of clients. In FedAvg, clients keep their data locally for privacy protection; a central parameter server is used to communicate between clients. This central server distributes the parameters to each client and collects the updated parameters from clients. FedAvg is mostly studied in centralized fashions, requiring massive communications between the central server and clients, which leads to possible channel blocking. Moreover, attacking the central server can break the whole system&#39;s privacy. Indeed, decentralization can significantly reduce the communication of the busiest node (the central one) because all nodes only communicate with their neighbors. To this end, in this paper, we study the decentralized FedAvg with momentum (DFedAvgM), implemented on clients that are connected by an undirected graph. In DFedAvgM, all clients perform stochastic gradient descent with momentum and communicate with their neighbors only. To further reduce the communication cost, we also consider the quantized DFedAvgM. The proposed algorithm involves the mixing matrix, momentum, client training with multiple local iterations, and quantization, introducing extra items in the Lyapunov analysis. Thus, the analysis of this paper is much more challenging than previous decentralized (momentum) SGD or FedAvg. We prove convergence of the (quantized) DFedAvgM under trivial assumptions; the convergence rate can be improved to sublinear when the loss function satisfies the PŁ property. Numerically, we find that the proposed algorithm outperforms FedAvg in both convergence speed and communication cost.},
  archive      = {J_TPAMI},
  author       = {Tao Sun and Dongsheng Li and Bao Wang},
  doi          = {10.1109/TPAMI.2022.3196503},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4289-4301},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Decentralized federated averaging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Debiased scene graph generation for dual imbalance learning.
<em>TPAMI</em>, <em>45</em>(4), 4274–4288. (<a
href="https://doi.org/10.1109/TPAMI.2022.3198965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) is one of the hottest topics in computer vision and has attracted many interests since it provides rich semantic information between objects. In practice, the SGG datasets are often dual imbalanced, presented as a large number of backgrounds and rarely few foregrounds, and highly skewed foreground relationships categories (i.e., the long-tailed distribution). How to tackle this dual imbalanced problem is crucial but rarely studied in literature. Existing methods only consider the long-tailed distribution of foregrounds classes and ignore the background-foreground imbalance in SGG, which results in a biased model and prevents it from being applied in the downstream tasks widely. To reduce its side effect and make the contributions of different categories equally, we propose a novel debiased SGG method (named DSDI) by incorporating biased resistance loss and causal intervention tree. We first deeply analyze the potential causes of dual imbalanced problem in SGG. Then, to learn more discriminate representation of the foreground by expanding the foreground features space, the biased resistance loss decouples the background classification from foreground relationship recognition. Meanwhile, a causal graph of content and context is designed to remove the context bias and learn unbiased relationship features via casual intervention tree. Extensive experimental results on two extremely imbalanced datasets: VG150 and VrR-VG, demonstrate our DSDI outperforms other state-of-the-art methods. All our models will be available in https://github.com/zhouhao0515/unbiasedSGG-DSDI .},
  archive      = {J_TPAMI},
  author       = {Hao Zhou and Jun Zhang and Tingjin Luo and Yazhou Yang and Jun Lei},
  doi          = {10.1109/TPAMI.2022.3198965},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4274-4288},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Debiased scene graph generation for dual imbalance learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive active learning under class distribution
mismatch. <em>TPAMI</em>, <em>45</em>(4), 4260–4273. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning(AL) has been successful based on the premise that labeled and unlabeled data come from the same class distribution. However, its performance undergoes a severe deterioration under class distribution mismatch, wherein the unlabeled data contain numerous instances out of the class distribution of labeled data. In this article, we solve this practical yet rarely studied problem by minimizing the AL error, which is formally defined and decomposed as the valid query error and invalid query error. Specifically, the invalid query error is associated with the queries from unknown categories, and the valid query error is attributed to less informative queries from target categories. In light of this discovery, we propose a contrastive AL framework, named ConAL, to simultaneously learn the semantics and distinctiveness of the instances by contrastive techniques, thereby reducing the invalid query error and valid query error, respectively. Theoretically, we prove that the AL error of ConAL has a tight upper bound. Experimentally, ConAL achieves superior performance on two benchmark datasets, CIFAR10 and CIFAR100, and a cross-dataset with class distribution across multi-datasets. Furthermore, we validate that the ConAL technique performs admirably even on the realistic dataset. To the best of our knowledge, ConAL is the first AL work for class distribution mismatch.},
  archive      = {J_TPAMI},
  author       = {Pan Du and Hui Chen and Suyun Zhao and Shuwen Chai and Hong Chen and Cuiping Li},
  doi          = {10.1109/TPAMI.2022.3188807},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4260-4273},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contrastive active learning under class distribution mismatch},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Computational optics for mobile terminals in mass
production. <em>TPAMI</em>, <em>45</em>(4), 4245–4259. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correcting the optical aberrations and the manufacturing deviations of cameras is a challenging task. Due to the limitation on volume and the demand for mass production, existing mobile terminals cannot rectify optical degradation. In this work, we systematically construct the perturbed lens system model to illustrate the relationship between the deviated system parameters and the spatial frequency response (SFR) measured from photographs. To further address this issue, an optimization framework is proposed based on this model to build proxy cameras from the machining samples’ SFRs. Engaging with the proxy cameras, we synthetic data pairs, which encode the optical aberrations and the random manufacturing biases, for training the learning-based algorithms. In correcting aberration, although promising results have been shown recently with convolutional neural networks, they are hard to generalize to stochastic machining biases. Therefore, we propose a dilated Omni-dimensional dynamic convolution (DOConv) and implement it in post-processing to account for the manufacturing degradation. Extensive experiments which evaluate multiple samples of two representative devices demonstrate that the proposed optimization framework accurately constructs the proxy camera. And the dynamic processing model is well-adapted to manufacturing deviations of different cameras, realizing perfect computational photography. The evaluation shows that the proposed method bridges the gap between optical design, system machining, and post-processing pipeline, shedding light on the joint of image signal reception (lens and sensor) and image signal processing (ISP).},
  archive      = {J_TPAMI},
  author       = {Shiqi Chen and Ting Lin and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
  doi          = {10.1109/TPAMI.2022.3200725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4245-4259},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computational optics for mobile terminals in mass production},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex network evolution model based on turing pattern
dynamics. <em>TPAMI</em>, <em>45</em>(4), 4229–4244. (<a
href="https://doi.org/10.1109/TPAMI.2022.3197276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex network models are helpful to explain the evolution rules of network structures, and also are the foundations of understanding and controlling complex networks. The existing studies (e.g., scale-free model, small-world model) are insufficient to uncover the internal mechanisms of the emergence and evolution of communities in networks. To overcome the above limitation, in consideration of the fact that a network can be regarded as a pattern composed of communities, we introduce Turing pattern dynamic as theory support to construct the network evolution model. Specifically, we develop a Reaction-Diffusion model according to Q-Learning technology (RDQL), in which each node regarded as an intelligent agent makes a behavior choice to update its relationships, based on the utility and behavioral strategy at every time step. Extensive experiments indicate that our model not only reveals how communities form and evolve, but also can generate networks with the properties of scale-free, small-world and assortativity. The effectiveness of the RDQL model has also been verified by its application in real networks. Furthermore, the depth analysis of the RDQL model provides a conclusion that the proportion of exploration and exploitation behaviors of nodes is the only factor affecting the formation of communities. The proposed RDQL model has potential to be the basic theoretical tool for studying network stability and dynamics.},
  archive      = {J_TPAMI},
  author       = {Dong Li and Wenbo Song and Jiming Liu},
  doi          = {10.1109/TPAMI.2022.3197276},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4229-4244},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Complex network evolution model based on turing pattern dynamics},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Class-specific semantic reconstruction for open set
recognition. <em>TPAMI</em>, <em>45</em>(4), 4214–4228. (<a
href="https://doi.org/10.1109/TPAMI.2022.3200384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition enables deep neural networks (DNNs) to identify samples of unknown classes, while maintaining high classification accuracy on samples of known classes. Existing methods based on auto-encoder (AE) and prototype learning show great potential in handling this challenging task. In this study, we propose a novel method, called Class-Specific Semantic Reconstruction (CSSR), that integrates the power of AE and prototype learning. Specifically, CSSR replaces prototype points with manifolds represented by class-specific AEs. Unlike conventional prototype-based methods, CSSR models each known class on an individual AE manifold, and measures class belongingness through AE&#39;s reconstruction error. Class-specific AEs are plugged into the top of the DNN backbone and reconstruct the semantic representations learned by the DNN instead of the raw image. Through end-to-end learning, the DNN and the AEs boost each other to learn both discriminative and representative information. The results of experiments conducted on multiple datasets show that the proposed method achieves outstanding performance in both close and open set recognition and is sufficiently simple and flexible to incorporate into existing frameworks.},
  archive      = {J_TPAMI},
  author       = {Hongzhi Huang and Yu Wang and Qinghua Hu and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2022.3200384},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4214-4228},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Class-specific semantic reconstruction for open set recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BuresNet: Conditional bures metric for transferable
representation learning. <em>TPAMI</em>, <em>45</em>(4), 4198–4213. (<a
href="https://doi.org/10.1109/TPAMI.2022.3190645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental manner for learning and cognition, transfer learning has attracted widespread attention in recent years. Typical transfer learning tasks include unsupervised domain adaptation (UDA) and few-shot learning (FSL), which both attempt to sufficiently transfer discriminative knowledge from the training environment to the test environment to improve the model&#39;s generalization performance. Previous transfer learning methods usually ignore the potential conditional distribution shift between environments. This leads to the discriminability degradation in the test environments. Therefore, how to construct a learnable and interpretable metric to measure and then reduce the gap between conditional distributions is very important in the literature. In this article, we design the Conditional Kernel Bures (CKB) metric for characterizing conditional distribution discrepancy, and derive an empirical estimation with convergence guarantee. CKB provides a statistical and interpretable approach, under the optimal transportation framework, to understand the knowledge transfer mechanism. It is essentially an extension of optimal transportation from the marginal distributions to the conditional distributions. CKB can be used as a plug-and-play module and placed onto the loss layer in deep networks, thus, it plays the bottleneck role in representation learning. From this perspective, the new method with network architecture is abbreviated as BuresNet, and it can be used extract conditional invariant features for both UDA and FSL tasks. BuresNet can be trained in an end-to-end manner. Extensive experiment results on several benchmark datasets validate the effectiveness of BuresNet.},
  archive      = {J_TPAMI},
  author       = {Chuan-Xian Ren and You-Wei Luo and Dao-Qing Dai},
  doi          = {10.1109/TPAMI.2022.3190645},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4198-4213},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BuresNet: Conditional bures metric for transferable representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting photon-efficient image reconstruction with a
unified deep neural network. <em>TPAMI</em>, <em>45</em>(4), 4180–4197.
(<a href="https://doi.org/10.1109/TPAMI.2022.3200745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photon-efficient imaging, which captures 3D images with single-photon sensors, has enabled a wide range of applications. However, two major challenges limit the reconstruction performance, i.e., the low photon counts accompanied by low signal-to-background ratio (SBR) and the multiple returns. In this paper, we propose a unified deep neural network that, for the first time, explicitly addresses these two challenges, and simultaneously recovers depth maps and intensity images from photon-efficient measurements. Starting from a general image formation model, our network is constituted of one encoder, where a non-local block is utilized to exploit the long-range correlations in both spatial and temporal dimensions of the raw measurement, and two decoders, which are designed to recover depth and intensity, respectively. Meanwhile, we investigate the statistics of the background noise photons and propose a noise prior block to further improve the reconstruction performance. The proposed network achieves decent reconstruction fidelity even under extremely low photon counts / SBR and heavy blur caused by the multiple-return effect, which significantly surpasses the existing methods. Moreover, our network trained on simulated data generalizes well to real-world imaging systems, which greatly extends the application scope of photon-efficient imaging in challenging scenarios with a strict limit on optical flux. Code is available at https://github.com/JiayongO-O/PENonLocal .},
  archive      = {J_TPAMI},
  author       = {Jiayong Peng and Zhiwei Xiong and Hao Tan and Xin Huang and Zheng-Ping Li and Feihu Xu},
  doi          = {10.1109/TPAMI.2022.3200745},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4180-4197},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Boosting photon-efficient image reconstruction with a unified deep neural network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarially robust one-class novelty detection.
<em>TPAMI</em>, <em>45</em>(4), 4167–4179. (<a
href="https://doi.org/10.1109/TPAMI.2022.3189638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class novelty detectors are trained with examples of a particular class and are tasked with identifying whether a query example belongs to the same known class. Most recent advances adopt a deep auto-encoder style architecture to compute novelty scores for detecting novel class data. Deep networks have shown to be vulnerable to adversarial attacks, yet little focus is devoted to studying the adversarial robustness of deep novelty detectors. In this article, we first show that existing novelty detectors are susceptible to adversarial examples. We further demonstrate that commonly-used defense approaches for classification tasks have limited effectiveness in one-class novelty detection. Hence, we need a defense specifically designed for novelty detection. To this end, we propose a defense strategy that manipulates the latent space of novelty detectors to improve the robustness against adversarial examples. The proposed method, referred to as Principal Latent Space (PrincipaLS), learns the incrementally-trained cascade principal components in the latent space to robustify novelty detectors. PrincipaLS can purify latent space against adversarial examples and constrain latent space to exclusively model the known class distribution. We conduct extensive experiments on eight attacks, five datasets and seven novelty detectors, showing that PrincipaLS consistently enhances the adversarial robustness of novelty detection models.},
  archive      = {J_TPAMI},
  author       = {Shao-Yuan Lo and Poojan Oza and Vishal M. Patel},
  doi          = {10.1109/TPAMI.2022.3189638},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4167-4179},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarially robust one-class novelty detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjacency constraint for efficient hierarchical
reinforcement learning. <em>TPAMI</em>, <em>45</em>(4), 4152–4166. (<a
href="https://doi.org/10.1109/TPAMI.2022.3192418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is large. Searching in a large goal space poses difficulty for both high-level subgoal generation and low-level policy learning. In this article, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a $k$ -step adjacent region of the current state using an adjacency constraint. We theoretically prove that in a deterministic Markov Decision Process (MDP), the proposed adjacency constraint preserves the optimal hierarchical policy, while in a stochastic MDP the adjacency constraint induces a bounded state-value suboptimality determined by the MDP&#39;s transition structure. We further show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks including challenging simulated robot locomotion and manipulation tasks show that incorporating the adjacency constraint significantly boosts the performance of state-of-the-art goal-conditioned HRL approaches.},
  archive      = {J_TPAMI},
  author       = {Tianren Zhang and Shangqi Guo and Tian Tan and Xiaolin Hu and Feng Chen},
  doi          = {10.1109/TPAMI.2022.3192418},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4152-4166},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adjacency constraint for efficient hierarchical reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive two-stream consensus network for weakly-supervised
temporal action localization. <em>TPAMI</em>, <em>45</em>(4), 4136–4151.
(<a href="https://doi.org/10.1109/TPAMI.2022.3189662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised temporal action localization (W-TAL) aims to classify and localize all action instances in untrimmed videos under only video-level supervision. Without frame-level annotations, it is challenging for W-TAL methods to clearly distinguish actions and background, which severely degrades the action boundary localization and action proposal scoring. In this paper, we present an adaptive two-stream consensus network (A-TSCN) to address this problem. Our A-TSCN features an iterative refinement training scheme: a frame-level pseudo ground truth is generated and iteratively updated from a late-fusion activation sequence, and used to provide frame-level supervision for improved model training. Besides, we introduce an adaptive attention normalization loss, which adaptively selects action and background snippets according to video attention distribution. By differentiating the attention values of the selected action snippets and background snippets, it forces the predicted attention to act as a binary selection and promotes the precise localization of action boundaries. Furthermore, we propose a video-level and a snippet-level uncertainty estimator, and they can mitigate the adverse effect caused by learning from noisy pseudo ground truth. Experiments conducted on the THUMOS14, ActivityNet v1.2, ActivityNet v1.3, and HACS datasets show that our A-TSCN outperforms current state-of-the-art methods, and even achieves comparable performance with several fully-supervised methods.},
  archive      = {J_TPAMI},
  author       = {Yuanhao Zhai and Le Wang and Wei Tang and Qilin Zhang and Nanning Zheng and David Doermann and Junsong Yuan and Gang Hua},
  doi          = {10.1109/TPAMI.2022.3189662},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4136-4151},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive two-stream consensus network for weakly-supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive multi-view and temporal fusing transformer for 3D
human pose estimation. <em>TPAMI</em>, <em>45</em>(4), 4122–4135. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a unified framework dubbed Multi-view and Temporal Fusing Transformer (MTF-Transformer) to adaptively handle varying view numbers and video length without camera calibration in 3D Human Pose Estimation (HPE). It consists of Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose from each image and fuses the prediction according to the confidence. It provides pose-focused feature embedding and makes subsequent modules computationally lightweight. MFT fuses the features of a varying number of views with a novel Relative-Attention block. It adaptively measures the implicit relative relationship between each pair of views and reconstructs more informative features. TFT aggregates the features of the whole sequence and predicts 3D pose via a transformer. It adaptively deals with the video of arbitrary length and fully unitizes the temporal information. The migration of transformers enables our model to learn spatial geometry better and preserve robustness for varying application scenarios. We report quantitative and qualitative results on the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared with state-of-the-art methods with camera parameters, MTF-Transformer obtains competitive results and generalizes well to dynamic capture with an arbitrary number of unseen views.},
  archive      = {J_TPAMI},
  author       = {Hui Shuai and Lele Wu and Qingshan Liu},
  doi          = {10.1109/TPAMI.2022.3188716},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4122-4135},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive multi-view and temporal fusing transformer for 3D human pose estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action recognition from a single coded image.
<em>TPAMI</em>, <em>45</em>(4), 4109–4121. (<a
href="https://doi.org/10.1109/TPAMI.2022.3196350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented success of deep convolutional neural networks (CNN) on the task of video-based human action recognition assumes the availability of good resolution videos and resources to develop and deploy complex models. Unfortunately, certain budgetary and environmental constraints on the camera system and the recognition model may not be able to accommodate these assumptions and require reducing their complexity. To alleviate these issues, we introduce a deep sensing solution to directly recognize human actions from coded exposure images. Our deep sensing solution consists of a binary CNN-based encoder network that emulates the capturing of a coded exposure image of a dynamic scene using a coded exposure camera, followed by a 2D CNN for recognizing human action in the captured coded exposure image. Furthermore, we propose a novel knowledge distillation framework to jointly train the encoder and the action recognition model and show that the proposed training approach improves the action recognition accuracy by an absolute margin of 6.2\%, 2.9\%, and 7.9\% on Something $^{2}$ -v2, Kinetics-400, and UCF-101 datasets, respectively, in comparison to our previous approach. Finally, we built a prototype coded exposure camera using LCoS to validate the feasibility of our deep sensing solution. Our evaluation of the prototype camera show results that are consistent with the simulation results.},
  archive      = {J_TPAMI},
  author       = {Sudhakar Kumawat and Tadashi Okawara and Michitaka Yoshida and Hajime Nagahara and Yasushi Yagi},
  doi          = {10.1109/TPAMI.2022.3196350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4109-4121},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Action recognition from a single coded image},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tale of HodgeRank and spectral method: Target attack
against rank aggregation is the fixed point of adversarial game.
<em>TPAMI</em>, <em>45</em>(4), 4090–4108. (<a
href="https://doi.org/10.1109/TPAMI.2022.3190939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank aggregation with pairwise comparisons has shown promising results in elections, sports competitions, recommendations, and information retrieval. However, little attention has been paid to the security issue of such algorithms, in contrast to numerous research work on the computational and statistical characteristics. Driven by huge profit, the potential adversary has strong motivation and incentives to manipulate the ranking list. Meanwhile, the intrinsic vulnerability of the rank aggregation methods is not well studied in the literature. To fully understand the possible risks, we focus on the purposeful adversary who desires to designate the aggregated results by modifying the pairwise data in this paper. From the perspective of the dynamical system, the attack behavior with a target ranking list is a fixed point belonging to the composition of the adversary and the victim. To perform the targeted attack, we formulate the interaction between the adversary and the victim as a game-theoretic framework consisting of two continuous operators while Nash equilibrium is established. Then two procedures against HodgeRank and RankCentrality are constructed to produce the modification of the original data. Furthermore, we prove that the victims will produce the target ranking list once the adversary masters the complete information. It is noteworthy that the proposed methods allow the adversary only to hold incomplete information or imperfect feedback and perform the purposeful attack. The effectiveness of the suggested target attack strategies is demonstrated by a series of toy simulations and several real-world data experiments. These experimental results show that the proposed methods could achieve the attacker&#39;s goal in the sense that the leading candidate of the perturbed ranking list is the designated one by the adversary.},
  archive      = {J_TPAMI},
  author       = {Ke Ma and Qianqian Xu and Jinshan Zeng and Guorong Li and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3190939},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4090-4108},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A tale of HodgeRank and spectral method: Target attack against rank aggregation is the fixed point of adversarial game},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of self-supervised and few-shot object detection.
<em>TPAMI</em>, <em>45</em>(4), 4071–4089. (<a
href="https://doi.org/10.1109/TPAMI.2022.3199617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeling data is often expensive and time-consuming, especially for tasks such as object detection and instance segmentation, which require dense labeling of the image. While few-shot object detection is about training a model on novel (unseen) object classes with little data, it still requires prior training on many labeled examples of base (seen) classes. On the other hand, self-supervised methods aim at learning representations from unlabeled data which transfer well to downstream tasks such as object detection. Combining few-shot and self-supervised object detection is a promising research direction. In this survey, we review and characterize the most recent approaches on few-shot and self-supervised object detection. Then, we give our main takeaways and discuss future research directions. Project page: https://gabrielhuang.github.io/fsod-survey/ .},
  archive      = {J_TPAMI},
  author       = {Gabriel Huang and Issam Laradji and David Vázquez and Simon Lacoste-Julien and Pau Rodríguez},
  doi          = {10.1109/TPAMI.2022.3199617},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4071-4089},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey of self-supervised and few-shot object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of generalized zero-shot learning methods.
<em>TPAMI</em>, <em>45</em>(4), 4051–4070. (<a
href="https://doi.org/10.1109/TPAMI.2022.3191696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) aims to train a model for classifying data samples under the condition that some output classes are unknown during supervised learning. To address this challenging task, GZSL leverages semantic information of the seen (source) and unseen (target) classes to bridge the gap between both seen and unseen classes. Since its introduction, many GZSL models have been formulated. In this review paper, we present a comprehensive review on GZSL. First, we provide an overview of GZSL including the problems and challenges. Then, we introduce a hierarchical categorization for the GZSL methods and discuss the representative methods in each category. In addition, we discuss the available benchmark data sets and applications of GZSL, along with a discussion on the research gaps and directions for future investigations.},
  archive      = {J_TPAMI},
  author       = {Farhad Pourpanah and Moloud Abdar and Yuxuan Luo and Xinlei Zhou and Ran Wang and Chee Peng Lim and Xi-Zhao Wang and Q. M. Jonathan Wu},
  doi          = {10.1109/TPAMI.2022.3191696},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4051-4070},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A review of generalized zero-shot learning methods},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new automatic hyperparameter recommendation approach under
low-rank tensor completion e framework. <em>TPAMI</em>, <em>45</em>(4),
4038–4050. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter optimization (HPO), characterized by hyperparameter tuning, is not only a critical step for effective modeling but also is the most time-consuming process in machine learning. Traditional search-based algorithms tend to require extensive configuration evaluations for each round to select the desirable hyperparameters during the process, and they are often very inefficient for the implementations on large-scale tasks. In this paper, we study the HPO problem via meta-learning (MtL) approach under the low-rank tensor completion (LRTC) framework. Our proposed approach predicts the performance for hyperparameters of new problems based on their previous performance so that the underlying suitable hyperparameters with better efficiency can be attained. Different from existing approaches, the hyperparameter performance space is instantiated under tensor framework that can preserve the spatial structure and reflect the correlations among the adjacent hyperparameters. When some partial evaluations are available for a new problem, the task of estimating the performance of the unevaluated hyperparameters can be formulated as a tensor completion (TC) problem. Toward the completion purpose, we develop an LRTC algorithm utilizing the sum of nuclear norm (SNN) model. A kernelized version is further developed to capture the nonlinear structure of the performance space. In addition, a corresponding coupled matrix factorization (CMF) algorithm is established to render the predictions solely depend on the meta-features to avoid additional hyperparameter evaluations. Finally, a strategy integrating LRTC and CMF is provided to further enhance the recommendation capacity. We test recommendation performance with our proposed methods for classical SVM and the state-of-the-art deep neural networks such as vision transformer (ViT) and residual network (ResNet), and the obtained results demonstrate the effectiveness of our approaches under various evaluation metrics by comparing with the baselines commonly used for MtL.},
  archive      = {J_TPAMI},
  author       = {Liping Deng and Mingqing Xiao},
  doi          = {10.1109/TPAMI.2022.3195658},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4038-4050},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A new automatic hyperparameter recommendation approach under low-rank tensor completion e framework},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deterministic approximation to neural SDEs.
<em>TPAMI</em>, <em>45</em>(4), 4023–4037. (<a
href="https://doi.org/10.1109/TPAMI.2022.3202237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Stochastic Differential Equations (NSDEs) model the drift and diffusion functions of a stochastic process as neural networks. While NSDEs are known to make accurate predictions, their uncertainty quantification properties have been remained unexplored so far. We report the empirical finding that obtaining well-calibrated uncertainty estimations from NSDEs is computationally prohibitive. As a remedy, we develop a computationally affordable deterministic scheme which accurately approximates the transition kernel, when dynamics is governed by a NSDE. Our method introduces a bidimensional moment matching algorithm: vertical along the neural net layers and horizontal along the time direction, which benefits from an original combination of effective approximations. Our deterministic approximation of the transition kernel is applicable to both training and prediction. We observe in multiple experiments that the uncertainty calibration quality of our method can be matched by Monte Carlo sampling only after introducing high computational cost. Thanks to the numerical stability of deterministic training, our method also improves prediction accuracy.},
  archive      = {J_TPAMI},
  author       = {Andreas Look and Melih Kandemir and Barbara Rakitsch and Jan Peters},
  doi          = {10.1109/TPAMI.2022.3202237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4023-4037},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A deterministic approximation to neural SDEs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deeper look into DeepCap (invited paper). <em>TPAMI</em>,
<em>45</em>(4), 4009–4022. (<a
href="https://doi.org/10.1109/TPAMI.2021.3093553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human performance capture is a highly important computer vision problem with many applications in movie production and virtual/augmented reality. Many previous performance capture approaches either required expensive multi-view setups or did not recover dense space-time coherent geometry with frame-to-frame correspondences. We propose a novel deep learning approach for monocular dense human performance capture. Our method is trained in a weakly supervised manner based on multi-view supervision completely removing the need for training data with 3D ground truth annotations. The network architecture is based on two separate networks that disentangle the task into a pose estimation and a non-rigid surface deformation step. Extensive qualitative and quantitative evaluations show that our approach outperforms the state of the art in terms of quality and robustness. This work is an extended version of [1] where we provide more detailed explanations, comparisons and results as well as applications.},
  archive      = {J_TPAMI},
  author       = {Marc Habermann and Weipeng Xu and Michael Zollhoefer and Gerard Pons-Moll and Christian Theobalt},
  doi          = {10.1109/TPAMI.2021.3093553},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {4009-4022},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A deeper look into DeepCap (Invited paper)},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 1xN pattern for pruning convolutional neural networks.
<em>TPAMI</em>, <em>45</em>(4), 3999–4008. (<a
href="https://doi.org/10.1109/TPAMI.2022.3195774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though network pruning receives popularity in reducing the complexity of convolutional neural networks (CNNs), it remains an open issue to concurrently maintain model accuracy as well as achieve significant speedups on general CPUs. In this paper, we propose a novel 1×N pruning pattern to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1×N pattern prunes these blocks considered unimportant. We also provide a workflow of filter rearrangement that first rearranges the weight matrix in the output channel dimension to derive more influential blocks for accuracy improvements and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1×N pruning can be realized via a parallelized block-wise vectorized operation, leading to significant speedups on general CPUs. The efficacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, given the pruning rate of 50\% and N=4, our pattern obtains about 3.0\% improvements over filter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Our project is made available at https://github.com/lmbxmu/1xN .},
  archive      = {J_TPAMI},
  author       = {Mingbao Lin and Yuxin Zhang and Yuchao Li and Bohong Chen and Fei Chao and Mengdi Wang and Shen Li and Yonghong Tian and Rongrong Ji},
  doi          = {10.1109/TPAMI.2022.3195774},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {3999-4008},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {1xN pattern for pruning convolutional neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding the constraints in maximum entropy methods for
modeling and inference. <em>TPAMI</em>, <em>45</em>(3), 3994–3998. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principle of maximum entropy, developed more than six decades ago, provides a systematic approach to modeling inference, and data analysis grounded in the principles of information theory, Bayesian probability and constrained optimization. Since its formulation, criticisms about the consistency of that method and the role of constraints have been raised. Among these, the chief criticism is that maximum entropy does not satisfy the principle of causation, or similarly, that maximum entropy updating is inconsistent due to an inadequate representation of causal information. We show that these criticisms rest on misunderstanding and misapplication of the way constraints have to be specified within the maximum entropy method. Correction of these problems eliminates the seeming paradoxes and inconsistencies critics claim to have detected. We demonstrate that properly formulated maximum entropy models satisfy the principle of causation.},
  archive      = {J_TPAMI},
  author       = {Amos Golan and Duncan K. Foley},
  doi          = {10.1109/TPAMI.2022.3185394},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3994-3998},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Understanding the constraints in maximum entropy methods for modeling and inference},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold neural network with non-gradient optimization.
<em>TPAMI</em>, <em>45</em>(3), 3986–3993. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) generally takes thousands of iterations to optimize via gradient descent and thus has a slow convergence. In addition, softmax, as a decision layer, may ignore the distribution information of the data during classification. Aiming to tackle the referred problems, we propose a novel manifold neural network based on non-gradient optimization, i.e., the analytical-form solutions. Considering that the activation function is generally invertible, we reconstruct the network via forward ridge regression and low-rank backward approximation, which achieve rapid convergence. Moreover, by unifying the flexible Stiefel manifold and adaptive support vector machine, we devise the novel decision layer which efficiently fits the manifold structure of the data and label information. Consequently, a jointly non-gradient optimization method is designed to generate the network with analytical-form results. Furthermore, an acceleration strategy is utilize to reduce the time complexity for handling high dimensional datasets. Eventually, extensive experiments validate the superior performance of the model.},
  archive      = {J_TPAMI},
  author       = {Rui Zhang and Ziheng Jiao and Hongyuan Zhang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3174574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3986-3993},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Manifold neural network with non-gradient optimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning general and distinctive 3D local deep descriptors
for point cloud registration. <em>TPAMI</em>, <em>45</em>(3), 3979–3985.
(<a href="https://doi.org/10.1109/TPAMI.2022.3175371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective 3D descriptor should be invariant to different geometric transformations, such as scale and rotation, robust to occlusions and clutter, and capable of generalising to different application domains. We present a simple yet effective method to learn general and distinctive 3D local descriptors that can be used to register point clouds that are captured in different domains. Point cloud patches are extracted, canonicalised with respect to their local reference frame, and encoded into scale and rotation-invariant compact descriptors by a deep neural network that is invariant to permutations of the input points. This design is what enables our descriptors to generalise across domains. We evaluate and compare our descriptors with alternative handcrafted and deep learning-based descriptors on several indoor and outdoor datasets that are reconstructed by using both RGBD sensors and laser scanners. Our descriptors outperform most recent descriptors by a large margin in terms of generalisation, and also become the state of the art in benchmarks where training and testing are performed in the same domain.},
  archive      = {J_TPAMI},
  author       = {Fabio Poiesi and Davide Boscaini},
  doi          = {10.1109/TPAMI.2022.3175371},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3979-3985},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning general and distinctive 3D local deep descriptors for point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDFace-celeb-1 m: Benchmarking face hallucination with a
million-scale dataset. <em>TPAMI</em>, <em>45</em>(3), 3968–3978. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep face hallucination methods show stunning performance in super-resolving severely degraded facial images, even surpassing human ability. However, these algorithms are mainly evaluated on non-public synthetic datasets. It is thus unclear how these algorithms perform on public face hallucination datasets. Meanwhile, most of the existing datasets do not well consider the distribution of races, which makes face hallucination methods trained on these datasets biased toward some specific races. To address the above two problems, in this paper, we build a public Ethnically Diverse Face dataset, EDFace-Celeb-1 M, and design a benchmark task for face hallucination. Our dataset includes 1.7 million photos that cover different countries, with relatively balanced race composition. To the best of our knowledge, it is the largest-scale and publicly available face hallucination dataset in the wild. Associated with this dataset, this paper also contributes various evaluation protocols and provides comprehensive analysis to benchmark the existing state-of-the-art methods. The benchmark evaluations demonstrate the performance and limitations of state-of-the-art algorithms. https://github.com/HDCVLab/EDFace-Celeb-1M .},
  archive      = {J_TPAMI},
  author       = {Kaihao Zhang and Dongxu Li and Wenhan Luo and Jingyu Liu and Jiankang Deng and Wei Liu and Stefanos Zafeiriou},
  doi          = {10.1109/TPAMI.2022.3181579},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3968-3978},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EDFace-celeb-1 m: Benchmarking face hallucination with a million-scale dataset},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). You only train once: Learning general and distinctive 3D
local descriptors. <em>TPAMI</em>, <em>45</em>(3), 3949–3967. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting distinctive, robust, and general 3D local features is essential to downstream tasks such as point cloud registration. However, existing methods either rely on noise-sensitive handcrafted features, or depend on rotation-variant neural architectures. It remains challenging to learn robust and general local feature descriptors for surface matching. In this paper, we propose a new, simple yet effective neural network, termed SpinNet, to extract local surface descriptors which are rotation-invariant whilst sufficiently distinctive and general. A Spatial Point Transformer is first introduced to embed the input local surface into an elaborate cylindrical representation (SO(2) rotation-equivariant), further enabling end-to-end optimization of the entire framework. A Neural Feature Extractor, composed of point-based and 3D cylindrical convolutional layers, is then presented to learn representative and general geometric patterns. An invariant layer is finally used to generate rotation-invariant feature descriptors. Extensive experiments on both indoor and outdoor datasets demonstrate that SpinNet outperforms existing state-of-the-art techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with different sensor modalities.},
  archive      = {J_TPAMI},
  author       = {Sheng Ao and Yulan Guo and Qingyong Hu and Bo Yang and Andrew Markham and Zengping Chen},
  doi          = {10.1109/TPAMI.2022.3180341},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3949-3967},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {You only train once: Learning general and distinctive 3D local descriptors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly-supervised video object grounding via causal
intervention. <em>TPAMI</em>, <em>45</em>(3), 3933–3948. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We target at the task of weakly-supervised video object grounding (WSVOG), where only video-sentence annotations are available during model learning. It aims to localize objects described in the sentence to visual regions in the video, which is a fundamental capability needed in pattern analysis and machine learning. Despite the recent progress, existing methods all suffer from the severe problem of spurious association, which will harm the grounding performance. In this paper, we start from the definition of WSVOG and pinpoint the spurious association from two aspects: (1) the association itself is not object-relevant but extremely ambiguous due to weak supervision; and (2) the association is unavoidably confounded by the observational bias when taking the statistics-based matching strategy in existing methods. With this in mind, we design a unified causal framework to learn the deconfounded object-relevant association for more accurate and robust video object grounding. Specifically, we learn the object-relevant association by causal intervention from the perspective of video data generation process. To overcome the problems of lacking fine-grained supervision in terms of intervention, we propose a novel spatial-temporal adversarial contrastive learning paradigm. To further remove the accompanying confounding effect within the object-relevant association, we pursue the true causality by conducting causal intervention via backdoor adjustment. Finally, the deconfounded object-relevant association is learned and optimized under a unified causal framework in an end-to-end manner. Extensive experiments on both IID and OOD testing sets of three benchmarks demonstrate its accurate and robust grounding performance against state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Wei Wang and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TPAMI.2022.3180025},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3933-3948},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Weakly-supervised video object grounding via causal intervention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Video pivoting unsupervised multi-modal machine
translation. <em>TPAMI</em>, <em>45</em>(3), 3918–3932. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenge in the field of unsupervised machine translation (UMT) is to associate source-target sentences in the latent space. As people who speak different languages share biologically similar visual systems, various unsupervised multi-modal machine translation (UMMT) models have been proposed to improve the performances of UMT by employing visual contents in natural images to facilitate alignment. Commonly, relation information is the important semantic in a sentence. Compared with images, videos can better present the interactions between objects and the ways in which an object transforms over time. However, current state-of-the-art methods only explore scene-level or object-level information from images without explicitly modeling objects relation; thus, they are sensitive to spurious correlations, which poses a new challenge for UMMT models. In this paper, we employ a spatial-temporal graph obtained from videos to exploit object interactions in space and time for disambiguation purposes and to promote latent space alignment in UMMT. Our model employs multi-modal back-translation and features pseudo-visual pivoting, in which we learn a shared multilingual visual-semantic embedding space and incorporate visually pivoted captioning as additional weak supervision. Experimental results on the VATEX Translation 2020 and HowToWorld datasets validate the translation capabilities of our model on both sentence-level and word-level and generalizes well when videos are not available during the testing phase.},
  archive      = {J_TPAMI},
  author       = {Mingjie Li and Po-Yao Huang and Xiaojun Chang and Junjie Hu and Yi Yang and Alex Hauptmann},
  doi          = {10.1109/TPAMI.2022.3181116},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3918-3932},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video pivoting unsupervised multi-modal machine translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video joint modelling based on hierarchical transformer for
co-summarization. <em>TPAMI</em>, <em>45</em>(3), 3904–3917. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization aims to automatically generate a summary (storyboard or video skim) of a video, which can facilitate large-scale video retrieval and browsing. Most of the existing methods perform video summarization on individual videos, which neglects the correlations among similar videos. Such correlations, however, are also informative for video understanding and video summarization. To address this limitation, we propose V ideo J oint M odelling based on H ierarchical T ransformer ( VJMHT ) for co-summarization, which takes into consideration the semantic dependencies across videos. Specifically, VJMHT consists of two layers of Transformer: the first layer extracts semantic representation from individual shots of similar videos, while the second layer performs shot-level video joint modelling to aggregate cross-video semantic information. By this means, complete cross-video high-level patterns are explicitly modelled and learned for the summarization of individual videos. Moreover, Transformer-based video representation reconstruction is introduced to maximize the high-level similarity between the summary and the original video. Extensive experiments are conducted to verify the effectiveness of the proposed modules and the superiority of VJMHT in terms of F-measure and rank-based evaluation.},
  archive      = {J_TPAMI},
  author       = {Haopeng Li and Qiuhong Ke and Mingming Gong and Rui Zhang},
  doi          = {10.1109/TPAMI.2022.3186506},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3904-3917},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video joint modelling based on hierarchical transformer for co-summarization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning for maximum consensus robust fitting:
A reinforcement learning approach. <em>TPAMI</em>, <em>45</em>(3),
3890–3903. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust model fitting is a core algorithm in several computer vision applications. Despite being studied for decades, solving this problem efficiently for datasets that are heavily contaminated by outliers is still challenging: due to the underlying computational complexity. A recent focus has been on learning-based algorithms. However, most of these approaches are supervised (which require a large amount of labelled training data). In this paper, we introduce a novel unsupervised learning framework: that learns to directly (without labelled data) solve robust model fitting. Moreover, unlike other learning-based methods , our work is agnostic to the underlying input features, and can be easily generalized to a wide variety of LP-type problems with quasi-convex residuals. We empirically show that our method outperforms existing (un)supervised learning approaches, and also achieves competitive results compared to traditional (non-learning-based) methods. Our approach is designed to try to maximise consensus (MaxCon), similar to the popular RANSAC. The basis of our approach, is to adopt a Reinforcement Learning framework. This requires designing appropriate reward functions, and state encodings. We provide a family of reward functions, tunable by choice of a parameter. We also investigate the application of different basic and enhanced Q-learning components.},
  archive      = {J_TPAMI},
  author       = {Giang Truong and Huu Le and Erchuan Zhang and David Suter and Syed Zulqarnain Gilani},
  doi          = {10.1109/TPAMI.2022.3178442},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3890-3903},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised learning for maximum consensus robust fitting: A reinforcement learning approach},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised contrastive cross-modal hashing.
<em>TPAMI</em>, <em>45</em>(3), 3877–3889. (<a
href="https://doi.org/10.1109/TPAMI.2022.3177356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study how to make unsupervised cross-modal hashing (CMH) benefit from contrastive learning (CL) by overcoming two challenges. To be exact, i) to address the performance degradation issue caused by binary optimization for hashing, we propose a novel momentum optimizer that performs hashing operation learnable in CL, thus making on-the-shelf deep cross-modal hashing possible. In other words, our method does not involve binary-continuous relaxation like most existing methods, thus enjoying better retrieval performance; ii) to alleviate the influence brought by false-negative pairs (FNPs), we propose a Cross-modal Ranking Learning loss (CRL) which utilizes the discrimination from all instead of only the hard negative pairs, where FNP refers to the within-class pairs that were wrongly treated as negative pairs. Thanks to such a global strategy, CRL endows our method with better performance because CRL will not overuse the FNPs while ignoring the true-negative pairs. To the best of our knowledge, the proposed method could be one of the first successful contrastive hashing methods. To demonstrate the effectiveness of the proposed method, we carry out experiments on five widely-used datasets compared with 13 state-of-the-art methods. The code is available at https://github.com/penghu-cs/UCCH .},
  archive      = {J_TPAMI},
  author       = {Peng Hu and Hongyuan Zhu and Jie Lin and Dezhong Peng and Yin-Ping Zhao and Xi Peng},
  doi          = {10.1109/TPAMI.2022.3177356},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3877-3889},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised contrastive cross-modal hashing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer kernel learning for multi-source transfer gaussian
process regression. <em>TPAMI</em>, <em>45</em>(3), 3862–3876. (<a
href="https://doi.org/10.1109/TPAMI.2022.3184696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source transfer regression is a practical and challenging problem where capturing the diverse relatedness of different domains is the key of adaptive knowledge transfer. In this article, we propose an effective way of explicitly modeling the domain relatedness of each domain pair through transfer kernel learning. Specifically, we first discuss the advantages and disadvantages of existing transfer kernels in handling the multi-source transfer regression problem. To cope with the limitations of the existing transfer kernels, we further propose a novel multi-source transfer kernel $k_{ms}$ . The proposed $k_{ms}$ assigns a learnable parametric coefficient to model the relatedness of each inter-domain pair, and simultaneously regulates the relatedness of the intra-domain pair to be 1. Moreover, to capture the heterogeneous data characteristics of multiple domains, $k_{ms}$ exploits different standard kernels for different domain pairs. We further provide a theorem that not only guarantees the positive semi-definiteness of $k_{ms}$ but also conveys a semantic interpretation to the learned domain relatedness. Moreover, the theorem can be easily used in the learning of the corresponding transfer Gaussian process model with $k_{ms}$ . Extensive empirical studies show the effectiveness of our proposed method on domain relatedness modelling and transfer performance.},
  archive      = {J_TPAMI},
  author       = {Pengfei Wei and Thanh Vinh Vo and Xinghua Qu and Yew Soon Ong and Zejun Ma},
  doi          = {10.1109/TPAMI.2022.3184696},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3862-3876},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transfer kernel learning for multi-source transfer gaussian process regression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TN-ZSTAD: Transferable network for zero-shot temporal
activity detection. <em>TPAMI</em>, <em>45</em>(3), 3848–3861. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An integral part of video analysis and surveillance is temporal activity detection, which means to simultaneously recognize and localize activities in long untrimmed videos. Currently, the most effective methods of temporal activity detection are based on deep learning, and they typically perform very well with large scale annotated videos for training. However, these methods are limited in real applications due to the unavailable videos about certain activity classes and the time-consuming data annotation. To solve this challenging problem, we propose a novel task setting called zero-shot temporal activity detection (ZSTAD), where activities that have never been seen in training still need to be detected. We design an end-to-end deep transferable network TN-ZSTAD as the architecture for this solution. On the one hand, this network utilizes an activity graph transformer to predict a set of activity instances that appear in the video, rather than produces many activity proposals in advance. On the other hand, this network captures the common semantics of seen and unseen activities from their corresponding label embeddings, and it is optimized with an innovative loss function that considers the classification property on seen activities and the transfer property on unseen activities together. Experiments on the THUMOS’14, Charades, and ActivityNet datasets show promising performance in terms of detecting unseen activities.},
  archive      = {J_TPAMI},
  author       = {Lingling Zhang and Xiaojun Chang and Jun Liu and Minnan Luo and Zhihui Li and Lina Yao and Alex Hauptmann},
  doi          = {10.1109/TPAMI.2022.3183586},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3848-3861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TN-ZSTAD: Transferable network for zero-shot temporal activity detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Superadditivity and convex optimization for globally optimal
cell segmentation using deformable shape models. <em>TPAMI</em>,
<em>45</em>(3), 3831–3847. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell nuclei segmentation is challenging due to shape variation and closely clustered or partially overlapping objects. Most previous methods are not globally optimal, limited to elliptical models, or are computationally expensive. In this work, we introduce a globally optimal approach based on deformable shape models and global energy minimization for cell nuclei segmentation and cluster splitting. We propose an implicit parameterization of deformable shape models and show that it leads to a convex energy. Convex energy minimization yields the global solution independently of the initialization, is fast, and robust. To jointly perform cell nuclei segmentation and cluster splitting, we developed a novel iterative global energy minimization method, which leverages the inherent property of superadditivity of the convex energy. This property exploits the lower bound of the energy of the union of the models and improves the computational efficiency. Our method provably determines a solution close to global optimality. In addition, we derive a closed-form solution of the proposed global minimization based on the superadditivity property for non-clustered cell nuclei. We evaluated our method using fluorescence microscopy images of five different cell types comprising various challenges, and performed a quantitative comparison with previous methods. Our method achieved state-of-the-art or improved performance.},
  archive      = {J_TPAMI},
  author       = {Leonid Kostrykin and Karl Rohr},
  doi          = {10.1109/TPAMI.2022.3185583},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3831-3847},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Superadditivity and convex optimization for globally optimal cell segmentation using deformable shape models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spoof trace disentanglement for generic face anti-spoofing.
<em>TPAMI</em>, <em>45</em>(3), 3813–3830. (<a
href="https://doi.org/10.1109/TPAMI.2022.3176387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior studies show that the key to face anti-spoofing lies in the subtle image patterns, termed “spoof trace,” e.g. , color distortion, 3D mask edge, and Moiré pattern. Spoof detection rooted on those spoof traces can improve not only the model&#39;s generalization but also the interpretability. Yet, it is a challenging task due to the diversity of spoof attacks and the lack of ground truth for spoof traces. In this work, we propose a novel adversarial learning framework to explicitly estimate the spoof related patterns for face anti-spoofing. Inspired by the physical process, spoof faces are disentangled into spoof traces and the live counterparts in two steps: additive step and inpainting step. This two-step modeling can effectively narrow down the searching space for adversarial learning of spoof trace. Based on the trace modeling, the disentangled spoof traces can be utilized to reversely construct new spoof faces, which is used as data augmentation to effectively tackle long-tail spoof types. In addition, we apply frequency-based image decomposition in both the input and disentangled traces to better reflect the low-level vision cues. Our approach demonstrates superior spoof detection performance on 3 testing scenarios: known attacks, unknown attacks, and open-set attacks. Meanwhile, it provides a visually-convincing estimation of the spoof traces. Source code and pre-trained models will be publicly available upon publication.},
  archive      = {J_TPAMI},
  author       = {Yaojie Liu and Xiaoming Liu},
  doi          = {10.1109/TPAMI.2022.3176387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3813-3830},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spoof trace disentanglement for generic face anti-spoofing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SipMaskv2: Enhanced fast image and video instance
segmentation. <em>TPAMI</em>, <em>45</em>(3), 3798–3812. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast single-stage method for both image and video instance segmentation, called SipMask, that preserves the instance spatial information by performing multiple sub-region mask predictions. The main module in our method is a light-weight spatial preservation (SP) module that generates a separate set of spatial coefficients for the sub-regions within a bounding-box, enabling a better delineation of spatially adjacent instances. To better correlate mask prediction with object detection, we further propose a mask alignment weighting loss and a feature alignment scheme. In addition, we identify two issues that impede the performance of single-stage instance segmentation and introduce two modules, including a sample selection scheme and an instance refinement module, to address these two issues. Experiments are performed on both image instance segmentation dataset MS COCO and video instance segmentation dataset YouTube-VIS. On MS COCO test-dev set, our method achieves a state-of-the-art performance. In terms of real-time capabilities, it outperforms YOLACT by a gain of 3.0\% (mask AP) under the similar settings, while operating at a comparable speed. On YouTube-VIS validation set, our method also achieves promising results. The source code is available at https://github.com/JialeCao001/SipMask .},
  archive      = {J_TPAMI},
  author       = {Jiale Cao and Yanwei Pang and Rao Muhammad Anwer and Hisham Cholakkal and Fahad Shahbaz Khan and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3180564},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3798-3812},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SipMaskv2: Enhanced fast image and video instance segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIGN: Statistical inference graphs based on probabilistic
network activity interpretation. <em>TPAMI</em>, <em>45</em>(3),
3783–3797. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved superior accuracy in many visual-related tasks. However, the inference process through a CNN&#39;s intermediate layers is opaque, making it difficult to interpret such networks or develop trust in their operation. In this article, we introduce SIGN method for modeling the network&#39;s hidden layer activity using probabilistic models. The activity patterns in layers of interest are modeled as Gaussian mixture models, and transition probabilities between clusters in consecutive modeled layers are estimated to identify paths of inference. For fully connected networks, the entire layer activity is clustered, and the resulting model is a hidden Markov model. For convolutional layers, spatial columns of activity are clustered, and a maximum likelihood model is developed for mining an explanatory inference graph. The graph describes the hierarchy of activity clusters most relevant for network prediction. We show that such inference graphs are useful for understanding the general inference process of a class, as well as explaining the (correct or incorrect) decisions the network makes about specific images. In addition, SIGN provide interesting observations regarding hidden layer activity in general, including the concentration of memorization in a single middle layer in fully connected networks, and a highly local nature of column activities in the top CNN layers.},
  archive      = {J_TPAMI},
  author       = {Yael Konforti and Alon Shpigler and Boaz Lerner and Aharon Bar-Hillel},
  doi          = {10.1109/TPAMI.2022.3181472},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3783-3797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SIGN: Statistical inference graphs based on probabilistic network activity interpretation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic layout manipulation with high-resolution sparse
attention. <em>TPAMI</em>, <em>45</em>(3), 3768–3782. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of semantic image layout manipulation, which aims to manipulate an input image by editing its semantic label map. A core problem of this task is how to transfer visual details from the input images to the new semantic layout while making the resulting image visually realistic. Recent work on learning cross-domain correspondence has shown promising results for global layout transfer with dense attention-based warping. However, this method tends to lose texture details due to the resolution limitation and the lack of smoothness constraint on correspondence. To adapt this paradigm for the layout manipulation task, we propose a high-resolution sparse attention module that effectively transfers visual details to new layouts at a resolution up to 512x512. To further improve visual quality, we introduce a novel generator architecture consisting of a semantic encoder and a two-stage decoder for coarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets demonstrate that our proposed approach achieves substantial improvements over the existing inpainting and layout manipulation methods.},
  archive      = {J_TPAMI},
  author       = {Haitian Zheng and Zhe Lin and Jingwan Lu and Scott Cohen and Jianming Zhang and Ning Xu and Jiebo Luo},
  doi          = {10.1109/TPAMI.2022.3181587},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3768-3782},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semantic layout manipulation with high-resolution sparse attention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seed the views: Hierarchical semantic alignment for
contrastive representation learning. <em>TPAMI</em>, <em>45</em>(3),
3753–3767. (<a
href="https://doi.org/10.1109/TPAMI.2022.3176690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, conventional contrastive learning does not model the relation between semantically similar samples explicitly. In this paper, we propose a general module that considers the semantic similarity among images. This is achieved by expanding the views generated by a single image to Cross-Samples and Multi-Levels , and modeling the invariance to semantically similar images in a hierarchical way. Specifically, the cross-samples are generated by a data mixing operation, which is constrained within samples that are semantically similar, while the multi-level samples are expanded at the intermediate layers of a network. In this way, the contrastive loss is extended to allow for multiple positives per anchor, and explicitly pulling semantically similar images together at different layers of the network. Our method, termed as CSML, has the ability to integrate multi-level representations across samples in a robust way. CSML is applicable to current contrastive based methods and consistently improves the performance. Notably, using MoCo v2 as an instantiation, CSML achieves 76.6\% top-1 accuracy with linear evaluation using ResNet-50 as backbone, 66.7\% and 75.1\% top-1 accuracy with only 1\% and 10\% labels, respectively. All these numbers set the new state-of-the-art. The code is available at https://github.com/haohang96/CSML .},
  archive      = {J_TPAMI},
  author       = {Haohang Xu and Xiaopeng Zhang and Hao Li and Lingxi Xie and Wenrui Dai and Hongkai Xiong and Qi Tian},
  doi          = {10.1109/TPAMI.2022.3176690},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3753-3767},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Seed the views: Hierarchical semantic alignment for contrastive representation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salient object detection via integrity learning.
<em>TPAMI</em>, <em>45</em>(3), 3738–3752. (<a
href="https://doi.org/10.1109/TPAMI.2022.3179526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although current salient object detection (SOD) works have achieved significant progress, they are limited when it comes to the integrity of the predicted salient regions. We define the concept of integrity at both a micro and macro level. Specifically, at the micro level, the model should highlight all parts that belong to a certain salient object. Meanwhile, at the macro level, the model needs to discover all salient objects in a given image. To facilitate integrity learning for SOD, we design a novel I ntegrity Co gnition N etwork ( ICON ), which explores three important components for learning strong integrity features. 1) Unlike existing models, which focus more on feature discriminability, we introduce a diverse feature aggregation (DFA) component to aggregate features with various receptive fields (i.e., kernel shape and context) and increase feature diversity. Such diversity is the foundation for mining the integral salient objects. 2) Based on the DFA features, we introduce an integrity channel enhancement (ICE) component with the goal of enhancing feature channels that highlight the integral salient objects, while suppressing the other distracting ones. 3) After extracting the enhanced features, the part-whole verification (PWV) method is employed to determine whether the part and whole object features have strong agreement. Such part-whole agreements can further improve the micro-level integrity for each salient object. To demonstrate the effectiveness of our ICON, comprehensive experiments are conducted on seven challenging benchmarks. Our ICON outperforms the baseline methods in terms of a wide range of metrics. Notably, our ICON achieves $\sim$ 10\% relative improvement over the previous best model in terms of average false negative ratio (FNR), on six datasets. Codes and results are available at: https://github.com/mczhuge/ICON .},
  archive      = {J_TPAMI},
  author       = {Mingchen Zhuge and Deng-Ping Fan and Nian Liu and Dingwen Zhang and Dong Xu and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3179526},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3738-3752},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Salient object detection via integrity learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting unsupervised meta-learning via the
characteristics of few-shot tasks. <em>TPAMI</em>, <em>45</em>(3),
3721–3737. (<a
href="https://doi.org/10.1109/TPAMI.2022.3179368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning has become a practical approach towards few-shot image classification, where “a strategy to learn a classifier” is meta-learned on labeled base classes and can be applied to tasks with novel classes. We remove the requirement of base class labels and learn generalizable embeddings via Unsupervised Meta-Learning (UML). Specifically, episodes of tasks are constructed with data augmentations from unlabeled base classes during meta-training, and we apply embedding-based classifiers to novel tasks with labeled few-shot examples during meta-test. We observe two elements play important roles in UML, i.e., the way to sample tasks and measure similarities between instances. Thus we obtain a strong baseline with two simple modifications — a sufficient sampling strategy constructing multiple tasks per episode efficiently together with a semi-normalized similarity. We then take advantage of the characteristics of tasks from two directions to get further improvements. First, synthesized confusing instances are incorporated to help extract more discriminative embeddings. Second, we utilize an additional task-specific embedding transformation as an auxiliary component during meta-training to promote the generalization ability of the pre-adapted embeddings. Experiments on few-shot learning benchmarks verify that our approaches outperform previous UML methods and achieve comparable or even better performance than its supervised variants.},
  archive      = {J_TPAMI},
  author       = {Han-Jia Ye and Lu Han and De-Chuan Zhan},
  doi          = {10.1109/TPAMI.2022.3179368},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3721-3737},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting unsupervised meta-learning via the characteristics of few-shot tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ResNet-LDDMM: Advancing the LDDMM framework using deep
residual networks. <em>TPAMI</em>, <em>45</em>(3), 3707–3720. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deformable registration, the Riemannian framework – Large Deformation Diffeomorphic Metric Mapping, or LDDMM for short – has inspired numerous techniques for comparing, deforming, averaging and analyzing shapes or images. Grounded in flows of vector fields, akin to the equations of motion used in fluid dynamics, LDDMM algorithms solve the flow equation in the space of plausible deformations, i.e., diffeomorphisms. In this work, we make use of deep residual neural networks to solve the non-stationary ODE (flow equation) based on an Euler&#39;s discretization scheme. The central idea is to represent time-dependent velocity fields as fully connected ReLU neural networks (building blocks) and derive optimal weights by minimizing a regularized loss function. Computing minimizing paths between deformations, thus between shapes, turns to find optimal network parameters by back-propagating over the intermediate building blocks. Geometrically, at each time step, our algorithm searches for an optimal partition of the space into multiple polytopes, and then computes optimal velocity vectors as affine transformations on each of these polytopes. As a result, different parts of the shape, even if they are close (such as two fingers of a hand), can be made to belong to different polytopes, and therefore be moved in different directions without costing too much energy. Importantly, we show how diffeomorphic transformations, or more precisely bilipshitz transformations, are predicted by our registration algorithm. We illustrate these ideas on diverse registration problems of 3D shapes under complex topology-preserving transformations. We thus provide essential foundations for more advanced shape variability analysis under a novel joint geometric-neural networks Riemannian-like framework, i.e., ResNet-LDDMM.},
  archive      = {J_TPAMI},
  author       = {Boulbaba Ben Amor and Sylvain Arguillère and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3174908},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3707-3720},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ResNet-LDDMM: Advancing the LDDMM framework using deep residual networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ResLT: Residual learning for long-tailed recognition.
<em>TPAMI</em>, <em>45</em>(3), 3695–3706. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms face great challenges with long-tailed data distribution which, however, is quite a common case in real-world scenarios. Previous methods tackle the problem from either the aspect of input space (re-sampling classes with different frequencies) or loss space (re-weighting classes with different weights), suffering from heavy over-fitting to tail classes or hard optimization during training. To alleviate these issues, we propose a more fundamental perspective for long-tailed recognition, i.e., from the aspect of parameter space, and aims to preserve specific capacity for classes with low frequencies. From this perspective, the trivial solution utilizes different branches for the head, medium, tail classes respectively, and then sums their outputs as the final results is not feasible. Instead, we design the effective residual fusion mechanism – with one main branch optimized to recognize images from all classes, another two residual branches are gradually fused and optimized to enhance images from medium+tail classes and tail classes respectively. Then the branches are aggregated into final results by additive shortcuts. We test our method on several benchmarks, i.e., long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and iNaturalist 2018. Experimental results manifest the effectiveness of our method. Our code is available at https://github.com/jiequancui/ResLT .},
  archive      = {J_TPAMI},
  author       = {Jiequan Cui and Shu Liu and Zhuotao Tian and Zhisheng Zhong and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3174892},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3695-3706},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ResLT: Residual learning for long-tailed recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation matters: Foreground-aware graph-based relational
reasoning for domain adaptive object detection. <em>TPAMI</em>,
<em>45</em>(3), 3677–3694. (<a
href="https://doi.org/10.1109/TPAMI.2022.3179445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptive Object Detection (DAOD) focuses on improving the generalization ability of object detectors via knowledge transfer. Recent advances in DAOD strive to change the emphasis of the adaptation process from global to local in virtue of fine-grained feature alignment methods. However, both the global and local alignment approaches fail to capture the topological relations among different foreground objects as the explicit dependencies and interactions between and within domains are neglected. In this case, only seeking one-vs-one alignment does not necessarily ensure the precise knowledge transfer. Moreover, conventional alignment-based approaches may be vulnerable to catastrophic overfitting regarding those less transferable regions (e.g., backgrounds) due to the accumulation of inaccurate localization results in the target domain. To remedy these issues, we first formulate DAOD as an open-set domain adaptation problem, in which the foregrounds and backgrounds are seen as the “known classes” and “unknown class” respectively. Accordingly, we propose a new and general framework for DAOD, named Foreground-aware Graph-based Relational Reasoning (FGRR), which incorporates graph structures into the detection pipeline to explicitly model the intra- and inter-domain foreground object relations on both pixel and semantic spaces, thereby endowing the DAOD model with the capability of relational reasoning beyond the popular alignment-based paradigm. FGRR first identifies the foreground pixels and regions by searching reliable correspondence and cross-domain similarity regularization respectively. The inter-domain visual and semantic correlations are hierarchically modeled via bipartite graph structures, and the intra-domain relations are encoded via graph attention mechanisms. Through message-passing, each node aggregates semantic and contextual information from the same and opposite domain to substantially enhance its expressive power. Empirical results demonstrate that the proposed FGRR exceeds the state-of-the-art performance on four DAOD benchmarks.},
  archive      = {J_TPAMI},
  author       = {Chaoqi Chen and Jiongcheng Li and Hong-Yu Zhou and Xiaoguang Han and Yue Huang and Xinghao Ding and Yizhou Yu},
  doi          = {10.1109/TPAMI.2022.3179445},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3677-3694},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Relation matters: Foreground-aware graph-based relational reasoning for domain adaptive object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RED++: Data-free pruning of deep neural networks via input
splitting and output merging. <em>TPAMI</em>, <em>45</em>(3), 3664–3676.
(<a href="https://doi.org/10.1109/TPAMI.2022.3179616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning Deep Neural Networks (DNNs) is a prominent field of study in the goal of inference runtime acceleration. In this paper, we introduce a novel data-free pruning protocol RED++. Only requiring a trained neural network, and not specific to any particular DNN, we exploit an adaptive data-free scalar hashing which exhibits redundancies among neuron weight values. We study the theoretical and empirical guarantees on the preservation of the accuracy from the hashing as well as the expected pruning ratio resulting from the exploitation of said redundancies. We propose a novel data-free pruning technique of DNN layers which removes the input-wise redundant operations. This algorithm is straightforward, parallelizable and offers novel perspective on DNN pruning by shifting the burden of large computation to efficient memory access and allocation. We provide theoretical guarantees on RED++ performance and empirically demonstrate its superiority over other data-free pruning methods and its competitiveness with data-driven ones on ResNets, MobileNets, and EfficientNets.},
  archive      = {J_TPAMI},
  author       = {Edouard Yvinec and Arnaud Dapogny and Matthieu Cord and Kevin Bailly},
  doi          = {10.1109/TPAMI.2022.3179616},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3664-3676},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RED++: Data-free pruning of deep neural networks via input splitting and output merging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rectified wasserstein generative adversarial networks for
perceptual image restoration. <em>TPAMI</em>, <em>45</em>(3), 3648–3663.
(<a href="https://doi.org/10.1109/TPAMI.2022.3185316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wasserstein generative adversarial network (WGAN) has attracted great attention due to its solid mathematical background, i.e., to minimize the Wasserstein distance between the generated distribution and the distribution of interest. In WGAN, the Wasserstein distance is quantitatively evaluated by the discriminator, also known as the critic . The vanilla WGAN trained the critic with the simple Lipschitz condition, which was later shown less effective for modeling complex distributions, like the distribution of natural images. We try to improve the WGAN training by introducing pairwise constraint on the critic, oriented to image restoration tasks. In principle, pairwise constraint is to suggest the critic assign a higher rating to the original (real) image than to the restored (generated) image, as long as such a pair of images are available. We show that such pairwise constraint may be implemented by rectifying the gradients in WGAN training, which leads to the proposed rectified Wasserstein generative adversarial network (ReWaGAN). In addition, we build interesting connections between ReWaGAN and the perception-distortion tradeoff. We verify ReWaGAN on two representative image restoration tasks: single image super-resolution (4× and 8×) and compression artifact reduction, where our ReWaGAN not only beats the vanilla WGAN consistently, but also outperforms the state-of-the-art perceptual quality-oriented methods significantly. Our code and models are publicly available at https://github.com/mahaichuan/ReWaGAN .},
  archive      = {J_TPAMI},
  author       = {Haichuan Ma and Dong Liu and Feng Wu},
  doi          = {10.1109/TPAMI.2022.3185316},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3648-3663},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rectified wasserstein generative adversarial networks for perceptual image restoration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random and adversarial bit error robustness:
Energy-efficient and secure DNN accelerators. <em>TPAMI</em>,
<em>45</em>(3), 3632–3647. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) accelerators received considerable attention in recent years due to the potential to save energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption, however, causes bit-level failures in the memory storing the quantized weights. Furthermore, DNN accelerators are vulnerable to adversarial attacks on voltage controllers or individual bits. In this paper, we show that a combination of robust fixed-point quantization , weight clipping , as well as random bit error training ( RandBET ) or adversarial bit error training ( AdvBET ) improves robustness against random or adversarial bit errors in quantized DNN weights significantly . This leads not only to high energy savings for low-voltage operation as well as low-precision quantization, but also improves security of DNN accelerators. In contrast to related work, our approach generalizes across operating voltages and accelerators and does not require hardware changes. Moreover, we present a novel adversarial bit error attack and are able to obtain robustness against both targeted and untargeted bit-level attacks. Without losing more than 0.8\%/2\% in test accuracy, we can reduce energy consumption on CIFAR10by 20\%/30\% for 8/4-bit quantization. Allowing up to 320 adversarial bit errors, we reduce test error from above 90\% (chance level) to 26.22\%.},
  archive      = {J_TPAMI},
  author       = {David Stutz and Nandhini Chandramoorthy and Matthias Hein and Bernt Schiele},
  doi          = {10.1109/TPAMI.2022.3181972},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3632-3647},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Random and adversarial bit error robustness: Energy-efficient and secure DNN accelerators},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radar-based shape and reflectivity reconstruction using
active surfaces and the level set method. <em>TPAMI</em>,
<em>45</em>(3), 3617–3631. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a multiview shape reconstruction problem based on an active surface model whose geometric evolution is driven by radar measurements acquired at sparse locations. Building on our previous work in the context of variational methods for the reconstruction of a scene conceptualized as the graph of a function, we generalize this inversion approach for a general geometry, now described by an active surface, strongly motivated by prior variational computer vision approaches to multiview stereo reconstruction from camera images. While conceptually similar, use of radar echoes within a variational scheme to drive the active surface evolution requires significant changes in regularization strategies compared to prior image based methodologies for the active surface evolution to work effectively. We describe all of these aspects and how we addressed them. While our long term objective is to develop a framework capable of fusing radar as well as other image based information, in which the active surface becomes an explicit shared reference for data fusion. In this paper, we explore the reconstruction using radar as a single modality, demonstrating that the presented approach can provide reconstructions of quality comparable to those from image based methods showing great potential for further development toward data fusion.},
  archive      = {J_TPAMI},
  author       = {Samuel Bignardi and Romeil Sandhu and Anthony Yezzi},
  doi          = {10.1109/TPAMI.2022.3178969},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3617-3631},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Radar-based shape and reflectivity reconstruction using active surfaces and the level set method},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization induced equilibrium networks: An explicit
optimization perspective for understanding equilibrium models.
<em>TPAMI</em>, <em>45</em>(3), 3604–3616. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reveal the mystery behind deep neural networks (DNNs), optimization may offer a good perspective. There are already some clues showing the strong connection between DNNs and optimization problems, e.g., under a mild condition, DNN&#39;s activation function is indeed a proximal operator. In this paper, we are committed to providing a unified optimization induced interpretability for a special class of networks—equilibrium models, i.e., neural networks defined by fixed point equations, which have become increasingly attractive recently. To this end, we first decompose DNNs into a new class of unit layer that is the proximal operator of an implicit convex function while keeping its output unchanged. Then, the equilibrium model of the unit layer can be derived, we name it Optimization Induced Equilibrium Networks (OptEq). The equilibrium point of OptEq can be theoretically connected to the solution of a convex optimization problem with explicit objectives. Based on this, we can flexibly introduce prior properties to the equilibrium points: 1) modifying the underlying convex problems explicitly so as to change the architectures of OptEq; and 2) merging the information into the fixed point iteration, which guarantees to choose the desired equilibrium point when the fixed point set is non-singleton. We show that OptEq outperforms previous implicit models even with fewer parameters.},
  archive      = {J_TPAMI},
  author       = {Xingyu Xie and Qiuhao Wang and Zenan Ling and Xia Li and Guangcan Liu and Zhouchen Lin},
  doi          = {10.1109/TPAMI.2022.3181425},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3604-3616},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimization induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OPOM: Customized invisible cloak towards face privacy
protection. <em>TPAMI</em>, <em>45</em>(3), 3590–3603. (<a
href="https://doi.org/10.1109/TPAMI.2022.3175602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While convenient in daily life, face recognition technologies also raise privacy concerns for regular users on the social media since they could be used to analyze face images and videos, efficiently and surreptitiously without any security restrictions. In this paper, we investigate the face privacy protection from a technology standpoint based on a new type of customized cloak, which can be applied to all the images of a regular user, to prevent malicious face recognition systems from uncovering their identity. Specifically, we propose a new method, named one person one mask (OPOM), to generate person-specific (class-wise) universal masks by optimizing each training sample in the direction away from the feature subspace of the source identity. To make full use of the limited training images, we investigate several modeling methods, including affine hulls, class centers and convex hulls, to obtain a better description of the feature subspace of source identities. The effectiveness of the proposed method is evaluated on both common and celebrity datasets against black-box face recognition models with different loss functions and network architectures. In addition, we discuss the advantages and potential problems of the proposed method. In particular, we conduct an application study on the privacy protection of a video dataset, Sherlock, to demonstrate the potential practical usage of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Yaoyao Zhong and Weihong Deng},
  doi          = {10.1109/TPAMI.2022.3175602},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3590-3603},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {OPOM: Customized invisible cloak towards face privacy protection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online metro origin-destination prediction via heterogeneous
information aggregation. <em>TPAMI</em>, <em>45</em>(3), 3574–3589. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metro origin-destination prediction is a crucial yet challenging time-series analysis task in intelligent transportation systems, which aims to accurately forecast two specific types of cross-station ridership, i.e., Origin-Destination (OD) one and Destination-Origin (DO) one. However, complete OD matrices of previous time intervals can not be obtained immediately in online metro systems, and conventional methods only used limited information to forecast the future OD and DO ridership separately. In this work, we proposed a novel neural network module termed Heterogeneous Information Aggregation Machine (HIAM), which fully exploits heterogeneous information of historical data (e.g., incomplete OD matrices, unfinished order vectors, and DO matrices) to jointly learn the evolutionary patterns of OD and DO ridership. Specifically, an OD modeling branch estimates the potential destinations of unfinished orders explicitly to complement the information of incomplete OD matrices, while a DO modeling branch takes DO matrices as input to capture the spatial-temporal distribution of DO ridership. Moreover, a Dual Information Transformer is introduced to propagate the mutual information among OD features and DO features for modeling the OD-DO causality and correlation. Based on the proposed HIAM, we develop a unified Seq2Seq network to forecast the future OD and DO ridership simultaneously. Extensive experiments conducted on two large-scale benchmarks demonstrate the effectiveness of our method for online metro origin-destination prediction. Our code is resealed at https://github.com/HCPLab-SYSU/HIAM .},
  archive      = {J_TPAMI},
  author       = {Lingbo Liu and Yuying Zhu and Guanbin Li and Ziyi Wu and Lei Bai and Liang Lin},
  doi          = {10.1109/TPAMI.2022.3178184},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3574-3589},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Online metro origin-destination prediction via heterogeneous information aggregation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the optimality of sufficient statistics-based quantizers.
<em>TPAMI</em>, <em>45</em>(3), 3567–3573. (<a
href="https://doi.org/10.1109/TPAMI.2022.3172282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $X$ be a random variable taking values in a set $\mathcal {X}$ , and let $\lbrace P_{\theta }; \theta \in \Theta \rbrace$ be a family of distributions indexed by the parameter vector $\theta$ taking values in a set $\Theta$ . A quantized random variable $\gamma (X)$ is obtained by employing a quantizer $\gamma : \mathcal {X}\rightarrow \lbrace 1,\ldots,K\rbrace$ . It is shown that any extreme point of the set of all possible probability distributions of $\gamma (X)$ can be achieved by a deterministic quantizer that decides based only on the sufficient statistics. Using this fact, optimality properties of deterministic sufficient statistics-based quantizers are established for the problem of parameter estimation. It is proven that there always exists an optimal partitioning of sufficient statistics into $K$ convex polytopes which maximizes the trace of the Fisher information matrix when $\lbrace P_{\theta }; \theta \in \Theta \rbrace$ belongs to the exponential family. Furthermore, the optimality of likelihood ratio statistic for simple hypothesis testing follows as a consequence of this result, thereby demonstrating a link between parameter estimation and hypothesis testing.},
  archive      = {J_TPAMI},
  author       = {Berkan Dulek},
  doi          = {10.1109/TPAMI.2022.3172282},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3567-3573},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the optimality of sufficient statistics-based quantizers},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). On the eigenvalues of global covariance pooling for
fine-grained visual recognition. <em>TPAMI</em>, <em>45</em>(3),
3554–3566. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fine-Grained Visual Categorization (FGVC) is challenging because the subtle inter-class variations are difficult to be captured. One notable research line uses the Global Covariance Pooling (GCP) layer to learn powerful representations with second-order statistics, which can effectively model inter-class differences. In our previous conference paper, we show that truncating small eigenvalues of the GCP covariance can attain smoother gradient and improve the performance on large-scale benchmarks. However, on fine-grained datasets, truncating the small eigenvalues would make the model fail to converge. This observation contradicts the common assumption that the small eigenvalues merely correspond to the noisy and unimportant information. Consequently, ignoring them should have little influence on the performance. To diagnose this peculiar behavior, we propose two attribution methods whose visualizations demonstrate that the seemingly unimportant small eigenvalues are crucial as they are in charge of extracting the discriminative class-specific features. Inspired by this observation, we propose a network branch dedicated to magnifying the importance of small eigenvalues. Without introducing any additional parameters, this branch simply amplifies the small eigenvalues and achieves state-of-the-art performances of GCP methods on three fine-grained benchmarks. Furthermore, the performance is also competitive against other FGVC approaches on larger datasets. Code is available at https://github.com/KingJamesSong/DifferentiableSVD .},
  archive      = {J_TPAMI},
  author       = {Yue Song and Nicu Sebe and Wei Wang},
  doi          = {10.1109/TPAMI.2022.3178802},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3554-3566},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the eigenvalues of global covariance pooling for fine-grained visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). MVSS-net: Multi-view multi-scale supervised networks for
image manipulation detection. <em>TPAMI</em>, <em>45</em>(3), 3539–3553.
(<a href="https://doi.org/10.1109/TPAMI.2022.3180556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As manipulating images by copy-move, splicing and/or inpainting may lead to misinterpretation of the visual content, detecting these sorts of manipulations is crucial for media forensics. Given the variety of possible attacks on the content, devising a generic method is nontrivial. Current deep learning based methods are promising when training and test data are well aligned, but perform poorly on independent tests. Moreover, due to the absence of authentic test images, their image-level detection specificity is in doubt. The key question is how to design and train a deep neural network capable of learning generalizable features sensitive to manipulations in novel data, whilst specific to prevent false alarms on the authentic. We propose multi-view feature learning to jointly exploit tampering boundary artifacts and the noise view of the input image. As both clues are meant to be semantic-agnostic, the learned features are thus generalizable. For effectively learning from authentic images, we train with multi-scale (pixel / edge / image) supervision. We term the new network MVSS-Net and its enhanced version MVSS-Net++. Experiments are conducted in both within-dataset and cross-dataset scenarios, showing that MVSS-Net++ performs the best, and exhibits better robustness against JPEG compression, Gaussian blur and screenshot based image re-capturing.},
  archive      = {J_TPAMI},
  author       = {Chengbo Dong and Xinru Chen and Ruohan Hu and Juan Cao and Xirong Li},
  doi          = {10.1109/TPAMI.2022.3180556},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3539-3553},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MVSS-net: Multi-view multi-scale supervised networks for image manipulation detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMNet: A model-based multimodal network for human action
recognition in RGB-d videos. <em>TPAMI</em>, <em>45</em>(3), 3522–3538.
(<a href="https://doi.org/10.1109/TPAMI.2022.3177813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) in RGB-D videos has been widely investigated since the release of affordable depth sensors. Currently, unimodal approaches (e.g., skeleton-based and RGB video-based) have realized substantial improvements with increasingly larger datasets. However, multimodal methods specifically with model-level fusion have seldom been investigated. In this article, we propose a model-based multimodal network (MMNet) that fuses skeleton and RGB modalities via a model-based approach. The objective of our method is to improve ensemble recognition accuracy by effectively applying mutually complementary information from different data modalities. For the model-based fusion scheme, we use a spatiotemporal graph convolution network for the skeleton modality to learn attention weights that will be transferred to the network of the RGB modality. Extensive experiments are conducted on five benchmark datasets: NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, Northwestern-UCLA Multiview, and Toyota Smarthome. Upon aggregating the results of multiple modalities, our method is found to outperform state-of-the-art approaches on six evaluation protocols of the five datasets; thus, the proposed MMNet can effectively capture mutually complementary features in different RGB-D video modalities and provide more discriminative features for HAR. We also tested our MMNet on an RGB video dataset Kinetics 400 that contains more outdoor actions, which shows consistent results with those of RGB-D video datasets.},
  archive      = {J_TPAMI},
  author       = {Bruce X.B. Yu and Yan Liu and Xiang Zhang and Sheng-hua Zhong and Keith C.C. Chan},
  doi          = {10.1109/TPAMI.2022.3177813},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3522-3538},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MMNet: A model-based multimodal network for human action recognition in RGB-D videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MLR-SNet: Transferable LR schedules for heterogeneous tasks.
<em>TPAMI</em>, <em>45</em>(3), 3505–3521. (<a
href="https://doi.org/10.1109/TPAMI.2022.3184315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning rate (LR) is one of the most important hyperparameters in stochastic gradient descent (SGD) algorithm for training deep neural networks (DNN). However, current hand-designed LR schedules need to manually pre-specify a fixed form, which limits their ability to adapt to practical non-convex optimization problems due to the significant diversification of training dynamics. Meanwhile, it always needs to search proper LR schedules from scratch for new tasks, which, however, are often largely different with task variations, like data modalities, network architectures, or training data capacities. To address this learning-rate-schedule setting issue, we propose to parameterize LR schedules with an explicit mapping formulation, called MLR-SNet . The learnable parameterized structure brings more flexibility for MLR-SNet to learn a proper LR schedule to comply with the training dynamics of DNN. Image and text classification benchmark experiments substantiate the capability of our method for achieving proper LR schedules. Moreover, the explicit parameterized structure makes the meta-learned LR schedules capable of being transferable and plug-and-play, which can be easily generalized to new heterogeneous tasks. We transfer our meta-learned MLR-SNet to query tasks like different training epochs, network architectures, data modalities, dataset sizes from the training ones, and achieve comparable or even better performance compared with hand-designed LR schedules specifically designed for the query tasks. The robustness of MLR-SNet is also substantiated when the training data are biased with corrupted noise. We further prove the convergence of the SGD algorithm equipped with LR schedule produced by our MLR-SNet, with the convergence rate comparable to the best-known ones of the algorithm for solving the problem. The source code of our method is released at https://github.com/xjtushujun/MLR-SNet .},
  archive      = {J_TPAMI},
  author       = {Jun Shu and Yanwen Zhu and Qian Zhao and Deyu Meng and Zongben Xu},
  doi          = {10.1109/TPAMI.2022.3184315},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3505-3521},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MLR-SNet: Transferable LR schedules for heterogeneous tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mirror detection with the visual chirality cue.
<em>TPAMI</em>, <em>45</em>(3), 3492–3504. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mirror detection is challenging because the visual appearances of mirrors change depending on those of their surroundings. As existing mirror detection methods are mainly based on extracting contextual contrast and relational similarity between mirror and non-mirror regions, they may fail to identify a mirror region if these assumptions are violated. Inspired by a recent study of applying a CNN to help distinguish whether an image is flipped or not based on the visual chirality property, in this paper, we rethink this image-level visual chirality property and reformulate it as a learnable pixel level cue for mirror detection. Specifically, we first propose a novel flipping-convolution-flipping (FCF) transformation to model visual chirality as learnable commutative residual. We then propose a novel visual chirality embedding (VCE) module to exploit this commutative residual in multi-scale feature maps, to embed the visual chirality features into our mirror detection model. Besides, we also propose a visual chirality-guided edge detection (CED) module to integrate the visual chirality features with contextual features for detection refinement. Extensive experiments show that the proposed method outperforms state-of-the-art methods on three benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Xin Tan and Jiaying Lin and Ke Xu and Pan Chen and Lizhuang Ma and Rynson W.H. Lau},
  doi          = {10.1109/TPAMI.2022.3181030},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3492-3504},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mirror detection with the visual chirality cue},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-reinforcement learning in non-stationary and dynamic
environments. <em>TPAMI</em>, <em>45</em>(3), 3476–3491. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks.},
  archive      = {J_TPAMI},
  author       = {Zhenshan Bing and David Lerch and Kai Huang and Alois Knoll},
  doi          = {10.1109/TPAMI.2022.3185549},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3476-3491},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Meta-reinforcement learning in non-stationary and dynamic environments},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaDrive: Composing diverse driving scenarios for
generalizable reinforcement learning. <em>TPAMI</em>, <em>45</em>(3),
3461–3475. (<a
href="https://doi.org/10.1109/TPAMI.2022.3190471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving safely requires multiple capabilities from human and intelligent agents, such as the generalizability to unseen environments, the safety awareness of the surrounding traffic, and the decision-making in complex multi-agent settings. Despite the great success of Reinforcement Learning (RL), most of the RL research works investigate each capability separately due to the lack of integrated environments. In this work, we develop a new driving simulation platform called MetaDrive to support the research of generalizable reinforcement learning algorithms for machine autonomy. MetaDrive is highly compositional, which can generate an infinite number of diverse driving scenarios from both the procedural generation and the real data importing. Based on MetaDrive, we construct a variety of RL tasks and baselines in both single-agent and multi-agent settings, including benchmarking generalizability across unseen scenes, safe exploration, and learning multi-agent traffic. The generalization experiments conducted on both procedurally generated scenarios and real-world scenarios show that increasing the diversity and the size of the training set leads to the improvement of the RL agent&#39;s generalizability. We further evaluate various safe reinforcement learning and multi-agent reinforcement learning algorithms in MetaDrive environments and provide the benchmarks. Source code, documentation, and demo video are available at https://metadriverse.github.io/metadrive .},
  archive      = {J_TPAMI},
  author       = {Quanyi Li and Zhenghao Peng and Lan Feng and Qihang Zhang and Zhenghai Xue and Bolei Zhou},
  doi          = {10.1109/TPAMI.2022.3190471},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3461-3475},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory uncertainty learning for real-world single image
deraining. <em>TPAMI</em>, <em>45</em>(3), 3446–3460. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining has witnessed dramatic improvements by training deep neural networks on large-scale synthetic data. However, due to the discrepancy between authentic and synthetic rain images, it is challenging to directly extend existing methods to real-world scenes. To address this issue, we propose a memory-uncertainty guided semi-supervised method to learn rain properties simultaneously from synthetic and real data. The key aspect is developing a stochastic memory network that is equipped with memory modules to record prototypical rain patterns. The memory modules are updated in a self-supervised way, allowing the network to comprehensively capture rainy styles without the need for clean labels. The memory items are read stochastically according to their similarities with rain representations, leading to diverse predictions and efficient uncertainty estimation. Furthermore, we present an uncertainty-aware self-training mechanism to transfer knowledge from supervised deraining to unsupervised cases. An additional target network is adopted to produce pseudo-labels for unlabeled data, of which the incorrect ones are rectified by uncertainty estimates. Finally, we construct a new large-scale image deraining dataset of 10.2 k real rain images, significantly improving the diversity of real rain scenes. Experiments show that our method achieves more appealing results for real-world rain removal than recent state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Huaibo Huang and Mandi Luo and Ran He},
  doi          = {10.1109/TPAMI.2022.3180560},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3446-3460},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Memory uncertainty learning for real-world single image deraining},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum structural generation discrepancy for unsupervised
domain adaptation. <em>TPAMI</em>, <em>45</em>(3), 3434–3445. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has recently become an appealing research topic in visual recognition, since it exploits all accessible well-labeled source data to train a model with high generalization on target domain without any annotations. However, due to the significant domain discrepancy, the bottleneck for UDA is to learn effective domain-invariant feature representations. To fight off such an obstacle, we propose a novel cross-domain learning framework named Maximum Structural Generation Discrepancy (MSGD) to accurately estimate and mitigate domain shift via introducing an intermediate domain. First, the cross-domain topological structure is explored to propagate target samples to generate a novel intermediate domain paired with the specific source instances. The intermediate domain plays as the bridge to gradually reduce distribution divergence across source and target domains. Concretely, the similar category semantic across source and intermediate features tends to naturally conduct the class-level alignment on eliminating their domain shift. In terms of no target annotation, the domain-level alignment manner is suitable to narrow down the distance between intermediate and target domains. Moreover, to produce high-quality generative instances, we develop the class-driven collaborative translation (CDCT) module to generate class-consistent cross-domain samples in each mini-batch with the assistance of pseudo-labels. Extensive experimental analyses on five domain adaptation benchmarks demonstrate the effectiveness of our MSGD on solving UDA problem.},
  archive      = {J_TPAMI},
  author       = {Haifeng Xia and Taotao Jing and Zhengming Ding},
  doi          = {10.1109/TPAMI.2022.3174526},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3434-3445},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Maximum structural generation discrepancy for unsupervised domain adaptation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Masked contrastive representation learning for reinforcement
learning. <em>TPAMI</em>, <em>45</em>(3), 3421–3433. (<a
href="https://doi.org/10.1109/TPAMI.2022.3176413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pixel-based reinforcement learning (RL), the states are raw video frames, which are mapped into hidden representation before feeding to a policy network. To improve sample efficiency of state representation learning, recently, the most prominent work is based on contrastive unsupervised representation. Witnessing that consecutive video frames in a game are highly correlated, to further improve data efficiency, we propose a new algorithm, i.e. , masked contrastive representation learning for RL (M-CURL), which takes the correlation among consecutive inputs into consideration. In our architecture, besides a CNN encoder for hidden presentation of input state and a policy network for action selection, we introduce an auxiliary Transformer encoder module to leverage the correlations among video frames. During training, we randomly mask the features of several frames, and use the CNN encoder and Transformer to reconstruct them based on context frames. The CNN encoder and Transformer are jointly trained via contrastive learning where the reconstructed features should be similar to the ground-truth ones while dissimilar to others. During policy evaluation, the CNN encoder and the policy network are used to take actions, and the Transformer module is discarded. Our method achieves consistent improvements over CURL on 14 out of 16 environments from DMControl suite and 23 out of 26 environments from Atari 2600 Games. The code is available at https://github.com/teslacool/m-curl .},
  archive      = {J_TPAMI},
  author       = {Jinhua Zhu and Yingce Xia and Lijun Wu and Jiajun Deng and Wengang Zhou and Tao Qin and Tie-Yan Liu and Houqiang Li},
  doi          = {10.1109/TPAMI.2022.3176413},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3421-3433},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Masked contrastive representation learning for reinforcement learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low dimensional trajectory hypothesis is true: DNNs can be
trained in tiny subspaces. <em>TPAMI</em>, <em>45</em>(3), 3411–3420.
(<a href="https://doi.org/10.1109/TPAMI.2022.3178101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) usually contain massive parameters, but there is redundancy such that it is guessed that they could be trained in low-dimensional subspaces. In this paper, we propose a Dynamic Linear Dimensionality Reduction (DLDR) based on the low-dimensional properties of the training trajectory. The reduction method is efficient, supported by comprehensive experiments: optimizing DNNs in 40-dimensional spaces can achieve comparable performance as regular training over thousands or even millions of parameters. Since there are only a few variables to optimize, we develop an efficient quasi-Newton-based algorithm, obtain robustness to label noise, and improve the performance of well-trained models, which are three follow-up experiments that can show the advantages of finding such low-dimensional subspaces. The code is released (Pytorch: https://github.com/nblt/DLDR and Mindspore: https://gitee.com/mindspore/docs/tree/r1.6/docs/sample_code/dimension_reduce_training ).},
  archive      = {J_TPAMI},
  author       = {Tao Li and Lei Tan and Zhehao Huang and Qinghua Tao and Yipeng Liu and Xiaolin Huang},
  doi          = {10.1109/TPAMI.2022.3178101},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3411-3420},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Low dimensional trajectory hypothesis is true: DNNs can be trained in tiny subspaces},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logarithmic schatten-<span
class="math inline"><em>p</em></span>p norm minimization for tensorial
multi-view subspace clustering. <em>TPAMI</em>, <em>45</em>(3),
3396–3410. (<a
href="https://doi.org/10.1109/TPAMI.2022.3179556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank tensor could characterize inner structure and explore high-order correlation among multi-view representations, which has been widely used in multi-view clustering. Existing approaches adopt the tensor nuclear norm (TNN) as a convex approximation of non-convex tensor rank function. However, TNN treats the different singular values equally and over-penalizes the main rank components, leading to sub-optimal tensor representation. In this paper, we devise a better surrogate of tensor rank, namely the tensor logarithmic Schatten- $p$ norm ( $\text{TLS}_{p}$ N), which fully considers the physical difference between singular values by the non-convex and non-linear penalty function. Further, a tensor logarithmic Schatten- $p$ norm minimization ( $\text{TLS}_{p}$ NM)-based multi-view subspace clustering ( $\text{TLS}_{p}$ NM-MSC) model is proposed. Specially, the proposed $\text{TLS}_{p}$ NM can not only protect the larger singular values encoded with useful structural information, but also remove the smaller ones encoded with redundant information. Thus, the learned tensor representation with compact low-rank structure will well explore the complementary information and accurately characterize the high-order correlation among multi-views. The alternating direction method of multipliers (ADMM) is used to solve the non-convex multi-block $\text{TLS}_{p}$ NM-MSC model where the challenging $\text{TLS}_{p}$ NM problem is carefully handled. Importantly, the algorithm convergence analysis is mathematically established by showing that the sequence generated by the algorithm is of Cauchy and converges to a Karush-Kuhn-Tucker (KKT) point. Experimental results on nine benchmark databases reveal the superiority of the $\text{TLS}_{p}$ NM-MSC model.},
  archive      = {J_TPAMI},
  author       = {Jipeng Guo and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TPAMI.2022.3179556},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3396-3410},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Logarithmic schatten-$p$p norm minimization for tensorial multi-view subspace clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to explore distillability and sparsability: A joint
framework for model compression. <em>TPAMI</em>, <em>45</em>(3),
3378–3395. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning shows excellent performance usually at the expense of heavy computation. Recently, model compression has become a popular way of reducing the computation. Compression can be achieved using knowledge distillation or filter pruning. Knowledge distillation improves the accuracy of a lightweight network, while filter pruning removes redundant architecture in a cumbersome network. They are two different ways of achieving model compression, but few methods simultaneously consider both of them. In this paper, we revisit model compression and define two attributes of a model: distillability and sparsability, which reflect how much useful knowledge can be distilled and how many pruned ratios can be obtained, respectively. Guided by our observations and considering both accuracy and model size, a dynamically distillability-and-sparsability learning framework (DDSL) is introduced for model compression. DDSL consists of teacher, student and dean. Knowledge is distilled from the teacher to guide the student. The dean controls the training process by dynamically adjusting the distillation supervision and the sparsity supervision in a meta-learning framework. An alternating direction method of multiplier (ADMM)-based knowledge distillation-with-pruning (KDP) joint optimization algorithm is proposed to train the model. Extensive experimental results show that DDSL outperforms 24 state-of-the-art methods, including both knowledge distillation and filter pruning methods.},
  archive      = {J_TPAMI},
  author       = {Yufan Liu and Jiajiong Cao and Bing Li and Weiming Hu and Stephen Maybank},
  doi          = {10.1109/TPAMI.2022.3185317},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3378-3395},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to explore distillability and sparsability: A joint framework for model compression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning structural representations for recipe generation
and food retrieval. <em>TPAMI</em>, <em>45</em>(3), 3363–3377. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food is significant to human daily life. In this paper, we are interested in learning structural representations for lengthy recipes, that can benefit the recipe generation and food cross-modal retrieval tasks. Different from the common vision-language data, here the food images contain mixed ingredients and target recipes are lengthy paragraphs, where we do not have annotations on structure information. To address the above limitations, we propose a novel method to unsupervisedly learn the sentence-level tree structures for the cooking recipes. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the learned tree structures into the recipe generation and food cross-modal retrieval procedure. Our proposed model can produce good-quality sentence-level tree structures and coherent recipes. We achieve the state-of-the-art recipe generation and food cross-modal retrieval performance on the benchmark Recipe1M dataset.},
  archive      = {J_TPAMI},
  author       = {Hao Wang and Guosheng Lin and Steven C. H. Hoi and Chunyan Miao},
  doi          = {10.1109/TPAMI.2022.3181294},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3363-3377},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning structural representations for recipe generation and food retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning SpatioTemporal and motion features in a unified 2D
network for action recognition. <em>TPAMI</em>, <em>45</em>(3),
3347–3362. (<a
href="https://doi.org/10.1109/TPAMI.2022.3173658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods for action recognition always apply 3D Convolutional Neural Networks (CNNs) to extract spatiotemporal features and introduce optical flows to present motion features. Although achieving state-of-the-art performance, they are expensive in both time and space. In this paper, we propose to represent both two kinds of features in a unified 2D CNN without any 3D convolution or optical flows calculation. In particular, we first design a channel-wise spatiotemporal module to present the spatiotemporal features and a channel-wise motion module to encode feature-level motion features efficiently. Besides, we provide a distinctive illustration of the two modules from the frequency domain by interpreting them as advanced and learnable versions of frequency components. Second, we combine these two modules and an identity mapping path into one united block that can easily replace the original residual block in the ResNet architecture, forming a simple yet effective network dubbed STM network by introducing very limited extra computation cost and parameters. Third, we propose a novel Twins Training framework for action recognition by incorporating a correlation loss to optimize the inter-class and intra-class correlation and a siamese structure to fully stretch the training data. We extensively validate the proposed STM on both temporal-related datasets (i.e., Something-Something v1 &amp; v2) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51). It achieves favorable results against state-of-the-art methods in all the datasets.},
  archive      = {J_TPAMI},
  author       = {Mengmeng Wang and Jiazheng Xing and Jing Su and Jun Chen and Yong Liu},
  doi          = {10.1109/TPAMI.2022.3173658},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3347-3362},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning SpatioTemporal and motion features in a unified 2D network for action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-field contextual feature learning for glass detection.
<em>TPAMI</em>, <em>45</em>(3), 3329–3346. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glass is very common in our daily life. Existing computer vision systems neglect it and thus may have severe consequences, e.g., a robot may crash into a glass wall. However, sensing the presence of glass is not straightforward. The key challenge is that arbitrary objects/scenes can appear behind the glass. In this paper, we propose an important problem of detecting glass surfaces from a single RGB image. To address this problem, we construct the first large-scale glass detection dataset (GDD) and propose a novel glass detection network, called GDNet-B, which explores abundant contextual cues in a large field-of-view via a novel large-field contextual feature integration (LCFI) module and integrates both high-level and low-level boundary features with a boundary feature enhancement (BFE) module. Extensive experiments demonstrate that our GDNet-B achieves satisfying glass detection results on the images within and beyond the GDD testing set. We further validate the effectiveness and generalization capability of our proposed GDNet-B by applying it to other vision tasks, including mirror segmentation and salient object detection. Finally, we show the potential applications of glass detection and discuss possible future research directions.},
  archive      = {J_TPAMI},
  author       = {Haiyang Mei and Xin Yang and Letian Yu and Qiang Zhang and Xiaopeng Wei and Rynson W. H. Lau},
  doi          = {10.1109/TPAMI.2022.3181973},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3329-3346},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large-field contextual feature learning for glass detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label-guided generative adversarial network for realistic
image synthesis. <em>TPAMI</em>, <em>45</em>(3), 3311–3328. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating photo-realistic images from labels (e.g., semantic labels or sketch labels) is much more challenging than the general image-to-image translation task, mainly due to the large differences between extremely sparse labels and detail rich images. We propose a general framework Lab2Pix to tackle this issue from two aspects: 1) how to extract useful information from the input; and 2) how to efficiently bridge the gap between the labels and images. Specifically, we propose a Double-Guided Normalization (DG-Norm) to use the input label for semantically guiding activations in normalization layers, and use global features with large receptive fields for differentiating the activations within the same semantic region. To efficiently generate the images, we further propose Label Guided Spatial Co-Attention (LSCA) to encourage the learning of incremental visual information using limited model parameters while storing the well-synthesized part in lower-level features. Accordingly, Hierarchical Perceptual Discriminators with Foreground Enhancement Masks are proposed to toughly work against the generator thus encouraging realistic image generation and a sharp enhancement loss is further introduced for high-quality sharp image generation. We instantiate our Lab2Pix for the task of label-to-image in both unpaired (Lab2Pix-V1) and paired settings (Lab2Pix-V2). Extensive experiments conducted on various datasets demonstrate that our method significantly outperforms state-of-the-art methods quantitatively and qualitatively in both settings.},
  archive      = {J_TPAMI},
  author       = {Junchen Zhu and Lianli Gao and Jingkuan Song and Yuan-Fang Li and Feng Zheng and Xuelong Li and Heng Tao Shen},
  doi          = {10.1109/TPAMI.2022.3186752},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3311-3328},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Label-guided generative adversarial network for realistic image synthesis},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KITTI-360: A novel dataset and benchmarks for urban scene
understanding in 2D and 3D. <em>TPAMI</em>, <em>45</em>(3), 3292–3310.
(<a href="https://doi.org/10.1109/TPAMI.2022.3179507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today&#39;s grand challenges: the development of fully autonomous self-driving systems.},
  archive      = {J_TPAMI},
  author       = {Yiyi Liao and Jun Xie and Andreas Geiger},
  doi          = {10.1109/TPAMI.2022.3179507},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3292-3310},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic and isotropic resampling for 3D point clouds.
<em>TPAMI</em>, <em>45</em>(3), 3274–3291. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development of 3D scanning technology, 3D point cloud based research and applications are becoming more popular. However, major difficulties are still exist which affect the performance of point cloud utilization. Such difficulties include lack of local adjacency information, non-uniform point density, and control of point numbers. In this paper, we propose a two-step intrinsic and isotropic (I&amp;I) resampling framework to address the challenge of these three major difficulties. The efficient intrinsic control provides geodesic measurement for a point cloud to improve local region detection and avoids redundant geodesic calculation. Then the geometrically-optimized resampling uses a geometric update process to optimize a point cloud into an isotropic or adaptively-isotropic one. The point cloud density can be adjusted to global uniform (isotropic) or local uniform with geometric feature keeping (being adaptively isotropic). The point cloud number can be controlled based on application requirement or user-specification. Experiments show that our point cloud resampling framework achieves outstanding performance in different applications: point cloud simplification, mesh reconstruction and shape registration. We provide the implementation codes of our resampling method at https://github.com/vvvwo/II-resampling .},
  archive      = {J_TPAMI},
  author       = {Chenlei Lv and Weisi Lin and Baoquan Zhao},
  doi          = {10.1109/TPAMI.2022.3185644},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3274-3291},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Intrinsic and isotropic resampling for 3D point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance shadow detection with a single-stage detector.
<em>TPAMI</em>, <em>45</em>(3), 3259–3273. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article formulates a new problem, instance shadow detection, which aims to detect shadow instance and the associated object instance that cast each shadow in the input image. To approach this task, we first compile a new dataset with the masks for shadow instances, object instances, and shadow-object associations. We then design an evaluation metric for quantitative evaluation of the performance of instance shadow detection. Further, we design a single-stage detector to perform instance shadow detection in an end-to-end manner, where the bidirectional relation learning module and the deformable maskIoU head are proposed in the detector to directly learn the relation between shadow instances and object instances and to improve the accuracy of the predicted masks. Finally, we quantitatively and qualitatively evaluate our method on the benchmark dataset of instance shadow detection and show the applicability of our method on light direction estimation and photo editing.},
  archive      = {J_TPAMI},
  author       = {Tianyu Wang and Xiaowei Hu and Pheng-Ann Heng and Chi-Wing Fu},
  doi          = {10.1109/TPAMI.2022.3185628},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3259-3273},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Instance shadow detection with a single-stage detector},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraph collaborative network on vertices and hyperedges.
<em>TPAMI</em>, <em>45</em>(3), 3245–3258. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical datasets, such as co-citation and co-authorship, relationships across the samples are more complex than pair-wise. Hypergraphs provide a flexible and natural representation for such complex correlations and thus obtain increasing attention in the machine learning and data mining communities. Existing deep learning-based hypergraph approaches seek to learn the latent vertex representations based on either vertices or hyperedges from previous layers and focus on reducing the cross-entropy error over labeled vertices to obtain a classifier. In this paper, we propose a novel model called Hypergraph Collaborative Network (HCoN), which takes the information from both previous vertices and hyperedges into consideration to achieve informative latent representations and further introduces the hypergraph reconstruction error as a regularizer to learn an effective classifier. We evaluate the proposed method on two cases, i.e., semi-supervised vertex and hyperedge classifications. We carry out the experiments on several benchmark datasets and compare our method with several state-of-the-art approaches. Experimental results demonstrate that the performance of the proposed method is better than that of the baseline methods.},
  archive      = {J_TPAMI},
  author       = {Hanrui Wu and Yuguang Yan and Michael Kwok-Po Ng},
  doi          = {10.1109/TPAMI.2022.3178156},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3245-3258},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hypergraph collaborative network on vertices and hyperedges},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid ISTA: Unfolding ISTA with convergence guarantees
using free-form deep neural networks. <em>TPAMI</em>, <em>45</em>(3),
3226–3244. (<a
href="https://doi.org/10.1109/TPAMI.2022.3172214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is promising to solve linear inverse problems by unfolding iterative algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep neural networks (DNNs) with learnable parameters. However, existing ISTA-based unfolded algorithms restrict the network architectures for iterative updates with the partial weight coupling structure to guarantee convergence. In this paper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned parameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible and reasonable network architectures), while ensuring theoretical convergence. We first develop HCISTA to improve the efficiency and flexibility of classical ISTA (with pre-computed parameters) without compromising the convergence rate in theory. Furthermore, the DNN-based hybrid algorithm is generalized to popular variants of learned ISTA, dubbed HLISTA, to enable a free architecture of learned parameters with a guarantee of linear convergence. To our best knowledge, this paper is the first to provide a convergence-provable framework that enables free-form DNNs in ISTA-based unfolded algorithms. This framework is general to endow arbitrary DNNs for solving linear inverse problems with convergence guarantees. Extensive experiments demonstrate that hybrid ISTA can reduce the reconstruction error with an improved convergence rate in the tasks of sparse recovery and compressive sensing.},
  archive      = {J_TPAMI},
  author       = {Ziyang Zheng and Wenrui Dai and Duoduo Xue and Chenglin Li and Junni Zou and Hongkai Xiong},
  doi          = {10.1109/TPAMI.2022.3172214},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3226-3244},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hybrid ISTA: Unfolding ISTA with convergence guarantees using free-form deep neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human action recognition from various data modalities: A
review. <em>TPAMI</em>, <em>45</em>(3), 3200–3225. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Action Recognition (HAR) aims to understand human behavior and assign a label to each action. It has a wide range of applications, and therefore has been attracting increasing attention in the field of computer vision. Human actions can be represented using various data modalities, such as RGB, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, and WiFi signal, which encode different sources of useful yet distinct information and have various advantages depending on the application scenarios. Consequently, lots of existing works have attempted to investigate different types of approaches for HAR using various modalities. In this article, we present a comprehensive survey of recent progress in deep learning methods for HAR based on the type of input data modality. Specifically, we review the current mainstream deep learning methods for single data modalities and multiple data modalities, including the fusion-based and the co-learning-based frameworks. We also present comparative results on several benchmark datasets for HAR, together with insightful observations and inspiring future research directions.},
  archive      = {J_TPAMI},
  author       = {Zehua Sun and Qiuhong Ke and Hossein Rahmani and Mohammed Bennamoun and Gang Wang and Jun Liu},
  doi          = {10.1109/TPAMI.2022.3183112},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3200-3225},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human action recognition from various data modalities: A review},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HGNN+: General hypergraph neural networks. <em>TPAMI</em>,
<em>45</em>(3), 3181–3199. (<a
href="https://doi.org/10.1109/TPAMI.2022.3182052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks have attracted increasing attention in recent years. However, existing GNN frameworks are deployed based upon simple graphs, which limits their applications in dealing with complex data correlation of multi-modal/multi-type data in practice. A few hypergraph-based methods have recently been proposed to address the problem of multi-modal/multi-type data correlation by directly concatenating the hypergraphs constructed from each single individual modality/type, which is difficult to learn an adaptive weight for each modality/type. In this paper, we extend the original conference version HGNN, and introduce a general high-order multi-modal/multi-type data correlation modeling framework called HGNN $^+$ to learn an optimal representation in a single hypergraph based framework. It is achieved by bridging multi-modal/multi-type data and hyperedge with hyperedge groups. Specifically, in our method, hyperedge groups are first constructed to represent latent high-order correlations in each specific modality/type with explicit or implicit graph structures. An adaptive hyperedge group fusion strategy is then used to effectively fuse the correlations from different modalities/types in a unified hypergraph. After that a new hypergraph convolution scheme performed in spatial domain is used to learn a general data representation for various tasks. We have evaluated this framework on several popular datasets and compared it with recent state-of-the-art methods. The comprehensive evaluations indicate that the proposed HGNN $^+$ framework can consistently outperform existing methods with a significant margin, especially when modeling implicit data correlations. We also release a toolbox called THU-DeepHypergraph for the proposed framework, which can be used for various of applications, such as data classification, retrieval and recommendation.},
  archive      = {J_TPAMI},
  author       = {Yue Gao and Yifan Feng and Shuyi Ji and Rongrong Ji},
  doi          = {10.1109/TPAMI.2022.3182052},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3181-3199},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HGNN+: General hypergraph neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group contrastive self-supervised learning on graphs.
<em>TPAMI</em>, <em>45</em>(3), 3169–3180. (<a
href="https://doi.org/10.1109/TPAMI.2022.3177295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study self-supervised learning on graphs using contrastive methods. A general scheme of prior methods is to optimize two-view representations of input graphs. In many studies, a single graph-level representation is computed as one of the contrastive objectives, capturing limited characteristics of graphs. We argue that contrasting graphs in multiple subspaces enables graph encoders to capture more abundant characteristics. To this end, we propose a group contrastive learning framework in this work. Our framework embeds the given graph into multiple subspaces, of which each representation is prompted to encode specific characteristics of graphs. To learn diverse and informative representations, we develop principled objectives that enable us to capture the relations among both intra-space and inter-space representations in groups. Under the proposed framework, we further develop an attention-based group generator to compute representations that capture different substructures of a given graph. Built upon our framework, we extend two current methods into GroupCL and GroupIG, equipped with the proposed objective. Comprehensive experimental results show our framework achieves a promising boost in performance on a variety of datasets. In addition, our qualitative results show that features generated from our representor successfully capture various specific characteristics of graphs.},
  archive      = {J_TPAMI},
  author       = {Xinyi Xu and Cheng Deng and Yaochen Xie and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2022.3177295},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3169-3180},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Group contrastive self-supervised learning on graphs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GLEAN: Generative latent bank for image super-resolution and
beyond. <em>TPAMI</em>, <em>45</em>(3), 3154–3168. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method, G enerative L at E nt b AN k (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for restoration. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (e.g., human faces, cats, buildings, and cars). We further present a lightweight version of GLEAN, named LightGLEAN, which retains only the critical components in GLEAN. Notably, LightGLEAN consists of only 21\% of parameters and 35\% of FLOPs while achieving comparable image quality. We extend our method to different tasks including image colorization and blind image restoration, and extensive experiments show that our proposed models perform favorably in comparison to existing methods. Codes and models are available at https://github.com/open-mmlab/mmediting .},
  archive      = {J_TPAMI},
  author       = {Kelvin C.K. Chan and Xiangyu Xu and Xintao Wang and Jinwei Gu and Chen Change Loy},
  doi          = {10.1109/TPAMI.2022.3186715},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3154-3168},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GLEAN: Generative latent bank for image super-resolution and beyond},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized focal loss: Towards efficient representation
learning for dense object detection. <em>TPAMI</em>, <em>45</em>(3),
3139–3153. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a fundamental computer vision task that simultaneously predicts the category and localization of the targets of interest. Recently one-stage (also termed “dense”) detectors have gained much attention over two-stage ones due to their simple pipeline and friendly application to end devices. Dense object detectors basically formulate object detection as dense classification and localization (i.e., bounding box regression). The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for dense detectors is to introduce an individual prediction branch to estimate the quality of localization, which facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Three problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference, (2) the inflexible Dirac delta distribution for localization, and (3) the deficient and implicit guidance for accurate quality estimation. To address these problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation, use a vector to represent arbitrary distribution of box locations, and extract discriminant feature descriptors from the distribution vector for more reliable quality estimation. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFocal) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. Extensive experiments demonstrate the effectiveness of our method, without sacrificing the efficiency both in training and inference. Based on GFocal, we construct a considerably fast and lightweight detector termed NanoDet under mobile settings, which is 1.8 AP higher, 2x faster and 6x smaller than scaled YoloV4-Tiny.},
  archive      = {J_TPAMI},
  author       = {Xiang Li and Chengqi Lv and Wenhai Wang and Gang Li and Lingfeng Yang and Jian Yang},
  doi          = {10.1109/TPAMI.2022.3180392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3139-3153},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized focal loss: Towards efficient representation learning for dense object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAN inversion: A survey. <em>TPAMI</em>, <em>45</em>(3),
3121–3138. (<a
href="https://doi.org/10.1109/TPAMI.2022.3181070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model so that the image can be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling pretrained GAN models, such as StyleGAN and BigGAN, for applications of real image editing. Moreover, GAN inversion interprets GAN&#39;s latent space and examines how realistic images can be generated. In this paper, we provide a survey of GAN inversion with a focus on its representative algorithms and its applications in image restoration and image manipulation. We further discuss the trends and challenges for future research. A curated list of GAN inversion methods, datasets, and other related information can be found at https://github.com/weihaox/awesome-gan-inversion .},
  archive      = {J_TPAMI},
  author       = {Weihao Xia and Yulun Zhang and Yujiu Yang and Jing-Hao Xue and Bolei Zhou and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2022.3181070},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3121-3138},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GAN inversion: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From pose to part: Weakly-supervised pose evolution for
human part segmentation. <em>TPAMI</em>, <em>45</em>(3), 3107–3120. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human part segmentation is a crucial but challenging task in computer vision. Recent works have achieved progress with the help of pixel-wise annotations. However, annotating pixel-wise masks especially at part-level is a tedious and labor-intensive procedure. To overcome this problem, we propose a part evolution framework to learn reliable predictions from weak pose annotations, which are much easier to collect. Our framework is composed of two essential modules: the first part adaptation module is designed to learn the deep prior knowledge from three related tasks, i.e., pose estimation, part-level and object-level segmentation; the second module is the part evolution module, which refines the part priors from deep predictions with the boundary-aware optimization algorithm. These two modules are conducted iteratively to evolve pose keypoint annotations into reliable part priors. Experimental evidence shows that our weakly-supervised approach generates comparable results with the state-of-the-art strongly-supervised methods on public benchmarks, and also validates the potential of notable improvements when combining weak labels with existing part segmentation masks.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhao and Jia Li and Yu Zhang and Yonghong Tian},
  doi          = {10.1109/TPAMI.2022.3174529},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3107-3120},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From pose to part: Weakly-supervised pose evolution for human part segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot object detection and viewpoint estimation for
objects in the wild. <em>TPAMI</em>, <em>45</em>(3), 3090–3106. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting objects and estimating their viewpoints in images are key tasks of 3D scene understanding. Recent approaches have achieved excellent results on very large benchmarks for object detection and viewpoint estimation. However, performances are still lagging behind for novel object categories with few samples. In this paper, we tackle the problems of few-shot object detection and few-shot viewpoint estimation. We demonstrate on both tasks the benefits of guiding the network prediction with class-representative features extracted from data in different modalities: image patches for object detection, and aligned 3D models for viewpoint estimation. Despite its simplicity, our method outperforms state-of-the-art methods by a large margin on a range of datasets, including PASCAL and COCO for few-shot object detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint estimation. Furthermore, when the 3D model is not available, we introduce a simple category-agnostic viewpoint estimation method by exploiting geometrical similarities and consistent pose labeling across different classes. While it moderately reduces performance, this approach still obtains better results than previous methods in this setting. Last, for the first time, we tackle the combination of both few-shot tasks, on three challenging benchmarks for viewpoint estimation in the wild, ObjectNet3D, Pascal3D+ and Pix3D, showing very promising results.},
  archive      = {J_TPAMI},
  author       = {Yang Xiao and Vincent Lepetit and Renaud Marlet},
  doi          = {10.1109/TPAMI.2022.3174072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3090-3106},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Few-shot object detection and viewpoint estimation for objects in the wild},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamMask: A framework for fast online object tracking and
segmentation. <em>TPAMI</em>, <em>45</em>(3), 3072–3089. (<a
href="https://doi.org/10.1109/TPAMI.2022.3172932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce SiamMask, a framework to perform both visual object tracking and video object segmentation, in real-time, with the same simple method. We improve the offline training procedure of popular fully-convolutional Siamese approaches by augmenting their losses with a binary segmentation task. Once the offline training is completed, SiamMask only requires a single bounding box for initialization and can simultaneously carry out visual object tracking and segmentation at high frame-rates. Moreover, we show that it is possible to extend the framework to handle multiple object tracking and segmentation by simply re-using the multi-task model in a cascaded fashion. Experimental results show that our approach has high processing efficiency, at around 55 frames per second. It yields real-time state-of-the art results on visual-object tracking benchmarks, while at the same time demonstrating competitive performance at a high speed for video object segmentation benchmarks.},
  archive      = {J_TPAMI},
  author       = {Weiming Hu and Qiang Wang and Li Zhang and Luca Bertinetto and Philip H.S. Torr},
  doi          = {10.1109/TPAMI.2022.3172932},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3072-3089},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SiamMask: A framework for fast online object tracking and segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast component tree computation for images of limited
levels. <em>TPAMI</em>, <em>45</em>(3), 3059–3071. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Component trees have many applications. We introduce a new component tree computation algorithm, applicable to 4-/8-connectivity and 6-connectivity. The algorithm consists of two steps: building level line trees using an optimized top-down algorithm, and computing components from level lines by a novel line-by-line approach. As compared with traditional component computation algorithms, the new algorithm is fast for images of limited levels. It represents components by level lines, offering boundary information which traditional algorithms do not provide.},
  archive      = {J_TPAMI},
  author       = {Rui Tao and Jiangang Qiao},
  doi          = {10.1109/TPAMI.2022.3180827},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3059-3071},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast component tree computation for images of limited levels},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extended <span class="math inline"><em>T</em></span>t:
Learning with mixed closed-set and open-set noisy labels.
<em>TPAMI</em>, <em>45</em>(3), 3047–3058. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The noise transition matrix $T$ , reflecting the probabilities that true labels flip into noisy ones, is of vital importance to model label noise and build statistically consistent classifiers. The traditional transition matrix is limited to model closed-set label noise, where noisy training data have true class labels within the noisy label set. It is unfitted to employ such a transition matrix to model open-set label noise, where some true class labels are outside the noisy label set. Therefore, when considering a more realistic situation, i.e., both closed-set and open-set label noises occur, prior works will give unbelievable solutions. Besides, the traditional transition matrix is mostly limited to model instance-independent label noise, which may not perform well in practice. In this paper, we focus on learning with the mixed closed-set and open-set noisy labels. We address the aforementioned issues by extending the traditional transition matrix to be able to model mixed label noise, and further to the cluster-dependent transition matrix to better combat the instance-dependent label noise in real-world applications. We term the proposed transition matrix as the cluster-dependent extended transition matrix. An unbiased estimator (i.e., extended $T$ -estimator) has been designed to estimate the cluster-dependent extended transition matrix by only exploiting the noisy data. Comprehensive experiments validate that our method can better cope with realistic label noise, following its more robust performance than the prior state-of-the-art label-noise learning methods.},
  archive      = {J_TPAMI},
  author       = {Xiaobo Xia and Bo Han and Nannan Wang and Jiankang Deng and Jiatong Li and Yinian Mao and Tongliang Liu},
  doi          = {10.1109/TPAMI.2022.3180545},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3047-3058},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Extended $T$T: Learning with mixed closed-set and open-set noisy labels},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Exploring simple and transferable recognition-aware image
processing. <em>TPAMI</em>, <em>45</em>(3), 3032–3046. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in image recognition has stimulated the deployment of vision systems at an unprecedented scale. As a result, visual data are now often consumed not only by humans but also by machines. Existing image processing methods only optimize for better human perception, yet the resulting images may not be accurately recognized by machines. This can be undesirable, e.g., the images can be improperly handled by search engines or recommendation systems. In this work, we examine simple approaches to improve machine recognition of processed images: optimizing the recognition loss directly on the image processing network or through an intermediate input transformation model. Interestingly, the processing model&#39;s ability to enhance recognition quality can transfer when evaluated on models of different architectures, recognized categories, tasks, and training datasets. This makes the methods applicable even when we do not have the knowledge of future recognition models, e.g., when uploading processed images to the Internet. We conduct experiments on multiple image processing tasks paired with ImageNet classification and PASCAL VOC detection as recognition tasks. With these simple yet effective methods, substantial accuracy gain can be achieved with strong transferability and minimal image quality loss. Through a user study we further show that the accuracy gain can transfer to a black-box cloud model. Finally, we try to explain this transferability phenomenon by demonstrating the similarities of different models’ decision boundaries. Code is available at https://github.com/liuzhuang13/Transferable_RA .},
  archive      = {J_TPAMI},
  author       = {Zhuang Liu and Hungju Wang and Tinghui Zhou and Zhiqiang Shen and Bingyi Kang and Evan Shelhamer and Trevor Darrell},
  doi          = {10.1109/TPAMI.2022.3183243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3032-3046},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploring simple and transferable recognition-aware image processing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equivalent classification mapping for weakly supervised
temporal action localization. <em>TPAMI</em>, <em>45</em>(3), 3019–3031.
(<a href="https://doi.org/10.1109/TPAMI.2022.3178957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization is a newly emerging yet widely studied topic in recent years. The existing methods can be categorized into two localization-by-classification pipelines, i.e., the pre-classification pipeline and the post-classification pipeline. The pre-classification pipeline first performs classification on each video snippet, and then, aggregates the snippet-level classification scores to obtain the video-level classification score. In contrast, the post-classification pipeline aggregates the snippet-level features first and then predicts the video-level classification score based on the aggregated feature. Although the classifiers in these two pipelines are used in different ways, the role they play is exactly the same—to classify the given features to identify the corresponding action categories. To this end, an ideal classifier can make both pipelines work. This inspires us to simultaneously learn these two pipelines in a unified framework to obtain an effective classifier. Specifically, in the proposed learning framework, we implement two parallel network streams to model the two localization-by-classification pipelines simultaneously and make the two network streams share the same classifier. This achieves the novel Equivalent Classification Mapping (ECM) mechanism. Moreover, we discover that an ideal classifier may possess two characteristics: 1) the frame-level classification scores obtained from the pre-classification stream and the feature aggregation weights in the post-classification stream should be consistent; and 2) the classification results of these two streams should be identical. Based on these two characteristics, we further introduce a weight-transition module and an equivalent training strategy into the proposed learning framework, which assists to thoroughly mine the equivalence mechanism. Comprehensive experiments are conducted on three benchmarks and ECM achieves accurate action localization results.},
  archive      = {J_TPAMI},
  author       = {Tao Zhao and Junwei Han and Le Yang and Dingwen Zhang},
  doi          = {10.1109/TPAMI.2022.3178957},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3019-3031},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Equivalent classification mapping for weakly supervised temporal action localization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entity-enhanced adaptive reconstruction network for weakly
supervised referring expression grounding. <em>TPAMI</em>,
<em>45</em>(3), 3003–3018. (<a
href="https://doi.org/10.1109/TPAMI.2022.3186410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised Referring Expression Grounding (REG) aims to ground a particular target in an image described by a language expression while lacking the correspondence between target and expression. Two main problems exist in weakly supervised REG. First, the lack of region-level annotations introduces ambiguities between proposals and queries. Second, most previous weakly supervised REG methods ignore the discriminative location and context of the referent, causing difficulties in distinguishing the target from other same-category objects. To address the above challenges, we design an entity-enhanced adaptive reconstruction network (EARN). Specifically, EARN includes three modules: entity enhancement, adaptive grounding, and collaborative reconstruction. In entity enhancement, we calculate semantic similarity as supervision to select the candidate proposals. Adaptive grounding calculates the ranking score of candidate proposals upon subject, location and context with hierarchical attention. Collaborative reconstruction measures the ranking result from three perspectives: adaptive reconstruction, language reconstruction and attribute classification. The adaptive mechanism helps to alleviate the variance of different referring expressions. Experiments on five datasets show EARN outperforms existing state-of-the-art methods. Qualitative results demonstrate that the proposed EARN can better handle the situation where multiple objects of a particular category are situated together.},
  archive      = {J_TPAMI},
  author       = {Xuejing Liu and Liang Li and Shuhui Wang and Zheng-Jun Zha and Zechao Li and Qi Tian and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3186410},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {3003-3018},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Entity-enhanced adaptive reconstruction network for weakly supervised referring expression grounding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RF-next: Efficient receptive field search for convolutional
neural networks. <em>TPAMI</em>, <em>45</em>(3), 2984–3002. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal/spatial receptive fields of models play an important role in sequential/spatial tasks. Large receptive fields facilitate long-term relations, while small receptive fields help to capture the local details. Existing methods construct models with hand-designed receptive fields in layers. Can we effectively search for receptive field combinations to replace hand-designed patterns? To answer this question, we propose to find better receptive field combinations through a global-to-local search scheme. Our search scheme exploits both global search to find the coarse combinations and local search to get the refined receptive field combinations further. The global search finds possible coarse combinations other than human-designed patterns. On top of the global search, we propose an expectation-guided iterative local search scheme to refine combinations effectively. Our RF-Next models, plugging receptive field search to various models, boost the performance on many tasks, e.g., temporal action segmentation, object detection, instance segmentation, and speech synthesis. The source code is publicly available on http://mmcheng.net/rfnext .},
  archive      = {J_TPAMI},
  author       = {Shanghua Gao and Zhong-Yu Li and Qi Han and Ming-Ming Cheng and Liang Wang},
  doi          = {10.1109/TPAMI.2022.3183829},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2984-3002},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RF-next: Efficient receptive field search for convolutional neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Efficient image and sentence matching. <em>TPAMI</em>,
<em>45</em>(3), 2970–2983. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the accuracy of image and sentence matching has been continuously improved by larger and larger models. However, such large models not only need huge storage space but also slow down inference speed, which are not very suitable for low-cost devices in real-world applications. To our knowledge, this work makes the first attempt to improve the model efficiency in the context of image and sentence matching, and accordingly proposes a simple yet effective Whitened Similarity Distillation (WSD) method, which can distill cross-modal knowledge from a large teacher model to a small student model of both high efficiency and accuracy. The high efficiency is achieved by performing: 1) feature representation based on efficient backbone networks; and 2) similarity measurement in a fast N-to-N manner. However, the accuracy of such a student model is much worse than that of teacher model, because there exists very large variation inconsistency between two cross-modal similarity matrices of teacher and student models, which is hard to reduce during the similarity distillation. By performing two whitening-like transformations in the orthogonal space, the proposed WSD can reduce the large variation inconsistency more isotropically and is able to improve the accuracy of student model. We perform extensive experiments on two benchmark datasets and demonstrate the effectiveness of the proposed WSD. Compared with the teacher model, our distilled student model is 7× smaller (in model size) and 9× faster (in testing speed), only at the cost of $&amp;lt; $ 2\% accuracy decrease.},
  archive      = {J_TPAMI},
  author       = {Yan Huang and Yuming Wang and Liang Wang},
  doi          = {10.1109/TPAMI.2022.3178485},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2970-2983},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient image and sentence matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E<span
class="math inline"><sup>3</sup></span><!-- -->3Outlier: A
self-supervised framework for unsupervised deep outlier detection.
<em>TPAMI</em>, <em>45</em>(3), 2952–2969. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing unsupervised outlier detection (OD) solutions face a grave challenge with surging visual data like images. Although deep neural networks (DNNs) prove successful for visual data, deep OD remains difficult due to OD&#39;s unsupervised nature. This paper proposes a novel framework named E$^{3}$3Outlier that can perform e ffective and e nd-to- e nd deep outlier removal. Its core idea is to introduce self-supervision into deep OD. Specifically, our major solution is to adopt a discriminative learning paradigm that creates multiple pseudo classes from given unlabeled data by various data operations, which enables us to apply prevalent discriminative DNNs (e.g., ResNet) to the unsupervised OD problem. Then, with theoretical and empirical demonstration, we argue that inlier priority, a property that encourages DNN to prioritize inliers during self-supervised learning, makes it possible to perform end-to-end OD. Meanwhile, unlike frequently-used outlierness measures (e.g., density, proximity) in previous OD methods, we explore network uncertainty and validate it as a highly effective outlierness measure, while two practical score refinement strategies are also designed to improve OD performance. Finally, in addition to the discriminative learning paradigm above, we also explore the solutions that exploit other learning paradigms (i.e., generative learning and contrastive learning) to introduce self-supervision for E$^{3}$3Outlier . Such extendibility not only brings further performance gain on relatively difficult datasets, but also enables E$^{3}$3Outlier to be applied to other OD applications like video abnormal event detection. Extensive experiments demonstrate that E$^{3}$3Outlier can considerably outperform state-of-the-art counterparts by 10\%-30\% AUROC. Demo codes are available at https://github.com/demonzyj56/E3Outlier .},
  archive      = {J_TPAMI},
  author       = {Siqi Wang and Yijie Zeng and Guang Yu and Zhen Cheng and Xinwang Liu and Sihang Zhou and En Zhu and Marius Kloft and Jianping Yin and Qing Liao},
  doi          = {10.1109/TPAMI.2022.3188763},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2952-2969},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {E$^{3}$3Outlier: A self-supervised framework for unsupervised deep outlier detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic support network for few-shot class incremental
learning. <em>TPAMI</em>, <em>45</em>(3), 2945–2951. (<a
href="https://doi.org/10.1109/TPAMI.2022.3175849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class-incremental learning (FSCIL) is challenged by catastrophically forgetting old classes and over-fitting new classes. Revealed by our analyses, the problems are caused by feature distribution crumbling, which leads to class confusion when continuously embedding few samples to a fixed feature space. In this study, we propose a Dynamic Support Network (DSN), which refers to an adaptively updating network with compressive node expansion to “support” the feature space. In each training session, DSN tentatively expands network nodes to enlarge feature representation capacity for incremental classes. It then dynamically compresses the expanded network by node self-activation to pursue compact feature representation, which alleviates over-fitting. Simultaneously, DSN selectively recalls old class distributions during incremental learning to support feature distributions and avoid confusion between classes. DSN with compressive node expansion and class distribution recalling provides a systematic solution for the problems of catastrophic forgetting and overfitting. Experiments on CUB, CIFAR-100, and miniImage datasets show that DSN significantly improves upon the baseline approach, achieving new state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Boyu Yang and Mingbao Lin and Yunxiao Zhang and Binghao Liu and Xiaodan Liang and Rongrong Ji and Qixiang Ye},
  doi          = {10.1109/TPAMI.2022.3175849},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2945-2951},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic support network for few-shot class incremental learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepTag: A general framework for fiducial marker design and
detection. <em>TPAMI</em>, <em>45</em>(3), 2931–2944. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fiducial marker system usually consists of markers, a detection algorithm, and a coding system. The appearance of markers and the detection robustness are generally limited by the existing detection algorithms, which are hand-crafted with traditional low-level image processing techniques. Furthermore, a sophisticatedly designed coding system is required to overcome the shortcomings of both markers and detection algorithms. To improve the flexibility and robustness in various applications, we propose a general deep learning based framework, DeepTag , for fiducial marker design and detection. DeepTag not only supports detection of a wide variety of existing marker families, but also makes it possible to design new marker families with customized local patterns. Moreover, we propose an effective procedure to synthesize training data on the fly without manual annotations. Thus, DeepTag can easily adapt to existing and newly-designed marker families. To validate DeepTag and existing methods, beside existing datasets, we further collect a new large and challenging dataset where markers are placed in different view distances and angles. Experiments show that DeepTag well supports different marker families and greatly outperforms the existing methods in terms of both detection robustness and pose accuracy. Both code and dataset are available at https://herohuyongtao.github.io/research/publications/deep-tag/ .},
  archive      = {J_TPAMI},
  author       = {Zhuming Zhang and Yongtao Hu and Guoxing Yu and Jingwen Dai},
  doi          = {10.1109/TPAMI.2022.3174603},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2931-2944},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepTag: A general framework for fiducial marker design and detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep point set resampling via gradient fields.
<em>TPAMI</em>, <em>45</em>(3), 2913–2930. (<a
href="https://doi.org/10.1109/TPAMI.2022.3175183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds acquired by scanning real-world objects or scenes have found a wide range of applications including immersive telepresence, autonomous driving, surveillance, etc. They are often perturbed by noise or suffer from low density, which obstructs downstream tasks such as surface reconstruction and understanding. In this paper, we propose a novel paradigm of point set resampling for restoration, which learns continuous gradient fields of point clouds that converge points towards the underlying surface. In particular, we represent a point cloud via its gradient field—the gradient of the log-probability density function, and enforce the gradient field to be continuous, thus guaranteeing the continuity of the model for solvable optimization. Based on the continuous gradient fields estimated via a proposed neural network, resampling a point cloud amounts to performing gradient-based Markov Chain Monte Carlo (MCMC) on the input noisy or sparse point cloud. Further, we propose to introduce regularization into the gradient-based MCMC during point cloud restoration, which essentially refines the intermediate resampled point cloud iteratively and accommodates various priors in the resampling process. Extensive experimental results demonstrate that the proposed point set resampling achieves the state-of-the-art performance in representative restoration tasks including point cloud denoising and upsampling.},
  archive      = {J_TPAMI},
  author       = {Haolan Chen and Bi&#39;an Du and Shitong Luo and Wei Hu},
  doi          = {10.1109/TPAMI.2022.3175183},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2913-2930},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep point set resampling via gradient fields},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep generative mixture model for robust imbalance
classification. <em>TPAMI</em>, <em>45</em>(3), 2897–2912. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering hidden pattern from imbalanced data is a critical issue in various real-world applications. Existing classification methods usually suffer from the limitation of data especially for minority classes, and result in unstable prediction and low performance. In this paper, a deep generative classifier is proposed to mitigate this issue via both model perturbation and data perturbation. Specially, the proposed generative classifier is derived from a deep latent variable model where two variables are involved. One variable is to capture the essential information of the original data, denoted as latent codes, which are represented by a probability distribution rather than a single fixed value. The learnt distribution aims to enforce the uncertainty of model and implement model perturbation, thus, lead to stable predictions. The other variable is a prior to latent codes so that the codes are restricted to lie on components in Gaussian Mixture Model. As a confounder affecting generative processes of data (feature/label), the latent variables are supposed to capture the discriminative latent distribution and implement data perturbation. Extensive experiments have been conducted on widely-used real imbalanced image datasets. Experimental results demonstrate the superiority of our proposed model by comparing with popular imbalanced classification baselines on imbalance classification task.},
  archive      = {J_TPAMI},
  author       = {Xinyue Wang and Liping Jing and Yilin Lyu and Mingzhe Guo and Jiaqi Wang and Huafeng Liu and Jian Yu and Tieyong Zeng},
  doi          = {10.1109/TPAMI.2022.3178914},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2897-2912},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep generative mixture model for robust imbalance classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation in high dimensional low sample size
setting using a geometry-based variational autoencoder. <em>TPAMI</em>,
<em>45</em>(3), 2879–2896. (<a
href="https://doi.org/10.1109/TPAMI.2022.3185773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method to perform data augmentation in a reliable way in the High Dimensional Low Sample Size (HDLSS) setting using a geometry-based variational autoencoder (VAE). Our approach combines the proposal of 1) a new VAE model, the latent space of which is modeled as a Riemannian manifold and which combines both Riemannian metric learning and normalizing flows and 2) a new generation scheme which produces more meaningful samples especially in the context of small data sets. The method is tested through a wide experimental study where its robustness to data sets, classifiers and training samples size is stressed. It is also validated on a medical imaging classification task on the challenging ADNI database where a small number of 3D brain magnetic resonance images (MRIs) are considered and augmented using the proposed VAE framework. In each case, the proposed method allows for a significant and reliable gain in the classification metrics. For instance, balanced accuracy jumps from 66.3\% to 74.3\% for a state-of-the-art convolutional neural network classifier trained with 50 MRIs of cognitively normal (CN) and 50 Alzheimer disease (AD) patients and from 77.7\% to 86.3\% when trained with 243 CN and 210 AD while improving greatly sensitivity and specificity metrics.},
  archive      = {J_TPAMI},
  author       = {Clément Chadebec and Elina Thibeau-Sutre and Ninon Burgos and Stéphanie Allassonnière},
  doi          = {10.1109/TPAMI.2022.3185773},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2879-2896},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Data augmentation in high dimensional low sample size setting using a geometry-based variational autoencoder},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual learning for blind image quality assessment.
<em>TPAMI</em>, <em>45</em>(3), 2864–2878. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of image data facilitates the fast development of image processing and computer vision methods for emerging visual applications, meanwhile introducing novel distortions to processed images. This poses a grand challenge to existing blind image quality assessment (BIQA) models, which are weak at adapting to subpopulation shift. Recent work suggests training BIQA methods on the combination of all available human-rated IQA datasets. However, this type of approach is not scalable to a large number of datasets and is cumbersome to incorporate a newly created dataset as well. In this paper, we formulate continual learning for BIQA, where a model learns continually from a stream of IQA datasets, building on what was learned from previously seen data. We first identify five desiderata in the continual setting with three criteria to quantify the prediction accuracy, plasticity, and stability, respectively. We then propose a simple yet effective continual learning method for BIQA. Specifically, based on a shared backbone network, we add a prediction head for a new dataset and enforce a regularizer to allow all prediction heads to evolve with new data while being resistant to catastrophic forgetting of old data. We compute the overall quality score by a weighted summation of predictions from all heads. Extensive experiments demonstrate the promise of the proposed continual learning method in comparison to standard training techniques for BIQA, with and without experience replay. We made the code publicly available at https://github.com/zwx8981/BIQA_CL .},
  archive      = {J_TPAMI},
  author       = {Weixia Zhang and Dingquan Li and Chao Ma and Guangtao Zhai and Xiaokang Yang and Kede Ma},
  doi          = {10.1109/TPAMI.2022.3178874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2864-2878},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Continual learning for blind image quality assessment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Content-aware unsupervised deep homography estimation and
its extensions. <em>TPAMI</em>, <em>45</em>(3), 2849–2863. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homography estimation is a basic image alignment method in many applications. It is usually done by extracting and matching sparse feature points, which are error-prone in low-light and low-texture images. On the other hand, previous deep homography approaches use either synthetic images for supervised learning or aerial images for unsupervised learning, both ignoring the importance of handling depth disparities and moving objects in real-world applications. To overcome these problems, in this work, we propose an unsupervised deep homography method with a new architecture design. In the spirit of the RANSAC procedure in traditional methods, we specifically learn an outlier mask to only select reliable regions for homography estimation. We calculate loss with respect to our learned deep features instead of directly comparing image content as did previously. To achieve the unsupervised training, we also formulate a novel triplet loss customized for our network. We verify our method by conducting comprehensive comparisons on a new dataset that covers a wide range of scenes with varying degrees of difficulties for the task. Experimental results reveal that our method outperforms the state-of-the-art, including deep solutions and feature-based solutions.},
  archive      = {J_TPAMI},
  author       = {Shuaicheng Liu and Nianjin Ye and Chuan Wang and Jirong Zhang and Lanpeng Jia and Kunming Luo and Jue Wang and Jian Sun},
  doi          = {10.1109/TPAMI.2022.3174130},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2849-2863},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Content-aware unsupervised deep homography estimation and its extensions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-wise denoising for robust learning under label noise.
<em>TPAMI</em>, <em>45</em>(3), 2835–2848. (<a
href="https://doi.org/10.1109/TPAMI.2022.3178690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise is ubiquitous in many real-world scenarios which often misleads training algorithm and brings about the degraded classification performance. Therefore, many approaches have been proposed to correct the loss function given corrupted labels to combat such label noise. Among them, a trend of works achieve this goal by unbiasedly estimating the data centroid, which plays an important role in constructing an unbiased risk estimator for minimization. However, they usually handle the noisy labels in different classes all at once, so the local information inherited by each class is ignored which often leads to unsatisfactory performance. To address this defect, this paper presents a novel robust learning algorithm dubbed “ C lass- W ise D enoising” (CWD), which tackles the noisy labels in a class-wise way to ease the entire noise correction task. Specifically, two virtual auxiliary sets are respectively constructed by presuming that the positive and negative labels in the training set are clean, so the original false-negative labels and false-positive ones are tackled separately. As a result, an improved centroid estimator can be designed which helps to yield more accurate risk estimator. Theoretically, we prove that: 1) the variance in centroid estimation can often be reduced by our CWD when compared with existing methods with unbiased centroid estimator; and 2) the performance of CWD trained on the noisy set will converge to that of the optimal classifier trained on the clean set with a convergence rate $\mathcal {O}(\frac{1}{\sqrt{n}})$ where $n$ is the number of the training examples. These sound theoretical properties critically enable our CWD to produce the improved classification performance under label noise, which is also demonstrated by the comparisons with ten representative state-of-the-art methods on a variety of benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Chen Gong and Yongliang Ding and Bo Han and Gang Niu and Jian Yang and Jane You and Dacheng Tao and Masashi Sugiyama},
  doi          = {10.1109/TPAMI.2022.3178690},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2835-2848},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Class-wise denoising for robust learning under label noise},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Certifiably optimal outlier-robust geometric perception:
Semidefinite relaxations and scalable global optimization.
<em>TPAMI</em>, <em>45</em>(3), 2816–2834. (<a
href="https://doi.org/10.1109/TPAMI.2022.3179463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the first general and scalable framework to design certifiable algorithms for robust geometric perception in the presence of outliers. Our first contribution is to show that estimation using common robust costs, such as truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey&#39;s biweight, among others, can be reformulated as polynomial optimization problems (POPs). By focusing on the TLS cost, our second contribution is to exploit sparsity in the POP and propose a sparse semidefinite programming (SDP) relaxation that is much smaller than the standard Lasserre&#39;s hierarchy while preserving empirical exactness , i.e., the SDP recovers the optimizer of the nonconvex POP with an optimality certificate . Our third contribution is to solve the SDP relaxations at an unprecedented scale and accuracy by presenting ${{\sf STRIDE}}$ , a solver that blends global descent on the convex SDP with fast local search on the nonconvex POP. Our fourth contribution is an evaluation of the proposed framework on six geometric perception problems including single and multiple rotation averaging, point cloud and mesh registration, absolute pose estimation, and category-level object pose and shape estimation. Our experiments demonstrate that (i) our sparse SDP relaxation is empirically exact with up to $60\%$ – $90\%$ outliers across applications; (ii) while still being far from real-time, ${{\sf STRIDE}}$ is up to 100 times faster than existing SDP solvers on medium-scale problems, and is the only solver that can solve large-scale SDPs with hundreds of thousands of constraints to high accuracy; (iii) ${{\sf STRIDE}}$ safeguards existing fast heuristics for robust estimation (e.g., ${{\sf RANSAC}}$ or Graduated Non-Convexity), i.e., it certifies global optimality if the heuristic estimates are optimal, or detects and allows escaping local optima when the heuristic estimates are suboptimal.},
  archive      = {J_TPAMI},
  author       = {Heng Yang and Luca Carlone},
  doi          = {10.1109/TPAMI.2022.3179463},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2816-2834},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Certifiably optimal outlier-robust geometric perception: Semidefinite relaxations and scalable global optimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CASIA-e: A large comprehensive dataset for gait recognition.
<em>TPAMI</em>, <em>45</em>(3), 2801–2815. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition plays a special role in visual surveillance due to its unique advantage, e.g. , long-distance, cross-view and non-cooperative recognition. However, it has not yet been widely applied. One reason for this awkwardness is the lack of a truly big dataset captured in practical outdoor scenarios. Here, the “big” at least means: (1) huge amount of gait videos; (2) sufficient subjects; (3) rich attributes; and (4) spatial and temporal variations. Moreover, most existing large-scale gait datasets are collected indoors, which have few challenges from real scenes, such as the dynamic and complex background clutters, illumination variations, vertical view variations, etc . In this article, we introduce a newly built big outdoor gait dataset, called CASIA-E. It contains more than one thousand people distributed over near one million videos. Each person involves 26 view angles and varied appearances caused by changes of bag carrying, dressing and walking styles. The videos are captured across five months and across three kinds of outdoor scenes. Soft biometric features are also recorded for all subjects including age, gender, height, weight, and nationality. Besides, we report an experimental benchmark and examine some meaningful problems that have not been well studied previously, e.g. , the influence of million-level training videos, vertical view angles, walking styles, and the thermal infrared modality. We believe that such a big outdoor dataset and the experimental benchmark will promote the development of gait recognition in both academic research and industrial applications.},
  archive      = {J_TPAMI},
  author       = {Chunfeng Song and Yongzhen Huang and Weining Wang and Liang Wang},
  doi          = {10.1109/TPAMI.2022.3183288},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2801-2815},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CASIA-E: A large comprehensive dataset for gait recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). CAS(ME)3: A third generation facial spontaneous
micro-expression database with depth information and high ecological
validity. <em>TPAMI</em>, <em>45</em>(3), 2782–2800. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression (ME) is a significant non-verbal communication clue that reveals one person&#39;s genuine emotional state. The development of micro-expression analysis (MEA) has just gained attention in the last decade. However, the small sample size problem constrains the use of deep learning on MEA. Besides, ME samples distribute in six different databases, leading to database bias. Moreover, the ME database development is complicated. In this article, we introduce a large-scale spontaneous ME database: CAS(ME) $^{3}$ . The contribution of this article is summarized as follows: (1) CAS(ME) $^{3}$ offers around 80 hours of videos with over 8,000,000 frames, including manually labeled 1,109 MEs and 3,490 macro-expressions. Such a large sample size allows effective MEA method validation while avoiding database bias. (2) Inspired by psychological experiments, CAS(ME) $^{3}$ provides the depth information as an additional modality unprecedentedly, contributing to multi-modal MEA. (3) For the first time, CAS(ME) $^{3}$ elicits ME with high ecological validity using the mock crime paradigm, along with physiological and voice signals, contributing to practical MEA. (4) Besides, CAS(ME) $^{3}$ provides 1,508 unlabeled videos with more than 4,000,000 frames, i.e., a data platform for unsupervised MEA methods. (5) Finally, we demonstrate the effectiveness of depth information by the proposed depth flow algorithm and RGB-D information.},
  archive      = {J_TPAMI},
  author       = {Jingting Li and Zizhao Dong and Shaoyuan Lu and Su-Jing Wang and Wen-Jing Yan and Yinhuan Ma and Ye Liu and Changbing Huang and Xiaolan Fu},
  doi          = {10.1109/TPAMI.2022.3174895},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2782-2800},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CAS(ME)3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bag of tricks for training deeper graph neural networks: A
comprehensive benchmark study. <em>TPAMI</em>, <em>45</em>(3),
2769–2781. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep graph neural networks (GNNs) is notoriously hard. Besides the standard plights in training deep architectures such as vanishing gradients and overfitting, it also uniquely suffers from over-smoothing, information squashing, and so on, which limits their potential power for encoding the high-order neighbor structure in large-scale graphs. Although numerous efforts are proposed to address these limitations, such as various forms of skip connections, graph normalization, and random dropping, it is difficult to disentangle the advantages brought by a deep GNN architecture from those “tricks” necessary to train such an architecture. Moreover, the lack of a standardized benchmark with fair and consistent experimental settings poses an almost insurmountable obstacle to gauge the effectiveness of new mechanisms. In view of those, we present the first fair and reproducible benchmark dedicated to assessing the “tricks” of training deep GNNs. We categorize existing approaches, investigate their hyperparameter sensitivity, and unify the basic configuration. Comprehensive evaluations are then conducted on tens of representative graph datasets including the recent large-scale Open Graph Benchmark, with diverse deep GNN backbones. We demonstrate that an organic combo of initial connection, identity mapping, group and batch normalization attains the new state-of-the-art results for deep GNNs on large datasets. Codes are available: https://github.com/VITA-Group/Deep_GCN_Benchmarking .},
  archive      = {J_TPAMI},
  author       = {Tianlong Chen and Kaixiong Zhou and Keyu Duan and Wenqing Zheng and Peihao Wang and Xia Hu and Zhangyang Wang},
  doi          = {10.1109/TPAMI.2022.3174515},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2769-2781},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Are graph convolutional networks with random weights
feasible? <em>TPAMI</em>, <em>45</em>(3), 2751–2768. (<a
href="https://doi.org/10.1109/TPAMI.2022.3183143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs), as a prominent example of graph neural networks, are receiving extensive attention for their powerful capability in learning node representations on graphs. There are various extensions, either in sampling and/or node feature aggregation, to further improve GCNs’ performance, scalability and applicability in various domains. Still, there is room for further improvements on learning efficiency because performing batch gradient descent using the full dataset for every training iteration, as unavoidable for training (vanilla) GCNs, is not a viable option for large graphs. The good potential of random features in speeding up the training phase in large-scale problems motivates us to consider carefully whether GCNs with random weights are feasible. To investigate theoretically and empirically this issue, we propose a novel model termed Graph Convolutional Networks with Random Weights (GCN-RW) by revising the convolutional layer with random filters and simultaneously adjusting the learning objective with regularized least squares loss. Theoretical analyses on the model&#39;s approximation upper bound, structure complexity, stability and generalization, are provided with rigorous mathematical proofs. The effectiveness and efficiency of GCN-RW are verified on semi-supervised node classification task with several benchmark datasets. Experimental results demonstrate that, in comparison with some state-of-the-art approaches, GCN-RW can achieve better or matched accuracies with less training time cost.},
  archive      = {J_TPAMI},
  author       = {Changqin Huang and Ming Li and Feilong Cao and Hamido Fujita and Zhao Li and Xindong Wu},
  doi          = {10.1109/TPAMI.2022.3183143},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2751-2768},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Are graph convolutional networks with random weights feasible?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Arbitrary shape text detection via segmentation with
probability maps. <em>TPAMI</em>, <em>45</em>(3), 2736–2750. (<a
href="https://doi.org/10.1109/TPAMI.2022.3176122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary shape text detection is a challenging task due to the significantly varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate annotations, etc. Due to the scalability of pixel-level prediction, segmentation-based methods can adapt to various shape texts and hence attracted considerable attention recently. However, accurate pixel-level annotations of texts are formidable, and the existing datasets for scene text detection only provide coarse-grained boundary annotations. Consequently, numerous misclassified text pixels or background pixels inside annotations always exist, degrading the performance of segmentation-based text detection methods. Generally speaking, whether a pixel belongs to text or not is highly related to the distance with the adjacent annotation boundary. With this observation, in this paper, we propose an innovative and robust segmentation-based detection method via probability maps for accurately detecting text instances. To be concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances between boundaries and their inside pixels to a probability map. However, one probability map can not cover complex probability distributions well because of the uncertainty of coarse-grained text boundary annotations. Therefore, we adopt a group of probability maps computed by a series of Sigmoid Alpha Functions to describe the possible probability distributions. In addition, we propose an iterative model to learn to predict and assimilate probability maps for providing enough information to reconstruct text instances. Finally, simple region growth algorithms are adopted to aggregate probability maps to complete text instances. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy on several benchmarks. Notably, our method with Watershed Algorithm as post-processing achieves the best F-measure on Total-Text (88.79\%), CTW1500 (85.75\%), and MSRA-TD500 (88.93\%). Besides, our method achieves promising performance on multi-oriented datasets (ICDAR2015) and multilingual datasets (ICDAR2017-MLT). Code is available at: https://github.com/GXYM/TextPMs .},
  archive      = {J_TPAMI},
  author       = {Shi-Xue Zhang and Xiaobin Zhu and Lei Chen and Jie-Bo Hou and Xu-Cheng Yin},
  doi          = {10.1109/TPAMI.2022.3176122},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2736-2750},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Arbitrary shape text detection via segmentation with probability maps},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intermediate-level attack framework on the basis of
linear regression. <em>TPAMI</em>, <em>45</em>(3), 2726–2735. (<a
href="https://doi.org/10.1109/TPAMI.2022.3188044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article substantially extends our work published at ECCV (Li et al. , 2020), in which an intermediate-level attack was proposed to improve the transferability of some baseline adversarial examples. Specifically, we advocate a framework in which a direct linear mapping from the intermediate-level discrepancies (between adversarial features and benign features) to prediction loss of the adversarial example is established. By delving deep into the core components of such a framework, we show that a variety of linear regression models can all be considered in order to establish the mapping, the magnitude of the finally obtained intermediate-level adversarial discrepancy is correlated with the transferability, and further boost of the performance can be achieved by performing multiple runs of the baseline attack with random initialization. In addition, by leveraging these findings, we achieve new state-of-the-arts on transfer-based $\ell _\infty$ and $\ell _{2}$ attacks. Our code is publicly available at https://github.com/qizhangli/ila-plus-plus-lr .},
  archive      = {J_TPAMI},
  author       = {Yiwen Guo and Qizhang Li and Wangmeng Zuo and Hao Chen},
  doi          = {10.1109/TPAMI.2022.3188044},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2726-2735},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An intermediate-level attack framework on the basis of linear regression},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial sticker: A stealthy attack method in the
physical world. <em>TPAMI</em>, <em>45</em>(3), 2711–2725. (<a
href="https://doi.org/10.1109/TPAMI.2022.3176760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life. Unlike the previous adversarial patches by designing perturbations, our method manipulates the sticker&#39;s pasting position and rotation angle on the objects to perform physical attacks. Because the position and rotation angle are less affected by the printing loss and color distortion, adversarial stickers can keep good attacking performance in the physical world. Besides, to make adversarial stickers more practical in real scenes, we conduct attacks in the black-box setting with the limited information rather than the white-box setting with all the details of threat models. To effectively solve for the sticker&#39;s parameters, we design the Region based Heuristic Differential Evolution Algorithm, which utilizes the new-found regional aggregation of effective solutions and the adaptive adjustment strategy of the evaluation criteria. Our method is comprehensively verified in the face recognition and then extended to the image retrieval and traffic sign recognition. Extensive experiments show the proposed method is effective and efficient in complex physical conditions and has a good generalization for different tasks.},
  archive      = {J_TPAMI},
  author       = {Xingxing Wei and Ying Guo and Jie Yu},
  doi          = {10.1109/TPAMI.2022.3176760},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2711-2725},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial sticker: A stealthy attack method in the physical world},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial robustness via fisher-rao regularization.
<em>TPAMI</em>, <em>45</em>(3), 2698–2710. (<a
href="https://doi.org/10.1109/TPAMI.2022.3174724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness has become a topic of growing interest in machine learning since it was observed that neural networks tend to be brittle. We propose an information-geometric formulation of adversarial defense and introduce Fire , a new Fisher-Rao regularization for the categorical cross-entropy loss, which is based on the geodesic distance between the softmax outputs corresponding to natural and perturbed input features. Based on the information-geometric properties of the class of softmax distributions, we derive an explicit characterization of the Fisher-Rao Distance ( FRD ) for the binary and multiclass cases, and draw some interesting properties as well as connections with standard regularization metrics. Furthermore, we verify on a simple linear and Gaussian model, that all Pareto-optimal points in the accuracy-robustness region can be reached by Fire while other state-of-the-art methods fail. Empirically, we evaluate the performance of various classifiers trained with the proposed loss on standard datasets, showing up to a simultaneous 1\% of improvement in terms of clean and robust performances while reducing the training time by 20\% over the best-performing methods.},
  archive      = {J_TPAMI},
  author       = {Marine Picot and Francisco Messina and Malik Boudiaf and Fabrice Labeau and Ismail Ben Ayed and Pablo Piantanida},
  doi          = {10.1109/TPAMI.2022.3174724},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2698-2710},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial robustness via fisher-rao regularization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate 3-DoF camera geo-localization via
ground-to-satellite image matching. <em>TPAMI</em>, <em>45</em>(3),
2682–2697. (<a
href="https://doi.org/10.1109/TPAMI.2022.3189702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of ground-to-satellite image geo-localization, that is, estimating the camera latitude, longitude and orientation (azimuth angle) by matching a query image captured at the ground level against a large-scale database with geotagged satellite images. Our prior arts treat the above task as pure image retrieval by selecting the most similar satellite reference image matching the ground-level query image. However, such an approach often produces coarse location estimates because the geotag of the retrieved satellite image only corresponds to the image center while the ground camera can be located at any point within the image. To further consolidate our prior research finding, we present a novel geometry-aware geo-localization method. Our new method is able to achieve the fine-grained location of a query image, up to pixel size precision of the satellite image, once its coarse location and orientation have been determined. Moreover, we propose a new geometry-aware image retrieval pipeline to improve the coarse localization accuracy. Apart from a polar transform in our conference work, this new pipeline also maps satellite image pixels to the ground-level plane in the ground-view via a geometry-constrained projective transform to emphasize informative regions, such as road structures, for cross-view geo-localization. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our newly proposed framework. We also significantly improve the performance of coarse localization results compared to the state-of-the-art in terms of location recalls.},
  archive      = {J_TPAMI},
  author       = {Yujiao Shi and Xin Yu and Liu Liu and Dylan Campbell and Piotr Koniusz and Hongdong Li},
  doi          = {10.1109/TPAMI.2022.3189702},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2682-2697},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Accurate 3-DoF camera geo-localization via ground-to-satellite image matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A low rank promoting prior for unsupervised contrastive
learning. <em>TPAMI</em>, <em>45</em>(3), 2667–2681. (<a
href="https://doi.org/10.1109/TPAMI.2022.3180995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised learning is just at a tipping point where it could really take off. Among these approaches, contrastive learning has led to state-of-the-art performance. In this paper, we construct a novel probabilistic graphical model that effectively incorporates the low rank promoting prior into the framework of contrastive learning, referred to as LORAC. In contrast to the existing conventional self-supervised approaches that only considers independent learning, our hypothesis explicitly requires that all the samples belonging to the same instance class lie on the same subspace with small dimension. This heuristic poses particular joint learning constraints to reduce the degree of freedom of the problem during the search of the optimal network parameterization. Most importantly, we argue that the low rank prior employed here is not unique, and many different priors can be invoked in a similar probabilistic way, corresponding to different hypotheses about underlying truth behind the contrastive features. Empirical evidences show that the proposed algorithm clearly surpasses the state-of-the-art approaches on multiple benchmarks, including image classification, object detection, instance segmentation and keypoint detection. Code is available: https://github.com/ssl-codelab/lorac .},
  archive      = {J_TPAMI},
  author       = {Yu Wang and Jingyang Lin and Qi Cai and Yingwei Pan and Ting Yao and Hongyang Chao and Tao Mei},
  doi          = {10.1109/TPAMI.2022.3180995},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {2667-2681},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A low rank promoting prior for unsupervised contrastive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust pose transfer with dynamic details using neural video
rendering. <em>TPAMI</em>, <em>45</em>(2), 2660–2666. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose transfer of human videos aims to generate a high-fidelity video of a target person imitating actions of a source person. A few studies have made great progress either through image translation with deep latent features or neural rendering with explicit 3D features. However, both of them rely on large amounts of training data to generate realistic results, and the performance degrades on more accessible Internet videos due to insufficient training frames. In this paper, we demonstrate that the dynamic details can be preserved even when trained from short monocular videos. Overall, we propose a neural video rendering framework coupled with an image-translation-based dynamic details generation network (D $^{2}$ G-Net), which fully utilizes both the stability of explicit 3D features and the capacity of learning components. To be specific, a novel hybrid texture representation is presented to encode both the static and pose-varying appearance characteristics, which is then mapped to the image space and rendered as a detail-rich frame in the neural rendering stage. Through extensive comparisons, we demonstrate that our neural human video renderer is capable of achieving both clearer dynamic details and more robust performance even on accessible short videos with only 2 k $\sim$ 4 k frames, as illustrated in Fig. 1.},
  archive      = {J_TPAMI},
  author       = {Yang-Tian Sun and Hao-Zhi Huang and Xuan Wang and Yu-Kun Lai and Wei Liu and Lin Gao},
  doi          = {10.1109/TPAMI.2022.3166989},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2660-2666},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust pose transfer with dynamic details using neural video rendering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum block energy guided robust subspace clustering.
<em>TPAMI</em>, <em>45</em>(2), 2652–2659. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is useful for clustering data points according to the underlying subspaces. Many methods have been presented in recent years, among which Sparse Subspace Clustering (SSC), Low-Rank Representation (LRR) and Least Squares Regression clustering (LSR) are three representative methods. These approaches achieve good results by assuming the structure of errors as a prior and removing errors in the original input space by modeling them in their objective functions. In this paper, we propose a novel method from an energy perspective to eliminate errors in the projected space rather than the input space. Since the block diagonal property can lead to correct clustering, we measure the correctness in terms of a block in the projected space with an energy function. A correct block corresponds to the subset of columns with the maximal energy. The energy of a block is defined based on the unary column, pairwise and high-order similarity of columns for each block. We relax the energy function of a block and approximate it by a constrained homogenous function. Moreover, we propose an efficient iterative algorithm to remove errors in the projected space. Both theoretical analysis and experiments show the superiority of our method over existing solutions to the clustering problem, especially when noise exists.},
  archive      = {J_TPAMI},
  author       = {Yalan Qin and Xinpeng Zhang and Liquan Shen and Guorui Feng},
  doi          = {10.1109/TPAMI.2022.3168882},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2652-2659},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Maximum block energy guided robust subspace clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). GradDiv: Adversarial robustness of randomized neural
networks via gradient diversity regularization. <em>TPAMI</em>,
<em>45</em>(2), 2645–2651. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is vulnerable to adversarial examples. Many defenses based on randomized neural networks have been proposed to solve the problem, but fail to achieve robustness against attacks using proxy gradients such as the Expectation over Transformation (EOT) attack. We investigate the effect of the adversarial attacks using proxy gradients on randomized neural networks and demonstrate that it highly relies on the directional distribution of the loss gradients of the randomized neural network. We show in particular that proxy gradients are less effective when the gradients are more scattered. To this end, we propose Gradient Diversity (GradDiv) regularizations that minimize the concentration of the gradients to build a robust randomized neural network. Our experiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv regularizations improve the adversarial robustness of randomized neural networks against a variety of state-of-the-art attack methods. Moreover, our method efficiently reduces the transferability among sample models of randomized neural networks.},
  archive      = {J_TPAMI},
  author       = {Sungyoon Lee and Hoki Kim and Jaewook Lee},
  doi          = {10.1109/TPAMI.2022.3169217},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2645-2651},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GradDiv: Adversarial robustness of randomized neural networks via gradient diversity regularization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WebFace260M: A benchmark for million-scale deep face
recognition. <em>TPAMI</em>, <em>45</em>(2), 2627–2644. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face benchmarks empower the research community to train and evaluate high-performance face recognition systems. In this paper, we contribute a new million-scale recognition benchmark, containing uncurated 4M identities/260M faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training data, as well as an elaborately designed time-constrained evaluation protocol. First, we collect 4M name lists and download 260M faces from the Internet. Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is devised to purify the tremendous WebFace260M, which is efficient and scalable. To the best of our knowledge, the cleaned WebFace42M is the largest public face recognition training set and we expect to close the data gap between academia and industry. Referring to practical deployments, Face Recognition Under Inference Time conStraint (FRUITS) protocol and a new test set with rich attributes are constructed. Besides, we gather a large-scale masked face sub-set for biometrics assessment under COVID-19. For a comprehensive evaluation of face matchers, three recognition tasks are performed under standard, masked and unbiased settings, respectively. Equipped with this benchmark, we delve into million-scale face recognition problems. A distributed framework is developed to train face recognition models efficiently without tampering with the performance. Enabled by WebFace42M, we reduce 40\% failure rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT. Even 10\% data (WebFace4M) shows superior performance compared with the public training sets. Furthermore, comprehensive baselines are established under the FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows enormous potential on standard, masked and unbiased face recognition scenarios. Our WebFace260M website is https://www.face-benchmark.org .},
  archive      = {J_TPAMI},
  author       = {Zheng Zhu and Guan Huang and Jiankang Deng and Yun Ye and Junjie Huang and Xinze Chen and Jiagang Zhu and Tian Yang and Dalong Du and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2022.3169734},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2627-2644},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {WebFace260M: A benchmark for million-scale deep face recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VoxelTrack: Multi-person 3D human pose estimation and
tracking in the wild. <em>TPAMI</em>, <em>45</em>(2), 2613–2626. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present VoxelTrack for multi-person 3D pose estimation and tracking from a few cameras which are separated by wide baselines. It employs a multi-branch network to jointly estimate 3D poses and re-identification (Re-ID) features for all people in the environment. In contrast to previous efforts which require to establish cross-view correspondence based on noisy 2D pose estimates, it directly estimates and tracks 3D poses from a 3D voxel-based representation constructed from multi-view images. We first discretize the 3D space by regular voxels and compute a feature vector for each voxel by averaging the body joint heatmaps that are inversely projected from all views. We estimate 3D poses from the voxel representation by predicting whether each voxel contains a particular body joint. Similarly, a Re-ID feature is computed for each voxel which is used to track the estimated 3D poses over time. The main advantage of the approach is that it avoids making any hard decisions based on individual images. The approach can robustly estimate and track 3D poses even when people are severely occluded in some cameras. It outperforms the state-of-the-art methods by a large margin on four public datasets including Shelf, Campus, Human3.6 M and CMU Panoptic.},
  archive      = {J_TPAMI},
  author       = {Yifu Zhang and Chunyu Wang and Xinggang Wang and Wenyu Liu and Wenjun Zeng},
  doi          = {10.1109/TPAMI.2022.3163709},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2613-2626},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VoxelTrack: Multi-person 3D human pose estimation and tracking in the wild},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video object segmentation using kernelized memory network
with multiple kernels. <em>TPAMI</em>, <em>45</em>(2), 2595–2612. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised video object segmentation (VOS) is to predict the segment of a target object in a video when a ground truth segmentation mask for the target is given in the first frame. Recently, space-time memory networks (STM) have received significant attention as a promising approach for semi-supervised VOS. However, an important point has been overlooked in applying STM to VOS: The solution (=STM) is non-local, but the problem (=VOS) is predominantly local. To solve this mismatch between STM and VOS, we propose new VOS networks called kernelized memory network (KMN) and KMN with multiple kernels (KMN $^{M}$ ). Our networks conduct not only Query-to-Memory matching but also Memory-to-Query matching. In Memory-to-Query matching, a kernel is employed to reduce the degree of non-localness of the STM. In addition, we present a Hide-and-Seek strategy in pre-training to handle occlusions effectively. The proposed networks surpass the state-of-the-art results on standard benchmarks by a significant margin (+4\% in $\mathcal {J_{M}}$ on DAVIS 2017 test-dev set). The runtimes of our proposed KMN and KMN $^{M}$ on DAVIS 2016 validation set are 0.12 and 0.13 seconds per frame, respectively, and the two networks have similar computation times to STM.},
  archive      = {J_TPAMI},
  author       = {Hongje Seong and Junhyuk Hyun and Euntai Kim},
  doi          = {10.1109/TPAMI.2022.3163375},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2595-2612},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video object segmentation using kernelized memory network with multiple kernels},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised feature selection via graph regularized
nonnegative CP decomposition. <em>TPAMI</em>, <em>45</em>(2), 2582–2594.
(<a href="https://doi.org/10.1109/TPAMI.2022.3160205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection has attracted remarkable attention recently. With the development of data acquisition technology, multi-dimensional tensor data has been appeared in enormous real-world applications. However, most existing unsupervised feature selection methods are non-tensor-based which results the vectorization of tensor data as a preprocessing step. This seemingly ordinary operation has led to an unnecessary loss of the multi-dimensional structural information and eventually restricted the quality of the selected features. To overcome the limitation, in this paper, we propose a novel unsupervised feature selection model: Nonnegative tensor CP (CANDECOMP/PARAFAC) decomposition based unsupervised feature selection, CPUFS for short. In specific, we devise new tensor-oriented linear classifier and feature selection matrix for CPUFS. In addition, CPUFS simultaneously conducts graph regularized nonnegative CP decomposition and newly-designed tensor-oriented pseudo label regression and feature selection to fully preserve the multi-dimensional data structure. To solve the CPUFS model, we propose an efficient iterative optimization algorithm with theoretically guaranteed convergence, whose computational complexity scales linearly in the number of features. A variation of the CPUFS model by incorporating nonnegativity into the linear classifier, namely CPUFSnn, is also proposed and studied. Experimental results on ten real-world benchmark datasets demonstrate the effectiveness of both CPUFS and CPUFSnn over the state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Bilian Chen and Jiewen Guan and Zhening Li},
  doi          = {10.1109/TPAMI.2022.3160205},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2582-2594},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised feature selection via graph regularized nonnegative CP decomposition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware contrastive distillation for incremental
semantic segmentation. <em>TPAMI</em>, <em>45</em>(2), 2567–2581. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental and challenging problem in deep learning is catastrophic forgetting, i.e., the tendency of neural networks to fail to preserve the knowledge acquired from old tasks when learning new tasks. This problem has been widely investigated in the research community and several Incremental Learning (IL) approaches have been proposed in the past years. While earlier works in computer vision have mostly focused on image classification and object detection, more recently some IL approaches for semantic segmentation have been introduced. These previous works showed that, despite its simplicity, knowledge distillation can be effectively employed to alleviate catastrophic forgetting. In this paper, we follow this research direction and, inspired by recent literature on contrastive learning, we propose a novel distillation framework, Uncertainty-aware Contrastive Distillation (UCD). In a nutshell, UCDis operated by introducing a novel distillation loss that takes into account all the images in a mini-batch, enforcing similarity between features associated to all the pixels from the same classes, and pulling apart those corresponding to pixels from different classes. In order to mitigate catastrophic forgetting, we contrast features of the new model with features extracted by a frozen model learned at the previous incremental step. Our experimental results demonstrate the advantage of the proposed distillation technique, which can be used in synergy with previous IL approaches, and leads to state-of-art performance on three commonly adopted benchmarks for incremental semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Guanglei Yang and Enrico Fini and Dan Xu and Paolo Rota and Mingli Ding and Moin Nabi and Xavier Alameda-Pineda and Elisa Ricci},
  doi          = {10.1109/TPAMI.2022.3163806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2567-2581},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uncertainty-aware contrastive distillation for incremental semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trusted multi-view classification with dynamic evidential
fusion. <em>TPAMI</em>, <em>45</em>(2), 2551–2566. (<a
href="https://doi.org/10.1109/TPAMI.2022.3171983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-view classification algorithms focus on promoting accuracy by exploiting different views, typically integrating them into common representations for follow-up tasks. Although effective, it is also crucial to ensure the reliability of both the multi-view integration and the final decision, especially for noisy, corrupted and out-of-distribution data. Dynamically assessing the trustworthiness of each view for different samples could provide reliable integration. This can be achieved through uncertainty estimation. With this in mind, we propose a novel multi-view classification algorithm, termed trusted multi-view classification (TMC), providing a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The proposed TMC can promote classification reliability by considering evidence from each view. Specifically, we introduce the variational Dirichlet to characterize the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness against possible noise or corruption. Both theoretical and experimental results validate the effectiveness of the proposed model in accuracy, robustness and trustworthiness.},
  archive      = {J_TPAMI},
  author       = {Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},
  doi          = {10.1109/TPAMI.2022.3171983},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2551-2566},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Trusted multi-view classification with dynamic evidential fusion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toyota smarthome untrimmed: Real-world untrimmed videos for
activity detection. <em>TPAMI</em>, <em>45</em>(2), 2533–2550. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing activity detection systems that can be successfully deployed in daily-living environments requires datasets that pose the challenges typical of real-world scenarios. In this paper, we introduce a new untrimmed daily-living dataset that features several real-world challenges: Toyota Smarthome Untrimmed (TSU). TSU contains a wide variety of activities performed in a spontaneous manner. The dataset contains dense annotations including elementary, composite activities and activities involving interactions with objects. We provide an analysis of the real-world challenges featured by our dataset, highlighting the open issues for detection algorithms. We show that current state-of-the-art methods fail to achieve satisfactory performance on the TSU dataset. Therefore, we propose a new baseline method for activity detection to tackle the novel challenges provided by our dataset. This method leverages one modality (i.e. optic flow) to generate the attention weights to guide another modality (i.e RGB) to better detect the activity boundaries. This is particularly beneficial to detect activities characterized by high temporal variance. We show that the method we propose outperforms state-of-the-art methods on TSU and on another popular challenging dataset, Charades.},
  archive      = {J_TPAMI},
  author       = {Rui Dai and Srijan Das and Saurav Sharma and Luca Minciullo and Lorenzo Garattoni and Francois Bremond and Gianpiero Francesca},
  doi          = {10.1109/TPAMI.2022.3169976},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2533-2550},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toyota smarthome untrimmed: Real-world untrimmed videos for activity detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-ordered recent event (TORE) volumes for event cameras.
<em>TPAMI</em>, <em>45</em>(2), 2519–2532. (<a
href="https://doi.org/10.1109/TPAMI.2022.3172212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras are an exciting, new sensor modality enabling high-speed imaging with extremely low-latency and wide dynamic range. Unfortunately, most machine learning architectures are not designed to directly handle sparse data, like that generated from event cameras. Many state-of-the-art algorithms for event cameras rely on interpolated event representations—obscuring crucial timing information, increasing the data volume, and limiting overall network performance. This paper details an event representation called Time-Ordered Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw spike timing information with minimal information loss. This bio-inspired design is memory efficient, computationally fast, avoids time-blocking (i.e., fixed and predefined frame rates), and contains “local memory” from past data. The design is evaluated on a wide range of challenging tasks (e.g., event denoising, image reconstruction, classification, and human pose estimation) and is shown to dramatically improve state-of-the-art performance. TORE volumes are an easy-to-implement replacement for any algorithm currently utilizing event representations.},
  archive      = {J_TPAMI},
  author       = {R. Wes Baldwin and Ruixu Liu and Mohammed Almatrafi and Vijayan Asari and Keigo Hirakawa},
  doi          = {10.1109/TPAMI.2022.3172212},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2519-2532},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Time-ordered recent event (TORE) volumes for event cameras},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The group loss++: A deeper look into group loss for deep
metric learning. <em>TPAMI</em>, <em>45</em>(2), 2505–2518. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss , a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that “similar objects should belong to the same group”, the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We design a set of inference strategies tailored towards our algorithm, named Group Loss++ that further improve the results of our model. We show state-of-the-art results on clustering and image retrieval on four retrieval datasets, and present competitive results on two person re-identification datasets, providing a unified framework for retrieval and re-identification.},
  archive      = {J_TPAMI},
  author       = {Ismail Elezi and Jenny Seidenschwarz and Laurin Wagner and Sebastiano Vascon and Alessandro Torcinovich and Marcello Pelillo and Laura Leal-Taixé},
  doi          = {10.1109/TPAMI.2022.3163846},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2505-2518},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The group loss++: A deeper look into group loss for deep metric learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text-driven video acceleration: A weakly-supervised
reinforcement learning method. <em>TPAMI</em>, <em>45</em>(2),
2492–2504. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of videos in our digital age and the users’ limited time raise the demand for processing untrimmed videos to produce shorter versions conveying the same information. Despite the remarkable progress that summarization methods have made, most of them can only select a few frames or skims, creating visual gaps and breaking the video context. This paper presents a novel weakly-supervised methodology based on a reinforcement learning formulation to accelerate instructional videos using text. A novel joint reward function guides our agent to select which frames to remove and reduce the input video to a target length without creating gaps in the final video. We also propose the Extended Visually-guided Document Attention Network (VDAN+), which can generate a highly discriminative embedding space to represent both textual and visual data. Our experiments show that our method achieves the best performance in Precision, Recall, and F1 Score against the baselines while effectively controlling the video’s output length.},
  archive      = {J_TPAMI},
  author       = {Washington Ramos and Michel Silva and Edson Araujo and Victor Moura and Keller Oliveira and Leandro Soriano Marcolino and Erickson R. Nascimento},
  doi          = {10.1109/TPAMI.2022.3157198},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2492-2504},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Text-driven video acceleration: A weakly-supervised reinforcement learning method},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TAGNet: Learning configurable context pathways for semantic
segmentation. <em>TPAMI</em>, <em>45</em>(2), 2475–2491. (<a
href="https://doi.org/10.1109/TPAMI.2022.3165034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art semantic segmentation methods capture the relationship between pixels to facilitate contextual information exchange. Advanced methods utilize fixed pathways for context exchange, lacking the flexibility to harness the most relevant context for each pixel. In this paper, we present Configurable Context Pathways (CCPs), a novel model for establishing pathways for augmenting contextual information. In contrast to previous pathway models, CCPs are learned, leveraging configurable regions to form information flows between pairs of pixels. We propose TAGNet to adaptively configure the regions, which span over the entire image space, driven by the relationships between the remote pixels. Subsequently, the information flows along the pathways are updated gradually by the information provided by sequences of configurable regions, forming more powerful contextual information. We extensively evaluate the traveling, adaption, and gathering (TAG) stages of our network on the public benchmarks, demonstrating that all of the stages successfully improve the segmentation accuracy and help to surpass the state-of-the-art results. The code package is available at: https://github.com/dilincv/TAGNet .},
  archive      = {J_TPAMI},
  author       = {Di Lin and Dingguo Shen and Yuanfeng Ji and Siting Shen and Mingrui Xie and Wei Feng and Hui Huang},
  doi          = {10.1109/TPAMI.2022.3165034},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2475-2491},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TAGNet: Learning configurable context pathways for semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SphereFace revived: Unifying hyperspherical face
recognition. <em>TPAMI</em>, <em>45</em>(2), 2458–2474. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the deep face recognition problem under an open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. To this end, hyperspherical face recognition, as a promising line of research, has attracted increasing attention and gradually become a major focus in face recognition research. As one of the earliest works in hyperspherical face recognition, SphereFace explicitly proposed to learn face embeddings with large inter-class angular margin. However, SphereFace still suffers from severe training instability which limits its application in practice. In order to address this problem, we introduce a unified framework to understand large angular margin in hyperspherical face recognition. Under this framework, we extend the study of SphereFace and propose an improved variant with substantially better training stability – SphereFace-R. Specifically, we propose two novel ways to implement the multiplicative margin, and study SphereFace-R under three different feature normalization schemes (no feature normalization, hard feature normalization and soft feature normalization). We also propose an implementation strategy – “characteristic gradient detachment” – to stabilize training. Extensive experiments on SphereFace-R show that it is consistently better than or competitive with state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Weiyang Liu and Yandong Wen and Bhiksha Raj and Rita Singh and Adrian Weller},
  doi          = {10.1109/TPAMI.2022.3159732},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2458-2474},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SphereFace revived: Unifying hyperspherical face recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIFT matching by context exposed. <em>TPAMI</em>,
<em>45</em>(2), 2445–2457. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how to step up local image descriptor matching by exploiting matching context information. Two main contexts are identified, originated respectively from the descriptor space and from the keypoint space. The former is generally used to design the actual matching strategy while the latter to filter matches according to the local spatial consistency. On this basis, a new matching strategy and a novel local spatial filter, named respectively blob matching and Delaunay Triangulation Matching (DTM) are devised. Blob matching provides a general matching framework by merging together several strategies, including rank-based pre-filtering as well as many-to-many and symmetric matching, enabling to achieve a global improvement upon each individual strategy. DTM alternates between Delaunay triangulation contractions and expansions to figure out and adjust keypoint neighborhood consistency. Experimental evaluation shows that DTM is comparable or better than the state-of-the-art in terms of matching accuracy and robustness. Evaluation is carried out according to a new benchmark devised for analyzing the matching pipeline in terms of correct correspondences on both planar and non-planar scenes, including several state-of-the-art methods as well as the common SIFT matching approach for reference. This evaluation can be of assistance for future research in this field.},
  archive      = {J_TPAMI},
  author       = {Fabio Bellavia},
  doi          = {10.1109/TPAMI.2022.3161853},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2445-2457},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SIFT matching by context exposed},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shaping deep feature space towards gaussian mixture for
visual classification. <em>TPAMI</em>, <em>45</em>(2), 2430–2444. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The softmax cross-entropy loss function has been widely used to train deep models for various tasks. In this work, we propose a Gaussian mixture (GM) loss function for deep neural networks for visual classification. Unlike the softmax cross-entropy loss, our method explicitly shapes the deep feature space towards a Gaussian Mixture distribution. With a classification margin and a likelihood regularization, the GM loss facilitates both high classification performance and accurate modeling of the feature distribution. The GM loss can be readily used to distinguish the adversarial examples based on the discrepancy between feature distributions of clean and adversarial examples. Furthermore, theoretical analysis shows that a symmetric feature space can be achieved by using the GM loss, which enables the models to perform robustly against adversarial attacks. The proposed model can be implemented easily and efficiently without introducing more trainable parameters. Extensive evaluations demonstrate that the method with the GM loss performs favorably on image classification, face recognition, and detection as well as recognition of adversarial examples generated by various attacks.},
  archive      = {J_TPAMI},
  author       = {Weitao Wan and Cheng Yu and Jiansheng Chen and Tong Wu and Yuanyi Zhong and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2022.3166879},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2430-2444},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Shaping deep feature space towards gaussian mixture for visual classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of graph neural networks: A unified
review. <em>TPAMI</em>, <em>45</em>(2), 2412–2429. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.},
  archive      = {J_TPAMI},
  author       = {Yaochen Xie and Zhao Xu and Jingtun Zhang and Zhengyang Wang and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2022.3170559},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2412-2429},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised learning of graph neural networks: A unified review},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SegBlocks: Block-based dynamic resolution networks for
real-time segmentation. <em>TPAMI</em>, <em>45</em>(2), 2400–2411. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SegBlocks reduces the computational cost of existing neural networks, by dynamically adjusting the processing resolution of image regions based on their complexity. Our method splits an image into blocks and downsamples blocks of low complexity, reducing the number of operations and memory consumption. A lightweight policy network, selecting the complex regions, is trained using reinforcement learning. In addition, we introduce several modules implemented in CUDA to process images in blocks. Most important, our novel BlockPad module prevents the feature discontinuities at block borders of which existing methods suffer, while keeping memory consumption under control. Our experiments on Cityscapes, Camvid and Mapillary Vistas datasets for semantic segmentation show that dynamically processing images offers a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For instance, our method reduces the number of floating-point operations of SwiftNet-RN18 by 60\% and increases the inference speed by 50\%, with only 0.3\% decrease in mIoU accuracy on Cityscapes.},
  archive      = {J_TPAMI},
  author       = {Thomas Verelst and Tinne Tuytelaars},
  doi          = {10.1109/TPAMI.2022.3162528},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2400-2411},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SegBlocks: Block-based dynamic resolution networks for real-time segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCRDet++: Detecting small, cluttered and rotated objects via
instance-level feature denoising and rotation loss smoothing.
<em>TPAMI</em>, <em>45</em>(2), 2384–2399. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small and cluttered objects are common in real-world which are challenging for detection. The difficulty is further pronounced when the objects are rotated, as traditional detectors often routinely locate the objects in horizontal bounding box such that the region of interest is contaminated with background or nearby interleaved objects. In this paper, we first innovatively introduce the idea of denoising to object detection. Instance-level denoising on the feature map is performed to enhance the detection to small and cluttered objects. To handle the rotation variation, we also add a novel IoU constant factor to the smooth L1 loss to address the long standing boundary problem, which to our analysis, is mainly caused by the periodicity of angular (PoA) and exchangeability of edges (EoE). By combing these two features, our proposed detector is termed as SCRDet++. Extensive experiments are performed on large aerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image dataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD and our released S $^{2}$ TLD by this paper. The results show the effectiveness of our approach. The released dataset S $^{2}$ TLD is made public available, which contains 5,786 images with 14,130 traffic light instances across five categories.},
  archive      = {J_TPAMI},
  author       = {Xue Yang and Junchi Yan and Wenlong Liao and Xiaokang Yang and Jin Tang and Tao He},
  doi          = {10.1109/TPAMI.2022.3166956},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2384-2399},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SCRDet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scale-aware automatic augmentations for object detection
with dynamic training. <em>TPAMI</em>, <em>45</em>(2), 2367–2383. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a critical technique in object detection, especially the augmentations targeting at scale invariance training (scale-aware augmentation). However, there has been little systematic investigation of how to design scale-aware data augmentation for object detection. We propose Scale-aware AutoAug to learn data augmentation policies for object detection. We define a new scale-aware search space, where both image- and instance-level augmentations are designed for maintaining scale robust feature learning. Upon this search space, we propose a new search metric, termed Pareto Scale Balance, to facilitate efficient augmentation policy search. In experiments, Scale-aware AutoAug yields significant and consistent improvement on various object detectors (e.g., RetinaNet, Faster R-CNN, Mask R-CNN, and FCOS), even compared with strong multi-scale training baselines. Our searched augmentation policies are generalized well to other datasets and instance-level tasks beyond object detection, e.g., instance segmentation. The search cost is much less than previous automated augmentation approaches for object detection, i.e., 8 GPUs across 2.5 days versus. 800 TPU-days. In addition, meaningful patterns can be summarized from our searched policies, which intuitively provide valuable knowledge for hand-crafted data augmentation design. Based on the searched scale-aware augmentation policies, we further introduce a dynamic training paradigm to adaptively determine specific augmentation policy usage during training. The dynamic paradigm consists of an heuristic manner for image-level augmentations and a differentiable copy-paste-based method for instance-level augmentations. The dynamic paradigm achieves further performance improvements to Scale-aware AutoAug without any additional burden on the long tailed LVIS benchmarks. We also demonstrate its ability to prevent over-fitting for large models, e.g., the Swin Transformer large model. Code and models are available at https://github.com/dvlab-research/SA-AutoAug .},
  archive      = {J_TPAMI},
  author       = {Yukang Chen and Peizhen Zhang and Tao Kong and Yanwei Li and Xiangyu Zhang and Lu Qi and Jian Sun and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3166905},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2367-2383},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scale-aware automatic augmentations for object detection with dynamic training},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salient objects in clutter. <em>TPAMI</em>, <em>45</em>(2),
2344–2366. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we identify and address a serious design bias of existing salient object detection (SOD) datasets, which unrealistically assume that each image should contain at least one clear and uncluttered salient object. This design bias has led to a saturation in performance for state-of-the-art SOD models when evaluated on existing datasets. However, these models are still far from satisfactory when applied to real-world scenes. Based on our analyses, we propose a new high-quality dataset and update the previous saliency benchmark. Specifically, our dataset, called Salient Objects in Clutter (SOC) , includes images with both salient and non-salient objects from several common object categories. In addition to object category annotations, each salient image is accompanied by attributes that reflect common challenges in common scenes, which can help provide deeper insight into the SOD problem. Further, with a given saliency encoder, e.g., the backbone network, existing saliency models are designed to achieve mapping from the training image set to the training ground-truth set. We therefore argue that improving the dataset can yield higher performance gains than focusing only on the decoder design. With this in mind, we investigate several dataset-enhancement strategies, including label smoothing to implicitly emphasize salient boundaries, random image augmentation to adapt saliency models to various scenarios, and self-supervised learning as a regularization strategy to learn from small datasets. Our extensive results demonstrate the effectiveness of these tricks. We also provide a comprehensive benchmark for SOD, which can be found in our repository: https://github.com/DengPingFan/SODBenchmark .},
  archive      = {J_TPAMI},
  author       = {Deng-Ping Fan and Jing Zhang and Gang Xu and Ming-Ming Cheng and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3166451},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2344-2366},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Salient objects in clutter},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe RuleFit: Learning optimal sparse rule model by meta
safe screening. <em>TPAMI</em>, <em>45</em>(2), 2330–2343. (<a
href="https://doi.org/10.1109/TPAMI.2022.3167993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning a sparse rule model , a prediction model in the form of a sparse linear combination of rules, where a rule is an indicator function defined over a hyper-rectangle in the input space. Since the number of all possible such rules is extremely large, it has been computationally intractable to select the optimal set of active rules. In this paper, to solve this difficulty for learning the optimal sparse rule model, we propose Safe RuleFit (SRF) . Our basic idea is to develop meta safe screening (mSS) , which is a non-trivial extension of well-known safe screening (SS) techniques. While SS is used for screening out one feature, mSS can be used for screening out multiple features by exploiting the inclusion-relations of hyper-rectangles in the input space. SRF provides a general framework for fitting sparse rule models for regression and classification, and it can be extended to handle more general sparse regularizations such as group regularization. We demonstrate the advantages of SRF through intensive numerical experiments.},
  archive      = {J_TPAMI},
  author       = {Hiroki Kato and Hiroyuki Hanada and Ichiro Takeuchi},
  doi          = {10.1109/TPAMI.2022.3167993},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2330-2343},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Safe RuleFit: Learning optimal sparse rule model by meta safe screening},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review of serial and parallel min-cut/max-flow algorithms
for computer vision. <em>TPAMI</em>, <em>45</em>(2), 2310–2329. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of problems in computer vision and thus significant effort has been put into developing fast min-cut/max-flow algorithms. As a result, it is difficult to choose an ideal algorithm for a given problem. Furthermore, parallel algorithms have not been thoroughly compared. In this paper, we evaluate the state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest set of computer vision problems yet. We focus on generic algorithms, i.e., for unstructured graphs, but also compare with the specialized GridCut implementation. When applicable, GridCut performs best. Otherwise, the two pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth first search, achieves the overall best performance. The most memory efficient implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic parallel algorithms, we find the bottom-up merging approach by Liu and Sun to be best, but no method is dominant. Of the generic parallel methods, only the parallel preflow push-relabel algorithm is able to efficiently scale with many processors across problem sizes, and no generic parallel method consistently outperforms serial algorithms. Finally, we provide and evaluate strategies for algorithm selection to obtain good expected performance. We make our dataset and implementations publicly available for further research.},
  archive      = {J_TPAMI},
  author       = {Patrick M. Jensen and Niels Jeppesen and Anders B. Dahl and Vedrana A. Dahl},
  doi          = {10.1109/TPAMI.2022.3170096},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2310-2329},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Review of serial and parallel min-Cut/Max-flow algorithms for computer vision},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Reinforced causal explainer for graph neural networks.
<em>TPAMI</em>, <em>45</em>(2), 2297–2309. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability is crucial for probing graph neural networks (GNNs), answering questions like “Why the GNN model makes a certain prediction?”. Feature attribution is a prevalent technique of highlighting the explanatory subgraph in the input graph, which plausibly leads the GNN model to make its prediction. Various attribution methods have been proposed to exploit gradient-like or attention scores as the attributions of edges, then select the salient edges with top attribution scores as the explanation. However, most of these works make an untenable assumption — the selected edges are linearly independent — thus leaving the dependencies among edges largely unexplored, especially their coalition effect. We demonstrate unambiguous drawbacks of this assumption — making the explanatory subgraph unfaithful and verbose. To address this challenge, we propose a reinforcement learning agent, Reinforced Causal Explainer (RC-Explainer). It frames the explanation task as a sequential decision process — an explanatory subgraph is successively constructed by adding a salient edge to connect the previously selected subgraph. Technically, its policy network predicts the action of edge addition, and gets a reward that quantifies the action’s causal effect on the prediction. Such reward accounts for the dependency of the newly-added edge and the previously-added edges, thus reflecting whether they collaborate together and form a coalition to pursue better explanations. It is trained via policy gradient to optimize the reward stream of edge sequences. As such, RC-Explainer is able to generate faithful and concise explanations, and has a better generalization power to unseen graphs. When explaining different GNNs on three graph classification datasets, RC-Explainer achieves better or comparable performance to state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks and visual inspections. Codes and datasets are available at https://github.com/xiangwang1223/reinforced_causal_explainer .},
  archive      = {J_TPAMI},
  author       = {Xiang Wang and Yingxin Wu and An Zhang and Fuli Feng and Xiangnan He and Tat-Seng Chua},
  doi          = {10.1109/TPAMI.2022.3170302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2297-2309},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reinforced causal explainer for graph neural networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reduced-rank tensor-on-tensor regression and tensor-variate
analysis of variance. <em>TPAMI</em>, <em>45</em>(2), 2282–2296. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting regression models with many multivariate responses and covariates can be challenging, but such responses and covariates sometimes have tensor-variate structure. We extend the classical multivariate regression model to exploit such structure in two ways: first, we impose four types of low-rank tensor formats on the regression coefficients. Second, we model the errors using the tensor-variate normal distribution that imposes a Kronecker separable format on the covariance matrix. We obtain maximum likelihood estimators via block-relaxation algorithms and derive their computational complexity and asymptotic distributions. Our regression framework enables us to formulate tensor-variate analysis of variance (TANOVA) methodology. This methodology, when applied in a one-way TANOVA layout, enables us to identify cerebral regions significantly associated with the interaction of suicide attempters or non-attemptor ideators and positive-, negative- or death-connoting words in a functional Magnetic Resonance Imaging study. Another application uses three-way TANOVA on the Labeled Faces in the Wild image dataset to distinguish facial characteristics related to ethnic origin, age group and gender. A R package totr implements the methodology.},
  archive      = {J_TPAMI},
  author       = {Carlos Llosa-Vite and Ranjan Maitra},
  doi          = {10.1109/TPAMI.2022.3164836},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2282-2296},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reduced-rank tensor-on-tensor regression and tensor-variate analysis of variance},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent neural networks for snapshot compressive imaging.
<em>TPAMI</em>, <em>45</em>(2), 2264–2281. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional high-speed and spectral imaging systems are expensive and they usually consume a significant amount of memory and bandwidth to save and transmit the high-dimensional data. By contrast, snapshot compressive imaging (SCI), where multiple sequential frames are coded by different masks and then summed to a single measurement, is a promising idea to use a 2-dimensional camera to capture 3-dimensional scenes. In this paper, we consider the reconstruction problem in SCI, i.e., recovering a series of scenes from a compressed measurement. Specifically, the measurement and modulation masks are fed into our proposed network, dubbed BI directional R ecurrent N eural networks with A dversarial T raining (BIRNAT) to reconstruct the desired frames. BIRNAT employs a deep convolutional neural network with residual blocks and self-attention to reconstruct the first frame, based on which a bidirectional recurrent neural network is utilized to sequentially reconstruct the following frames. Moreover, we build an extended BIRNAT-color algorithm for color videos aiming at joint reconstruction and demosaicing. Extensive results on both video and spectral, simulation and real data from three SCI cameras demonstrate the superior performance of BIRNAT.},
  archive      = {J_TPAMI},
  author       = {Ziheng Cheng and Bo Chen and Ruiying Lu and Zhengjue Wang and Hao Zhang and Ziyi Meng and Xin Yuan},
  doi          = {10.1109/TPAMI.2022.3161934},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2264-2281},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recurrent neural networks for snapshot compressive imaging},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reciprocal GAN through characteristic functions (RCF-GAN).
<em>TPAMI</em>, <em>45</em>(2), 2246–2263. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integral probability metric (IPM) equips generative adversarial nets (GANs) with the necessary theoretical support for comparing statistical moments in an embedded domain of the critic , while stabilising their training and mitigating the mode collapse issues. For enhanced intuition and physical insight, we introduce a generalisation of IPM-GANs which operates by directly comparing probability distributions rather than their moments. This is achieved through characteristic functions (CFs), a powerful tool that uniquely comprises all information about any general distribution. For rigour, we first theoretically prove the ability of the CF loss to compare probability distributions, and proceed to establish the physical meaning of the phase and amplitude of CFs. An optimal sampling strategy is then developed to calculate the CFs, and an equivalence between the embedded and data domains is proved under the reciprocal theory. This makes it possible to seamlessly combine IPM-GAN with an auto-encoder structure by an advanced anchor architecture, which adversarially learns a semantic low-dimensional manifold for both generation and reconstruction. This efficient reciprocal CF GAN (RCF-GAN) structure, uses only two modules and a simple training strategy to achieve the state-of-the-art bi-directional generation. Experiments demonstrate the superior performance of RCF-GAN on both regular (images) and irregular (graph) domains.},
  archive      = {J_TPAMI},
  author       = {Shengxi Li and Zeyang Yu and Min Xiang and Danilo Mandic},
  doi          = {10.1109/TPAMI.2022.3157444},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2246-2263},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reciprocal GAN through characteristic functions (RCF-GAN)},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Query-efficient black-box adversarial attack with customized
iteration and sampling. <em>TPAMI</em>, <em>45</em>(2), 2226–2245. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to fool an image classifier based on deep neural networks under the black-box setting where the target model can only be queried. Among existing black-box attacks, transfer-based methods tend to overfit the substitute model on parameter settings. Decision-based methods have low query efficiency due to fixed sampling and greedy search strategy. To alleviate the above problems, we present a new framework for query-efficient black-box adversarial attack by bridging transfer-based and decision-based attacks. We reveal the relationship between current noise and variance of sampling, the monotonicity of noise compression, and the influence of transition function on the decision-based attack. Guided by the new framework, we propose a black-box adversarial attack named Customized Iteration and Sampling Attack (CISA). CISA estimates the distance from nearby decision boundary to set the stepsize, and uses a dual-direction iterative trajectory to find the intermediate adversarial example. Based on the intermediate adversarial example, CISA conducts customized sampling according to the noise sensitivity of each pixel to further compress noise, and relaxes the state transition function to achieve higher query efficiency. Extensive experiments demonstrate CISA’s advantage in query efficiency of black-box adversarial attacks.},
  archive      = {J_TPAMI},
  author       = {Yucheng Shi and Yahong Han and Qinghua Hu and Yi Yang and Qi Tian},
  doi          = {10.1109/TPAMI.2022.3169802},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2226-2245},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Query-efficient black-box adversarial attack with customized iteration and sampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). PredRNN: A recurrent neural network for spatiotemporal
predictive learning. <em>TPAMI</em>, <em>45</em>(2), 2208–2225. (<a
href="https://doi.org/10.1109/TPAMI.2022.3165153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further propose a new curriculum learning strategy to force PredRNN to learn long-term dynamics from context frames, which can be generalized to most sequence-to-sequence models. We provide detailed ablation studies to verify the effectiveness of each component. Our approach is shown to obtain highly competitive results on five datasets for both action-free and action-conditioned predictive learning scenarios.},
  archive      = {J_TPAMI},
  author       = {Yunbo Wang and Haixu Wu and Jianjin Zhang and Zhifeng Gao and Jianmin Wang and Philip S. Yu and Mingsheng Long},
  doi          = {10.1109/TPAMI.2022.3165153},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2208-2225},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PredRNN: A recurrent neural network for spatiotemporal predictive learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PointGLR: Unsupervised structural representation learning of
3D point clouds. <em>TPAMI</em>, <em>45</em>(2), 2193–2207. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores the use of global and local structures of 3D point clouds as a free and powerful supervision signal for representation learning. Local and global patterns of a 3D object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning about the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose a new framework to learn point cloud representations by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape. Moreover, we extend the unsupervised structural representation learning method to more complex 3D scenes. By introducing structural proxies as the intermediate-level representations between local and global ones, we propose a hierarchical reasoning scheme among local parts, structural proxies, and the overall point cloud to learn powerful 3D representations in an unsupervised manner. Extensive experimental results demonstrate that the unsupervised representations can be very competitive alternatives of supervised representations in discriminative power, and exhibit better performance in generalization ability and robustness. Our method establishes the new state-of-the-art of unsupervised/few-shot 3D object classification and part segmentation. We also show our method can serve as a simple yet effective regime for model pre-training on 3D scene segmentation and detection tasks. We expect our observations to offer a new perspective on learning better representations from data structures instead of human annotations for point cloud understanding.},
  archive      = {J_TPAMI},
  author       = {Yongming Rao and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2022.3159794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2193-2207},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PointGLR: Unsupervised structural representation learning of 3D point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point spatio-temporal transformer networks for point cloud
video modeling. <em>TPAMI</em>, <em>45</em>(2), 2181–2192. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent unorderliness and irregularity of point cloud, points emerge inconsistently across different frames in a point cloud video. To capture the dynamics in point cloud videos, tracking points and limiting temporal modeling range are usually employed to preserve spatio-temporal structure. However, as points may flow in and out across frames, computing accurate point trajectories is extremely difficult, especially for long videos. Moreover, when points move fast, even in a small temporal window, points may still escape from a region. Besides, using the same temporal range for different motions may not accurately capture the temporal structure. In this paper, we propose a Point Spatio-Temporal Transformer (PST-Transformer). To preserve the spatio-temporal structure, PST-Transformer adaptively searches related or similar points across the entire video by performing self-attention on point features. Moreover, our PST-Transformer is equipped with an ability to encode spatio-temporal structure. Because point coordinates are irregular and unordered but point timestamps exhibit regularities and order, the spatio-temporal encoding is decoupled to reduce the impact of the spatial irregularity on the temporal modeling. By properly preserving and encoding spatio-temporal structure, our PST-Transformer effectively models point cloud videos and shows superior performance on 3D action recognition and 4D semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Hehe Fan and Yi Yang and Mohan Kankanhalli},
  doi          = {10.1109/TPAMI.2022.3161735},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2181-2192},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Point spatio-temporal transformer networks for point cloud video modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pixel2Mesh++: 3D mesh generation and refinement from
multi-view images. <em>TPAMI</em>, <em>45</em>(2), 2166–2180. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of shape generation in 3D mesh representation from a small number of color images with or without camera poses. While many previous works learn to hallucinate the shape directly from priors, we adopt to further improve the shape quality by leveraging cross-view information with a graph convolution network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh’s vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shapes that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, and the number of input images. Model analysis experiments show that our model is robust to the quality of the initial mesh and the error of camera pose, and can be combined with a differentiable renderer for test-time optimization.},
  archive      = {J_TPAMI},
  author       = {Chao Wen and Yinda Zhang and Chenjie Cao and Zhuwen Li and Xiangyang Xue and Yanwei Fu},
  doi          = {10.1109/TPAMI.2022.3169735},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2166-2180},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pixel2Mesh++: 3D mesh generation and refinement from multi-view images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics-guided reflection separation from a pair of
unpolarized and polarized images. <em>TPAMI</em>, <em>45</em>(2),
2151–2165. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undesirable reflections contained in photos taken in front of glass windows or doors often degrade visual quality of the image. Separating two layers apart benefits both human and machine perception. The polarization status of the light changes after refraction or reflection, providing more observations of the scene, which can benefit the reflection separation. Different from previous works that take three or more polarization images as input, we propose to exploit physical constraints from a pair of unpolarized and polarized images to separate reflection and transmission layers in this paper. Due to the simplified capturing setup, the system is more under-determined compared to the existing polarization-based works. In order to solve this problem, we propose to estimate the semi-reflector orientation first to make the physical image formation well-posed, and then learn to reliably separate two layers using additional networks based on both physical and numerical analysis. In addition, a motion estimation network is introduced to handle the misalignment of paired input. Quantitative and qualitative experimental results show our approach performs favorably over existing polarization and single image based solutions.},
  archive      = {J_TPAMI},
  author       = {Youwei Lyu and Zhaopeng Cui and Si Li and Marc Pollefeys and Boxin Shi},
  doi          = {10.1109/TPAMI.2022.3162716},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2151-2165},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Physics-guided reflection separation from a pair of unpolarized and polarized images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics perception in sloshing scenes with guaranteed
thermodynamic consistency. <em>TPAMI</em>, <em>45</em>(2), 2136–2150.
(<a href="https://doi.org/10.1109/TPAMI.2022.3160100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics perception very often faces the problem that only limited data or partial measurements on the scene are available. In this work, we propose a strategy to learn the full state of sloshing liquids from measurements of the free surface. Our approach is based on recurrent neural networks (RNN) that project the limited information available to a reduced-order manifold to not only reconstruct the unknown information but also be capable of performing fluid reasoning about future scenarios in real-time. To obtain physically consistent predictions, we train deep neural networks on the reduced-order manifold that, through the employ of inductive biases, ensure the fulfillment of the principles of thermodynamics. RNNs learn from history the required hidden information to correlate the limited information with the latent space where the simulation occurs. Finally, a decoder returns data to the high-dimensional manifold, to provide the user with insightful information in the form of augmented reality. This algorithm is connected to a computer vision system to test the performance of the proposed methodology with real information, resulting in a system capable of understanding and predicting future states of the observed fluid in real-time.},
  archive      = {J_TPAMI},
  author       = {Beatriz Moya and Alberto Badías and David González and Francisco Chinesta and Elías Cueto},
  doi          = {10.1109/TPAMI.2022.3160100},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2136-2150},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Physics perception in sloshing scenes with guaranteed thermodynamic consistency},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Optimization-based post-training quantization with
bit-split and stitching. <em>TPAMI</em>, <em>45</em>(2), 2119–2135. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have shown great promise in various domains. Meanwhile, problems including the storage and computing overheads arise along with these breakthroughs. To solve these problems, network quantization has received increasing attention due to its high efficiency and hardware-friendly property. Nonetheless, most existing quantization approaches rely on the full training dataset and the time-consuming fine-tuning process to retain accuracy. Post-training quantization does not have these problems, however, it has mainly been shown effective for 8-bit quantization. In this paper, we theoretically analyze the effect of network quantization and show that the quantization loss in the final output layer is bounded by the layer-wise activation reconstruction error. Based on this analysis, we propose an Optimization-based Post-training Quantization framework and a novel Bit-split optimization approach to achieve minimal accuracy degradation. The proposed framework is validated on a variety of computer vision tasks, including image classification, object detection, instance segmentation, with various network architectures. Specifically, we achieve near-original model performance even when quantizing FP32 models to 3-bit without fine-tuning.},
  archive      = {J_TPAMI},
  author       = {Peisong Wang and Weihan Chen and Xiangyu He and Qiang Chen and Qingshan Liu and Jian Cheng},
  doi          = {10.1109/TPAMI.2022.3159369},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2119-2135},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimization-based post-training quantization with bit-split and stitching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal transport for unsupervised denoising learning.
<em>TPAMI</em>, <em>45</em>(2), 2104–2118. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, much progress has been made in unsupervised denoising learning. However, existing methods more or less rely on some assumptions on the signal and/or degradation model, which limits their practical performance. How to construct an optimal criterion for unsupervised denoising learning without any prior knowledge on the degradation model is still an open question. Toward answering this question, this work proposes a criterion for unsupervised denoising learning based on the optimal transport theory. This criterion has favorable properties, e.g., approximately maximal preservation of the information of the signal, whilst achieving perceptual reconstruction. Furthermore, though a relaxed unconstrained formulation is used in practical implementation, we prove that the relaxed formulation in theory has the same solution as the original constrained formulation. Experiments on synthetic and real-world data, including realistic photographic, microscopy, depth, and raw depth images, demonstrate that the proposed method even compares favorably with supervised methods, e.g., approaching the PSNR of supervised methods while having better perceptual quality. Particularly, for spatially correlated noise and realistic microscopy images, the proposed method not only achieves better perceptual quality but also has higher PSNR than supervised methods. Besides, it shows remarkable superiority in harsh practical conditions with complex noise, e.g., raw depth images. Code is available at https://github.com/wangweiSJTU/OTUR .},
  archive      = {J_TPAMI},
  author       = {Wei Wang and Fei Wen and Zeyu Yan and Peilin Liu},
  doi          = {10.1109/TPAMI.2022.3170155},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2104-2118},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimal transport for unsupervised denoising learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). On distinctive image captioning via comparing and
reweighting. <em>TPAMI</em>, <em>45</em>(2), 2088–2103. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent image captioning models are achieving impressive results based on popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most popular metrics that only consider the overlap between the generated captions and human annotation could result in using common words and phrases, which lacks distinctiveness, i.e., many similar images have the same caption. In this paper, we aim to improve the distinctiveness of image captions via comparing and reweighting with a set of similar images. First, we propose a distinctiveness metric—between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric reveals that the human annotations of each image in the MSCOCO dataset are not equivalent based on distinctiveness; however, previous works normally treat the human annotations equally during training, which could be a reason for generating less distinctive captions. In contrast, we reweight each ground-truth caption according to its distinctiveness during training. We further integrate a long-tailed weight strategy to highlight the rare words that contain more information, and captions from the similar image set are sampled as negative examples to encourage the generated sentence to be unique. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study.},
  archive      = {J_TPAMI},
  author       = {Jiuniu Wang and Wenjia Xu and Qingzhong Wang and Antoni B. Chan},
  doi          = {10.1109/TPAMI.2022.3159811},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2088-2103},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On distinctive image captioning via comparing and reweighting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Old photo restoration via deep latent space translation.
<em>TPAMI</em>, <em>45</em>(2), 2071–2087. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Specifically, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting the structured defects, such as scratches and dust spots, and a local branch targeting the unstructured defects, such as noises and blurriness. We also extend the global branch with a more memory-efficient scheme, named multi-scale patch-based attention to processing high-resolution photos. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. Furthermore, we apply another face refinement network to recover fine details of faces in the old photos, thus ultimately generating photos with enhanced perceptual quality. With comprehensive experiments, the proposed pipeline demonstrates superior performance over state-of-the-art methods as well as existing commercial tools in terms of visual quality for old photos restoration. Both code and models could be found at https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life .},
  archive      = {J_TPAMI},
  author       = {Ziyu Wan and Bo Zhang and Dong Chen and Pan Zhang and Dong Chen and Fang Wen and Jing Liao},
  doi          = {10.1109/TPAMI.2022.3163183},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2071-2087},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Old photo restoration via deep latent space translation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Negation of the quantum mass function for multisource
quantum information fusion with its application to pattern
classification. <em>TPAMI</em>, <em>45</em>(2), 2054–2070. (<a
href="https://doi.org/10.1109/TPAMI.2022.3167045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In artificial intelligence systems, a question on how to express the uncertainty in knowledge remains an open issue. The negation scheme provides a new perspective to solve this issue. In this paper, we study quantum decisions from the negation perspective. Specifically, complex evidence theory (CET) is considered to be effective to express and handle uncertain information in a complex plane. Therefore, we first express CET in the quantum framework of Hilbert space. On this basis, a generalized negation method is proposed for quantum basic belief assignment (QBBA), called QBBA negation. In addition, a QBBA entropy is revisited to study the QBBA negation process to reveal the variation tendency of negation iteration. Meanwhile, the properties of the QBBA negation function are analyzed and discussed along with special cases. Then, several multisource quantum information fusion (MSQIF) algorithms are designed to support decision making. Finally, these MSQIF algorithms are applied in pattern classification to demonstrate their effectiveness. This is the first work to design MSQIF algorithms to support quantum decision making from a new perspective of “negation”, which provides promising solutions to knowledge representation, uncertainty measure, and fusion of quantum information.},
  archive      = {J_TPAMI},
  author       = {Fuyuan Xiao and Witold Pedrycz},
  doi          = {10.1109/TPAMI.2022.3167045},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2054-2070},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Negation of the quantum mass function for multisource quantum information fusion with its application to pattern classification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiway non-rigid point cloud registration via learned
functional map synchronization. <em>TPAMI</em>, <em>45</em>(2),
2038–2053. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SyNoRiM, a novel way to jointly register multiple non-rigid shapes by synchronizing the maps that relate learned functions defined on the point clouds. Even though the ability to process non-rigid shapes is critical in various applications ranging from computer animation to 3D digitization, the literature still lacks a robust and flexible framework to match and align a collection of real, noisy scans observed under occlusions. Given a set of such point clouds, our method first computes the pairwise correspondences parameterized via functional maps. We simultaneously learn potentially non-orthogonal basis functions to effectively regularize the deformations, while handling the occlusions in an elegant way. To maximally benefit from the multi-way information provided by the inferred pairwise deformation fields, we synchronize the pairwise functional maps into a cycle-consistent whole thanks to our novel and principled optimization formulation. We demonstrate via extensive experiments that our method achieves a state-of-the-art performance in registration accuracy, while being flexible and efficient as we handle both non-rigid and multi-body cases in a unified framework and avoid the costly optimization over point-wise permutations by the use of basis function maps.},
  archive      = {J_TPAMI},
  author       = {Jiahui Huang and Tolga Birdal and Zan Gojcic and Leonidas J. Guibas and Shi-Min Hu},
  doi          = {10.1109/TPAMI.2022.3164653},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2038-2053},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiway non-rigid point cloud registration via learned functional map synchronization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modality deep restoration of extremely compressed face
videos. <em>TPAMI</em>, <em>45</em>(2), 2024–2037. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Xi Zhang and Xiaolin Wu},
  doi          = {10.1109/TPAMI.2022.3157388},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2024-2037},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-modality deep restoration of extremely compressed face videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motif-GCNs with local and non-local temporal blocks for
skeleton-based action recognition. <em>TPAMI</em>, <em>45</em>(2),
2009–2023. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have achieved remarkable performance for action recognition with human skeletal data by utilizing graph convolutional models. Existing models mainly focus on developing graph convolutional operations to encode structural properties of a skeletal graph, whose topology is manually predefined and fixed over all action samples. Some recent works further take sample-dependent relationships among joints into consideration. However, the complex relationships between arbitrary pairwise joints are difficult to learn and the temporal features between frames are not fully exploited by simply using traditional convolutions with small local kernels. In this paper, we propose a motif-based graph convolution method, which makes use of sample-dependent latent relations among non-physically connected joints to impose a high-order locality and assigns different semantic roles to physical neighbors of a joint to encode hierarchical structures. Furthermore, we propose a sparsity-promoting loss function to learn a sparse motif adjacency matrix for latent dependencies in non-physical connections. For extracting effective temporal information, we propose an efficient local temporal block. It adopts partial dense connections to reuse temporal features in local time windows, and enrich a variety of information flow by gradient combination. In addition, we introduce a non-local temporal block to capture global dependencies among frames. Our model can capture local and non-local relationships both spatially and temporally, by integrating the local and non-local temporal blocks into the sparse motif-based graph convolutional networks (SMotif-GCNs). Comprehensive experiments on four large-scale datasets show that our model outperforms the state-of-the-art methods. Our code is publicly available at https://github.com/wenyh1616/SAMotif-GCN .},
  archive      = {J_TPAMI},
  author       = {Yu-Hui Wen and Lin Gao and Hongbo Fu and Fang-Lue Zhang and Shihong Xia and Yong-Jin Liu},
  doi          = {10.1109/TPAMI.2022.3170511},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {2009-2023},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Motif-GCNs with local and non-local temporal blocks for skeleton-based action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular quasi-dense 3D object tracking. <em>TPAMI</em>,
<em>45</em>(2), 1992–2008. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reliable and accurate 3D tracking framework is essential for predicting future locations of surrounding objects and planning the observer’s actions in numerous applications such as autonomous driving. We propose a framework that can effectively associate moving objects over time and estimate their full 3D bounding box information from a sequence of 2D images captured on a moving platform. The object association leverages quasi-dense similarity learning to identify objects in various poses and viewpoints with appearance cues only. After initial 2D association, we further utilize 3D bounding boxes depth-ordering heuristics for robust instance association and motion-based 3D trajectory prediction for re-identification of occluded vehicles. In the end, an LSTM-based object velocity learning module aggregates the long-term trajectory information for more accurate motion extrapolation. Experiments on our proposed simulation data and real-world benchmarks, including KITTI, nuScenes, and Waymo datasets, show that our tracking framework offers robust object association and tracking on urban-driving scenarios. On the Waymo Open benchmark, we establish the first camera-only baseline in the 3D tracking and 3D detection challenges. Our quasi-dense 3D tracking pipeline achieves impressive improvements on the nuScenes 3D tracking benchmark with near five times tracking accuracy of the best vision-only submission among all published methods.},
  archive      = {J_TPAMI},
  author       = {Hou-Ning Hu and Yung-Hsu Yang and Tobias Fischer and Trevor Darrell and Fisher Yu and Min Sun},
  doi          = {10.1109/TPAMI.2022.3168781},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1992-2008},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Monocular quasi-dense 3D object tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matrix completion via non-convex relaxation and adaptive
correlation learning. <em>TPAMI</em>, <em>45</em>(2), 1981–1991. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing matrix completion methods focus on optimizing the relaxation of rank function such as nuclear norm, Schatten- $p$ norm, etc . They usually need many iterations to converge. Moreover, only the low-rank property of matrices is utilized in most existing models and several methods that incorporate other knowledge are quite time-consuming in practice. To address these issues, we propose a novel non-convex surrogate that can be optimized by closed-form solutions, such that it empirically converges within dozens of iterations. Besides, the optimization is parameter-free and the convergence is proved. Compared with the relaxation of rank, the surrogate is motivated by optimizing an upper-bound of rank. We theoretically validate that it is equivalent to the existing matrix completion models. Besides the low-rank assumption, we intend to exploit the column-wise correlation for matrix completion, and thus an adaptive correlation learning, which is scaling-invariant, is developed. More importantly, after incorporating the correlation learning, the model can be still solved by closed-form solutions such that it still converges fast. Experiments show the effectiveness of the non-convex surrogate and adaptive correlation learning.},
  archive      = {J_TPAMI},
  author       = {Xuelong Li and Hongyuan Zhang and Rui Zhang},
  doi          = {10.1109/TPAMI.2022.3157083},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1981-1991},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Matrix completion via non-convex relaxation and adaptive correlation learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to adapt across dual discrepancy for cross-domain
person re-identification. <em>TPAMI</em>, <em>45</em>(2), 1963–1980. (<a
href="https://doi.org/10.1109/TPAMI.2022.3167053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the advent of deep neural networks, recent years have witnessed rapid progress in person re-identification (re-ID). Deep-learning-based methods dominate the leadership of large-scale benchmarks, some of which even surpass the human-level performance. Despite their impressive performance under the single-domain setup, current fully-supervised re-ID models degrade significantly when transplanted to an unseen domain. According to the characteristics of the re-ID task, such degradation is mainly attributed to the dramatic variation within the target domain and the severe shift between the source and target domain, which we call dual discrepancy in this paper. To achieve a model that generalizes well to the target domain, it is desirable to take such dual discrepancy into account. In terms of the former issue, a prevailing solution is to enforce consistency between nearest-neighbors in the embedding space. However, we find that the search of neighbors is highly biased in our case due to the discrepancy across cameras. For this reason, we equip the vanilla neighborhood invariance approach with a camera-aware learning scheme. As for the latter issue, we propose a novel cross-domain mixup scheme. It works in conjunction with virtual prototypes which are employed to handle the disjoint label space between the two domains. In this way, we can realize the smooth transfer by introducing the interpolation between the two domains as a transition state. Extensive experiments on four public benchmarks demonstrate the superiority of our method. Without any auxiliary models and offline clustering procedure, it achieves competitive performance against existing state-of-the-art methods. The code is available at https://github.com/LuckyDC/generalizing-reid-improved .},
  archive      = {J_TPAMI},
  author       = {Chuanchen Luo and Chunfeng Song and Zhaoxiang Zhang},
  doi          = {10.1109/TPAMI.2022.3167053},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1963-1980},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to adapt across dual discrepancy for cross-domain person re-identification},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning hierarchical variational autoencoders with mutual
information maximization for autoregressive sequence modeling.
<em>TPAMI</em>, <em>45</em>(2), 1949–1962. (<a
href="https://doi.org/10.1109/TPAMI.2022.3160509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are a class of effective deep generative models, with the objective to approximate the true, but unknown data distribution. VAEs make use of latent variables to capture high-level semantics so as to reconstruct the data well with the help of informative latent variables. Yet, training VAEs tends to suffer from posterior collapse, when the decoder is parameterized by an autoregressive model for sequence generation. VAEs can be further enhanced by introducing multiple layers of latent variables, but the posterior collapse issue hinders the adoption of such hierarchical VAEs in real-world applications. In this paper, we introduce InfoMaxHVAE, which integrates mutual information estimated via neural networks into hierarchical VAEs to alleviate posterior collapse, when powerful autoregressive models are used for modeling sequences. Experimental results on a number of text and image datasets show that InfoMaxHVAE can outperform the state-of-the-art baselines and exhibits less posterior collapse. We further show that InfoMaxHVAE can shape a coarse-to-fine hierarchical organization of the latent space.},
  archive      = {J_TPAMI},
  author       = {Dong Qian and William K. Cheung},
  doi          = {10.1109/TPAMI.2022.3160509},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1949-1962},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning hierarchical variational autoencoders with mutual information maximization for autoregressive sequence modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning enriched features for fast image restoration and
enhancement. <em>TPAMI</em>, <em>45</em>(2), 1934–1948. (<a
href="https://doi.org/10.1109/TPAMI.2022.3167175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2, achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2 .},
  archive      = {J_TPAMI},
  author       = {Syed Waqas Zamir and Aditya Arora and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Ming-Hsuan Yang and Ling Shao},
  doi          = {10.1109/TPAMI.2022.3167175},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1934-1948},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning enriched features for fast image restoration and enhancement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning deep binary descriptors via bitwise interaction
mining. <em>TPAMI</em>, <em>45</em>(2), 1919–1933. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a GraphBit method to learn unsupervised deep binary descriptors for efficient image representation. Conventional binary representation learning methods directly quantize each element according to the threshold without considering the quantization ambiguousness. The elements near the boundary dubbed as “ambiguous bits” fail to collect effective information for reliable binarization and are sensitive to noise that causes reversed bits. We argue that there are implicit inner relationships among bits in binary descriptors called bitwise interaction, where the related bits can provide extra instruction as prior knowledge for ambiguousness reduction. Specifically, we design a deep reinforcement learning model to learn the structure of the graph for bitwise interaction mining, and the uncertainty of binary codes is reduced by maximizing the mutual information with input and related bits. Consequently, the ambiguous bits receive additional instruction from the graph for reliable binarization. Moreover, we further present a differentiable search method (GraphBit+) that mines the bitwise interaction in continuous space, so that the heavy search cost caused by the training difficulties in reinforcement learning is significantly reduced. Since the GraphBit and GraphBit+ methods learn fixed bitwise interaction which is suboptimal for various input, the inaccurate instruction from the fixed bitwise interaction cannot effectively decrease the ambiguousness of binary descriptors. To address this, we further propose the unsupervised binary descriptor learning method via dynamic bitwise interaction mining (D-GraphBit), where a graph convolutional network called GraphMiner reasons the optimal bitwise interaction for each input sample. Extensive experimental results on the CIFAR-10, NUS-WIDE, ImageNet-100, Brown and HPatches datasets demonstrate the efficiency and effectiveness of the proposed GraphBit, GraphBit+ and D-GraphBit.},
  archive      = {J_TPAMI},
  author       = {Ziwei Wang and Han Xiao and Yueqi Duan and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2022.3161600},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1919-1933},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning deep binary descriptors via bitwise interaction mining},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning continuous-time dynamics with attention.
<em>TPAMI</em>, <em>45</em>(2), 1906–1918. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the hidden dynamics from sequence data is crucial. Attention mechanism can be introduced to spotlight on the region of interest for sequential learning. Traditional attention was measured between a query and a sequence based on a discrete-time state trajectory. Such a mechanism could not characterize the irregularly-sampled sequence data. This paper presents an attentive differential network (ADN) where the attention over continuous-time dynamics is developed. The continuous-time attention is performed over the dynamics at all time. The missing information in irregular or sparse samples can be seamlessly compensated and attended. Self attention is computed to find the attended state trajectory. However, the memory cost for attention score between a query and a sequence is demanding since self attention treats all time instants as query points in an ordinary differential equation solver. This issue is tackled by imposing the causality constraint in causal ADN (CADN) where the query is merged up to current time. To enhance the model robustness, this study further explores a latent CADN where the attended dynamics are calculated in an encoder-decoder structure via Bayesian learning. Experiments on the irregularly-sampled actions, dialogues and bio-signals illustrate the merits of the proposed methods in action recognition, emotion recognition and mortality prediction, respectively.},
  archive      = {J_TPAMI},
  author       = {Jen-Tzung Chien and Yi-Hsiang Chen},
  doi          = {10.1109/TPAMI.2022.3162711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1906-1918},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning continuous-time dynamics with attention},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent gaussian model boosting. <em>TPAMI</em>,
<em>45</em>(2), 1894–1905. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Gaussian models and boosting are widely used techniques in statistics and machine learning. Tree-boosting shows excellent prediction accuracy on many data sets, but potential drawbacks are that it assumes conditional independence of samples, produces discontinuous predictions for, e.g., spatial data, and it can have difficulty with high-cardinality categorical variables. Latent Gaussian models, such as Gaussian process and grouped random effects models, are flexible prior models which explicitly model dependence among samples and which allow for efficient learning of predictor functions and for making probabilistic predictions. However, existing latent Gaussian models usually assume either a zero or a linear prior mean function which can be an unrealistic assumption. This article introduces a novel approach that combines boosting and latent Gaussian models to remedy the above-mentioned drawbacks and to leverage the advantages of both techniques. We obtain increased prediction accuracy compared to existing approaches in both simulated and real-world data experiments.},
  archive      = {J_TPAMI},
  author       = {Fabio Sigrist},
  doi          = {10.1109/TPAMI.2022.3168152},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1894-1905},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Latent gaussian model boosting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental ensemble gaussian processes. <em>TPAMI</em>,
<em>45</em>(2), 1876–1893. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belonging to the family of Bayesian nonparametrics, Gaussian process (GP) based approaches have well-documented merits not only in learning over a rich class of nonlinear functions, but also in quantifying the associated uncertainty. However, most GP methods rely on a single preselected kernel function, which may fall short in characterizing data samples that arrive sequentially in time-critical applications. To enable online kernel adaptation, the present work advocates an incremental ensemble (IE-) GP framework, where an EGP assembler employs an ensemble of GP learners, each having a unique kernel belonging to a prescribed kernel dictionary. With each GP expert leveraging the random feature-based approximation to perform online prediction and model update with scalability , the EGP assembler capitalizes on data-adaptive weights to synthesize the per-expert predictions. Further, the novel IE-GP is generalized to accommodate time-varying functions by modeling structured dynamics at the EGP assembler and within each GP learner. To benchmark the performance of IE-GP and its dynamic variant in the adversarial setting where the modeling assumptions are violated, rigorous performance analysis has been conducted via the notion of regret, as the norm in online convex optimization. Last but not the least, online unsupervised learning for dimensionality reduction is explored under the novel IE-GP framework. Synthetic and real data tests demonstrate the effectiveness of the proposed schemes.},
  archive      = {J_TPAMI},
  author       = {Qin Lu and Georgios V. Karanikolas and Georgios B. Giannakis},
  doi          = {10.1109/TPAMI.2022.3157197},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1876-1893},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Incremental ensemble gaussian processes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous domain adaptation with adversarial neural
representation learning: Experiments on e-commerce and cybersecurity.
<em>TPAMI</em>, <em>45</em>(2), 1862–1875. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning predictive models in new domains with scarce training data is a growing challenge in modern supervised learning scenarios. This incentivizes developing domain adaptation methods that leverage the knowledge in known domains (source) and adapt to new domains (target) with a different probability distribution. This becomes more challenging when the source and target domains are in heterogeneous feature spaces, known as heterogeneous domain adaptation (HDA). While most HDA methods utilize mathematical optimization to map source and target data to a common space, they suffer from low transferability. Neural representations have proven to be more transferable; however, they are mainly designed for homogeneous environments. Drawing on the theory of domain adaptation, we propose a novel framework, Heterogeneous Adversarial Neural Domain Adaptation (HANDA), to effectively maximize the transferability in heterogeneous environments. HANDA conducts feature and distribution alignment in a unified neural network architecture and achieves domain invariance through adversarial kernel learning. Three experiments were conducted to evaluate the performance against the state-of-the-art HDA methods on major image and text e-commerce benchmarks. HANDA shows statistically significant improvement in predictive performance. The practical utility of HANDA was shown in real-world dark web online markets. HANDA is an important step towards successful domain adaptation in e-commerce applications.},
  archive      = {J_TPAMI},
  author       = {Mohammadreza Ebrahimi and Yidong Chai and Hao Helen Zhang and Hsinchun Chen},
  doi          = {10.1109/TPAMI.2022.3163338},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1862-1875},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Heterogeneous domain adaptation with adversarial neural representation learning: Experiments on E-commerce and cybersecurity},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hawkes processes with stochastic exogenous effects for
continuous-time interaction modelling. <em>TPAMI</em>, <em>45</em>(2),
1848–1861. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time interaction data is usually generated under time-evolving environment. Hawkes processes (HP) are commonly used mechanisms for the analysis of such data. However, typical model implementations (such as e.g., stochastic block models) assume that the exogenous (background) interaction rate is constant, and so they are limited in their ability to adequately describe any complex time-evolution in the background rate of a process. In this paper, we introduce a stochastic exogenous rate Hawkes process (SE-HP) which is able to learn time variations in the exogenous rate. The model affiliates each node with a piecewise-constant membership distribution with an unknown number of changepoint locations, and allows these distributions to be related to the membership distributions of interacting nodes. The time-varying background rate function is derived through combinations of these membership functions. We introduce a stochastic gradient MCMC algorithm for efficient, scalable inference. The performance of the SE-HP is explored on real world, continuous-time interaction datasets, where we demonstrate that the SE-HP strongly outperforms comparable state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Xuhui Fan and Yaqiong Li and Ling Chen and Bin Li and Scott A. Sisson},
  doi          = {10.1109/TPAMI.2022.3161649},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1848-1861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hawkes processes with stochastic exogenous effects for continuous-time interaction modelling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph learning on millions of data in seconds: Label
propagation acceleration on graph using data distribution.
<em>TPAMI</em>, <em>45</em>(2), 1835–1847. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised learning methods have been used in a wide range of real-world applications, e.g., from social relationship mining to multimedia classification and retrieval. However, existing methods are limited along with high computational complexity or not facilitating incremental learning, which may not be powerful to deal with large-scale data, whose scale may continuously increase, in real world. This paper proposes a new method called Data Distribution Based Graph Learning (DDGL) for semi-supervised learning on large-scale data. This method can achieve a fast and effective label propagation and supports incremental learning. The key motivation is to propagate the labels along smaller-scale data distribution model parameters, rather than directly dealing with the raw data as previous methods, which accelerate the data propagation significantly. It also improves the prediction accuracy since the loss of structure information can be alleviated in this way. To enable incremental learning, we propose an adaptive graph updating strategy which can update the model when there is distribution bias between new data and the already seen data. We have conducted comprehensive experiments on multiple datasets with sample sizes increasing from seven thousand to five million. Experimental results on the classification task on large-scale data demonstrate that our proposed DDGL method improves the classification accuracy by a large margin while consuming much less time compared to state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Yubo Zhang and Shuyi Ji and Changqing Zou and Xibin Zhao and Shihui Ying and Yue Gao},
  doi          = {10.1109/TPAMI.2022.3166894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1835-1847},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph learning on millions of data in seconds: Label propagation acceleration on graph using data distribution},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized knowledge distillation via relationship
matching. <em>TPAMI</em>, <em>45</em>(2), 1817–1834. (<a
href="https://doi.org/10.1109/TPAMI.2022.3160328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge of a well-trained deep neural network (a.k.a. the “teacher”) is valuable for learning similar tasks. Knowledge distillation extracts knowledge from the teacher and integrates it with the target model (a.k.a. the “student”), which expands the student’s knowledge and improves its learning efficacy. Instead of enforcing the teacher to work on the same task as the student, we borrow the knowledge from a teacher trained from a general label space — in this “Generalized Knowledge Distillation (GKD),” the classes of the teacher and the student may be the same, completely different, or partially overlapped . We claim that the comparison ability between instances acts as an essential factor threading knowledge across tasks, and propose the RE lationship F ac I litated L ocal c L assifi E r D istillation ( ReFilled ) approach, which decouples the GKD flow of the embedding and the top-layer classifier. In particular, different from reconciling the instance-label confidence between models, ReFilled requires the teacher to reweight the hard tuples pushed forward by the student and then matches the similarity comparison levels between instances. An embedding-induced classifier based on the teacher model supervises the student’s classification confidence and adaptively emphasizes the most related supervision from the teacher. ReFilled demonstrates strong discriminative ability when the classes of the teacher vary from the same to a fully non-overlapped set w.r.t. the student. It also achieves state-of-the-art performance on standard knowledge distillation, one-step incremental learning, and few-shot learning tasks.},
  archive      = {J_TPAMI},
  author       = {Han-Jia Ye and Su Lu and De-Chuan Zhan},
  doi          = {10.1109/TPAMI.2022.3160328},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1817-1834},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized knowledge distillation via relationship matching},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization performance of pure accuracy and its
application in selective ensemble learning. <em>TPAMI</em>,
<em>45</em>(2), 1798–1816. (<a
href="https://doi.org/10.1109/TPAMI.2022.3171436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pure accuracy measure is used to eliminate random consistency from the accuracy measure. Biases to both majority and minority classes in the pure accuracy are lower than that in the accuracy measure. In this paper, we demonstrate that compared with the accuracy measure and F-measure, the pure accuracy measure is class distribution insensitive and discriminative for good classifiers. The advantages make the pure accuracy measure suitable for traditional classification. Further, we mainly focus on two points: exploring a tighter generalization bound on pure accuracy based learning paradigm and designing a learning algorithm based on the pure accuracy measure. Particularly, with the self-bounding property, we build an algorithm-independent generalization bound on the pure accuracy measure, which is tighter than the existing bound of an order $O(1/\sqrt{N})$ (N is the number of instances). The proposed bound is free from making a smoothness or convex assumption on the hypothesis functions. In addition, we design a learning algorithm optimizing the pure accuracy measure and use it in the selective ensemble learning setting. The experiments on sixteen benchmark data sets and four image data sets demonstrate that the proposed method statistically performs better than the other eight representative benchmark algorithms.},
  archive      = {J_TPAMI},
  author       = {Jieting Wang and Yuhua Qian and Feijiang Li and Jiye Liang and Qingfu Zhang},
  doi          = {10.1109/TPAMI.2022.3171436},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1798-1816},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalization performance of pure accuracy and its application in selective ensemble learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). From human pose similarity metric to 3D human pose
estimator: Temporal propagating LSTM networks. <em>TPAMI</em>,
<em>45</em>(2), 1781–1797. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting a 3D pose directly from a monocular image is a challenging problem. Most pose estimation methods proposed in recent years have shown ‘quantitatively’ good results (below $\sim$ 50 mm ). However, these methods remain ‘perceptually’ flawed because their performance is only measured via a simple distance metric. Although this fact is well understood, the reliance on ‘quantitative’ information implies that the development of 3D pose estimation methods has been slowed down. To address this issue, we first propose a perceptual Pose SIMilarity (PSIM) metric, by assuming that human perception (HP) is highly adapted to extracting structural information from a given signal. Second, we present a perceptually robust 3D pose estimation framework: Temporal Propagating Long Short-Term Memory networks (TP-LSTMs). Toward this, we analyze the information-theory-based spatio-temporal posture correlations, including joint interdependency, temporal consistency, and HP. The experimental results clearly show that the proposed PSIM metric achieves a superior correlation with users’ subjective opinions than conventional pose metrics. Furthermore, we demonstrate the significant quantitative and perceptual performance improvements of TP-LSTMs compared to existing state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Kyoungoh Lee and Woojae Kim and Sanghoon Lee},
  doi          = {10.1109/TPAMI.2022.3164344},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1781-1797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From human pose similarity metric to 3D human pose estimator: Temporal propagating LSTM networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From big to small: Adaptive learning to partial-set domains.
<em>TPAMI</em>, <em>45</em>(2), 1766–1780. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation targets at knowledge acquisition and dissemination from a labeled source domain to an unlabeled target domain under distribution shift. Still, the common requirement of identical class space shared across domains hinders applications of domain adaptation to partial-set domains. Recent advances show that deep pre-trained models of large scale endow rich knowledge to tackle diverse downstream tasks of small scale. Thus, there is a strong incentive to adapt models from large-scale domains to small-scale domains. This paper introduces Partial Domain Adaptation (PDA), a learning paradigm that relaxes the identical class space assumption to that the source class space subsumes the target class space. First, we present a theoretical analysis of partial domain adaptation, which uncovers the importance of estimating the transferable probability of each class and each instance across domains. Then, we propose Selective Adversarial Network (SAN and SAN++) with a bi-level selection strategy and an adversarial adaptation mechanism. The bi-level selection strategy up-weighs each class and each instance simultaneously for source supervised training, target self-training, and source-target adversarial adaptation through the transferable probability estimated alternately by the model. Experiments on standard partial-set datasets and more challenging tasks with superclasses show that SAN++ outperforms several domain adaptation methods.},
  archive      = {J_TPAMI},
  author       = {Zhangjie Cao and Kaichao You and Ziyang Zhang and Jianmin Wang and Mingsheng Long},
  doi          = {10.1109/TPAMI.2022.3159831},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1766-1780},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From big to small: Adaptive learning to partial-set domains},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring structural sparsity of deep networks via inverse
scale spaces. <em>TPAMI</em>, <em>45</em>(2), 1749–1765. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great success of deep neural networks is built upon their over-parameterization, which smooths the optimization landscape without degrading the generalization ability. Despite the benefits of over-parameterization, a huge amount of parameters makes deep networks cumbersome in daily life applications. On the other hand, training neural networks without over-parameterization faces many practical problems, e.g., being trapped in the local optimal. Though techniques such as pruning and distillation are developed, they are expensive in fully training a dense network as backward selection methods; and there is still a void on systematically exploring forward selection methods for learning structural sparsity in deep networks. To fill in this gap, this paper proposes a new approach based on differential inclusions of inverse scale spaces. Specifically, our method can generate a family of models from simple to complex ones along the dynamics via coupling a pair of parameters, such that over-parameterized deep models and their structural sparsity can be explored simultaneously. This kind of differential inclusion scheme has a simple discretization, dubbed Deep structure splitting Linearized Bregman Iteration ( DessiLBI ), whose global convergence in learning deep networks could be established under the Kurdyka-Łojasiewicz framework. Particularly, we explore several applications of DessiLBI, including finding sparse structures of networks directly via the coupled structure parameter and growing networks from simple to complex ones progressively. Experimental evidence shows that our method achieves comparable and even better performance than the competitive optimizers in exploring the sparse structure of several widely used backbones on the benchmark datasets. Remarkably, with early stopping, our method unveils “winning tickets” in early epochs: the effective sparse network structures with comparable test accuracy to fully trained over-parameterized models, that are further transferable to similar alternative tasks. Furthermore, our method is able to grow networks efficiently with adaptive filter configurations, demonstrating the good performance with much less computational cost. Codes and models can be downloaded at https://github.com/DessiLBI2020/DessiLBI .},
  archive      = {J_TPAMI},
  author       = {Yanwei Fu and Chen Liu and Donghao Li and Zuyuan Zhong and Xinwei Sun and Jinshan Zeng and Yuan Yao},
  doi          = {10.1109/TPAMI.2022.3168881},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1749-1765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploring structural sparsity of deep networks via inverse scale spaces},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation for weakly supervised object localization:
Protocol, metrics, and datasets. <em>TPAMI</em>, <em>45</em>(2),
1732–1748. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised object localization (WSOL) has gained popularity over the last years for its promise to train localization models with only image-level labels. Since the seminal WSOL work of class activation mapping (CAM), the field has focused on how to expand the attention regions to cover objects more broadly and localize them better. However, these strategies rely on full localization supervision for validating hyperparameters and model selection, which is in principle prohibited under the WSOL setup. In this paper, we argue that WSOL task is ill-posed with only image-level labels, and propose a new evaluation protocol where full supervision is limited to only a small held-out set not overlapping with the test set. We observe that, under our protocol, the five most recent WSOL methods have not made a major improvement over the CAM baseline. Moreover, we report that existing WSOL methods have not reached the few-shot learning baseline, where the full-supervision at validation time is used for model training instead. Based on our findings, we discuss some future directions for WSOL. Source code and dataset are available at https://github.com/clovaai/wsolevaluation https://github.com/clovaai/wsolevaluation .},
  archive      = {J_TPAMI},
  author       = {Junsuk Choe and Seong Joon Oh and Sanghyuk Chun and Seungho Lee and Zeynep Akata and Hyunjung Shim},
  doi          = {10.1109/TPAMI.2022.3169881},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1732-1748},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evaluation for weakly supervised object localization: Protocol, metrics, and datasets},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equivariant wavelets: Fast rotation and translation
invariant wavelet scattering transforms. <em>TPAMI</em>, <em>45</em>(2),
1716–1731. (<a
href="https://doi.org/10.1109/TPAMI.2022.3165730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet scattering networks, which are convolutional neural networks (CNNs) with fixed filters and weights, are promising tools for image analysis. Imposing symmetry on image statistics can improve human interpretability, aid in generalization, and provide dimension reduction. In this work, we introduce a fast-to-compute, translationally invariant and rotationally equivariant wavelet scattering network (EqWS) and filter bank of wavelets (triglets). We demonstrate the interpretability and quantify the invariance/equivariance of the coefficients, briefly commenting on difficulties with implementing scale equivariance. On MNIST, we show that training on a rotationally invariant reduction of the coefficients maintains rotational invariance when generalized to test data and visualize residual symmetry breaking terms. Rotation equivariance is leveraged to estimate the rotation angle of digits and reconstruct the full rotation dependence of each coefficient from a single angle. We benchmark EqWS with linear classifiers on EMNIST and CIFAR-10/100, introducing a new second-order, cross-color channel coupling for the color images. We conclude by comparing the performance of an isotropic reduction of the scattering coefficients and RWST, a previous coefficient reduction, on an isotropic classification of magnetohydrodynamic simulations with astrophysical relevance.},
  archive      = {J_TPAMI},
  author       = {Andrew K. Saydjari and Douglas P. Finkbeiner},
  doi          = {10.1109/TPAMI.2022.3165730},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1716-1731},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Equivariant wavelets: Fast rotation and translation invariant wavelet scattering transforms},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing photorealism enhancement. <em>TPAMI</em>,
<em>45</em>(2), 1700–1715. (<a
href="https://doi.org/10.1109/TPAMI.2022.3166687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.},
  archive      = {J_TPAMI},
  author       = {Stephan R. Richter and Hassan Abu Alhaija and Vladlen Koltun},
  doi          = {10.1109/TPAMI.2022.3166687},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1700-1715},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhancing photorealism enhancement},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotional attention: From eye tracking to computational
modeling. <em>TPAMI</em>, <em>45</em>(2), 1682–1699. (<a
href="https://doi.org/10.1109/TPAMI.2022.3169234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attending selectively to emotion-eliciting stimuli is intrinsic to human vision. In this research, we investigate how emotion-elicitation features of images relate to human selective attention. We create the EMOtional attention dataset (EMOd). It is a set of diverse emotion-eliciting images, each with (1) eye-tracking data from 16 subjects, (2) image context labels at both object- and scene-level. Based on analyses of human perceptions of EMOd, we report an emotion prioritization effect: emotion-eliciting content draws stronger and earlier human attention than neutral content, but this advantage diminishes dramatically after initial fixation. We find that human attention is more focused on awe eliciting and aesthetic vehicle and animal scenes in EMOd. Aiming to model the above human attention behavior computationally, we design a deep neural network (CASNet II), which includes a channel weighting subnetwork that prioritizes emotion-eliciting objects, and an Atrous Spatial Pyramid Pooling (ASPP) structure that learns the relative importance of image regions at multiple scales. Visualizations and quantitative analyses demonstrate the model’s ability to simulate human attention behavior, especially on emotion-eliciting content.},
  archive      = {J_TPAMI},
  author       = {Shaojing Fan and Zhiqi Shen and Ming Jiang and Bryan L. Koenig and Mohan S. Kankanhalli and Qi Zhao},
  doi          = {10.1109/TPAMI.2022.3169234},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1682-1699},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Emotional attention: From eye tracking to computational modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic differential image circle diameter measurement
precision assessment: Application to burning droplets. <em>TPAMI</em>,
<em>45</em>(2), 1668–1681. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic measurement precision assessment has been achieved for a differential circle measurement application. Differential circle diameter measurement, in image analysis, typically requires fitting a circle model that optimizes for image distortions, defects or occlusions. The differential task occurs when precise measurements of diameter change are required given object size variation with time. An automated system was designed to provide diameter measurements and associated measurement precision of images of a fuel droplet undergoing combustion in zero gravity for the FLEX-2 dataset. An image gradient-based, least-squares boundary point fitting method to a circle or ellipse model is used for diameter measurement. The presence of soot aggregates poses significant challenges for diameter measurements when it occludes part of the droplet boundary. The precision of the diameter measurements depends upon the image quality. Using synthetic image simulations that model the soot behavior, we developed a model based on image quality measures that assesses the measurement precision for each individual diameter measurement. Thus, diameter measurements with precision assessments were made available for follow-up scientific analysis. The algorithm&#39;s success rate for measurable runs was 98\%. In cases of limited occlusion, a measurement precision of ±0.2 pixels for the FLEX-2 dataset was achieved.},
  archive      = {J_TPAMI},
  author       = {Raisa B. Rasul and C. Thomas Avedisian and Yuhao Xu and Michael C. Hicks and Anthony P. Reeves},
  doi          = {10.1109/TPAMI.2022.3170926},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1668-1681},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic differential image circle diameter measurement precision assessment: Application to burning droplets},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Duality-induced regularizer for semantic matching knowledge
graph embeddings. <em>TPAMI</em>, <em>45</em>(2), 1652–1667. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic matching models—which assume that entities with similar semantics have similar embeddings—have shown great power in knowledge graph embeddings (KGE). Many existing semantic matching models use inner products in embedding spaces to measure the plausibility of triples and quadruples in static and temporal knowledge graphs. However, vectors that have the same inner products with another vector can still be orthogonal to each other, which implies that entities with similar semantics may have dissimilar embeddings. This property of inner products significantly limits the performance of semantic matching models. To address this challenge, we propose a novel regularizer—namely, DU ality-induced R egul A rizer (DURA)—which effectively encourages the entities with similar semantics to have similar embeddings. The major novelty of DURA is based on the observation that, for an existing semantic matching KGE model ( primal ), there is often another distance based KGE model ( dual ) closely associated with it, which can be used as effective constraints for entity embeddings. Experiments demonstrate that DURA consistently and significantly improves the performance of state-of-the-art semantic matching models on both static and temporal knowledge graph benchmarks.},
  archive      = {J_TPAMI},
  author       = {Jie Wang and Zhanqiu Zhang and Zhihao Shi and Jianyu Cai and Shuiwang Ji and Feng Wu},
  doi          = {10.1109/TPAMI.2022.3161804},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1652-1667},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Duality-induced regularizer for semantic matching knowledge graph embeddings},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual networks based 3D multi-person pose estimation from
monocular video. <em>TPAMI</em>, <em>45</em>(2), 1636–1651. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D human pose estimation has made progress in recent years. Most of the methods focus on single persons, which estimate the poses in the person-centric coordinates, i.e., the coordinates based on the center of the target person. Hence, these methods are inapplicable for multi-person 3D pose estimation, where the absolute coordinates (e.g., the camera coordinates) are required. Moreover, multi-person pose estimation is more challenging than single pose estimation, due to inter-person occlusion and close human interactions. Existing top-down multi-person methods rely on human detection (i.e., top-down approach), and thus suffer from the detection errors and cannot produce reliable pose estimation in multi-person scenes. Meanwhile, existing bottom-up methods that do not use human detection are not affected by detection errors, but since they process all persons in a scene at once, they are prone to errors, particularly for persons in small scales. To address all these challenges, we propose the integration of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottom-up network incorporates human-detection based normalized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for final 3D poses. To address the common gaps between training and testing data, we do optimization during the test time, by refining the estimated 3D human poses using high-order temporal constraint, re-projection loss, and bone length regularizations. We also introduce a two-person pose discriminator that enforces natural two-person interactions. Finally, we apply a semi-supervised method to overcome the 3D ground-truth data scarcity. Our evaluations demonstrate the effectiveness of the proposed method and its individual components. Our code and pretrained models are available publicly: https://github.com/3dpose/3D-Multi-Person-Pose .},
  archive      = {J_TPAMI},
  author       = {Yu Cheng and Bo Wang and Robby T. Tan},
  doi          = {10.1109/TPAMI.2022.3170353},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1636-1651},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual networks based 3D multi-person pose estimation from monocular video},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminant feature extraction by generalized difference
subspace. <em>TPAMI</em>, <em>45</em>(2), 1618–1635. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we reveal the discriminant capacity of orthogonal data projection onto the generalized difference subspace (GDS), both theoretically and experimentally. In our previous work, we demonstrated that the GDS projection works as a quasi-orthogonalization of class subspaces, which is an effective feature extraction for subspace based classifiers. Here, we further show that GDS projection also works as a discriminant feature extraction through a similar mechanism to the Fisher discriminant analysis (FDA). A direct proof of the connection between GDS projection and FDA is difficult due to the significant difference in their formulations. To circumvent the complication, we first introduce geometrical Fisher discriminant analysis (gFDA) based on a simplified Fisher criterion. It is derived from a heuristic yet practically plausible assumption: the direction of the sample mean vector of a class is largely aligned to the first principal component vector of the class, given that the principal component analysis (PCA) is applied without data centering. gFDA works stably even under few samples, bypassing the small sample size (SSS) problem of FDA. We then prove that gFDA is equivalent to GDS projection with a small correction term. This equivalence ensures GDS projection to inherit the discriminant ability from FDA via gFDA. Furthermore, we discuss two useful extensions of these methods, 1) a nonlinear extension by kernel trick, 2) a combination with CNN features. The equivalence and the effectiveness of the extensions have been verified through extensive experiments on the extended Yale B+, CMU face database, ALOI, ETH80, MNIST, and CIFAR10, mainly focusing on image recognition under small samples.},
  archive      = {J_TPAMI},
  author       = {Kazuhiro Fukui and Naoya Sogi and Takumi Kobayashi and Jing-Hao Xue and Atsuto Maki},
  doi          = {10.1109/TPAMI.2022.3168557},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1618-1635},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discriminant feature extraction by generalized difference subspace},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable graph module (DGM) for graph convolutional
networks. <em>TPAMI</em>, <em>45</em>(2), 1606–1617. (<a
href="https://doi.org/10.1109/TPAMI.2022.3170249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed . Often, this assumption is not true since the graph may be noisy, or partially and even completely unknown. In such cases, it would be helpful to infer the graph directly from the data, especially in inductive settings where some nodes were not present in the graph at training time. Furthermore, learning a graph may become an end in itself, as the inferred structure may provide complementary insights next to the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function that predicts edge probabilities in the graph which are optimal for the downstream task. DGM can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.},
  archive      = {J_TPAMI},
  author       = {Anees Kazi and Luca Cosmo and Seyed-Ahmad Ahmadi and Nassir Navab and Michael M. Bronstein},
  doi          = {10.1109/TPAMI.2022.3170249},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1606-1617},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Differentiable graph module (DGM) for graph convolutional networks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnosing and preventing instabilities in recurrent video
processing. <em>TPAMI</em>, <em>45</em>(2), 1594–1605. (<a
href="https://doi.org/10.1109/TPAMI.2022.3160350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent models are a popular choice for video enhancement tasks such as video denoising or super-resolution. In this work, we focus on their stability as dynamical systems and show that they tend to fail catastrophically at inference time on long video sequences. To address this issue, we (1) introduce a diagnostic tool which produces input sequences optimized to trigger instabilities and that can be interpreted as visualizations of temporal receptive fields, and (2) propose two approaches to enforce the stability of a model during training: constraining the spectral norm or constraining the stable rank of its convolutional layers. We then introduce Stable Rank Normalization for Convolutional layers (SRN-C), a new algorithm that enforces these constraints. Our experimental results suggest that SRN-C successfully enforces stablility in recurrent video processing models without a significant performance loss.},
  archive      = {J_TPAMI},
  author       = {Thomas Tanay and Aivar Sootla and Matteo Maggioni and Puneet K. Dokania and Philip Torr and Aleš Leonardis and Gregory Slabaugh},
  doi          = {10.1109/TPAMI.2022.3160350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1594-1605},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Diagnosing and preventing instabilities in recurrent video processing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCloth: Neural garment representation for shape and style
editing. <em>TPAMI</em>, <em>45</em>(2), 1581–1593. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Garment representation, editing and animation are challenging topics in the area of computer vision and graphics. It remains difficult for existing garment representations to achieve smooth and plausible transitions between different shapes and topologies. In this work, we introduce, DeepCloth, a unified framework for garment representation, reconstruction, animation and editing. Our unified framework contains 3 components: First, we represent the garment geometry with a “topology-aware UV-position map”, which allows for the unified description of various garments with different shapes and topologies by introducing an additional topology-aware UV-mask for the UV-position map. Second, to further enable garment reconstruction and editing, we contribute a method to embed the UV-based representations into a continuous feature space, which enables garment shape reconstruction and editing by optimization and control in the latent space, respectively. Finally, we propose a garment animation method by unifying our neural garment representation with body shape and pose, which achieves plausible garment animation results leveraging the dynamic information encoded by our shape and style representation, even under drastic garment editing operations. To conclude, with DeepCloth, we move a step forward in establishing a more flexible and general 3D garment digitization framework. Experiments demonstrate that our method can achieve state-of-the-art garment representation performance compared with previous methods.},
  archive      = {J_TPAMI},
  author       = {Zhaoqi Su and Tao Yu and Yangang Wang and Yebin Liu},
  doi          = {10.1109/TPAMI.2022.3168569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1581-1593},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepCloth: Neural garment representation for shape and style editing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dataset security for machine learning: Data poisoning,
backdoor attacks, and defenses. <em>TPAMI</em>, <em>45</em>(2),
1563–1580. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space.},
  archive      = {J_TPAMI},
  author       = {Micah Goldblum and Dimitris Tsipras and Chulin Xie and Xinyun Chen and Avi Schwarzschild and Dawn Song and Aleksander Mądry and Bo Li and Tom Goldstein},
  doi          = {10.1109/TPAMI.2022.3162397},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1563-1580},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curvature-adaptive meta-learning for fast adaptation to
manifold data. <em>TPAMI</em>, <em>45</em>(2), 1545–1562. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning methods are shown to be effective in quickly adapting a model to novel tasks. Most existing meta-learning methods represent data and carry out fast adaptation in euclidean space. In fact, data of real-world applications usually resides in complex and various Riemannian manifolds. In this paper, we propose a curvature-adaptive meta-learning method that achieves fast adaptation to manifold data by producing suitable curvature. Specifically, we represent data in the product manifold of multiple constant curvature spaces and build a product manifold neural network as the base-learner. In this way, our method is capable of encoding complex manifold data into discriminative and generic representations. Then, we introduce curvature generation and curvature updating schemes, through which suitable product manifolds for various forms of data manifolds are constructed via few optimization steps. The curvature generation scheme identifies task-specific curvature initialization, leading to a shorter optimization trajectory. The curvature updating scheme automatically produces appropriate learning rate and search direction for curvature, making a faster and more adaptive optimization paradigm compared to hand-designed optimization schemes. We evaluate our method on a broad set of problems including few-shot classification, few-shot regression, and reinforcement learning tasks. Experimental results show that our method achieves substantial improvements as compared to meta-learning methods ignoring the geometry of the underlying space.},
  archive      = {J_TPAMI},
  author       = {Zhi Gao and Yuwei Wu and Mehrtash Harandi and Yunde Jia},
  doi          = {10.1109/TPAMI.2022.3164894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1545-1562},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Curvature-adaptive meta-learning for fast adaptation to manifold data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal learning for domain adaptation in 3D semantic
segmentation. <em>TPAMI</em>, <em>45</em>(2), 1533–1544. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is an important task to enable learning when labels are scarce. While most works focus only on the image modality, there are many important multi-modal datasets. In order to leverage multi-modality for domain adaptation, we propose cross-modal learning, where we enforce consistency between the predictions of two modalities via mutual mimicking. We constrain our network to make correct predictions on labeled data and consistent predictions across modalities on unlabeled target-domain data. Experiments in unsupervised and semi-supervised domain adaptation settings prove the effectiveness of this novel domain adaptation strategy. Specifically, we evaluate on the task of 3D semantic segmentation from either the 2D image, the 3D point cloud or from both. We leverage recent driving datasets to produce a wide variety of domain adaptation scenarios including changes in scene layout, lighting, sensor setup and weather, as well as the synthetic-to-real setup. Our method significantly improves over previous uni-modal adaptation baselines on all adaption scenarios. Code will be made available upon publication.},
  archive      = {J_TPAMI},
  author       = {Maximilian Jaritz and Tuan-Hung Vu and Raoul de Charette and Émilie Wirbel and Patrick Pérez},
  doi          = {10.1109/TPAMI.2022.3159589},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1533-1544},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-modal learning for domain adaptation in 3D semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controllable image synthesis with attribute-decomposed GAN.
<em>TPAMI</em>, <em>45</em>(2), 1514–1532. (<a
href="https://doi.org/10.1109/TPAMI.2022.3161985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes Attribute-Decomposed GAN (ADGAN) and its enhanced version (ADGAN++) for controllable image synthesis, which can produce realistic images with desired attributes provided in various source inputs. The core ideas of the proposed ADGAN and ADGAN++ are both to embed component attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. The major difference between them is that ADGAN processes all component attributes simultaneously while ADGAN++ utilizes a serial encoding strategy. More specifically, ADGAN consists of two encoding pathways with style block connections and is capable of decomposing the original hard mapping into multiple more accessible subtasks. In the source pathway, component layouts are extracted via a semantic parser and the segmented components are fed into a shared global texture encoder to obtain decomposed latent codes. This strategy allows for the synthesis of more realistic output images and the automatic separation of un-annotated component attributes. Although the original ADGAN works in a delicate and efficient manner, intrinsically it fails to handle the semantic image synthesizing task when the number of attribute categories is huge. To address this problem, ADGAN++ employs the serial encoding of different component attributes to synthesize each part of the target real-world image, and adopts several residual blocks with segmentation guided instance normalization to assemble the synthesized component images and refine the original synthesis result. The two-stage ADGAN++ is designed to alleviate the massive computational costs required when synthesizing real-world images with numerous attributes while maintaining the disentanglement of different attributes to enable flexible control of arbitrary component attributes of the synthesized images. Experimental results demonstrate the proposed methods’ superiority over the state of the art in pose transfer, face style transfer, and semantic image synthesis, as well as their effectiveness in the task of component attribute transfer. Our code and data are publicly available at https://github.com/menyifang/ADGAN .},
  archive      = {J_TPAMI},
  author       = {Guo Pu and Yifang Men and Yiming Mao and Yuning Jiang and Wei-Ying Ma and Zhouhui Lian},
  doi          = {10.1109/TPAMI.2022.3161985},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1514-1532},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Controllable image synthesis with attribute-decomposed GAN},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contingency space: A semimetric space for classification
evaluation. <em>TPAMI</em>, <em>45</em>(2), 1501–1513. (<a
href="https://doi.org/10.1109/TPAMI.2022.3167007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Machine Learning, a supervised model’s performance is measured using the evaluation metrics. In this study, we first present our motivation by revisiting the major limitations of these metrics, namely one-dimensionality, lack of context, lack of intuitiveness, uncomparability, binary restriction, and uncustomizability of metrics. In response, we propose Contingency Space, a bounded semimetric space that provides a generic representation for any performance evaluation metric. Then we showcase how this space addresses the limitations. In this space, each metric forms a surface using which we visually compare different evaluation metrics. Taking advantage of the fact that a metric’s surface warps proportionally to the degree of which it is sensitive to the class-imbalance ratio of data, we introduce Imbalance Sensitivity that quantifies the skew-sensitivity. Since an arbitrary model is represented in this space by a single point, we introduce Learning Path for qualitative and quantitative analyses of the training process. Using the semimetric that contingency space is endowed with, we introduce Tau as a new cost sensitive and Imbalance Agnostic metric. Lastly, we show that contingency space addresses multi-class problems as well. Throughout this work, we define each concept through stipulated definitions and present every application with practical examples and visualizations.},
  archive      = {J_TPAMI},
  author       = {Azim Ahmadzadeh and Dustin J. Kempton and Petrus C. Martens and Rafal A. Angryk},
  doi          = {10.1109/TPAMI.2022.3167007},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1501-1513},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contingency space: A semimetric space for classification evaluation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual transformer networks for visual recognition.
<em>TPAMI</em>, <em>45</em>(2), 1489–1500. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer with self-attention has led to the revolutionizing of natural language processing field, and recently inspires the emergence of Transformer-style architecture design with competitive results in numerous computer vision tasks. Nevertheless, most of existing designs directly employ self-attention over a 2D feature map to obtain the attention matrix based on pairs of isolated queries and keys at each spatial location, but leave the rich contexts among neighbor keys under-exploited. In this work, we design a novel Transformer-style module, i.e., Contextual Transformer ( CoT ) block, for visual recognition. Such design fully capitalizes on the contextual information among input keys to guide the learning of dynamic attention matrix and thus strengthens the capacity of visual representation. Technically, CoT block first contextually encodes input keys via a $3\times 3$ convolution, leading to a static contextual representation of inputs. We further concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive $1\times 1$ convolutions. The learnt attention matrix is multiplied by input values to achieve the dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations are finally taken as outputs. Our CoT block is appealing in the view that it can readily replace each $3\times 3$ convolution in ResNet architectures, yielding a Transformer-style backbone named as Contextual Transformer Networks ( CoTNet ). Through extensive experiments over a wide range of applications (e.g., image recognition, object detection, instance segmentation, and semantic segmentation), we validate the superiority of CoTNet as a stronger backbone. Source code is available at https://github.com/JDAI-CV/CoTNet .},
  archive      = {J_TPAMI},
  author       = {Yehao Li and Ting Yao and Yingwei Pan and Tao Mei},
  doi          = {10.1109/TPAMI.2022.3164083},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1489-1500},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contextual transformer networks for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing stronger and faster baselines for
skeleton-based action recognition. <em>TPAMI</em>, <em>45</em>(2),
1474–1488. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly sophisticated and over-parameterized. The low efficiency in model training and inference has increased the validation costs of model architectures in large-scale datasets. To address the above issue, recent advanced separable convolutional layers are embedded into an early fused Multiple Input Branches (MIB) network, constructing an efficient Graph Convolutional Network (GCN) baseline for skeleton-based action recognition. In addition, based on such the baseline, we design a compound scaling strategy to expand the model’s width and depth synchronously, and eventually obtain a family of efficient GCN baselines with high accuracies and small amounts of trainable parameters, termed EfficientGCN-Bx, where “x” denotes the scaling coefficient. On two large-scale datasets, i.e. , NTU RGB+D 60 and 120, the proposed EfficientGCN-B4 baseline outperforms other SOTA methods, e.g. , achieving 92.1\% accuracy on the cross-subject benchmark of NTU 60 dataset, while being 5.82× smaller and 5.85× faster than MS-G3D, which is one of the SOTA methods. The source code in PyTorch version and the pretrained models are available at https://github.com/yfsong0709/EfficientGCNv1 .},
  archive      = {J_TPAMI},
  author       = {Yi-Fan Song and Zhang Zhang and Caifeng Shan and Liang Wang},
  doi          = {10.1109/TPAMI.2022.3157033},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1474-1488},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Constructing stronger and faster baselines for skeleton-based action recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Bilinear scoring function search for knowledge graph
learning. <em>TPAMI</em>, <em>45</em>(2), 1458–1473. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning embeddings for entities and relations in knowledge graph (KG) have benefited many downstream tasks. In recent years, scoring functions, the crux of KG learning, have been human designed to measure the plausibility of triples and capture different kinds of relations in KGs. However, as relations exhibit intricate patterns that are hard to infer before training, none of them consistently perform the best on benchmark tasks. In this paper, inspired by the recent success of automated machine learning (AutoML), we search bilinear scoring functions for different KG tasks through the AutoML techniques. However, it is non-trivial to explore domain-specific information here. We first set up a search space for AutoBLM by analyzing existing scoring functions. Then, we propose a progressive algorithm (AutoBLM) and an evolutionary algorithm (AutoBLM+), which are further accelerated by filter and predictor to deal with the domain-specific properties for KG learning. Finally, we perform extensive experiments on benchmarks in KG completion, multi-hop query, and entity classification tasks. Empirical results show that the searched scoring functions are KG dependent, new to the literature, and outperform the existing scoring functions. AutoBLM+ is better than AutoBLM as the evolutionary algorithm can flexibly explore better structures in the same budget.},
  archive      = {J_TPAMI},
  author       = {Yongqi Zhang and Quanming Yao and James T. Kwok},
  doi          = {10.1109/TPAMI.2022.3157321},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1458-1473},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bilinear scoring function search for knowledge graph learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond 3DMM: Learning to capture high-fidelity 3D face
shape. <em>TPAMI</em>, <em>45</em>(2), 1442–1457. (<a
href="https://doi.org/10.1109/TPAMI.2022.3164131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Morphable Model (3DMM) fitting has widely benefited face analysis due to its strong 3D priori. However, previous reconstructed 3D faces suffer from degraded visual verisimilitude due to the loss of fine-grained geometry, which is attributed to insufficient ground-truth 3D shapes, unreliable training strategies and limited representation power of 3DMM. To alleviate this issue, this paper proposes a complete solution to capture the personalized shape so that the reconstructed shape looks identical to the corresponding person. Specifically, given a 2D image as the input, we virtually render the image in several calibrated views to normalize pose variations while preserving the original image geometry. A many-to-one hourglass network serves as the encode-decoder to fuse multiview features and generate vertex displacements as the fine-grained geometry. Besides, the neural network is trained by directly optimizing the visual effect, where two 3D shapes are compared by measuring the similarity between the multiview images rendered from the shapes. Finally, we propose to generate the ground-truth 3D shapes by registering RGB-D images followed by pose and shape augmentation, providing sufficient data for network training. Experiments on several challenging protocols demonstrate the superior reconstruction accuracy of our proposal on the face shape.},
  archive      = {J_TPAMI},
  author       = {Xiangyu Zhu and Chang Yu and Di Huang and Zhen Lei and Hao Wang and Stan Z. Li},
  doi          = {10.1109/TPAMI.2022.3164131},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1442-1457},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Beyond 3DMM: Learning to capture high-fidelity 3D face shape},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking single-image reflection removal algorithms.
<em>TPAMI</em>, <em>45</em>(2), 1424–1441. (<a
href="https://doi.org/10.1109/TPAMI.2022.3168560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflection removal has been discussed for more than decades. This paper aims to provide the analysis for different reflection properties and factors that influence image formation, an up-to-date taxonomy for existing methods, a benchmark dataset, and the unified benchmarking evaluations for state-of-the-art (especially learning-based) methods. Specifically, this paper presents a SIngle-image Reflection Removal Plus dataset “SIR $^{2+}$ ” with the new consideration for in-the-wild scenarios and glass with diverse color and unplanar shapes. We further perform quantitative and visual quality comparisons for state-of-the-art single-image reflection removal algorithms. Open problems for improving reflection removal algorithms are discussed at the end. Our dataset and follow-up update can be found at https://reflectionremoval.github.io/sir2data/ .},
  archive      = {J_TPAMI},
  author       = {Renjie Wan and Boxin Shi and Haoliang Li and Yuchen Hong and Ling-Yu Duan and Alex C. Kot},
  doi          = {10.1109/TPAMI.2022.3168560},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1424-1441},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Benchmarking single-image reflection removal algorithms},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian image super-resolution with deep modeling of image
statistics. <em>TPAMI</em>, <em>45</em>(2), 1405–1423. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. In this work, we propose a Bayesian image restoration framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. Concretely, first we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. Then, we develop a variational Bayesian approach to infer their posteriors. Finally, we implement the variational approach for single image super-resolution (SISR) using deep neural networks, and propose an unsupervised training strategy. The experiments on three image restoration tasks, i.e., ideal SISR, realistic SISR, and real-world SISR, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised SISR. The code and resulting models are released via https://zmiclab.github.io/projects.html .},
  archive      = {J_TPAMI},
  author       = {Shangqi Gao and Xiahai Zhuang},
  doi          = {10.1109/TPAMI.2022.3163307},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1405-1423},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bayesian image super-resolution with deep modeling of image statistics},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial examples generation for deep product
quantization networks on image retrieval. <em>TPAMI</em>,
<em>45</em>(2), 1388–1404. (<a
href="https://doi.org/10.1109/TPAMI.2022.3165024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep product quantization networks (DPQNs) have been successfully used in image retrieval tasks, due to their powerful feature extraction ability and high efficiency of encoding high-dimensional visual features. Recent studies show that deep neural networks (DNNs) are vulnerable to input with small and maliciously designed perturbations (a.k.a., adversarial examples) for classification. However, little effort has been devoted to investigating how adversarial examples affect DPQNs, which raises the potential safety hazard when deploying DPQNs in a commercial search engine. To this end, we propose an adversarial example generation framework by generating adversarial query images for DPQN-based retrieval systems. Unlike the adversarial generation for the classic image classification task that heavily relies on ground-truth labels, we alternatively perturb the probability distribution of centroids assignments for a clean query, then we can induce effective non-targeted attacks on DPQNs in white-box and black-box settings. Moreover, we further extend the non-targeted attack to a targeted attack by a novel sample space averaging scheme ( $\text{S}^{2}$ AS), whose theoretical guarantee is also obtained. Extensive experiments show that our methods can create adversarial examples to successfully mislead the target DPQNs. Besides, we found that our methods both significantly degrade the retrieval performance under a wide variety of experimental settings. The source code is available at https://github.com/Kira0096/PQAG .},
  archive      = {J_TPAMI},
  author       = {Bin Chen and Yan Feng and Tao Dai and Jiawang Bai and Yong Jiang and Shu-Tao Xia and Xuan Wang},
  doi          = {10.1109/TPAMI.2022.3165024},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1388-1404},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial examples generation for deep product quantization networks on image retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive perspective distillation for semantic segmentation.
<em>TPAMI</em>, <em>45</em>(2), 1372–1387. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong semantic segmentation models require large backbones to achieve promising performance, making it hard to adapt to real applications where effective real-time algorithms are needed. Knowledge distillation tackles this issue by letting the smaller model (student) produce similar pixel-wise predictions to that of a larger model (teacher). However, the classifier, which can be deemed as the perspective by which models perceive the encoded features for yielding observations (i.e., predictions), is shared by all training samples, fitting a universal feature distribution. Since good generalization to the entire distribution may bring the inferior specification to individual samples with a certain capacity, the shared universal perspective often overlooks details existing in each sample, causing degradation of knowledge distillation. In this paper, we propose Adaptive Perspective Distillation (APD) that creates an adaptive local perspective for each individual training sample. It extracts detailed contextual information from each training sample specifically, mining more details from the teacher and thus achieving better knowledge distillation results on the student. APD has no structural constraints to both teacher and student models, thus generalizing well to different semantic segmentation models. Extensive experiments on Cityscapes, ADE20K, and PASCAL-Context manifest the effectiveness of our proposed APD. Besides, APD can yield favorable performance gain to the models in both object detection and instance segmentation without bells and whistles.},
  archive      = {J_TPAMI},
  author       = {Zhuotao Tian and Pengguang Chen and Xin Lai and Li Jiang and Shu Liu and Hengshuang Zhao and Bei Yu and Ming-Chang Yang and Jiaya Jia},
  doi          = {10.1109/TPAMI.2022.3159581},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1372-1387},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive perspective distillation for semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of the gumbel-max trick and its extensions for
discrete stochasticity in machine learning. <em>TPAMI</em>,
<em>45</em>(2), 1353–1371. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gumbel-max trick is a method to draw a sample from a categorical distribution, given by its unnormalized (log-)probabilities. Over the past years, the machine learning community has proposed several extensions of this trick to facilitate, e.g., drawing multiple samples, sampling from structured domains, or gradient estimation for error backpropagation in neural network optimization. The goal of this survey article is to present background about the Gumbel-max trick, and to provide a structured overview of its extensions to ease algorithm selection. Moreover, it presents a comprehensive outline of (machine learning) literature in which Gumbel-based algorithms have been leveraged, reviews commonly-made design choices, and sketches a future perspective.},
  archive      = {J_TPAMI},
  author       = {Iris A. M. Huijben and Wouter Kool and Max B. Paulus and Ruud J. G. van Sloun},
  doi          = {10.1109/TPAMI.2022.3157042},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1353-1371},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A review of the gumbel-max trick and its extensions for discrete stochasticity in machine learning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 4D atlas: Statistical analysis of the spatiotemporal
variability in longitudinal 3D shape data. <em>TPAMI</em>,
<em>45</em>(2), 1335–1352. (<a
href="https://doi.org/10.1109/TPAMI.2022.3163720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework to learn the spatiotemporal variability in longitudinal 3D shape data sets, which contain observations of objects that evolve and deform over time. This problem is challenging since surfaces come with arbitrary parameterizations and thus, they need to be spatially registered. Also, different deforming objects, hereinafter referred to as 4D surfaces , evolve at different speeds and thus they need to be temporally aligned. We solve this spatiotemporal registration problem using a Riemannian approach. We treat a 3D surface as a point in a shape space equipped with an elastic Riemannian metric that measures the amount of bending and stretching that the surfaces undergo. A 4D surface can then be seen as a trajectory in this space. With this formulation, the statistical analysis of 4D surfaces can be cast as the problem of analyzing trajectories embedded in a nonlinear Riemannian manifold. However, performing the spatiotemporal registration, and subsequently computing statistics, on such nonlinear spaces is not straightforward as they rely on complex nonlinear optimizations. Our core contribution is the mapping of the surfaces to the space of Square-Root Normal Fields (SRNF) where the $\mathbb {L}^{2}$ metric is equivalent to the partial elastic metric in the space of surfaces. Thus, by solving the spatial registration in the SRNF space, the problem of analyzing 4D surfaces becomes the problem of analyzing trajectories embedded in the SRNF space, which has a euclidean structure. In this paper, we develop the building blocks that enable such analysis. These include: (1) the spatiotemporal registration of arbitrarily parameterized 4D surfaces even in the presence of large elastic deformations and large variations in their execution rates; (2) the computation of geodesics between 4D surfaces; (3) the computation of statistical summaries, such as means and modes of variation, of collections of 4D surfaces; and (4) the synthesis of random 4D surfaces. We demonstrate the performance of the proposed framework using 4D facial surfaces and 4D human body shapes.},
  archive      = {J_TPAMI},
  author       = {Hamid Laga and Marcel Padilla and Ian H. Jermyn and Sebastian Kurtek and Mohammed Bennamoun and Anuj Srivastava},
  doi          = {10.1109/TPAMI.2022.3163720},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {1335-1352},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {4D atlas: Statistical analysis of the spatiotemporal variability in longitudinal 3D shape data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision permutator: A permutable MLP-like architecture for
visual recognition. <em>TPAMI</em>, <em>45</em>(1), 1328–1334. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present Vision Permutator, a conceptually simple and data efficient MLP-like architecture for visual recognition. By realizing the importance of the positional information carried by 2D feature representations, unlike recent MLP-like models that encode the spatial information along the flattened spatial dimensions, Vision Permutator separately encodes the feature representations along the height and width dimensions with linear projections. This allows Vision Permutator to capture long-range dependencies and meanwhile avoid the attention building process in transformers. The outputs are then aggregated in a mutually complementing manner to form expressive representations. We show that our Vision Permutators are formidable competitors to convolutional neural networks (CNNs) and vision transformers. Without the dependence on spatial convolutions or attention mechanisms, Vision Permutator achieves 81.5\% top-1 accuracy on ImageNet without extra large-scale training data (e.g., ImageNet-22k) using only 25M learnable parameters, which is much better than most CNNs and vision transformers under the same model size constraint. When scaling up to 88M, it attains 83.2\% top-1 accuracy, greatly improving the performance of recent state-of-the-art MLP-like networks for visual recognition. We hope this work could encourage research on rethinking the way of encoding spatial information and facilitate the development of MLP-like models. PyTorch/MindSpore/Jittor code is available at https://github.com/Andrew-Qibin/VisionPermutator .},
  archive      = {J_TPAMI},
  author       = {Qibin Hou and Zihang Jiang and Li Yuan and Ming-Ming Cheng and Shuicheng Yan and Jiashi Feng},
  doi          = {10.1109/TPAMI.2022.3145427},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1328-1334},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vision permutator: A permutable MLP-like architecture for visual recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vid2CAD: CAD model alignment using multi-view constraints
from videos. <em>TPAMI</em>, <em>45</em>(1), 1320–1327. (<a
href="https://doi.org/10.1109/TPAMI.2022.3146082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the task of aligning CAD models to a video sequence of a complex scene containing multiple objects. Our method can process arbitrary videos and fully automatically recover the 9 DoF pose for each object appearing in it, thus aligning them in a common 3D coordinate frame. The core idea of our method is to integrate neural network predictions from individual frames with a temporally global, multi-view constraint optimization formulation. This integration process resolves the scale and depth ambiguities in the per-frame predictions, and generally improves the estimate of all pose parameters. By leveraging multi-view constraints, our method also resolves occlusions and handles objects that are out of view in individual frames, thus reconstructing all objects into a single globally consistent CAD representation of the scene. In comparison to the state-of-the-art single-frame method Mask2CAD that we build on, we achieve substantial improvements on the Scan2CAD dataset (from 11.6\% to 30.7\% class average accuracy).},
  archive      = {J_TPAMI},
  author       = {Kevis-Kokitsi Maninis and Stefan Popov and Matthias Nießner and Vittorio Ferrari},
  doi          = {10.1109/TPAMI.2022.3146082},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1320-1327},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vid2CAD: CAD model alignment using multi-view constraints from videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PnP-3D: A plug-and-play for 3D point clouds. <em>TPAMI</em>,
<em>45</em>(1), 1312–1319. (<a
href="https://doi.org/10.1109/TPAMI.2021.3137794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of the deep learning paradigm, many point cloud networks have been invented for visual analysis. However, there is great potential for development of these networks since the given information of point cloud data has not been fully exploited. To improve the effectiveness of existing networks in analyzing point cloud data, we propose a plug-and-play module, PnP-3D, aiming to refine the fundamental point cloud feature representations by involving more local context and global bilinear response from explicit 3D space and implicit feature space. To thoroughly evaluate our approach, we conduct experiments on three standard point cloud analysis tasks, including classification, semantic segmentation, and object detection, where we select three state-of-the-art networks from each task for evaluation. Serving as a plug-and-play module, PnP-3D can significantly boost the performances of established networks. In addition to achieving state-of-the-art results on four widely used point cloud benchmarks, we present comprehensive ablation studies and visualizations to demonstrate our approach&#39;s advantages. The code will be available at https://github.com/ShiQiu0419/pnp-3d .},
  archive      = {J_TPAMI},
  author       = {Shi Qiu and Saeed Anwar and Nick Barnes},
  doi          = {10.1109/TPAMI.2021.3137794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1312-1319},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PnP-3D: A plug-and-play for 3D point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuron coverage-guided domain generalization.
<em>TPAMI</em>, <em>45</em>(1), 1302–1311. (<a
href="https://doi.org/10.1109/TPAMI.2022.3157441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the domain generalization task where domain knowledge is unavailable, and even worse, only samples from a single domain can be utilized during training. Our motivation originates from the recent progresses in deep neural network (DNN) testing, which has shown that maximizing neuron coverage of DNN can help to explore possible defects of DNN (i.e., misclassification). More specifically, by treating the DNN as a program and each neuron as a functional point of the code, during the network training we aim to improve the generalization capability by maximizing the neuron coverage of DNN with the gradient similarity regularization between the original and augmented samples. As such, the decision behavior of the DNN is optimized, avoiding the arbitrary neurons that are deleterious for the unseen samples, and leading to the trained DNN that can be better generalized to out-of-distribution samples. Extensive studies on various domain generalization tasks based on both single and multiple domain(s) setting demonstrate the effectiveness of our proposed approach compared with state-of-the-art baseline methods. We also analyze our method by conducting visualization based on network dissection. The results further provide useful evidence on the rationality and effectiveness of our approach.},
  archive      = {J_TPAMI},
  author       = {Chris Xing Tian and Haoliang Li and Xiaofei Xie and Yang Liu and Shiqi Wang},
  doi          = {10.1109/TPAMI.2022.3157441},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1302-1311},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neuron coverage-guided domain generalization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to extract building footprints from off-nadir
aerial images. <em>TPAMI</em>, <em>45</em>(1), 1294–1301. (<a
href="https://doi.org/10.1109/TPAMI.2022.3162583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting building footprints from aerial images is essential for precise urban mapping with photogrammetric computer vision technologies. Existing approaches mainly assume that the roof and footprint of a building are well overlapped, which may not hold in off-nadir aerial images as there is often a big offset between them. In this paper, we propose an offset vector learning scheme, which turns the building footprint extraction problem in off-nadir images into an instance-level joint prediction problem of the building roof and its corresponding “ roof to footprint ” offset vector. Thus the footprint can be estimated by translating the predicted roof mask according to the predicted offset vector. We further propose a simple but effective feature-level offset augmentation module, which can significantly refine the offset vector prediction by introducing little extra cost. Moreover, a new dataset, Buildings in Off-Nadir Aerial Images (BONAI), is created and released in this paper. It contains 268,958 building instances across 3,300 aerial images with fully annotated instance-level roof, footprint, and corresponding offset vector for each building. Experiments on the BONAI dataset demonstrate that our method achieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39 points in F1-score. The codes, datasets, and trained models are available at https://github.com/jwwangchn/BONAI.git .},
  archive      = {J_TPAMI},
  author       = {Jinwang Wang and Lingxuan Meng and Weijia Li and Wen Yang and Lei Yu and Gui-Song Xia},
  doi          = {10.1109/TPAMI.2022.3162583},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1294-1301},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to extract building footprints from off-nadir aerial images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced spatio-temporal interaction learning for video
deraining: Faster and better. <em>TPAMI</em>, <em>45</em>(1), 1287–1293.
(<a href="https://doi.org/10.1109/TPAMI.2022.3148707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video deraining is an important task in computer vision as the unwanted rain hampers the visibility of videos and deteriorates the robustness of most outdoor vision systems. Despite the significant success which has been achieved for video deraining recently, two major challenges remain: 1) how to exploit the vast information among successive frames to extract powerful spatio-temporal features across both the spatial and temporal domains, and 2) how to restore high-quality derained videos with a high-speed approach. In this paper, we present a new end-to-end video deraining framework, dubbed Enhanced Spatio-Temporal Interaction Network (ESTINet), which considerably boosts current state-of-the-art video deraining quality and speed. The ESTINet takes the advantage of deep residual networks and convolutional long short-term memory, which can capture the spatial features and temporal correlations among successive frames at the cost of very little computational resource. Extensive experiments on three public datasets show that the proposed ESTINet can achieve faster speed than the competitors, while maintaining superior performance over the state-of-the-art methods. https://github.com/HDCVLab/Enhanced-Spatio-Temporal-Interaction-Learning-for-Video-Deraining .},
  archive      = {J_TPAMI},
  author       = {Kaihao Zhang and Dongxu Li and Wenhan Luo and Wenqi Ren and Wei Liu},
  doi          = {10.1109/TPAMI.2022.3148707},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1287-1293},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced spatio-temporal interaction learning for video deraining: Faster and better},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual micro-pattern propagation. <em>TPAMI</em>,
<em>45</em>(1), 1267–1286. (<a
href="https://doi.org/10.1109/TPAMI.2022.3147974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistic observations demonstrate that visual feature patterns or structure patterns recur high-frequently within/across homo/heterogeneous images. Motivated by the interdependencies of visual patterns, we propose visual micro-pattern propagation (VMPP) to facilitate universal visual pattern learning. Especially, we present a graph framework to unify the conventional micro-pattern propagations in spatial, temporal, cross-modal and cross-task domains. A general formulation of pattern propagation named cross-graph model is presented under this framework, and accordingly a factorized version is derived for more efficient computation as well as better understanding. To correlate homo/heterogeneous patterns, in cross-graph we introduce two types of pattern relations from feature-level and structure-level. The structure pattern relation defines second-order visual connections for heterogeneous patterns by measuring first-order visual relations of homogeneous feature patterns. In virtue of the constructed first-/second-order connections, we design feature pattern diffusion and structure pattern diffusion to prop up various pattern propagation cases. To fulfill different pattern diffusions involved, further, we deeply study two fundamental visual problems, multi-task pixel-level prediction and online dual-modal object tracking, and accordingly propose two end-to-end pattern propagation networks by encapsulating and integrating some necessary diffusion modules therein. We conduct extensive experiments by dissecting every diffusion component as well as comparing numerous advanced methods. The experiments validate the effectiveness of our proposed various pattern diffusion ways and meantime report the state-of-the-art results on the two representative visual problems.},
  archive      = {J_TPAMI},
  author       = {Zhen Cui and Ling Zhou and Chaoqun Wang and Chunyan Xu and Jian Yang},
  doi          = {10.1109/TPAMI.2022.3147974},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1267-1286},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual micro-pattern propagation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Unsupervised face detection in the dark. <em>TPAMI</em>,
<em>45</em>(1), 1250–1266. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light face detection is challenging but critical for real-world applications, such as nighttime autonomous driving and city surveillance. Current face detection models rely on extensive annotations and lack generality and flexibility. In this paper, we explore how to learn face detectors without low-light annotations. Fully exploiting existing normal light data, we propose adapting face detectors from normal light to low light. This task is difficult because the gap between brightness and darkness is too large and complicated at the object level and pixel level. Accordingly, the performance of current low-light enhancement or adaptation methods is unsatisfactory. To solve this problem, we propose a joint High-Low Adaptation (HLA) framework. We design bidirectional low-level adaptation and multitask high-level adaptation. For low-level, we enhance the dark images and degrade the normal-light images, making both domains move toward each other. For high-level, we combine context-based and contrastive learning to comprehensively close the features on different domains. Experiments show that our HLA-Face v2 model obtains superior low-light face detection performance even without the use of low-light annotations. Moreover, our adaptation scheme can be extended to a wide range of applications, such as improving supervised learning and generic object detection. Project publicly available at: https://daooshee.github.io/HLA-Face-v2-Website/ .},
  archive      = {J_TPAMI},
  author       = {Wenjing Wang and Xinhao Wang and Wenhan Yang and Jiaying Liu},
  doi          = {10.1109/TPAMI.2022.3152562},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1250-1266},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised face detection in the dark},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra-high temporal resolution visual reconstruction from a
fovea-like spike camera via spiking neuron model. <em>TPAMI</em>,
<em>45</em>(1), 1233–1249. (<a
href="https://doi.org/10.1109/TPAMI.2022.3146140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic vision sensor is a new bio-inspired imaging paradigm emerged in recent years. It uses the asynchronous spike signals instead of the traditional frame-based manner to achieve ultra-high speed sampling. Unlike the dynamic vision sensor (DVS) that perceives movement by imitating the retinal periphery, the spike camera was developed recently to perceive fine textures by simulating a small retinal region called the fovea. For this new type of neuromorphic camera, how to reconstruct ultra-high speed visual images from spike data becomes an important yet challenging issue in visual scene perception, analysis, and recognition applications. In this paper, a bio-inspired visual reconstruction framework for the spike camera is proposed for the first time. Its core idea is to use the biologically inspired adaptive adjustment mechanisms, combined with the spatiotemporal spike information extracted by the proposed model, to reconstruct the full texture of natural scenes in an ultra-high temporal resolution. Specifically, the proposed model consists of a motion local excitation layer, a spike refining layer and a visual reconstruction layer motivated by the bio-realistic leaky integrate-and-fire (LIF) neurons and synapse connection with spike-timing dependent plasticity (STDP) rule. To evaluate the performance, a spike dataset was constructed for normal and high-speed scenes in real-world recorded by the spike camera. The experimental results show that the proposed approach can reconstruct the visual images with 40,000 frames per second in both normal and high-speed scenes, while achieving high dynamic range and high image quality.},
  archive      = {J_TPAMI},
  author       = {Lin Zhu and Siwei Dong and Jianing Li and Tiejun Huang and Yonghong Tian},
  doi          = {10.1109/TPAMI.2022.3146140},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1233-1249},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ultra-high temporal resolution visual reconstruction from a fovea-like spike camera via spiking neuron model},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards zero-shot sign language recognition. <em>TPAMI</em>,
<em>45</em>(1), 1217–1232. (<a
href="https://doi.org/10.1109/TPAMI.2022.3143074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the problem of zero-shot sign language recognition (ZSSLR), where the goal is to leverage models learned over the seen sign classes to recognize the instances of unseen sign classes. In this context, readily available textual sign descriptions and attributes collected from sign language dictionaries are utilized as semantic class representations for knowledge transfer. For this novel problem setup, we introduce three benchmark datasets with their accompanying textual and attribute descriptions to analyze the problem in detail. Our proposed approach builds spatiotemporal models of body and hand regions. By leveraging the descriptive text and attribute embeddings along with these visual representations within a zero-shot learning framework, we show that textual and attribute based class definitions can provide effective knowledge for the recognition of previously unseen sign classes. We additionally introduce techniques to analyze the influence of binary attributes in correct and incorrect zero-shot predictions. We anticipate that the introduced approaches and the accompanying datasets will provide a basis for further exploration of zero-shot learning in sign language recognition.},
  archive      = {J_TPAMI},
  author       = {Yunus Can Bilge and Ramazan Gokberk Cinbis and Nazli Ikizler-Cinbis},
  doi          = {10.1109/TPAMI.2022.3143074},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1217-1232},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards zero-shot sign language recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards pointsets representation learning via
self-supervised learning and set augmentation. <em>TPAMI</em>,
<em>45</em>(1), 1201–1216. (<a
href="https://doi.org/10.1109/TPAMI.2021.3139113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning is a supervised learning paradigm to construct a meaningful vector space to represent complex objects. A successful application of deep metric learning to pointsets means that we can avoid expensive retrieval operations on objects such as documents and can significantly facilitate many machine learning and data mining tasks involving pointsets. We propose a self-supervised deep metric learning solution for pointsets. The novelty of our proposed solution lies in a self-supervision mechanism that makes use of a distribution distance for set ranking called the Earth’s Mover Distance (EMD) to generate pseudo labels and a pointset augmentation method for supporting the learning solution. Our experimental studies on documents, graphs, and point clouds datasets show that our proposed solutions outperform baselines and state-of-the-art approaches under the unsupervised settings. The learned self-supervised representation can also be used as a pre-trained model, which can boost downstream tasks with a fine-tuning step and outperform state-of-the-art language models.},
  archive      = {J_TPAMI},
  author       = {Pattaramanee Arsomngern and Cheng Long and Supasorn Suwajanakorn and Sarana Nutanong},
  doi          = {10.1109/TPAMI.2021.3139113},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1201-1216},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards pointsets representation learning via self-supervised learning and set augmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards improved and interpretable deep metric learning via
attentive grouping. <em>TPAMI</em>, <em>45</em>(1), 1189–1200. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grouping has been commonly used in deep metric learning for computing diverse features. To improve the performance and interpretability, we propose an improved and interpretable grouping method to be integrated flexibly with any metric learning framework. Specifically, our method is based on the attention mechanism with a learnable query for each group. The query is fully trainable and can capture group-specific information when combined with the diversity loss. An appealing property of our method is that it naturally lends itself interpretability. The attention scores between the learnable query and each spatial position can be interpreted as the importance of that position. We formally show that our proposed grouping method is invariant to spatial permutations of features. When used as a module in convolutional neural networks, our method leads to translational invariance. We conduct comprehensive experiments to evaluate our method. Our quantitative results indicate that the proposed method outperforms prior methods consistently and significantly across different datasets, evaluation metrics, base models, and loss functions. For the first time to the best of our knowledge, our interpretation results clearly demonstrate that the proposed method enables the learning of diverse and stable semantic features across groups.},
  archive      = {J_TPAMI},
  author       = {Xinyi Xu and Zhengyang Wang and Cheng Deng and Hao Yuan and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2022.3152495},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1189-1200},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards improved and interpretable deep metric learning via attentive grouping},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards high performance low complexity calibration in
appearance based gaze estimation. <em>TPAMI</em>, <em>45</em>(1),
1174–1188. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appearance-based gaze estimation from RGB images provides relatively unconstrained gaze tracking from commonly available hardware. The accuracy of subject-independent models is limited partly by small intra-subject and large inter-subject variations in appearance, and partly by a latent subject-dependent bias. To improve estimation accuracy, we have previously proposed a gaze decomposition method that decomposes the gaze angle into the sum of a subject-independent gaze estimate from the image and a subject-dependent bias. Estimating the bias from images outperforms previously proposed calibration algorithms, unless the amount of calibration data is prohibitively large. This paper extends that work with a more complete characterization of the interplay between the complexity of the calibration dataset and estimation accuracy. In particular, we analyze the effect of the number of gaze targets, the number of images used per gaze target and the number of head positions in calibration data using a new NISLGaze dataset, which is well suited for analyzing these effects as it includes more diversity in head positions and orientations for each subject than other datasets. A better understanding of these factors enables low complexity high performance calibration. Our results indicate that using only a single gaze target and single head position is sufficient to achieve high quality calibration. However, it is useful to include variability in head orientation as the subject is gazing at the target. Our proposed estimator based on these studies (GEDDNet) outperforms state-of-the-art methods by more than $6.3\%$ . One of the surprising findings of our work is that the same estimator yields the best performance both with and without calibration. This is convenient, as the estimator works well ”straight out of the box,” but can be improved if needed by calibration. However, this seems to violate the conventional wisdom that train and test conditions must be matched. To better understand the reasons, we provide a new theoretical analysis that specifies the conditions under which this can be expected. The dataset is available at http://nislgaze.ust.hk . Source code is available at https://github.com/HKUST-NISL/GEDDnet .},
  archive      = {J_TPAMI},
  author       = {Zhaokang Chen and Bertram E. Shi},
  doi          = {10.1109/TPAMI.2022.3148386},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1174-1188},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards high performance low complexity calibration in appearance based gaze estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Switchable novel object captioner. <em>TPAMI</em>,
<em>45</em>(1), 1162–1173. (<a
href="https://doi.org/10.1109/TPAMI.2022.3144984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims at automatically describing images by sentences. It often requires lots of paired image-sentence data for training. However, trained captioning models can hardly be applied to new domains in which some novel words exist. In this paper, we introduce the zero-shot novel object captioning task, where the machine generates descriptions about novel objects without extra training sentences. To tackle the challenging task, we mimic the way that babies talk about something unknown, i.e., using the word of a similar known object. Following this motivation, we build a key-value object memory by detection models, containing visual information and corresponding words for objects in the image. For those novel objects, we use words of most similar seen objects as proxy visual words to solve the out-of-vocabulary issue. We then propose a Switchable LSTM that incorporates knowledge from the object memory into sentence generation. The model has two switchable working modes, i.e., 1) generating the sentences like standard LSTMs and 2) retrieving proper nouns from the key-value memory. Thus our model is learned to fully disentangle language generation from training objects, and requires zero training sentence in describing novel objects. Experiments on three large-scale datasets demonstrate the ability of our method to describe novel concepts.},
  archive      = {J_TPAMI},
  author       = {Yu Wu and Lu Jiang and Yi Yang},
  doi          = {10.1109/TPAMI.2022.3144984},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1162-1173},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Switchable novel object captioner},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Streaming variational monte carlo. <em>TPAMI</em>,
<em>45</em>(1), 1150–1161. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear state-space models are powerful tools to describe dynamical structures in complex time series. In a streaming setting where data are processed one sample at a time, simultaneous inference of the state and its nonlinear dynamics has posed significant challenges in practice. We develop a novel online learning framework, leveraging variational inference and sequential Monte Carlo, which enables flexible and accurate Bayesian joint filtering. Our method provides an approximation of the filtering posterior which can be made arbitrarily close to the true filtering distribution for a wide class of dynamics models and observation models. Specifically, the proposed framework can efficiently approximate a posterior over the dynamics using sparse Gaussian processes, allowing for an interpretable model of the latent dynamics. Constant time complexity per sample makes our approach amenable to online learning scenarios and suitable for real-time applications.},
  archive      = {J_TPAMI},
  author       = {Yuan Zhao and Josue Nassar and Ian Jordan and Mónica Bugallo and Il Memming Park},
  doi          = {10.1109/TPAMI.2022.3153225},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1150-1161},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Streaming variational monte carlo},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STORM: Structure-based overlap matching for partial point
cloud registration. <em>TPAMI</em>, <em>45</em>(1), 1135–1149. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial point cloud registration aims to transform partial scans into a common coordinate system. It is an important preprocessing step to generate complete 3D shapes. Although previous registration methods have made great progress in recent decades, traditional registration methods, such as Iterative Closest Point (ICP) and its variants, all these methods highly depend on the sufficient overlaps between two point clouds, because they cannot distinguish outlier correspondences. Note that the overlap between point clouds could always be small, which limits the application of these methods. To tackle this problem, we present a StrucTure-based OveRlap Matching (STORM) method for partial point cloud registration. In our method, an overlap prediction module with differentiable sampling is designed to detect points in overlap utilizing structure information, and facilitates exact partial correspondence generation, which is based on discriminative pointwise feature similarity. The pointwise features which contain effective structural information are extracted by graph-based methods. Experimental results and comparison with state-of-the-art methods demonstrate that STORM can achieve better performance. Moreover, most registration methods perform worse when the overlap ratio decreases, while STORM can still achieve satisfactory performance when the overlap ratio is small.},
  archive      = {J_TPAMI},
  author       = {Yujie Wang and Chenggang Yan and Yutong Feng and Shaoyi Du and Qionghai Dai and Yue Gao},
  doi          = {10.1109/TPAMI.2022.3148308},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1135-1149},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {STORM: Structure-based overlap matching for partial point cloud registration},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving inverse problems with deep neural networks –
robustness included? <em>TPAMI</em>, <em>45</em>(1), 1119–1134. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past five years, deep learning methods have become state-of-the-art in solving various inverse problems. Before such approaches can find application in safety-critical fields, a verification of their reliability appears mandatory. Recent works have pointed out instabilities of deep neural networks for several image reconstruction tasks. In analogy to adversarial attacks in classification, it was shown that slight distortions in the input domain may cause severe artifacts. The present article sheds new light on this concern, by conducting an extensive study of the robustness of deep-learning-based algorithms for solving underdetermined inverse problems. This covers compressed sensing with Gaussian measurements as well as image recovery from Fourier and Radon measurements, including a real-world scenario for magnetic resonance imaging (using the NYU-fastMRI dataset). Our main focus is on computing adversarial perturbations of the measurements that maximize the reconstruction error. A distinctive feature of our approach is the quantitative and qualitative comparison with total-variation minimization, which serves as a provably robust reference method. In contrast to previous findings, our results reveal that standard end-to-end network architectures are not only resilient against statistical noise, but also against adversarial perturbations. All considered networks are trained by common deep learning techniques, without sophisticated defense strategies.},
  archive      = {J_TPAMI},
  author       = {Martin Genzel and Jan Macdonald and Maximilian März},
  doi          = {10.1109/TPAMI.2022.3148324},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1119-1134},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Solving inverse problems with deep neural networks – robustness included?},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Simultaneously-collected multimodal lying pose dataset:
Enabling in-bed human pose monitoring. <em>TPAMI</em>, <em>45</em>(1),
1106–1118. (<a
href="https://doi.org/10.1109/TPAMI.2022.3155712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision field has achieved great success in interpreting semantic meanings from images, yet its algorithms can be brittle for tasks with adverse vision conditions and the ones suffering from data/label pair limitation. Among these tasks is in-bed human pose monitoring with significant value in many healthcare applications. In-bed pose monitoring in natural settings involves pose estimation in complete darkness or full occlusion. The lack of publicly available in-bed pose datasets hinders the applicability of many successful human pose estimation algorithms for this task. In this paper, we introduce our Simultaneously-collected multimodal Lying Pose (SLP) dataset, which includes in-bed pose images from 109 participants captured using multiple imaging modalities including RGB, long wave infrared (LWIR), depth, and pressure map. We also present a physical hyper parameter tuning strategy for ground truth pose label generation under adverse vision conditions. The SLP design is compatible with the mainstream human pose datasets; therefore, the state-of-the-art 2D pose estimation models can be trained effectively with the SLP data with promising performance as high as 95\% at PCKh@0.5 on a single modality. The pose estimation performance of these models can be further improved by including additional modalities through the proposed collaborative scheme.},
  archive      = {J_TPAMI},
  author       = {Shuangjun Liu and Xiaofei Huang and Nihang Fu and Cheng Li and Zhongnan Su and Sarah Ostadabbas},
  doi          = {10.1109/TPAMI.2022.3155712},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1106-1118},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Simultaneously-collected multimodal lying pose dataset: Enabling in-bed human pose monitoring},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised heterogeneous domain adaptation: Theory and
algorithms. <em>TPAMI</em>, <em>45</em>(1), 1087–1105. (<a
href="https://doi.org/10.1109/TPAMI.2022.3146234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised heterogeneous domain adaptation (SsHeDA) aims to train a classifier for the target domain, in which only unlabeled and a small number of labeled data are available. This is done by leveraging knowledge acquired from a heterogeneous source domain. From algorithmic perspectives, several methods have been proposed to solve the SsHeDA problem; yet there is still no theoretical foundation to explain the nature of the SsHeDA problem or to guide new and better solutions. Motivated by compatibility condition in semi-supervised probably approximately correct (PAC) theory, we explain the SsHeDA problem by proving its generalization error – that is, why labeled heterogeneous source data and unlabeled target data help to reduce the target risk. Guided by our theory, we devise two algorithms as proof of concept. One, kernel heterogeneous domain alignment (KHDA), is a kernel-based algorithm; the other, joint mean embedding alignment (JMEA), is a neural network-based algorithm. When a dataset is small, KHDA’s training time is less than JMEA’s. When a dataset is large, JMEA is more accurate in the target domain. Comprehensive experiments with image/text classification tasks show KHDA to be the most accurate among all non-neural network baselines, and JMEA to be the most accurate among all baselines.},
  archive      = {J_TPAMI},
  author       = {Zhen Fang and Jie Lu and Feng Liu and Guangquan Zhang},
  doi          = {10.1109/TPAMI.2022.3146234},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1087-1105},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised heterogeneous domain adaptation: Theory and algorithms},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SEEM: A sequence entropy energy-based model for pedestrian
trajectory all-then-one prediction. <em>TPAMI</em>, <em>45</em>(1),
1070–1086. (<a
href="https://doi.org/10.1109/TPAMI.2022.3147639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the future trajectories of pedestrians is of increasing importance for many applications such as autonomous driving and social robots. Nevertheless, current trajectory prediction models suffer from limitations such as lack of diversity in candidate trajectories, poor accuracy, and instability. In this paper, we propose a novel Sequence Entropy Energy-based Model named SEEM, which consists of a generator network and an energy network. Within SEEM we optimize the sequence entropy by taking advantage of the local variational inference of $f$ -divergence estimation to maximize the mutual information across the generator in order to cover all modes of the trajectory distribution, thereby ensuring SEEM achieves full diversity in candidate trajectory generation. Then, we introduce a probability distribution clipping mechanism to draw samples towards regions of high probability in the trajectory latent space, while our energy network determines which trajectory is most representative of the ground truth. This dual approach is our so-called all-then-one strategy. Finally, a zero-centered potential energy regularization is proposed to ensure stability and convergence of the training process. Through experiments on both synthetic and public benchmark datasets, SEEM is shown to substantially outperform the current state-of-the-art approaches in terms of diversity, accuracy and stability of pedestrian trajectory prediction.},
  archive      = {J_TPAMI},
  author       = {Dafeng Wang and Hongbo Liu and Naiyao Wang and Yiyang Wang and Hua Wang and Seán McLoone},
  doi          = {10.1109/TPAMI.2022.3147639},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1070-1086},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SEEM: A sequence entropy energy-based model for pedestrian trajectory all-then-one prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust multi-view clustering with incomplete information.
<em>TPAMI</em>, <em>45</em>(1), 1055–1069. (<a
href="https://doi.org/10.1109/TPAMI.2022.3155499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of existing multi-view clustering methods heavily relies on the assumption of view consistency and instance completeness, referred to as the complete information. However, these two assumptions would be inevitably violated in data collection and transmission, thus leading to the so-called Partially View-unaligned Problem (PVP) and Partially Sample-missing Problem (PSP). To overcome such incomplete information challenges, we propose a novel method, termed robuSt mUlti-view clusteRing with incomplEte information (SURE), which solves PVP and PSP under a unified framework. In brief, SURE is a novel contrastive learning paradigm which uses the available pairs as positives and randomly chooses some cross-view samples as negatives. To reduce the influence of the false negatives caused by random sampling, SURE is with a noise-robust contrastive loss that theoretically and empirically mitigates or even eliminates the influence of the false negatives. To the best of our knowledge, this could be the first successful attempt that simultaneously handles PVP and PSP using a unified solution. In addition, this could be one of the first studies on the noisy correspondence problem (i.e., the false negatives) which is a novel paradigm of noisy labels. Extensive experiments demonstrate the effectiveness and efficiency of SURE comparing with 10 state-of-the-art approaches on the multi-view clustering task.},
  archive      = {J_TPAMI},
  author       = {Mouxing Yang and Yunfan Li and Peng Hu and Jinfeng Bai and Jiancheng Lv and Xi Peng},
  doi          = {10.1109/TPAMI.2022.3155499},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1055-1069},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust multi-view clustering with incomplete information},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROAD: The road event awareness dataset for autonomous
driving. <em>TPAMI</em>, <em>45</em>(1), 1036–1054. (<a
href="https://doi.org/10.1109/TPAMI.2022.3150906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans drive in a holistic fashion which entails, in particular, understanding dynamic road events and their evolution. Injecting these capabilities in autonomous vehicles can thus take situational awareness and decision making closer to human-level performance. To this purpose, we introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge the first of its kind. ROAD is designed to test an autonomous vehicle’s ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from the Oxford RobotCar Dataset, annotated with bounding boxes showing the location in the image plane of each road event. We benchmark various detection tasks, proposing as a baseline a new incremental algorithm for online road event awareness termed 3D-RetinaNet. We also report the performance on the ROAD tasks of Slowfast and YOLOv5 detectors, as well as that of the winners of the ICCV2021 ROAD challenge, which highlight the challenges faced by situation awareness in autonomous driving. ROAD is designed to allow scholars to investigate exciting tasks such as complex (road) activity detection, future event anticipation and continual learning. The dataset is available at https://github.com/gurkirt/road-dataset ; the baseline can be found at https://github.com/gurkirt/3D-RetinaNet .},
  archive      = {J_TPAMI},
  author       = {Gurkirt Singh and Stephen Akrigg and Manuele Di Maio and Valentina Fontana and Reza Javanmard Alitappeh and Salman Khan and Suman Saha and Kossar Jeddisaravi and Farzad Yousefi and Jacob Culley and Tom Nicholson and Jordan Omokeowa and Stanislao Grazioso and Andrew Bradley and Giuseppe Di Gironimo and Fabio Cuzzolin},
  doi          = {10.1109/TPAMI.2022.3150906},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1036-1054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ROAD: The road event awareness dataset for autonomous driving},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking collaborative metric learning: Toward an
efficient alternative without negative sampling. <em>TPAMI</em>,
<em>45</em>(1), 1017–1035. (<a
href="https://doi.org/10.1109/TPAMI.2022.3141095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed Collaborative Metric Learning (CML) paradigm has aroused wide interest in the area of recommendation systems (RS) owing to its simplicity and effectiveness. Typically, the existing literature of CML depends largely on the negative sampling strategy to alleviate the time-consuming burden of pairwise computation. However, in this work, by taking a theoretical analysis, we find that negative sampling would lead to a biased estimation of the generalization error. Specifically, we show that the sampling-based CML would introduce a bias term in the generalization bound, which is quantified by the per-user Total Variance (TV) between the distribution induced by negative sampling and the ground truth distribution. This suggests that optimizing the sampling-based CML loss function does not ensure a small generalization error even with sufficiently large training data. Moreover, we show that the bias term will vanish without the negative sampling strategy. Motivated by this, we propose an efficient alternative without negative sampling for CML named Sampling-Free Collaborative Metric Learning (SFCML), to get rid of the sampling bias in a practical sense. Finally, comprehensive experiments over seven benchmark datasets speak to the supriority of the proposed algorithm.},
  archive      = {J_TPAMI},
  author       = {Shilong Bao and Qianqian Xu and Zhiyong Yang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2022.3141095},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1017-1035},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rethinking collaborative metric learning: Toward an efficient alternative without negative sampling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Representing graphs via gromov-wasserstein factorization.
<em>TPAMI</em>, <em>45</em>(1), 999–1016. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation is a challenging and significant problem for many real-world applications. In this work, we propose a novel paradigm called “Gromov-Wasserstein Factorization” (GWF) to learn graph representations in a flexible and interpretable way. Given a set of graphs, whose correspondence between nodes is unknown and whose sizes can be different, our GWF model reconstructs each graph by a weighted combination of some “graph factors” under a pseudo-metric called Gromov-Wasserstein (GW) discrepancy. This model leads to a new nonlinear factorization mechanism of the graphs. The graph factors are shared by all the graphs, which represent the typical patterns of the graphs’ structures. The weights associated with each graph indicate the graph factors’ contributions to the graph&#39;s reconstruction, which lead to a permutation-invariant graph representation. We learn the graph factors of the GWF model and the weights of the graphs jointly by minimizing the overall reconstruction error. When learning the model, we reparametrize the graph factors and the weights to unconstrained model parameters and simplify the backpropagation of gradient with the help of the envelope theorem. For the GW discrepancy (the critical training step), we consider two algorithms to compute it, which correspond to the proximal point algorithm (PPA) and Bregman alternating direction method of multipliers (BADMM), respectively. Furthermore, we propose some extensions of the GWF model, including (i) combining with a graph neural network and learning graph representations in an auto-encoding manner, (ii) representing the graphs with node attributes, and (iii) working as a regularizer for semi-supervised graph classification. Experiments on various datasets demonstrate that our GWF model is comparable to the state-of-the-art methods. The graph representations derived by it perform well in graph clustering and classification tasks.},
  archive      = {J_TPAMI},
  author       = {Hongteng Xu and Jiachang Liu and Dixin Luo and Lawrence Carin},
  doi          = {10.1109/TPAMI.2022.3153126},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {999-1016},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Representing graphs via gromov-wasserstein factorization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforced, incremental and cross-lingual event detection
from social messages. <em>TPAMI</em>, <em>45</em>(1), 980–998. (<a
href="https://doi.org/10.1109/TPAMI.2022.3144993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting hot social events (e.g., political scandal, momentous meetings, natural hazards, etc.) from social messages is crucial as it highlights significant happenings to help people understand the real world. On account of the streaming nature of social messages, incremental social event detection models in acquiring, preserving, and updating messages over time have attracted great attention. However, the challenge is that the existing event detection methods towards streaming social messages are generally confronted with ambiguous events features, dispersive text contents, and multiple languages, and hence result in low accuracy and generalization ability. In this paper, we present a novel rein F orced, i ncremental and cross-li n gual social Event detection architecture, namely FinEvent , from streaming social messages. Concretely, we first model social messages into heterogeneous graphs integrating both rich meta-semantics and diverse meta-relations, and convert them to weighted multi-relational message graphs. Second, we propose a new reinforced weighted multi-relational graph neural network framework by using a Multi-agent Reinforcement Learning algorithm to select optimal aggregation thresholds across different relations/edges to learn social message embeddings. To solve the long-tail problem in social event detection, a balanced sampling strategy guided Contrastive Learning mechanism is designed for incremental social message representation learning. Third, a new Deep Reinforcement Learning guided density-based spatial clustering model is designed to select the optimal minimum number of samples required to form a cluster and optimal minimum distance between two clusters in social event detection tasks. Finally, we implement incremental social message representation learning based on knowledge preservation on the graph neural network and achieve the transferring cross-lingual social event detection. We conduct extensive experiments to evaluate the FinEvent on Twitter streams, demonstrating a significant and consistent improvement in model quality with 14\%–118\%, 8\%–170\%, and 2\%–21\% increases in performance on offline, online, and cross-lingual social event detection tasks.},
  archive      = {J_TPAMI},
  author       = {Hao Peng and Ruitong Zhang and Shaoning Li and Yuwei Cao and Shirui Pan and Philip S. Yu},
  doi          = {10.1109/TPAMI.2022.3144993},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {980-998},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reinforced, incremental and cross-lingual event detection from social messages},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reframing neural networks: Deep structure in overcomplete
representations. <em>TPAMI</em>, <em>45</em>(1), 964–979. (<a
href="https://doi.org/10.1109/TPAMI.2022.3149445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparison to classical shallow representation learning techniques, deep neural networks have achieved superior performance in nearly every application benchmark. But despite their clear empirical advantages, it is still not well understood what makes them so effective. To approach this question, we introduce deep frame approximation: a unifying framework for constrained representation learning with structured overcomplete frames. While exact inference requires iterative optimization, it may be approximated by the operations of a feed-forward deep neural network. We indirectly analyze how model capacity relates to frame structures induced by architectural hyperparameters such as depth, width, and skip connections. We quantify these structural differences with the deep frame potential, a data-independent measure of coherence linked to representation uniqueness and stability. As a criterion for model selection, we show correlation with generalization error on a variety of common deep network architectures and datasets. We also demonstrate how recurrent networks implementing iterative optimization algorithms can achieve performance comparable to their feed-forward approximations while improving adversarial robustness. This connection to the established theory of overcomplete representations suggests promising new directions for principled deep network architecture design with less reliance on ad-hoc engineering.},
  archive      = {J_TPAMI},
  author       = {Calvin Murdock and George Cazenavette and Simon Lucey},
  doi          = {10.1109/TPAMI.2022.3149445},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {964-979},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reframing neural networks: Deep structure in overcomplete representations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refine-net: Normal refinement neural network for noisy point
clouds. <em>TPAMI</em>, <em>45</em>(1), 946–963. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point normal, as an intrinsic geometric property of 3D objects, not only serves conventional geometric tasks such as surface consolidation and reconstruction, but also facilitates cutting-edge learning-based techniques for shape analysis and generation. In this paper, we propose a normal refinement network, called Refine-Net, to predict accurate normals for noisy point clouds. Traditional normal estimation wisdom heavily depends on priors such as surface shapes or noise distributions, while learning-based solutions settle for single types of hand-crafted features. Differently, our network is designed to refine the initial normal of each point by extracting additional information from multiple feature representations. To this end, several feature modules are developed and incorporated into Refine-Net by a novel connection module. Besides the overall network architecture of Refine-Net, we propose a new multi-scale fitting patch selection scheme for the initial normal estimation, by absorbing geometry domain knowledge. Also, Refine-Net is a generic normal estimation framework: 1) point normals obtained from other methods can be further refined, and 2) any feature module related to the surface geometric structures can be potentially integrated into the framework. Qualitative and quantitative evaluations demonstrate the clear superiority of Refine-Net over the state-of-the-arts on both synthetic and real-scanned datasets.},
  archive      = {J_TPAMI},
  author       = {Haoran Zhou and Honghua Chen and Yingkui Zhang and Mingqiang Wei and Haoran Xie and Jun Wang and Tong Lu and Jing Qin and Xiao-Ping Zhang},
  doi          = {10.1109/TPAMI.2022.3145877},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {946-963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Refine-net: Normal refinement neural network for noisy point clouds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent 3D hand pose estimation using cascaded pose-guided
3D alignments. <em>TPAMI</em>, <em>45</em>(1), 932–945. (<a
href="https://doi.org/10.1109/TPAMI.2022.3159725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D hand pose estimation is a challenging problem in computer vision due to the high degrees-of-freedom of hand articulated motion space and large viewpoint variation. As a consequence, similar poses observed from multiple views can be dramatically different. In order to deal with this issue, view-independent features are required to achieve state-of-the-art performance. In this paper, we investigate the impact of view-independent features on 3D hand pose estimation from a single depth image, and propose a novel recurrent neural network for 3D hand pose estimation, in which a cascaded 3D pose-guided alignment strategy is designed for view-independent feature extraction and a recurrent hand pose module is designed for modeling the dependencies among sequential aligned features for 3D hand pose estimation. In particular, our cascaded pose-guided 3D alignments are performed in 3D space in a coarse-to-fine fashion. First, hand joints are predicted and globally transformed into a canonical reference frame; Second, the palm of the hand is detected and aligned; Third, local transformations are applied to the fingers to refine the final predictions. The proposed recurrent hand pose module for aligned 3D representation can extract recurrent pose-aware features and iteratively refines the estimated hand pose. Our recurrent module could be utilized for both single-view estimation and sequence-based estimation with 3D hand pose tracking. Experiments show that our method improves the state-of-the-art by a large margin on popular benchmarks with the simple yet efficient alignment and network architectures.},
  archive      = {J_TPAMI},
  author       = {Xiaoming Deng and Dexin Zuo and Yinda Zhang and Zhaopeng Cui and Jian Cheng and Ping Tan and Liang Chang and Marc Pollefeys and Sean Fanello and Hongan Wang},
  doi          = {10.1109/TPAMI.2022.3159725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {932-945},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recurrent 3D hand pose estimation using cascaded pose-guided 3D alignments},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time scene text detection with differentiable
binarization and adaptive scale fusion. <em>TPAMI</em>, <em>45</em>(1),
919–931. (<a href="https://doi.org/10.1109/TPAMI.2022.3155612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.},
  archive      = {J_TPAMI},
  author       = {Minghui Liao and Zhisheng Zou and Zhaoyi Wan and Cong Yao and Xiang Bai},
  doi          = {10.1109/TPAMI.2022.3155612},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {919-931},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Real-time scene text detection with differentiable binarization and adaptive scale fusion},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quality metric guided portrait line drawing generation from
unpaired training data. <em>TPAMI</em>, <em>45</em>(1), 905–918. (<a
href="https://doi.org/10.1109/TPAMI.2022.3147570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face portrait line drawing is a unique style of art which is highly abstract and expressive. However, due to its high semantic constraints, many existing methods learn to generate portrait drawings using paired training data, which is costly and time-consuming to obtain. In this paper, we propose a novel method to automatically transform face photos to portrait drawings using unpaired training data with two new features; i.e., our method can (1) learn to generate high quality portrait drawings in multiple styles using a single network and (2) generate portrait drawings in a “new style” unseen in the training data. To achieve these benefits, we (1) propose a novel quality metric for portrait drawings which is learned from human perception, and (2) introduce a quality loss to guide the network toward generating better looking portrait drawings. We observe that existing unpaired translation methods such as CycleGAN tend to embed invisible reconstruction information indiscriminately in the whole drawings due to significant information imbalance between the photo and portrait drawing domains, which leads to important facial features missing. To address this problem, we propose a novel asymmetric cycle mapping that enforces the reconstruction information to be visible and only embedded in the selected facial regions. Along with localized discriminators for important facial regions, our method well preserves all important facial features in the generated drawings. Generator dissection further explains that our model learns to incorporate face semantic information during drawing generation. Extensive experiments including a user study show that our model outperforms state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Ran Yi and Yong-Jin Liu and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TPAMI.2022.3147570},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {905-918},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Quality metric guided portrait line drawing generation from unpaired training data},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PoolNet+: Exploring the potential of pooling for salient
object detection. <em>TPAMI</em>, <em>45</em>(1), 887–904. (<a
href="https://doi.org/10.1109/TPAMI.2021.3140168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the potential of pooling techniques on the task of salient object detection by expanding its role in convolutional neural networks. In general, two pooling-based modules are proposed. A global guidance module (GGM) is first built based on the bottom-up pathway of the U-shape architecture, which aims to guide the location information of the potential salient objects into layers at different feature levels. A feature aggregation module (FAM) is further designed to seamlessly fuse the coarse-level semantic information with the fine-level features in the top-down pathway. We can progressively refine the high-level semantic features with these two modules and obtain detail enriched saliency maps. Experimental results show that our proposed approach can locate the salient objects more accurately with sharpened details and substantially improve the performance compared with the existing state-of-the-art methods. Besides, our approach is fast and can run at a speed of 53 FPS when processing a $300 \times 400$ image. To make our approach better applied to mobile applications, we take MobileNetV2 as our backbone and re-tailor the structure of our pooling-based modules. Our mobile version model achieves a running speed of 66 FPS yet still performs better than most existing state-of-the-art methods. To verify the generalization ability of the proposed method, we apply it to the edge detection, RGB-D salient object detection, and camouflaged object detection tasks, and our method achieves better results than the corresponding state-of-the-art methods of these three tasks. Code can be found at http://mmcheng.net/poolnet/ .},
  archive      = {J_TPAMI},
  author       = {Jiang-Jiang Liu and Qibin Hou and Zhi-Ang Liu and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2021.3140168},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {887-904},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PoolNet+: Exploring the potential of pooling for salient object detection},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point cloud sampling via graph balancing and gershgorin disc
alignment. <em>TPAMI</em>, <em>45</em>(1), 868–886. (<a
href="https://doi.org/10.1109/TPAMI.2022.3143089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud (PC)—a collection of discrete geometric samples of a 3D object’s surface—is typically large, which entails expensive subsequent operations. Thus, PC sub-sampling is of practical importance. Previous model-based sub-sampling schemes are ad-hoc in design and do not preserve the overall shape sufficiently well, while previous data-driven schemes are trained for specific pre-determined input PC sizes and sub-sampling rates and thus do not generalize well. Leveraging advances in graph sampling, we propose a fast PC sub-sampling algorithm of linear time complexity that chooses a 3D point subset while minimizing a global reconstruction error. Specifically, to articulate a sampling objective, we first assume a super-resolution (SR) method based on feature graph Laplacian regularization (FGLR) that reconstructs the original high-res PC, given points chosen by a sampling matrix ${\mathbf H}$ . We prove that minimizing a worst-case SR reconstruction error is equivalent to maximizing the smallest eigenvalue $\lambda _{\min }$ of matrix ${\mathbf H}^{\top } {\mathbf H}+ \mu {\boldsymbol{\mathcal{L}}}$ , where ${\boldsymbol{\mathcal{L}}}$ is a symmetric, positive semi-definite matrix derived from a neighborhood graph connecting the 3D points. To arrive at a fast algorithm, instead of maximizing $\lambda _{\min }$ , we maximize a lower bound $\lambda ^-_{\min }({\mathbf H}^{\top } {\mathbf H}+ \mu {\boldsymbol{\mathcal{L}}})$ via selection of ${\mathbf H}$ —this translates to a graph sampling problem for a signed graph ${\mathcal G}$ with self-loops specified by graph Laplacian ${\boldsymbol{\mathcal{L}}}$ . We tackle this general graph sampling problem in three steps. First, we approximate ${\mathcal G}$ with a balanced graph ${\mathcal G}_B$ specified by Laplacian ${\boldsymbol{\mathcal{L}}}_B$ . Second, leveraging a recent linear algebraic theorem called Gershgorin disc perfect alignment (GDPA), we perform a similarity transform ${\boldsymbol{\mathcal{L}}}_p~=~{\mathbf S}{\boldsymbol{\mathcal{L}}}_B {\mathbf S}^{-1}$ , so that all Gershgorin disc left-ends of ${\boldsymbol{\mathcal{L}}}_p$ are aligned exactly at $\lambda _{\min }({\boldsymbol{\mathcal{L}}}_B)$ . Finally, we choose samples on ${\mathcal G}_B$ using a previous graph sampling algorithm to maximize $\lambda ^-_{\min }({\mathbf H}^{\top } {\mathbf H}+ \mu {\boldsymbol{\mathcal{L}}}_p)$ in linear time. Experimental results show that 3D points chosen by our algorithm outperformed competing schemes both numerically and visually in reconstruction quality.},
  archive      = {J_TPAMI},
  author       = {Chinthaka Dinesh and Gene Cheung and Ivan V. Bajić},
  doi          = {10.1109/TPAMI.2022.3143089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {868-886},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Point cloud sampling via graph balancing and gershgorin disc alignment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PMP-net++: Point cloud completion by transformer-enhanced
multi-step point moving paths. <em>TPAMI</em>, <em>45</em>(1), 852–867.
(<a href="https://doi.org/10.1109/TPAMI.2022.3159003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion concerns to predict missing part for incomplete 3D shapes. A common strategy is to generate complete shape according to incomplete input. However, unordered nature of point clouds will degrade generation of high-quality 3D shapes, as detailed topology and structure of unordered points are hard to be captured during the generative process using an extracted latent code. We address this problem by formulating completion as point cloud deformation process. Specifically, we design a novel neural network, named PMP-Net++, to mimic behavior of an earth mover. It moves each point of incomplete input to obtain a complete point cloud, where total distance of point moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts unique PMP for each point according to constraint of point moving distances. The network learns a strict and unique correspondence on point-level, and thus improves quality of predicted complete shape. Moreover, since moving points heavily relies on per-point features learned by network, we further introduce a transformer-enhanced representation learning network, which significantly improves completion performance of PMP-Net++. We conduct comprehensive experiments in shape completion, and further explore application on point cloud up-sampling, which demonstrate non-trivial improvement of PMP-Net++ over state-of-the-art point cloud completion/up-sampling methods.},
  archive      = {J_TPAMI},
  author       = {Xin Wen and Peng Xiang and Zhizhong Han and Yan-Pei Cao and Pengfei Wan and Wen Zheng and Yu-Shen Liu},
  doi          = {10.1109/TPAMI.2022.3159003},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {852-867},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PMP-net++: Point cloud completion by transformer-enhanced multi-step point moving paths},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAC-bayes meta-learning with implicit task-specific
posteriors. <em>TPAMI</em>, <em>45</em>(1), 841–851. (<a
href="https://doi.org/10.1109/TPAMI.2022.3147798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new and rigorously-formulated PAC-Bayes meta-learning algorithm that solves few-shot learning. Our proposed method extends the PAC-Bayes framework from a single-task setting to the meta-learning multiple-task setting to upper-bound the error evaluated on any, even unseen, tasks and samples. We also propose a generative-based approach to estimate the posterior of task-specific model parameters more expressively compared to the usual assumption based on a multivariate normal distribution with a diagonal covariance matrix. We show that the models trained with our proposed meta-learning algorithm are well-calibrated and accurate, with state-of-the-art calibration errors while still being competitive on classification results on few-shot classification (mini-ImageNet and tiered-ImageNet) and regression (multi-modal task-distribution regression) benchmarks.},
  archive      = {J_TPAMI},
  author       = {Cuong Nguyen and Thanh-Toan Do and Gustavo Carneiro},
  doi          = {10.1109/TPAMI.2022.3147798},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {841-851},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PAC-bayes meta-learning with implicit task-specific posteriors},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood preserving kernels for attributed graphs.
<em>TPAMI</em>, <em>45</em>(1), 828–840. (<a
href="https://doi.org/10.1109/TPAMI.2022.3143806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe the design of a reproducing kernel suitable for attributed graphs, in which the similarity between two graphs is defined based on the neighborhood information of the graph nodes with the aid of a product graph formulation. Attributed graphs are those which contain a piece of vector information and a discrete label over the nodes and edges. We represent the proposed kernel as the weighted sum of two other kernels of which one is an R-convolution kernel that processes the attribute information of the graph and the other is an optimal assignment kernel that processes label information. They are formulated in such a way that the edges processed as part of the kernel computation have the same neighborhood properties and hence the kernel proposed makes a well-defined correspondence between regions processed in graphs. These concepts are also extended to the case of the shortest paths. We identified the state-of-the-art kernels that can be mapped to such a neighborhood preserving framework. We found that the kernel value of the argument graphs in each iteration of the Weisfeiler-Lehman color refinement algorithm can be obtained recursively from the product graph formulated in our method. By incorporating the proposed kernel on support vector machines we analyzed the real-world data sets and it showed superior performance in comparison with that of the other state-of-the-art graph kernels.},
  archive      = {J_TPAMI},
  author       = {Asif Salim and S. S. Shiju and S. Sumitra},
  doi          = {10.1109/TPAMI.2022.3143806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {828-840},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neighborhood preserving kernels for attributed graphs},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MutualNet: Adaptive ConvNet via mutual learning from
different model configurations. <em>TPAMI</em>, <em>45</em>(1), 811–827.
(<a href="https://doi.org/10.1109/TPAMI.2021.3138389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep neural networks are static, which means they can only perform inference at a fixed complexity. But the resource budget can vary substantially across different devices. Even on a single device, the affordable budget can change with different scenarios, and repeatedly training networks for each required budget would be incredibly expensive. Therefore, in this work, we propose a general method called MutualNet to train a single network that can run at a diverse set of resource constraints. Our method trains a cohort of model configurations with various network widths and input resolutions. This mutual learning scheme not only allows the model to run at different width-resolution configurations but also transfers the unique knowledge among these configurations, helping the model to learn stronger representations overall. MutualNet is a general training methodology that can be applied to various network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks: SlowFast, X3D) and various tasks (e.g., image classification, object detection, segmentation, and action recognition), and is demonstrated to achieve consistent improvements on a variety of datasets. Since we only train the model once, it also greatly reduces the training cost compared to independently training several models. Surprisingly, MutualNet can also be used to significantly boost the performance of a single network, if dynamic resource constraints are not a concern. In summary, MutualNet is a unified method for both static and adaptive, 2D and 3D networks. Code and pre-trained models are available at https://github.com/taoyang1122/MutualNet .},
  archive      = {J_TPAMI},
  author       = {Taojiannan Yang and Sijie Zhu and Matias Mendieta and Pu Wang and Ravikumar Balakrishnan and Minwoo Lee and Tao Han and Mubarak Shah and Chen Chen},
  doi          = {10.1109/TPAMI.2021.3138389},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {811-827},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MutualNet: Adaptive ConvNet via mutual learning from different model configurations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSeg: A composite dataset for multi-domain semantic
segmentation. <em>TPAMI</em>, <em>45</em>(1), 796–810. (<a
href="https://doi.org/10.1109/TPAMI.2022.3151200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MSeg, a composite dataset that unifies semantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images, requiring more than 1.34 years of collective annotator effort. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model&#39;s robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash-v1 leaderboard for robust semantic segmentation, with no exposure to WildDash data during training. We evaluate our models in the 2020 Robust Vision Challenge (RVC) as an extreme generalization experiment. MSeg training sets include only three of the seven datasets in the RVC; more importantly, the evaluation taxonomy of RVC is different and more detailed. Surprisingly, our model shows competitive performance and ranks second. To evaluate how close we are to the grand aim of robust, efficient, and complete scene understanding, we go beyond semantic segmentation by training instance segmentation and panoptic segmentation models using our dataset. Moreover, we also evaluate various engineering design decisions and metrics, including resolution and computational efficiency. Although our models are far from this grand aim, our comprehensive evaluation is crucial for progress. We share all the models and code with the community.},
  archive      = {J_TPAMI},
  author       = {John Lambert and Zhuang Liu and Ozan Sener and James Hays and Vladlen Koltun},
  doi          = {10.1109/TPAMI.2022.3151200},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {796-810},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MSeg: A composite dataset for multi-domain semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low cost and latency event camera background activity
denoising. <em>TPAMI</em>, <em>45</em>(1), 785–795. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Vision Sensor (DVS) event camera output includes uninformative background activity (BA) noise events that increase dramatically under dim lighting. Existing denoising algorithms are not effective under these high noise conditions. Furthermore, it is difficult to quantitatively compare algorithm accuracy. This paper proposes a novel framework to better quantify BA denoising algorithms by measuring receiver operating characteristics with known mixtures of signal and noise DVS events. New datasets for stationary and moving camera applications of DVS in surveillance and driving are used to compare 3 new low-cost algorithms: Algorithm 1 checks distance to past events using a tiny fixed size window and removes most of the BA while preserving most of the signal for stationary camera scenarios. Algorithm 2 uses a memory proportional to the number of pixels for improved correlation checking. Compared with existing methods, it removes more noise while preserving more signal. Algorithm 3 uses a lightweight multilayer perceptron classifier driven by local event time surfaces to achieve the best accuracy over all datasets. The code and data are shared with the paper as DND21.},
  archive      = {J_TPAMI},
  author       = {Shasha Guo and Tobi Delbruck},
  doi          = {10.1109/TPAMI.2022.3152999},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {785-795},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Low cost and latency event camera background activity denoising},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local and global GANs with semantic-aware upsampling for
image generation. <em>TPAMI</em>, <em>45</em>(1), 768–784. (<a
href="https://doi.org/10.1109/TPAMI.2022.3155989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the task of semantic-guided image generation. One challenge common to most existing image-level generation methods is the difficulty in generating small objects and detailed local textures. To address this, in this work we consider generating images using local context. As such, we design a local class-specific generative network using semantic maps as guidance, which separately constructs and learns subgenerators for different classes, enabling it to capture finer details. To learn more discriminative class-specific feature representations for the local generation, we also propose a novel classification module. To combine the advantages of both global image-level and local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Lastly, we propose a novel semantic-aware upsampling method, which has a larger receptive field and can take far-away pixels that are semantically related for feature upsampling, enabling it to better preserve semantic consistency for instances with the same semantic labels. Extensive experiments on two image generation tasks show the superior performance of the proposed method. State-of-the-art results are established by large margins on both tasks and on nine challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN .},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Ling Shao and Philip H.S. Torr and Nicu Sebe},
  doi          = {10.1109/TPAMI.2022.3155989},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {768-784},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Local and global GANs with semantic-aware upsampling for image generation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LSV-LP: Large-scale video-based license plate detection and
recognition. <em>TPAMI</em>, <em>45</em>(1), 752–767. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few decades, license plate detection and recognition (LPDR) systems have made great strides relying on Convolutional Neural Networks (CNN). However, these methods are evaluated on small and non-representative datasets that perform poorly in complex natural scenes. Besides, most of existing license plate datasets are based on a single image, while the information source in the actual application of license plates is frequently based on video. The mainstream algorithms also ignore the dynamic clue between consecutive frames in the video, which makes the LPDR system have a lot of room for improvement. In order to solve these problems, this paper constructs a large-scale video-based license plate dataset named LSV-LP, which consists of 1,402 videos, 401,347 frames and 364,607 annotated license plates. Compared with other data sets, LSV-LP has stronger diversity, and at the same time, it has multiple sources due to different collection methods. There may be multiple license plates in a frame, which is more in line with complex natural scenes. Based on the proposed dataset, we further design a new framework that explores the information between adjacent frames, called MFLPR-Net. In addition to these, we release the annotation tools for license plates or vehicles in videos. By evaluating the performance of MFLPR-Net and some mainstream methods, it is proved that the proposed model is superior to other LPDR systems.In order to be more intuitive, we put some samples on https://drive.google.com/file/d/1udqRddpJZMpTdHHQdwZRll6vaYALUiql/view?usp=sharing Google Drive. The whole dataset is available at https://github.com/Forest-art/LSV-LP .},
  archive      = {J_TPAMI},
  author       = {Qi Wang and Xiaocheng Lu and Cong Zhang and Yuan Yuan and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3153691},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {752-767},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LSV-LP: Large-scale video-based license plate detection and recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to guide a saturation-based theorem prover.
<em>TPAMI</em>, <em>45</em>(1), 738–751. (<a
href="https://doi.org/10.1109/TPAMI.2022.3140382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional automated theorem provers have relied on manually tuned heuristics to guide how they perform proof search. Recently, however, there has been a surge of interest in the design of learning mechanisms that can be integrated into theorem provers to improve their performance automatically. In this work, we describe TRAIL (Trial Reasoner for AI that Learns), a deep learning-based approach to theorem proving that characterizes core elements of saturation-based theorem proving within a neural framework. TRAIL leverages (a) an effective graph neural network for representing logical formulas, (b) a novel neural representation of the state of a saturation-based theorem prover in terms of processed clauses and available actions, and (c) a novel representation of the inference selection process as an attention-based action policy. We show through a systematic analysis that these components allow TRAIL to significantly outperform previous reinforcement learning-based theorem provers on two standard benchmark datasets (up to 36\% more theorems proved). In addition, to the best of our knowledge, TRAIL is the first reinforcement learning-based approach to exceed the performance of a state-of-the-art traditional theorem prover on a standard theorem proving benchmark (solving up to 17\% more theorems).},
  archive      = {J_TPAMI},
  author       = {Ibrahim Abdelaziz and Maxwell Crouse and Bassem Makni and Vernon Austel and Cristina Cornelio and Shajith Ikbal and Pavan Kapanipathi and Ndivhuwo Makondo and Kavitha Srinivas and Michael Witbrock and Achille Fokoue},
  doi          = {10.1109/TPAMI.2022.3140382},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {738-751},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to guide a saturation-based theorem prover},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning representations by graphical mutual information
estimation and maximization. <em>TPAMI</em>, <em>45</em>(1), 722–737.
(<a href="https://doi.org/10.1109/TPAMI.2022.3147886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rich content in various real-world networks such as social networks, biological networks, and communication networks provides unprecedented opportunities for unsupervised machine learning on graphs. This paper investigates the fundamental problem of preserving and extracting abundant information from graph-structured data into embedding space without external supervision. To this end, we generalize conventional mutual information computation from vector space to graph domain and present a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graph and hidden representation. Except for standard GMI which considers graph structures from a local perspective, our further proposed GMI++ additionally captures global topological properties by analyzing the co-occurrence relationship of nodes. GMI and its extension exhibit several benefits: First, they are invariant to the isomorphic transformation of input graphs—an inevitable constraint in many existing methods; Second, they can be efficiently estimated and maximized by current mutual information estimation methods; Lastly, our theoretical analysis confirms their correctness and rationality. With the aid of GMI, we develop an unsupervised embedding model and adapt it to the specific anomaly detection task. Extensive experiments indicate that our GMI methods achieve promising performance in various downstream tasks, such as node classification, link prediction, and anomaly detection.},
  archive      = {J_TPAMI},
  author       = {Zhen Peng and Minnan Luo and Wenbing Huang and Jundong Li and Qinghua Zheng and Fuchun Sun and Junzhou Huang},
  doi          = {10.1109/TPAMI.2022.3147886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {722-737},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning representations by graphical mutual information estimation and maximization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning polymorphic neural ODEs with time-evolving mixture.
<em>TPAMI</em>, <em>45</em>(1), 712–721. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural ordinary differential equations (NODE) present a new way of considering a deep residual network as a continuous structure by layer depth. However, it fails to overcome its representational limits, where it cannot learn all possible homeomorphisms of input data space, and therefore quickly saturates in terms of performance even as the number of layers increases. Here, we show that simply stacking Neural ODE blocks could easily improve performance by alleviating this issue. Furthermore, we suggest a more effective way of training neural ODE by using a time-evolving mixture weight on multiple ODE functions that also evolves with a separate neural ODE. We provide empirical results that are suggestive of improved performance over stacked as well as vanilla neural ODEs where we also confirm our approach can be orthogonally combined with recent advances in neural ODEs.},
  archive      = {J_TPAMI},
  author       = {Tehrim Yoon and Sumin Shin and Eunho Yang},
  doi          = {10.1109/TPAMI.2022.3145013},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {712-721},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning polymorphic neural ODEs with time-evolving mixture},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Key.net: Keypoint detection by handcrafted and learned CNN
filters revisited. <em>TPAMI</em>, <em>45</em>(1), 698–711. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel approach for keypoint detection that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score, and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches and other benchmarks. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance, and complexity. Key.Net implementations in TensorFlow and PyTorch are available online.},
  archive      = {J_TPAMI},
  author       = {Axel Barroso-Laguna and Krystian Mikolajczyk},
  doi          = {10.1109/TPAMI.2022.3145820},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {698-711},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Key.Net: Keypoint detection by handcrafted and learned CNN filters revisited},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating pose representations and motion contexts
modeling for 3D motion prediction. <em>TPAMI</em>, <em>45</em>(1),
681–697. (<a href="https://doi.org/10.1109/TPAMI.2021.3139918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting human motion from historical pose sequence is crucial for a machine to succeed in intelligent interactions with humans. One aspect that has been obviated so far, is the fact that how we represent the skeletal pose has a critical impact on the prediction results. Yet there is no effort that investigates across different pose representation schemes. We conduct an indepth study on various pose representations with a focus on their effects on the motion prediction task. Moreover, recent approaches build upon off-the-shelf RNN units for motion prediction. These approaches process input pose sequence sequentially and inherently have difficulties in capturing long-term dependencies. In this paper, we propose a novel RNN architecture termed AHMR (Attentive Hierarchical Motion Recurrent network) for motion prediction which simultaneously models local motion contexts and a global context. We further explore a geodesic loss and a forward kinematics loss for the motion prediction task, which have more geometric significance than the widely employed L2 loss. Interestingly, we applied our method to a range of articulate objects including human, fish, and mouse. Empirical results show that our approach outperforms the state-of-the-art methods in short-term prediction and achieves much enhanced long-term prediction proficiency, such as retaining natural human-like motions over 50 seconds predictions. Our codes are released.},
  archive      = {J_TPAMI},
  author       = {Zhenguang Liu and Shuang Wu and Shuyuan Jin and Shouling Ji and Qi Liu and Shijian Lu and Li Cheng},
  doi          = {10.1109/TPAMI.2021.3139918},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {681-697},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Investigating pose representations and motion contexts modeling for 3D motion prediction},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance and panoptic segmentation using conditional
convolutions. <em>TPAMI</em>, <em>45</em>(1), 669–680. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple yet effective framework for instance and panoptic segmentation, termed CondInst (conditional convolutions for instance and panoptic segmentation). In the literature, top-performing instance segmentation methods typically follow the paradigm of Mask R-CNN and rely on ROI operations (typically ROIAlign) to attend to each instance. In contrast, we propose to attend to the instances with dynamic conditional convolutions. Instead of using instance-wise ROIs as inputs to the instance mask head of fixed weights, we design dynamic instance-aware mask heads, conditioned on the instances to be predicted. CondInst enjoys three advantages: 1) Instance and panoptic segmentation are unified into a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) The elimination of the ROI cropping also significantly improves the output instance mask resolution. 3) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference time per instance and making the overall inference time less relevant to the number of instances. We demonstrate a simpler method that can achieve improved accuracy and inference speed on both instance and panoptic segmentation tasks. On the COCO dataset, we outperform a few state-of-the-art methods. We hope that CondInst can be a strong baseline for instance and panoptic segmentation. Code is available at: https://git.io/AdelaiDet .},
  archive      = {J_TPAMI},
  author       = {Zhi Tian and Bowen Zhang and Hao Chen and Chunhua Shen},
  doi          = {10.1109/TPAMI.2022.3145407},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {669-680},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Instance and panoptic segmentation using conditional convolutions},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving graph neural network expressivity via subgraph
isomorphism counting. <em>TPAMI</em>, <em>45</em>(1), 657–668. (<a
href="https://doi.org/10.1109/TPAMI.2022.3154319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often intimately related to downstream tasks. To this end, we propose ”Graph Substructure Networks” (GSN), a topologically-aware message passing scheme based on substructure encoding. We theoretically analyse the expressive power of our architecture, showing that it is strictly more expressive than the WL test, and provide sufficient conditions for universality. Importantly, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We perform an extensive experimental evaluation on graph classification and regression tasks and obtain state-of-the-art results in diverse real-world settings including molecular graphs and social networks.},
  archive      = {J_TPAMI},
  author       = {Giorgos Bouritsas and Fabrizio Frasca and Stefanos Zafeiriou and Michael M. Bronstein},
  doi          = {10.1109/TPAMI.2022.3154319},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {657-668},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving graph neural network expressivity via subgraph isomorphism counting},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-text embedding learning via visual and textual
semantic reasoning. <em>TPAMI</em>, <em>45</em>(1), 641–656. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a bridge between language and vision domains, cross-modal retrieval between images and texts is a hot research topic in recent years. It remains challenging because the current image representations usually lack semantic concepts in the corresponding sentence captions. To address this issue, we introduce an intuitive and interpretable model to learn a common embedding space for alignments between images and text descriptions. Specifically, our model first incorporates the semantic relationship information into visual and textual features by performing region or word relationship reasoning. Then it utilizes the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually grow representations for the whole scene. Through the alignment learning, the learned visual representations capture key objects and semantic concepts of a scene as in the corresponding text caption. Experiments on MS-COCO [1] and Flickr30K [2] datasets validate that our method surpasses many recent state-of-the-arts with a clear margin. In addition to the effectiveness, our methods are also very efficient at the inference stage. Thanks to the effective overall representation learning with visual semantic reasoning, our methods can already achieve very strong performance by only relying on the simple inner-product to obtain similarity scores between images and captions. Experiments validate the proposed methods are more than 30-75 times faster than many recent methods with code public available. Instead of following the recent trend of using complex local matching strategies [3], [4], [5], [6] to pursue good performance while sacrificing efficiency, we show that the simple global matching strategy can still be very effective, efficient and achieve even better performance based on our framework.},
  archive      = {J_TPAMI},
  author       = {Kunpeng Li and Yulun Zhang and Kai Li and Yuanyuan Li and Yun Fu},
  doi          = {10.1109/TPAMI.2022.3148470},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {641-656},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image-text embedding learning via visual and textual semantic reasoning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HUMBI: A large multiview dataset of human body expressions
and benchmark challenge. <em>TPAMI</em>, <em>45</em>(1), 623–640. (<a
href="https://doi.org/10.1109/TPAMI.2021.3138762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new large multiview dataset called HUMBI for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of five primary body signals including gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and style. With the multiview image streams, we reconstruct the geometry of body expressions using 3D mesh models, which allows representing view-specific appearance. We demonstrate that HUMBI is highly effective in learning and reconstructing a complete human model and is complementary to the existing datasets of human body expressions with limited views and subjects such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI, we formulate a new benchmark challenge of a pose-guided appearance rendering task that aims to substantially extend photorealism in modeling diverse human expressions in 3D, which is the key enabling factor of authentic social tele-presence. HUMBI is publicly available at http://humbi-data.net .},
  archive      = {J_TPAMI},
  author       = {Jae Shin Yoon and Zhixuan Yu and Jaesik Park and Hyun Soo Park},
  doi          = {10.1109/TPAMI.2021.3138762},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {623-640},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HUMBI: A large multiview dataset of human body expressions and benchmark challenge},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher-order multicuts for geometric model fitting and
motion segmentation. <em>TPAMI</em>, <em>45</em>(1), 608–622. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum cost lifted multicut problem is a generalization of the multicut problem (also known as correlation clustering) and is a means to optimizing a decomposition of a graph w.r.t. both positive and negative edge costs. It has been shown to be useful in a large variety of applications in computer vision thanks to the fact that multicut-based formulations do not require the number of components given a priori; instead, it is deduced from the solution. However, the standard multicut cost function is limited to pairwise relationships between nodes, while several important applications either require or can benefit from a higher-order cost function, i.e., hyper-edges. In this paper, we propose a pseudo-boolean formulation for a multiple model fitting problem. It is based on a formulation of any-order minimum cost lifted multicuts, which allows to partition an undirected graph with pairwise connectivity such as to minimize costs defined over any set of hyper-edges. As the proposed formulation is np -hard and the branch-and-bound algorithm (as well as obtaining lower bounds) is too slow in practice, we propose an efficient local search algorithm for inference into resulting problems. We demonstrate versatility and effectiveness of our approach in several applications: 1) We define a geometric multiple model fitting, more specifically, a line fitting problem on all triplets of points and group points, that belong to the same line, together. 2) We formulate homography and motion estimation as a geometric model fitting problem where the task is to find groups of points that can be explained by the same geometrical transformation. 3) In motion segmentation our model allows to go from modeling translational motion to euclidean or affine transformations, which improves the segmentation quality in terms of F-measure.},
  archive      = {J_TPAMI},
  author       = {Evgeny Levinkov and Amirhossein Kardoost and Bjoern Andres and Margret Keuper},
  doi          = {10.1109/TPAMI.2022.3148795},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {608-622},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Higher-order multicuts for geometric model fitting and motion segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guiding labelling effort for efficient learning with
georeferenced images. <em>TPAMI</em>, <em>45</em>(1), 593–607. (<a
href="https://doi.org/10.1109/TPAMI.2021.3140060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel semi-supervised learning method that reduces the labelling effort needed to train convolutional neural networks (CNNs) when processing georeferenced imagery. This allows deep learning CNNs to be trained on a per-dataset basis, which is useful in domains where there is limited learning transferability across datasets. The method identifies representative subsets of images from an unlabelled dataset based on the latent representation of a location guided autoencoder. We assess the method&#39;s sensitivities to design options using four different ground-truthed datasets of georeferenced environmental monitoring images, where these include various scenes in aerial and seafloor imagery. Efficiency gains are achieved for all the aerial and seafloor image datasets analysed in our experiments, demonstrating the benefit of the method across application domains. Compared to CNNs of the same architecture trained using conventional transfer and active learning, the method achieves equivalent accuracy with an order of magnitude fewer annotations, and 85\% of the accuracy of CNNs trained conventionally with approximately 10,000 human annotations using just 40 prioritised annotations. The biggest gains in efficiency are seen in datasets with unbalanced class distributions and rare classes that have a relatively small number of observations.},
  archive      = {J_TPAMI},
  author       = {Takaki Yamada and Miquel Massot-Campos and Adam Prügel-Bennett and Oscar Pizarro and Stefan B. Williams and Blair Thornton},
  doi          = {10.1109/TPAMI.2021.3140060},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {593-607},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guiding labelling effort for efficient learning with georeferenced images},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Global instance tracking: Locating target more like humans.
<em>TPAMI</em>, <em>45</em>(1), 576–592. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target tracking, the essential ability of the human visual system, has been simulated by computer vision tasks. However, existing trackers perform well in austere experimental environments but fail in challenges like occlusion and fast motion. The massive gap indicates that researches only measure tracking performance rather than intelligence. How to scientifically judge the intelligence level of trackers? Distinct from decision-making problems, lacking three requirements (a challenging task, a fair environment, and a scientific evaluation procedure) makes it strenuous to answer the question. In this article, we first propose the global instance tracking (GIT) task, which is supposed to search an arbitrary user-specified instance in a video without any assumptions about camera or motion consistency, to model the human visual tracking ability. Whereafter, we construct a high-quality and large-scale benchmark VideoCube to create a challenging environment. Finally, we design a scientific evaluation procedure using human capabilities as the baseline to judge tracking intelligence. Additionally, we provide an online platform with toolkit and an updated leaderboard. Although the experimental results indicate a definite gap between trackers and humans, we expect to take a step forward to generate authentic human-like trackers. The database, toolkit, evaluation server, and baseline results are available at http://videocube.aitestunion.com .},
  archive      = {J_TPAMI},
  author       = {Shiyu Hu and Xin Zhao and Lianghua Huang and Kaiqi Huang},
  doi          = {10.1109/TPAMI.2022.3153312},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {576-592},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global instance tracking: Locating target more like humans},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSGANv2: Improved subject agnostic face swapping and
reenactment. <em>TPAMI</em>, <em>45</em>(1), 560–575. (<a
href="https://doi.org/10.1109/TPAMI.2022.3155571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, we offer a subject agnostic swapping scheme that can be applied to pairs of faces without requiring training on those faces. We derive a novel iterative deep learning–based approach for face reenactment which adjusts significant pose and expression variations that can be applied to a single image or a video sequence. For video sequences, we introduce a continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving the target skin color and lighting conditions. This network uses a novel Poisson blending loss combining Poisson optimization with a perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior. This work describes extensions of the FSGAN method, proposed in an earlier conference version of our work (Nirkin et al. 2019), as well as additional experiments and results.},
  archive      = {J_TPAMI},
  author       = {Yuval Nirkin and Yosi Keller and Tal Hassner},
  doi          = {10.1109/TPAMI.2022.3155571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {560-575},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FSGANv2: Improved subject agnostic face swapping and reenactment},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From show to tell: A survey on deep learning-based image
captioning. <em>TPAMI</em>, <em>45</em>(1), 539–559. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.},
  archive      = {J_TPAMI},
  author       = {Matteo Stefanini and Marcella Cornia and Lorenzo Baraldi and Silvia Cascianelli and Giuseppe Fiameni and Rita Cucchiara},
  doi          = {10.1109/TPAMI.2022.3148210},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {539-559},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From show to tell: A survey on deep learning-based image captioning},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair representation: Guaranteeing approximate multiple group
fairness for unknown tasks. <em>TPAMI</em>, <em>45</em>(1), 525–538. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by scenarios where data is used for diverse prediction tasks, we study whether fair representation can be used to guarantee fairness for unknown tasks and for multiple fairness notions . We consider seven group fairness notions that cover the concepts of independence, separation, and calibration. Against the backdrop of the fairness impossibility results, we explore approximate fairness. We prove that, although fair representation might not guarantee fairness for all prediction tasks, it does guarantee fairness for an important subset of tasks—the tasks for which the representation is discriminative. Specifically, all seven group fairness notions are linearly controlled by fairness and discriminativeness of the representation. When an incompatibility exists between different fairness notions, fair and discriminative representation hits the sweet spot that approximately satisfies all notions. Motivated by our theoretical findings, we propose to learn both fair and discriminative representations using pretext loss which self-supervises learning, and Maximum Mean Discrepancy as a fair regularizer. Experiments on tabular, image, and face datasets show that using the learned representation, downstream predictions that we are unaware of when learning the representation indeed become fairer. The fairness guarantees computed from our theoretical results are all valid.},
  archive      = {J_TPAMI},
  author       = {Xudong Shen and Yongkang Wong and Mohan Kankanhalli},
  doi          = {10.1109/TPAMI.2022.3148905},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {525-538},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fair representation: Guaranteeing approximate multiple group fairness for unknown tasks},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). End-to-end handwritten paragraph text recognition using a
vertical attention network. <em>TPAMI</em>, <em>45</em>(1), 508–524. (<a
href="https://doi.org/10.1109/TPAMI.2022.3144899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unconstrained handwritten text recognition remains challenging for computer vision systems. Paragraph text recognition is traditionally achieved by two models: the first one for line segmentation and the second one for text line recognition. We propose a unified end-to-end model using hybrid attention to tackle this task. This model is designed to iteratively process a paragraph image line by line. It can be split into three modules. An encoder generates feature maps from the whole paragraph image. Then, an attention module recurrently generates a vertical weighted mask enabling to focus on the current text line features. This way, it performs a kind of implicit line segmentation. For each text line features, a decoder module recognizes the character sequence associated, leading to the recognition of a whole paragraph. We achieve state-of-the-art character error rate at paragraph level on three popular datasets: 1.91\% for RIMES, 4.45\% for IAM and 3.59\% for READ 2016. Our code and trained model weights are available at https://github.com/FactoDeepLearning/VerticalAttentionOCR .},
  archive      = {J_TPAMI},
  author       = {Denis Coquenet and Clément Chatelain and Thierry Paquet},
  doi          = {10.1109/TPAMI.2022.3144899},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {508-524},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {End-to-end handwritten paragraph text recognition using a vertical attention network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Egocentric action recognition by automatic relation
modeling. <em>TPAMI</em>, <em>45</em>(1), 489–507. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric videos, which record the daily activities of individuals from a first-person point of view, have attracted increasing attention during recent years because of their growing use in many popular applications, including life logging, health monitoring and virtual reality. As a fundamental problem in egocentric vision, one of the tasks of egocentric action recognition aims to recognize the actions of the camera wearers from egocentric videos. In egocentric action recognition, relation modeling is important, because the interactions between the camera wearer and the recorded persons or objects form complex relations in egocentric videos. However, only a few of existing methods model the relations between the camera wearer and the interacting persons for egocentric action recognition, and moreover they require prior knowledge or auxiliary data to localize the interacting persons. In this work, we consider modeling the relations in a weakly supervised manner, i.e., without using annotations or prior knowledge about the interacting persons or objects, for egocentric action recognition. We form a weakly supervised framework by unifying automatic interactor localization and explicit relation modeling for the purpose of automatic relation modeling. First, we learn to automatically localize the interactors, i.e., the body parts of the camera wearer and the persons or objects that the camera wearer interacts with, by learning a series of keypoints directly from video data to localize the action-relevant regions with only action labels and some constraints on these keypoints. Second, more importantly, to explicitly model the relations between the interactors, we develop an ego-relational LSTM (long short-term memory) network with several candidate connections to model the complex relations in egocentric videos, such as the temporal, interactive, and contextual relations. In particular, to reduce human efforts and manual interventions needed to construct an optimal ego-relational LSTM structure, we search for the optimal connections by employing a differentiable network architecture search mechanism, which automatically constructs the ego-relational LSTM network to explicitly model different relations for egocentric action recognition. We conduct extensive experiments on egocentric video datasets to illustrate the effectiveness of our method.},
  archive      = {J_TPAMI},
  author       = {Haoxin Li and Wei-Shi Zheng and Jianguo Zhang and Haifeng Hu and Jiwen Lu and Jian-Huang Lai},
  doi          = {10.1109/TPAMI.2022.3148790},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {489-507},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Egocentric action recognition by automatic relation modeling},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient variational bayes learning of graphical models
with smooth structural changes. <em>TPAMI</em>, <em>45</em>(1), 475–488.
(<a href="https://doi.org/10.1109/TPAMI.2022.3140886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating a sequence of dynamic undirected graphical models, in which adjacent graphs share similar structures, is of paramount importance in various social, financial, biological, and engineering systems, since the evolution of such networks can be utilized for example to spot trends, detect anomalies, predict vulnerability, and evaluate the impact of interventions. Existing methods for learning dynamic graphical models require the tuning parameters that control the graph sparsity and the temporal smoothness to be selected via brute-force grid search. Furthermore, these methods are computationally burdensome with time complexity $\mathcal {O}(NP^3)$ for $P$ variables and $N$ time points. As a remedy, we propose a low-complexity tuning-free Bayesian approach, named BASS. Specifically, we impose temporally dependent spike and slab priors on the graphs such that they are sparse and varying smoothly across time. An efficient variational inference algorithm based on natural gradients is then derived to learn the graph structures from the data in an automatic manner. Owing to the pseudo-likelihood and the mean-field approximation, the time complexity of BASS is only $\mathcal {O}(NP^2)$ . To cope with the local maxima problem of variational inference, we resort to simulated annealing and propose a method based on bootstrapping of the observations to generate the annealing noise. We provide numerical evidence that BASS outperforms existing methods on synthetic data in terms of structure estimation, while being more efficient especially when the dimension $P$ becomes high. We further apply the approach to the stock return data of 78 banks from 2005 to 2013 and find that the number of edges in the financial network as a function of time contains three peaks, in coincidence with the 2008 global financial crisis and the two subsequent European debt crisis. On the other hand, by identifying the frequency-domain resemblance to the time-varying graphical models, we show that BASS can be extended to learning frequency-varying inverse spectral density matrices, and further yields graphical models for multivariate stationary time series. As an illustration, we analyze scalp EEG signals of patients at the early stages of Alzheimer’s disease (AD) and show that the brain networks extracted by BASS can better distinguish between the patients and the healthy controls.},
  archive      = {J_TPAMI},
  author       = {Hang Yu and Songwei Wu and Justin Dauwels},
  doi          = {10.1109/TPAMI.2022.3140886},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {475-488},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient variational bayes learning of graphical models with smooth structural changes},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective local and global search for fast long-term
tracking. <em>TPAMI</em>, <em>45</em>(1), 460–474. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with short-term tracking, long-term tracking remains a challenging task that usually requires the tracking algorithm to track targets within a local region and re-detect targets over the entire image. However, few works have been done and their performances have also been limited. In this paper, we present a novel robust and real-time long-term tracking framework based on the proposed local search module and re-detection module. The local search module consists of an effective bounding box regressor to generate a series of candidate proposals and a target verifier to infer the optimal candidate with its confidence score. For local search, we design a long short-term updated scheme to improve the target verifier. The verification capability of the tracker can be improved by using several templates updated at different times. Based on the verification scores, our tracker determines whether the tracked object is present or absent and then chooses the tracking strategies of local or global search, respectively, in the next frame. For global re-detection, we develop a novel re-detection module that can estimate the target position and target size for a given base tracker. We conduct a series of experiments to demonstrate that this module can be flexibly integrated into many other tracking algorithms for long-term tracking and that it can improve long-term tracking performance effectively. Numerous experiments and discussions are conducted on several popular tracking datasets, including VOT, OxUvA, TLP, and LaSOT. The experimental results demonstrate that the proposed tracker achieves satisfactory performance with a real-time speed. Code is available at https://github.com/difhnp/ELGLT .},
  archive      = {J_TPAMI},
  author       = {Haojie Zhao and Bin Yan and Dong Wang and Xuesheng Qian and Xiaoyun Yang and Huchuan Lu},
  doi          = {10.1109/TPAMI.2022.3153645},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {460-474},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Effective local and global search for fast long-term tracking},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DoTA: Unsupervised detection of traffic anomaly in driving
videos. <em>TPAMI</em>, <em>45</em>(1), 444–459. (<a
href="https://doi.org/10.1109/TPAMI.2022.3150763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) has been extensively studied for static cameras but is much more challenging in egocentric driving videos where the scenes are extremely dynamic. This paper proposes an unsupervised method for traffic VAD based on future object localization. The idea is to predict future locations of traffic participants over a short horizon, and then monitor the accuracy and consistency of these predictions as evidence of an anomaly. Inconsistent predictions tend to indicate an anomaly has occurred or is about to occur. To evaluate our method, we introduce a new large-scale benchmark dataset called Detection of Traffic Anomaly (DoTA)containing 4,677 videos with temporal, spatial, and categorical annotations. We also propose a new VAD evaluation metric, called spatial-temporal area under curve (STAUC), and show that it captures how well a model detects both temporal and spatial locations of anomalies unlike existing metrics that focus only on temporal localization. Experimental results show our method outperforms state-of-the-art methods on DoTA in terms of both metrics. We offer rich categorical annotations in DoTA to benchmark video action detection and online action detection methods. The DoTA dataset has been made available at: https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly},
  archive      = {J_TPAMI},
  author       = {Yu Yao and Xizi Wang and Mingze Xu and Zelin Pu and Yuchen Wang and Ella Atkins and David J. Crandall},
  doi          = {10.1109/TPAMI.2022.3150763},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {444-459},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DoTA: Unsupervised detection of traffic anomaly in driving videos},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Disentangling light fields for super-resolution and
disparity estimation. <em>TPAMI</em>, <em>45</em>(1), 425–443. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Specifically, we first design a class of domain-specific convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-specific modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. Project page: https://yingqianwang.github.io/DistgLF/ .},
  archive      = {J_TPAMI},
  author       = {Yingqian Wang and Longguang Wang and Gaochang Wu and Jungang Yang and Wei An and Jingyi Yu and Yulan Guo},
  doi          = {10.1109/TPAMI.2022.3152488},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {425-443},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Disentangling light fields for super-resolution and disparity estimation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Disentangled representation learning for recommendation.
<em>TPAMI</em>, <em>45</em>(1), 408–424. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist complex interactions among a large number of latent factors behind the decision making processes of different individuals, which drive the various user behavior patterns in recommender systems. These factors hidden in those diverse behaviors demonstrate highly entangled patterns, covering from high-level user intentions to low-level individual preferences. Uncovering the disentanglement of these latent factors can benefit in enhanced robustness, interpretability, and controllability during representation learning for recommendation. However, the large degree of entanglement within latent factors poses great challenges for learning representations that disentangle them, and remains largely unexplored in literature. In this paper, we present the SEMantic MACRo-mIcro Disentangled Variational Auto-Encoder (SEM-MacridVAE) model for learning disentangled representations from user behaviors, taking item semantic information into account. Our SEM-MacridVAE model achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a pair of shoes or a laptop) through a prototype routing mechanism, as well as capturing the individual preferences with respect to different concepts separately. The micro disentanglement is guaranteed through a micro-disentanglement regularizer stemming from an information-theoretic interpretation of VAEs, which forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). The semantic information including visual and categorical signals extracted from candidate items is utilized to further boost the recommendation performance of the proposed SEM-MacridVAE model. Empirical experiments demonstrate that our proposed approach is able to achieve significant improvement over the state-of-the-art baselines. We also show that the learned representations are interpretable and controllable, capable of potentially leading to a new paradigm for recommendation where users have fine-grained control over some target aspects of the recommendation candidates.},
  archive      = {J_TPAMI},
  author       = {Xin Wang and Hong Chen and Yuwei Zhou and Jianxin Ma and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2022.3153112},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {408-424},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Disentangled representation learning for recommendation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deformable protein shape classification based on deep
learning, and the fractional fokker–planck and kähler–dirac equations.
<em>TPAMI</em>, <em>45</em>(1), 391–407. (<a
href="https://doi.org/10.1109/TPAMI.2022.3146796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of deformable protein shapes, based solely on their macromolecular surfaces, is a challenging problem in protein–protein interaction prediction and protein design. Shape classification is made difficult by the fact that proteins are dynamic, flexible entities with high geometrical complexity. In this paper, we introduce a novel description for such deformable shapes. This description is based on the bifractional Fokker–Planck and Dirac–Kähler equations. These equations analyse and probe protein shapes in terms of a scalar, vectorial and non-commuting quaternionic field, allowing for a more comprehensive description of the protein shapes. An underlying non-Markovian Lévy random walk establishes geometrical relationships between distant regions while recalling previous analyses. Classification is performed with a multiobjective deep hierarchical pyramidal neural network, thus performing a multilevel analysis of the description. Our approach is applied to the SHREC’19 dataset for deformable protein shapes classification and to the SHREC’16 dataset for deformable partial shapes classification, demonstrating the effectiveness and generality of our approach.},
  archive      = {J_TPAMI},
  author       = {Eric Paquet and Herna L. Viktor and Kamel Madi and Junzheng Wu},
  doi          = {10.1109/TPAMI.2022.3146796},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {391-407},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deformable protein shape classification based on deep learning, and the fractional Fokker–Planck and Kähler–Dirac equations},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepMIH: Deep invertible network for multiple image hiding.
<em>TPAMI</em>, <em>45</em>(1), 372–390. (<a
href="https://doi.org/10.1109/TPAMI.2022.3141725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple image hiding aims to hide multiple secret images into a single cover image, and then recover all secret images perfectly. Such high-capacity hiding may easily lead to contour shadows or color distortion, which makes multiple image hiding a very challenging task. In this paper, we propose a novel multiple image hiding framework based on invertible neural network, namely DeepMIH. Specifically, we develop an invertible hiding neural network (IHNN) to innovatively model the image concealing and revealing as its forward and backward processes, making them fully coupled and reversible. The IHNN is highly flexible, which can be cascaded as many times as required to achieve the hiding of multiple images. To enhance the invisibility, we design an importance map (IM) module to guide the current image hiding based on the previous image hiding results. In addition, we find that the image hidden in the high-frequency sub-bands tends to achieve better hiding performance, and thus propose a low-frequency wavelet loss to constrain that no secret information is hidden in the low-frequency sub-bands. Experimental results show that our DeepMIH significantly outperforms other state-of-the-art methods, in terms of hiding invisibility, security and recovery accuracy on a variety of datasets.},
  archive      = {J_TPAMI},
  author       = {Zhenyu Guan and Junpeng Jing and Xin Deng and Mai Xu and Lai Jiang and Zhou Zhang and Yipeng Li},
  doi          = {10.1109/TPAMI.2022.3141725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {372-390},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeepMIH: Deep invertible network for multiple image hiding},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep video prior for video consistency and propagation.
<em>TPAMI</em>, <em>45</em>(1), 356–371. (<a
href="https://doi.org/10.1109/TPAMI.2022.3142071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying an image processing algorithm independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with Deep Video Prior (DVP). Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. We further extend DVP to video propagation and demonstrate its effectiveness in propagating three different types of information (color, artistic style, and object segmentation). A progressive propagation strategy with pseudo labels is also proposed to enhance DVP’s performance on video propagation. Our source codes are publicly available at https://github.com/ChenyangLEI/deep-video-prior .},
  archive      = {J_TPAMI},
  author       = {Chenyang Lei and Yazhou Xing and Hao Ouyang and Qifeng Chen},
  doi          = {10.1109/TPAMI.2022.3142071},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {356-371},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep video prior for video consistency and propagation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep time series forecasting with shape and temporal
criteria. <em>TPAMI</em>, <em>45</em>(1), 342–355. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of multi-step time series forecasting for non-stationary signals that can present sudden changes. Current state-of-the-art deep learning forecasting methods, often trained with variants of the MSE, lack the ability to provide sharp predictions in deterministic and probabilistic contexts. To handle these challenges, we propose to incorporate shape and temporal criteria in the training objective of deep models. We define shape and temporal similarities and dissimilarities, based on a smooth relaxation of Dynamic Time Warping (DTW) and Temporal Distortion Index (TDI), that enable to build differentiable loss functions and positive semi-definite (PSD) kernels. With these tools, we introduce DILATE (DIstortion Loss including shApe and TimE), a new objective for deterministic forecasting, that explicitly incorporates two terms supporting precise shape and temporal change detection. For probabilistic forecasting, we introduce STRIPE++ (Shape and Time diverRsIty in Probabilistic for Ecasting), a framework for providing a set of sharp and diverse forecasts, where the structured shape and time diversity is enforced with a determinantal point process (DPP) diversity loss. Extensive experiments and ablations studies on synthetic and real-world datasets confirm the benefits of leveraging shape and time features in time series forecasting.},
  archive      = {J_TPAMI},
  author       = {Vincent Le Guen and Nicolas Thome},
  doi          = {10.1109/TPAMI.2022.3152862},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {342-355},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep time series forecasting with shape and temporal criteria},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep ROC analysis and AUC as balanced average accuracy, for
improved classifier selection, audit and explanation. <em>TPAMI</em>,
<em>45</em>(1), 329–341. (<a
href="https://doi.org/10.1109/TPAMI.2022.3145392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal performance is desired for decision-making in any field with binary classifiers and diagnostic tests, however common performance measures lack depth in information. The area under the receiver operating characteristic curve (AUC) and the area under the precision recall curve are too general because they evaluate all decision thresholds including unrealistic ones. Conversely, accuracy, sensitivity, specificity, positive predictive value and the F1 score are too specific—they are measured at a single threshold that is optimal for some instances, but not others, which is not equitable. In between both approaches, we propose deep ROC analysis to measure performance in multiple groups of predicted risk (like calibration), or groups of true positive rate or false positive rate. In each group, we measure the group AUC (properly), normalized group AUC, and averages of: sensitivity, specificity, positive and negative predictive value, and likelihood ratio positive and negative. The measurements can be compared between groups, to whole measures, to point measures and between models. We also provide a new interpretation of AUC in whole or part, as balanced average accuracy, relevant to individuals instead of pairs. We evaluate models in three case studies using our method and Python toolkit and confirm its utility.},
  archive      = {J_TPAMI},
  author       = {André M. Carrington and Douglas G. Manuel and Paul W. Fieguth and Tim Ramsay and Venet Osmani and Bernhard Wernly and Carol Bennett and Steven Hawken and Olivia Magwood and Yusuf Sheikh and Matthew McInnes and Andreas Holzinger},
  doi          = {10.1109/TPAMI.2022.3145392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {329-341},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep ROC analysis and AUC as balanced average accuracy, for improved classifier selection, audit and explanation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep order-preserving learning with adaptive optimal
transport distance. <em>TPAMI</em>, <em>45</em>(1), 313–328. (<a
href="https://doi.org/10.1109/TPAMI.2022.3156885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a framework for taking into consideration the relative importance (ordinality) of object labels in the process of learning a label predictor function. The commonly used loss functions are not well matched to this problem, as they exhibit deficiencies in capturing natural correlations of the labels and the corresponding data. We propose to incorporate such correlations into our learning algorithm using an optimal transport formulation. Our approach is to learn the ground metric, which is partly involved in forming the optimal transport distance, by leveraging ordinality as a general form of side information in its formulation. Based on this idea, we then develop a novel loss function for training deep neural networks. A highly efficient alternating learning method is then devised to alternatively optimise the ground metric and the deep model in an end-to-end learning manner. This scheme allows us to adaptively adjust the shape of the ground metric, and consequently the shape of the loss function for each application. We back up our approach by theoretical analysis and verify the performance of our proposed scheme by applying it to two learning tasks, i.e. chronological age estimation from the face and image aesthetic assessment. The numerical results on several benchmark datasets demonstrate the superiority of the proposed algorithm.},
  archive      = {J_TPAMI},
  author       = {Ali Akbari and Muhammad Awais and Soroush Fatemifar and Josef Kittler},
  doi          = {10.1109/TPAMI.2022.3156885},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {313-328},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep order-preserving learning with adaptive optimal transport distance},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for free-hand sketch: A survey.
<em>TPAMI</em>, <em>45</em>(1), 285–312. (<a
href="https://doi.org/10.1109/TPAMI.2022.3148853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-hand sketches are highly illustrative, and have been widely used by humans to depict objects or stories from ancient times to the present. The recent prevalence of touchscreen devices has made sketch creation a much easier task than ever and consequently made sketch-oriented applications increasingly popular. The progress of deep learning has immensely benefited free-hand sketch research and applications. This paper presents a comprehensive survey of the deep learning techniques oriented at free-hand sketch data, and the applications that they enable. The main contents of this survey include: (i) A discussion of the intrinsic traits and unique challenges of free-hand sketch, to highlight the essential differences between sketch data and other data modalities, e.g., natural photos. (ii) A review of the developments of free-hand sketch research in the deep learning era, by surveying existing datasets, research topics, and the state-of-the-art methods through a detailed taxonomy and experimental evaluation. (iii) Promotion of future work via a discussion of bottlenecks, open problems, and potential research directions for the community.},
  archive      = {J_TPAMI},
  author       = {Peng Xu and Timothy M. Hospedales and Qiyue Yin and Yi-Zhe Song and Tao Xiang and Liang Wang},
  doi          = {10.1109/TPAMI.2022.3148853},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {285-312},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning for free-hand sketch: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep gait recognition: A survey. <em>TPAMI</em>,
<em>45</em>(1), 264–284. (<a
href="https://doi.org/10.1109/TPAMI.2022.3151865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is an appealing biometric modality which aims to identify individuals based on the way they walk. Deep learning has reshaped the research landscape in this area since 2015 through the ability to automatically learn discriminative representations. Gait recognition methods based on deep learning now dominate the state-of-the-art in the field and have fostered real-world applications. In this paper, we present a comprehensive overview of breakthroughs and recent developments in gait recognition with deep learning, and cover broad topics including datasets, test protocols, state-of-the-art solutions, challenges, and future research directions. We first review the commonly used gait datasets along with the principles designed for evaluating them. We then propose a novel taxonomy made up of four separate dimensions namely body representation, temporal representation, feature representation, and neural architecture, to help characterize and organize the research landscape and literature in this area. Following our proposed taxonomy, a comprehensive survey of gait recognition methods using deep learning is presented with discussions on their performances, characteristics, advantages, and limitations. We conclude this survey with a discussion on current challenges and mention a number of promising directions for future research in gait recognition.},
  archive      = {J_TPAMI},
  author       = {Alireza Sepas-Moghaddam and Ali Etemad},
  doi          = {10.1109/TPAMI.2022.3151865},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {264-284},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep gait recognition: A survey},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dataset-driven unsupervised object discovery for
region-based instance image retrieval. <em>TPAMI</em>, <em>45</em>(1),
247–263. (<a href="https://doi.org/10.1109/TPAMI.2022.3141433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance image retrieval could greatly benefit from discovering objects in the image dataset. This not only helps produce more reliable feature representation but also better informs users by delineating query-matched object regions. However, object classes are usually not predefined in a retrieval dataset and class label information is generally unavailable in image retrieval. This situation makes object discovery a challenging task. To address this, we propose a novel dataset-driven unsupervised object discovery framework. By utilizing deep feature representation and weakly-supervised object detection, we explore supervisory information from within an image dataset, construct class-wise object detectors, and assign multiple detectors to each image for detection. To efficiently construct object detectors for large image datasets, we propose a novel “base-detector repository” and derive a fast way to generate the base detectors. In addition, the whole framework is designed to work in a self-boosting manner to iteratively refine object discovery. Compared with existing unsupervised object detection methods, our framework produces more accurate object discovery results. Different from supervised detection, we need neither manual annotation nor auxiliary datasets to train object detectors. Experimental study demonstrates the effectiveness of the proposed framework and the improved performance for region-based instance image retrieval.},
  archive      = {J_TPAMI},
  author       = {Zhongyan Zhang and Lei Wang and Yang Wang and Luping Zhou and Jianjia Zhang and Fang Chen},
  doi          = {10.1109/TPAMI.2022.3141433},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {247-263},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dataset-driven unsupervised object discovery for region-based instance image retrieval},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dataset bias in few-shot image recognition. <em>TPAMI</em>,
<em>45</em>(1), 229–246. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of few-shot image recognition (FSIR) is to identify novel categories with a small number of annotated samples by exploiting transferable knowledge from training data (base categories). Most current studies assume that the transferable knowledge can be well used to identify novel categories. However, such transferable capability may be impacted by the dataset bias, and this problem has rarely been investigated before. Besides, most of few-shot learning methods are biased to different datasets, which is also an important issue that needs to be investigated deeply. In this paper, we first investigate the impact of transferable capabilities learned from base categories. Specifically, we use the relevance to measure relationships between base categories and novel categories. Distributions of base categories are depicted via the instance density and category diversity. The FSIR model learns better transferable knowledge from relevant training data. In the relevant data, dense instances or diverse categories can further enrich the learned knowledge. Experimental results on different sub-datasets of Imagenet demonstrate category relevance, instance density and category diversity can depict transferable bias from distributions of base categories. Second, we investigate performance differences on different datasets from the aspects of dataset structures and different few-shot learning methods. Specifically, we introduce image complexity, intra-concept visual consistency, and inter-concept visual similarity to quantify characteristics of dataset structures. We use these quantitative characteristics and eight few-shot learning methods to analyze performance differences on multiple datasets. Based on the experimental analysis, some insightful observations are obtained from the perspective of both dataset structures and few-shot learning methods. We hope these observations are useful to guide future few-shot learning research on new datasets or tasks. Our data is available at http://123.57.42.89/dataset-bias/dataset-bias.html .},
  archive      = {J_TPAMI},
  author       = {Shuqiang Jiang and Yaohui Zhu and Chenlong Liu and Xinhang Song and Xiangyang Li and Weiqing Min},
  doi          = {10.1109/TPAMI.2022.3153611},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {229-246},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dataset bias in few-shot image recognition},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cyclic differentiable architecture search. <em>TPAMI</em>,
<em>45</em>(1), 211–228. (<a
href="https://doi.org/10.1109/TPAMI.2022.3153065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in neural architecture search. It tries to find the optimal architecture in a shallow search network and then measures its performance in a deep evaluation network. The independent optimization of the search and evaluation networks, however, leaves a room for potential improvement by allowing interaction between the two networks. To address the problematic optimization issue, we propose new joint optimization objectives and a novel Cyclic Differentiable ARchiTecture Search framework, dubbed CDARTS. Considering the structure difference, CDARTS builds a cyclic feedback mechanism between the search and evaluation networks with introspective distillation. First, the search network generates an initial architecture for evaluation, and the weights of the evaluation network are optimized. Second, the architecture weights in the search network are further optimized by the label supervision in classification, as well as the regularization from the evaluation network through feature distillation. Repeating the above cycle results in a joint optimization of the search and evaluation networks and thus enables the evolution of the architecture to fit the final evaluation network. The experiments and analysis on CIFAR, ImageNet and NATS-Bench [95] demonstrate the effectiveness of the proposed approach over the state-of-the-art ones. Specifically, in the DARTS search space, we achieve 97.52\% top-1 accuracy on CIFAR10 and 76.3\% top-1 accuracy on ImageNet. In the chain-structured search space, we achieve 78.2\% top-1 accuracy on ImageNet, which is 1.1\% higher than EfficientNet-B0. Our code and models are publicly available at https://github.com/microsoft/Cream .},
  archive      = {J_TPAMI},
  author       = {Hongyuan Yu and Houwen Peng and Yan Huang and Jianlong Fu and Hao Du and Liang Wang and Haibin Ling},
  doi          = {10.1109/TPAMI.2022.3153065},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {211-228},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cyclic differentiable architecture search},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistency and diversity induced human motion segmentation.
<em>TPAMI</em>, <em>45</em>(1), 197–210. (<a
href="https://doi.org/10.1109/TPAMI.2022.3147841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is a classical technique that has been widely used for human motion segmentation and other related tasks. However, existing segmentation methods often cluster data without guidance from prior knowledge, resulting in unsatisfactory segmentation results. To this end, we propose a novel C onsistency and D iversity induced human M otion S egmentation (CDMS) algorithm. Specifically, our model factorizes the source and target data into distinct multi-layer feature spaces, in which transfer subspace learning is conducted on different layers to capture multi-level information. A multi-mutual consistency learning strategy is carried out to reduce the domain gap between the source and target data. In this way, the domain-specific knowledge and domain-invariant properties can be explored simultaneously. Besides, a novel constraint based on the Hilbert Schmidt Independence Criterion (HSIC) is introduced to ensure the diversity of multi-level subspace representations, which enables the complementarity of multi-level representations to be explored to boost the transfer learning performance. Moreover, to preserve the temporal correlations, an enhanced graph regularizer is imposed on the learned representation coefficients and the multi-level representations of the source data. The proposed model can be efficiently solved using the Alternating Direction Method of Multipliers (ADMM) algorithm. Extensive experimental results on public human motion datasets demonstrate the effectiveness of our method against several state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Tao Zhou and Huazhu Fu and Chen Gong and Ling Shao and Fatih Porikli and Haibin Ling and Jianbing Shen},
  doi          = {10.1109/TPAMI.2022.3147841},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {197-210},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Consistency and diversity induced human motion segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex-valued iris recognition network. <em>TPAMI</em>,
<em>45</em>(1), 182–196. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we design a fully complex-valued neural network for the task of iris recognition. Unlike the problem of general object recognition, where real-valued neural networks can be used to extract pertinent features, iris recognition depends on the extraction of both phase and magnitude information from the input iris texture in order to better represent its biometric content. This necessitates the extraction and processing of phase information that cannot be effectively handled by a real-valued neural network. In this regard, we design a fully complex-valued neural network that can better capture the multi-scale, multi-resolution, and multi-orientation phase and amplitude features of the iris texture. We show a strong correspondence of the proposed complex-valued iris recognition network with Gabor wavelets that are used to generate the classical IrisCode; however, the proposed method enables a new capability of automatic complex-valued feature learning that is tailored for iris recognition. We conduct experiments on three benchmark datasets - ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit of the proposed network for the task of iris recognition. We exploit visualization schemes to convey how the complex-valued network, when compared to standard real-valued networks, extracts fundamentally different features from the iris texture.},
  archive      = {J_TPAMI},
  author       = {Kien Nguyen and Clinton Fookes and Sridha Sridharan and Arun Ross},
  doi          = {10.1109/TPAMI.2022.3152857},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {182-196},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Complex-valued iris recognition network},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Centerless clustering. <em>TPAMI</em>, <em>45</em>(1),
167–181. (<a href="https://doi.org/10.1109/TPAMI.2022.3150981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although lots of clustering models have been proposed recently, $k$ -means and the family of spectral clustering methods are both still drawing a lot of attention due to their simplicity and efficacy. We first reviewed the unified framework of $k$ -means and graph cut models, and then proposed a clustering method called $k$ -sums where a $k$ -nearest neighbor ( $k$ -NN) graph is adopted. The main idea of $k$ -sums is to minimize directly the sum of the distances between points in the same cluster. To deal with the situation where the graph is unavailable, we proposed $k$ -sums-x that takes features as input. The computational and memory overhead of $k$ -sums are both $O(nk)$ , indicating that it can scale linearly w.r.t. the number of objects to group. Moreover, the costs of computational and memory are Irrelevant to the product of the number of points and clusters. The computational and memory complexity of $k$ -sums-x are both linear w.r.t. the number of points. To validate the advantage of $k$ -sums and $k$ -sums-x on facial datasets, extensive experiments have been conducted on 10 synthetic datasets and 17 benchmark datasets. While having a low time complexity, the performance of $k$ -sums is comparable with several state-of-the-art clustering methods.},
  archive      = {J_TPAMI},
  author       = {Shenfei Pei and Huimin Chen and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TPAMI.2022.3150981},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {167-181},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Centerless clustering},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CCMN: A general framework for learning with
class-conditional multi-label noise. <em>TPAMI</em>, <em>45</em>(1),
154–166. (<a href="https://doi.org/10.1109/TPAMI.2022.3141240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-conditional noise commonly exists in machine learning tasks, where the class label is corrupted with a probability depending on its ground-truth. Many research efforts have been made to improve the model robustness against the class-conditional noise. However, they typically focus on the single label case by assuming that only one label is corrupted. In real applications, an instance is usually associated with multiple labels, which could be corrupted simultaneously with their respective conditional probabilities. In this paper, we formalize this problem as a general framework of learning with Class-Conditional Multi-label Noise (CCMN for short). We establish two unbiased estimators with error bounds for solving the CCMN problems, and further prove that they are consistent with commonly used multi-label loss functions. Finally, a new method for partial multi-label learning is implemented with the unbiased estimator under the CCMN framework. Empirical studies on multiple datasets and various evaluation metrics validate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Ming-Kun Xie and Sheng-Jun Huang},
  doi          = {10.1109/TPAMI.2022.3141240},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {154-166},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CCMN: A general framework for learning with class-conditional multi-label noise},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BodyPressure - inferring body pose and contact pressure from
a depth image. <em>TPAMI</em>, <em>45</em>(1), 137–153. (<a
href="https://doi.org/10.1109/TPAMI.2022.3158902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact pressure between the human body and its surroundings has important implications. For example, it plays a role in comfort, safety, posture, and health. We present a method that infers contact pressure between a human body and a mattress from a depth image. Specifically, we focus on using a depth image from a downward facing camera to infer pressure on a body at rest in bed occluded by bedding, which is directly applicable to the prevention of pressure injuries in healthcare. Our approach involves augmenting a real dataset with synthetic data generated via a soft-body physics simulation of a human body, a mattress, a pressure sensing mat, and a blanket. We introduce a novel deep network that we trained on an augmented dataset and evaluated with real data. The network contains an embedded human body mesh model and uses a white-box model of depth and pressure image generation. Our network successfully infers body pose, outperforming prior work. It also infers contact pressure across a 3D mesh model of the human body, which is a novel capability, and does so in the presence of occlusion from blankets.},
  archive      = {J_TPAMI},
  author       = {Henry M. Clever and Patrick L. Grady and Greg Turk and Charles C. Kemp},
  doi          = {10.1109/TPAMI.2022.3158902},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {137-153},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BodyPressure - inferring body pose and contact pressure from a depth image},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Binaural SoundNet: Predicting semantics, depth and motion
with binaural sounds. <em>TPAMI</em>, <em>45</em>(1), 123–136. (<a
href="https://doi.org/10.1109/TPAMI.2022.3155643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can robustly recognize and localize objects by using visual and/or auditory cues. While machines are able to do the same with visual data already, less work has been done with sounds. This work develops an approach for scene understanding purely based on binaural sounds. The considered tasks include predicting the semantic masks of sound-making objects, the motion of sound-making objects, and the depth map of the scene. To this aim, we propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360 $\mathrm{^{\circ }}$ camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of multiple vision ‘teacher’ methods and a sound ‘student’ method – the student method is trained to generate the same results as the teacher methods do. This way, the auditory system can be trained without using human annotations. To further boost the performance, we propose another novel auxiliary task, coined Spatial Sound Super-Resolution, to increase the directional resolution of sounds. We then formulate the four tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results show that 1) our method achieves good results for all four tasks, 2) the four tasks are mutually beneficial – training them together achieves the best performance, 3) the number and orientation of microphones are both important, and 4) features learned from the standard spectrogram and features obtained by the classic signal processing pipeline are complementary for auditory perception tasks. The data and code are released on the project page: https://www.trace.ethz.ch/publications/2020/sound_perception/index.html .},
  archive      = {J_TPAMI},
  author       = {Dengxin Dai and Arun Balajee Vasudevan and Jiri Matas and Luc Van Gool},
  doi          = {10.1109/TPAMI.2022.3155643},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {123-136},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Binaural SoundNet: Predicting semantics, depth and motion with binaural sounds},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Berrut approximated coded computing: Straggler resistance
beyond polynomial computing. <em>TPAMI</em>, <em>45</em>(1), 111–122.
(<a href="https://doi.org/10.1109/TPAMI.2022.3151434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in using distributed learning to train complicated models with large data sets is to deal with stragglers effect. As a solution, coded computation has been recently proposed to efficiently add redundancy to the computation tasks. In this technique, coding is used across data sets, and computation is done over coded data, such that the results of an arbitrary subset of worker nodes with a certain size are enough to recover the final results. The major drawbacks with those approaches are (1) they are limited to polynomial functions, (2) the number of servers that we need to wait for grows with the degree of the model, (3) they are not numerically stable for computation over real numbers. In this paper, we propose Berrut Approximated Coded Computing (BACC), as an alternative approach, as a numerically stable solution, which works beyond polynomial functions computation and with any number of servers. The accuracy of the approximation is established theoretically and verified by simulation. In particular, BACC is used to train a deep neural network on a cluster of servers, which outperforms alternative uncoded solutions in terms of the rate of convergence.},
  archive      = {J_TPAMI},
  author       = {Tayyebeh Jahani-Nezhad and Mohammad Ali Maddah-Ali},
  doi          = {10.1109/TPAMI.2022.3151434},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {111-122},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Berrut approximated coded computing: Straggler resistance beyond polynomial computing},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on vision transformer. <em>TPAMI</em>,
<em>45</em>(1), 87–110. (<a
href="https://doi.org/10.1109/TPAMI.2022.3152247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
  archive      = {J_TPAMI},
  author       = {Kai Han and Yunhe Wang and Hanting Chen and Xinghao Chen and Jianyuan Guo and Zhenhua Liu and Yehui Tang and An Xiao and Chunjing Xu and Yixing Xu and Zhaohui Yang and Yiman Zhang and Dacheng Tao},
  doi          = {10.1109/TPAMI.2022.3152247},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {87-110},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey on vision transformer},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pose-only solution to visual reconstruction and
navigation. <em>TPAMI</em>, <em>45</em>(1), 73–86. (<a
href="https://doi.org/10.1109/TPAMI.2021.3139681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual navigation and three-dimensional (3D) scene reconstruction are essential for robotics to interact with the surrounding environment. Large-scale scenarios and computational robustness are great challenges facing the research community to achieve this goal. This paper raises a pose-only imaging geometry representation and algorithms that might help solve these challenges. The pose-only representation, equivalent to the classical multiple-view geometry, is discovered to be linearly related to camera global translations, which allows for efficient and robust camera motion estimation. As a result, the spatial feature coordinates can be analytically reconstructed and do not require nonlinear optimization. Comprehensive experiments demonstrate that the computational efficiency of recovering the scene and associated camera poses is significantly improved by 2-4 orders of magnitude.},
  archive      = {J_TPAMI},
  author       = {Qi Cai and Lilian Zhang and Yuanxin Wu and Wenxian Yu and Dewen Hu},
  doi          = {10.1109/TPAMI.2021.3139681},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {73-86},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A pose-only solution to visual reconstruction and navigation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A one-stage domain adaptation network with image alignment
for unsupervised nighttime semantic segmentation. <em>TPAMI</em>,
<em>45</em>(1), 58–72. (<a
href="https://doi.org/10.1109/TPAMI.2021.3138829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of semantic segmentation for nighttime images that plays an equally important role as that for daytime images in autonomous driving, but is also much more challenging due to very poor illuminations and scarce annotated datasets. It can be treated as an unsupervised domain adaptation (UDA) problem, i.e., applying other labeled dataset taken in the daytime to guide the network training meanwhile reducing the domain shift, so that the trained model can generalize well to the desired domain of nighttime images. However, current general-purpose UDA approaches are insufficient to address the significant appearance difference between the day and night domains. To overcome such a large domain gap, we propose a novel domain adaptation network “DANIA” for nighttime semantic image segmentation by leveraging a labeled daytime dataset (the source domain) and an unlabeled dataset that contains coarsely aligned day-night image pairs (the target daytime and nighttime domains). These three domains are used to perform a multi-target adaptation via adversarial training in the network. Specifically, for the unlabeled day-night image pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We also include a step of image alignment to relieve the inaccuracy caused by the misalignment between day-night image pairs by estimating a flow to refine the pseudo supervision produced by daytime images. Finally, a re-weighting strategy is applied to further improve the predictions, especially boosting the prediction accuracy of small objects. The proposed DANIA is a one-stage adaptation framework for nighttime semantic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage. Extensive experiments on Dark Zurich and Nighttime Driving datasets show that our DANIA achieves state-of-the-art performance for nighttime semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Xinyi Wu and Zhenyao Wu and Lili Ju and Song Wang},
  doi          = {10.1109/TPAMI.2021.3138829},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {58-72},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A one-stage domain adaptation network with image alignment for unsupervised nighttime semantic segmentation},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general descent aggregation framework for gradient-based
bi-level optimization. <em>TPAMI</em>, <em>45</em>(1), 38–57. (<a
href="https://doi.org/10.1109/TPAMI.2022.3140249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a variety of gradient-based methods have been developed to solve Bi-Level Optimization (BLO) problems in machine learning and computer vision areas. However, the theoretical correctness and practical effectiveness of these existing approaches always rely on some restrictive conditions (e.g., Lower-Level Singleton, LLS), which could hardly be satisfied in real-world applications. Moreover, previous literature only proves theoretical results based on their specific iteration strategies, thus lack a general recipe to uniformly analyze the convergence behaviors of different gradient-based BLOs. In this work, we formulate BLOs from an optimistic bi-level viewpoint and establish a new gradient-based algorithmic framework, named Bi-level Descent Aggregation (BDA), to partially address the above issues. Specifically, BDA provides a modularized structure to hierarchically aggregate both the upper- and lower-level subproblems to generate our bi-level iterative dynamics. Theoretically, we establish a general convergence analysis template and derive a new proof recipe to investigate the essential theoretical properties of gradient-based BLO methods. Furthermore, this work systematically explores the convergence behavior of BDA in different optimization scenarios, i.e., considering various solution qualities (i.e., global/local/stationary solution) returned from solving approximation subproblems. Extensive experiments justify our theoretical results and demonstrate the superiority of the proposed algorithm for hyper-parameter optimization and meta-learning tasks. Source code is available at https://github.com/vis-opt-group/BDA .},
  archive      = {J_TPAMI},
  author       = {Risheng Liu and Pan Mu and Xiaoming Yuan and Shangzhi Zeng and Jin Zhang},
  doi          = {10.1109/TPAMI.2022.3140249},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {38-57},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A general descent aggregation framework for gradient-based bi-level optimization},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A function space analysis of finite neural networks with
insights from sampling theory. <em>TPAMI</em>, <em>45</em>(1), 27–37.
(<a href="https://doi.org/10.1109/TPAMI.2022.3155238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work suggests using sampling theory to analyze the function space represented by interpolating mappings. While the analysis in this paper is general, we focus it on neural networks with bounded weights that are known for their ability to interpolate (fit) the training data. First, we show, under the assumption of a finite input domain, which is the common case in training neural networks, that the function space generated by multi-layer networks with bounded weights, and non-expansive activation functions are smooth. This extends over previous works that show results for the case of infinite width ReLU networks. Then, under the assumption that the input is band-limited, we provide novel error bounds for univariate neural networks. We analyze both deterministic uniform and random sampling showing the advantage of the former.},
  archive      = {J_TPAMI},
  author       = {Raja Giryes},
  doi          = {10.1109/TPAMI.2022.3155238},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {27-37},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A function space analysis of finite neural networks with insights from sampling theory},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey of scene graphs: Generation and
application. <em>TPAMI</em>, <em>45</em>(1), 1–26. (<a
href="https://doi.org/10.1109/TPAMI.2021.3137605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also understand the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarize the general definition of the scene graph, then conducte a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigate the main applications of scene graphs and summarize the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs.},
  archive      = {J_TPAMI},
  author       = {Xiaojun Chang and Pengzhen Ren and Pengfei Xu and Zhihui Li and Xiaojiang Chen and Alex Hauptmann},
  doi          = {10.1109/TPAMI.2021.3137605},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1-26},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A comprehensive survey of scene graphs: Generation and application},
  volume       = {45},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
