<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---458">TVCG - 458</h2>
<ul>
<li><details>
<summary>
(2023). Color-encoded links improve homophily perception in
node-link diagrams. <em>TVCG</em>, <em>29</em>(12), 5593–5598. (<a
href="https://doi.org/10.1109/TVCG.2022.3221014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-link diagrams enable visual assessment of homophily when viewers can identify and evaluate the relative number of intra-cluster and inter-cluster links. Our online experiment shows that a new design with link type encoded edge color leads to more accurate perception of homophily than a design with same-color edges.},
  archive      = {J_TVCG},
  author       = {Daniel Reimann and André Schulz and Nilam Ram and Robert Gaschler},
  doi          = {10.1109/TVCG.2022.3221014},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5593-5598},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Color-encoded links improve homophily perception in node-link diagrams},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A monocular projector-camera system using modular
architecture. <em>TVCG</em>, <em>29</em>(12), 5586–5592. (<a
href="https://doi.org/10.1109/TVCG.2022.3217266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a monocular projector-camera (procam) system using modular architecture based on relay optics. Conventional coaxial procam systems cannot support (1) online changes to lens settings (zoom and focus) and (2) wide-angle projection mapping. We develop design guidelines for a proposed procam system that would solve these restrictions and address the proposed system&#39;s unique technical issue of crosstalk between the camera and projector pixels. We conducted experiments using prototypes to validate the feasibility of the proposed framework. First, we confirmed that the proposed crosstalk reduction technique worked well. Second, we found our technique could achieve correct alignment of a projected image onto a moving surface while changing the zoom and focus of the objective lens. The monocular procam system also achieved radiometric compensation where a surface texture was visually concealed by pixel-wise control of a projection color based on the captured results of offline color pattern projections. Finally, we demonstrated the high expandability of our modular architecture, through the creation of a high dynamic range projection.},
  archive      = {J_TVCG},
  author       = {Kenta Yamamoto and Daisuke Iwai and Ikuho Tani and Kosuke Sato},
  doi          = {10.1109/TVCG.2022.3217266},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5586-5592},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A monocular projector-camera system using modular architecture},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect of vibrations on impression of walking and embodiment
with first- and third-person avatar. <em>TVCG</em>, <em>29</em>(12),
5579–5585. (<a href="https://doi.org/10.1109/TVCG.2022.3212089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how underfoot vibrotactile feedback can be used to increase the impression of walking and embodiment of static users represented by a first- or third-person avatar. We designed a multi-sensory setup involving avatar displayed on an HMD, and a set of vibrotactile effects displayed at every footstep. In a first study (N = 44), we compared the impression of walking in 3 vibrotactile conditions : 1) with a ”constant” vibrotactile rendering reproducing simple contact information, 2) with a more sophisticated ”phase-based” vibrotactile rendering the successive contacts of a walking cycle and 3) without vibrotactile feedback. The results show that overall both constant and phase-based rendering significantly improve the impression of walking in first and third-person perspective. Interestingly, the more realistic phase-based rendering seems to increase significantly the impression of walking in the third-person condition, but not in the first-person condition. In a second study (N=28), we evaluated the embodiment towards first- and third-person avatar while receiving no vibrotactile feedback or by receiving vibrotactile feedback. The results show that vibrotactile feedback improves embodiment in both perspectives of the avatar. Taken together, our results support the use of vibrotactile feedback when users observe first- and third-person avatar. They also suggest that constant and phase-based rendering could be used with first-person avatar and support the use of phase-based rendering with third-person avatar. They provide valuable insight for stimulations in any VR applications in which the impression of walking is prominent such as for virtual visits, walking rehabilitation, video games, etc.},
  archive      = {J_TVCG},
  author       = {Justine Saint-Aubert and Julien Manson and Isabelle Bonan and Yoann Launey and Anatole Lécuyer and Mélanie Cogné},
  doi          = {10.1109/TVCG.2022.3212089},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5579-5585},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effect of vibrations on impression of walking and embodiment with first- and third-person avatar},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural temporal denoising for indirect illumination.
<em>TVCG</em>, <em>29</em>(12), 5569–5578. (<a
href="https://doi.org/10.1109/TVCG.2022.3217305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various temporal denoising methods have been proposed to clean up the noise for real-time ray tracing (RTRT). These methods rely on the temporal correspondences of pixels between the current and previous frames, i.e. per-pixel screen-space motion vectors. However, the state-of-the-art temporal reuse methods with traditional motion vectors cause artifacts in motion occlusions. We accordingly propose a novel neural temporal denoising method for indirect illumination of Monte Carlo (MC) ray tracing at 1 sample per pixel. Based on end-to-end multi-scale kernel-based reconstruction, we apply temporally reliable dual motion vectors to facilitate better reconstruction of the occlusions, and also introduce additional motion occlusion loss to reduce ghosting artifacts. Experiments show that our method significantly reduces the over-blurring and ghosting artifacts while generating high-quality images at real-time rates.},
  archive      = {J_TVCG},
  author       = {Yan Zeng and Lu Wang and Yanning Xu and Xiangxu Meng},
  doi          = {10.1109/TVCG.2022.3217305},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5569-5578},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural temporal denoising for indirect illumination},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explore contextual information for 3D scene graph
generation. <em>TVCG</em>, <em>29</em>(12), 5556–5568. (<a
href="https://doi.org/10.1109/TVCG.2022.3219451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene graph generation (SGG) has been of high interest in computer vision. Although the accuracy of 3D SGG on coarse classification and single relation label has been gradually improved, the performance of existing works is still far from being perfect for fine-grained and multi-label situations. In this article, we propose a framework fully exploring contextual information for the 3D SGG task, which attempts to satisfy the requirements of fine-grained entity class, multiple relation labels, and high accuracy simultaneously. Our proposed approach is composed of a Graph Feature Extraction module and a Graph Contextual Reasoning module, achieving appropriate information-redundancy feature extraction, structured organization, and hierarchical inferring. Our approach achieves superior or competitive performance over previous methods on the 3DSSG dataset, especially on the relationship prediction sub-task.},
  archive      = {J_TVCG},
  author       = {Yuanyuan Liu and Chengjiang Long and Zhaoxuan Zhang and Bokai Liu and Qiang Zhang and Baocai Yin and Xin Yang},
  doi          = {10.1109/TVCG.2022.3219451},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5556-5568},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explore contextual information for 3D scene graph generation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strolling in room-scale VR: Hex-core-MK1 omnidirectional
treadmill. <em>TVCG</em>, <em>29</em>(12), 5538–5555. (<a
href="https://doi.org/10.1109/TVCG.2022.3216211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The natural locomotion interface is critical to the development of many VR applications. For household VR applications, there are two basic requirements: natural immersive experience and minimized space occupation. The existing locomotion strategies generally do not simultaneously satisfy these two requirements well. This article presents a novel omnidirectional treadmill (ODT) system named Hex-Core-MK1 (HCMK1). By implementing two kinds of mirror-symmetrical spiral rollers to generate the omnidirectional velocity field, this proposed system is capable of providing real walking experiences with a full-degree of freedom in an area as small as 1.76 m $^{2}$ , while delivering great advantages over several existing ODT systems in terms of weight, volume, latency and dynamic performance. Compared with the sizes of Infinadeck and HCP, the two best motor-driven ODTs so far, the 8 cm height of HCMK1 is only 20\% of Infinadeck and 50\% of HCP. In addition, HCMK1 is a lightweight device weighing only 110 kg, which provides possibilities for further expanding VR scenarios, such as terrain simulation. The system latency of HCMK1 is only 9ms. The experiments show that HCMK1 can deliver a starting acceleration of 16.00 m/s $^{2}$ and a braking acceleration of 30.00 m/s $^{2}$ .},
  archive      = {J_TVCG},
  author       = {Ziyao Wang and Chiyi Liu and Jialiang Chen and Yao Yao and Dazheng Fang and Zhiyi Shi and Rui Yan and Yiye Wang and KanJian Zhang and Hai Wang and Haikun Wei},
  doi          = {10.1109/TVCG.2022.3216211},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5538-5555},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Strolling in room-scale VR: Hex-core-MK1 omnidirectional treadmill},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SceneViewer: Automating residential photography in virtual
environments. <em>TVCG</em>, <em>29</em>(12), 5523–5537. (<a
href="https://doi.org/10.1109/TVCG.2022.3214836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting views is one of the most common but overlooked procedures in topics related to 3D scenes. Typically, existing applications and researchers manually select views through a trial-and-error process or “preset” a direction, such as the top-down views. For example, literature for scene synthesis requires views for visualizing scenes. Research on panorama and VR also require initial placements for cameras, etc. This article presents SceneViewer, an integrated system for automatic view selections. Our system is achieved by applying rules of interior photography, which guides potential views and seeks better views. Through experiments and applications, we show the potentiality and novelty of the proposed method.},
  archive      = {J_TVCG},
  author       = {Shao-Kui Zhang and Hou Tam and Yi-Xiao Li and Tai-Jiang Mu and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2022.3214836},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5523-5537},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SceneViewer: Automating residential photography in virtual environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metameric inpainting for image warping. <em>TVCG</em>,
<em>29</em>(12), 5511–5522. (<a
href="https://doi.org/10.1109/TVCG.2022.3216712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-warping , a per-pixel deformation of one image into another, is an essential component in immersive visual experiences such as virtual reality or augmented reality. The primary issue with image warping is disocclusions, where occluded (and hence unknown) parts of the input image would be required to compose the output image. We introduce a new image warping method, Metameric image inpainting - an approach for hole-filling in real-time with foundations in human visual perception. Our method estimates image feature statistics of disoccluded regions from their neighbours. These statistics are inpainted and used to synthesise visuals in real-time that are less noticeable to study participants, particularly in peripheral vision. Our method offers speed improvements over the standard structured image inpainting methods while improving realism over colour-based inpainting such as push-pull. Hence, our work paves the way towards future applications such as depth image-based rendering, 6-DoF 360 rendering, and remote render-streaming.},
  archive      = {J_TVCG},
  author       = {Rafael Kuffner dos Anjos and David Walton and Kaan Akşit and Sebastian Friston and David Swapp and Anthony Steed and Tobias Ritschel},
  doi          = {10.1109/TVCG.2022.3216712},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5511-5522},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Metameric inpainting for image warping},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rainbow colormaps: What are they good and bad for?
<em>TVCG</em>, <em>29</em>(12), 5496–5510. (<a
href="https://doi.org/10.1109/TVCG.2022.3214771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guidelines for color use in quantitative visualizations have strongly discouraged the use of rainbow colormaps, arguing instead for smooth designs that do not induce visual discontinuities or implicit color categories. However, the empirical evidence behind this argument has been mixed and, at times, even contradictory. In practice, rainbow colormaps are widely used, raising questions about the true utility or dangers of such designs. We study how color categorization impacts the interpretation of scalar fields. We first introduce an approach to detect latent categories in colormaps. We hypothesize that the appearance of color categories in scalar visualizations can be beneficial in that they enhance the perception of certain features, although at the cost of rendering other features less noticeable. In three crowdsourced experiments, we show that observers are more likely to discriminate global, distributional features when viewing colorful scales that induce categorization (e.g., rainbow or diverging schemes). Conversely, when seeing the same data through a less colorful representation, observers are more likely to report localized features defined by small variations in the data. Participants showed awareness of these different affordances, and exhibited bias for exploiting the more discriminating colormap, given a particular feature type. Our results demonstrate costs and benefits for rainbows (and similarly colorful schemes), suggesting that their complementary utility for analyzing scalar data should not be dismissed. In addition to explaining potentially valid uses of rainbow, our study provides actionable guidelines, including on when such designs can be more harmful than useful. Data and materials are available at https://osf.io/xjhtf},
  archive      = {J_TVCG},
  author       = {Khairi Reda},
  doi          = {10.1109/TVCG.2022.3214771},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5496-5510},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rainbow colormaps: What are they good and bad for?},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep hierarchical super resolution for scientific data.
<em>TVCG</em>, <em>29</em>(12), 5483–5495. (<a
href="https://doi.org/10.1109/TVCG.2022.3214420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel technique for hierarchical super resolution (SR) with neural networks (NNs), which upscales volumetric data represented with an octree data structure to a high-resolution uniform gridwith minimal seam artifacts on octree node boundaries. Our method uses existing state-of-the-art SR models and adds flexibility to upscale input data with varying levels of detail across the domain, instead of only uniform grid data that are supported in previous approaches.The key is to use a hierarchy of SR NNs, each trained to perform $2\times$ SR between two levels of detail, with a hierarchical SR algorithm that minimizes seam artifacts by starting from the coarsest level of detail and working up.We show that our hierarchical approach outperforms baseline interpolation and hierarchical upscaling methods, and demonstrate the usefulness of our proposed approach across three use cases including data reduction using hierarchical downsampling+SR instead of uniform downsampling+SR, computation savings for hierarchical finite-time Lyapunov exponent field calculation, and super-resolving low-resolution simulation results for a high-resolution approximation visualization.},
  archive      = {J_TVCG},
  author       = {Skylar W. Wurster and Hanqi Guo and Han-Wei Shen and Tom Peterka and Jiayi Xu},
  doi          = {10.1109/TVCG.2022.3214420},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5483-5495},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep hierarchical super resolution for scientific data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Out of the plane: Flower versus star glyphs to support
high-dimensional exploration in two-dimensional embeddings.
<em>TVCG</em>, <em>29</em>(12), 5468–5482. (<a
href="https://doi.org/10.1109/TVCG.2022.3216919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring high-dimensional data is a common task in many scientific disciplines. To address this task, two-dimensional embeddings, such as tSNE and UMAP, are widely used. While these determine the 2D position of data items, effectively encoding the first two dimensions, suitable visual encodings can be employed to communicate higher-dimensional features. To investigate such encodings, we have evaluated two commonly used glyph types, namely flower glyphs and star glyphs. To evaluate their capabilities for communicating higher-dimensional features in two-dimensional embeddings, we ran a large set of crowd-sourced user studies using real-world data obtained from data.gov. During these studies, participants completed a broad set of relevant tasks derived from related research. This article describes the evaluated glyph designs, details our tasks, and the quantitative study setup before discussing the results. Finally, we will present insights and provide guidance on the choice of glyph encodings when exploring high-dimensional data.},
  archive      = {J_TVCG},
  author       = {Christian van Onzenoodt and Pere-Pau Vázquez and Timo Ropinski},
  doi          = {10.1109/TVCG.2022.3216919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5468-5482},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Out of the plane: Flower versus star glyphs to support high-dimensional exploration in two-dimensional embeddings},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GoTreeScape: Navigate and explore the tree visualization
design space. <em>TVCG</em>, <em>29</em>(12), 5451–5467. (<a
href="https://doi.org/10.1109/TVCG.2022.3215070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Declarative grammar is becoming an increasingly important technique for understanding visualization design spaces. The GoTreeScape system presented in the paper allows users to navigate and explore the vast design space implied by GoTree, a declarative grammar for visualizing tree structures. To provide an overview of the design space, GoTreeScape, which is based on an encoder-decoder architecture, projects the tree visualizations onto a 2D landscape. Significantly, this landscape takes the relationships between different design features into account. GoTreeScape also includes an exploratory framework that allows top-down, bottom-up, and hybrid modes of exploration to support the inherently undirected nature of exploratory searches. Two case studies demonstrate the diversity with which GoTreeScape expands the universe of designed tree visualizations for users. The source code associated with GoTreeScape is available at https://github.com/bitvis2021/gotreescape .},
  archive      = {J_TVCG},
  author       = {Guozheng Li and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2022.3215070},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5451-5467},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GoTreeScape: Navigate and explore the tree visualization design space},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward feature-preserving vector field compression.
<em>TVCG</em>, <em>29</em>(12), 5434–5450. (<a
href="https://doi.org/10.1109/TVCG.2022.3214821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this work is to develop error-bounded lossy compression methods to preserve topological features in 2D and 3D vector fields. Specifically, we explore the preservation of critical points in piecewise linear and bilinear vector fields. We define the preservation of critical points as, without any false positive, false negative, or false type in the decompressed data, (1) keeping each critical point in its original cell and (2) retaining the type of each critical point (e.g., saddle and attracting node). The key to our method is to adapt a vertex-wise error bound for each grid point and to compress input data together with the error bound field using a modified lossy compressor. Our compression algorithm can be also embarrassingly parallelized for large data handling and in situ processing. We benchmark our method by comparing it with existing lossy compressors in terms of false positive/negative/type rates, compression ratio, and various vector field visualizations with several scientific applications.},
  archive      = {J_TVCG},
  author       = {Xin Liang and Sheng Di and Franck Cappello and Mukund Raj and Chunhui Liu and Kenji Ono and Zizhong Chen and Tom Peterka and Hanqi Guo},
  doi          = {10.1109/TVCG.2022.3214821},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5434-5450},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward feature-preserving vector field compression},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A psychoacoustic quality criterion for path-traced sound
propagation. <em>TVCG</em>, <em>29</em>(12), 5422–5433. (<a
href="https://doi.org/10.1109/TVCG.2022.3213514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In developing virtual acoustic environments, it is important to understand the relationship between the computation cost and the perceptual significance of the resultant numerical error. In this article, we propose a quality criterion that evaluates the error significance of path-tracing-based sound propagation simulators. We present an analytical formula that estimates the error signal power spectrum. With this spectrum estimation, we can use a modified Zwicker&#39;s loudness model to calculate the relative loudness of the error signal masked by the ideal output. Our experimental results show that the proposed criterion can explain the human perception of simulation error in a variety of cases.},
  archive      = {J_TVCG},
  author       = {Chunxiao Cao and Zili An and Zhong Ren and Dinesh Manocha and Kun Zhou},
  doi          = {10.1109/TVCG.2022.3213514},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5422-5433},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A psychoacoustic quality criterion for path-traced sound propagation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting the design patterns of composite visualizations.
<em>TVCG</em>, <em>29</em>(12), 5406–5421. (<a
href="https://doi.org/10.1109/TVCG.2022.3213565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite visualization is a popular design strategy that represents complex datasets by integrating multiple visualizations in a meaningful and aesthetic layout, such as juxtaposition, overlay, and nesting. With this strategy, numerous novel designs have been proposed in visualization publications to accomplish various visual analytic tasks. However, there is a lack of understanding of design patterns of composite visualization, thus failing to provide holistic design space and concrete examples for practical use. In this article, we opted to revisit the composite visualizations in IEEE VIS publications and answered what and how visualizations of different types are composed together. To achieve this, we first constructed a corpus of composite visualizations from the publications and analyzed common practices, such as the pattern distributions and co-occurrence of visualization types. From the analysis, we obtained insights into different design patterns on the utilities and their potential pros and cons. Furthermore, we discussed usage scenarios of our taxonomy and corpus and how future research on visualization composition can be conducted on the basis of this study.},
  archive      = {J_TVCG},
  author       = {Dazhen Deng and Weiwei Cui and Xiyu Meng and Mengye Xu and Yu Liao and Haidong Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3213565},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5406-5421},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting the design patterns of composite visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A current loop model for the fast simulation of ferrofluids.
<em>TVCG</em>, <em>29</em>(12), 5394–5405. (<a
href="https://doi.org/10.1109/TVCG.2022.3211414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ferrofluids are oil-based liquids containing magnetic particles that interact with magnetic fields without solidifying. Leveraging the exploration of new applications of these promising materials (such as in optics, medicine and engineering) requires high fidelity modeling and simulation capabilities in order to accurately explore ferrofluids in silico . While recent work addressed the macroscopic simulation of large-scale ferrofluids using smoothed-particle hydrodynamics (SPH), such simulations are computationally expensive. In their work, the Kelvin force model has been used to calculate interactions between different SPH particles. The application of this model results in a force pointing outwards with respect to the fluid surface causing significant levitation problems. This drawback limits the application of more advanced and efficient SPH frameworks such as divergence-free SPH (DFSPH) or implicit incompressible SPH (IISPH). In this contribution, we propose a current loop magnetic force model which enables the fast macroscopic simulation of ferrofluids. Our new force model results in a force term pointing inwards allowing for more stable and fast simulations of ferrofluids using DFSPH and IISPH.},
  archive      = {J_TVCG},
  author       = {Han Shao and Libo Huang and Dominik L. Michels},
  doi          = {10.1109/TVCG.2022.3211414},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5394-5405},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A current loop model for the fast simulation of ferrofluids},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast computation of neck-like features. <em>TVCG</em>,
<em>29</em>(12), 5384–5393. (<a
href="https://doi.org/10.1109/TVCG.2022.3211781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating neck-like features, or locally narrow parts, of a surface is crucial in various applications such as segmentation, shape analysis, path planning, and robotics. Topological methods are often utilized to find the set of shortest loops around handles and tunnels. However, there are abundant neck-like features on genus-0 shapes without any handles. While 3D geometry-aware topological approaches exist to find neck loops, their construction can be cumbersome and may even lead to geometrically wide loops. Thus we propose a “topology-aware geometric approach” to compute the tightest loops around neck features on surfaces, including genus-0 surfaces. Our algorithm starts with a volumetric representation of an input surface and then calculates the distance function of mesh points to the boundary surface as a Morse function. All neck features induce critical points of this Morse function where the Hessian matrix has precisely one positive eigenvalue, i.e., type-2 saddles. As we focus on geometric neck features, we bypass a topological construction such as the Morse-Smale complex or a lower-star filtration. Instead, we directly create a cutting plane through each neck feature. Each resulting loop can then be tightened to form a closed geodesic representation of the neck feature. Moreover, we offer criteria to measure the significance of a neck feature through the evolution of critical points when smoothing the distance function. Furthermore, we speed up the detection process through mesh simplification without compromising the quality of the output loops.},
  archive      = {J_TVCG},
  author       = {Hayam Abdelrahman and Yiying Tong},
  doi          = {10.1109/TVCG.2022.3211781},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5384-5393},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast computation of neck-like features},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting bell curves to data distributions using
visualization. <em>TVCG</em>, <em>29</em>(12), 5372–5383. (<a
href="https://doi.org/10.1109/TVCG.2022.3210763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This article shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation—which we dub the “umbrella effect” because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above—and that strip plots yield the best accuracy.},
  archive      = {J_TVCG},
  author       = {Eric Newburger and Michael Correll and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2022.3210763},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5372-5383},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fitting bell curves to data distributions using visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Electromechanical coupling in electroactive polymers – a
visual analysis of a third-order tensor field. <em>TVCG</em>,
<em>29</em>(12), 5357–5371. (<a
href="https://doi.org/10.1109/TVCG.2022.3209328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroactive polymers are frequently used in engineering applications due to their ability to change their shape and properties under the influence of an electric field. This process also works vice versa, such that mechanical deformation of the material induces an electric field in the EAP device. This specific behavior makes such materials highly attractive for the construction of actuators and sensors in various application areas. The electromechanical behaviour of electroactive polymers can be described by a third-order coupling tensor, which represents the sensitivity of mechanical stresses concerning the electric field, i.e., it establishes a relation between a second-order and a first-order tensor field. Due to this coupling tensor&#39;s complexity and the lack of meaningful visualization methods for third-order tensors in general, an interpretation of the tensor is rather difficult. Thus, the central engineering research question that this contribution deals with is a deeper understanding of electromechanical coupling by analyzing the third-order coupling tensor with the help of specific visualization methods. Starting with a deviatoric decomposition of the tensor, the multipoles of each deviator are visualized, which allows a first insight into this highly complex third-order tensor. In the present contribution, four examples, including electromechanical coupling, are simulated within a finite element framework and subsequently analyzed using the tensor visualization method.},
  archive      = {J_TVCG},
  author       = {Chiara Hergl and Carina Witt and Baldwin Nsonga and Andreas Menzel and Gerik Scheuermann},
  doi          = {10.1109/TVCG.2022.3209328},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5357-5371},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Electromechanical coupling in electroactive polymers – a visual analysis of a third-order tensor field},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DOMINO: Visual causal reasoning with time-dependent
phenomena. <em>TVCG</em>, <em>29</em>(12), 5342–5356. (<a
href="https://doi.org/10.1109/TVCG.2022.3207929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current work on using visual analytics to determine causal relations among variables has mostly been based on the concept of counterfactuals. As such the derived static causal networks do not take into account the effect of time as an indicator. However, knowing the time delay of a causal relation can be crucial as it instructs how and when actions should be taken. Yet, similar to static causality, deriving causal relations from observational time-series data, as opposed to designed experiments, is not a straightforward process. It can greatly benefit from human insight to break ties and resolve errors. We hence propose a set of visual analytics methods that allow humans to participate in the discovery of causal relations associated with windows of time delay. Specifically, we leverage a well-established method, logic-based causality, to enable analysts to test the significance of potential causes and measure their influences toward a certain effect. Furthermore, since an effect can be a cause of other effects, we allow users to aggregate different temporal cause-effect relations found with our method into a visual flow diagram to enable the discovery of temporal causal networks. To demonstrate the effectiveness of our methods we constructed a prototype system named DOMINO and showcase it via a number of case studies using real-world datasets. Finally, we also used DOMINO to conduct several evaluations with human analysts from different science domains in order to gain feedback on the utility of our system in practical scenarios.},
  archive      = {J_TVCG},
  author       = {Jun Wang and Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3207929},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5342-5356},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DOMINO: Visual causal reasoning with time-dependent phenomena},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural global illumination: Interactive indirect
illumination prediction under dynamic area lights. <em>TVCG</em>,
<em>29</em>(12), 5325–5341. (<a
href="https://doi.org/10.1109/TVCG.2022.3209963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose neural global illumination, a novel method for fast rendering full global illumination in static scenes with dynamic viewpoint and area lighting. The key idea of our method is to utilize a deep rendering network to model the complex mapping from each shading point to global illumination. To efficiently learn the mapping, we propose a neural-network-friendly input representation including attributes of each shading point, viewpoint information, and a combinational lighting representation that enables high-quality fitting with a compact neural network. To synthesize high-frequency global illumination effects, we transform the low-dimension input to higher-dimension space by positional encoding and model the rendering network as a deep fully-connected network. Besides, we feed a screen-space neural buffer to our rendering network to share global information between objects in the screen-space to each shading point. We have demonstrated our neural global illumination method in rendering a wide variety of scenes exhibiting complex and all-frequency global illumination effects such as multiple-bounce glossy interreflection, color bleeding, and caustics.},
  archive      = {J_TVCG},
  author       = {Duan Gao and Haoyuan Mu and Kun Xu},
  doi          = {10.1109/TVCG.2022.3209963},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5325-5341},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural global illumination: Interactive indirect illumination prediction under dynamic area lights},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A segmented redirection mapping method for roadmaps of large
constrained virtual environments. <em>TVCG</em>, <em>29</em>(12),
5308–5324. (<a href="https://doi.org/10.1109/TVCG.2022.3207004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) enables users to explore large virtual spaces by real walking in small real spaces. How to effectively reduce physical collisions and decrease user perceptions of redirection are important for most RDW methods. This article proposes a segmented redirection mapping method to calculate and map the roadmap of a large virtual space with inner obstacles to a mapped roadmap within a small real space. We adopt a Voronoi-based pruning method to extract the roadmap of the virtual space and design an RDW platform to interactively modify the virtual roadmap. We propose a roadmap mapping method based on divide-and-conquer and dynamic planning strategies to subdivide the virtual roadmap into several sub-virtual roads that are mapped individually. By recording connections of different sub-virtual roads, our method is applicable to virtual roadmaps with loop structures. During mapping, we apply the reset and redirection gains of the RDW technique as optimal aims and restrict conditions to obtain the mapped roadmap, which has small path curving and contains as few resets as possible. By real walking along the mapped roadmap, users perceive moving along the virtual roadmap to explore the entire virtual space. The experiment shows that our method works effectively for various virtual spaces with or without inner obstacles. Furthermore, our method is flexible in obtaining mapped roadmaps of different real spaces when the virtual space is fixed. Compared to prevalent RDW methods, our method can significantly reduce physical boundary collisions and maintain user experience of virtual roaming.},
  archive      = {J_TVCG},
  author       = {Huiyu Li and Linwei Fan},
  doi          = {10.1109/TVCG.2022.3207004},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5308-5324},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A segmented redirection mapping method for roadmaps of large constrained virtual environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the evolution of sensemaking strategies in
immersive space to think. <em>TVCG</em>, <em>29</em>(12), 5294–5307. (<a
href="https://doi.org/10.1109/TVCG.2022.3207357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on immersive analytics to support the sensemaking process focuses on single-session sensemaking tasks. However, in the wild, sensemaking can take days or months to complete. In order to understand the full benefits of immersive analytic systems, we need to understand how immersive analytic systems provide flexibility for the dynamic nature of the sensemaking process. In our work, we build upon an existing immersive analytic system – Immersive Space to Think, to evaluate how immersive analytic systems can support sensemaking tasks over time. We conducted a user study with eight participants with three separate analysis sessions each. We found significant differences between analysis strategies between sessions one, two, and three, which suggest that immersive space to think can benefit analysts during multiple stages in the sensemaking process.},
  archive      = {J_TVCG},
  author       = {Kylie Davidson and Lee Lisle and Kirsten Whitley and Doug A. Bowman and Chris North},
  doi          = {10.1109/TVCG.2022.3207357},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5294-5307},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the evolution of sensemaking strategies in immersive space to think},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensemaking sans power: Interactive data visualization using
color-changing ink. <em>TVCG</em>, <em>29</em>(12), 5282–5293. (<a
href="https://doi.org/10.1109/TVCG.2022.3209631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this article, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.},
  archive      = {J_TVCG},
  author       = {Biswaksen Patnaik and Huaishu Peng and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2022.3209631},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5282-5293},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sensemaking sans power: Interactive data visualization using color-changing ink},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating continuous and teleporting VR locomotion into a
seamless “HyperJump” paradigm. <em>TVCG</em>, <em>29</em>(12),
5265–5281. (<a href="https://doi.org/10.1109/TVCG.2022.3207157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous locomotion in VR provides uninterrupted optical flow, which mimics real-world locomotion and supports path integration . However, optical flow limits the maximum speed and acceleration that can be effectively used without inducing cybersickness. In contrast, teleportation provides neither optical flow nor acceleration cues, and users can jump to any length without increasing cybersickness. However, teleportation cannot support continuous spatial updating and can increase disorientation. Thus, we designed ‘HyperJump’ in an attempt to merge benefits from continuous locomotion and teleportation. HyperJump adds iterative jumps every half a second on top of the continuous movement and was hypothesized to facilitate faster travel without compromising spatial awareness/orientation. In a user study, Participants travelled around a naturalistic virtual city with and without HyperJump (equivalent maximum speed). They followed waypoints to new landmarks, stopped near them and pointed back to all previously visited landmarks in random order. HyperJump was added to two continuous locomotion interfaces (controller- and leaning-based). Participants had better spatial awareness/orientation with leaning-based interfaces compared to controller-based (assessed via rapid pointing). With HyperJump, participants travelled significantly faster, while staying on the desired course without impairing their spatial knowledge. This provides evidence that optical flow can be effectively limited such that it facilitates faster travel without compromising spatial orientation. In future design iterations, we plan to utilize audio-visual effects to support jumping metaphors that help users better anticipate and interpret jumps, and use much larger virtual environments requiring faster speeds, where cybersickness will become increasingly prevalent and thus teleporting will become more important.},
  archive      = {J_TVCG},
  author       = {Ashu Adhikari and Daniel Zielasko and Ivan Aguilar and Alexander Bretin and Ernst Kruijff and Markus von der Heyde and Bernhard E. Riecke},
  doi          = {10.1109/TVCG.2022.3207157},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5265-5281},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integrating continuous and teleporting VR locomotion into a seamless ‘HyperJump’ paradigm},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Yarn-level simulation of hygroscopicity of woven textiles.
<em>TVCG</em>, <em>29</em>(12), 5250–5264. (<a
href="https://doi.org/10.1109/TVCG.2022.3206579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating liquid-textile interaction has received great attention in computer graphics recently. Most existing methods take textiles as particles or parameterized meshes. Although these methods can generate visually pleasing results, they cannot simulate water content at a microscopic level due to the lack of geometrically modeling of textile&#39;s anisotropic structure. In this paper, we develop a method for yarn-level simulation of hygroscopicity of textiles and evaluate it using various quantitative metrics. We model textiles in a fiber-yarn-fabric multi-scale manner and consider the dynamic coupled physical mechanisms of liquid spreading, including wetting, wicking, moisture sorption/desorption, and transient moisture-heat transfer in textiles. Our method can accurately simulate liquid spreading on textiles with different fiber materials and geometrical structures with consideration of air temperatures and humidity conditions. It visualizes the hygroscopicity of textiles to demonstrate their moisture management ability. We conduct qualitative and quantitative experiments to validate our method and explore various factors to analyze their influence on liquid spreading and hygroscopicity of textiles.},
  archive      = {J_TVCG},
  author       = {Aihua Mao and Wenbo Dong and Chaoqiang Xie and Huamin Wang and Yong-Jin Liu and Guiqing Li and Ying He},
  doi          = {10.1109/TVCG.2022.3206579},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5250-5264},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Yarn-level simulation of hygroscopicity of woven textiles},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and accurate illumination estimation using LDR
panoramic images for realistic rendering. <em>TVCG</em>,
<em>29</em>(12), 5235–5249. (<a
href="https://doi.org/10.1109/TVCG.2022.3205614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A high dynamic range (HDR) image is commonly used to reveal stereo illumination, which is crucial for generating high-quality realistic rendering effects. Compared to the high-cost HDR imaging technique, low dynamic range (LDR) imaging provides a low-cost alternative and is preferable for interactive graphics applications. However, the limited LDR pixel bit depth significantly bothers accurate illumination estimation using LDR images. The conflict between the realism and promptness of illumination estimation for realistic rendering is yet to be resolved. In this paper, an efficient method that accurately infers illuminations of real-world scenes using LDR panoramic images is proposed. It estimates multiple lighting parameters, including locations, types and intensities of light sources. In our approach, a new algorithm that extracts illuminant characteristics during the exposure attenuation process is developed to locate light sources and outline their boundaries. To better predict realistic illuminations, a new deep learning model is designed to efficiently parse complex LDR panoramas and classify detected light sources. Finally, realistic illumination intensities are calculated by recovering the inverse camera response function and extending the dynamic range of pixel values based on previously estimated parameters of light sources. The reconstructed radiance map can be used to compute high-quality image-based lighting of virtual models. Experimental results demonstrate that the proposed method is capable of efficiently and accurately computing comprehensive illuminations using LDR images. Our method can be used to produce better realistic rendering results than existing approaches.},
  archive      = {J_TVCG},
  author       = {Haojie Cheng and Chunxiao Xu and Jiajun Wang and Zhenxin Chen and Lingxiao Zhao},
  doi          = {10.1109/TVCG.2022.3205614},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5235-5249},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast and accurate illumination estimation using LDR panoramic images for realistic rendering},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Putting vision and touch into conflict: Results from a
multimodal mixed reality setup. <em>TVCG</em>, <em>29</em>(12),
5224–5234. (<a href="https://doi.org/10.1109/TVCG.2022.3207241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What happens if we put vision and touch into conflict? Which modality “wins”? Although several previous studies have addressed this topic, they have solely focused on integration of vision and touch for low-level object properties (such as curvature, slant, or depth). In the present study, we introduce a multimodal mixed-reality setup based on real-time hand-tracking, which was used to display real-world, haptic exploration of objects in a virtual environment through a head-mounted-display (HMD). With this setup we studied multimodal conflict situations of objects varying along higher-level, parametrically-controlled global shape properties. Participants explored these objects in both unimodal and multimodal settings with the latter including congruent and incongruent conditions and differing instructions for weighting the input modalities. Results demonstrated a surprisingly clear touch dominance throughout all experiments, which in addition was only marginally influenceable through instructions to bias their modality weighting. We also present an initial analysis of the hand-tracking patterns that illustrates the potential for our setup to investigate exploration behavior in more detail.},
  archive      = {J_TVCG},
  author       = {Hyeokmook Kang and Taeho Kang and Christian Wallraven},
  doi          = {10.1109/TVCG.2022.3207241},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5224-5234},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Putting vision and touch into conflict: Results from a multimodal mixed reality setup},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic literature review of virtual reality locomotion
taxonomies. <em>TVCG</em>, <em>29</em>(12), 5208–5223. (<a
href="https://doi.org/10.1109/TVCG.2022.3206915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The change of the user&#39;s viewpoint in an immersive virtual environment, called locomotion, is one of the key components in a virtual reality interface. Effects of locomotion, such as simulator sickness or disorientation, depend on the specific design of the locomotion method and can influence the task performance as well as the overall acceptance of the virtual reality system. Thus, it is important that a locomotion method achieves the intended effects. The complexity of this task has increased with the growing number of locomotion methods and design choices in recent years. Locomotion taxonomies are classification schemes that group multiple locomotion methods and can aid in the design and selection of locomotion methods. Like locomotion methods themselves, there exist multiple locomotion taxonomies, each with a different focus and, consequently, a different possible outcome. However, there is little research that focuses on locomotion taxonomies. We performed a systematic literature review to provide an overview of possible locomotion taxonomies and analysis of possible decision criteria such as impact, common elements, and use cases for locomotion taxonomies. We aim to support future research on the design, choice, and evaluation of locomotion taxonomies and thereby support future research on virtual reality locomotion.},
  archive      = {J_TVCG},
  author       = {Lisa Marie Prinz and Tintu Mathew and Benjamin Weyers},
  doi          = {10.1109/TVCG.2022.3206915},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5208-5223},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A systematic literature review of virtual reality locomotion taxonomies},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning reliable gradients from undersampled circular light
field for 3D reconstruction. <em>TVCG</em>, <em>29</em>(12), 5194–5207.
(<a href="https://doi.org/10.1109/TVCG.2022.3206207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a 3D reconstruction algorithm from an undersampled circular light ﬁeld (LF). With an ultra-dense angular sampling rate, every scene point captured by a circular LF corresponds to a smooth trajectory in the circular epipolar plane volume (CEPV). Thus per-pixel disparities can be calculated by retrieving the local gradients of the CEPV-trajectories. However, the continuous curve will be broken up into discrete segments in an undersampled circular LF, which leads to a noticeable deterioration of the 3D reconstruction accuracy. We observe that the coherent structure is still embedded in the discrete segments. With less noise and ambiguity, the scene points can be reconstructed using gradients from reliable epipolar plane image (EPI) regions. By analyzing the geometric characteristics of the coherent structure in the CEPV, both the trajectory itself and its gradients could be modeled as 3D predictable series. Thus a mask-guided CNN+LSTM network is proposed to learn the mapping from the CEPV with a lower angular sampling rate to the gradients under a higher angular sampling rate. To segment the reliable regions, the reliable-mask-based loss that assesses the difference between learned gradients and ground truth gradients is added to the loss function. We construct a synthetic circular LF dataset with ground truth for depth and foreground/background segmentation to train the network. Moreover, a real-scene circular LF dataset is collected for performance evaluation. Experimental results on both public and self-constructed datasets demonstrate the superiority of the proposed method over existing state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Zhengxi Song and Xue Wang and Hao Zhu and Guoqing Zhou and Qing Wang},
  doi          = {10.1109/TVCG.2022.3206207},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5194-5207},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning reliable gradients from undersampled circular light field for 3D reconstruction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Team-builder: Toward more effective lineup selection in
soccer. <em>TVCG</em>, <em>29</em>(12), 5178–5193. (<a
href="https://doi.org/10.1109/TVCG.2022.3207147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lineup selection is an essential and important task in soccer matches. To win a match, coaches must consider various factors and select appropriate players for a planned formation. Computation-based tools have been proposed to help coaches on this complex task, but they are usually based on over-simplified models on player performances, do not support interactive analysis, and overlook the inputs by coaches. In this article, we propose a method for visual analytics of soccer lineup selection by tackling two challenges: characterizing essential factors involved in generating optimal lineup, and supporting coach-driven visual analytics of lineup selection. We develop a lineup selection model that integrates such important factors, such as spatial regions of player actions and defensive interactions with opponent players. A visualization system, Team-Builder, is developed to help coaches control the process of lineup generation, explanation, and comparison through multiple coordinated views. The usefulness and effectiveness of our system are demonstrated by two case studies on a real-world soccer event dataset.},
  archive      = {J_TVCG},
  author       = {Anqi Cao and Ji Lan and Xiao Xie and Hongyu Chen and Xiaolong Zhang and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3207147},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5178-5193},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Team-builder: Toward more effective lineup selection in soccer},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScrollyVis: Interactive visual authoring of guided dynamic
narratives for scientific scrollytelling. <em>TVCG</em>,
<em>29</em>(12), 5165–5177. (<a
href="https://doi.org/10.1109/TVCG.2022.3205769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual stories are an effective and powerful tool to convey specific information to a diverse public. Scrollytelling is a recent visual storytelling technique extensively used on the web, where content appears or changes as users scroll up or down a page. By employing the familiar gesture of scrolling as its primary interaction mechanism, it provides users with a sense of control, exploration and discoverability while still offering a simple and intuitive interface. In this article, we present a novel approach for authoring, editing, and presenting data-driven scientific narratives using scrollytelling. Our method flexibly integrates common sources such as images, text, and video, but also supports more specialized visualization techniques such as interactive maps as well as scalar field and mesh data visualizations. We show that scrolling navigation can be used to traverse dynamic narratives and demonstrate how it can be combined with interactive parameter exploration. The resulting system consists of an extensible web-based authoring tool capable of exporting stand-alone stories that can be hosted on any web server. We demonstrate the power and utility of our approach with case studies from several diverse scientific fields and with a user study including 12 participants of diverse professional backgrounds. Furthermore, an expert in creating interactive articles assessed the usefulness of our approach and the quality of the created stories.},
  archive      = {J_TVCG},
  author       = {Eric Mörth and Stefan Bruckner and Noeska N. Smit},
  doi          = {10.1109/TVCG.2022.3205769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5165-5177},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ScrollyVis: Interactive visual authoring of guided dynamic narratives for scientific scrollytelling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PropelWalker: A leg-based wearable system with
propeller-based force feedback for walking in fluids in VR.
<em>TVCG</em>, <em>29</em>(12), 5149–5164. (<a
href="https://doi.org/10.1109/TVCG.2022.3205181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increasing focus on haptic interfaces for virtual reality (VR), to support a high-quality touch experience. However, it is still challenging to haptically simulate the real-world walking experience in different fluid mediums. To tackle this problem, we present PropelWalker , a pair of calf-worn haptic devices for simulating the buoyancy and the resistant force when the human&#39;s lower limbs are interacting with different fluids and materials in VR. By using four ducted fans, two installed on each calf, the system can control the strength and the direction of the airflow in real time to provide different levels of force. Our technical evaluation shows that PropelWalker can generate vertical forces up to 27N in two directions (i.e., upward and downward) within 0.85 seconds. Furthermore, the system can stably maintain the generated force with minor turbulence. We further conducted three user-perception studies to understand the capability of PropelWalker to generate distinguishable force stimuli. First, we conducted the just-noticeable-difference (JND) experiments to investigate the threshold of the human perception of on-leg air-flow force feedback. Our second perception study showed that users could distinguish four PropelWalker -generated force levels for simulating different walking mediums (i.e., dry ground, water, mud, and sand), with an average accuracy of 94.2\%. Lastly, our VR user study showed that PropelWalker could significantly improve the users’ sense of presence in VR.},
  archive      = {J_TVCG},
  author       = {Pingchuan Ke and Shaoyu Cai and Haichen Gao and Kening Zhu},
  doi          = {10.1109/TVCG.2022.3205181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5149-5164},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PropelWalker: A leg-based wearable system with propeller-based force feedback for walking in fluids in VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effects of spatial complexity on narrative experience in
space-adaptive AR storytelling. <em>TVCG</em>, <em>29</em>(12),
5137–5148. (<a href="https://doi.org/10.1109/TVCG.2022.3201934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical yet unresolved challenge in designing space-adaptive narratives for Augmented Reality (AR) is to provide consistently immersive user experiences anywhere, regardless of physical features specific to a space. For this, we present a comprehensive analysis on a series of user studies investigating how the size, density, and layout of real indoor spaces affect users playing Fragments, a space-adaptive AR detective game. Based on the studies, we assert that moderate levels of traversability and visual complexity afforded in counteracting combinations of size and complexity are beneficial for narrative experience. To confirm our argument, we combined the experimental data of the studies (n=112) to compare how five different spatial complexity conditions impact narrative experience when applied to contrasting room sizes. Results show that whereas factors of narrative experience are rated significantly higher in relatively simple settings for a small space, they are less affected by complexity in a large space. Ultimately, we establish guidelines on the design and placement of space-adaptive augmentations in location-independent AR narratives to compensate for the lack or excess of affordances in various real spaces and enhance user experiences therein.},
  archive      = {J_TVCG},
  author       = {Jae-eun Shin and Boram Yoon and Dooyoung Kim and Woontack Woo},
  doi          = {10.1109/TVCG.2022.3201934},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5137-5148},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effects of spatial complexity on narrative experience in space-adaptive AR storytelling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recursive-NeRF: An efficient and dynamically growing NeRF.
<em>TVCG</em>, <em>29</em>(12), 5124–5136. (<a
href="https://doi.org/10.1109/TVCG.2022.3204608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on public datasets and a large-scale scene dataset we collected shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF},
  archive      = {J_TVCG},
  author       = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
  doi          = {10.1109/TVCG.2022.3204608},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5124-5136},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Recursive-NeRF: An efficient and dynamically growing NeRF},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical sampling for the visualization of large
scale-free graphs. <em>TVCG</em>, <em>29</em>(12), 5111–5123. (<a
href="https://doi.org/10.1109/TVCG.2022.3201567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph sampling frequently compresses a large graph into a limited screen space. This paper proposes a hierarchical structure model that partitions scale-free graphs into three blocks: the core, which captures the underlying community structure, the vertical graph, which represents minority structures that are important in visual analysis, and the periphery, which describes the connection structure between low-degree nodes. A new algorithm named hierarchical structure sampling (HSS) was then designed to preserve the characteristics of the three blocks, including complete replication of the connection relationship between high-degree nodes in the core, joint node/degree distribution between high- and low-degree nodes in the vertical graph, and proportional replication of the connection relationship between low-degree nodes in the periphery. Finally, the importance of some global statistical properties in visualization was analyzed. Both the global statistical properties and local visual features were used to evaluate the proposed algorithm, which verify that the algorithm can be applied to sample scale-free graphs with hundreds to one million nodes from a visualization perspective.},
  archive      = {J_TVCG},
  author       = {Bo Jiao and Xin Lu and Jingbo Xia and Brij Bhooshan Gupta and Lei Bao and Qingshan Zhou},
  doi          = {10.1109/TVCG.2022.3201567},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5111-5123},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hierarchical sampling for the visualization of large scale-free graphs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NerfCap: Human performance capture with dynamic neural
radiance fields. <em>TVCG</em>, <em>29</em>(12), 5097–5110. (<a
href="https://doi.org/10.1109/TVCG.2022.3202503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of human performance capture from sparse multi-view or monocular videos. Given a template mesh of the performer, previous methods capture the human motion by non-rigidly registering the template mesh to images with 2D silhouettes or dense photometric alignment. However, the detailed surface deformation cannot be recovered from the silhouettes, while the photometric alignment suffers from instability caused by appearance variation in the videos. To solve these problems, we propose NerfCap, a novel performance capture method based on the dynamic neural radiance field (NeRF) representation of the performer. Specifically, a canonical NeRF is initialized from the template geometry and registered to the video frames by optimizing the deformation field and the appearance model of the canonical NeRF. To capture both large body motion and detailed surface deformation, NerfCap combines linear blend skinning with embedded graph deformation. In contrast to the mesh-based methods that suffer from fixed topology and texture, NerfCap is able to flexibly capture complex geometry and appearance variation across the videos, and synthesize more photo-realistic images. In addition, NerfCap can be pre-trained end to end in a self-supervised manner by matching the synthesized videos with the input videos. Experimental results on various datasets show that NerfCap outperforms prior works in terms of both surface reconstruction accuracy and novel-view synthesis quality.},
  archive      = {J_TVCG},
  author       = {Kangkan Wang and Sida Peng and Xiaowei Zhou and Jian Yang and Guofeng Zhang},
  doi          = {10.1109/TVCG.2022.3202503},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5097-5110},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NerfCap: Human performance capture with dynamic neural radiance fields},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to infer inner-body under clothing from monocular
video. <em>TVCG</em>, <em>29</em>(12), 5083–5096. (<a
href="https://doi.org/10.1109/TVCG.2022.3202240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the human inner-body under clothing is very important for body measurement, virtual try-on and VR/AR applications. In this article, we propose the first method to allow everyone to easily reconstruct their own 3D inner-body under daily clothing from a self-captured video with the mean reconstruction error of 0.73cm within 15s. This avoids privacy concerns arising from nudity or minimal clothing. Specifically, we propose a novel two-stage framework with a Semantic-guided Undressing Network (SUNet) and an Intra-Inter Transformer Network (IITNet). SUNet learns semantically related body features to alleviate the complexity and uncertainty of directly estimating 3D inner-bodies under clothing. IITNet reconstructs the 3D inner-body model by making full use of intra-frame and inter-frame information, which addresses the misalignment of inconsistent poses in different frames. Experimental results on both public datasets and our collected dataset demonstrate the effectiveness of the proposed method. The code and dataset is available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/Inner-Body},
  archive      = {J_TVCG},
  author       = {Xiongzheng Li and Jing Huang and Jinsong Zhang and Xiaokun Sun and Haibiao Xuan and Yu-Kun Lai and Yingdi Xie and Jingyu Yang and Kun Li},
  doi          = {10.1109/TVCG.2022.3202240},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5083-5096},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning to infer inner-body under clothing from monocular video},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Systematic review of augmented reality training systems.
<em>TVCG</em>, <em>29</em>(12), 5062–5082. (<a
href="https://doi.org/10.1109/TVCG.2022.3201120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent augmented reality (AR) advancements have enabled the development of effective training systems, especially in the medical, rehabilitation, and industrial fields. However, it is unclear from the literature what the intrinsic value of AR to training is and how it differs across multiple application fields. In this work, we gathered and reviewed the prototypes and applications geared towards training the intended user&#39;s knowledge, skills, and abilities. Specifically, from IEEE Xplore plus other digital libraries, we collected 64 research papers present in high-impact publications about augmented reality training systems (ARTS). All 64 papers were then categorized according to the training method used, and each paper&#39;s evaluations were identified by validity. The summary of the results shows trends in the training methods and evaluations that incorporate ARTS in each field. The narrative synthesis illustrates the different implementations of AR for each of the training methods. In addition, examples of the different evaluation types of the current ARTS are described for each of the aforementioned training methods. We also investigated the different training strategies used by the prevailing ARTS. The insights gleaned from this review can suggest standards for designing ARTS regarding training strategy, and recommendations are provided for the implementation and evaluation of future ARTS.},
  archive      = {J_TVCG},
  author       = {Isidro III Mendoza Butaslac and Yuichiro Fujimoto and Taishi Sawabe and Masayuki Kanbara and Hirokazu Kato},
  doi          = {10.1109/TVCG.2022.3201120},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5062-5082},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Systematic review of augmented reality training systems},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating talking face with controllable eye movements by
disentangled blinking feature. <em>TVCG</em>, <em>29</em>(12),
5050–5061. (<a href="https://doi.org/10.1109/TVCG.2022.3199412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, talking face generation is committed to using voice and face images to generate real face speech videos to improve the communication experience in the case of limited user information exchange. In a real video, blinking is an action often accompanied by speech, and it is also one of the indispensable actions in real face speech videos. However, the current methods either do not pay attention to the generation of eye movements, or cannot control the blinking in the generated results. To this end, this article proposes a novel system which produces vivid talking face with controllable eye blinks driven by the joint features including identity feature, audio feature, and blink feature. In order to disentangle the blinking action, we designed three independent features to individually drive the main components in the generated frame, namely the facial appearance, mouth movements, and eye movements. Through the adversarial training of the identity encoder, we filter out the information of the eye state from the identity feature, thereby strengthening the independence of the blinking feature. We introduced the blink score as the leading information of the blink feature, and through training, the value can be consistent with human perception to form a complete and independent control of the eyes. Experimental results on multiple datasets show that our method can not only reproduce real talking faces, but also ensure that the blinking pattern and time are fully controllable.},
  archive      = {J_TVCG},
  author       = {Shiguang Liu and Jiaqi Hao},
  doi          = {10.1109/TVCG.2022.3199412},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5050-5061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Generating talking face with controllable eye movements by disentangled blinking feature},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When, where and how does it fail? A spatial-temporal visual
analytics approach for interpretable object detection in autonomous
driving. <em>TVCG</em>, <em>29</em>(12), 5033–5049. (<a
href="https://doi.org/10.1109/TVCG.2022.3201101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arguably the most representative application of artificial intelligence, autonomous driving systems usually rely on computer vision techniques to detect the situations of the external environment. Object detection underpins the ability of scene understanding in such systems. However, existing object detection algorithms often behave as a black box, so when a model fails, no information is available on When, Where and How the failure happened. In this paper, we propose a visual analytics approach to help model developers interpret the model failures. The system includes the micro- and macro- interpreting modules to address the interpretability problem of object detection in autonomous driving. The micro- interpreting module extracts and visualizes the features of a convolutional neural network (CNN) algorithm with density maps, while the macro- interpreting module provides spatial-temporal information of an autonomous driving vehicle and its environment. With the situation awareness of the spatial, temporal and neural network information, our system facilitates the understanding of the results of object detection algorithms, and helps the model developers better understand, tune and develop the models. We use real-world autonomous driving data to perform case studies by involving domain experts in computer vision and autonomous driving to evaluate our system. The results from our interviews with them show the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Junhong Wang and Yun Li and Zhaoyu Zhou and Chengshun Wang and Yijie Hou and Li Zhang and Xiangyang Xue and Michael Kamp and Xiaolong Luke Zhang and Siming Chen},
  doi          = {10.1109/TVCG.2022.3201101},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5033-5049},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {When, where and how does it fail? a spatial-temporal visual analytics approach for interpretable object detection in autonomous driving},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient registration for human surfaces via isometric
regularization on embedded deformation. <em>TVCG</em>, <em>29</em>(12),
5020–5032. (<a href="https://doi.org/10.1109/TVCG.2022.3197383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D registration is a fundamental step to obtain the correspondences between surfaces. Traditional mesh alignment methods tackle this problem through non-rigid deformation, mostly accomplished by applying ICP-based (Iterative Closest Point) optimization. The embedded deformation method is proposed for the purpose of acceleration, which enables various real-time applications. However, it regularizes on an underlying simplified structure, which could be problematic for intricate cases when the simplified graph doesn&#39;t fully represent the surface attributes. Moreover, without elaborate parameter-tuning, deformation usually performs suboptimally, leading to slow convergence or a local minimum if all regions on the surface are assumed to share the same rigidity during the optimization. In this article, we propose a novel solution that decouples regularization from the underlying deformation model by explicitly managing the rigidity of vertex clusters. We further design an efficient two-step solution that alternates between isometric deformation and embedded deformation with cluster-based regularization. Our method can easily support region-adaptive regularization with cluster refinement and execute efficiently. Extensive experiments demonstrate the effectiveness of our approach for mesh alignment tasks even under large-scale deformation and imperfect data. Our method outperforms state-of-the-art methods both numerically and visually.},
  archive      = {J_TVCG},
  author       = {Kunyao Chen and Fei Yin and Bang Du and Baichuan Wu and Truong Q. Nguyen},
  doi          = {10.1109/TVCG.2022.3197383},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5020-5032},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient registration for human surfaces via isometric regularization on embedded deformation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural modeling of portrait bas-relief from a single
photograph. <em>TVCG</em>, <em>29</em>(12), 5008–5019. (<a
href="https://doi.org/10.1109/TVCG.2022.3197354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an end-to-end neural solution to model portrait bas-relief from a single photograph, which is cast as a problem of image-to-depth translation. The main challenge is the lack of bas-relief data for network training. To solve this problem, we propose a semi-automatic pipeline to synthesize bas-relief samples. The main idea is to first construct normal maps from photos, and then generate bas-relief samples by reconstructing pixel-wise depths. In total, our synthetic dataset contains 23 k pixel-wise photo/bas-relief pairs. Since the process of bas-relief synthesis requires a certain amount of user interactions, we propose end-to-end solutions with various network architectures, and train them on the synthetic data. We select the one that gave the best results through qualitative and quantitative comparisons. Experiments on numerous portrait photos, comparisons with state-of-the-art methods and evaluations by artists have proven the effectiveness and efficiency of the selected network.},
  archive      = {J_TVCG},
  author       = {Yu-Wei Zhang and Ping Luo and Hao Zhou and Zhongping Ji and Hui Liu and Yanzhao Chen and Caiming Zhang},
  doi          = {10.1109/TVCG.2022.3197354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {5008-5019},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural modeling of portrait bas-relief from a single photograph},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event related brain responses reveal the impact of spatial
augmented reality predictive cues on mental effort. <em>TVCG</em>,
<em>29</em>(12), 4990–5007. (<a
href="https://doi.org/10.1109/TVCG.2022.3197810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the results from a Spatial Augmented Reality (SAR) study which evaluated the cognitive cost of several predictive cues. Participants performed a validated procedural button pressing task, where the predictive cue annotations guided them to the upcoming task. While existing research has evaluated predictive cues based on their performance and self-rated mental effort, actual cognitive cost has yet to be investigated. To measure the user&#39;s brain activity, this study utilized electroencephalogram (EEG) recordings. Cognitive load was evaluated by measuring brain responses for a secondary auditory oddball task, with reduced brain responses to oddball tones expected when cognitive load in the primary task is highest. A simple monitor n-back task and procedural task comparing monitor versus SAR were conducted, followed by a version of the procedural task comparing the SAR predictive cues. Results from the brain responses were able to distinguish between performance enhancing cues with a high and low cognitive load. Electrical brain responses also revealed that having an arc or arrow guide towards the upcoming task required the least amount of mental effort.},
  archive      = {J_TVCG},
  author       = {Benjamin Volmer and James Baumeister and Stewart Von Itzstein and Matthias Schlesewsky and Ina Bornkessel-Schlesewsky and Bruce H. Thomas},
  doi          = {10.1109/TVCG.2022.3197810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4990-5007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Event related brain responses reveal the impact of spatial augmented reality predictive cues on mental effort},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance perception in virtual reality: A meta-analysis of
the effect of head-mounted display characteristics. <em>TVCG</em>,
<em>29</em>(12), 4978–4989. (<a
href="https://doi.org/10.1109/TVCG.2022.3196606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distances are commonly underperceived in virtual reality (VR), and this finding has been documented repeatedly over more than two decades of research. Yet, there is evidence that perceived distance is more accurate in modern compared to older head-mounted displays (HMDs). This meta-analysis, based on 137 samples from 61 publications, describes egocentric distance perception across 20 HMDs and examines the relationship between perceived distance and technical HMD characteristics. Judged distance was positively associated with HMD field of view (FOV), positively associated with HMD resolution, and negatively associated with HMD weight. The effects of FOV and resolution were more pronounced among heavier HMDs. These findings suggest that future improvements in these technical characteristics may be central to resolving the problem of distance underperception in VR.},
  archive      = {J_TVCG},
  author       = {Jonathan W. Kelly},
  doi          = {10.1109/TVCG.2022.3196606},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4978-4989},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Distance perception in virtual reality: A meta-analysis of the effect of head-mounted display characteristics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PU-flow: A point cloud upsampling network with normalizing
flows. <em>TVCG</em>, <em>29</em>(12), 4964–4977. (<a
href="https://doi.org/10.1109/TVCG.2022.3196334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud upsampling aims to generate dense point clouds from given sparse ones, which is a challenging task due to the irregular and unordered nature of point sets. To address this issue, we present a novel deep learning-based model, called PU-Flow, which incorporates normalizing flows and weight prediction techniques to produce dense points uniformly distributed on the underlying surface. Specifically, we exploit the invertible characteristics of normalizing flows to transform points between euclidean and latent spaces and formulate the upsampling process as ensemble of neighbouring points in a latent space, where the ensemble weights are adaptively learned from local geometric context. Extensive experiments show that our method is competitive and, in most test cases, it outperforms state-of-the-art methods in terms of reconstruction quality, proximity-to-surface accuracy, and computation efﬁciency. The source code will be publicly available at https://github.com/unknownue/puflow .},
  archive      = {J_TVCG},
  author       = {Aihua Mao and Zihui Du and Junhui Hou and Yaqi Duan and Yong-Jin Liu and Ying He},
  doi          = {10.1109/TVCG.2022.3196334},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4964-4977},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PU-flow: A point cloud upsampling network with normalizing flows},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoordNet: Data generation and visualization generation for
time-varying volumes via a coordinate-based neural network.
<em>TVCG</em>, <em>29</em>(12), 4951–4963. (<a
href="https://doi.org/10.1109/TVCG.2022.3197203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has demonstrated its capability in solving diverse scientific visualization problems, it still lacks generalization power across different tasks. To address this challenge, we propose CoordNet, a single coordinate-based framework that tackles various tasks relevant to time-varying volumetric data visualization without modifying the network architecture. The core idea of our approach is to decompose diverse task inputs and outputs into a unified representation (i.e., coordinates and values) and learn a function from coordinates to their corresponding values. We achieve this goal using a residual block-based implicit neural representation architecture with periodic activation functions. We evaluate CoordNet on data generation (i.e., temporal super-resolution and spatial super-resolution) and visualization generation (i.e., view synthesis and ambient occlusion prediction) tasks using time-varying volumetric data sets of various characteristics. The experimental results indicate that CoordNet achieves better quantitative and qualitative results than the state-of-the-art approaches across all the evaluated tasks. Source code and pre-trained models are available at https://github.com/stevenhan1991/CoordNet .},
  archive      = {J_TVCG},
  author       = {Jun Han and Chaoli Wang},
  doi          = {10.1109/TVCG.2022.3197203},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4951-4963},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CoordNet: Data generation and visualization generation for time-varying volumes via a coordinate-based neural network},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of the saliency of color-based dichoptic cues in
optical see-through augmented reality. <em>TVCG</em>, <em>29</em>(12),
4936–4950. (<a href="https://doi.org/10.1109/TVCG.2022.3195111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a future of pervasive augmented reality (AR), AR systems will need to be able to efficiently draw or guide the attention of the user to visual points of interest in their physical-virtual environment. Since AR imagery is overlaid on top of the user&#39;s view of their physical environment, these attention guidance techniques must not only compete with other virtual imagery, but also with distracting or attention-grabbing features in the user&#39;s physical environment. Because of the wide range of physical-virtual environments that pervasive AR users will find themselves in, it is difficult to design visual cues that “pop out” to the user without performing a visual analysis of the user&#39;s environment, and changing the appearance of the cue to stand out from its surroundings. In this article, we present an initial investigation into the potential uses of dichoptic visual cues for optical see-through AR displays, specifically cues that involve having a difference in hue, saturation, or value between the user&#39;s eyes. These types of cues have been shown to be preattentively processed by the user when presented on other stereoscopic displays, and may also be an effective method of drawing user attention on optical see-through AR displays. We present two user studies: one that evaluates the saliency of dichoptic visual cues on optical see-through displays, and one that evaluates their subjective qualities. Our results suggest that hue-based dichoptic cues or “Forbidden Colors” may be particularly effective for these purposes, achieving significantly lower error rates in a pop out task compared to value-based and saturation-based cues.},
  archive      = {J_TVCG},
  author       = {Austin Erickson and Gerd Bruder and Gregory F. Welch},
  doi          = {10.1109/TVCG.2022.3195111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4936-4950},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis of the saliency of color-based dichoptic cues in optical see-through augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling of the 3D tree skeleton using real-world data: A
survey. <em>TVCG</em>, <em>29</em>(12), 4920–4935. (<a
href="https://doi.org/10.1109/TVCG.2022.3193018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree modeling has been extensively studied in computer graphics. Recent advances in the development of high-resolution sensors and data processing techniques are extremely useful for collecting 3D datasets of real-world trees and generating increasingly plausible branching structures. The wide availability of versatile acquisition platforms allows us to capture multi-view images and scanned data that can be used for guided 3D tree modeling. In this paper, we carry out a comprehensive review of the state-of-the-art methods for the 3D modeling of botanical tree geometry by taking input data from real scenarios. A wide range of studies has been proposed following different approaches. The most relevant contributions are summarized and classified into three categories: (1) procedural reconstruction, (2) geometry-based extraction, and (3) image-based modeling. In addition, we describe other approaches focused on the reconstruction process by adding additional features to achieve a realistic appearance of the tree models. Thus, we provide an overview of the most effective procedures to assist researchers in the photorealistic modeling of trees in geometry and appearance. The article concludes with remarks and trends for promising research opportunities in 3D tree modeling using real-world data.},
  archive      = {J_TVCG},
  author       = {José L. Cárdenas and Carlos J. Ogayar and Francisco R. Feito and Juan M. Jurado},
  doi          = {10.1109/TVCG.2022.3193018},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4920-4935},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling of the 3D tree skeleton using real-world data: A survey},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSRNet: Scalable 3D surface reconstruction network.
<em>TVCG</em>, <em>29</em>(12), 4906–4919. (<a
href="https://doi.org/10.1109/TVCG.2022.3193406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based surface reconstruction methods have received considerable attention in recent years due to their excellent expressiveness. However, existing learning-based methods lack scalability in processing large-scale point clouds. This paper proposes a novel scalable learning-based 3D surface reconstruction method based on octree, called SSRNet. SSRNet works in a scalable reconstruction pipeline, which divides oriented point clouds into different local parts and then processes them in parallel. Accommodating this scalable design pattern, SSRNet constructs local geometric features for octree vertices. Such features comprise the relation between the vertices and the implicit surface, ensuring geometric perception. Focusing on local geometric information also enables the network to avoid the overfitting problem and generalize well on different datasets. Finally, as a learning-based method, SSRNet can process large-scale point clouds in a short time. And to further solve the efficiency problem, we provide a lightweight and efficient version that is about five times faster while maintaining reconstruction performance. Experiments show that our methods achieve state-of-the-art performance with outstanding efficiency.},
  archive      = {J_TVCG},
  author       = {Ganzhangqin Yuan and Qiancheng Fu and Zhenxing Mi and Yiming Luo and Wenbing Tao},
  doi          = {10.1109/TVCG.2022.3193406},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4906-4919},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SSRNet: Scalable 3D surface reconstruction network},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controllable free viewpoint video reconstruction based on
neural radiance fields and motion graphs. <em>TVCG</em>,
<em>29</em>(12), 4891–4905. (<a
href="https://doi.org/10.1109/TVCG.2022.3192713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a controllable high-quality free viewpoint video generation method based on the motion graph and neural radiance fields (NeRF). Different from existing pose-driven NeRF or time/structure conditioned NeRF works, we propose to first construct a directed motion graph of the captured sequence. Such a sequence-motion-parameterization strategy not only enables flexible pose control for free viewpoint video rendering but also avoids redundant calculation of similar poses and thus improves the overall reconstruction efficiency. Moreover, to support body shape control without losing the realistic free viewpoint rendering performance, we improve the vanilla NeRF by combining explicit surface deformation and implicit neural scene representations. Specifically, we train a local surface-guided NeRF for each valid frame on the motion graph, and the volumetric rendering was only performed in the local space around the real surface, thus enabling plausible shape control ability. As far as we know, our method is the first method that supports both realistic free viewpoint video reconstruction and motion graph-based user-guided motion traversal. The results and comparisons further demonstrate the effectiveness of the proposed method.},
  archive      = {J_TVCG},
  author       = {He Zhang and Fan Li and Jianhui Zhao and Chao Tan and Dongming Shen and Yebin Liu and Tao Yu},
  doi          = {10.1109/TVCG.2022.3192713},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4891-4905},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Controllable free viewpoint video reconstruction based on neural radiance fields and motion graphs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CreatureShop: Interactive 3D character modeling and
texturing from a single color drawing. <em>TVCG</em>, <em>29</em>(12),
4874–4890. (<a href="https://doi.org/10.1109/TVCG.2022.3197560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating 3D shapes from 2D drawings is an important problem with applications in content creation for computer animation and virtual reality. We introduce a new sketch-based system, CreatureShop , that enables amateurs to create high-quality textured 3D character models from 2D drawings with ease and efficiency. CreatureShop takes an input bitmap drawing of a character (such as an animal or other creature), depicted from an arbitrary descriptive pose and viewpoint, and creates a 3D shape with plausible geometric details and textures from a small number of user annotations on the 2D drawing. Our key contributions are a novel oblique view modeling method, a set of systematic approaches for producing plausible textures on the invisible or occluded parts of the 3D character (as viewed from the direction of the input drawing), and a user-friendly interactive system. We validate our system and methods by creating numerous 3D characters from various drawings, and compare our results with related works to show the advantages of our method. We perform a user study to evaluate the usability of our system, which demonstrates that our system is a practical and efficient approach to create fully-textured 3D character models for novice users.},
  archive      = {J_TVCG},
  author       = {Congyi Zhang and Lei Yang and Nenglun Chen and Nicholas Vining and Alla Sheffer and Francis C.M. Lau and Guoping Wang and Wenping Wang},
  doi          = {10.1109/TVCG.2022.3197560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4874-4890},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CreatureShop: Interactive 3D character modeling and texturing from a single color drawing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual cue effects on a classification accuracy estimation
task in immersive scatterplots. <em>TVCG</em>, <em>29</em>(12),
4858–4873. (<a href="https://doi.org/10.1109/TVCG.2022.3192364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor and a head-mounted display (HMD), we assessed scatterplot designs which vary their use of visual cues—motion, shading, perspective (graphical projection), and dimensionality—on two sets of data. We conducted a user study with a summary task in which 32 participants estimated the classification accuracy of an artificial neural network from the scatterplots. With Bayesian multilevel modeling, we capture the intricate visual effects and find that no cue alone explains all the variance in estimation error. Visual motion cues generally reduce participants’ estimation error; besides this motion, using other cues may increase participants’ estimation error. Using an HMD, adding visual motion cues, providing a third data dimension, or showing a more complicated dataset leads to longer response times. We speculate that most visual cues may not strongly affect perception in immersive analytics unless they change people&#39;s mental model about data. In summary, by studying participants as they interpret the output from a complicated machine learning model, we advance our understanding of how to use the visual cues in immersive analytics.},
  archive      = {J_TVCG},
  author       = {Fumeng Yang and James Tompkin and Lane Harrison and David H. Laidlaw},
  doi          = {10.1109/TVCG.2022.3192364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4858-4873},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cue effects on a classification accuracy estimation task in immersive scatterplots},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating graphical perception of visual motion for
quantitative data encoding. <em>TVCG</em>, <em>29</em>(12), 4845–4857.
(<a href="https://doi.org/10.1109/TVCG.2022.3193756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualization uses various types of representations to encode data into graphical formats. Prior work on visualization techniques has evaluated the accuracy of perceived numerical data values from visual data encodings such as graphical position, length, orientation, size, and color. Our work aims to extend the research of graphical perception to the use of motion as data encodings for quantitative values. We present two experiments implementing multiple fundamental aspects of motion such as type, speed, and synchronicity that can be used for numerical value encoding as well as comparing motion to static visual encodings in terms of user perception and accuracy. We studied how well users can assess the differences between several types of motion and static visual encodings and present an updated ranking of accuracy for quantitative judgments. Our results indicate that non-synchronized motion can be interpreted more quickly and more accurately than synchronized motion. Moreover, our ranking of static and motion visual representations shows that motion, especially expansion and translational types, has great potential as a data encoding technique for quantitative value. Finally, we discuss the implications for the use of animation and motion for numerical representations in data visualization.},
  archive      = {J_TVCG},
  author       = {Shaghayegh Esmaeili and Samia Kabir and Anthony M. Colas and Rhema P. Linder and Eric D. Ragan},
  doi          = {10.1109/TVCG.2022.3193756},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4845-4857},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating graphical perception of visual motion for quantitative data encoding},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MD-cave: An immersive visualization workbench for
radiologists. <em>TVCG</em>, <em>29</em>(12), 4832–4844. (<a
href="https://doi.org/10.1109/TVCG.2022.3193672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MD-Cave is an immersive analytics system that provides enhanced stereoscopic visualizations to support visual diagnoses performed by radiologists. The system harnesses contemporary paradigms in immersive visualization and 3D interaction, which are better suited for investigating 3D volumetric data. We retain practicality through efficient utilization of desk space and comfort for radiologists in terms of frequent long duration use. MD-Cave is general and incorporates: (1) high resolution stereoscopic visualizations through a surround triple-monitor setup, (2) 3D interactions through head and hand tracking, (3) and a general framework that supports 3D visualization of deep-seated anatomical structures without the need for explicit segmentation algorithms. Such a general framework expands the utility of our system to many diagnostic scenarios. We have developed MD-Cave through close collaboration and feedback from two expert radiologists who evaluated the utility of MD-Cave and the 3D interactions in the context of radiological examinations. We also provide evaluation of MD-Cave through case studies performed by an expert radiologist and concrete examples on multiple real-world diagnostic scenarios, such as pancreatic cancer, shoulder-CT, and COVID-19 Chest CT examination.},
  archive      = {J_TVCG},
  author       = {Shreeraj Jadhav and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2022.3193672},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4832-4844},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MD-cave: An immersive visualization workbench for radiologists},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provectories: Embedding-based analysis of interaction
provenance data. <em>TVCG</em>, <em>29</em>(12), 4816–4831. (<a
href="https://doi.org/10.1109/TVCG.2021.3135697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this article, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.},
  archive      = {J_TVCG},
  author       = {Conny Walchshofer and Andreas Hinterreiter and Kai Xu and Holger Stitz and Marc Streit},
  doi          = {10.1109/TVCG.2021.3135697},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4816-4831},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Provectories: Embedding-based analysis of interaction provenance data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Message from the ISMAR 2023 science and technology journal
program chairs and TVCG guest editors. <em>TVCG</em>, <em>29</em>(11),
vi. (<a href="https://doi.org/10.1109/TVCG.2023.3322308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG) , we are pleased to present the journal papers from the 22nd IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2023), which will be held as a hybrid conference between October 16 and 20, 2023 in Sydney, Australia. ISMAR continues the over twenty-year long tradition of IWAR, ISMR, and ISAR, and is the premier conference for Mixed and Augmented Reality in the world.},
  archive      = {J_TVCG},
  author       = {Michele Fiorentino and Joseph L. Gabbard and Gun Lee and Maud Marchal and Guillaume Moreau},
  doi          = {10.1109/TVCG.2023.3322308},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {vi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the ISMAR 2023 science and technology journal program chairs and TVCG guest editors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>29</em>(11), v. (<a
href="https://doi.org/10.1109/TVCG.2023.3322187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the November 2023 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) . This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR). The conference took place from October 16 to 20, 2023 in Sydney, Australia, in a hybrid mode.},
  archive      = {J_TVCG},
  author       = {Han-Wei Shen and Kiyoshi Kiyokawa},
  doi          = {10.1109/TVCG.2023.3322187},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {v},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Is that my heartbeat? Measuring and understanding
modality-dependent cardiac interoception in virtual reality.
<em>TVCG</em>, <em>29</em>(11), 4805–4815. (<a
href="https://doi.org/10.1109/TVCG.2023.3320228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring interoception (‘perceiving internal bodily states’) has diagnostic and wellbeing implications. Since heartbeats are distinct and frequent, various methods aim at measuring cardiac interoceptive accuracy (CIAcc). However, the role of exteroceptive modalities for representing heart rate (HR) across screen-based and Virtual Reality (VR) environments remains unclear. Using a PolarH10 HR monitor, we develop a modality-dependent cardiac recognition task that modifies displayed HR. In a mixed-factorial design (N=50), we investigate how task environment (Screen, VR), modality (Audio, Visual, Audio-Visual), and real-time HR modifications (±15\%, ±30\%, None) influence CIAcc, interoceptive awareness, mind-body measures, VR presence, and post-experience responses. Findings showed that participants confused their HR with underestimates up to 30\%; environment did not affect CIAcc but influenced mind-related measures; modality did not influence CIAcc, however including audio increased interoceptive awareness; and VR presence inversely correlated with CIAcc. We contribute a lightweight and extensible cardiac interoception measurement method, and implications for biofeedback displays.},
  archive      = {J_TVCG},
  author       = {Abdallah El Ali and Rayna Ney and Zeph M. C. van Berlo and Pablo Cesar},
  doi          = {10.1109/TVCG.2023.3320228},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4805-4815},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Is that my heartbeat? measuring and understanding modality-dependent cardiac interoception in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). “To be or not to be me?”: Exploration of self-similar
effects of avatars on social virtual reality experiences. <em>TVCG</em>,
<em>29</em>(11), 4794–4804. (<a
href="https://doi.org/10.1109/TVCG.2023.3320240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interest in the self-similarity effect of avatars in virtual reality (VR) has spurred the creation of realistic avatars that closely mirror their users. However, despite extensive research on the self-similarity effect in single-user VR environments, our understanding of its impact in social VR settings remains underdeveloped. This shortfall exists despite the unique socio-psychological phenomena arising from the illusion of embodiment that could potentially alter these effects. To fill this gap, this paper provides an in-depth empirical investigation of how avatars&#39; self-similarity influences social VR experiences. Our research uncovers several notable findings: 1) A high level of avatar self-similarity boosts users&#39; sense of embodiment and social presence but has minimal effects on the overall presence and even slightly hinders immersion. These results are driven by increased self-awareness. 2) Among various factors that contribute to the self-similarity of avatars, voice stands out as a significant influencer of social VR experiences, surpassing other representational factors. 3) The impact of avatar self-similarity shows negligible differences between male and female users. Based on these findings, we discuss the pros and cons of incorporating self-similarity into social VR avatars. Our study serves as a foundation for further research in this field.},
  archive      = {J_TVCG},
  author       = {Hayeon Kim and Jinhyung Park and In-Kwon Lee},
  doi          = {10.1109/TVCG.2023.3320240},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4794-4804},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“To be or not to be me?”: Exploration of self-similar effects of avatars on social virtual reality experiences},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wearable augmented reality: Research trends and future
directions from three major venues. <em>TVCG</em>, <em>29</em>(11),
4782–4793. (<a href="https://doi.org/10.1109/TVCG.2023.3320231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable Augmented Reality (AR) has attracted considerable attention in recent years, as evidenced by the growing number of research publications and industry investments. With swift advancements and a multitude of interdisciplinary research areas within wearable AR, a comprehensive review is crucial for integrating the current state of the field. In this paper, we present a review of 389 research papers on wearable AR, published between 2018 and 2022 in three major venues: ISMAR, TVCG, and CHI. Drawing inspiration from previous works by Zhou et al. and Kim et al., which summarized AR research at ISMAR over the past two decades (1998–2017), we categorize the papers into different topics and identify prevailing trends. One notable finding is that wearable AR research is increasingly geared towards enabling broader consumer adoption. From our analysis, we highlight key observations related to potential future research areas essential for capitalizing on this trend and achieving widespread adoption. These include addressing challenges in Display, Tracking, Interaction, and Applications, and exploring emerging frontiers in Ethics, Accessibility, Avatar and Embodiment, and Intelligent Virtual Agents.},
  archive      = {J_TVCG},
  author       = {Tram Thi Minh Tran and Shane Brown and Oliver Weidlich and Mark Billinghurst and Callum Parker},
  doi          = {10.1109/TVCG.2023.3320231},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4782-4793},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wearable augmented reality: Research trends and future directions from three major venues},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gestures vs. Emojis: Comparing non-verbal reaction
visualizations for immersive collaboration. <em>TVCG</em>,
<em>29</em>(11), 4772–4781. (<a
href="https://doi.org/10.1109/TVCG.2023.3320254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative virtual environments afford new capabilities in telepresence applications, allowing participants to co-inhabit an environment to interact while being embodied via avatars. However, shared content within these environments often takes away the attention of collaborators from observing the non-verbal cues conveyed by their peers, resulting in less effective communication. Exaggerated gestures, abstract visuals, as well as a combination of the two, have the potential to improve the effectiveness of communication within these environments in comparison to familiar, natural non-verbal visualizations. We designed and conducted a user study where we evaluated the impact of these different non-verbal visualizations on users&#39; identification time, understanding, and perception. We found that exaggerated gestures generally perform better than non-exaggerated gestures, abstract visuals are an effective means to convey intentional reactions, and the combination of gestures with abstract visuals provides some benefits compared to their standalone counterparts.},
  archive      = {J_TVCG},
  author       = {Alexander Giovannelli and Jerald Thomas and Logan Lane and Francielly Rodrigues and Doug A. Bowman},
  doi          = {10.1109/TVCG.2023.3320254},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4772-4781},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gestures vs. emojis: Comparing non-verbal reaction visualizations for immersive collaboration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-latency beaming display: Implementation of wearable, 133
μs motion-to-photon latency near-eye display. <em>TVCG</em>,
<em>29</em>(11), 4761–4771. (<a
href="https://doi.org/10.1109/TVCG.2023.3320212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a low-latency Beaming Display system with a $133\ \mu\mathrm{s}$ motion-to-photon (M2P) latency, the delay from head motion to the corresponding image motion. The Beaming Display represents a recent near-eye display paradigm that involves a steerable remote projector and a passive wearable headset. This system aims to overcome typical trade-offs of Optical See-Through Head-Mounted Displays (OST-HMDs), such as weight and computational resources. However, since the Beaming Display projects a small image onto a moving, distant viewpoint, M2P latency significantly affects displacement. To reduce M2P latency, we propose a low-latency Beaming Display system that can be modularized without relying on expensive high-speed devices. In our system, a 2D position sensor, which is placed coaxially on the projector, detects the light from the IR-LED on the headset and generates a differential signal for tracking. An analog closed-loop control of the steering mirror based on this signal continuously projects images onto the headset. We have implemented a proof-of-concept prototype, evaluated the latency and the augmented reality experience through a user-perspective camera, and discussed the limitations and potential improvements of the prototype.},
  archive      = {J_TVCG},
  author       = {Yuichi Hiroi and Akira Watanabe and Yuri Mikawa and Yuta Itoh},
  doi          = {10.1109/TVCG.2023.3320212},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4761-4771},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Low-latency beaming display: Implementation of wearable, 133 μs motion-to-photon latency near-eye display},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual facial enhancements can significantly improve speech
perception in the presence of noise. <em>TVCG</em>, <em>29</em>(11),
4751–4760. (<a href="https://doi.org/10.1109/TVCG.2023.3320247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human speech perception is generally optimal in quiet environments, however it becomes more difficult and error prone in the presence of noise, such as other humans speaking nearby or ambient noise. In such situations, human speech perception is improved by speech reading , i.e., watching the movements of a speaker&#39;s mouth and face, either consciously as done by people with hearing loss or subconsciously by other humans. While previous work focused largely on speech perception of two-dimensional videos of faces, there is a gap in the research field focusing on facial features as seen in head-mounted displays, including the impacts of display resolution, and the effectiveness of visually enhancing a virtual human face on speech perception in the presence of noise. In this paper, we present a comparative user study ( $N=21$ ) in which we investigated an audio-only condition compared to two levels of head-mounted display resolution ( $1832\times 1920$ or $916\times 960$ pixels per eye) and two levels of the native or visually enhanced appearance of a virtual human, the latter consisting of an up-scaled facial representation and simulated lipstick (lip coloring) added to increase contrast. To understand effects on speech perception in noise, we measured participants&#39; speech reception thresholds (SRTs) for each audio-visual stimulus condition. These thresholds indicate the decibel levels of the speech signal that are necessary for a listener to receive the speech correctly 50\% of the time. First, we show that the display resolution significantly affected participants&#39; ability to perceive the speech signal in noise, which has practical implications for the field, especially in social virtual environments. Second, we show that our visual enhancement method was able to compensate for limited display resolution and was generally preferred by participants. Specifically, our participants indicated that they benefited from the head scaling more than the added facial contrast from the simulated lipstick. We discuss relationships, implications, and guidelines for applications that aim to leverage such enhancements.},
  archive      = {J_TVCG},
  author       = {Zubin Datta Choudhary and Gerd Bruder and Gregory F. Welch},
  doi          = {10.1109/TVCG.2023.3320247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4751-4760},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual facial enhancements can significantly improve speech perception in the presence of noise},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing gaze, head and controller selection of dynamically
revealed targets in head-mounted displays. <em>TVCG</em>,
<em>29</em>(11), 4740–4750. (<a
href="https://doi.org/10.1109/TVCG.2023.3320235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a head-mounted virtual reality study that compared gaze, head, and controller pointing for selection of dynamically revealed targets. Existing studies on head-mounted 3D interaction have focused on pointing and selection tasks where all targets are visible to the user. Our study compared the effects of screen width (field of view), target amplitude and width, and prior knowledge of target location on modality performance. Results show that gaze and controller pointing are significantly faster than head pointing and that increased screen width only positively impacts performance up to a certain point. We further investigated the applicability of existing pointing models. Our analysis confirmed the suitability of previously proposed two-component models for all modalities while uncovering differences for gaze at known and unknown target positions. Our findings provide new empirical evidence for understanding input with gaze, head, and controller and are significant for applications that extend around the user.},
  archive      = {J_TVCG},
  author       = {Ludwig Sidenmark and Franziska Prummer and Joshua Newn and Hans Gellersen},
  doi          = {10.1109/TVCG.2023.3320235},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4740-4750},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing gaze, head and controller selection of dynamically revealed targets in head-mounted displays},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Bag of world anchors for instant large-scale localization.
<em>TVCG</em>, <em>29</em>(11), 4730–4739. (<a
href="https://doi.org/10.1109/TVCG.2023.3320264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel scene description to perform large-scale localization using only geometric constraints. Our work extends compact world anchors with a search data structure to efficiently perform localization and pose estimation of mobile augmented reality devices across multiple platforms ( e.g. , HoloLens 2, iPad). The algorithm uses a bag-of-words approach to characterize distinct scenes ( e.g. , rooms). Since the individual scene representations rely on compact geometric (rather than appearance-based) features, the resulting search structure is very lightweight and fast, lending itself to deployment on mobile devices. We present a set of experiments demonstrating the accuracy, performance and scalability of our novel localization method. In addition, we describe several use cases demonstrating how efficient cross-platform localization facilitates sharing of augmented reality experiences.},
  archive      = {J_TVCG},
  author       = {Fernando Reyes-Aviles and Philipp Fleck and Dieter Schmalstieg and Clemens Arth},
  doi          = {10.1109/TVCG.2023.3320264},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4730-4739},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bag of world anchors for instant large-scale localization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer scene representation from composed focal stacks.
<em>TVCG</em>, <em>29</em>(11), 4719–4729. (<a
href="https://doi.org/10.1109/TVCG.2023.3320248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-layer images are a powerful scene representation for high-performance rendering in virtual/augmented reality (VR/AR). The major approach to generate such images is to use a deep neural network trained to encode colors and alpha values of depth certainty on each layer using registered multi-view images. A typical network is aimed at using a limited number of nearest views. Therefore, local noises in input images from a user-navigated camera deteriorate the final rendering quality and interfere with coherency over view transitions. We propose to use a focal stack composed of multi-view inputs to diminish such noises. We also provide theoretical analysis for ideal focal stacks to generate multi-layer images. Our results demonstrate the advantages of using focal stacks in coherent rendering, memory footprint, and AR-supported data capturing. We also show three applications of imaging for VR.},
  archive      = {J_TVCG},
  author       = {Reina Ishikawa and Hideo Saito and Denis Kalkofen and Shohei Mori},
  doi          = {10.1109/TVCG.2023.3320248},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4719-4729},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-layer scene representation from composed focal stacks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep scene synthesis of atlanta-world interiors from a
single omnidirectional image. <em>TVCG</em>, <em>29</em>(11), 4708–4718.
(<a href="https://doi.org/10.1109/TVCG.2023.3320219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new data-driven approach for extracting geometric and structural information from a single spherical panorama of an interior scene, and for using this information to render the scene from novel points of view, enhancing 3D immersion in VR applications. The approach copes with the inherent ambiguities of single-image geometry estimation and novel view synthesis by focusing on the very common case of Atlanta-world interiors, bounded by horizontal floors and ceilings and vertical walls. Based on this prior, we introduce a novel end-to-end deep learning approach to jointly estimate the depth and the underlying room structure of the scene. The prior guides the design of the network and of novel domain-specific loss functions, shifting the major computational load on a training phase that exploits available large-scale synthetic panoramic imagery. An extremely lightweight network uses geometric and structural information to infer novel panoramic views from translated positions at interactive rates, from which perspective views matching head rotations are produced and upsampled to the display size. As a result, our method automatically produces new poses around the original camera at interactive rates, within a working area suitable for producing depth cues for VR applications, especially when using head-mounted displays connected to graphics servers. The extracted floor plan and 3D wall structure can also be used to support room exploration. The experimental results demonstrate that our method provides low-latency performance and improves over current state-of-the-art solutions in prediction accuracy on available commonly used indoor panoramic benchmarks.},
  archive      = {J_TVCG},
  author       = {Giovanni Pintore and Fabio Bettio and Marco Agus and Enrico Gobbetti},
  doi          = {10.1109/TVCG.2023.3320219},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4708-4718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep scene synthesis of atlanta-world interiors from a single omnidirectional image},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding effects of visual feedback delay in AR on fine
motor surgical tasks. <em>TVCG</em>, <em>29</em>(11), 4697–4707. (<a
href="https://doi.org/10.1109/TVCG.2023.3320214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latency is a pervasive issue in various systems that can significantly impact motor performance and user perception. In medical settings, latency can hinder surgeons&#39; ability to quickly correct movements, resulting in an experience that doesn&#39;t align with user expectations and standards of care. Despite numerous studies reporting on the negative effects of latency, there is still a gap in understanding how it impacts the use of augmented reality (AR) in medical settings. This study aims to address this gap by examining how latency impacts motor task performance and subjective perceptions, such as cognitive load, on two display types: a monitor display, traditionally used inside an operating room (OR), and a Microsoft HoloLens 2 display. Our findings indicate that both level of latency and display type impact motor performance, and higher latencies on the HoloLens result in relatively poor performance. However, cognitive load was found to be unrelated to display type or latency, but was dependent on the surgeon&#39;s training level. Surgeons did not compromise accuracy to gain more speed and were generally well aware of the latency in the system irrespective of their performance on task. Our study provides valuable insights into acceptable thresholds of latency for AR displays and proposes design implications for the successful implementation and use of AR in surgical settings.},
  archive      = {J_TVCG},
  author       = {Talha Khan and Toby S. Zhu and Thomas Downes and Lucille Cheng and Nicolás M. Kass and Edward G. Andrews and Jacob T. Biehl},
  doi          = {10.1109/TVCG.2023.3320214},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4697-4707},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding effects of visual feedback delay in AR on fine motor surgical tasks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HEADSET: Human emotion awareness under partial occlusions
multimodal DataSET. <em>TVCG</em>, <em>29</em>(11), 4686–4696. (<a
href="https://doi.org/10.1109/TVCG.2023.3320236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.},
  archive      = {J_TVCG},
  author       = {Fatemeh Ghorbani Lohesara and Davi Rabbouni Freitas and Christine Guillemot and Karen Eguiazarian and Sebastian Knorr},
  doi          = {10.1109/TVCG.2023.3320236},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4686-4696},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HEADSET: Human emotion awareness under partial occlusions multimodal DataSET},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GuitARhero: Interactive augmented reality guitar tutorials.
<em>TVCG</em>, <em>29</em>(11), 4676–4685. (<a
href="https://doi.org/10.1109/TVCG.2023.3320266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents guitARhero, an Augmented Reality application for interactively teaching guitar playing to beginners through responsive visualizations overlaid on the guitar neck. We support two types of visual guidance, a highlighting of the frets that need to be pressed and a 3D hand overlay, as well as two display scenarios, one using a desktop magic mirror and one using a video see-through head-mounted display. We conducted a user study with 20 participants to evaluate how well users could follow instructions presented with different guidance and display combinations and compare these to a baseline where users had to follow video instructions. Our study highlights the trade-off between the provided information and visual clarity affecting the user&#39;s ability to interpret and follow instructions for fine-grained tasks. We show that the perceived usefulness of instruction integration into an HMD view highly depends on the hardware capabilities and instruction details.},
  archive      = {J_TVCG},
  author       = {Lucchas Ribeiro Skreinig and Denis Kalkofen and Ana Stanescu and Peter Mohr and Frank Heyen and Shohei Mori and Michael Sedlmair and Dieter Schmalstieg and Alexander Plopski},
  doi          = {10.1109/TVCG.2023.3320266},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4676-4685},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GuitARhero: Interactive augmented reality guitar tutorials},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual cues for a steadier you: Visual feedback methods
improved standing balance in virtual reality for people with balance
impairments. <em>TVCG</em>, <em>29</em>(11), 4666–4675. (<a
href="https://doi.org/10.1109/TVCG.2023.3320244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of head-mounted displays (HMDs) for virtual reality (VR) sometimes have balance issues since HMDs impede their view of the outside world. This has a greater impact on people with balance impairments since many rely more heavily on their visual cues to keep their balance. This is a significant obstacle to the universal usability and accessibility of VR. Although previous studies have verified the imbalance issue, not much work has been done to diminish it. In this study, we investigated how to increase VR balance by utilizing additional visual cues. To examine how different visual approaches (static, rhythmic, spatial, and center of pressure (CoP) based feedback) affect balance in VR, we recruited 100 people (50 with balance impairments due to multiple sclerosis and 50 without balance impairments) across two different geographic locations (United States and Bangladesh). All people completed both standing visual exploration as well as standing reach and grasp tasks. Results demonstrated that static, rhythmic, and CoP visual feedback approaches enhanced balance significantly ( $p &amp;lt;. 05$ ) in VR for people with balance impairments. The methods described in this study could be applied to design more accessible virtual environments for people with balance impairments.},
  archive      = {J_TVCG},
  author       = {M. Rasel Mahmud and Alberto Cordova and John Quarles},
  doi          = {10.1109/TVCG.2023.3320244},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4666-4675},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cues for a steadier you: Visual feedback methods improved standing balance in virtual reality for people with balance impairments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating the impact of augmented reality and BIM on
retrofitting training for non-experts. <em>TVCG</em>, <em>29</em>(11),
4655–4665. (<a href="https://doi.org/10.1109/TVCG.2023.3320223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) tools have shown significant potential in providing on-site visualization of Building Information Modeling (BIM) data and models for supporting construction evaluation, inspection, and guidance. Retrofitting existing buildings, however, remains a challenging task requiring more innovative solutions to successfully integrate AR and BIM. This study aims to investigate the impact of AR+BIM technology on the retrofitting training process and assess the potential for future on-site usage. We conducted a study with 64 non-expert participants, who were asked to perform a common retrofitting procedure of an electrical outlet installation using either an AR+BIM system or a standard printed blueprint documentation set. Our findings indicate that AR+BIM reduced task time significantly and improved performance consistency across participants, while also decreasing the physical and cognitive demands of the training. This study provides a foundation for augmenting future retrofitting construction research that can extend the use of $\text{AR}+\text{BIM}$ technology, thus facilitating more efficient retrofitting of existing buildings. A video presentation of this article and all supplemental materials are available at https://github.com/DesignLabUCF/SENSEable_RetrofittingTraining .},
  archive      = {J_TVCG},
  author       = {John Sermarini and Robert A. Michlowitz and Joseph J. LaViola and Lori C. Walters and Roger Azevedo and Joseph T. Kider},
  doi          = {10.1109/TVCG.2023.3320223},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4655-4665},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the impact of augmented reality and BIM on retrofitting training for non-experts},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Exemplar-based inpainting for 6DOF virtual reality photos.
<em>TVCG</em>, <em>29</em>(11), 4644–4654. (<a
href="https://doi.org/10.1109/TVCG.2023.3320220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-layer images are currently the most prominent scene representation for viewing natural scenes under full-motion parallax in virtual reality. Layers ordered in diopter space contain color and transparency so that a complete image is formed when the layers are composited in a view-dependent manner. Once baked, the same limitations apply to multi-layer images as to conventional single-layer photography, making it challenging to remove obstructive objects or otherwise edit the content. Object removal before baking can benefit from filling disoccluded layers with pixels from background layers. However, if no such background pixels have been observed, an inpainting algorithm must fill the empty spots with fitting synthetic content. We present and study a multi-layer inpainting approach that addresses this problem in two stages: First, a volumetric area of interest specified by the user is classified with respect to whether the background pixels have been observed or not. Second, the unobserved pixels are filled with multi-layer inpainting. We report on experiments using multiple variants of multi-layer inpainting and compare our solution to conventional inpainting methods that consider each layer individually.},
  archive      = {J_TVCG},
  author       = {Shohei Mori and Dieter Schmalstieg and Denis Kalkofen},
  doi          = {10.1109/TVCG.2023.3320220},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4644-4654},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exemplar-based inpainting for 6DOF virtual reality photos},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controllers or bare hands? A controlled evaluation of input
techniques on interaction performance and exertion in virtual reality.
<em>TVCG</em>, <em>29</em>(11), 4633–4643. (<a
href="https://doi.org/10.1109/TVCG.2023.3320211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) systems have traditionally required users to operate the user interface with controllers in mid-air. More recent VR systems, however, integrate cameras to track the headset&#39;s position inside the environment as well as the user&#39;s hands when possible. This allows users to directly interact with virtual content in mid-air just by reaching out, thus discarding the need for hand-held physical controllers. However, it is unclear which of these two modalities—controller-based or free-hand interaction—is more suitable for efficient input, accurate interaction, and long-term use under reliable tracking conditions. While interacting with hand-held controllers introduces weight, it also requires less finger movement to invoke actions (e.g., pressing a button) and allows users to hold on to a physical object during virtual interaction. In this paper, we investigate the effect of VR input modality (controller vs. free-hand interaction) on physical exertion, agency, task performance, and motor behavior across two mid-air interaction techniques (touch, raycast) and tasks (selection, trajectory-tracing). Participants reported less physical exertion, felt more in control, and were faster and more accurate when using VR controllers compared to free-hand interaction in the raycast setting. Regarding personal preference, participants chose VR controllers for raycast but free-hand interaction for mid-air touch. Our correlation analysis revealed that participants&#39; physical exertion increased with selection speed, quantity of arm motion, variation in motion speed, and bad postures, following ergonomics metrics such as consumed endurance and rapid upper limb assessment. We also found a negative correlation between physical exertion and the participant&#39;s sense of agency, and between physical exertion and task accuracy.},
  archive      = {J_TVCG},
  author       = {Tiffany Luong and Yi Fei Cheng and Max Möbus and Andreas Fender and Christian Holz},
  doi          = {10.1109/TVCG.2023.3320211},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4633-4643},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Controllers or bare hands? a controlled evaluation of input techniques on interaction performance and exertion in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and robust mid-air gesture typing for AR headsets using
3D trajectory decoding. <em>TVCG</em>, <em>29</em>(11), 4622–4632. (<a
href="https://doi.org/10.1109/TVCG.2023.3320218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a fast mid-air gesture keyboard for head-mounted optical see-through augmented reality (OST AR) that supports users in articulating word patterns by merely moving their own physical index finger in relation to a virtual keyboard plane without a need to indirectly control a visual 2D cursor on a keyboard plane. To realize this, we introduce a novel decoding method that directly translates users&#39; three-dimensional fingertip gestural trajectories into their intended text. We evaluate the efficacy of the system in three studies that investigate various design aspects, such as immediate efficacy, accelerated learning, and whether it is possible to maintain performance without providing visual feedback. We find that the new 3D trajectory decoding design results in significant improvements in entry rates while maintaining low error rates. In addition, we demonstrate that users can maintain their performance even without fingertip and gesture trace visualization.},
  archive      = {J_TVCG},
  author       = {Junxiao Shen and John Dudley and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2023.3320218},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4622-4632},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast and robust mid-air gesture typing for AR headsets using 3D trajectory decoding},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing hand force with wearable muscle sensing for
enhanced mixed reality remote collaboration. <em>TVCG</em>,
<em>29</em>(11), 4611–4621. (<a
href="https://doi.org/10.1109/TVCG.2023.3320210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a prototype system for sharing a user&#39;s hand force in mixed reality (MR) remote collaboration on physical tasks, where hand force is estimated using wearable surface electromyography (sEMG) sensor. In a remote collaboration between a worker and an expert, hand activity plays a crucial role. However, the force exerted by the worker&#39;s hand has not been extensively investigated. Our sEMG-based system reliably captures the worker&#39;s hand force during physical tasks and conveys this information to the expert through hand force visualization, overlaid on the worker&#39;s view or on the worker&#39;s avatar. A user study was conducted to evaluate the impact of visualizing a worker&#39;s hand force on collaboration, employing three distinct visualization methods across two view modes. Our findings demonstrate that sensing and sharing hand force in MR remote collaboration improves the expert&#39;s awareness of the worker&#39;s task, significantly enhances the expert&#39;s perception of the collaborator&#39;s hand force and the weight of the interacting object, and promotes a heightened sense of social presence for the expert. Based on the findings, we provide design implications for future mixed reality remote collaboration systems that incorporate hand force sensing and visualization.},
  archive      = {J_TVCG},
  author       = {Hyung-il Kim and Boram Yoon and Seo Young Oh and Woontack Woo},
  doi          = {10.1109/TVCG.2023.3320210},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4611-4621},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing hand force with wearable muscle sensing for enhanced mixed reality remote collaboration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HotGestures: Complementing command selection and use with
delimiter-free gesture-based shortcuts in virtual reality.
<em>TVCG</em>, <em>29</em>(11), 4600–4610. (<a
href="https://doi.org/10.1109/TVCG.2023.3320257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional desktop applications provide users with hotkeys as shortcuts for triggering different functionality. In this paper we consider what constitutes an effective parallel to hotkeys in a 3D interaction space where the input modality is no longer limited to the use of a keyboard. We propose HotGestures: a gesture-based interaction system for rapid tool selection and usage. Hand gestures are frequently used during human communication to convey information and provide natural associations with meaning. HotGestures provide shortcuts for users to seamlessly activate and use virtual tools by performing hand gestures. This approach naturally complements conventional menu interactions. We evaluate the potential of HotGestures in a set of two user studies and observe that our gesture-based technique provides fast and effective shortcuts for tool selection and usage. Participants found HotGestures to be distinctive, fast, and easy to use while also complementing conventional menu-based interaction.},
  archive      = {J_TVCG},
  author       = {Zhaomou Song and John J. Dudley and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2023.3320257},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4600-4610},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HotGestures: Complementing command selection and use with delimiter-free gesture-based shortcuts in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual indicators representing avatars’ authenticity in
social virtual reality and their impacts on perceived trustworthiness.
<em>TVCG</em>, <em>29</em>(11), 4589–4599. (<a
href="https://doi.org/10.1109/TVCG.2023.3320234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic avatars show great potential in social VR and VR collaboration. However, identity and privacy issues are threatening avatars&#39; authenticity in social VR. In addition to the necessary authentication and protection, effective solutions are needed to convey avatars&#39; authenticity status to users and thereby enhance the overall trustworthiness. We designed several visual indicators (VIs) using static or dynamic visual effects on photorealistic avatars and evaluated their effectiveness in visualizing avatars&#39; authenticity status. In this study we explored suitable attributes and designs for conveying the authenticity of photorealistic avatars and influencing their perceived trustworthiness. Furthermore, we investigated how different interactivity levels influence their effectiveness (the avatar was either presented in a static image, an animated video clip, or an immersive virtual environment). Our findings showed that using a full name can increase trust, while most other VIs could decrease users&#39; trust. We also found that interactivity levels significantly impacted users&#39; trust and the effectiveness of VIs. Based on our results, we developed design guidelines for visual indicators as effective tools to convey authenticity, as a first step towards the improvement of trustworthiness in social VR with identity management.},
  archive      = {J_TVCG},
  author       = {Jinghuai Lin and Johrine Cronjé and Carolin Wienrich and Paul Pauli and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2023.3320234},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4589-4599},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual indicators representing avatars&#39; authenticity in social virtual reality and their impacts on perceived trustworthiness},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of avatar transparency on social presence in
task-centric mixed reality remote collaboration. <em>TVCG</em>,
<em>29</em>(11), 4578–4588. (<a
href="https://doi.org/10.1109/TVCG.2023.3320258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the importance of avatar representation on user experience for Mixed Reality (MR) remote collaboration involving various device environments and large amounts of task-related information, studies on how controlling visual parameters for avatars can benefit users in such situations have been scarce. Thus, we conducted a user study comparing the effects of three avatars with different transparency levels (Nontransparent, Semi-transparent, and Near-transparent) on social presence for users in Augmented Reality (AR) and Virtual Reality (VR) during task-centric MR remote collaboration. Results show that avatars with a strong visual presence are not required in situations where accomplishing the collaborative task is prioritized over social interaction. However, AR users preferred more vivid avatars than VR users. Based on our findings, we suggest guidelines on how different levels of avatar transparency should be applied based on the context of the task and device type for MR remote collaboration.},
  archive      = {J_TVCG},
  author       = {Boram Yoon and Jae-eun Shin and Hyung-il Kim and Seo Young Oh and Dooyoung Kim and Woontack Woo},
  doi          = {10.1109/TVCG.2023.3320258},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4578-4588},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of avatar transparency on social presence in task-centric mixed reality remote collaboration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the performance of hand-based probabilistic text
input methods on a mid-air virtual qwerty keyboard. <em>TVCG</em>,
<em>29</em>(11), 4567–4577. (<a
href="https://doi.org/10.1109/TVCG.2023.3320238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated hand-tracking on modern virtual reality (VR) headsets can be readily exploited to deliver mid-air virtual input surfaces for text entry. These virtual input surfaces can closely replicate the experience of typing on a Qwerty keyboard on a physical touchscreen, thereby allowing users to leverage their pre-existing typing skills. However, the lack of passive haptic feedback, unconstrained user motion, and potential tracking inaccuracies or observability issues encountered in this interaction setting typically degrades the accuracy of user articulations. We present a comprehensive exploration of error-tolerant probabilistic hand-based input methods to support effective text input on a mid-air virtual Qwerty keyboard. Over three user studies we examine the performance potential of hand-based text input under both gesture and touch typing paradigms. We demonstrate typical entry rates in the range of 20 to 30 wpm and average peak entry rates of 40 to 45 wpm.},
  archive      = {J_TVCG},
  author       = {John J. Dudley and Jingyao Zheng and Aakar Gupta and Hrvoje Benko and Matt Longest and Robert Wang and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2023.3320238},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4567-4577},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating the performance of hand-based probabilistic text input methods on a mid-air virtual qwerty keyboard},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel design and evaluation of redirection controllers using
optimized alignment and artificial potential field. <em>TVCG</em>,
<em>29</em>(11), 4556–4566. (<a
href="https://doi.org/10.1109/TVCG.2023.3320208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking allows users to naturally locomote within virtual environments that are larger than or different in layout from the physically tracked space. In this paper, we proposed novel optimization-driven alignment-based and Artificial Potential Field (APF) redirected walking controllers, as well as an integrated version of the two. The first two controllers employ objective functions of one variable, which is the included angle between the user&#39;s heading vector and the target vector originating from the user&#39;s physical position. The optimized angle represents the physical cell that is best aligned with the virtual cell or the target vector on which the designated point has the minimum APF value. The derived optimized angle is used to finely set RDW gains. The two objective functions can be optimized simultaneously, leading to an integrated controller that is potentially able to take advantage of the alignment-based controller and APF-based controller. Through extensive simulation-based studies, we found that the proposed alignment-based and integrated controllers significantly outperform the state-of-the-art controllers and the proposed APF based controller in terms of the number of resets. Furthermore, the proposed alignment controller and integrated controller provide a more uniform likelihood distribution across distance between resets, as compared to the other controllers.},
  archive      = {J_TVCG},
  author       = {Xue-Liang Wu and Huan-Chang Hung and Sabarish V. Babu and Jung-Hong Chuang},
  doi          = {10.1109/TVCG.2023.3320208},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4556-4566},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Novel design and evaluation of redirection controllers using optimized alignment and artificial potential field},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EV-LFV: Synthesizing light field event streams from an event
camera and multiple RGB cameras. <em>TVCG</em>, <em>29</em>(11),
4546–4555. (<a href="https://doi.org/10.1109/TVCG.2023.3320271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field videos captured in RGB frames (RGB-LFV) can provide users with a 6 degree-of-freedom immersive video experience by capturing dense multi-subview video. Despite its potential benefits, the processing of dense multi-subview video is extremely resource-intensive, which currently limits the frame rate of RGB-LFV (i.e., lower than 30 fps) and results in blurred frames when capturing fast motion. To address this issue, we propose leveraging event cameras, which provide high temporal resolution for capturing fast motion. However, the cost of current event camera models makes it prohibitive to use multiple event cameras for RGB-LFV platforms. Therefore, we propose EV-LFV, an event synthesis framework that generates full multi-subview event-based RGB-LFV with only one event camera and multiple traditional RGB cameras. EV-LFV utilizes spatial-angular convolution, ConvLSTM, and Transformer to model RGB-LFV&#39;s angular features, temporal features, and long-range dependency, respectively, to effectively synthesize event streams for RGB-LFV. To train EV-LFV, we construct the first event-to-LFV dataset consisting of 200 RGB-LFV sequences with ground-truth event streams. Experimental results demonstrate that EV-LFV outperforms state-of-the-art event synthesis methods for generating event-based RGB-LFV, effectively alleviating motion blur in the reconstructed RGB-LFV.},
  archive      = {J_TVCG},
  author       = {Zhicheng Lu and Xiaoming Chen and Vera Yuk Ying Chung and Weidong Cai and Yiran Shen},
  doi          = {10.1109/TVCG.2023.3320271},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4546-4555},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EV-LFV: Synthesizing light field event streams from an event camera and multiple RGB cameras},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring users pointing performance on large displays with
different curvatures in virtual reality. <em>TVCG</em>, <em>29</em>(11),
4535–4545. (<a href="https://doi.org/10.1109/TVCG.2023.3320242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large curved displays inside Virtual Reality environments are becoming popular for visualizing high-resolution content during analytical tasks, gaming or entertainment. Prior research showed that such displays provide a wide field of view and offer users a high level of immersion. However, little is known about users&#39; performance (e.g., pointing speed and accuracy) on them. We explore users&#39; pointing performance on large virtual curved displays. We investigate standard pointing factors (e.g., target width and amplitude) in combination with relevant curve-related factors, namely display curvature and both linear and angular measures. Our results show that the less curved the display, the higher the performance, i.e., faster movement time. This result holds for pointing tasks controlled via their visual properties (linear widths and amplitudes) or their motor properties (angular widths and amplitudes). Additionally, display curvatures significantly affect the error rate for both linear and angular conditions. Furthermore, we observe that curved displays perform better or similar to flat displays based on throughput analysis. Finally, we discuss our results and provide suggestions regarding pointing tasks on large curved displays in VR.},
  archive      = {J_TVCG},
  author       = {A K M Amanat Ullah and William Delamare and Khalad Hasan},
  doi          = {10.1109/TVCG.2023.3320242},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4535-4545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring users pointing performance on large displays with different curvatures in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual reality telepresence: 360-degree video streaming
with edge-compute assisted static foveated compression. <em>TVCG</em>,
<em>29</em>(11), 4525–4534. (<a
href="https://doi.org/10.1109/TVCG.2023.3320255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time communication with immersive 360° video can enable users to be telepresent within a remotely streamed environment. Increasingly, users are shifting to mobile devices and connecting to the Internet via mobile-cellular networks. As the ideal media for 360° videos, some VR headsets now also come with cellular capacity, giving them potential for mobile applications. However, streaming high-quality 360° live video poses challenges for network bandwidth, particularly on cellular connections. To reduce bandwidth requirements, videos can be compressed using viewport-adaptive streaming or foveated rendering techniques. Such approaches require very low latency in order to be effective, which has previously limited their applications on traditional cellular networks. In this work, we demonstrate an end-to-end virtual reality telepresence system that streams ∼6K 360° video over 5G millimeter-wave (mmW) radio. Our use of 5G technologies, in conjunction with mobile edge compute nodes, substantially reduces latency when compared with existing 4G networks, enabling high-efficiency foveated compression over modern cellular networks on par with WiFi. We performed a technical evaluation of our system&#39;s visual quality post-compression with peak signal-to-noise ratio (PSNR) and FOVVideoVDP. We also conducted a user study to evaluate users&#39; sensitivity to compressed video. Our findings demonstrate that our system achieves visually indistinguishable video streams while using up to 80\% less data when compared with un-foveated video. We demonstrate our video compression system in the context of an immersive, telepresent video calling application.},
  archive      = {J_TVCG},
  author       = {Xincheng Huang and James Riddell and Robert Xiao},
  doi          = {10.1109/TVCG.2023.3320255},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4525-4534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual reality telepresence: 360-degree video streaming with edge-compute assisted static foveated compression},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring horizontally flipped interaction in virtual
reality for improving spatial ability. <em>TVCG</em>, <em>29</em>(11),
4514–4524. (<a href="https://doi.org/10.1109/TVCG.2023.3320241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is a high-fidelity medium that can offer experiences that are close to real-life. Spatial ability plays an important role in human life, including academic achievement and advancement in work settings. Spatial ability is known to be improved by practicing relevant tasks. Mental rotation and spatial perception are among such tasks that improve spatial skills. In this research, we investigated a “mirror-reversed” interaction technique in a cup stacking task in VR and looked into its effects on spatial ability, brain activity regarding spatial processing and attention (measured with EEG), performance, and user experience in male participants. Participants stacked cups according to given patterns using direct manipulation with horizontally flipped controls, similar to looking in a mirror while performing object manipulation in real life. In a between-subjects user study, we compared this novel interaction with a baseline where the participants completed the same task with regular controls. Although there was no significant main effect of group on the mental rotation and perspective taking/spatial orientation tests scores, within-group analysis indicated a trend toward an improvement in the mirror-reversed group in spatial orientation, while both groups showed a trend toward improvement in mental rotation. Participants in both groups got better at the task over time (their task completion durations decreased). EEG data revealed significant theta band power increase in the mirror-reversed group whereas there was no difference in the alpha band power between the two groups. Our results are encouraging for exploring spatially challenging interactions in VR for spatial skills training. We share the implementation and user study results, and discuss the implications.},
  archive      = {J_TVCG},
  author       = {Lal Lila Bozgeyikli and Evren Bozgeyikli and Christopher Schnell and Jaclynn Clark},
  doi          = {10.1109/TVCG.2023.3320241},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4514-4524},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring horizontally flipped interaction in virtual reality for improving spatial ability},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Injured avatars: The impact of embodied anatomies and
virtual injuries on well-being and performance. <em>TVCG</em>,
<em>29</em>(11), 4503–4513. (<a
href="https://doi.org/10.1109/TVCG.2023.3320224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human cognition relies on embodiment as a fundamental mechanism. Virtual avatars allow users to experience the adaptation, control, and perceptual illusion of alternative bodies. Although virtual bodies have medical applications in motor rehabilitation and therapeutic interventions, their potential for learning anatomy and medical communication remains underexplored. For learners and patients, anatomy, procedures, and medical imaging can be abstract and difficult to grasp. Experiencing anatomies, injuries, and treatments virtually through one&#39;s own body could be a valuable tool for fostering understanding. This work investigates the impact of avatars displaying anatomy and injuries suitable for such medical simulations. We ran a user study utilizing a skeleton avatar and virtual injuries, comparing to a healthy human avatar as a baseline. We evaluate the influence on embodiment, well-being, and presence with self-report questionnaires, as well as motor performance via an arm movement task. Our results show that while both anatomical representation and injuries increase feelings of eeriness, there are no negative effects on embodiment, well-being, presence, or motor performance. These findings suggest that virtual representations of anatomy and injuries are suitable for medical visualizations targeting learning or communication without significantly affecting users&#39; mental state or physical control within the simulation.},
  archive      = {J_TVCG},
  author       = {Constantin Kleinbeck and Hannah Schieber and Julian Kreimeier and Alejandro Martin-Gomez and Mathias Unberath and Daniel Roth},
  doi          = {10.1109/TVCG.2023.3320224},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4503-4513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Injured avatars: The impact of embodied anatomies and virtual injuries on well-being and performance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impact of occlusion on depth perception at arm’s length.
<em>TVCG</em>, <em>29</em>(11), 4494–4502. (<a
href="https://doi.org/10.1109/TVCG.2023.3320239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the accuracy of Augmented Reality (AR) technologies, particularly commercially available optical see-through displays, in depicting virtual content inside the human body for surgical planning. Their inherent limitations result in inaccuracies in perceived object positioning. We examine how occlusion, specifically with opaque surfaces, affects perceived depth of virtual objects at arm&#39;s length working distances. A custom apparatus with a half-silvered mirror was developed, providing accurate depth cues excluding occlusion, differing from commercial displays. We carried out a study, contrasting our apparatus with a HoloLens 2, involving a depth estimation task under varied surface complexities and illuminations. In addition, we explored the effects of creating a virtual “hole” in the surface. Subjects&#39; depth estimation accuracy and confidence were a ssessed. Results showed more depth estimation variation with HoloLens and significant depth error beneath complex occluding surfaces. However, creating a virtual hole significantly reduced depth errors and increased subjects&#39; confidence, irrespective of accuracy enhancement. These findings have important implications for the design and use of mixed-reality technologies in surgical applications, and industrial applications such as using virtual content to guide maintenance or repair of components hidden beneath the opaque outer surface of equipment. A free copy of this paper and all supplemental materials are available at https://bit.ly/3YbkwjU .},
  archive      = {J_TVCG},
  author       = {Marc Fischer and Jarrett Rosenberg and Christoph Leuze and Brian Hargreaves and Bruce Daniel},
  doi          = {10.1109/TVCG.2023.3320239},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4494-4502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of occlusion on depth perception at arm&#39;s length},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). More arrows in the quiver: Investigating the use of
auxiliary models to localize in-view components with augmented reality.
<em>TVCG</em>, <em>29</em>(11), 4483–4493. (<a
href="https://doi.org/10.1109/TVCG.2023.3320229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation and management of content are among the main open issues for the spread of Augmented Reality. In Augmented Reality interfaces for procedural tasks, a key authoring strategy is chunking instructions and using optimized visual cues, i.e., tailored to the specific information to convey. Nevertheless, research works rarely present rationales behind their choice. This work aims to provide design guidelines for the localization of in-view and not occluded components, which is recurrent information in technical documentation. Previous studies revealed that the most suited visual cues to convey this information are auxiliary models, i.e., abstract shapes that highlight the space region where the component is located. Among them, 3D arrows are widely used, but they may produce ambiguity of information. Furthermore, from the literature, it is unclear how to design auxiliary model shapes and if they are affected by the component shapes. To fill this gap, we conducted two user studies. In the first study, we collected the preference of 45 users regarding the shape, color, and animation of auxiliary models for the localization of various component shapes. According to the results of this study, we defined guidelines for designing optimized auxiliary models based on the component shapes. In the second user study, we validated these guidelines by evaluating the performance (localization time and recognition accuracy) and user experience of 24 users. The results of this study allowed us to confirm that designing auxiliary models following our guidelines leads to a higher recognition accuracy and user experience than using 3D arrows.},
  archive      = {J_TVCG},
  author       = {Sara Romano and Enricoandrea Laviola and Michele Gattullo and Michele Fiorentino and Antonio Emmanuele Uva},
  doi          = {10.1109/TVCG.2023.3320229},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4483-4493},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {More arrows in the quiver: Investigating the use of auxiliary models to localize in-view components with augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-immersion in audio augmented virtuality: The case study
of a static and approximated late reverberation algorithm.
<em>TVCG</em>, <em>29</em>(11), 4472–4482. (<a
href="https://doi.org/10.1109/TVCG.2023.3320213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In immersive Audio Augmented Reality, a virtual sound source should be indistinguishable from the existing real ones. This property can be evaluated with the co-immersion criterion, which encompasses scenes constituted by arbitrary configurations of real and virtual objects. Thus, we introduce the term Audio Augmented Virtuality (AAV) to describe a fully virtual environment consisting of auditory content captured from the real world, augmented by synthetic sound generation. We propose an experimental design in AAV investigating how simplified late reverberation (LR) affects the co-immersion of a sound source. Participants listened to simultaneous virtual speakers dynamically rendered through spatial Room Impulse Responses, and were asked to detect the presence of an impostor, i.e., a speaker rendered with one of two simplified LR conditions. Detection rates were found to be close to chance level, especially for one condition, suggesting a limited influence on co-immersion of the simplified LR in the evaluated AAV scenes. This methodology can be straightforwardly extended and applied to different acoustics scenes, complexities, i.e., the number of simultaneous speakers, and rendering parameters in order to further investigate the requirements for immersive audio technologies in AAR and AAV applications.},
  archive      = {J_TVCG},
  author       = {Davide Fantini and Giorgio Presti and Michele Geronazzo and Riccardo Bona and Alessandro Giuseppe Privitera and Federico Avanzini},
  doi          = {10.1109/TVCG.2023.3320213},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4472-4482},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Co-immersion in audio augmented virtuality: The case study of a static and approximated late reverberation algorithm},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SmartSpring: A low-cost wearable haptic VR display with
controllable passive feedback. <em>TVCG</em>, <em>29</em>(11),
4460–4471. (<a href="https://doi.org/10.1109/TVCG.2023.3320249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of virtual reality, the practical requirements of the wearable haptic interface have been greatly emphasized. While passive haptic devices are commonly used in virtual reality, they lack generality and are difficult to precisely generate continuous force feedback to users. In this work, we present SmartSpring, a new solution for passive haptics, which is inexpensive, lightweight and capable of providing controllable force feedback in virtual reality. We propose a hybrid spring-linkage structure as the proxy and flexibly control the mechanism for adjustable system stiffness. By analyzing the structure and force model, we enable a smart transform of the structure for producing continuous force signals. We quantitatively examine the real-world performance of SmartSpring to verify our model. By asymmetrically moving or actively pressing the end-effector, we show that our design can further support rendering torque and stiffness. Finally, we demonstrate the SmartSpring in a series of scenarios with user studies and a just noticeable difference analysis. Experimental results show the potential of the developed haptic display in virtual reality.},
  archive      = {J_TVCG},
  author       = {Hongkun Zhang and Kehong Zhou and Ke Shi and Yunhai Wang and Aiguo Song and Lifeng Zhu},
  doi          = {10.1109/TVCG.2023.3320249},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4460-4471},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SmartSpring: A low-cost wearable haptic VR display with controllable passive feedback},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level precues for guiding tasks within and between
workspaces in spatial augmented reality. <em>TVCG</em>, <em>29</em>(11),
4449–4459. (<a href="https://doi.org/10.1109/TVCG.2023.3320246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore Spatial Augmented Reality (SAR) precues (predictive cues) for procedural tasks within and between workspaces and for visualizing multiple upcoming steps in advance. We designed precues based on several factors: cue type, color transparency, and multi-level (number of precues). Precues were evaluated in a procedural task requiring the user to press buttons in three surrounding workspaces. Participants performed fastest in conditions where tasks were linked with line cues with different levels of color transparency. Precue performance was also affected by whether the next task was in the same workspace or a different one.},
  archive      = {J_TVCG},
  author       = {Benjamin Volmer and Jen-Shuo Liu and Brandon Matthews and Ina Bornkessel-Schlesewsky and Steven Feiner and Bruce H. Thomas},
  doi          = {10.1109/TVCG.2023.3320246},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4449-4459},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-level precues for guiding tasks within and between workspaces in spatial augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handwriting for efficient text entry in industrial VR
applications: Influence of board orientation and sensory feedback on
performance. <em>TVCG</em>, <em>29</em>(11), 4438–4448. (<a
href="https://doi.org/10.1109/TVCG.2023.3320215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text entry in Virtual Reality (VR) is becoming an increasingly important task as the availability of hardware increases and the range of VR applications widens. This is especially true for VR industrial applications where users need to input data frequently. Large-scale industrial adoption of VR is still hampered by the productivity gap between entering data via a physical keyboard and VR data entry methods. Data entry needs to be efficient, easy-to-use and to learn and not frustrating. In this paper, we present a new data entry method based on handwriting recognition (HWR). Users can input text by simply writing on a virtual surface. We conduct a user study to determine the best writing conditions when it comes to surface orientation and sensory feedback. This feedback consists of visual, haptic, and auditory cues. We find that using a slanted board with sensory feedback is best to maximize writing speeds and minimize physical demand. We also evaluate the performance of our method in terms of text entry speed, error rate, usability and workload. The results show that handwriting in VR has high entry speed, usability with little training compared to other controller-based virtual text entry techniques. The system could be further improved by reducing high error rates through the use of more efficient handwriting recognition tools. In fact, the total error rate is 9.28\% in the best condition. After 40 phrases of training, participants reach an average of 14.5 WPM, while a group with high VR familiarity reach 16.16 WPM after the same training. The highest observed textual data entry speed is 21.11 WPM.},
  archive      = {J_TVCG},
  author       = {Nicolas Fourrier and Guillaume Moreau and Mustapha Benaouicha and Jean-Marie Norm},
  doi          = {10.1109/TVCG.2023.3320215},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4438-4448},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handwriting for efficient text entry in industrial VR applications: Influence of board orientation and sensory feedback on performance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond my real body: Characterization, impacts, applications
and perspectives of “dissimilar” avatars in virtual reality.
<em>TVCG</em>, <em>29</em>(11), 4426–4437. (<a
href="https://doi.org/10.1109/TVCG.2023.3320209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, the avatar - the user&#39;s digital representation - is an important element which can drastically influence the immersive experience. In this paper, we especially focus on the use of “dissimilar” avatars i.e., avatars diverging from the real appearance of the user, whether they preserve an anthropomorphic aspect or not. Previous studies reported that dissimilar avatars can positively impact the user experience, in terms for example of interaction, perception or behaviour. However, given the sparsity and multi-disciplinary character of research related to dissimilar avatars, it tends to lack common understanding and methodology, hampering the establishment of novel knowledge on this topic. In this paper, we propose to address these limitations by discussing: (i) a methodology for dissimilar avatars characterization, (ii) their impacts on the user experience, (iii) their different fields of application, and finally, (iv) future research direction on this topic. Taken together, we believe that this paper can support future research related to dissimilar avatars, and help designers of VR applications to leverage dissimilar avatars appropriately.},
  archive      = {J_TVCG},
  author       = {Antonin Cheymol and Rebecca Fribourg and Anatole Lécuyer and Jean-Marie Normand and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2023.3320209},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4426-4437},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beyond my real body: Characterization, impacts, applications and perspectives of “Dissimilar” avatars in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-dependent visual behavior in immersive environments: A
comparative study of free exploration, memory and visual search.
<em>TVCG</em>, <em>29</em>(11), 4417–4425. (<a
href="https://doi.org/10.1109/TVCG.2023.3320259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual behavior depends on both bottom-up mechanisms, where gaze is driven by the visual conspicuity of the stimuli, and top-down mechanisms, guiding attention towards relevant areas based on the task or goal of the viewer. While this is well-known, visual attention models often focus on bottom-up mechanisms. Existing works have analyzed the effect of high-level cognitive tasks like memory or visual search on visual behavior; however, they have often done so with different stimuli, methodology, metrics and participants, which makes drawing conclusions and comparisons between tasks particularly difficult. In this work we present a systematic study of how different cognitive tasks affect visual behavior in a novel within-subjects design scheme. Participants performed free exploration, memory and visual search tasks in three different scenes while their eye and head movements were being recorded. We found significant, consistent differences between tasks in the distributions of fixations, saccades and head movements. Our findings can provide insights for practitioners and content creators designing task-oriented immersive applications.},
  archive      = {J_TVCG},
  author       = {Sandra Malpica and Daniel Martin and Ana Serrano and Diego Gutierrez and Belen Masia},
  doi          = {10.1109/TVCG.2023.3320259},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4417-4425},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Task-dependent visual behavior in immersive environments: A comparative study of free exploration, memory and visual search},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local-to-global panorama inpainting for locale-aware indoor
lighting prediction. <em>TVCG</em>, <em>29</em>(11), 4405–4416. (<a
href="https://doi.org/10.1109/TVCG.2023.3320233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting panoramic indoor lighting from a single perspective image is a fundamental but highly ill-posed problem in computer vision and graphics. To achieve locale-aware and robust prediction, this problem can be decomposed into three sub-tasks: depth-based image warping, panorama inpainting and high-dynamic-range (HDR) reconstruction, among which the success of panorama inpainting plays a key role. Recent methods mostly rely on convolutional neural networks (CNNs) to fill the missing contents in the warped panorama. However, they usually achieve suboptimal performance since the missing contents occupy a very large portion in the panoramic space while CNNs are plagued by limited receptive fields. The spatially-varying distortion in the spherical signals further increases the difficulty for conventional CNNs. To address these issues, we propose a local-to-global strategy for large-scale panorama inpainting. In our method, a depth-guided local inpainting is first applied on the warped panorama to fill small but dense holes. Then, a transformer-based network, dubbed PanoTransformer , is designed to hallucinate reasonable global structures in the large holes. To avoid distortion, we further employ cubemap projection in our design of PanoTransformer. The high-quality panorama recovered at any locale helps us to capture spatially-varying indoor illumination with physically-plausible global structures and fine details.},
  archive      = {J_TVCG},
  author       = {Jiayang Bai and Zhen He and Shan Yang and Jie Guo and Zhenyu Chen and Yan Zhang and Yanwen Guo},
  doi          = {10.1109/TVCG.2023.3320233},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4405-4416},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Local-to-global panorama inpainting for locale-aware indoor lighting prediction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual reality sickness reduces attention during immersive
experiences. <em>TVCG</em>, <em>29</em>(11), 4394–4404. (<a
href="https://doi.org/10.1109/TVCG.2023.3320222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show that Virtual Reality (VR) sickness is associated with a reduction in attention, which was detected with the P3b Event-Related Potential (ERP) component from electroencephalography (EEG) measurements collected in a dual-task paradigm. We hypothesized that sickness symptoms such as nausea, eyestrain, and fatigue would reduce the users&#39; capacity to pay attention to tasks completed in a virtual environment, and that this reduction in attention would be dynamically reflected in a decrease of the P3b amplitude while VR sickness was experienced. In a user study, participants were taken on a tour through a museum in VR along paths with varying amounts of rotation, shown previously to cause different levels of VR sickness. While paying attention to the virtual museum (the primary task), participants were asked to silently count tones of a different frequency (the secondary task). Control measurements for comparison against the VR sickness conditions were taken when the users were not wearing the Head-Mounted Display (HMD) and while they were immersed in VR but not moving through the environment. This exploratory study shows, across multiple analyses, that the effect mean amplitude of the P3b collected during the task is associated with both sickness severity measured after the task with a questionnaire (SSQ) and with the number of counting errors on the secondary task. Thus, VR sickness may impair attention and task performance, and these changes in attention can be tracked with ERP measures as they happen, without asking participants to assess their sickness symptoms in the moment.},
  archive      = {J_TVCG},
  author       = {Katherine J. Mimnaugh and Evan G. Center and Markku Suomalainen and Israel Becerra and Eliezer Lozano and Rafael Murrieta-Cid and Timo Ojala and Steven M. LaValle and Kara D. Federmeier},
  doi          = {10.1109/TVCG.2023.3320222},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4394-4404},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual reality sickness reduces attention during immersive experiences},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MagLoc-AR: Magnetic-based localization for visual-free
augmented reality in large-scale indoor environments. <em>TVCG</em>,
<em>29</em>(11), 4383–4393. (<a
href="https://doi.org/10.1109/TVCG.2023.3321088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate localization of a display device is essential for AR in large-scale environments. Visual-based localization is the most commonly used solution, but poses privacy risks, suffers from robustness issues and consumes high power. Wireless signal-based localization is a potential visual-free solution, but its accuracy is not enough for AR. In this paper, we present MagLoc-AR, a novel visual-free localization solution that achieves sufficient accuracy for some AR applications (e.g. AR navigation) in large-scale indoor environments. We exploit the location-dependent magnetic field interference that is ubiquitous indoors as a localization signal. Our method requires only a consumer-grade 9-axis IMU, with the gyroscope and acceleration measurements used to recover the motion trajectory, and the magnetic measurements used to register the trajectory to the global map. To meet the accuracy requirement of AR, we propose a mapping method to reconstruct a globally consistent magnetic field of the environment, and a localization method fusing the biased magnetic measurements with the network-predicted motion to improve localization accuracy. In addition, we provide the first dataset for both visual-based and geomagnetic-based localization in large-scale indoor environments. Evaluations on the dataset demonstrate that our proposed method is sufficiently accurate for AR navigation and has advantages over the visual-based methods in terms of power consumption and robustness. Project page: https://github.com/zju3dv/MagLoc-AR/},
  archive      = {J_TVCG},
  author       = {Haomin Liu and Hua Xue and Linsheng Zhao and Danpeng Chen and Zhen Peng and Guofeng Zhang},
  doi          = {10.1109/TVCG.2023.3321088},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4383-4393},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MagLoc-AR: Magnetic-based localization for visual-free augmented reality in large-scale indoor environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of reward schedule and avatar visibility on joint
agency during VR collaboration. <em>TVCG</em>, <em>29</em>(11),
4372–4382. (<a href="https://doi.org/10.1109/TVCG.2023.3320221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint agency, a group-level sense of agency, has been studied as an essential social cognitive element while engaging in collaborative tasks. The joint agency has been actively investigated in diverse contexts (e.g., performance, reward schedules, and predictability), yet the studies were mostly conducted in traditional 2D computer environments. Since virtual reality (VR) is an emerging technology for remote collaboration, we aimed to probe the effects of traditional reward schedule factors along with novel VR features (i.e., avatar visibility) on joint agency during remote collaboration. In this study, we implemented an experiment based on a card-matching game to test the effects of the reward schedule (fair or equal) and the counterpart&#39;s avatar hand visibility (absent or present) on the sense of joint agency. The results showed that participants felt a higher sense of joint agency when the reward was distributed equally regardless of the individual performance and when the counterpart&#39;s avatar hand was present. Moreover, the effects of reward schedule and avatar hand visibility interacted, with a bigger amount of deficit for the absent avatar hand when the reward was distributed differentially according to performance. Interestingly, the sense of joint agency was strongly correlated to the level of collaborative performance, as well as to perceptions of other social cognitive factors, including cooperativeness, reward fairness, and social presence. These results contribute to the understanding of joint agency perceptions during VR collaboration and provide design guidelines for remote collaborative tasks and environments for users&#39; optimal social experience and performance.},
  archive      = {J_TVCG},
  author       = {Seung Un Lee and Jinwook Kim and Jeongmi Lee},
  doi          = {10.1109/TVCG.2023.3320221},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4372-4382},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of reward schedule and avatar visibility on joint agency during VR collaboration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FineStyle: Semantic-aware fine-grained motion style transfer
with dual interactive-flow fusion. <em>TVCG</em>, <em>29</em>(11),
4361–4371. (<a href="https://doi.org/10.1109/TVCG.2023.3320216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present FineStyle, a novel framework for motion style transfer that generates expressive human animations with specific styles for virtual reality and vision fields. It incorporates semantic awareness, which improves motion representation and allows for precise and stylish animation generation. Existing methods for motion style transfer have all failed to consider the semantic meaning behind the motion, resulting in limited controls over the generated human animations. To improve, FineStyle introduces a new cross-modality fusion module called Dual Interactive-Flow Fusion (DIFF). As the first attempt, DIFF integrates motion style features and semantic flows, producing semantic-aware style codes for fine-grained motion style transfer. FineStyle uses an innovative two-stage semantic guidance approach that leverages semantic clues to enhance the discriminative power of both semantic and style features. At an early stage, a semantic-guided encoder introduces distinct semantic clues into the style flow. Then, at a fine stage, both flows are further fused interactively, selecting the matched and critical clues from both flows. Extensive experiments demonstrate that FineStyle outperforms state-of-the-art methods in visual quality and controllability. By considering the semantic meaning behind motion style patterns, FineStyle allows for more precise control over motion styles. Source code and model are available on https://github.com/XingliangJin/Fine-Style.git .},
  archive      = {J_TVCG},
  author       = {Wenfeng Song and Xingliang Jin and Shuai Li and Chenglizhao Chen and Aimin Hao and Xia Hou},
  doi          = {10.1109/TVCG.2023.3320216},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4361-4371},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FineStyle: Semantic-aware fine-grained motion style transfer with dual interactive-flow fusion},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D-SAV360: A dataset of gaze scanpaths on 360° ambisonic
videos. <em>TVCG</em>, <em>29</em>(11), 4350–4360. (<a
href="https://doi.org/10.1109/TVCG.2023.3320237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human visual behavior within virtual reality environments is crucial to fully leverage their potential. While previous research has provided rich visual data from human observers, existing gaze datasets often suffer from the absence of multimodal stimuli. Moreover, no dataset has yet gathered eye gaze trajectories (i.e., scanpaths) for dynamic content with directional ambisonic sound, which is a critical aspect of sound perception by humans. To address this gap, we introduce D-SAV360, a dataset of 4,609 head and eye scanpaths for 360° videos with first-order ambisonics. This dataset enables a more comprehensive study of multimodal interaction on visual behavior in virtual reality environments. We analyze our collected scanpaths from a total of 87 participants viewing 85 different videos and show that various factors such as viewing mode, content type, and gender significantly impact eye movement statistics. We demonstrate the potential of D-SAV360 as a benchmarking resource for state-of-the-art attention prediction models and discuss its possible applications in further research. By providing a comprehensive dataset of eye movement data for dynamic, multimodal virtual environments, our work can facilitate future investigations of visual behavior and attention in virtual reality.},
  archive      = {J_TVCG},
  author       = {Edurne Bernal-Berdun and Daniel Martin and Sandra Malpica and Pedro J. Perez and Diego Gutierrez and Belen Masia and Ana Serrano},
  doi          = {10.1109/TVCG.2023.3320237},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4350-4360},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {D-SAV360: A dataset of gaze scanpaths on 360° ambisonic videos},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural projection mapping using reflectance fields.
<em>TVCG</em>, <em>29</em>(11), 4339–4349. (<a
href="https://doi.org/10.1109/TVCG.2023.3320256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a high resolution spatially adaptive light source, or a projector, into a neural reflectance field that allows to both calibrate the projector and photo realistic light editing. The projected texture is fully differentiable with respect to all scene parameters, and can be optimized to yield a desired appearance suitable for applications in augmented reality and projection mapping. Our neural field consists of three neural networks, estimating geometry, material, and transmittance. Using an analytical BRDF model and carefully selected projection patterns, our acquisition process is simple and intuitive, featuring a fixed uncalibrated projected and a handheld camera with a co-located light source. As we demonstrate, the virtual projector incorporated into the pipeline improves scene understanding and enables various projection mapping applications, alleviating the need for time consuming calibration steps performed in a traditional setting per view or projector location. In addition to enabling novel viewpoint synthesis, we demonstrate state-of-the-art performance projector compensation for novel viewpoints, improvement over the baselines in material and scene reconstruction, and three simply implemented scenarios where projection image optimization is performed, including the use of a 2D generative model to consistently dictate scene appearance from multiple viewpoints. We believe that neural projection mapping opens up the door to novel and exciting downstream tasks, through the joint optimization of the scene and projection images.},
  archive      = {J_TVCG},
  author       = {Yotam Erel and Daisuke Iwai and Amit H. Bermano},
  doi          = {10.1109/TVCG.2023.3320256},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4339-4349},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural projection mapping using reflectance fields},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). I am a genius! Influence of virtually embodying leonardo da
vinci on creative performance. <em>TVCG</em>, <em>29</em>(11),
4328–4338. (<a href="https://doi.org/10.1109/TVCG.2023.3320225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) provides users with the ability to substitute their physical appearance by embodying virtual characters (avatars) using head-mounted displays and motion-capture technologies. Previous research demonstrated that the sense of embodiment toward an avatar can impact user behavior and cognition. In this paper, we present an experiment designed to investigate whether embodying a well-known creative genius could enhance participants&#39; creative performance. Following a preliminary online survey ( $N = 157$ ) to select a famous character suited to the purpose of this study, we developed a VR application allowing participants to embody Leonardo da Vinci or a self-avatar. Self-avatars were approximately matched with participants in terms of skin tone and morphology. 40 participants took part in three tasks seamlessly integrated in a virtual workshop. The first task was based on a Guilford&#39;s Alternate Uses test (GAU) to assess participants&#39; divergent abilities in terms of fluency and originality. The second task was based on a Remote Associates Test (RAT) to evaluate convergent abilities. Lastly, the third task consisted in designing potential alternative uses of an object displayed in the virtual environment using a 3D sketching tool. Participants embodying Leonardo da Vinci demonstrated significantly higher divergent thinking abilities, with a substantial difference in fluency between the groups. Conversely, participants embodying a self-avatar performed significantly better in the convergent thinking task. Taken together, these results promote the use of our virtual embodiment approach, especially in applications where divergent creativity plays an important role, such as design and innovation.},
  archive      = {J_TVCG},
  author       = {Geoffrey Gorisse and Simon Wellenreiter and Sylvain Fleury and Anatole Lécuyer and Simon Richir and Olivier Christmann},
  doi          = {10.1109/TVCG.2023.3320225},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4328-4338},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {I am a genius! influence of virtually embodying leonardo da vinci on creative performance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic scatterplot design optimization for clustering
identification. <em>TVCG</em>, <em>29</em>(10), 4312–4327. (<a
href="https://doi.org/10.1109/TVCG.2022.3189883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots are among the most widely used visualization techniques. Compelling scatterplot visualizations improve understanding of data by leveraging visual perception to boost awareness when performing specific visual analytic tasks. Design choices in scatterplots, such as graphical encodings or data aspects, can directly impact decision-making quality for low-level tasks like clustering. Hence, constructing frameworks that consider both the perceptions of the visual encodings and the task being performed enables optimizing visualizations to maximize efficacy. In this article, we propose an automatic tool to optimize the design factors of scatterplots to reveal the most salient cluster structure. Our approach leverages the merge tree data structure to identify the clusters and optimize the choice of subsampling algorithm, sampling rate, marker size, and marker opacity used to generate a scatterplot image. We validate our approach with user and case studies that show it efficiently provides high-quality scatterplot designs from a large parameter space.},
  archive      = {J_TVCG},
  author       = {Ghulam Jilani Quadri and Jennifer Adorno Nieves and Brenton M. Wiernik and Paul Rosen},
  doi          = {10.1109/TVCG.2022.3189883},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4312-4327},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic scatterplot design optimization for clustering identification},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Voxel2vec: A natural language processing approach to
learning distributed representations for scientific data. <em>TVCG</em>,
<em>29</em>(10), 4296–4311. (<a
href="https://doi.org/10.1109/TVCG.2022.3189094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relationships in scientific data, such as the numerical and spatial distribution relations of features in univariate data, the scalar-value combinations’ relations in multivariate data, and the association of volumes in time-varying and ensemble data, are intricate and complex. This paper presents voxel2vec, a novel unsupervised representation learning model, which is used to learn distributed representations of scalar values/scalar-value combinations in a low-dimensional vector space. Its basic assumption is that if two scalar values/scalar-value combinations have similar contexts, they usually have high similarity in terms of features. By representing scalar values/scalar-value combinations as symbols, voxel2vec learns the similarity between them in the context of spatial distribution and then allows us to explore the overall association between volumes by transfer prediction. We demonstrate the usefulness and effectiveness of voxel2vec by comparing it with the isosurface similarity map of univariate data and applying the learned distributed representations to feature classification for multivariate data and to association analysis for time-varying and ensemble data.},
  archive      = {J_TVCG},
  author       = {Xiangyang He and Yubo Tao and Shuoliu Yang and Haoran Dai and Hai Lin},
  doi          = {10.1109/TVCG.2022.3189094},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4296-4311},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Voxel2vec: A natural language processing approach to learning distributed representations for scientific data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic mesh and shader level of detail. <em>TVCG</em>,
<em>29</em>(10), 4284–4295. (<a
href="https://doi.org/10.1109/TVCG.2022.3188775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The level of detail (LOD) technique has been widely exploited as a key rendering optimization in many graphics applications. Numerous approaches have been proposed to automatically generate different kinds of LODs, such as geometric LOD or shader LOD. However, none of them have considered simplifying the geometry and shader at the same time. In this paper, we explore the observation that simplifications of geometric and shading details can be combined to provide a greater variety of tradeoffs between performance and quality. We present a new discrete multiresolution representation of objects, which consists of mesh and shader LODs. Each level of the representation could contain both simplified representations of shader and mesh. To create such LODs, we propose two automatic algorithms that pursue the best simplifications of meshes and shaders at adaptively selected distances. The results show that our mesh and shader LOD achieves better performance-quality tradeoffs than prior LOD representations, such as those that only consider simplified meshes or shaders.},
  archive      = {J_TVCG},
  author       = {Yuzhi Liang and Qi Song and Rui Wang and Yuchi Huo and Hujun Bao},
  doi          = {10.1109/TVCG.2022.3188775},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4284-4295},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic mesh and shader level of detail},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-based OA-style paper pop-up design via mixed-integer
programming. <em>TVCG</em>, <em>29</em>(10), 4269–4283. (<a
href="https://doi.org/10.1109/TVCG.2022.3189569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Origami architecture (OA) is a fascinating papercraft that involves only a piece of paper with cuts and folds. Interesting geometric structures ‘pop up’ when the paper is opened. However, manually designing such a physically valid 2D paper pop-up plan is challenging since fold lines must jointly satisfy hard spatial constraints. Existing works on automatic OA-style paper pop-up design all focused on how to generate a pop-up structure that approximates a given target 3D model. This article presents the first OA-style paper pop-up design framework that takes 2D images instead of 3D models as input. Our work is inspired by the fact that artists often use 2D profiles to guide the design process, thus benefited from the high availability of 2D image resources. Due to the lack of 3D geometry information, we perform novel theoretic analysis to ensure the foldability and stability of the resultant design. Based on a novel graph representation of the paper pop-up plan, we further propose a practical optimization algorithm via mixed-integer programming that jointly optimizes the topology and geometry of the 2D plan. We also allow the user to interactively explore the design space by specifying constraints on fold lines. Finally, we evaluate our framework on various images with interesting 2D shapes. Experiments and comparisons exhibit both the efficacy and efficiency of our framework.},
  archive      = {J_TVCG},
  author       = {Fei Huang and Chen Liu and Kai-Wen Hsiao and Ying-Miao Kuo and Hung-Kuo Chu and Yong-Liang Yang},
  doi          = {10.1109/TVCG.2022.3189569},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4269-4283},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image-based OA-style paper pop-up design via mixed-integer programming},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Target netgrams: An annulus-constrained stress model for
radial graph visualization. <em>TVCG</em>, <em>29</em>(10), 4256–4268.
(<a href="https://doi.org/10.1109/TVCG.2022.3187425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Target Netgrams as a visualization technique for radial layouts of graphs. Inspired by manually created target sociograms, we propose an annulus-constrained stress model that aims to position nodes onto the annuli between adjacent circles for indicating their radial hierarchy, while maintaining the network structure (clusters and neighborhoods) and improving readability as much as possible. This is achieved by having more space on the annuli than traditional layout techniques. By adapting stress majorization to this model, the layout is computed as a constrained least square optimization problem. Additional constraints (e.g., parent-child preservation, attribute-based clusters and structure-aware radii) are provided for exploring nodes, edges, and levels of interest. We demonstrate the effectiveness of our method through a comprehensive evaluation, a user study, and a case study.},
  archive      = {J_TVCG},
  author       = {Mingliang Xue and Yunhai Wang and Chang Han and Jian Zhang and Zheng Wang and Kaiyi Zhang and Christophe Hurter and Jian Zhao and Oliver Deussen},
  doi          = {10.1109/TVCG.2022.3187425},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4256-4268},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Target netgrams: An annulus-constrained stress model for radial graph visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring the effectiveness of static maps to communicate
changes over time. <em>TVCG</em>, <em>29</em>(10), 4243–4255. (<a
href="https://doi.org/10.1109/TVCG.2022.3188940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both in digital and print media, it is common to use static maps to show the evolution of values in various regions over time. The ability to communicate local or global trends, while reducing the cognitive load on readers, is of vital importance for an audience that is not always well versed in map interpretation. This study aims to measure the efficiency of four static maps (choropleth, tile grid map and their banded versions) to test their usefulness in presenting changes over time from a user experience perspective. We first evaluate the effectiveness of these map types by quantitative performance analysis (time and success rates). In a second phase, we gather qualitative data to detect which type of map favors decision-making. On a quantitative level, our results show that certain types of maps work better to show global trends, while other types are more useful when analyzing regional trends or detecting the regions that fit a specific pattern. On a qualitative level, those representations which are already familiar to the user are often better valued despite having lower measured success rates.},
  archive      = {J_TVCG},
  author       = {Luz Calvo and Fernando Cucchietti and Mario Pérez-Montoro},
  doi          = {10.1109/TVCG.2022.3188940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4243-4255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring the effectiveness of static maps to communicate changes over time},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point cloud completion via skeleton-detail transformer.
<em>TVCG</em>, <em>29</em>(10), 4229–4242. (<a
href="https://doi.org/10.1109/TVCG.2022.3185247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud shape completion plays a central role in diverse 3D vision and robotics applications. Early methods used to generate global shapes without local detail refinement. Current methods tend to leverage local features to preserve the observed geometric details. However, they usually adopt the convolutional architecture over the incomplete point cloud to extract local features to restore the diverse information of both latent shape skeleton and geometric details, where long-distance correlation among the skeleton and details is ignored. In this work, we present a coarse-to-fine completion framework, which makes full use of both neighboring and long-distance region cues for point cloud completion. Our network leverages a Skeleton-Detail Transformer, which contains cross-attention and self-attention layers, to fully explore the correlation from local patterns to global shape and utilize it to enhance the overall skeleton. Also, we propose a selective attention mechanism to save memory usage in the attention process without significantly affecting performance. We conduct extensive experiments on the ShapeNet dataset and real-scanned datasets. Qualitative and quantitative evaluations demonstrate that our proposed network outperforms current state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Wenxiao Zhang and Huajian Zhou and Zhen Dong and Jun Liu and Qingan Yan and Chunxia Xiao},
  doi          = {10.1109/TVCG.2022.3185247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4229-4242},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Point cloud completion via skeleton-detail transformer},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Role-exchange playing: An exploration of role-playing
effects for anti-bullying in immersive virtual environments.
<em>TVCG</em>, <em>29</em>(10), 4215–4228. (<a
href="https://doi.org/10.1109/TVCG.2022.3184986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Role-playing is widely used in many areas, such as psychotherapy and behavior change. However, few studies have explored the possible effects of playing multiple roles in a single role-playing process. We propose a new role-playing paradigm, called role-exchange playing, in which a user plays two opposite roles successively in the same simulated event for better cognitive enhancement. We designed an experiment with this novel role-exchange playing strategy in the immersive virtual environments; and school bullying was chosen as a scenario in this case. A total of 234 middle/high school students were enrolled in the mixed-design experiment. From the user study, we found that through role-exchange, students developed more morally correct opinions about bullying, as well as increased empathy and willingness to engage in supportive behavior. They also showed increased commitment to stopping bullying others. Our role-exchange paradigm could achieve a better effect than traditional role-playing methods in situations where participants have no prior experience associated with the roles they play. Therefore, using role-exchange playing in the immersive virtual environments to educate minors can help prevent them from bullying others in the real world. Our study indicates a positive significance in moral education of teenagers. Our role-exchange playing may have the potential to be extended to such applications as counseling, therapy, and crime prevention.},
  archive      = {J_TVCG},
  author       = {Xiang Gu and Sheng Li and Kangrui Yi and Xiaojuan Yang and Huiling Liu and Guoping Wang},
  doi          = {10.1109/TVCG.2022.3184986},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4215-4228},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Role-exchange playing: An exploration of role-playing effects for anti-bullying in immersive virtual environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding nano-ötzi: Cryo-electron tomography visualization
guided by learned segmentation. <em>TVCG</em>, <em>29</em>(10),
4198–4214. (<a href="https://doi.org/10.1109/TVCG.2022.3186146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.},
  archive      = {J_TVCG},
  author       = {Ngan Nguyen and Ciril Bohak and Dominik Engel and Peter Mindek and Ondřej Strnad and Peter Wonka and Sai Li and Timo Ropinski and Ivan Viola},
  doi          = {10.1109/TVCG.2022.3186146},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4198-4214},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Finding nano-Ötzi: Cryo-electron tomography visualization guided by learned segmentation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep light field spatial super-resolution using
heterogeneous imaging. <em>TVCG</em>, <em>29</em>(10), 4183–4197. (<a
href="https://doi.org/10.1109/TVCG.2022.3184047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) imaging expands traditional imaging techniques by simultaneously capturing the intensity and direction information of light rays, and promotes many visual applications. However, owing to the inherent trade-off between the spatial and angular dimensions, LF images acquired by LF cameras usually suffer from low spatial resolution. Many current approaches increase the spatial resolution by exploring the four-dimensional (4D) structure of the LF images, but they have difficulties in recovering fine textures at a large upscaling factor. To address this challenge, this paper proposes a new deep learning-based LF spatial super-resolution method using heterogeneous imaging (LFSSR-HI). The designed heterogeneous imaging system uses an extra high-resolution (HR) traditional camera to capture the abundant spatial information in addition to the LF camera imaging, where the auxiliary information from the HR camera is utilized to super-resolve the LF image. Specifically, an LF feature alignment module is constructed to learn the correspondence between the 4D LF image and the 2D HR image to realize information alignment. Subsequently, a multi-level spatial-angular feature enhancement module is designed to gradually embed the aligned HR information into the rough LF features. Finally, the enhanced LF features are reconstructed into a super-resolved LF image using a simple feature decoder. To improve the flexibility of the proposed method, a pyramid reconstruction strategy is leveraged to generate multi-scale super-resolution results in one forward inference. The experimental results show that the proposed LFSSR-HI method achieves significant advantages over the state-of-the-art methods in both qualitative and quantitative comparisons. Furthermore, the proposed method preserves more accurate angular consistency.},
  archive      = {J_TVCG},
  author       = {Yeyao Chen and Gangyi Jiang and Mei Yu and Haiyong Xu and Yo-Sung Ho},
  doi          = {10.1109/TVCG.2022.3184047},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4183-4197},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep light field spatial super-resolution using heterogeneous imaging},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning implicit glyph shape representation. <em>TVCG</em>,
<em>29</em>(10), 4172–4182. (<a
href="https://doi.org/10.1109/TVCG.2022.3183400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic generation of fonts can greatly facilitate the font design process, and provide prototypes where designers can draw inspiration from. Existing generation methods are mainly built upon rasterized glyph images to utilize the successful convolutional architecture, but ignore the vector nature of glyph shapes. We present an implicit representation, modeling each glyph as shape primitives enclosed by several quadratic curves. This structured implicit representation is shown to be better suited for glyph modeling, and enables rendering glyph images at arbitrary high resolutions. Our representation gives high-quality glyph reconstruction and interpolation results, and performs well on the challenging one-shot font style transfer task comparing to other alternatives both qualitatively and quantitatively.},
  archive      = {J_TVCG},
  author       = {Ying-Tian Liu and Yuan-Chen Guo and Yi-Xiao Li and Chen Wang and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2022.3183400},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4172-4182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning implicit glyph shape representation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EBBE-text: Explaining neural networks by exploring text
classification decision boundaries. <em>TVCG</em>, <em>29</em>(10),
4154–4171. (<a href="https://doi.org/10.1109/TVCG.2022.3184247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.},
  archive      = {J_TVCG},
  author       = {Alexis Delaforge and Jérôme Azé and Sandra Bringay and Caroline Mollevi and Arnaud Sallaberry and Maximilien Servajean},
  doi          = {10.1109/TVCG.2022.3184247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4154-4171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EBBE-text: Explaining neural networks by exploring text classification decision boundaries},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Compact world anchors: Registration using parametric
primitives as scene description. <em>TVCG</em>, <em>29</em>(10),
4140–4153. (<a href="https://doi.org/10.1109/TVCG.2022.3183264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a registration method relying on geometric constraints extracted from parametric primitives contained in 3D parametric models. Our method solves the registration in closed-form from three line-to-line, line-to-plane or plane-to-plane correspondences. The approach either works with semantically segmented RGB-D scans of the scene or with the output of plane detection in common frameworks like ARKit and ARCore. Based on the primitives detected in the scene, we build a list of descriptors using the normals and centroids of all the found primitives, and match them against the pre-computed list of descriptors from the model in order to find the scene-to-model primitive correspondences. Finally, we use our closed-form solver to estimate the 6DOFtransformation from three lines and one point, which we obtain from the parametric representations of the model and scene parametric primitives. Quantitative and qualitative experiments on synthetic and real-world data sets demonstrate the performance and robustness of our method. We show that it can be used to create compact world anchors for indoor localization in AR applications on mobile devices leveraging commercial SLAM capabilities.},
  archive      = {J_TVCG},
  author       = {Fernando Reyes-Aviles and Philipp Fleck and Dieter Schmalstieg and Clemens Arth},
  doi          = {10.1109/TVCG.2022.3183264},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4140-4153},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Compact world anchors: Registration using parametric primitives as scene description},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing the effect of diverse gaze and head direction on
facial expression recognition with photo-reflective sensors embedded in
a head-mounted display. <em>TVCG</em>, <em>29</em>(10), 4124–4139. (<a
href="https://doi.org/10.1109/TVCG.2022.3179766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the facial expression recognition techniques for Head-Mounted Display (HMD) users, embedded photo-reflective sensors have been used. In this paper, we investigate how gaze and face directions affect facial expression recognition using the embedded photo-reflective sensors. First, we collected a dataset of five facial expressions (Neutral, Happy, Angry, Sad, Surprised) while looking in diverse directions by moving 1) the eyes and 2) the head. Using the dataset, we analyzed the effect of gaze and face directions by constructing facial expression classifiers in five ways and evaluating the classification accuracy of each classifier. The results revealed that the single classifier that learned the data for all gaze points achieved the highest classification performance. Then, we investigated which facial part was affected by the gaze and face direction. The results showed that the gaze directions affected the upper facial parts, while the face directions affected the lower facial parts. In addition, by removing the bias of facial expression reproducibility, we investigated the pure effect of gaze and face directions in three conditions. The results showed that, in terms of gaze direction, building classifiers for each direction significantly improved the classification accuracy. However, in terms of face directions, there were slight differences between the classifier conditions. Our experimental results implied that multiple classifiers corresponding to multiple gaze and face directions improved facial expression recognition accuracy, but collecting the data of the vertical movement of gaze and face is a practical solution to improving facial expression recognition accuracy.},
  archive      = {J_TVCG},
  author       = {Fumihiko Nakamura and Masaaki Murakami and Katsuhiro Suzuki and Masaaki Fukuoka and Katsutoshi Masai and Maki Sugimoto},
  doi          = {10.1109/TVCG.2022.3179766},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4124-4139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analyzing the effect of diverse gaze and head direction on facial expression recognition with photo-reflective sensors embedded in a head-mounted display},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Redirected walking for exploring immersive virtual spaces
with HMD: A comprehensive review and recent advances. <em>TVCG</em>,
<em>29</em>(10), 4104–4123. (<a
href="https://doi.org/10.1109/TVCG.2022.3179269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real walking techniques can provide the user with a more natural, highly immersive walking experience compared to the experience of other locomotion techniques. In contrast to the direct mapping between the virtual space and an equal-sized physical space that can be simply realized, the nonequivalent mapping that enables the user to explore a large virtual space by real walking within a confined physical space is complex. To address this issue, the redirected walking (RDW) technique is proposed by many works to adjust the user&#39;s virtual and physical movements based on some redirection manipulations. In this manner, subtle or overt motion deviations can be injected between the user&#39;s virtual and physical movements, allowing the user to undertake real walking in large virtual spaces by using different redirection controller methods. In this paper, we present a brief review to describe major concepts and methodologies in the field of redirected walking. First, we provide the fundamentals and basic criteria of RDW, and then we describe the redirection manipulations that can be applied to adjust the user&#39;s movements during virtual exploration. Furthermore, we clarify the redirection controller methods that properly adopt strategies for combining different redirection manipulations and present a classification of these methods by several categories. Finally, we summarize several experimental metrics to evaluate the performance of redirection controller methods and discuss current challenges and future work. Our study systematically classifies the relevant theories, concepts, and methods of RDW, and provides assistance to the newcomers in understanding and implementing the RDW technique.},
  archive      = {J_TVCG},
  author       = {Linwei Fan and Huiyu Li and Miaowen Shi},
  doi          = {10.1109/TVCG.2022.3179269},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4104-4123},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirected walking for exploring immersive virtual spaces with HMD: A comprehensive review and recent advances},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigation of VR sickness during locomotion with a
motion-based dynamic vision modulator. <em>TVCG</em>, <em>29</em>(10),
4089–4103. (<a href="https://doi.org/10.1109/TVCG.2022.3181262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, VR sickness resulting from continuous locomotion via controllers or joysticks is still a significant problem. In this article, we present a set of algorithms to mitigate VR sickness that dynamically modulate the user&#39;s field of view by modifying the contrast of the periphery based on movement, color, and depth. In contrast with previous work, this vision modulator is a shader that is triggered by specific motions known to cause VR sickness, such as acceleration, strafing, and linear velocity. Moreover, the algorithm is governed by delta velocity, delta angle, and average color of the view. We ran two experiments with different washout periods to investigate the effectiveness of dynamic modulation on the symptoms of VR sickness, in which we compared this approach against a baseline and pitch-black field-of-view restrictors. Our first experiment made use of a just-noticeable-sickness design, which can be useful for building experiments with a short washout period.},
  archive      = {J_TVCG},
  author       = {Guanghan Zhao and Jason Orlosky and Steven Feiner and Photchara Ratsamee and Yuki Uranishi},
  doi          = {10.1109/TVCG.2022.3181262},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4089-4103},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mitigation of VR sickness during locomotion with a motion-based dynamic vision modulator},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DrawingInStyles: Portrait image generation and editing with
spatially conditioned StyleGAN. <em>TVCG</em>, <em>29</em>(10),
4074–4088. (<a href="https://doi.org/10.1109/TVCG.2022.3178734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research topic of sketch-to-portrait generation has witnessed a boost of progress with deep learning techniques. The recently proposed StyleGAN architectures achieve state-of-the-art generation ability but the original StyleGAN is not friendly for sketch-based creation due to its unconditional generation nature. To address this issue, we propose a direct conditioning strategy to better preserve the spatial information under the StyleGAN framework. Specifically, we introduce Spatially Conditioned StyleGAN ( SC-StyleGAN for short), which explicitly injects spatial constraints to the original StyleGAN generation process. We explore two input modalities, sketches and semantic maps, which together allow users to express desired generation results more precisely and easily. Based on SC-StyleGAN , we present DrawingInStyles , a novel drawing interface for non-professional users to easily produce high-quality, photo-realistic face images with precise control, either from scratch or editing existing ones. Qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.},
  archive      = {J_TVCG},
  author       = {Wanchao Su and Hui Ye and Shu-Yu Chen and Lin Gao and Hongbo Fu},
  doi          = {10.1109/TVCG.2022.3178734},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4074-4088},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DrawingInStyles: Portrait image generation and editing with spatially conditioned StyleGAN},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-occlusion aware lighting model for real-time dynamic
reconstruction. <em>TVCG</em>, <em>29</em>(10), 4062–4073. (<a
href="https://doi.org/10.1109/TVCG.2022.3178237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-time dynamic reconstruction, geometry and motion are the major focuses while appearance is not fully explored, leading to the low-quality appearance of the reconstructed surfaces. In this article, we propose a lightweight lighting model that considers spatially varying lighting conditions caused by self-occlusion. This model estimates per-vertex masks on top of a single Spherical Harmonic (SH) lighting to represent spatially varying lighting conditions without adding too much computation cost. The mask is estimated based on the local geometry of a vertex to model the self-occlusion effect, which is the major reason leading to the spatial variation of lighting. Furthermore, to use this model in dynamic reconstruction, we also improve the motion estimation quality by adding a real-time per-vertex displacement estimation step. Experiments demonstrate that both the reconstructed appearance and the motion are largely improved compared with the current state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Chengwei Zheng and Wenbin Lin and Feng Xu},
  doi          = {10.1109/TVCG.2022.3178237},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4062-4073},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A self-occlusion aware lighting model for real-time dynamic reconstruction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COMPO*SED: Composite parallel coordinates for co-dependent
multi-attribute choices. <em>TVCG</em>, <em>29</em>(10), 4047–4061. (<a
href="https://doi.org/10.1109/TVCG.2022.3180899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Composite Parallel Coordinates, a novel parallel coordinates technique to effectively represent the interplay of component alternatives in a system. It builds upon a dedicated data model that formally describes the interaction of components. Parallel coordinates can help decision-makers identify the most preferred solution among a number of alternatives. Multi-component systems require one such multi-attribute choice for each component. Each of these choices might have side effects on the system&#39;s operability and performance, making them co-dependent. Common approaches employ complex multi-component models or involve back-and-forth iterations between single components until an acceptable compromise is reached. A simultaneous visual exploration across independently modeled but connected components is needed to make system design more efficient. Using dedicated layout and interaction strategies, our Composite Parallel Coordinates allow analysts to explore both individual properties of components as well as their interoperability and joint performance. We showcase the effectiveness of Composite Parallel Coordinates for co-dependent multi-attribute choices by means of three real-world scenarios from distinct application areas. In addition to the case studies, we reflect on observing two domain experts collaboratively working with the proposed technique and communicating along the way.},
  archive      = {J_TVCG},
  author       = {Lena Cibulski and Thorsten May and Johanna Schmidt and Jörn Kohlhammer},
  doi          = {10.1109/TVCG.2022.3180899},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4047-4061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {COMPO*SED: Composite parallel coordinates for co-dependent multi-attribute choices},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ClinicalPath: A visualization tool to improve the evaluation
of electronic health records in clinical decision-making. <em>TVCG</em>,
<em>29</em>(10), 4031–4046. (<a
href="https://doi.org/10.1109/TVCG.2022.3175626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians’ daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients’ history and reduce the time spent analyzing the patient&#39;s data. However, better tools to support decision-making are still needed. In this article, we propose ClinicalPath, a visualization tool for users to track a patient&#39;s clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient&#39;s data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician&#39;s daily activities. Our results show that our proposed system improves the physicians’ experience in decision-making tasks, made with more confidence and better usage of the physicians’ time, allowing them to take other needed care for the patients.},
  archive      = {J_TVCG},
  author       = {Claudio D. G. Linhares and Daniel M. Lima and Jean R. Ponciano and Mauro M. Olivatto and Marco A. Gutierrez and Jorge Poco and Caetano Traina and Agma J. M. Traina},
  doi          = {10.1109/TVCG.2022.3175626},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4031-4046},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ClinicalPath: A visualization tool to improve the evaluation of electronic health records in clinical decision-making},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MUSE: Visual analysis of musical semantic sequence.
<em>TVCG</em>, <em>29</em>(9), 4015–4030. (<a
href="https://doi.org/10.1109/TVCG.2022.3175364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization has the capacity of converting auditory perceptions of music into visual perceptions, which consequently opens the door to music visualization (e.g., exploring group style transitions and analyzing performance details). Current research either focuses on low-level analysis without constructing and comparing music group characteristics, or concentrates on high-level group analysis without analyzing and exploring detailed information. To fill this gap, integrating the high-level group analysis and low-level details exploration of music, we design a musical semantic sequence visualization analytics prototype system (MUSE) that mainly combines a distribution view and a semantic detail view, assisting analysts in obtaining the group characteristics and detailed interpretation. In the MUSE, we decompose the music into note sequences for modeling and abstracting music into three progressively fine-grained pieces of information (i.e., genres, instruments and notes). The distribution view integrates a new density contour, which considers sequence distance and semantic similarity, and helps analysts quickly identify the distribution features of the music group. The semantic detail view displays the music note sequences and combines the window moving to avoid visual clutter while ensuring the presentation of complete semantic details. To prove the usefulness and effectiveness of MUSE, we perform two case studies based on real-world music MIDI data. In addition, we conduct a quantitative user study and an expert evaluation.},
  archive      = {J_TVCG},
  author       = {Baofeng Chang and Guodao Sun and Tong Li and Houchao Huang and Ronghua Liang},
  doi          = {10.1109/TVCG.2022.3175364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {4015-4030},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MUSE: Visual analysis of musical semantic sequence},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale flow-based occluding effect and content
separation for cartoon animations. <em>TVCG</em>, <em>29</em>(9),
4001–4014. (<a href="https://doi.org/10.1109/TVCG.2022.3174656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluding effects have been frequently used to present weather conditions and environments in cartoon animations, such as raining, snowing, moving leaves, and moving petals. While these effects greatly enrich the visual appeal of the cartoon animations, they may also cause undesired occlusions on the content area, which significantly complicate the analysis and processing of the cartoon animations. In this article, we make the first attempt to separate the occluding effects and content for cartoon animations. The major challenge of this problem is that, unlike natural effects that are realistic and small-sized, the effects of cartoons are usually stylistic and large-sized. Besides, effects in cartoons are manually drawn, so their motions are more unpredictable than realistic effects. To separate occluding effects and content for cartoon animations, we propose to leverage the difference in the motion patterns of the effects and the content, and capture the locations of the effects based on a multi-scale flow-based effect prediction (MFEP) module. A dual-task learning system is designed to extract the effect video and reconstruct the effect-removed content video at the same time. We apply our method on a large number of cartoon videos of different content and effects. Experiments show that our method significantly outperforms the existing methods. We further demonstrate how the separated effects and content facilitate the analysis and processing of cartoon videos through different applications, including segmentation, inpainting, and effect migration.},
  archive      = {J_TVCG},
  author       = {Cheng Xu and Wei Qu and Xuemiao Xu and Xueting Liu},
  doi          = {10.1109/TVCG.2022.3174656},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {4001-4014},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-scale flow-based occluding effect and content separation for cartoon animations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Good keyframes to inpaint. <em>TVCG</em>, <em>29</em>(9),
3989–4000. (<a href="https://doi.org/10.1109/TVCG.2022.3176958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diminished Reality (DR) propagates pixels from a keyframe to subsequent frames for real-time inpainting. Keyframe selection has a significant impact on the inpainting quality, but untrained users struggle to identify good keyframes. Automatic selection is not straightforward either, since no previous work has formalized or verified what determines a good keyframe. We propose a novel metric to select good keyframes to inpaint . We examine the heuristics adopted in existing DR inpainting approaches and derive multiple simple criteria measurable from SLAM. To combine these criteria, we empirically analyze their effect on the quality using a novel representative test dataset. Our results demonstrate that the combined metric selects RGBD keyframes leading to high-quality inpainting results more often than a baseline approach in both color and depth domains. Also, we confirmed that our approach has a better ranking ability of distinguishing good and bad keyframes. Compared to random selections, our metric selects keyframes that would lead to higher-quality and more stably converging inpainting results. We present three DR examples, automatic keyframe selection, user navigation, and marker hiding.},
  archive      = {J_TVCG},
  author       = {Shohei Mori and Dieter Schmalstieg and Denis Kalkofen},
  doi          = {10.1109/TVCG.2022.3176958},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3989-4000},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Good keyframes to inpaint},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Casual 6-DoF: Free-viewpoint panorama using a handheld 360°
camera. <em>TVCG</em>, <em>29</em>(9), 3976–3988. (<a
href="https://doi.org/10.1109/TVCG.2022.3176832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Six degrees-of-freedom (6-DoF) video provides telepresence by enabling users to move around in the captured scene with a wide field of regard. Compared to methods requiring sophisticated camera setups, the image-based rendering method based on photogrammetry can work with images captured with any poses, which is more suitable for casual users. However, existing image-based rendering methods are based on perspective images. When used to reconstruct 6-DoF views, it often requires capturing hundreds of images, making data capture a tedious and time-consuming process. In contrast to traditional perspective images, 360° images capture the entire surrounding view in a single shot, thus, providing a faster capturing process for 6-DoF view reconstruction. This article presents a novel method to provide 6-DoF experiences over a wide area using an unstructured collection of 360° panoramas captured by a conventional 360° camera. Our method consists of 360° data capturing, novel depth estimation to produce a high-quality spherical depth panorama, and high-fidelity free-viewpoint generation. We compared our method against state-of-the-art methods, using data captured in various environments. Our method shows better visual quality and robustness in the tested scenes.},
  archive      = {J_TVCG},
  author       = {Rongsen Chen and Fang-Lue Zhang and Simon Finnie and Andrew Chalmers and Taehyun Rhee},
  doi          = {10.1109/TVCG.2022.3176832},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3976-3988},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Casual 6-DoF: Free-viewpoint panorama using a handheld 360° camera},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using extended reality in flight simulators: A literature
review. <em>TVCG</em>, <em>29</em>(9), 3961–3975. (<a
href="https://doi.org/10.1109/TVCG.2022.3173921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended reality has long been utilized in the games industry and is emergent for pilot training in the military and commercial airline sectors. This paper follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to present a systematic quantitative literature review (SQLR) on the use of extended reality in flight simulators. It also encompasses recent studies of teaching and learning in immersive, virtual environments in non-aviation disciplines. The review identified 39 papers spanning all areas of the virtuality continuum across academic, commercial, and military aviation sectors, as well as engineering and medicine. The SQLR found that extended reality in flight simulators is being introduced in the commercial and military aviation sectors. However, within academia, hardware constraints have hindered the ability to provide positive empirical evidence of simulator effectiveness. While virtual reality may not replace traditional flight simulators in the near future, the technology is available to supplement classroom training activities and some aspects of simulator procedure training with promising cognitive learning outcomes. However, its usefulness as a mechanism of skills transfer to the real world has not been evaluated, highlighting numerous research opportunities.},
  archive      = {J_TVCG},
  author       = {Jamie Cross and Christine Boag-Hodgson and Tim Ryley and Timothy J Mavin and Leigh Ellen Potter},
  doi          = {10.1109/TVCG.2022.3173921},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3961-3975},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using extended reality in flight simulators: A literature review},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practice improves performance of a 2D uncertainty
integration task within and across visualizations. <em>TVCG</em>,
<em>29</em>(9), 3949–3960. (<a
href="https://doi.org/10.1109/TVCG.2022.3173889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information uncertainty is ubiquitous in everyday life, including in domains as diverse as weather forecasts, investments, and health risks. Knowing how to interpret and integrate this uncertain information is vital for making good decisions, but this can be difficult for experts and novices alike. In this study, we examine whether brief, focused practice can improve people&#39;s ability to understand and integrate bivariate Gaussian uncertainty visualized via ensemble displays, summary displays, and distributional displays, and we examine whether this is influenced by the complexity of the displayed information. In two experiments (N=118 and 56), decision making was faster and more accurate after practice relative to before practice. Furthermore, the performance improvements transferred to use of display types that were not practiced. This suggests that practice with feedback may improve underlying skills in probabilistic reasoning and provides a promising approach to improve people&#39;s decision making under uncertainty.},
  archive      = {J_TVCG},
  author       = {Sarah A. Kusumastuti and Kimberly A. Pollard and Ashley H. Oiknine and Bianca Dalangin and Tiffany R. Raber and Benjamin T. Files},
  doi          = {10.1109/TVCG.2022.3173889},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3949-3960},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Practice improves performance of a 2D uncertainty integration task within and across visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual data exploration as a statistical testing procedure:
Within-view and between-view multiple comparisons. <em>TVCG</em>,
<em>29</em>(9), 3937–3948. (<a
href="https://doi.org/10.1109/TVCG.2022.3175532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental problem in visual data exploration concerns whether observed patterns are true or merely random noise. This problem is especially pertinent in visual analytics, where the user is presented with a barrage of patterns, without any guarantees of their statistical validity. Recently this problem has been formulated in terms of statistical testing and the multiple comparisons problem. In this paper, we identify two levels of multiple comparisons problems in visualization: the within-view and the between-view problem. We develop a statistical testing procedure for interactive data exploration that controls the family-wise error rate on both levels. The procedure enables the user to determine the compatibility of their assumptions about the data with visually observed patterns. We present use-cases where we visualize and evaluate patterns in real-world data.},
  archive      = {J_TVCG},
  author       = {Rafael Savvides and Andreas Henelius and Emilia Oikarinen and Kai Puolamäki},
  doi          = {10.1109/TVCG.2022.3175532},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3937-3948},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual data exploration as a statistical testing procedure: Within-view and between-view multiple comparisons},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design order guided visual note layout optimization.
<em>TVCG</em>, <em>29</em>(9), 3922–3936. (<a
href="https://doi.org/10.1109/TVCG.2022.3171839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the goal of making contents easy to understand, memorize and share, a clear and easy-to-follow layout is important for visual notes. Unfortunately, since visual notes are often taken by the designers in real time while watching a video or listening to a presentation, the contents are usually not carefully structured, resulting in layouts that may be difficult for others to follow. In this article, we address this problem by proposing a novel approach to automatically optimize the layouts of visual notes. Our approach predicts the design order of a visual note and then warps the contents along the predicted design order such that the visual note can be easier to follow and understand. At the core of our approach is a learning-based framework to reason about the element-wise design orders of visual notes. In particular, we first propose a hierarchical LSTM-based architecture to predict a grid-based design order of the visual note, based on the graphical and textual information. We then derive the element-wise order from the grid-based prediction. Such an idea allows our network to be weakly-supervised, i.e., making it possible to predict dense grid-based orders from visual notes with only coarse annotations. We evaluate the effectiveness of our approach on visual notes with diverse content densities and layouts. The results show that our network can predict plausible design orders for various types of visual notes and our approach can effectively optimize their layouts in order for them to be easier to follow.},
  archive      = {J_TVCG},
  author       = {Xiaotian Qiao and Ying Cao and Rynson W. H. Lau},
  doi          = {10.1109/TVCG.2022.3171839},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3922-3936},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design order guided visual note layout optimization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of interaction techniques for immersive
environments. <em>TVCG</em>, <em>29</em>(9), 3900–3921. (<a
href="https://doi.org/10.1109/TVCG.2022.3174805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent proliferation of immersive technology has led to the rapid adoption of consumer-ready hardware for Augmented Reality (AR) and Virtual Reality (VR). While this increase has resulted in a variety of platforms that can offer a richer interactive experience, the advances in technology bring more variability in display types, interaction sensors and use cases. This provides a spectrum of device-specific interaction possibilities, with each offering a tailor-made solution for delivering immersive experiences to users, but often with an inherent lack of standardisation across devices and applications. To address this, a systematic review and an evaluation of explicit, task-based interaction methods in immersive environments are presented in this paper. A corpus of papers published between 2013 and 2020 is reviewed to thoroughly explore state-of-the-art user studies, which investigate input methods and their implementation for immersive interaction tasks (pointing, selection, translation, rotation, scale, viewport, menu-based and abstract). Focus is given to how input methods have been applied within the spectrum of immersive technology (AR, VR, XR). This is achieved by categorising findings based on display type, input method, study type, use case and task. Results illustrate key trends surrounding the benefits and limitations of each interaction technique and highlight the gaps in current research. The review provides a foundation for understanding the current and future directions for interaction studies in immersive environments, which, at this pivotal point in XR technology adoption, provides routes forward for achieving more valuable, intuitive and natural interactive experiences.},
  archive      = {J_TVCG},
  author       = {Becky Spittle and Maite Frutos-Pascual and Chris Creed and Ian Williams},
  doi          = {10.1109/TVCG.2022.3174805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3900-3921},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A review of interaction techniques for immersive environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Watertight incremental heightfield tessellation.
<em>TVCG</em>, <em>29</em>(9), 3888–3899. (<a
href="https://doi.org/10.1109/TVCG.2022.3173081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.},
  archive      = {J_TVCG},
  author       = {Daniel Cornel and Silvana Zechmeister and Eduard Gröller and Jürgen Waser},
  doi          = {10.1109/TVCG.2022.3173081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3888-3899},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Watertight incremental heightfield tessellation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A GPU parallel algorithm for computing morse-smale
complexes. <em>TVCG</em>, <em>29</em>(9), 3873–3887. (<a
href="https://doi.org/10.1109/TVCG.2022.3174769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Morse-Smale complex is a well studied topological structure that represents the gradient flow behavior between critical points of a scalar function. It supports multi-scale topological analysis and visualization of feature-rich scientific data. Several parallel algorithms have been proposed towards the fast computation of the 3D Morse-Smale complex. Its computation continues to pose significant algorithmic challenges. In particular, the non-trivial structure of the connections between the saddle critical points are not amenable to parallel computation. This paper describes a fine grained parallel algorithm for computing the Morse-Smale complex and a GPU implementation (g msc ). The algorithm first determines the saddle-saddle reachability via a transformation into a sequence of vector operations, and next computes the paths between saddles by transforming it into a sequence of matrix operations. Computational experiments show that the method achieves up to 8.6× speedup over pyms3d and 6× speedup over TTK, the current shared memory implementations. The paper also presents a comprehensive experimental analysis of different steps of the algorithm and reports on their contribution towards runtime performance. Finally, it introduces a CPU based data parallel algorithm for simplifying the Morse-Smale complex via iterative critical point pair cancellation.},
  archive      = {J_TVCG},
  author       = {Varshini Subhash and Karran Pandey and Vijay Natarajan},
  doi          = {10.1109/TVCG.2022.3174769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3873-3887},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A GPU parallel algorithm for computing morse-smale complexes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncover: Toward interpretable models for detecting new star
cluster members. <em>TVCG</em>, <em>29</em>(9), 3855–3872. (<a
href="https://doi.org/10.1109/TVCG.2022.3172560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this design study, we present Uncover, an interactive tool aimed at astronomers to find previously unidentified member stars in stellar clusters. We contribute data and task abstraction in the domain of astronomy and provide an approach for the non-trivial challenge of finding a suitable hyper-parameter set for highly flexible novelty detection models. We achieve this by substituting the tedious manual trial and error process, which usually results in finding a small subset of passable models with a five-step workflow approach. We utilize ranges of a priori defined, interpretable summary statistics models have to adhere to. Our goal is to enable astronomers to use their domain expertise to quantify model goodness effectively. We attempt to change the current culture of blindly accepting a machine learning model to one where astronomers build and modify a model based on their expertise. We evaluate the tools’ usability and usefulness in a series of interviews with domain experts.},
  archive      = {J_TVCG},
  author       = {Sebastian Ratzenböck and Verena Obermüller and Torsten Möller and João Alves and Immanuel M. Bomze},
  doi          = {10.1109/TVCG.2022.3172560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3855-3872},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncover: Toward interpretable models for detecting new star cluster members},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank-PointRetrieval: Reranking point cloud retrieval via a
visually consistent registration evaluation. <em>TVCG</em>,
<em>29</em>(9), 3840–3854. (<a
href="https://doi.org/10.1109/TVCG.2022.3170695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud-based place recognition is a fundamental part of the localization task, and it can be achieved through a retrieval process. Reranking is a critical step in improving the retrieval accuracy, yet little effort has been devoted to reranking in point cloud retrieval. In this paper, we investigate the versatility of rigid registration in reranking the point cloud retrieval results. Specifically, after obtaining the initial retrieval list based on the global point cloud feature distance, we perform registration between the query and point clouds in the retrieval list. We propose an efficient strategy based on visual consistency to evaluate each registration with a registration score in an unsupervised manner. The final reranked list is computed by considering both the original global feature distance and the registration score. In addition, we find that the registration score between two point clouds can also be used as a pseudo label to judge whether they represent the same place. Thus, we can create a self-supervised training dataset when there is no ground truth of positional information. Moreover, we develop a new probability-based loss to obtain more discriminative descriptors. The proposed reranking approach and the probability-based loss can be easily applied to current point cloud retrieval baselines to improve the retrieval accuracy. Experiments on various benchmark datasets show that both the reranking registration method and probability-based loss can significantly improve the current state-of-the-art baselines.},
  archive      = {J_TVCG},
  author       = {Wenxiao Zhang and Huajian Zhou and Zhen Dong and Qingan Yan and Chunxia Xiao},
  doi          = {10.1109/TVCG.2022.3170695},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3840-3854},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rank-PointRetrieval: Reranking point cloud retrieval via a visually consistent registration evaluation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sketch2PQ: Freeform planar quadrilateral mesh design via a
single sketch. <em>TVCG</em>, <em>29</em>(9), 3826–3839. (<a
href="https://doi.org/10.1109/TVCG.2022.3170853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The freeform architectural modeling process often involves two important stages: concept design and digital modeling. In the first stage, architects usually sketch the overall 3D shape and the panel layout on a physical or digital paper briefly. In the second stage, a digital 3D model is created using the sketch as a reference. The digital model needs to incorporate geometric requirements for its components, such as the planarity of panels due to consideration of construction costs, which can make the modeling process more challenging. In this work, we present a novel sketch-based system to bridge the concept design and digital modeling of freeform roof-like shapes represented as planar quadrilateral (PQ) meshes. Our system allows the user to sketch the surface boundary and contour lines under axonometric projection and supports the sketching of occluded regions. In addition, the user can sketch feature lines to provide directional guidance to the PQ mesh layout. Given the 2D sketch input, we propose a deep neural network to infer in real-time the underlying surface shape along with a dense conjugate direction field, both of which are used to extract the final PQ mesh. To train and validate our network, we generate a large synthetic dataset that mimics architect sketching of freeform quadrilateral patches. The effectiveness and usability of our system are demonstrated with quantitative and qualitative evaluation as well as user studies.},
  archive      = {J_TVCG},
  author       = {Zhi Deng and Yang Liu and Hao Pan and Wassim Jabi and Juyong Zhang and Bailin Deng},
  doi          = {10.1109/TVCG.2022.3170853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3826-3839},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch2PQ: Freeform planar quadrilateral mesh design via a single sketch},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning-from-disagreement: A model comparison and visual
analytics framework. <em>TVCG</em>, <em>29</em>(9), 3809–3825. (<a
href="https://doi.org/10.1109/TVCG.2022.3172107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast-growing number of classification models being produced every day, numerous model interpretation and comparison solutions have also been introduced. For example, LIME [1] and SHAP [2] can interpret what input features contribute more to a classifier’s output predictions. Different numerical metrics (e.g., accuracy) can be used to easily compare two classifiers. However, few works can interpret the contribution of a data feature to a classifier in comparison with its contribution to another classifier. This comparative interpretation can help to disclose the fundamental difference between two classifiers, select classifiers in different feature conditions, and better ensemble two classifiers. To accomplish it, we propose a learning-from-disagreement (LFD) framework to visually compare two classification models. Specifically, LFD identifies data instances with disagreed predictions from two compared classifiers and trains a discriminator to learn from the disagreed instances. As the two classifiers’ training features may not be available, we train the discriminator through a set of meta-features proposed based on certain hypotheses of the classifiers to probe their behaviors. Interpreting the trained discriminator with the SHAP values of different meta-features, we provide actionable insights into the compared classifiers. Also, we introduce multiple metrics to profile the importance of meta-features from different perspectives. With these metrics, one can easily identify meta-features with the most complementary behaviors in two classifiers, and use them to better ensemble the classifiers. We focus on binary classification models in the financial services and advertising industry to demonstrate the efficacy of our proposed framework and visualizations.},
  archive      = {J_TVCG},
  author       = {Junpeng Wang and Liang Wang and Yan Zheng and Chin-Chia Michael Yeh and Shubham Jain and Wei Zhang},
  doi          = {10.1109/TVCG.2022.3172107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3809-3825},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning-from-disagreement: A model comparison and visual analytics framework},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based intrinsic reflectional symmetry detection.
<em>TVCG</em>, <em>29</em>(9), 3799–3808. (<a
href="https://doi.org/10.1109/TVCG.2022.3172361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflectional symmetry is a ubiquitous pattern in nature. Previous works usually solve this problem by voting or sampling, suffering from high computational cost and randomness. In this article, we propose a learning-based approach to intrinsic reflectional symmetry detection. Instead of directly finding symmetric point pairs, we parametrize this self-isometry using a functional map matrix, which can be easily computed given the signs of Laplacian eigenfunctions under the symmetric mapping. Therefore, we manually label the eigenfunction signs for a variety of shapes and train a novel neural network to predict the sign of each eigenfunction under symmetry. Our network aims at learning the global property of functions and consequently converts the problem defined on the manifold to the functional domain. By disentangling the prediction of the matrix into separated bases, our method generalizes well to new shapes and is invariant under perturbation of eigenfunctions. Through extensive experiments, we demonstrate the robustness of our method in challenging cases, including different topology and incomplete shapes with holes. By avoiding random sampling, our learning-based algorithm is over 20 times faster than state-of-the-art methods, and meanwhile, is more robust, achieving higher correspondence accuracy in commonly used metrics.},
  archive      = {J_TVCG},
  author       = {Yi-Ling Qiao and Lin Gao and Shu-Zhi Liu and Ligang Liu and Yu-Kun Lai and Xilin Chen},
  doi          = {10.1109/TVCG.2022.3172361},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3799-3808},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning-based intrinsic reflectional symmetry detection},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid in situ approach for cost efficient image database
generation. <em>TVCG</em>, <em>29</em>(9), 3788–3798. (<a
href="https://doi.org/10.1109/TVCG.2022.3169590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability , limited scalability , overhead , and rightsizing . We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19 K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.},
  archive      = {J_TVCG},
  author       = {Valentin Bruder and Matthew Larsen and Thomas Ertl and Hank Childs and Steffen Frey},
  doi          = {10.1109/TVCG.2022.3169590},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3788-3798},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A hybrid in situ approach for cost efficient image database generation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphical enhancements for effective exemplar identification
in contextual data visualizations. <em>TVCG</em>, <em>29</em>(9),
3775–3787. (<a href="https://doi.org/10.1109/TVCG.2022.3170531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets. We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.},
  archive      = {J_TVCG},
  author       = {Xinyu Zhang and Shenghui Cheng and Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3170531},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3775-3787},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Graphical enhancements for effective exemplar identification in contextual data visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding how in-visualization provenance can support
trade-off analysis. <em>TVCG</em>, <em>29</em>(9), 3758–3774. (<a
href="https://doi.org/10.1109/TVCG.2022.3171074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In domains, such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit. In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. We implemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to help experts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore the potential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we make the following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating and characterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second, we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of these purposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches). Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both when revisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, for example related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenarios.},
  archive      = {J_TVCG},
  author       = {Mehdi Chakhchoukh and Nadia Boukhelifa and Anastasia Bezerianos},
  doi          = {10.1109/TVCG.2022.3171074},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3758-3774},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding how in-visualization provenance can support trade-off analysis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communicating uncertainty and risk in air quality maps.
<em>TVCG</em>, <em>29</em>(9), 3746–3757. (<a
href="https://doi.org/10.1109/TVCG.2022.3171443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental sensors provide crucial data for understanding our surroundings. For example, air quality maps based on sensor readings help users make decisions to mitigate the effects of pollution on their health. Standard maps show readings from individual sensors or colored contours indicating estimated pollution levels. However, showing a single estimate may conceal uncertainty and lead to underestimation of risk, while showing sensor data yields varied interpretations. We present several visualizations of uncertainty in air quality maps, including a frequency-framing “dotmap” and small multiples, and we compare them with standard contour and sensor-based maps. In a user study, we find that including uncertainty in maps has a significant effect on how much users would choose to reduce physical activity, and that people make more cautious decisions when using uncertainty-aware maps. Additionally, we analyze think-aloud transcriptions from the experiment to understand more about how the representation of uncertainty influences people’s decision-making. Our results suggest ways to design maps of sensor data that can encourage certain types of reasoning, yield more consistent responses, and convey risk better than standard maps.},
  archive      = {J_TVCG},
  author       = {Annie Preston and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3171443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3746-3757},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Communicating uncertainty and risk in air quality maps},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How augmented reality (AR) can help and hinder collaborative
learning: A study of AR in electromagnetism education. <em>TVCG</em>,
<em>29</em>(9), 3734–3745. (<a
href="https://doi.org/10.1109/TVCG.2022.3169980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning physics is often difficult for students because concepts such as electricity, magnetism and sound, cannot be seen with the naked eye. Emerging technologies such as Augmented Reality (AR) can transform education by making challenging concepts visible and accessible to novices. We present a Hololens-based augmented reality system where collaborators learn about the invisible electromagnetism phenomena involved in audio speakers, and we measure the benefits of AR technology through quantitative and qualitative methods. Specifically, we measure learning (knowledge gains and transfer) and collaborative knowledge exchange behaviors. Our results indicate that, while AR generally provides a novelty effect, specific educational AR visualizations can be both beneficial and detrimental to learning – they helped students to learn spatial content and structural relationships, but hindered their understanding of kinesthetic content. Furthermore, AR facilitated learning in collaborations by providing representational common ground, which improved communication and peer teaching. We discuss these effects, as well as identify factors that have positive impact (e.g., co-located representations, easier access to resources, better grounding) or negative impact (e.g., tunnel vision, overlooking kinesthetic feedback) on student collaborative learning with augmented reality applications.},
  archive      = {J_TVCG},
  author       = {Iulian Radu and Bertrand Schneider},
  doi          = {10.1109/TVCG.2022.3169980},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3734-3745},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How augmented reality (AR) can help and hinder collaborative learning: A study of AR in electromagnetism education},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DL4SciVis: A state-of-the-art survey on deep learning for
scientific visualization. <em>TVCG</em>, <em>29</em>(8), 3714–3733. (<a
href="https://doi.org/10.1109/TVCG.2022.3167896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey articles on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this article, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The article concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.},
  archive      = {J_TVCG},
  author       = {Chaoli Wang and Jun Han},
  doi          = {10.1109/TVCG.2022.3167896},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3714-3733},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DL4SciVis: A state-of-the-art survey on deep learning for scientific visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How immersion and self-avatars in VR affect learning
programming and computational thinking in middle school education.
<em>TVCG</em>, <em>29</em>(8), 3698–3713. (<a
href="https://doi.org/10.1109/TVCG.2022.3169426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an empirical evaluation of immersion and self-avatars as compared to desktop viewing in Virtual Reality (VR) for learning computer programming and computational thinking in middle school education using an educational VR simulation. Students were asked to programmatically choreograph dance performances for virtual characters within an educational desktop application we built earlier called Virtual Environment Interactions (VEnvI). As part of a middle school science class, 90 students from the 6th and 7th grades participated in our study. All students first visually programmed dance choreography for a virtual character they created in VEnvI on a laptop. Then, they viewed and interacted with the resulting dance performance in a between-subjects design in one of the three conditions. We compared and contrasted the benefits of embodied immersive virtual reality (EVR) viewing utilizing a head-mounted display with a body-scaled and gender-matched self-avatar, immersive virtual reality only (IVR) viewing, and desktop VR (NVR) viewing with VEnvI on pedagogical outcomes, programming performance, presence, and attitudes towards STEM and computational thinking. Results from a cognition questionnaire showed that, in the learning dimensions of Knowledge and Understanding (Bloom&#39;s taxonomy) as well as Multistructural (SOLO taxonomy), participants in EVR and IVR scored significantly higher than NVR. Also, participants in EVR scored significantly higher than IVR. We also discovered similar results in objective programming performance and presence scores in VEnvI. Furthermore, students’ attitudes towards computer science, programming confidence, and impressions significantly improved to be the highest in EVR and then IVR as compared to NVR condition. Our work suggests that educators and developers of educational VR simulations, who want to enhance knowledge and understanding as well as simultaneous acquisition of multiple abstract concepts, can do so by employing immersion and self-avatars in VR learning experiences.},
  archive      = {J_TVCG},
  author       = {Dhaval Parmar and Lorraine Lin and Nikeetha DSouza and Sophie Jörg and Alison E. Leonard and Shaundra B. Daily and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2022.3169426},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3698-3713},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How immersion and self-avatars in VR affect learning programming and computational thinking in middle school education},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GestureLens: Visual analysis of gestures in presentation
videos. <em>TVCG</em>, <em>29</em>(8), 3685–3697. (<a
href="https://doi.org/10.1109/TVCG.2022.3169175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this article, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this article, we propose GestureLens , a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.},
  archive      = {J_TVCG},
  author       = {Haipeng Zeng and Xingbo Wang and Yong Wang and Aoyu Wu and Ting-Chuen Pong and Huamin Qu},
  doi          = {10.1109/TVCG.2022.3169175},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3685-3697},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GestureLens: Visual analysis of gestures in presentation videos},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of transfer functions and body parts on body-centric
locomotion in virtual reality. <em>TVCG</em>, <em>29</em>(8), 3670–3684.
(<a href="https://doi.org/10.1109/TVCG.2022.3169222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body-centric locomotion allows users to control both movement speed and direction with body parts (e.g., head tilt, arm swing or torso lean) to navigate in virtual reality (VR). However, there is little research to systematically investigate the effects of body parts for speed and direction control on virtual locomotion by taking in account different transfer functions(L: linear function, P: power function, and CL: piecewise function with constant and linear function). Therefore, we conducted an experiment to evaluate the combinational effects of the three factors (body parts for direction control, body parts for speed control, and transfer functions) on virtual locomotion. Results showed that (1) the head outperformed the torso for movement direction control in task completion time and environmental collisions; (2) Arm-based speed control led to shorter traveled distances than both head and knee. Head-based speed control had fewer environmental collisions than knee; (3) Body-centric locomotion with CL function was faster but less accurate than both L and P functions. Task time significantly decreased from P, L to CL functions, while traveled distance and overshoot significantly increased from P, L to CL functions. L function was rated with the highest score of USE-S , -pragmatic and -hedonic ; (4) Transfer function had a significant main effect on motion sickness: the participants felt more headache and nausea when performing locomotion with CL function. Our results provide implications for body-centric locomotion design in VR applications.},
  archive      = {J_TVCG},
  author       = {BoYu Gao and Zijun Mai and Huawei Tu and Henry Been-Lirn Duh},
  doi          = {10.1109/TVCG.2022.3169222},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3670-3684},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of transfer functions and body parts on body-centric locomotion in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting subjective discomfort associated with lens
distortion in VR headsets during vestibulo-ocular response to VR scenes.
<em>TVCG</em>, <em>29</em>(8), 3656–3669. (<a
href="https://doi.org/10.1109/TVCG.2022.3168190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in Virtual Reality (VR) technology, user expectation for a near-perfect experience is also increasing. The push for a wider field-of-view can increase the challenges of correcting lens distortion. Past studies on imperfect VR experiences have focused on motion sickness provoked by vection-inducing VR stimuli and discomfort due to mismatches in accommodation and binocular convergence. Disorientation and discomfort due to unintended optical flow induced by lens distortion, referred to as dynamic distortion (DD), has, to date, received little attention. This study examines and models the effects of DD during head rotations with various fixed gazes stabilized by vestibulo-ocular reflex (VOR). Increases in DD levels comparable to lens parameters from poorly designed commercial VR lenses significantly increase discomfort scores of viewers in relation to disorientation, dizziness, and eye strain. Cross-validated results indicate that the model is able to predict significant differences in subjective scores resulting from different commercial VR lenses and these predictions correlated with empirical data. The present work provides new insights to understand symptoms of discomfort in VR during user interactions with static world-locked / space-stabilized scenes and contributes to the design of discomfort-free VR headset lenses.},
  archive      = {J_TVCG},
  author       = {Tsz Tai Chan and Yixuan Wang and Richard Hau Yue So and Jerry Jia},
  doi          = {10.1109/TVCG.2022.3168190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3656-3669},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Predicting subjective discomfort associated with lens distortion in VR headsets during vestibulo-ocular response to VR scenes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceptual quality assessment of colored 3D point clouds.
<em>TVCG</em>, <em>29</em>(8), 3642–3655. (<a
href="https://doi.org/10.1109/TVCG.2022.3167151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds have found a wide variety of applications in multimedia processing, remote sensing, and scientific computing. Although most point cloud processing systems are developed to improve viewer experiences, little work has been dedicated to perceptual quality assessment of 3D point clouds. In this work, we build a new 3D point cloud database, namely the Waterloo Point Cloud (WPC) database. In contrast to existing datasets consisting of small-scale and low-quality source content of constrained viewing angles, the WPC database contains 20 high quality, realistic, and omni-directional source point clouds and 740 diversely distorted point clouds. We carry out a subjective quality assessment experiment over the database in a controlled lab environment. Our statistical analysis suggests that existing objective point cloud quality assessment (PCQA) models only achieve limited success in predicting subjective quality ratings. We propose a novel objective PCQA model based on an attention mechanism and a variant of information content-weighted structural similarity, which significantly outperforms existing PCQA models. The database has been made publicly available at https://github.com/qdushl/Waterloo-Point-Cloud-Database .},
  archive      = {J_TVCG},
  author       = {Qi Liu and Honglei Su and Zhengfang Duanmu and Wentao Liu and Zhou Wang},
  doi          = {10.1109/TVCG.2022.3167151},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3642-3655},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual quality assessment of colored 3D point clouds},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantically disentangled variational autoencoder for
modeling 3D facial details. <em>TVCG</em>, <em>29</em>(8), 3630–3641.
(<a href="https://doi.org/10.1109/TVCG.2022.3166666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric face models, such as morphable and blendshape models, have shown great potential in face representation, reconstruction, and animation. However, all these models focus on large-scale facial geometry. Facial details such as wrinkles are not parameterized in these models, impeding accuracy and realism. In this article, we propose a method to learn a Semantically Disentangled Variational Autoencoder (SDVAE) to parameterize facial details and support independent detail manipulation as an extension of an off-the-shelf large-scale face model. Our method utilizes the non-linear capability of Deep Neural Networks for detail modeling, achieving better accuracy and greater representation power compared with linear models. In order to disentangle the semantic factors of identity, expression and age, we propose to eliminate the correlation between different factors in an adversarial manner. Therefore, wrinkle-level details of various identities, expressions, and ages can be generated and independently controlled by changing latent vectors of our SDVAE. We further leverage our model to reconstruct 3D faces via fitting to facial scans and images. Benefiting from our parametric model, we achieve accurate and robust reconstruction, and the reconstructed details can be easily animated and manipulated. We evaluate our method on practical applications, including scan fitting, image fitting, video tracking, model manipulation, and expression and age animation. Extensive experiments demonstrate that the proposed method can robustly model facial details and achieve better results than alternative methods.},
  archive      = {J_TVCG},
  author       = {Jingwang Ling and Zhibo Wang and Ming Lu and Quan Wang and Chen Qian and Feng Xu},
  doi          = {10.1109/TVCG.2022.3166666},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3630-3641},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantically disentangled variational autoencoder for modeling 3D facial details},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OrthoAligner: Image-based teeth alignment prediction via
latent style manipulation. <em>TVCG</em>, <em>29</em>(8), 3617–3629. (<a
href="https://doi.org/10.1109/TVCG.2022.3166159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present OrthoAligner, a novel method to predict the visual outcome of orthodontic treatment in a portrait image. Unlike the state-of-the-art method, which relies on a 3D teeth model obtained from dental scanning, our method generates realistic alignment effects in images without requiring additional 3D information as input and thus making our system readily available to average users. The key of our approach is to employ the 3D geometric information encoded in an unsupervised generative model, i.e., StyleGAN in this article. Instead of directly conducting translation in the image space, we embed the teeth region extracted from a given portrait to the latent space of the StyleGAN generator and propose a novel latent editing method to discover a geometrically meaningful editing path that yields the alignment process in the image space. To blend the edited mouth region with the original portrait image, we further introduce a BlendingNet to remove boundary artifacts and correct color inconsistency. We also extend our method to short video clips by propagating the alignment effects across neighboring frames. We evaluate our method in various orthodontic cases, compare it to the state-of-the-art and competitive baselines, and validate the effectiveness of each component.},
  archive      = {J_TVCG},
  author       = {Beijia Chen and Hongbo Fu and Kun Zhou and Youyi Zheng},
  doi          = {10.1109/TVCG.2022.3166159},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3617-3629},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OrthoAligner: Image-based teeth alignment prediction via latent style manipulation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual assistance in development and validation of bayesian
networks for clinical decision support. <em>TVCG</em>, <em>29</em>(8),
3602–3616. (<a href="https://doi.org/10.1109/TVCG.2022.3166071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts’ needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.},
  archive      = {J_TVCG},
  author       = {Juliane Müller-Sielaff and Seyed Behnam Beladi and Stephanie W. Vrede and Monique Meuschke and Peter J. F. Lucas and Johanna M. A. Pijnenborg and Steffen Oeltze-Jafra},
  doi          = {10.1109/TVCG.2022.3166071},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3602-3616},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual assistance in development and validation of bayesian networks for clinical decision support},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EpiMob: Interactive visual analytics of citywide human
mobility restrictions for epidemic control. <em>TVCG</em>,
<em>29</em>(8), 3586–3601. (<a
href="https://doi.org/10.1109/TVCG.2022.3165385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.},
  archive      = {J_TVCG},
  author       = {Chuang Yang and Zhiwen Zhang and Zipei Fan and Renhe Jiang and Quanjun Chen and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TVCG.2022.3165385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3586-3601},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EpiMob: Interactive visual analytics of citywide human mobility restrictions for epidemic control},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SD2: Slicing and dicing scholarly data for interactive
evaluation of academic performance. <em>TVCG</em>, <em>29</em>(8),
3569–3585. (<a href="https://doi.org/10.1109/TVCG.2022.3163727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensively evaluating and comparing researchers’ academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this article, we present an interactive visualization framework, SD $^{2}$ , to enable flexible data partition and composition to support various analysis requirements within a single system. SD $^{2}$ features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD $^{2}$ and revise the design and system implementation accordingly. The effectiveness of SD $^{2}$ is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.},
  archive      = {J_TVCG},
  author       = {Zhichun Guo and Jun Tao and Siming Chen and Nitesh V. Chawla and Chaoli Wang},
  doi          = {10.1109/TVCG.2022.3163727},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3569-3585},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SD2: Slicing and dicing scholarly data for interactive evaluation of academic performance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A model for types and levels of automation in visual
analytics: A survey, a taxonomy, and examples. <em>TVCG</em>,
<em>29</em>(8), 3550–3568. (<a
href="https://doi.org/10.1109/TVCG.2022.3163765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous growth in availability and access to data presents a major challenge to the human analyst. As the manual analysis of large and complex datasets is nowadays practically impossible, the need for assisting tools that can automate the analysis process while keeping the human analyst in the loop is imperative. A large and growing body of literature recognizes the crucial role of automation in Visual Analytics and suggests that automation is among the most important constituents for effective Visual Analytics systems. Today, however, there is no appropriate taxonomy nor terminology for assessing the extent of automation in a Visual Analytics system. In this article, we aim to address this gap by introducing a model of levels of automation tailored for the Visual Analytics domain. The consistent terminology of the proposed taxonomy could provide a ground for users/readers/reviewers to describe and compare automation in Visual Analytics systems. Our taxonomy is grounded on a combination of several existing and well-established taxonomies of levels of automation in the human-machine interaction domain and relevant models within the visual analytics field. To exemplify the proposed taxonomy, we selected a set of existing systems from the event-sequence analytics domain and mapped the automation of their visual analytics process stages against the automation levels in our taxonomy.},
  archive      = {J_TVCG},
  author       = {Veronika Domova and Katerina Vrotsou},
  doi          = {10.1109/TVCG.2022.3163765},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3550-3568},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A model for types and levels of automation in visual analytics: A survey, a taxonomy, and examples},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cognitive path planning with spatial memory distortion.
<em>TVCG</em>, <em>29</em>(8), 3535–3549. (<a
href="https://doi.org/10.1109/TVCG.2022.3163794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human path-planning operates differently from deterministic AI-based path-planning algorithms due to the decay and distortion in a human&#39;s spatial memory and the lack of complete scene knowledge. Here, we present a cognitive model of path-planning that simulates human-like learning of unfamiliar environments, supports systematic degradation in spatial memory, and distorts spatial recall during path-planning. We propose a Dynamic Hierarchical Cognitive Graph (DHCG) representation to encode the environment structure by incorporating two critical spatial memory biases during exploration: categorical adjustment and sequence order effect . We then extend the “Fine-To-Coarse” (FTC), the most prevalent path-planning heuristic, to incorporate spatial uncertainty during recall through the DHCG. We conducted a lab-based Virtual Reality (VR) experiment to validate the proposed cognitive path-planning model and made three observations: (1) a statistically significant impact of sequence order effect on participants’ route-choices, (2) approximately three hierarchical levels in the DHCG according to participants’ recall data, and (3) similar trajectories and significantly similar wayfinding performances between participants and simulated cognitive agents on identical path-planning tasks. Furthermore, we performed two detailed simulation experiments with different FTC variants on a Manhattan-style grid. Experimental results demonstrate that the proposed cognitive path-planning model successfully produces human-like paths and can capture human wayfinding&#39;s complex and dynamic nature, which traditional AI-based path-planning algorithms cannot capture.},
  archive      = {J_TVCG},
  author       = {Rohit K. Dubey and Samuel S. Sohn and Tyler Thrash and Christoph Hölscher and André Borrmann and Mubbasir Kapadia},
  doi          = {10.1109/TVCG.2022.3163794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3535-3549},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cognitive path planning with spatial memory distortion},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rhythm is a dancer: Music-driven motion synthesis with
global structure. <em>TVCG</em>, <em>29</em>(8), 3519–3534. (<a
href="https://doi.org/10.1109/TVCG.2022.3163676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing human motion with a global structure, such as a choreography, is a challenging task. Existing methods tend to concentrate on local smooth pose transitions and neglect the global context or the theme of the motion. In this work, we present a music-driven motion synthesis framework that generates long-term sequences of human motions which are synchronized with the input beats, and jointly form a global structure that respects a specific dance genre. In addition, our framework enables generation of diverse motions that are controlled by the content of the music, and not only by the beat. Our music-driven dance synthesis framework is a hierarchical system that consists of three levels: pose , motif , and choreography . The pose level consists of an LSTM component that generates temporally coherent sequences of poses. The motif level guides sets of consecutive poses to form a movement that belongs to a specific distribution using a novel motion perceptual-loss . And the choreography level selects the order of the performed movements and drives the system to follow the global structure of a dance genre. Our results demonstrate the effectiveness of our music-driven framework to generate natural and consistent movements on various dance types, having control over the content of the synthesized motions, and respecting the overall structure of the dance.},
  archive      = {J_TVCG},
  author       = {Andreas Aristidou and Anastasios Yiannakidis and Kfir Aberman and Daniel Cohen-Or and Ariel Shamir and Yiorgos Chrysanthou},
  doi          = {10.1109/TVCG.2022.3163676},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3519-3534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rhythm is a dancer: Music-driven motion synthesis with global structure},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influence of user posture and virtual exercise on impression
of locomotion during VR observation. <em>TVCG</em>, <em>29</em>(8),
3507–3518. (<a href="https://doi.org/10.1109/TVCG.2022.3161130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A seated user watching his avatar walking in Virtual Reality (VR) may have an impression of walking. In this paper, we show that such an impression can be extended to other postures and other locomotion exercises. We present two user studies in which participants wore a VR headset and observed a first-person avatar performing virtual exercises. In the first experiment, the avatar walked and the participants (n=36) tested the simulation in 3 different postures (standing, sitting and Fowler’s posture). In the second experiment, other participants (n=18) were sitting and observed the avatar walking, jogging or stepping over virtual obstacles. We evaluated the impression of locomotion by measuring the impression of walking (respectively jogging or stepping) and embodiment in both experiments. The results show that participants had the impression of locomotion in either sitting, standing and Fowler’s posture. However, Fowler’s posture significantly decreased both the level of embodiment and the impression of locomotion. The sitting posture seems to decrease the sense of agency compared to standing posture. Results also show that the majority of the participants experienced an impression of locomotion during the virtual walking, jogging, and stepping exercises. The embodiment was not influenced by the type of virtual exercise. Overall, our results suggest that an impression of locomotion can be elicited in different users’ postures and during different virtual locomotion exercises. They provide valuable insight for numerous VR applications in which the user observes a self-avatar moving, such as video games, gait rehabilitation, training, etc.},
  archive      = {J_TVCG},
  author       = {Justine Saint-Aubert and Mélanie Cogné and Isabelle Bonan and Yoann Launey and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2022.3161130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3507-3518},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence of user posture and virtual exercise on impression of locomotion during VR observation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry-aware merge tree comparisons for time-varying data
with interleaving distances. <em>TVCG</em>, <em>29</em>(8), 3489–3506.
(<a href="https://doi.org/10.1109/TVCG.2022.3163349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Merge trees, a type of topological descriptors, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions , clusters , and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.},
  archive      = {J_TVCG},
  author       = {Lin Yan and Talha Bin Masood and Farhan Rasheed and Ingrid Hotz and Bei Wang},
  doi          = {10.1109/TVCG.2022.3163349},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3489-3506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geometry-aware merge tree comparisons for time-varying data with interleaving distances},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robustly extracting concise 3D curve skeletons by enhancing
the capture of prominent features. <em>TVCG</em>, <em>29</em>(8),
3472–3488. (<a href="https://doi.org/10.1109/TVCG.2022.3161962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting concise 3D curve skeletons with existing methods is still a serious challenge as these methods require tedious parameter adjustment to suppress the influence of shape boundary perturbations to avoid spurious branches. In this paper, we address this challenge by enhancing the capture of prominent features and using them for skeleton extraction, motivated by the observation that the shape is mainly represented by prominent features. Our method takes the medial mesh of the shape as input, which can maintain the shape topology well. We develop a series of novel measures for simplifying and contracting the medial mesh to capture prominent features and represent them concisely, by which means the influences of shape boundary perturbations on skeleton extraction are suppressed and the quantity of data needed for skeleton extraction is significantly reduced. As a result, we can robustly and concisely extract the curve skeleton based on prominent features, avoiding the trouble of tuning parameters and saving computations, as shown by experimental results.},
  archive      = {J_TVCG},
  author       = {Yiyao Chu and Wencheng Wang and Lei Li},
  doi          = {10.1109/TVCG.2022.3161962},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3472-3488},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robustly extracting concise 3D curve skeletons by enhancing the capture of prominent features},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intentional head-motion assisted locomotion for reducing
cybersickness. <em>TVCG</em>, <em>29</em>(8), 3458–3471. (<a
href="https://doi.org/10.1109/TVCG.2022.3160232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient locomotion technique that can reduce cybersickness through aligning the visual and vestibular induced self-motion illusion. Our locomotion technique stimulates proprioception consistent with the visual sense by intentional head motion, which includes both the head’s translational movement and yaw rotation. A locomotion event is triggered by the hand-held controller together with an intended physical head motion simultaneously. Based on our method, we further explore the connections between the level of cybersickness and the velocity of self motion through a series of experiments. We first conduct Experiment 1 to investigate the cybersickness induced by different translation velocities using our method and then conduct Experiment 2 to investigate the cybersickness induced by different angular velocities. Our user studies from these two experiments reveal a new finding on the correlation between translation/angular velocities and the level of cybersickness. The cybersickness is greatest at the lowest velocity using our method, and the statistical analysis also indicates a possible U-shaped relation between the translation/angular velocity and cybersickness degree. Finally, we conduct Experiment 3 to evaluate the performances of our method and other commonly-used locomotion approaches, i.e., joystick-based steering and teleportation. The results show that our method can significantly reduce cybersickness compared with the joystick-based steering and obtain a higher presence compared with the teleportation. These advantages demonstrate that our method can be an optional locomotion solution for immersive VR applications using commercially available HMD suites only.},
  archive      = {J_TVCG},
  author       = {Zehui Lin and Xiang Gu and Sheng Li and Zhiming Hu and Guoping Wang},
  doi          = {10.1109/TVCG.2022.3160232},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3458-3471},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Intentional head-motion assisted locomotion for reducing cybersickness},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ManuKnowVis: How to support different user groups in
contextualizing and leveraging knowledge repositories. <em>TVCG</em>,
<em>29</em>(8), 3441–3457. (<a
href="https://doi.org/10.1109/TVCG.2023.3279857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ManuKnowVis, the result of a design study, in which we contextualize data from multiple knowledge repositories of a manufacturing process for battery modules used in electric vehicles. In data-driven analyses of manufacturing data, we observed a discrepancy between two stakeholder groups involved in serial manufacturing processes: Knowledge providers (e.g., engineers) have domain knowledge about the manufacturing process but have difficulties in implementing data-driven analyses. Knowledge consumers (e.g., data scientists) have no first-hand domain knowledge but are highly skilled in performing data-driven analyses. ManuKnowVis bridges the gap between providers and consumers and enables the creation and completion of manufacturing knowledge. We contribute a multi-stakeholder design study, where we developed ManuKnowVis in three main iterations with consumers and providers from an automotive company. The iterative development led us to a multiple linked view tool, in which, on the one hand, providers can describe and connect individual entities (e.g., stations or produced parts) of the manufacturing process based on their domain knowledge. On the other hand, consumers can leverage this enhanced data to better understand complex domain problems, thus, performing data analyses more efficiently. As such, our approach directly impacts the success of data-driven analyses from manufacturing data. To demonstrate the usefulness of our approach, we carried out a case study with seven domain experts, which demonstrates how providers can externalize their knowledge and consumers can implement data-driven analyses more efficiently.},
  archive      = {J_TVCG},
  author       = {Joscha Eirich and Dominik Jäckle and Michael Sedlmair and Christoph Wehner and Ute Schmid and Jürgen Bernard and Tobias Schreck},
  doi          = {10.1109/TVCG.2023.3279857},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3441-3457},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ManuKnowVis: How to support different user groups in contextualizing and leveraging knowledge repositories},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lollipops help align visual and statistical fit estimates in
scatterplots with nonlinear models. <em>TVCG</em>, <em>29</em>(7),
3436–3440. (<a href="https://doi.org/10.1109/TVCG.2022.3158093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots overlayed with a nonlinear model enable visual estimation of model-data fit. Although statistical fit is calculated using vertical distances, viewers’ subjective fit is often based on shortest distances. Our results suggest that adding vertical lines (“lollipops”) supports more accurate fit estimation in the steep area of model curves ( https://osf.io/fybx5/ ).},
  archive      = {J_TVCG},
  author       = {Daniel Reimann and Nilam Ram and Robert Gaschler},
  doi          = {10.1109/TVCG.2022.3158093},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3436-3440},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lollipops help align visual and statistical fit estimates in scatterplots with nonlinear models},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). B/surf: Interactive bézier splines on surface meshes.
<em>TVCG</em>, <em>29</em>(7), 3419–3435. (<a
href="https://doi.org/10.1109/TVCG.2022.3171179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a practical framework to port Bézier curves to surfaces. We support the interactive drawing and editing of Bézier splines on manifold meshes with millions of triangles, by relying on just repeated manifold averages. We show that direct extensions of the de Casteljau and Bernstein evaluation algorithms to the manifold setting are fragile, and prone to discontinuities when control polygons become large. Conversely, approaches based on subdivision are robust and can be implemented efficiently. We implement manifold extensions of the recursive de Casteljau bisection, and an open-uniform Lane-Riesenfeld subdivision scheme. For both schemes, we present algorithms for curve tracing, point evaluation, and approximated point insertion. We run bulk experiments to test our algorithms for robustness and performance, and we compare them with other methods at the state of the art, always achieving correct results and superior performance. For interactive editing, we port all the basic user interface interactions found in 2D tools directly to the mesh. We also support mapping complex SVG drawings to the mesh and their interactive editing.},
  archive      = {J_TVCG},
  author       = {Claudio Mancinelli and Giacomo Nazzaro and Fabio Pellacini and Enrico Puppo},
  doi          = {10.1109/TVCG.2022.3171179},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3419-3435},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {B/Surf: Interactive bézier splines on surface meshes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing higher-order 3D tensors by multipole lines.
<em>TVCG</em>, <em>29</em>(7), 3405–3418. (<a
href="https://doi.org/10.1109/TVCG.2022.3158869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e., stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J. C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.},
  archive      = {J_TVCG},
  author       = {Chiara Hergl and Thomas Nagel and Gerik Scheuermann},
  doi          = {10.1109/TVCG.2022.3158869},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3405-3418},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing higher-order 3D tensors by multipole lines},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exocentric control scheme for robot applications: An
immersive virtual reality approach. <em>TVCG</em>, <em>29</em>(7),
3392–3404. (<a href="https://doi.org/10.1109/TVCG.2022.3160389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) exhibit great agility but usually require an experienced pilot to operate them in certain applications such as inspection for disaster scenarios or buildings. The reduction of cognitive overload when driving this kind of aerial robot becomes a challenge and several solutions can be found in the literature. A new virtual control scheme for reducing this cognitive overload when controlling an aerial robot is proposed in this paper. The architecture is based on a novel interaction Drone Exocentric Advanced Metaphor (DrEAM) located in a Cave Automated Virtual Environment (CAVE) and a real robot containing an embedded controller based on quaternion formulation. The testing room, where real robots are evolving, is located away from the CAVE and they are connected via UDP in a ground station. The user controls manually a virtual drone through the DrEAM interaction metaphor, and the real robot imitates autonomously in real time the trajectory imposed by the user in the virtual environment. Experimental results illustrate the easy implementation and feasibility of the proposed scheme in two different scenarios. Results from these tests show that the mental effort when controlling a drone using the proposed virtual control scheme is lower than when controlling it in direct view. Moreover, the easy maneuverability and controllability of the real drone is also demonstrated in real time experiments.},
  archive      = {J_TVCG},
  author       = {Julio Betancourt and Baptiste Wojtkowski and Pedro Castillo and Indira Thouvenin},
  doi          = {10.1109/TVCG.2022.3160389},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3392-3404},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exocentric control scheme for robot applications: An immersive virtual reality approach},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On rotation gains within and beyond perceptual limitations
for seated VR. <em>TVCG</em>, <em>29</em>(7), 3380–3391. (<a
href="https://doi.org/10.1109/TVCG.2022.3159799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head tracking in head-mounted displays (HMDs) enables users to explore a 360-degree virtual scene with free head movements. However, for seated use of HMDs such as users sitting on a chair or a couch, physically turning around 360-degree is not possible. Redirection techniques decouple tracked physical motion and virtual motion, allowing users to explore virtual environments with more flexibility. In seated situations with only head movements available, the difference of stimulus might cause the detection thresholds of rotation gains to differ from that of redirected walking. Therefore we present an experiment with a two-alternative forced-choice (2AFC) design to compare the thresholds for seated and standing situations. Results indicate that users are unable to discriminate rotation gains between 0.89 and 1.28, a smaller range compared to the standing condition. We further treated head amplification as an interaction technique and found that a gain of 2.5, though not a hard threshold, was near the largest gain that users consider applicable. Overall, our work aims to better understand human perception of rotation gains in seated VR and the results provide guidance for future design choices of its applications.},
  archive      = {J_TVCG},
  author       = {Chen Wang and Song-Hai Zhang and Yizhuo Zhang and Stefanie Zollmann and Shi-Min Hu},
  doi          = {10.1109/TVCG.2022.3159799},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3380-3391},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On rotation gains within and beyond perceptual limitations for seated VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WSDesc: Weakly supervised 3D local descriptor learning for
point cloud registration. <em>TVCG</em>, <em>29</em>(7), 3368–3379. (<a
href="https://doi.org/10.1109/TVCG.2022.3160005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel method called WSDesc to learn 3D local descriptors in a weakly supervised manner for robust point cloud registration. Our work builds upon recent 3D CNN-based descriptor extractors, which leverage a voxel-based representation to parameterize local geometry of 3D points. Instead of using a predefined fixed-size local support in voxelization, we propose to learn the optimal support in a data-driven manner. To this end, we design a novel differentiable voxelization layer that can back-propagate the gradient to the support size optimization. To train the extracted descriptors, we propose a novel registration loss based on the deviation from rigidity of 3D transformations, and the loss is weakly supervised by the prior knowledge that the input point clouds have partial overlap, without requiring ground-truth alignment information. Through extensive experiments, we show that our learned descriptors yield superior performance on existing geometric registration benchmarks.},
  archive      = {J_TVCG},
  author       = {Lei Li and Hongbo Fu and Maks Ovsjanikov},
  doi          = {10.1109/TVCG.2022.3160005},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3368-3379},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WSDesc: Weakly supervised 3D local descriptor learning for point cloud registration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local latent representation based on geometric convolution
for particle data feature exploration. <em>TVCG</em>, <em>29</em>(7),
3354–3367. (<a href="https://doi.org/10.1109/TVCG.2022.3159114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this article, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.},
  archive      = {J_TVCG},
  author       = {Haoyu Li and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3159114},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3354-3367},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Local latent representation based on geometric convolution for particle data feature exploration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scientometric analysis of interdisciplinary collaboration
and gender trends in 30 years of IEEE VIS publications. <em>TVCG</em>,
<em>29</em>(7), 3340–3353. (<a
href="https://doi.org/10.1109/TVCG.2022.3158236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 articles and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors’ gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women&#39;s authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating ( $\approx$ 75\%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century ( $\approx$ 2070). Our primary goal in this work is to call the visualization community&#39;s attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.},
  archive      = {J_TVCG},
  author       = {Ali Sarvghad and Rolando Franqui-Nadal and Rebecca Reznik-Zellen and Ria Chawla and Narges Mahyar},
  doi          = {10.1109/TVCG.2022.3158236},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3340-3353},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scientometric analysis of interdisciplinary collaboration and gender trends in 30 years of IEEE VIS publications},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-step out-of-place resetting for redirected walking in
VR. <em>TVCG</em>, <em>29</em>(7), 3327–3339. (<a
href="https://doi.org/10.1109/TVCG.2022.3158609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) allows users to explore virtual environments in limited physical spaces by imperceptibly steering them away from obstacles and space boundaries. However, even with those techniques, the risk of collision cannot always be avoided. For such situations, resetting techniques have been proposed to provide an immediate adjustment of the physical walking direction of a user. Existing resetting techniques are either applied in-place, where the user changes orientation but stays in the same position or out-of-place methods where the user is guided to move from the current position to a safe location all while freezing the movement in the virtual world. While out-of-place methods have the potential to provide more freedom to user movements after resetting, current out-of-place methods do not provide enough guidance for the users to move to optimal locations. In this work, we propose a novel out-of-place resetting strategy that guides users to optimal physical locations with the most potential for free movement and a smaller amount of resetting required for their further movements. For this purpose, we calculate a heat map of the walking area according to the average walking distance using a simulation of the currently used RDW algorithm. Based on this heat map, we identify the most suitable position for a one-step reset within a predefined searching range and use this one as the reset point. Our results show that our method increases the average moving distance within one cycle of resetting. Furthermore, our resetting method can be applied to any physical area with obstacles. That means that RDW methods that were not suitable for such environments (e.g., Steer to Center) combined with our resetting can also be extended to such complex walking areas. In addition, we present a user interface to provide a similar visual experience between these methods, using a two-arrows indicator to help users adjust their position and direction.},
  archive      = {J_TVCG},
  author       = {Song-Hai Zhang and Chiahao Chen and Stefanie Zollmann},
  doi          = {10.1109/TVCG.2022.3158609},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3327-3339},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {One-step out-of-place resetting for redirected walking in VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual exploration of relationships and structure in
low-dimensional embeddings. <em>TVCG</em>, <em>29</em>(7), 3312–3326.
(<a href="https://doi.org/10.1109/TVCG.2022.3156760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and/or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and/or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.},
  archive      = {J_TVCG},
  author       = {Klaus Eckelt and Andreas Hinterreiter and Patrick Adelberger and Conny Walchshofer and Vaishali Dhanoa and Christina Humer and Moritz Heckmann and Christian Steinparz and Marc Streit},
  doi          = {10.1109/TVCG.2022.3156760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3312-3326},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual exploration of relationships and structure in low-dimensional embeddings},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VisImages: A fine-grained expert-annotated visualization
dataset. <em>TVCG</em>, <em>29</em>(7), 3298–3311. (<a
href="https://doi.org/10.1109/TVCG.2022.3155440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images in visualization publications contain rich information, e.g., novel visualization designs and implicit design patterns of visualizations. A systematic collection of these images can contribute to the community in many aspects, such as literature analysis and automated tasks for visualization. In this paper, we build and make public a dataset, VisImages, which collects 12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built upon a comprehensive visualization taxonomy, the dataset includes 35,096 visualizations and their bounding boxes in the images. We demonstrate the usefulness of VisImages through three use cases: 1) investigating the use of visualizations in the publications with VisImages Explorer, 2) training and benchmarking models for visualization classification, and 3) localizing visualizations in the visual analytics systems automatically.},
  archive      = {J_TVCG},
  author       = {Dazhen Deng and Yihong Wu and Xinhuan Shu and Jiang Wu and Siwei Fu and Weiwei Cui and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3155440},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3298-3311},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisImages: A fine-grained expert-annotated visualization dataset},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RagRug: A toolkit for situated analytics. <em>TVCG</em>,
<em>29</em>(7), 3281–3297. (<a
href="https://doi.org/10.1109/TVCG.2022.3157058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualizations with data streams from the Internet of Things using distributed dataflow. To this end, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit’s abilities.},
  archive      = {J_TVCG},
  author       = {Philipp Fleck and Aimée Sousa Calepso and Sebastian Hubenschmid and Michael Sedlmair and Dieter Schmalstieg},
  doi          = {10.1109/TVCG.2022.3157058},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3281-3297},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RagRug: A toolkit for situated analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aggregated contextual transformations for high-resolution
image inpainting. <em>TVCG</em>, <em>29</em>(7), 3266–3280. (<a
href="https://doi.org/10.1109/TVCG.2022.3156949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting that completes large free-form missing regions in images is a promising yet challenging task. State-of-the-art approaches have achieved significant progress by taking advantage of generative adversarial networks (GAN). However, these approaches can suffer from generating distorted structures and blurry textures in high-resolution images (e.g., $512\times 512$ ). The challenges mainly drive from (1) image content reasoning from distant contexts, and (2) fine-grained texture synthesis for a large missing region. To overcome these two challenges, we propose an enhanced GAN-based model, named A ggregated C O ntextual- T ransformation GAN ( AOT-GAN ), for high-resolution image inpainting. Specifically, to enhance context reasoning, we construct the generator of AOT-GAN by stacking multiple layers of a proposed AOT block. The AOT blocks aggregate contextual transformations from various receptive fields, allowing to capture both informative distant image contexts and rich patterns of interest for context reasoning. For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task. Such a training objective forces the discriminator to distinguish the detailed appearances of real and synthesized patches, and in turn facilitates the generator to synthesize clear textures. Extensive comparisons on Places2, the most challenging benchmark with 1.8 million high-resolution images of 365 complex scenes, show that our model outperforms the state-of-the-art. A user study including more than 30 subjects further validates the superiority of AOT-GAN. We further evaluate the proposed AOT-GAN in practical applications, e.g., logo removal, face editing, and object removal. Results show that our model achieves promising completions in the real world. We release codes and models in https://github.com/researchmm/AOT-GAN-for-Inpainting .},
  archive      = {J_TVCG},
  author       = {Yanhong Zeng and Jianlong Fu and Hongyang Chao and Baining Guo},
  doi          = {10.1109/TVCG.2022.3156949},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3266-3280},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Aggregated contextual transformations for high-resolution image inpainting},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Unsupervised category-specific partial point set
registration via joint shape completion and registration. <em>TVCG</em>,
<em>29</em>(7), 3251–3265. (<a
href="https://doi.org/10.1109/TVCG.2022.3157061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a self-supervised method for partial point set registration. Although recently proposed learning-based methods demonstrate impressive registration performance on full shape observations, these methods often suffer from performance degradation when dealing with partial shapes. To bridge the performance gap between partial and full point set registration, we propose to incorporate a shape completion network to benefit the registration process. To achieve this, we introduce a learnable latent code for each pair of shapes, which can be regarded as the geometric encoding of the target shape. By doing so, our model does not require an explicit feature embedding network to learn the feature encodings. More importantly, both our shape completion and point set registration networks take the shared latent codes as input, which are optimized simultaneously with the parameters of two decoder networks in the training process. Therefore, the point set registration process can benefit from the joint optimization process of latent codes, which are enforced to represent the information of full shapes instead of partial ones. In the inference stage, we fix the network parameters and optimize the latent codes to obtain the optimal shape completion and registration results. Our proposed method is purely unsupervised and does not require ground truth supervision. Experiments on the ModelNet40 dataset demonstrate the effectiveness of our model for partial point set registration.},
  archive      = {J_TVCG},
  author       = {Xiang Li and Lingjing Wang and Yi Fang},
  doi          = {10.1109/TVCG.2022.3157061},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3251-3265},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unsupervised category-specific partial point set registration via joint shape completion and registration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using heart rate variability for comparing the effectiveness
of virtual vs real training environments for firefighters.
<em>TVCG</em>, <em>29</em>(7), 3238–3250. (<a
href="https://doi.org/10.1109/TVCG.2022.3156734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Virtual Reality (VR) technology to train professionals has increased over the years due to its advantages over traditional training. This paper presents a study comparing the effectiveness of a Virtual Environment (VE) and a Real Environment (RE) designed to train firefighters. To measure the effectiveness of the environments, a new method based on participants’ Heart Rate Variability (HRV) was used. This method was complemented with self-reports, in the form of questionnaires, of fatigue, stress, sense of presence, and cybersickness. An additional questionnaire was used to measure and compare knowledge transfer enabled by the environments. The results from HRV analysis indicated that participants were under physiological stress in both environments, albeit with less intensity on the VE. Regarding reported fatigue and stress, the results showed that none of the environments increased such variables. The results of knowledge transfer showed that the VE obtained a significant increase while the RE obtained a positive but non-significant increase (median values, VE: before – 4 after – 7, p = .003; RE: before – 4 after – 5, p = .375). Lastly, the results of presence and cybersickness suggested that participants experienced high overall presence and no cybersickness. Considering all results, the authors conclude that the VE provided effective training but that its effectiveness was lower than that of the RE.},
  archive      = {J_TVCG},
  author       = {David Narciso and Miguel Melo and Susana Rodrigues and João Paulo Cunha and José Vasconcelos-Raposo and Maximino Bessa},
  doi          = {10.1109/TVCG.2022.3156734},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3238-3250},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using heart rate variability for comparing the effectiveness of virtual vs real training environments for firefighters},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point set self-embedding. <em>TVCG</em>, <em>29</em>(7),
3226–3237. (<a href="https://doi.org/10.1109/TVCG.2022.3155808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an innovative method for point set self-embedding, that encodes the structural information of a dense point set into its sparser version in a visual but imperceptible form. The self-embedded point set can function as the ordinary downsampled one and be visualized efficiently on mobile devices. Particularly, we can leverage the self-embedded information to fully restore the original point set for detailed analysis on remote servers. This task is challenging, since both the self-embedded point set and the restored point set should resemble the original one. To achieve a learnable self-embedding scheme, we design a novel framework with two jointly-trained networks: one to encode the input point set into its self-embedded sparse point set and the other to leverage the embedded information for inverting the original point set back. Further, we develop a pair of up-shuffle and down-shuffle units in the two networks, and formulate loss terms to encourage the shape similarity and point distribution in the results. Extensive qualitative and quantitative results demonstrate the effectiveness of our method on both synthetic and real-scanned datasets. The source code and trained models will be publicly available at https://github.com/liruihui/Self-Embedding .},
  archive      = {J_TVCG},
  author       = {Ruihui Li and Xianzhi Li and Tien-Tsin Wong and Chi-Wing Fu},
  doi          = {10.1109/TVCG.2022.3155808},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3226-3237},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Point set self-embedding},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological simplifications of hypergraphs. <em>TVCG</em>,
<em>29</em>(7), 3209–3225. (<a
href="https://doi.org/10.1109/TVCG.2022.3153895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations, known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general and mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.},
  archive      = {J_TVCG},
  author       = {Youjia Zhou and Archit Rathore and Emilie Purvine and Bei Wang},
  doi          = {10.1109/TVCG.2022.3153895},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3209-3225},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Topological simplifications of hypergraphs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep generative model for reordering adjacency matrices.
<em>TVCG</em>, <em>29</em>(7), 3195–3208. (<a
href="https://doi.org/10.1109/TVCG.2022.3153838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depending on the node ordering, an adjacency matrix can highlight distinct characteristics of a graph. Deriving a “proper” node ordering is thus a critical step in visualizing a graph as an adjacency matrix. Users often try multiple matrix reorderings using different methods until they find one that meets the analysis goal. However, this trial-and-error approach is laborious and disorganized, which is especially challenging for novices. This paper presents a technique that enables users to effortlessly find a matrix reordering they want. Specifically, we design a generative model that learns a latent space of diverse matrix reorderings of the given graph. We also construct an intuitive user interface from the learned latent space by creating a map of various matrix reorderings. We demonstrate our approach through quantitative and qualitative evaluations of the generated reorderings and learned latent spaces. The results show that our model is capable of learning a latent space of diverse matrix reorderings. Most existing research in this area generally focused on developing algorithms that can compute “better” matrix reorderings for particular circumstances. This paper introduces a fundamentally new approach to matrix visualization of a graph, where a machine learning model learns to generate diverse matrix reorderings of a graph.},
  archive      = {J_TVCG},
  author       = {Oh-Hyun Kwon and Chiun-How Kao and Chun-houh Chen and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3153838},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3195-3208},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A deep generative model for reordering adjacency matrices},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry-aware planar embedding of treelike structures.
<em>TVCG</em>, <em>29</em>(7), 3182–3194. (<a
href="https://doi.org/10.1109/TVCG.2022.3153871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).},
  archive      = {J_TVCG},
  author       = {Ping Hu and Saeed Boorboor and Joseph Marino and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2022.3153871},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3182-3194},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geometry-aware planar embedding of treelike structures},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VividGraph: Learning to extract and redesign network graphs
from visualization images. <em>TVCG</em>, <em>29</em>(7), 3169–3181. (<a
href="https://doi.org/10.1109/TVCG.2022.3153514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network graphs are common visualization charts. They often appear in the form of bitmaps in articles, web pages, magazine prints, and designer sketches. People often want to modify graphs because of their poor design, but it is difficult to obtain their underlying data. In this article, we present VividGraph, a pipeline for automatically extracting and redesigning graphs from static images. We propose using convolutional neural networks to solve the problem of graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing graphs, and interactively redesign poorly designed graphs.},
  archive      = {J_TVCG},
  author       = {Sicheng Song and Chenhui Li and Yujing Sun and Changbo Wang},
  doi          = {10.1109/TVCG.2022.3153514},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3169-3181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VividGraph: Learning to extract and redesign network graphs from visualization images},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TeethGNN: Semantic 3D teeth segmentation with graph neural
networks. <em>TVCG</em>, <em>29</em>(7), 3158–3168. (<a
href="https://doi.org/10.1109/TVCG.2022.3153501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present TeethGNN, a novel 3D tooth segmentation method based on graph neural networks (GNNs). Given a mesh-represented 3D dental model in non-euclidean domain, our method outputs accurate and fine-grained separation of each individual tooth robust to scanning noise, foreign matters (e.g., bubbles, dental accessories, etc.), and even severe malocclusion. Unlike previous CNN-based methods that bypass handling non-euclidean mesh data by reshaping hand-crafted geometric features into regular grids, we explore the non-uniform and irregular structure of mesh itself in its dual space and exploit graph neural networks for effective geometric feature learning. To address the crowded teeth issues and incomplete segmentation that commonly exist in previous methods, we design a two-branch network, one of which predicts a segmentation label for each facet while the other regresses each facet an offset away from its tooth centroid. Clustering are later conducted on offset-shifted locations, enabling both the separation of adjoining teeth and the adjustment of incompletely segmented teeth. Exploiting GNN for directly processing mesh data frees us from extracting hand-crafted feature, and largely speeds up the inference procedure. Extensive experiments have shown that our method achieves the new state-of-the-art results for teeth segmentation and outperforms previous methods both quantitatively and qualitatively.},
  archive      = {J_TVCG},
  author       = {Youyi Zheng and Beijia Chen and Yuefan Shen and Kaidi Shen},
  doi          = {10.1109/TVCG.2022.3153501},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3158-3168},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TeethGNN: Semantic 3D teeth segmentation with graph neural networks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collimated whole volume light scattering in homogeneous
finite media. <em>TVCG</em>, <em>29</em>(7), 3145–3157. (<a
href="https://doi.org/10.1109/TVCG.2021.3135764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crepuscular rays form when light encounters an optically thick or opaque medium which masks out portions of the visible scene. Real-time applications commonly estimate this phenomena by connecting paths between light sources and the camera after a single scattering event. We provide a set of algorithms for solving integration and sampling of single-scattered collimated light in a box-shaped medium and show how they extend to multiple scattering and convex media. First, a method for exactly integrating the unoccluded single scattering in rectilinear box-shaped medium is proposed and paired with a ratio estimator and moment-based approximation. Compared to previous methods, it requires only a single sample in unoccluded areas to compute the whole integral solution and provides greater convergence in the rest of the scene. Second, we derive an importance sampling scheme accounting for the entire geometry of the medium. This sampling strategy is then incorporated in an optimized Monte Carlo integration. The resulting integration scheme yields visible noise reduction and it is directly applicable to indoor scene rendering in room-scale interactive experiences. Furthermore, it extends to multiple light sources and achieves superior converge compared to independent sampling with existing algorithms. We validate our techniques against previous methods based on ray marching and distance sampling to prove their superior noise reduction capability.},
  archive      = {J_TVCG},
  author       = {Zdravko Velinov and Kenny Mitchell},
  doi          = {10.1109/TVCG.2021.3135764},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3145-3157},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Collimated whole volume light scattering in homogeneous finite media},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards natural language interfaces for data visualization:
A survey. <em>TVCG</em>, <em>29</em>(6), 3121–3144. (<a
href="https://doi.org/10.1109/TVCG.2022.3148007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than having to worry about how to operate visualization tools on the interface. In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed in academic research and commercial software, especially in recent years. In this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each article, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and presentation. Finally, we also shed light on several promising directions for future work in the V-NLI community.},
  archive      = {J_TVCG},
  author       = {Leixian Shen and Enya Shen and Yuyu Luo and Xiaocong Yang and Xuming Hu and Xiongshuai Zhang and Zhiwei Tai and Jianmin Wang},
  doi          = {10.1109/TVCG.2022.3148007},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3121-3144},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards natural language interfaces for data visualization: A survey},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LegalVis: Exploring and inferring precedent citations in
legal documents. <em>TVCG</em>, <em>29</em>(6), 3105–3120. (<a
href="https://doi.org/10.1109/TVCG.2022.3152450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF’s justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document’s text aiming to interpret the model’s output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96\%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.},
  archive      = {J_TVCG},
  author       = {Lucas E. Resck and Jean R. Ponciano and Luis Gustavo Nonato and Jorge Poco},
  doi          = {10.1109/TVCG.2022.3152450},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3105-3120},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LegalVis: Exploring and inferring precedent citations in legal documents},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aesthetics++: Refining graphic designs by exploring design
principles and human preference. <em>TVCG</em>, <em>29</em>(6),
3093–3104. (<a href="https://doi.org/10.1109/TVCG.2022.3151617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the creation of graphic designs, individuals inevitably spend a lot of time and effort on adjusting visual attributes (e.g., positions, colors, and fonts) of elements to make them more aesthetically pleasing. It is a trial-and-error process, requires repetitive edits, and relies on good design knowledge. In this work, we seek to alleviate such difficulty by automatically suggesting aesthetic improvements, i.e., taking an existing design as the input and generating a refined version with improved aesthetic quality as the output. This goal presents two challenges: proposing a refined design based on the user-given one, and assessing whether the new design is better aesthetically. To cope with these challenges, we propose a design principle-guided candidate generation stage and a data-driven candidate evaluation stage. In the candidate generation stage, we generate candidate designs by leveraging design principles as the guidance to make changes around the existing design. In the candidate evaluation stage, we learn a ranking model upon a dataset that can reflect humans’ aesthetic preference, and use it to choose the most aesthetically pleasing one from the generated candidates. We implement a prototype system on presentation slides and demonstrate the effectiveness of our approach through quantitative analysis, sample results, and user studies.},
  archive      = {J_TVCG},
  author       = {Wenyuan Kong and Zhaoyun Jiang and Shizhao Sun and Zhuoning Guo and Weiwei Cui and Ting Liu and Jianguang Lou and Dongmei Zhang},
  doi          = {10.1109/TVCG.2022.3151617},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3093-3104},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Aesthetics++: Refining graphic designs by exploring design principles and human preference},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impulse fluid simulation. <em>TVCG</em>, <em>29</em>(6),
3081–3092. (<a href="https://doi.org/10.1109/TVCG.2022.3149466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new incompressible Navier–Stokes solver based on the impulse gauge transformation. The mathematical model of our approach draws from the impulse–velocity formulation of Navier–Stokes equations, which evolves the fluid impulse as an auxiliary variable of the system that can be projected to obtain the incompressible flow velocities at the end of each time step. We solve the impulse-form equations numerically on a Cartesian grid. At the heart of our simulation algorithm is a novel model to treat the impulse stretching and a harmonic boundary treatment to incorporate the surface tension effects accurately. We also build an impulse PIC/FLIP solver to support free-surface fluid simulation. Our impulse solver can naturally produce rich vortical flow details without artificial enhancements. We showcase this feature by using our solver to facilitate a wide range of fluid simulation tasks including smoke, liquid, and surface-tension flow. In addition, we discuss a convenient mechanism in our framework to control the scale and strength of the turbulent effects of fluid.},
  archive      = {J_TVCG},
  author       = {Fan Feng and Jinyuan Liu and Shiying Xiong and Shuqi Yang and Yaorui Zhang and Bo Zhu},
  doi          = {10.1109/TVCG.2022.3149466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3081-3092},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impulse fluid simulation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceptual assessment of image and depth quality of
dynamically depth-compressed scene for automultiscopic 3D display.
<em>TVCG</em>, <em>29</em>(6), 3067–3080. (<a
href="https://doi.org/10.1109/TVCG.2022.3148419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the depth range which automultiscopic 3D (A3D) displays should reproduce for ensuring an adequate perceptual quality of substantially deep scenes. These displays usually need sufficient depth reconstruction capabilities covering the whole scene depth, but due to the inherent hardware restriction of these displays this is often difficult, particularly for showing deep scenes. Previous studies have addressed this limitation by introducing depth compression that contracts the scene depth into a smaller depth range by modifying the scene geometry, assuming that the scenes were represented as CG data. The previous results showed that reconstructing only a physical depth of 1 m is needed to show scenes with much deeper depth and without large perceptual quality degradation. However, reconstructing a depth of 1 m is still challenging for actual A3D displays. In this study, focusing on a personal viewing situation, we introduce a dynamic depth compression that combines viewpoint tracking with the previous approach and examines the extent to which scene depths can be compressed while keeping the original perceptual quality. Taking into account the viewer&#39;s viewpoint movements, which were considered a cause of unnaturalness in the previous approach, we performed an experiment with an A3D display simulator and found that a depth of just 10 cm was sufficient for showing deep scenes without inducing a feeling of unnaturalness. Next, we investigated whether the simulation results were valid even on a real A3D display and found that the dynamic approach induced better perceptual quality than the static one even on the real A3D display and that it had a depth enhancing effect without any hardware updates. These results suggest that providing a physical depth of 10 cm on personalized A3D displays is general enough for showing any deeper 3D scenes with appealing subjective quality.},
  archive      = {J_TVCG},
  author       = {Yamato Miyashita and Yasuhito Sawahata and Kazuteru Komine},
  doi          = {10.1109/TVCG.2022.3148419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3067-3080},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual assessment of image and depth quality of dynamically depth-compressed scene for automultiscopic 3D display},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for load-balanced parallel particle
tracing. <em>TVCG</em>, <em>29</em>(6), 3052–3066. (<a
href="https://doi.org/10.1109/TVCG.2022.3148745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes’ workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.},
  archive      = {J_TVCG},
  author       = {Jiayi Xu and Hanqi Guo and Han-Wei Shen and Mukund Raj and Skylar W. Wurster and Tom Peterka},
  doi          = {10.1109/TVCG.2022.3148745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3052-3066},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reinforcement learning for load-balanced parallel particle tracing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive joint optimization for 3D reconstruction with
differentiable rendering. <em>TVCG</em>, <em>29</em>(6), 3039–3051. (<a
href="https://doi.org/10.1109/TVCG.2022.3148245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to inevitable noises introduced during scanning and quantization, 3D reconstruction via RGB-D sensors suffers from errors both in geometry and texture, leading to artifacts such as camera drifting, mesh distortion, texture ghosting, and blurriness. Given an imperfect reconstructed 3D model, most previous methods have focused on refining either geometry, texture, or camera pose. Consequently, different optimization schemes and objectives for optimizing each component have been used in previous joint optimization methods, forming a complicated system. In this paper, we propose a novel optimization approach based on differentiable rendering, which integrates the optimization of camera pose, geometry, and texture into a unified framework by enforcing consistency between the rendered results and the corresponding RGB-D inputs. Based on the unified framework, we introduce a joint optimization approach to fully exploit the inter-relationships among the three objective components, and describe an adaptive interleaving strategy to improve optimization stability and efficiency. Using differentiable rendering, an image-level adversarial loss is applied to further improve the 3D model, making it more photorealistic. Experiments on synthetic and real data using quantitative and qualitative evaluation demonstrated the superiority of our approach in recovering both fine-scale geometry and high-fidelity texture.},
  archive      = {J_TVCG},
  author       = {Jingbo Zhang and Ziyu Wan and Jing Liao},
  doi          = {10.1109/TVCG.2022.3148245},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3039-3051},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive joint optimization for 3D reconstruction with differentiable rendering},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GNNLens: A visual analytics approach for prediction error
diagnosis of graph neural networks. <em>TVCG</em>, <em>29</em>(6),
3024–3038. (<a href="https://doi.org/10.1109/TVCG.2022.3148107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens , to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.},
  archive      = {J_TVCG},
  author       = {Zhihua Jin and Yong Wang and Qianwen Wang and Yao Ming and Tengfei Ma and Huamin Qu},
  doi          = {10.1109/TVCG.2022.3148107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3024-3038},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GNNLens: A visual analytics approach for prediction error diagnosis of graph neural networks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual reasoning for uncertainty in spatio-temporal events
of historical figures. <em>TVCG</em>, <em>29</em>(6), 3009–3023. (<a
href="https://doi.org/10.1109/TVCG.2022.3146508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of digitized humanity information provides a new perspective on data-oriented studies of history. Many previous studies have ignored uncertainty in the exploration of historical figures and events, which has limited the capability of researchers to capture complex processes associated with historical phenomena. We propose a visual reasoning system to support visual reasoning of uncertainty associated with spatio-temporal events of historical figures based on data from the China Biographical Database Project. We build a knowledge graph of entities extracted from a historical database to capture uncertainty generated by missing data and error. The proposed system uses an overview of chronology, a map view, and an interpersonal relation matrix to describe and analyse heterogeneous information of events. The system also includes uncertainty visualization to identify uncertain events with missing or imprecise spatio-temporal information. Results from case studies and expert evaluations suggest that the visual reasoning system is able to quantify and reduce uncertainty generated by the data.},
  archive      = {J_TVCG},
  author       = {Wei Zhang and Siwei Tan and Siming Chen and Linghao Meng and Tianye Zhang and Rongchen Zhu and Wei Chen},
  doi          = {10.1109/TVCG.2022.3146508},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3009-3023},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual reasoning for uncertainty in spatio-temporal events of historical figures},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StrategyAtlas: Strategy analysis for machine learning
interpretability. <em>TVCG</em>, <em>29</em>(6), 2996–3008. (<a
href="https://doi.org/10.1109/TVCG.2022.3146806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas , a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.},
  archive      = {J_TVCG},
  author       = {Dennis Collaris and Jarke J. van Wijk},
  doi          = {10.1109/TVCG.2022.3146806},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2996-3008},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StrategyAtlas: Strategy analysis for machine learning interpretability},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Roslingifier: Semi-automated storytelling for animated
scatterplots. <em>TVCG</em>, <em>29</em>(6), 2980–2995. (<a
href="https://doi.org/10.1109/TVCG.2022.3146329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948–2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time—such as countries and continents with their demographic data—into an engaging narrative elling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this article, we aim to define a design space for this new genre—data presentation—and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.},
  archive      = {J_TVCG},
  author       = {Minjeong Shin and Joohee Kim and Yunha Han and Lexing Xie and Mitchell Whitelaw and Bum Chul Kwon and Sungahn Ko and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2022.3146329},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2980-2995},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Roslingifier: Semi-automated storytelling for animated scatterplots},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reference-based deep line art video colorization.
<em>TVCG</em>, <em>29</em>(6), 2965–2979. (<a
href="https://doi.org/10.1109/TVCG.2022.3146000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coloring line art images based on the colors of reference images is a crucial stage in animation production, which is time-consuming and tedious. This paper proposes a deep architecture to automatically color line art videos with the same color style as the given reference images. Our framework consists of a color transform network and a temporal refinement network based on 3U-net. The color transform network takes the target line art images as well as the line art and color images of the reference images as input and generates corresponding target color images. To cope with the large differences between each target line art image and the reference color images, we propose a distance attention layer that utilizes non-local similarity matching to determine the region correspondences between the target image and the reference images and transforms the local color information from the references to the target. To ensure global color style consistency, we further incorporate Adaptive Instance Normalization (AdaIN) with the transformation parameters obtained from a multiple-layer AdaIN that describes the global color style of the references extracted by an embedder network. The temporal refinement network learns spatiotemporal features through 3D convolutions to ensure the temporal color consistency of the results. Our model can achieve even better coloring results by fine-tuning the parameters with only a small number of samples when dealing with an animation of a new style. To evaluate our method, we build a line art coloring dataset. Experiments show that our method achieves the best performance on line art video coloring compared to the current state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Min Shi and Jia-Qi Zhang and Shu-Yu Chen and Lin Gao and Yu-Kun Lai and Fang-Lue Zhang},
  doi          = {10.1109/TVCG.2022.3146000},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2965-2979},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reference-based deep line art video colorization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing the scripts of data wrangling with somnus.
<em>TVCG</em>, <em>29</em>(6), 2950–2964. (<a
href="https://doi.org/10.1109/TVCG.2022.3144975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this article, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus , that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus , and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus .},
  archive      = {J_TVCG},
  author       = {Kai Xiong and Siwei Fu and Guoming Ding and Zhongsu Luo and Rong Yu and Wei Chen and Hujun Bao and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3144975},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2950-2964},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing the scripts of data wrangling with somnus},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient specular glints rendering with differentiable
regularization. <em>TVCG</em>, <em>29</em>(6), 2940–2949. (<a
href="https://doi.org/10.1109/TVCG.2022.3144479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering glinty details from specular microstructure enhances the level of realism in computer graphics. However, naive sampling fails to render such effects, due to insufficient sampling of the contributing normals on the surface patch visible through a pixel. Other approaches resort to searching for the relevant normals in more explicit ways, but they rely on special acceleration structures, leading to increased storage costs and complexity. In this article, we propose to render specular glints through a different method: differentiable regularization. Our method includes two steps: first, we use differentiable path tracing to render a scene with a larger light size and/or rougher surfaces and record the gradients with respect to light size and roughness. Next, we use the result for the larger light size and rougher surfaces, together with their gradients, to predict the target value for the required light size and roughness by extrapolation. In the end, we get significantly reduced noise compared to rendering the scene directly. Our results are close to the reference, which uses many more samples per pixel, although our method cannot guarantee unbiased convergence to the reference. The overhead for differentiable rendering and prediction is small, so our improvement is almost free. We demonstrate our differentiable regularization on several normal maps, all of which benefit from the method.},
  archive      = {J_TVCG},
  author       = {Jiahui Fan and Beibei Wang and Wenshi Wu and Miloš Hašan and Jian Yang and Ling-Qi Yan},
  doi          = {10.1109/TVCG.2022.3144479},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2940-2949},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient specular glints rendering with differentiable regularization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic schelling point detection from meshes.
<em>TVCG</em>, <em>29</em>(6), 2926–2939. (<a
href="https://doi.org/10.1109/TVCG.2022.3144143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh Schelling points explain how humans focus on specific regions of a 3D object. They have a large number of important applications in computer graphics and provide valuable information for perceptual psychology studies. However, detecting mesh Schelling points is time-consuming and expensive since the existing techniques are mostly based on participant observation studies. To overcome these limitations, we propose to employ powerful deep learning techniques to detect mesh Schelling points in an automatic manner, free from participant observation studies. Specifically, we utilize the mesh convolution and pooling operations to extract informative features from mesh objects, and then predict the 3D heat map of Schelling points in an end-to-end manner. In addition, we propose a Deep Schelling Network (DS-Net) to automatically detect the Schelling points, including a multi-scale fusion component and a novel region-specific loss function to improve our network for a better regression of heat maps. To the best of our knowledge, DS-Net is the first deep neural network for detecting Schelling points from 3D meshes. We evaluate DS-Net on a mesh Schelling point dataset obtained from participant observation studies. The experimental results demonstrate that DS-Net is capable of detecting mesh Schelling points effectively and outperforms various state-of-the-art mesh saliency methods and deep learning models, both qualitatively and quantitatively.},
  archive      = {J_TVCG},
  author       = {Geng Chen and Hang Dai and Tao Zhou and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TVCG.2022.3144143},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2926-2939},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic schelling point detection from meshes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). SeamlessGAN: Self-supervised synthesis of tileable texture
maps. <em>TVCG</em>, <em>29</em>(6), 2914–2925. (<a
href="https://doi.org/10.1109/TVCG.2022.3143615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time graphics applications require high-quality textured materials to convey realism in virtual environments. Generating these textures is challenging as they need to be visually realistic, seamlessly tileable, and have a small impact on the memory consumption of the application. For this reason, they are often created manually by skilled artists. In this work, we present SeamlessGAN , a method capable of automatically generating tileable texture maps from a single input exemplar. In contrast to most existing methods, focused solely on solving the synthesis problem, our work tackles both problems, synthesis and tileability , simultaneously. Our key idea is to realize that tiling a latent space within a generative network trained using adversarial expansion techniques produces outputs with continuity at the seam intersection that can then be turned into tileable images by cropping the central area. Since not every value of the latent space is valid to produce high-quality outputs, we leverage the discriminator as a perceptual error metric capable of identifying artifact-free textures during a sampling process. Further, in contrast to previous work on deep texture synthesis, our model is designed and optimized to work with multi-layered texture representations, enabling textures composed of multiple maps such as albedo, normals, etc. We extensively test our design choices for the network architecture, loss function, and sampling parameters. We show qualitatively and quantitatively that our approach outperforms previous methods and works for textures of different types.},
  archive      = {J_TVCG},
  author       = {Carlos Rodriguez-Pardo and Elena Garces},
  doi          = {10.1109/TVCG.2022.3143615},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2914-2925},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SeamlessGAN: Self-supervised synthesis of tileable texture maps},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated visualization: A privacy-preserving strategy for
aggregated visual query. <em>TVCG</em>, <em>29</em>(6), 2901–2913. (<a
href="https://doi.org/10.1109/TVCG.2023.3261938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel privacy preservation strategy for aggregated visual query of decentralized data. The key idea is to imitate the flowchart of the federated learning framework, and reformulate the visualization process within a federated infrastructure. The federation of visualization is fulfilled by leveraging a shared global module that composes the encrypted externalizations of transformed visual features of data pieces in local modules. We design two implementations of federated visualization: a prediction-based scheme, and a query-based scheme. We demonstrate the effectiveness of our approach with a set of visual forms, and verify its robustness with evaluations. We report the value of federated visualization in real scenarios with an expert review.},
  archive      = {J_TVCG},
  author       = {Wei Chen and Yating Wei and Zhiyong Wang and Shuyue Zhou and Bingru Lin and Zhiguang Zhou},
  doi          = {10.1109/TVCG.2023.3261938},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2901-2913},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Federated visualization: A privacy-preserving strategy for aggregated visual query},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). How does attention work in vision transformers? A visual
analytics attempt. <em>TVCG</em>, <em>29</em>(6), 2888–2900. (<a
href="https://doi.org/10.1109/TVCG.2023.3261935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as the trend of attention strengths across attention layers. Third, using an autoencoder-based learning solution, we summarize all possible attention patterns that individual heads could learn. Examining the attention strengths and patterns of the important heads, we answer why they are important. Through concrete case studies with experienced deep learning experts on multiple ViTs, we validate the effectiveness of our solution that deepens the understanding of ViTs from head importance , head attention strength , and head attention pattern .},
  archive      = {J_TVCG},
  author       = {Yiran Li and Junpeng Wang and Xin Dai and Liang Wang and Chin-Chia Michael Yeh and Yan Zheng and Wei Zhang and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2023.3261935},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2888-2900},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How does attention work in vision transformers? a visual analytics attempt},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LinSets.zip: Compressing linear set diagrams. <em>TVCG</em>,
<em>29</em>(6), 2875–2887. (<a
href="https://doi.org/10.1109/TVCG.2023.3261934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear diagrams are used to visualize set systems by depicting set memberships as horizontal line segments in a matrix, where each set is represented as a row and each element as a column. Each such line segment of a set is shown in a contiguous horizontal range of cells of the matrix indicating that the corresponding elements in the columns belong to the set. As each set occupies its own row in the matrix, the total height of the resulting visualization is as large as the number of sets in the instance. Such a linear diagram can be visually sparse and intersecting sets containing the same element might be represented by distant rows. To alleviate such undesirable effects, we present LinSets.zip, a new approach that achieves a more space-efficient representation of linear diagrams. First, we minimize the total number of gaps in the horizontal segments by reordering columns, a criterion that has been shown to increase readability in linear diagrams. The main difference of LinSets.zip to linear diagrams is that multiple non-intersecting sets can be positioned in the same row of the matrix. Furthermore, we present several different rendering variations for a matrix-based representation that utilize the proposed row compression. We implemented the different steps of our approach in a visualization pipeline using integer-linear programming, and suitable heuristics aiming at sufficiently fast computations in practice. We conducted both a quantitative evaluation and a small-scale user experiment to compare the effects of compressing linear diagrams.},
  archive      = {J_TVCG},
  author       = {Markus Wallinger and Alexander Dobler and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2023.3261934},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2875-2887},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LinSets.zip: Compressing linear set diagrams},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDRQuerier: A visual querying framework for cross-national
survey data recycling. <em>TVCG</em>, <em>29</em>(6), 2862–2874. (<a
href="https://doi.org/10.1109/TVCG.2023.3261944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public opinion surveys constitute a widespread, powerful tool to study peoples’ attitudes and behaviors from comparative perspectives. However, even global surveys can have limited geographic and temporal coverage, which can hinder the production of comprehensive knowledge. To expand the scope of comparison, social scientists turn to ex-post harmonization of variables from datasets that cover similar topics but in different populations and/or at different times. These harmonized datasets can be analyzed as a single source and accessed through various data portals. However, the Survey Data Recycling (SDR) research project has identified three challenges faced by social scientists when using data portals: the lack of capability to explore data in-depth or query data based on customized needs, the difficulty in efficiently identifying related data for studies, and the incapability to evaluate theoretical models using sliced data. To address these issues, the SDR research project has developed the SDR Querier, which is applied to the harmonized SDR database. The SDR Querier includes a BERT-based model that allows for customized data queries through research questions or keywords (Query-by-Question), a visual design that helps users determine the availability of harmonized data for a given research question (Query-by-Condition), and the ability to reveal the underlying relational patterns among substantive and methodological variables in the database (Query-by-Relation), aiding in the rigorous evaluation or improvement of regression models. Case studies with multiple social scientists have demonstrated the usefulness and effectiveness of the SDR Querier in addressing daily challenges.},
  archive      = {J_TVCG},
  author       = {Yamei Tu and Olga Li and Junpeng Wang and Han-Wei Shen and Przemek Powałko and Irina Tomescu-Dubrow and Kazimierz M. Slomczynski and Spyros Blanas and J. Craig Jenkins},
  doi          = {10.1109/TVCG.2023.3261944},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2862-2874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SDRQuerier: A visual querying framework for cross-national survey data recycling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FraudAuditor: A visual analytics approach for collusive
fraud in health insurance. <em>TVCG</em>, <em>29</em>(6), 2849–2861. (<a
href="https://doi.org/10.1109/TVCG.2023.3261910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collusive fraud, in which multiple fraudsters collude to defraud health insurance funds, threatens the operation of the healthcare system. However, existing statistical and machine learning-based methods have limited ability to detect fraud in the scenario of health insurance due to the high similarity of fraudulent behaviors to normal medical visits and the lack of labeled data. To ensure the accuracy of the detection results, expert knowledge needs to be integrated with the fraud detection process. By working closely with health insurance audit experts, we propose FraudAuditor , a three-stage visual analytics approach to collusive fraud detection in health insurance. Specifically, we first allow users to interactively construct a co-visit network to holistically model the visit relationships of different patients. Second, an improved community detection algorithm that considers the strength of fraud likelihood is designed to detect suspicious fraudulent groups. Finally, through our visual interface, users can compare, investigate, and verify suspicious patient behavior with tailored visualizations that support different time scales. We conducted case studies in a real-world healthcare scenario, i.e., to help locate the actual fraud group and exclude the false positive group. The results and expert feedback proved the effectiveness and usability of the approach.},
  archive      = {J_TVCG},
  author       = {Jiehui Zhou and Xumeng Wang and Jie Wang and Hui Ye and Huanliang Wang and Zihan Zhou and Dongming Han and Haochao Ying and Jian Wu and Wei Chen},
  doi          = {10.1109/TVCG.2023.3261910},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2849-2861},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FraudAuditor: A visual analytics approach for collusive fraud in health insurance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Guest editors’ introduction: Special section on
IEEE PacificVis 2023. <em>TVCG</em>, <em>29</em>(6), 2847–2848. (<a
href="https://doi.org/10.1109/TVCG.2023.3265145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section of the IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) presents the five most highly rated papers from the 2023 IEEE Pacific Visualization Symposium (IEEE PacificVis), hosted in Seoul, Korea from April 18 to Apr 21, 2023. IEEE PacificVis, sponsored by the IEEE Visualization and Graphics Technical Committee (VGTC), aims to foster greater exchange between visualization researchers and practitioners, especially in the Asia-Pacific region. This forum has grown to be a truly international event, attracting submissions and attendees from many countries, not only in the Asia-Pacific but also in Europe, America, and beyond. Thus, IEEE PacificVis is serving the additional purpose of sharing the latest advances in the field of visualization with researchers and practitioners in the region and, also, introducing research developments from the region to the broader international visualization research community.},
  archive      = {J_TVCG},
  author       = {Jaegul Choo and Timo Ropinski and Yifan Hu},
  doi          = {10.1109/TVCG.2023.3265145},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2847-2848},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editorial: guest editors’ introduction: special section on IEEE PacificVis 2023},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IEEE VR 2023 message from the program chairs and guest
editors. <em>TVCG</em>, <em>29</em>(5), viii–ix. (<a
href="https://doi.org/10.1109/TVCG.2023.3249940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the top papers from the 30th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2023), held March 25–29, 2023, in Shanghai, China, in hybrid format.},
  archive      = {J_TVCG},
  author       = {Bobby Bodenheimer and Voicu Popescu and John Quarles and Lili Wang},
  doi          = {10.1109/TVCG.2023.3249940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {viii-ix},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2023 message from the program chairs and guest editors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). IEEE VR 2023 introducing the special issue. <em>TVCG</em>,
<em>29</em>(5), vii. (<a
href="https://doi.org/10.1109/TVCG.2023.3249919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 12th IEEE Transactions on Visualization and Computer Graphics (TVCG) special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 61 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2023), held in a hybrid style physically in Shanghai, China and virtually online, from March 25 to 29, 2023.},
  archive      = {J_TVCG},
  author       = {Han-Wei Shen and Kiyoshi Kiyokawa},
  doi          = {10.1109/TVCG.2023.3249919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {vii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2023 introducing the special issue},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ImTooth: Neural implicit tooth for dental augmented reality.
<em>TVCG</em>, <em>29</em>(5), 2837–2846. (<a
href="https://doi.org/10.1109/TVCG.2023.3247459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of augmented reality (AR) and medicine is an important trend in current research. The powerful display and interaction capabilities of the AR system can assist doctors to perform more complex operations. Since the tooth itself is an exposed rigid body structure, dental AR is a relatively hot research direction with application potential. However, none of the existing dental AR solutions are designed for wearable AR devices such as AR glasses. At the same time, these methods rely on high-precision scanning equipment or auxiliary positioning markers, which greatly increases the operational complexity and cost of clinical AR. In this work, we propose a simple and accurate neural-implicit model-driven dental AR system, named ImTooth, and adapted for AR glasses. Based on the modeling capabilities and differentiable optimization properties of state-of-the-art neural implicit representations, our system fuses reconstruction and registration in a single network, greatly simplifying the existing dental AR solutions and enabling reconstruction, registration, and interaction. Specifically, our method learns a scale-preserving voxel-based neural implicit model from multi-view images captured from a textureless plaster model of the tooth. Apart from color and surface, we also learn the consistent edge feature inside our representation. By leveraging the depth and edge information, our system can register the model to real images without additional training. In practice, our system uses a single Microsoft HoloLens 2 as the only sensor and display device. Experiments show that our method can reconstruct high-precision models and accomplish accurate registration. It is also robust to weak, repeating and inconsistent textures. We also show that our system can be easily integrated into dental diagnostic and therapeutic procedures, such as bracket placement guidance.},
  archive      = {J_TVCG},
  author       = {Hai Li and Hongjia Zhai and Xingrui Yang and Zhirong Wu and Yihao Zheng and Haofan Wang and Jianchao Wu and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2023.3247459},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2837-2846},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ImTooth: Neural implicit tooth for dental augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating both parallax and latency compensation into
video see-through head-mounted display. <em>TVCG</em>, <em>29</em>(5),
2826–2836. (<a href="https://doi.org/10.1109/TVCG.2023.3247460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a perspective-corrected video see-through mixed-reality head-mounted display with edge-preserving occlusion and low-latency capabilities. To realize the consistent spatial and temporal composition of a captured real world containing virtual objects, we perform three essential tasks: 1) to reconstruct captured images so as to match the user&#39;s view; 2) to occlude virtual objects with nearer real objects, to provide users with correct depth cues; and 3) to reproject the virtual and captured scenes to be matched and to keep up with users&#39; head motions. Captured image reconstruction and occlusion-mask generation require dense and accurate depth maps. However, estimating these maps is computationally difficult, which results in longer latencies. To obtain an acceptable balance between spatial consistency and low latency, we rapidly generated depth maps by focusing on edge smoothness and disocclusion (instead of fully accurate maps), to shorten the processing time. Our algorithm refines edges via a hybrid method involving infrared masks and color-guided filters, and it fills disocclusions using temporally cached depth maps. Our system combines these algorithms in a two-phase temporal warping architecture based upon synchronized camera pairs and displays. The first phase of warping is to reduce registration errors between the virtual and captured scenes. The second is to present virtual and captured scenes that correspond with the user&#39;s head motion. We implemented these methods on our wearable prototype and performed end-to-end measurements of its accuracy and latency. We achieved an acceptable latency due to head motion (less than 4 ms) and spatial accuracy (less than 0.1° in size and less than 0.3° in position) in our test environment. We anticipate that this work will help improve the realism of mixed reality systems.},
  archive      = {J_TVCG},
  author       = {Atsushi Ishihara and Hiroyuki Aga and Yasuko Ishihara and Hirotake Ichikawa and Hidetaka Kaji and Koichi Kawasaki and Daita Kobayashi and Toshimi Kobayashi and Ken Nishida and Takumi Hamasaki and Hideto Mori and Yuki Morikubo},
  doi          = {10.1109/TVCG.2023.3247460},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2826-2836},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integrating both parallax and latency compensation into video see-through head-mounted display},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-axis layered displays: Hybrid direct-view/near-eye mixed
reality with focus cues. <em>TVCG</em>, <em>29</em>(5), 2816–2825. (<a
href="https://doi.org/10.1109/TVCG.2023.3247077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces off-axis layered displays, the first approach to stereoscopic direct-view displays with support for focus cues. Off-axis layered displays combine a head-mounted display with a traditional direct-view display for encoding a focal stack and thus, for providing focus cues. To explore the novel display architecture, we present a complete processing pipeline for the real-time computation and post-render warping of off-axis display patterns. In addition, we build two prototypes using a head-mounted display in combination with a stereoscopic direct-view display, and a more widely available monoscopic direct-view display. In addition we show how extending off-axis layered displays with an attenuation layer and with eye-tracking can improve image quality. We thoroughly analyze each component in a technical evaluation and present examples captured through our prototypes.},
  archive      = {J_TVCG},
  author       = {Christoph Ebner and Peter Mohr and Tobias Langlotz and Yifan Peng and Dieter Schmalstieg and Gordon Wetzstein and Denis Kalkofen},
  doi          = {10.1109/TVCG.2023.3247077},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2816-2825},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Off-axis layered displays: Hybrid direct-View/Near-eye mixed reality with focus cues},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrowbarLimbs: A fatigue-reducing virtual reality text entry
metaphor. <em>TVCG</em>, <em>29</em>(5), 2806–2815. (<a
href="https://doi.org/10.1109/TVCG.2023.3247060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text entry remains challenging in virtual environments, where users may quickly experience physical fatigue in some body parts using existing methods. In this paper, we propose “CrowbarLimbs,” a novel virtual reality (VR) text entry metaphor with two deformable extended virtual limbs. By using a crowbar-like metaphor and placing the virtual keyboard at a user-preferred location based on the user&#39;s physical stature, our method can assist the user in placing their hands and arms in a comfortable posture, thus effectively reducing the physical fatigue in various body parts, such as hands, wrists, and elbows. In an initial user study, we found that CrowbarLimbs achieved text entry speed, accuracy, and system usability comparable to those of previous VR typing methods. To investigate the proposed metaphor in more depth, we further conducted two additional user studies to explore the ergonomically user-friendly shapes of CrowbarLimbs and virtual keyboard locations. The experimental results indicate that the shapes of CrowbarLimbs have significant effects on the fatigue ratings in various body parts and text entry speed. Furthermore, placing the virtual keyboard near the user and at half their height can lead to a satisfactory text entry rate of 28.37 words per minute.},
  archive      = {J_TVCG},
  author       = {Muhammad Abu Bakar and Yu-Ting Tsai and Hao-Han Hsueh and Elena Carolina Li},
  doi          = {10.1109/TVCG.2023.3247060},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2806-2815},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CrowbarLimbs: A fatigue-reducing virtual reality text entry metaphor},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assisted walking-in-place: Introducing assisted motion to
walking-by-cycling in embodied virtual reality. <em>TVCG</em>,
<em>29</em>(5), 2796–2805. (<a
href="https://doi.org/10.1109/TVCG.2023.3247070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the use of a motorized bike to support the walk of a self-avatar in virtual reality (VR). While existing walking-in-place (WIP) techniques render compelling walking experiences, they can be judged repetitive and strenuous. Our approach consists in assisting a WIP technique so that the user does not have to actively move in order to reduce effort and fatigue. We chose to assist a technique called walking-by-cycling, which consists in mapping the cycling motion of a bike onto the walking of the user&#39;s self-avatar, by using a motorized bike. We expected that our approach could provide participants with a compelling walking experience while reducing the effort required to navigate. We conducted a within-subjects study where we compared “assisted walking-by-cycling” to a traditional active walking-by-cycling implementation, and to a standard condition where the user is static. In the study, we measured embodiment, including ownership and agency, walking sensation, perceived effort and fatigue. Results showed that assisted walking-by-cycling induced more ownership, agency, and walking sensation than the static simulation. Additionally, assisted walking-by-cycling induced levels of ownership and walking sensation similar to that of active walking-by-cycling, but it induced less perceived effort. Taken together, this work promotes the use of assisted walking-by-cycling in situations where users cannot or do not want to exert much effort while walking in embodied VR such as for injured or disabled users, for prolonged uses, medical rehabilitation, or virtual visits.},
  archive      = {J_TVCG},
  author       = {Yann Moullec and Mélanie Cogné and Justine Saint-Aubert and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2023.3247070},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2796-2805},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Assisted walking-in-place: Introducing assisted motion to walking-by-cycling in embodied virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using virtual replicas to improve mixed reality remote
collaboration. <em>TVCG</em>, <em>29</em>(5), 2785–2795. (<a
href="https://doi.org/10.1109/TVCG.2023.3247113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore how virtual replicas can enhance Mixed Reality (MR) remote collaboration with a 3D reconstruction of the task space. People in different locations may need to work together remotely on complicated tasks. For example, a local user could follow a remote expert&#39;s instructions to complete a physical task. However, it could be challenging for the local user to fully understand the remote expert&#39;s intentions without effective spatial referencing and action demonstration. In this research, we investigate how virtual replicas can work as a spatial communication cue to improve MR remote collaboration. This approach segments the foreground manipulable objects in the local environment and creates corresponding virtual replicas of physical task objects. The remote user can then manipulate these virtual replicas to explain the task and guide their partner. This enables the local user to rapidly and accurately understand the remote expert&#39;s intentions and instructions. Our user study with an object assembly task found that using virtual replica manipulation was more efficient than using 3D annotation drawing in an MR remote collaboration scenario. We report and discuss the findings and limitations of our system and study, and present directions for future research.},
  archive      = {J_TVCG},
  author       = {Huayuan Tian and Gun A. Lee and Huidong Bai and Mark Billinghurst},
  doi          = {10.1109/TVCG.2023.3247113},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2785-2795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using virtual replicas to improve mixed reality remote collaboration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving datasets of eye-tracking samples with
applications in XR. <em>TVCG</em>, <em>29</em>(5), 2774–2784. (<a
href="https://doi.org/10.1109/TVCG.2023.3247048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual and mixed-reality (XR) technology has advanced significantly in the last few years and will enable the future of work, education, socialization, and entertainment. Eye-tracking data is required for supporting novel modes of interaction, animating virtual avatars, and implementing rendering or streaming optimizations. While eye tracking enables many beneficial applications in XR, it also introduces a risk to privacy by enabling re-identification of users. We applied privacy definitions of k-anonymity and plausible deniability (PD) to datasets of eye-tracking samples and evaluated them against the state-of-the-art differential privacy (DP) approach. Two VR datasets were processed to reduce identification rates while minimizing the impact on the performance of trained machine-learning models. Our results suggest that both PD and DP mechanisms produced practical privacy-utility trade-offs with respect to re-identification and activity classification accuracy, while k-anonymity performed best at retaining utility for gaze prediction.},
  archive      = {J_TVCG},
  author       = {Brendan David-John and Kevin Butler and Eakta Jain},
  doi          = {10.1109/TVCG.2023.3247048},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2774-2784},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Privacy-preserving datasets of eye-tracking samples with applications in XR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The design space of the auditory representation of objects
and their behaviours in virtual reality for blind people. <em>TVCG</em>,
<em>29</em>(5), 2763–2773. (<a
href="https://doi.org/10.1109/TVCG.2023.3247094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual reality (VR) is typically designed in terms of visual experience, it poses major challenges for blind people to understand and interact with the environment. To address this, we propose a design space to explore how to augment objects and their behaviours in VR with a nonvisual audio representation. It intends to support designers in creating accessible experiences by explicitly considering alternative representations to visual feedback. To demonstrate its potential, we recruited 16 blind users and explored the design space under two scenarios in the context of boxing: understanding the location of objects (the opponent&#39;s defensive stance) and their movement (opponent&#39;s punches). We found that the design space enables the exploration of multiple engaging approaches for the auditory representation of virtual objects. Our findings depicted shared preferences but no one-size-fits-all solution, suggesting the need to understand the consequences of each design choice and their impact on the individual user experience.},
  archive      = {J_TVCG},
  author       = {João Guerreiro and Yujin Kim and Rodrigo Nogueira and SeungA Chung and André Rodrigues and Uran Oh},
  doi          = {10.1109/TVCG.2023.3247094},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2763-2773},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The design space of the auditory representation of objects and their behaviours in virtual reality for blind people},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic redirection for VR haptics with a handheld stick.
<em>TVCG</em>, <em>29</em>(5), 2753–2762. (<a
href="https://doi.org/10.1109/TVCG.2023.3247047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general handheld stick haptic redirection method that allows the user to experience complex shapes with haptic feedback through both tapping and extended contact, such as in contour tracing. As the user extends the stick to make contact with a virtual object, the contact point with the virtual object and the targeted contact point with the physical object are continually updated, and the virtual stick is redirected to synchronize the virtual and real contacts. Redirection is applied either just to the virtual stick, or to both the virtual stick and hand. A user study (N = 26) confirms the effectiveness of the proposed redirection method. A first experiment following a two-interval forced-choice design reveals that the offset detection thresholds are [−15cm, +15cm]. A second experiment asks participants to guess the shape of an invisible virtual object by tapping it and by tracing its contour with the handheld stick, using a real world disk as a source of passive haptic feedback. The experiment reveals that using our haptic redirection method participants can identify the invisible object with 78\% accuracy.},
  archive      = {J_TVCG},
  author       = {Yuqi Zhou and Voicu Popescu},
  doi          = {10.1109/TVCG.2023.3247047},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2753-2762},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic redirection for VR haptics with a handheld stick},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When tangibles become deformable: Studying pseudo-stiffness
perceptual thresholds in a VR grasping task. <em>TVCG</em>,
<em>29</em>(5), 2743–2752. (<a
href="https://doi.org/10.1109/TVCG.2023.3247083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-Haptic techniques, or visuo-haptic illusions, leverage user&#39;s visual dominance over haptics to alter the users&#39; perception. As they create a discrepancy between virtual and physical interactions, these illusions are limited to a perceptual threshold. Many haptic properties have been studied using pseudo-haptic techniques, such as weight, shape or size. In this paper, we focus on estimating the perceptual thresholds for pseudo-stiffness in a virtual reality grasping task. We conducted a user study (n = 15) where we estimated if compliance can be induced on a non-compressible tangible object and to what extent. Our results show that (1) compliance can be induced in a rigid tangible object and that (2) pseudo-haptics can simulate beyond 24 N/cm stiffness (k &gt; 24N/cm, between a gummy bear and a raisin, up to rigid objects). Pseudo-stiffness efficiency is (3) enhanced by the objects&#39; scales, but mostly (4) correlated to the user input force. Taken altogether, our results offer novel opportunities to simplify the design of future haptic interfaces, and extend the haptic properties of passive props in VR.},
  archive      = {J_TVCG},
  author       = {Elodie Bouzbib and Claudio Pacchierotti and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2023.3247083},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2743-2752},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {When tangibles become deformable: Studying pseudo-stiffness perceptual thresholds in a VR grasping task},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRFPlayer: A streamable dynamic scene representation with
decomposed neural radiance fields. <em>TVCG</em>, <em>29</em>(5),
2732–2742. (<a href="https://doi.org/10.1109/TVCG.2023.3247082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and interactive rendering. Project website: https://bit.ly/nerfplayer.},
  archive      = {J_TVCG},
  author       = {Liangchen Song and Anpei Chen and Zhong Li and Zhang Chen and Lele Chen and Junsong Yuan and Yi Xu and Andreas Geiger},
  doi          = {10.1109/TVCG.2023.3247082},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2732-2742},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRFPlayer: A streamable dynamic scene representation with decomposed neural radiance fields},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing the effects of visual realism on size perception
in VR versus real world viewing through physical and verbal judgments.
<em>TVCG</em>, <em>29</em>(5), 2721–2731. (<a
href="https://doi.org/10.1109/TVCG.2023.3247109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) is well-known for its use in interdisciplinary applications and research. The visual representation of these applications could vary depending in their purpose and hardware limitation, and in those situations could require an accurate perception of size for task performance. However, the relationship between size perception and visual realism in VR has not yet been explored. In this contribution, we conducted an empirical evaluation using a between-subject design over four conditions of visual realism, namely Realistic, Local Lighting, Cartoon, and Sketch on size perception of target objects in the same virtual environment. Additionally, we gathered participants&#39; size estimates in the real world via a within-subject session. We measured size perception using concurrent verbal reports and physical judgments. Our result showed that although participants&#39; size perception was accurate in the realistic condition, surprisingly they could still tune into the invariant but meaningful information in the environment to accurately estimate the size of targets in the non-photorealistic conditions as well. We additionally found that size estimates in verbal and physical responses were generally different in real world and VR viewing and were moderated by trial presentation over time and target object widths.},
  archive      = {J_TVCG},
  author       = {Ignatius Alex Wijayanto and Sabarish V. Babu and Christopher C. Pagano and Jung Hong Chuang},
  doi          = {10.1109/TVCG.2023.3247109},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2721-2731},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing the effects of visual realism on size perception in VR versus real world viewing through physical and verbal judgments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating augmented reality landmark cues and frame of
reference displays with virtual reality. <em>TVCG</em>, <em>29</em>(5),
2710–2720. (<a href="https://doi.org/10.1109/TVCG.2023.3247078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily travel usually demands navigation on foot across a variety of different application domains, including tasks like search and rescue or commuting. Head-mounted augmented reality (AR) displays provide a preview of future navigation systems on foot, but designing them is still an open problem. In this paper, we look at two choices that such AR systems can make for navigation: 1) whether to denote landmarks with AR cues and 2) how to convey navigation instructions. Specifically, instructions can be given via a head-referenced display (screen-fixed frame of reference) or by giving directions fixed to global positions in the world (world-fixed frame of reference). Given limitations with the tracking stability, field of view, and brightness of most currently available head-mounted AR displays for lengthy routes outdoors, we decided to simulate these conditions in virtual reality. In the current study, participants navigated an urban virtual environment and their spatial knowledge acquisition was assessed. We experimented with whether or not landmarks in the environment were cued, as well as how navigation instructions were displayed (i.e., via screen-fixed or world-fixed directions). We found that the world-fixed frame of reference resulted in better spatial learning when there were no landmarks cued; adding AR landmark cues marginally improved spatial learning in the screen-fixed condition. These benefits in learning were also correlated with participants&#39; reported sense of direction. Our findings have implications for the design of future cognition-driven navigation systems.},
  archive      = {J_TVCG},
  author       = {Yu Zhao and Jeanine Stefanucci and Sarah Creem-Regehr and Bobby Bodenheimer},
  doi          = {10.1109/TVCG.2023.3247078},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2710-2720},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating augmented reality landmark cues and frame of reference displays with virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Add-on occlusion: Turning off-the-shelf optical see-through
head-mounted displays occlusion-capable. <em>TVCG</em>, <em>29</em>(5),
2700–2709. (<a href="https://doi.org/10.1109/TVCG.2023.3247064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occlusion-capable optical see-through head-mounted display (OC-OSTHMD) is actively developed in recent years since it allows mutual occlusion between virtual objects and the physical world to be correctly presented in augmented reality (AR). However, implementing occlusion with the special type of OSTHMDs prevents the appealing feature from the wide application. In this paper, a novel approach for realizing mutual occlusion for common OSTHMDs is proposed. A wearable device with per-pixel occlusion capability is designed. OSTHMD devices are upgraded to be occlusion-capable by attaching the device before optical combiners. A prototype with HoloLens 1 is built. The virtual display with mutual occlusion is demonstrated in real-time. A color correction algorithm is proposed to mitigate the color aberration caused by the occlusion device. Potential applications, including the texture replacement of real objects and the more realistic semi-transparent objects display, are demonstrated. The proposed system is expected to realize a universal implementation of mutual occlusion in AR.},
  archive      = {J_TVCG},
  author       = {Yan Zhang and Xiaodan Hu and Kiyoshi Kiyokawa and Xubo Yang},
  doi          = {10.1109/TVCG.2023.3247064},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2700-2709},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Add-on occlusion: Turning off-the-shelf optical see-through head-mounted displays occlusion-capable},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Masked360: Enabling robust 360-degree video streaming with
ultra low bandwidth consumption. <em>TVCG</em>, <em>29</em>(5),
2690–2699. (<a href="https://doi.org/10.1109/TVCG.2023.3247076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360-degree video streaming has gained tremendous growth over the past years. However, the delivery of 360-degree videos over the Internet still suffers from the scarcity of network bandwidth and adverse network conditions (e.g., packet loss, delay). In this paper, we propose a practical neural-enhanced 360-degree video streaming framework called Masked360, which can significantly reduce bandwidth consumption and achieve robustness against packet loss. In Masked360, instead of transmitting the complete video frame, the video server only transmits a masked low-resolution version of each video frame to reduce bandwidth significantly. When delivering masked video frames, the video server also sends a lightweight neural network model called MaskedEncoder to clients. Upon receiving masked frames, the client can reconstruct the original 360-degree video frames and start playback. To further improve the quality of video streaming, we also propose a set of optimization techniques, such as complexity-based patch selection, quarter masking strategy, redundant patch transmission and enhanced model training methods. In addition to bandwidth savings, Masked360 is also robust to packet loss during the transmission, because packet losses can be concealed by the reconstruction operation performed by the MaskedEncoder. Finally, we implement the whole Masked360 framework and evaluate its performance using real datasets. The experimental results show that Masked360 can achieve 4K 360-degree video streaming with bandwidth as low as 2.4 Mbps. Besides, video quality of Masked360 is also improved significantly, with an improvement of 5.24-16.61\% in terms of PSNR and 4.74-16.15\% in terms of SSIM compared to other baselines.},
  archive      = {J_TVCG},
  author       = {Zhenxiao Luo and Baili Chai and Zelong Wang and Miao Hu and Di Wu},
  doi          = {10.1109/TVCG.2023.3247076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2690-2699},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Masked360: Enabling robust 360-degree video streaming with ultra low bandwidth consumption},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring plausibility and presence in mixed reality
experiences. <em>TVCG</em>, <em>29</em>(5), 2680–2689. (<a
href="https://doi.org/10.1109/TVCG.2023.3247046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality (MR) applications along Milgram&#39;s Reality-Virtuality (RV) continuum motivated a number of recent theories on potential constructs and factors describing MR experiences. This paper investigates the impact of incongruencies that are processed on different information processing layers (i.e., sensation/perception and cognition layer) to provoke breaks in plausibility. It examines the effects on spatial and overall presence as prominent constructs of Virtual Reality (VR). We developed a simulated maintenance application to test virtual electrical devices. Participants performed test operations on these devices in a counterbalanced, randomized 2×2 between-subject design in either VR as congruent or Augmented Reality (AR) as incongruent on the sensation/perception layer. Cognitive incongruence was induced by the absence of traceable power outages, decoupling perceived cause and effect after activating potentially defective devices. Our results indicate that the effects of the power outages differ significantly in the perceived plausibility and spatial presence ratings between VR and AR. Both ratings decreased for the AR condition (incongruent sensation/perception) compared to VR (congruent sensation/perception) for the congruent cognitive case but increased for the incongruent cognitive case. The results are discussed and put into perspective in the scope of recent theories of MR experiences.},
  archive      = {J_TVCG},
  author       = {Franziska Westermeier and Larissa Brübach and Marc Erich Latoschik and Carolin Wienrich},
  doi          = {10.1109/TVCG.2023.3247046},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2680-2689},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring plausibility and presence in mixed reality experiences},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GroundFlow: Liquid-based haptics for simulating fluid on the
ground in virtual reality. <em>TVCG</em>, <em>29</em>(5), 2670–2679. (<a
href="https://doi.org/10.1109/TVCG.2023.3247073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing haptic devices to enhance the immersive experience is a direct approach in virtual reality (VR) applications. Various studies develop haptic feedback using force, wind, and thermal mechanisms. However, most haptic devices simulate feedback in dry environments such as the living room, prairie, or city. Water-related environments are thus less explored, for example, rivers, beaches, and swimming pools. In this paper we present GroundFlow, a liquid-based haptic floor system for simulating fluid on the ground in VR. We discuss design considerations and propose a system architecture and interaction design. We conduct two user studies to assist in the design of a multiple-flow feedback mechanism, develop three applications to explore the potential uses of the mechanism, and consider the limitations and challenges thereof to inform VR developers and haptic practitioners.},
  archive      = {J_TVCG},
  author       = {Ping-Hsuan Han and Tzu-Hua Wang and Chien-Hsing Chou},
  doi          = {10.1109/TVCG.2023.3247073},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2670-2679},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GroundFlow: Liquid-based haptics for simulating fluid on the ground in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text input for non-stationary XR workspaces: Investigating
tap and word-gesture keyboards in virtual and augmented reality.
<em>TVCG</em>, <em>29</em>(5), 2658–2669. (<a
href="https://doi.org/10.1109/TVCG.2023.3247098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article compares two state-of-the-art text input techniques between non-stationary virtual reality (VR) and video see-through augmented reality (VST AR) use-cases as XR display condition. The developed contact-based mid-air virtual tap and word-gesture (swipe) keyboard provide established support functions for text correction, word suggestions, capitalization, and punctuation. A user evaluation with 64 participants revealed that XR displays and input techniques strongly affect text entry performance, while subjective measures are only influenced by the input techniques. We found significantly higher usability and user experience ratings for tap keyboards compared to swipe keyboards in both VR and VST AR. Task load was also lower for tap keyboards. In terms of performance, both input techniques were significantly faster in VR than in VST AR. Further, the tap keyboard was significantly faster than the swipe keyboard in VR. Participants showed a significant learning effect with only ten sentences typed per condition. Our results are consistent with previous work in VR and optical see-through (OST) AR, but additionally provide novel insights into usability and performance of the selected text input techniques for VST AR. The significant differences in subjective and objective measures emphasize the importance of specific evaluations for each possible combination of input techniques and XR displays to provide reusable, reliable, and high-quality text input solutions. With our work, we form a foundation for future research and XR workspaces. Our reference implementation is publicly available to encourage replicability and reuse in future XR workspaces.},
  archive      = {J_TVCG},
  author       = {Florian Kern and Florian Niebling and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2023.3247098},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2658-2669},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text input for non-stationary XR workspaces: Investigating tap and word-gesture keyboards in virtual and augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConeSpeech: Exploring directional speech interaction for
multi-person remote communication in virtual reality. <em>TVCG</em>,
<em>29</em>(5), 2647–2657. (<a
href="https://doi.org/10.1109/TVCG.2023.3247085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.},
  archive      = {J_TVCG},
  author       = {Yukang Yan and Haohua Liu and Yingtian Shi and Jingying Wang and Ruici Guo and Zisu Li and Xuhai Xu and Chun Yu and Yuntao Wang and Yuanchun Shi},
  doi          = {10.1109/TVCG.2023.3247085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2647-2657},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ConeSpeech: Exploring directional speech interaction for multi-person remote communication in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monte-carlo redirected walking: Gain selection through
simulated walks. <em>TVCG</em>, <em>29</em>(5), 2637–2646. (<a
href="https://doi.org/10.1109/TVCG.2023.3247093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Monte-Carlo Redirected Walking (MCRDW), a gain selection algorithm for redirected walking. MCRDW applies the Monte-Carlo method to redirected walking by simulating a large number of simple virtual walks, then inversely applying redirection to the virtual paths. Different gain levels and directions are applied, producing differing physical paths. Each physical path is scored and the results used to select the best gain level and direction. We provide a simple example implementation and a simulation-based study for validation. In our study, when compared with the next best technique, MCRDW reduced incidence of boundary collisions by over 50\% while reducing total rotation and position gain.},
  archive      = {J_TVCG},
  author       = {Ben J. Congdon and Anthony Steed},
  doi          = {10.1109/TVCG.2023.3247093},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2637-2646},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Monte-carlo redirected walking: Gain selection through simulated walks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Body and time: Virtual embodiment and its effect on time
perception. <em>TVCG</em>, <em>29</em>(5), 2626–2636. (<a
href="https://doi.org/10.1109/TVCG.2023.3247040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the effect of one&#39;s body representation on time perception. Time Perception is modulated by a variety of factors including, e.g., the current situation or activity, it can display significant disturbances caused by psychological disorders, and it is influenced by emotional and interoceptive states, i.e., “the sense of the physiological condition of the body”. We investigated this relation between one&#39;s own body and the perception of time in a novel Virtual Reality (VR) experiment explicitly fostering user activity. Forty-Eight participants randomly experienced different degrees of embodiment: i) without an avatar (low), ii) with hands (medium), and iii) with a high-quality avatar (high). Participants had to repeatedly activate a virtual lamp and estimate the duration of time intervals as well as judge the passage of time. Our results show a significant effect of embodiment on time perception: time passes slower in the low embodiment condition compared to the medium and high conditions. In contrast to prior work, the study provides missing evidence that this effect is independent of the level of activity of participants: In our task, users were prompted to repeatedly perform body actions, thereby ruling-out a potential influence of the level of activity. Importantly, duration judgements in both the millisecond and minute ranges seemed unaffected by variations in embodiment. Taken together, these results lead to a better understanding of the relationship between the body and time.},
  archive      = {J_TVCG},
  author       = {Fabian Unruh and David Vogel and Maximilian Landeck and Jean-Luc Lugrin and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2023.3247040},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2626-2636},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Body and time: Virtual embodiment and its effect on time perception},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CardioGenesis4D: Interactive morphological transitions of
embryonic heart development in a virtual learning environment.
<em>TVCG</em>, <em>29</em>(5), 2615–2625. (<a
href="https://doi.org/10.1109/TVCG.2023.3247110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the embryonic human heart, complex dynamic shape changes take place in a short period of time on a microscopic scale, making this development difficult to visualize. However, spatial understanding of these processes is essential for students and future cardiologists to properly diagnose and treat congenital heart defects. Following a user centered approach, the most crucial embryological stages were identified and translated into a virtual reality learning environment (VRLE) to enable the understanding of the morphological transitions of these stages through advanced interactions. To address individual learning types, we implemented different features and evaluated the application regarding usability, perceived task load, and sense of presence in a user study. We also assessed spatial awareness and knowledge gain, and finally obtained feedback from domain experts. Overall, students and professionals rated the application positively. To minimize distraction from interactive learning content, such VRLEs should consider features for different learning types, allow for gradual habituation, and at the same time provide enough playful stimuli. Our work previews how VR can be integrated into a cardiac embryology education curriculum.},
  archive      = {J_TVCG},
  author       = {Danny Schott and Matthias Kunz and Tom Wunderling and Florian Heinrich and Rüdiger Braun-Dullaeus and Christian Hansen},
  doi          = {10.1109/TVCG.2023.3247110},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2615-2625},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CardioGenesis4D: Interactive morphological transitions of embryonic heart development in a virtual learning environment},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of the visual fidelity of virtual environments on
presence, context-dependent forgetting, and source-monitoring error.
<em>TVCG</em>, <em>29</em>(5), 2607–2614. (<a
href="https://doi.org/10.1109/TVCG.2023.3247063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in virtual reality technology have enabled the creation of virtual environments (VEs) with significantly high visual fidelity when compared to real environments (REs). In this study, we use a high-fidelity VE to examine two effects caused by alternating VE and RE experiences: “context-dependent forgetting” and “source-monitoring errors.” The former effect is that memories learned in VEs are more easily recalled in VEs than in REs, whereas memories learned in REs are more easily recalled in REs than in VEs. The source-monitoring error is that memories learned in VEs are easily confused with those learned in REs, making discriminating the source of the memory difficult. We hypothesized that the visual fidelity of VEs is responsible for these effects and conducted an experiment using two types of VEs: a high-fidelity VE created using photogrammetry techniques and low-fidelity VE created with primitive shapes and materials. The results show that the high-fidelity VE significantly improved the sense of presence. However, the level of the visual fidelity of the VEs did not show any effect on context-dependent forgetting and source-monitoring errors. Notably, the null results of the context-dependent forgetting between the VE and RE were strongly supported by Bayesian analysis. Thus, we indicate that context-dependent forgetting does not necessarily occur, which will be helpful for VR-based education and training.},
  archive      = {J_TVCG},
  author       = {Takato Mizuho and Takuji Narumi and Hideaki Kuzuoka},
  doi          = {10.1109/TVCG.2023.3247063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2607-2614},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of the visual fidelity of virtual environments on presence, context-dependent forgetting, and source-monitoring error},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic review on the visualization of avatars and
agents in AR &amp; VR displayed using head-mounted displays.
<em>TVCG</em>, <em>29</em>(5), 2596–2606. (<a
href="https://doi.org/10.1109/TVCG.2023.3247072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) and Virtual Reality (VR) are pushing from the labs towards consumers, especially with social applications. These applications require visual representations of humans and intelligent entities. However, displaying and animating photo-realistic models comes with a high technical cost while low-fidelity representations may evoke eeriness and overall could degrade an experience. Thus, it is important to carefully select what kind of avatar to display. This article investigates the effects of rendering style and visible body parts in AR and VR by adopting a systematic literature review. We analyzed 72 papers that compare various avatar representations. Our analysis includes an outline of the research published between 2015 and 2022 on the topic of avatars and agents in AR and VR displayed using head-mounted displays, covering aspects like visible body parts (e.g., hands only, hands and head, full-body) and rendering style (e.g., abstract, cartoon, realistic); an overview of collected objective and subjective measures (e.g., task performance, presence, user experience, body ownership); and a classification of tasks where avatars and agents were used into task domains (physical activity, hand interaction, communication, game-like scenarios, and education/training). We discuss and synthesize our results within the context of today&#39;s AR and VR ecosystem, provide guidelines for practitioners, and finally identify and present promising research opportunities to encourage future research of avatars and agents in AR/VR environments.},
  archive      = {J_TVCG},
  author       = {Florian Weidner and Gerd Boettcher and Stephanie Arevalo Arboleda and Chenyao Diao and Luljeta Sinani and Christian Kunert and Christoph Gerhardt and Wolfgang Broll and Alexander Raake},
  doi          = {10.1109/TVCG.2023.3247072},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2596-2606},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A systematic review on the visualization of avatars and agents in AR &amp; VR displayed using head-mounted displays},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GeoSynth: A photorealistic synthetic indoor dataset for
scene understanding. <em>TVCG</em>, <em>29</em>(5), 2586–2595. (<a
href="https://doi.org/10.1109/TVCG.2023.3247087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized many scene perception tasks over the past decade. Some of these improvements can be attributed to the development of large labeled datasets. The creation of such datasets can be an expensive, time-consuming, and imperfect process. To address these issues, we introduce GeoSynth, a diverse photorealistic synthetic dataset for indoor scene understanding tasks. Each GeoSynth exemplar contains rich labels including segmentation, geometry, camera parameters, surface material, lighting, and more. We demonstrate that supplementing real training data with GeoSynth can significantly improve network performance on perception tasks, like semantic segmentation. A subset of our dataset will be made publicly available at https://github.com/geomagical/GeoSynth.},
  archive      = {J_TVCG},
  author       = {Brian Pugh and Davin Chernak and Salma Jiddi},
  doi          = {10.1109/TVCG.2023.3247087},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2586-2595},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeoSynth: A photorealistic synthetic indoor dataset for scene understanding},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skeleton-based human action recognition via large-kernel
attention graph convolutional network. <em>TVCG</em>, <em>29</em>(5),
2575–2585. (<a href="https://doi.org/10.1109/TVCG.2023.3247075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The skeleton-based human action recognition has broad application prospects in the field of virtual reality, as skeleton data is more resistant to data noise such as background interference and camera angle changes. Notably, recent works treat the human skeleton as a non-grid representation, e.g., skeleton graph, then learns the spatio-temporal pattern via graph convolution operators. Still, the stacked graph convolution plays a marginal role in modeling long-range dependences that may contain crucial action semantic cues. In this work, we introduce a skeleton large kernel attention operator (SLKA), which can enlarge the receptive field and improve channel adaptability without increasing too much computational burden. Then a spatiotemporal SLKA module (ST-SLKA) is integrated, which can aggregate long-range spatial features and learn long-distance temporal correlations. Further, we have designed a novel skeleton-based action recognition network architecture called the spatiotemporal large-kernel attention graph convolution network (LKA-GCN). In addition, large-movement frames may carry significant action information. This work proposes a joint movement modeling strategy (JMM) to focus on valuable temporal interactions. Ultimately, on the NTU-RGBD 60, NTU-RGBD 120 and Kinetics-Skeleton 400 action datasets, the performance of our LKA-GCN has achieved a state-of-the-art level.},
  archive      = {J_TVCG},
  author       = {Yanan Liu and Hao Zhang and Yanqiu Li and Kangjian He and Dan Xu},
  doi          = {10.1109/TVCG.2023.3247075},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2575-2585},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Skeleton-based human action recognition via large-kernel attention graph convolutional network},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). I can’t see that! Considering the readability of small
objects in virtual environments. <em>TVCG</em>, <em>29</em>(5),
2567–2574. (<a href="https://doi.org/10.1109/TVCG.2023.3247468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though virtual reality has repeatedly seen usability improvements through higher fidelity headsets, interacting with small objects has remained an issue due to a reduction in visual acuity. Given the current uptake of virtual reality platforms and the range of real world applications that they may be used for, it is worth considering how such interactions can be accounted for. We propose three techniques for improving the usability of small objects in virtual environments: i) expanding them in place, ii) showing a zoomed-in twin above the original object, and iii) showing a large readout of the object&#39;s current state. We conducted a study comparing each technique&#39;s usability, induced presence, and effect on short-term knowledge retention in a VR training scenario that simulated the common geoscience exercise of measuring strike and dip. Participant feedback highlighted the need for this research, however simply scaling the area of interest may not be enough to improve the usability of information-bearing objects, while displaying this information in large text format can make tasks faster to complete at the cost of reducing the user&#39;s ability to transfer knowledge they&#39;ve learned to the real world. We discuss these results and their implications for the design of future virtual reality experiences.},
  archive      = {J_TVCG},
  author       = {Jacob Young and Nadia Pantidi and Matthew Wood},
  doi          = {10.1109/TVCG.2023.3247468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2567-2574},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {I can&#39;t see that! considering the readability of small objects in virtual environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inward VR: Toward a qualitative method for investigating
interoceptive awareness in VR. <em>TVCG</em>, <em>29</em>(5), 2557–2566.
(<a href="https://doi.org/10.1109/TVCG.2023.3247074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {lmmersive virtual reality (VR) technologies can produce powerful illusions of being in another place or inhabiting another body, and theories of presence and embodiment provide valuable guidance to designers of VR applications that use these illusions to “take us elsewhere.” However, an increasingly common design goal for VR experiences is to develop a deeper awareness of the internal landscape of one&#39;s own body (i.e., interoceptive awareness); here, design guidelines and evaluative techniques are less clear. To address this, we present a methodology, including a reusable codebook, for adapting the five dimensions of the Multidimensional Assessment of Interoceptive Awareness (MAIA) conceptual framework to explore interoceptive awareness in VR experiences via qualitative interviews. We report results from a first exploratory study (n=21) applying this method to understand the interoceptive experiences of users in a VR environment. The environment includes a guided body scan exercise with a motion-tracked avatar visible in a virtual mirror and an interactive visualization of a biometric signal detected via a heartbeat sensor. The results provide new insights on how this example VR experience might be refined to better support interoceptive awareness and how the methodology might continue to be refined for understanding other “inward-facing” VR experiences.},
  archive      = {J_TVCG},
  author       = {Alexander C. Haley and Don Thorpe and Alex Pelletier and Svetlana Yarosh and Daniel F. Keefe},
  doi          = {10.1109/TVCG.2023.3247074},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2557-2566},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Inward VR: Toward a qualitative method for investigating interoceptive awareness in VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introducing 3D thumbnails to access 360-degree videos in
virtual reality. <em>TVCG</em>, <em>29</em>(5), 2547–2556. (<a
href="https://doi.org/10.1109/TVCG.2023.3247462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° videos provide an immersive experience, especially when watched in virtual reality (VR). Yet, even though the video data is inherently three-dimensional, interfaces to access datasets of such videos in VR almost always use two-dimensional thumbnails shown in a grid on a flat or curved plane. We claim that using spherical and cube-shaped 3D thumbnails may provide a better user experience and be more effective at conveying the high-level subject matter of a video or when searching for a specific item in it. A comparative study against the most used existing representation, that is, 2D equirectangular projections, showed that the spherical 3D thumbnails did indeed provide the best user experience, whereas traditional 2D equirectangular projections still performed better for high-level classification tasks. Yet, they were outperformed by spherical thumbnails when participants had to search for details within the videos. Our results thus confirm a potential benefit of 3D thumbnail representations for 360-degree videos in VR, especially with respect to user experience and detailed content search and suggest a mixed interface design providing both options to the users. Supplemental materials about the user study and used data are available at https://osf.io/5vk49/.},
  archive      = {J_TVCG},
  author       = {Alissa Vermast and Wolfgang Hürst},
  doi          = {10.1109/TVCG.2023.3247462},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2547-2556},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing 3D thumbnails to access 360-degree videos in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PACE: Data-driven virtual agent interaction in dense and
cluttered environments. <em>TVCG</em>, <em>29</em>(5), 2536–2546. (<a
href="https://doi.org/10.1109/TVCG.2023.3247054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1\% of the time versus the state-of-the-art method using existing motions, and 81.0\% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2\% in terms of the non-collision metric and by over 18\% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.mnd.edu/pace/},
  archive      = {J_TVCG},
  author       = {James F. Mullen and Dinesh Manocha},
  doi          = {10.1109/TVCG.2023.3247054},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2536-2546},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PACE: Data-driven virtual agent interaction in dense and cluttered environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotional voice puppetry. <em>TVCG</em>, <em>29</em>(5),
2527–2535. (<a href="https://doi.org/10.1109/TVCG.2023.3247101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents emotional voice puppetry, an audio-based facial animation approach to portray characters with vivid emotional changes. The lips motion and the surrounding facial areas are controlled by the contents of the audio, and the facial dynamics are established by category of the emotion and the intensity. Our approach is exclusive because it takes account of perceptual validity and geometry instead of pure geometric processes. Another highlight of our approach is the generalizability to multiple characters. The findings showed that training new secondary characters when the rig parameters are categorized as eye, eyebrows, nose, mouth, and signature wrinkles is significant in achieving better generalization results compared to joint training. User studies demonstrate the effectiveness of our approach both qualitatively and quantitatively. Our approach can be applicable in AR/VR and 3DUI, namely, virtual reality avatars/self-avatars, teleconferencing and in-game dialogue.},
  archive      = {J_TVCG},
  author       = {Ye Pan and Ruisi Zhang and Shengran Cheng and Shuai Tan and Yu Ding and Kenny Mitchell and Xubo Yang},
  doi          = {10.1109/TVCG.2023.3247101},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2527-2535},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Emotional voice puppetry},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to maximise spatial presence: Design guidelines for a
virtual learning environment for school use. <em>TVCG</em>,
<em>29</em>(5), 2517–2526. (<a
href="https://doi.org/10.1109/TVCG.2023.3247111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on learning with and in immersive virtual reality (VR) continues to grow, yielding more insights into how immersive learning works. However, the actual use of VR learning environments in schools is still in its infancy. A major hurdle that hinders the use of immersive digital media in schools is the lack of guidelines for designing VR learning environments for practical use in schools. Such guidelines need to consider how students interact and learn in VR learning environments and how teachers can use such environments on a day-to-day basis. Using a design-based research approach, we explored the guidelines for creating VR learning content for tenth-grade students in a German secondary school and recreated a real-world, out-of-school VR learning space which can be used for hands-on instruction. This paper investigated how to maximise the experience of spatial presence by creating a VR learning environment in several microcycles. Furthermore, it took a closer look at the influence of the spatial situation model and cognitive involvement on this process. The results were evaluated with ANOVAs and path analyses, showing, for example, that involvement does not influence spatial presence in highly immersive and realistic VR learning environments.},
  archive      = {J_TVCG},
  author       = {Marc Bastian Rieger and Björn Risch},
  doi          = {10.1109/TVCG.2023.3247111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2517-2526},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How to maximise spatial presence: Design guidelines for a virtual learning environment for school use},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wavelet-based fast decoding of 360° videos. <em>TVCG</em>,
<em>29</em>(5), 2508–2516. (<a
href="https://doi.org/10.1109/TVCG.2023.3247080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a wavelet-based video codec specifically designed for VR displays that enables real-time playback of high-resolution 360° videos. Our codec exploits the fact that only a fraction of the full 360° video frame is visible on the display at any time. To load and decode the video viewport-dependently in real time, we make use of the wavelet transform for intra- as well as inter-frame coding. Thereby, the relevant content is directly streamed from the drive, without the need to hold the entire frames in memory. With an average of 193 frames per second at 8192 × 8192 -pixel full-frame resolution, the conducted evaluation demonstrates that our codec&#39;s decoding performance is up to 272\% higher than that of the state-of-the-art video codecs H.265 and AV1 for typical VR displays. By means of a perceptual study, we further illustrate the necessity of high frame rates for a better VR experience. Finally, we demonstrate how our wavelet-based codec can also directly be used in conjunction with foveation for further performance increase.},
  archive      = {J_TVCG},
  author       = {Colin Groth and Sascha Fricke and Susana Castillo and Marcus Magnor},
  doi          = {10.1109/TVCG.2023.3247080},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2508-2516},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wavelet-based fast decoding of 360° videos},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GestureSurface: VR sketching through assembling scaffold
surface with non-dominant hand. <em>TVCG</em>, <em>29</em>(5),
2499–2507. (<a href="https://doi.org/10.1109/TVCG.2023.3247059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D sketching in virtual reality (VR) provides an immersive drawing experience for designs. However, due to the lack of depth perception cues in VR, scaffolding surfaces that constrain strokes to 2D are usually used as visual guides to reduce the difficulty of drawing accurate strokes. When the dominant hand is occupied by the pen tool, the efficiency of scaffolding-based sketching can be improved by using gesture input to reduce the idleness of the non-dominant hand. This paper presents GestureSurface, a bi-manual interface that uses non-dominant hand performing gestures to operate scaffolding and the other hand drawing with controller. We designed a set of non-dominant gestures to create and manipulate scaffolding surfaces, which are assembled by automatic combination based on five predefined primitive surfaces. We evaluated GestureSurface through a 20-person user study and found that the method of scaffolding-based sketching using non-dominant hand has the advantages of high efficiency and low fatigue.},
  archive      = {J_TVCG},
  author       = {Xinchi Xu and Yang Zhou and Bingchan Shao and Guihuan Feng and Chun Yu},
  doi          = {10.1109/TVCG.2023.3247059},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2499-2507},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GestureSurface: VR sketching through assembling scaffold surface with non-dominant hand},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The dating metaverse: Why we need to design for consent in
social VR. <em>TVCG</em>, <em>29</em>(5), 2489–2498. (<a
href="https://doi.org/10.1109/TVCG.2023.3247065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a participatory design study about how consent to interaction and observation of other users can be supported in social VR. We use emerging VR dating applications, colloquially called the dating metaverse, as context for study of harm-mitigative design structures in social VR given the evidence of harms that occur through dating apps and general social VR applications individually, and the harms that may occur through their convergence. Through design workshops with potential dating metaverse users in the Midwest United States (n=18) we elucidate nonconsensual experiences that should be prevented and participant-created designs for informing and exchanging consent in VR. We position consent as a valuable lens for which to design preventative solutions to harm in social VR by reframing harm as unwanted experiences that happen because of the absence of mechanics to support users in giving and denying agreement to a virtual experience before it occurs.},
  archive      = {J_TVCG},
  author       = {Douglas Zytko and Jonathan Chan},
  doi          = {10.1109/TVCG.2023.3247065},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2489-2498},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The dating metaverse: Why we need to design for consent in social VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect of frame rate on user experience, performance, and
simulator sickness in virtual reality. <em>TVCG</em>, <em>29</em>(5),
2478–2488. (<a href="https://doi.org/10.1109/TVCG.2023.3247057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The refresh rate of virtual reality (VR) head-mounted displays (HMDs) has been growing rapidly in recent years because of the demand to provide higher frame rate content as it is often linked with a better experience. Today&#39;s HMDs come with different refresh rates ranging from 20Hz to 180Hz, which determines the actual maximum frame rate perceived by users&#39; naked eyes. VR users and content developers often face a choice because having high frame rate content and the hardware that supports it comes with higher costs and other trade-offs (such as heavier and bulkier HMDs). Both VR users and developers can choose a suitable frame rate if they are aware of the benefits of different frame rates in user experience, performance, and simulator sickness (SS). To our knowledge, limited research on frame rate in VR HMDs is available. In this paper, we aim to fill this gap and report a study with two VR application scenarios that compared four of the most common and highest frame rates currently available (60, 90, 120, and 180 frames per second (fps)) to explore their effect on users&#39; experience, performance, and SS symptoms. Our results show that 120fps is an important threshold for VR. After 120fps, users tend to feel lower SS symptoms without a significant negative effect on their experience. Higher frame rates (e.g., 120 and 180fps) can ensure better user performance than lower rates. Interestingly, we also found that at 60fps and when users are faced with fast-moving objects, they tend to adopt a strategy to compensate for the lack of visual details by predicting or filling the gaps to try to meet the performance needs. At higher fps, users do not need to follow this compensatory strategy to meet the fast response performance requirements.},
  archive      = {J_TVCG},
  author       = {Jialin Wang and Rongkai Shi and Wenxuan Zheng and Weijie Xie and Dominic Kao and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2023.3247057},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2478-2488},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effect of frame rate on user experience, performance, and simulator sickness in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaining the high ground: Teleportation to mid-air targets in
immersive virtual environments. <em>TVCG</em>, <em>29</em>(5),
2467–2477. (<a href="https://doi.org/10.1109/TVCG.2023.3247114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most prior teleportation techniques in virtual reality are bound to target positions in the vicinity of selectable scene objects. In this paper, we present three adaptations of the classic teleportation metaphor that enable the user to travel to mid-air targets as well. Inspired by related work on the combination of teleports with virtual rotations, our three techniques differ in the extent to which elevation changes are integrated into the conventional target selection process. Elevation can be specified either simultaneously, as a connected second step, or separately from horizontal movements. A user study with 30 participants indicated a trade-off between the simultaneous method leading to the highest accuracy and the two-step method inducing the lowest task load as well as receiving the highest usability ratings. The separate method was least suitable on its own but could serve as a complement to one of the other approaches. Based on these findings and previous research, we define initial design guidelines for mid-air navigation techniques.},
  archive      = {J_TVCG},
  author       = {Tim Weissker and Pauline Bimberg and Aalok Shashidhar Gokhale and Torsten Kuhlen and Bernd Froehlich},
  doi          = {10.1109/TVCG.2023.3247114},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2467-2477},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaining the high ground: Teleportation to mid-air targets in immersive virtual environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A video-based augmented reality system for human-in-the-loop
muscle strength assessment of juvenile dermatomyositis. <em>TVCG</em>,
<em>29</em>(5), 2456–2466. (<a
href="https://doi.org/10.1109/TVCG.2023.3247092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100\% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system.},
  archive      = {J_TVCG},
  author       = {Kanglei Zhou and Ruizhi Cai and Yue Ma and Qingqing Tan and Xinning Wang and Jianguo Li and Hubert P. H. Shum and Frederick W. B. Li and Song Jin and Xiaohui Liang},
  doi          = {10.1109/TVCG.2023.3247092},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2456-2466},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A video-based augmented reality system for human-in-the-loop muscle strength assessment of juvenile dermatomyositis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study of change blindness in immersive environments.
<em>TVCG</em>, <em>29</em>(5), 2446–2455. (<a
href="https://doi.org/10.1109/TVCG.2023.3247102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human performance is poor at detecting certain changes in a scene, a phenomenon known as change blindness. Although the exact reasons of this effect are not yet completely understood, there is a consensus that it is due to our constrained attention and memory capacity: We create our own mental, structured representation of what surrounds us, but such representation is limited and imprecise. Previous efforts investigating this effect have focused on 2D images; however, there are significant differences regarding attention and memory between 2D images and the viewing conditions of daily life. In this work, we present a systematic study of change blindness using immersive 3D environments, which offer more natural viewing conditions closer to our daily visual experience. We devise two experiments; first, we focus on analyzing how different change properties (namely type, distance, complexity, and field of view) may affect change blindness. We then further explore its relation with the capacity of our visual working memory and conduct a second experiment analyzing the influence of the number of changes. Besides gaining a deeper understanding of the change blindness effect, our results may be leveraged in several VR applications such as redirected walking, games, or even studies on saliency or attention prediction.},
  archive      = {J_TVCG},
  author       = {Daniel Martin and Xin Sun and Diego Gutierrez and Belen Masia},
  doi          = {10.1109/TVCG.2023.3247102},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2446-2455},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A study of change blindness in immersive environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of AR visualization approaches for catheter
insertion into the ventricle cavity. <em>TVCG</em>, <em>29</em>(5),
2434–2445. (<a href="https://doi.org/10.1109/TVCG.2023.3247042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) has shown potential in computer-aided surgery. It allows for the visualization of hidden anatomical structures as well as assists in navigating and locating surgical instruments at the surgical site. Various modalities (devices and/or visualizations) have been used in the literature, but few studies investigated the adequacy/superiority of one modality over the other. For instance, the use of optical see-through (OST) HMDs has not always been scientifically justified. Our goal is to compare various visualization modalities for catheter insertion in external ventricular drain and ventricular shunt procedures. We investigate two AR approaches: (1) 2D approaches consisting of a smartphone and a 2D window visualized through an OST (Microsoft HoloLens 2), and (2) 3D approaches consisting of a fully aligned patient model and a model that is adjacent to the patient and is rotationally aligned using an OST. 32 participants joined this study. For each visualization approach, participants were asked to perform five insertions after which they filled NASA-TLX and SUS forms. Moreover, the position and orientation of the needle with respect to the planning during the insertion task were collected. The results show that participants achieved a better insertion performance significantly under 3D visualizations, and the NASA-TLX and SUS forms reflected the preference of participants for these approaches compared to 2D approaches.},
  archive      = {J_TVCG},
  author       = {Mohamed Benmahdjoub and Abdullah Thabit and Marie-Lise C. van Veelen and Wiro J. Niessen and Eppo B. Wolvius and Theo van Walsum},
  doi          = {10.1109/TVCG.2023.3247042},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2434-2445},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluation of AR visualization approaches for catheter insertion into the ventricle cavity},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eating, smelling, and seeing: Investigating multisensory
integration and (in)congruent stimuli while eating in VR. <em>TVCG</em>,
<em>29</em>(5), 2423–2433. (<a
href="https://doi.org/10.1109/TVCG.2023.3247099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating taste in AR/VR applications has various promising use cases — from social eating to the treatment of disorders. Despite many successful AR/VR applications that alter the taste of beverages and food, the relationship between olfaction, gustation, and vision during the process of multisensory integration (MSI) has not been fully explored yet. Thus, we present the results of a study in which participants were confronted with congruent and incongruent visual and olfactory stimuli while eating a tasteless food product in VR. We were interested (1) if participants integrate bi-modal congruent stimuli and (2) if vision guides MSI during congruent/incongruent conditions. Our results contain three main findings: First, and surprisingly, participants were not always able to detect congruent visual-olfactory stimuli when eating a portion of tasteless food. Second, when confronted with tri-modal incongruent cues, a majority of participants did not rely on any of the presented cues when forced to identify what they eat; this includes vision which has previously been shown to dominate MSI. Third, although research has shown that basic taste qualities like sweetness, saltiness, or sourness can be influenced by congruent cues, doing so with more complex flavors (e.g., zucchini or carrot) proved to be harder to achieve. We discuss our results in the context of multimodal integration, and within the domain of multisensory AR/VR. Our results are a necessary building block for future human-food interaction in XR that relies on smell, taste, and vision and are foundational for applied applications such as affective AR/VR.},
  archive      = {J_TVCG},
  author       = {Florian Weidner and Jana E. Maier and Wolfgang Broll},
  doi          = {10.1109/TVCG.2023.3247099},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2423-2433},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eating, smelling, and seeing: Investigating multisensory integration and (In)congruent stimuli while eating in VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Give me a hand: Improving the effectiveness of near-field
augmented reality interactions by avatarizing users’ end effectors.
<em>TVCG</em>, <em>29</em>(5), 2412–2422. (<a
href="https://doi.org/10.1109/TVCG.2023.3247105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by previous works showing promise for AR self-avatarization - providing users with an augmented self avatar, we investigated whether avatarizing users&#39; end-effectors (hands) improved their interaction performance on a near-field, obstacle avoidance, object retrieval task wherein users were tasked with retrieving a target object from a field of non-target obstacles for a number of trials. We employed a 3 (Augmented hand representation) X 2 (density of obstacles) X 2 (size of obstacles) X 2 (virtual light intensity) multi-factorial design, manipulating the presence/absence and anthropomorphic fidelity of augmented self-avatars overlaid on the user&#39;s real hands, as a between subjects factor across three experimental conditions: (1) No-Augmented Avatar (using only real hands); (2) Iconic-Augmented Avatar; (3) Realistic Augmented Avatar. Results indicated that self-avatarization improved interaction performance and was perceived as more usable regardless of the anthropomorphic fidelity of avatar. We also found that the virtual light intensity used in illuminating holograms affects how visible one&#39;s real hands are. Overall, our findings seem to indicate that interaction performance may improve when users are provided with a visual representation of the AR system&#39;s interacting layer in the form of an augmented self-avatar.},
  archive      = {J_TVCG},
  author       = {Roshan Venkatakrishnan and Rohith Venkatakrishnan and Balagopal Raveendranath and Christopher C. Pagano and Andrew C. Robb and Wen-Chieh Lin and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2023.3247105},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2412-2422},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Give me a hand: Improving the effectiveness of near-field augmented reality interactions by avatarizing users&#39; end effectors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring interpersonal trust towards virtual humans with a
virtual maze paradigm. <em>TVCG</em>, <em>29</em>(5), 2401–2411. (<a
href="https://doi.org/10.1109/TVCG.2023.3247095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual humans, including virtual agents and avatars, play an increasingly important role as VR technology advances. For example, virtual humans are used as digital bodies of users in social VR or as interfaces for AI assistants in online financing. Interpersonal trust is an essential prerequisite in real-life interactions, as well as in the virtual world. However, to date, there are no established interpersonal trust measurement tools specifically for virtual humans in virtual reality. This study fills the gap, by contributing a novel validated behavioural tool to measure interpersonal trust towards a specific virtual social interaction partner in social VR. This validated paradigm is inspired by a previously proposed virtual maze task that measures trust towards virtual characters. In the current study, a variant of this paradigm was implemented. The task of the users (the trustors) is to navigate through a maze in virtual reality, where they can interact with a virtual human (the trustee). They can choose to 1) ask for advice and 2) follow the advice from the virtual human if they want to. These measures served as behavioural measures of trust. We conducted a validation study with 70 participants in a between-subject design. The two conditions did not differ in the content of the advice but in the appearance, tone of voice and engagement of the trustees (alleged as avatars controlled by other participants). Results indicate that the experimental manipulation was successful, as participants rated the virtual human as more trustworthy in the trustworthy condition than in the untrustworthy condition. Importantly, this manipulation affected the trust behaviour of our participants, who, in the trustworthy condition, asked for advice more often and followed advice more often, indicating that the paradigm is sensitive to assessing interpersonal trust towards virtual humans. Thus, our paradigm can be used to measure differences in interpersonal trust towards virtual humans and may serve as a valuable research tool to study trust in virtual reality.},
  archive      = {J_TVCG},
  author       = {Jinghuai Lin and Johrine Cronjé and Ivo Käthner and Paul Pauli and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2023.3247095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2401-2411},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring interpersonal trust towards virtual humans with a virtual maze paradigm},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lack of restraint: Comparing virtual reality interaction
techniques for constrained transport seating. <em>TVCG</em>,
<em>29</em>(5), 2390–2400. (<a
href="https://doi.org/10.1109/TVCG.2023.3247084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standalone Virtual Reality (VR) headsets can be used when travelling in cars, trains and planes. However, the constrained spaces around transport seating can leave users with little physical space in which to interact using their hands or controllers, and can increase the risk of invading other passengers&#39; personal space or hitting nearby objects and surfaces. This hinders transport VR users from using most commercial VR applications, which are designed for unobstructed 1-2m 360° home spaces. In this paper, we investigated whether three at-a-distance interaction techniques from the literature could be adapted to support common commercial VR movement inputs and so equalise the interaction capabilities of at-home and on-transport users: Linear Gain, Gaze-Supported Remote Hand, and AlphaCursor. First, we analysed commercial VR experiences to identify the most common movement inputs so that we could create gamified tasks based on them. We then investigated how well each technique could support these inputs from a constrained $50\mathrm{x}50\text{cm}$ space (representative of an economy plane seat) through a user study $(\mathrm{N}=16)$, where participants played all three games with each technique. We measured task performance, unsafe movements (play boundary violations, total arm movement) and subjective experience and compared results to a control ‘at-home’ condition (with unconstrained movement) to determine how similar performance and experience were. Results showed that Linear Gain was the best technique, with similar performance and user experience to the ‘at-home’ condition, albeit at the expense of a high number of boundary violations and large arm movements. In contrast, AlphaCursor kept users within bounds and minimised arm movement, but suffered from poorer performance and experience. Based on the results, we provide eight guidelines for the use of, and research into, at-a-distance techniques and constrained spaces.},
  archive      = {J_TVCG},
  author       = {Graham Wilson and Mark McGill and Daniel Medeiros and Stephen Brewster},
  doi          = {10.1109/TVCG.2023.3247084},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2390-2400},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A lack of restraint: Comparing virtual reality interaction techniques for constrained transport seating},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShadowMover: Automatically projecting real shadows onto
virtual object. <em>TVCG</em>, <em>29</em>(5), 2379–2389. (<a
href="https://doi.org/10.1109/TVCG.2023.3247066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inserting 3D virtual objects into real-world images has many applications in photo editing and augmented reality. One key issue to ensure the reality of the composite whole scene is to generate consistent shadows between virtual and real objects. However, it is challenging to synthesize visually realistic shadows for virtual and real objects without any explicit geometric information of the real scene or manual intervention, especially for the shadows on the virtual objects projected by real objects. In view of this challenge, we present, to our knowledge, the first end-to-end solution to fully automatically project real shadows onto virtual objects for outdoor scenes. In our method, we introduce the Shifted Shadow Map, a new shadow representation that encodes the binary mask of shifted real shadows after inserting virtual objects in an image. Based on the shifted shadow map, we propose a CNN-based shadow generation model named ShadowMover which first predicts the shifted shadow map for an input image and then automatically generates plausible shadows on any inserted virtual object. A large-scale dataset is constructed to train the model. Our ShadowMover is robust to various scene configurations without relying on any geometric information of the real scene and is free of manual intervention. Extensive experiments validate the effectiveness of our method.},
  archive      = {J_TVCG},
  author       = {Piaopiao Yu and Jie Guo and Fan Huang and Zhenyu Chen and Chen Wang and Yan Zhang and Yanwen Guo},
  doi          = {10.1109/TVCG.2023.3247066},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2379-2389},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ShadowMover: Automatically projecting real shadows onto virtual object},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing different grasping visualizations for object
manipulation in VR using controllers. <em>TVCG</em>, <em>29</em>(5),
2369–2378. (<a href="https://doi.org/10.1109/TVCG.2023.3247039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual grasping is one of the most common and important interactions performed in a Virtual Environment (VE). Even though there has been substantial research using hand tracking methods exploring different ways of visualizing grasping, there are only a few studies that focus on handheld controllers. This gap in research is particularly crucial, since controllers remain the most used input modality in commercial Virtual Reality (VR). Extending existing research, we designed an experiment comparing three different grasping visualizations when users are interacting with virtual objects in immersive VR using controllers. We examine the following visualizations: the Auto-Pose (AP), where the hand is automatically adjusted to the object upon grasping; the Simple-Pose (SP), where the hand closes fully when selecting the object; and the Disappearing-Hand (DH), where the hand becomes invisible after selecting an object, and turns visible again after positioning it on the target. We recruited 38 participants in order to measure if and how their performance, sense of embodiment, and preference are affected. Our results show that while in terms of performance there is almost no significant difference in any of the visualizations, the perceived sense of embodiment is stronger with the AP, and is generally preferred by the users. Thus, this study incentivizes the inclusion of similar visualizations in relevant future research and VR experiences.},
  archive      = {J_TVCG},
  author       = {Giorgos Ganias and Christos Lougiakis and Akrivi Katifori and Maria Roussou and Yannis Ioannidis and loannis Panagiotis Ioannidis},
  doi          = {10.1109/TVCG.2023.3247039},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2369-2378},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing different grasping visualizations for object manipulation in VR using controllers},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impact of avatar and environment congruence on
plausibility, embodiment, presence, and the proteus effect in virtual
reality. <em>TVCG</em>, <em>29</em>(5), 2358–2368. (<a
href="https://doi.org/10.1109/TVCG.2023.3247089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a $2\times 2$ between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar&#39;s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.},
  archive      = {J_TVCG},
  author       = {David Mal and Erik Wolf and Nina Döllinger and Carolin Wienrich and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2023.3247089},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2358-2368},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of avatar and environment congruence on plausibility, embodiment, presence, and the proteus effect in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Can i squeeze through? Effects of self-avatars and
calibration in a person-plus-virtual-object system on perceived lateral
passability in VR. <em>TVCG</em>, <em>29</em>(5), 2348–2357. (<a
href="https://doi.org/10.1109/TVCG.2023.3247067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of Virtual Reality (VR) on the rise, creators from a variety of fields are building increasingly complex experiences that allow users to express themselves more naturally. Self-avatars and object interaction in virtual worlds are at the heart of these experiences. However, these give rise to several perception based challenges that have been the focus of research in recent years. One area that garners most interest is understanding the effects of self-avatars and object interaction on action capabilities or affordances in VR. Affordances have been shown to be influenced by the anthropometric and anthropomorphic properties of the self-avatar embodied. However, self-avatars cannot fully represent real world interaction and fail to provide information about the dynamic properties of surfaces in the environment. For example, pressing against a board to feel its rigidity. This lack of accurate dynamic information can be further amplified when interacting with virtual handheld objects as the weight and inertial feedback associated with them is often mismatched. To investigate this phenomenon, we looked at how the absence of dynamic surface properties affect lateral passability judgments when carrying virtual handheld objects in the presence or absence of gender matched body-scaled self-avatars. Results suggest that participants can calibrate to the missing dynamic information in the presence of self-avatars to make lateral passability judgments, but rely on their internal body schema of a compressed physical body depth in the absence of self-avatars.},
  archive      = {J_TVCG},
  author       = {Ayush Bhargava and Rohith Venkatakrishnan and Roshan Venkatakrishnan and Kathryn Lucaites and Hannah Solini and Andrew C. Robb and Christopher C. Pagano and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2023.3247067},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2348-2357},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Can i squeeze through? effects of self-avatars and calibration in a person-plus-virtual-object system on perceived lateral passability in VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiDAR-aid inertial poser: Large-scale human motion capture
by sparse inertial and LiDAR sensors. <em>TVCG</em>, <em>29</em>(5),
2337–2347. (<a href="https://doi.org/10.1109/TVCG.2023.3247088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research.},
  archive      = {J_TVCG},
  author       = {Yiming Ren and Chengfeng Zhao and Yannan He and Peishan Cong and Han Liang and Jingyi Yu and Lan Xu and Yuexin Ma},
  doi          = {10.1109/TVCG.2023.3247088},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2337-2347},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiDAR-aid inertial poser: Large-scale human motion capture by sparse inertial and LiDAR sensors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cybersickness, cognition, &amp; motor skills: The effects of
music, gender, and gaming experience. <em>TVCG</em>, <em>29</em>(5),
2326–2336. (<a href="https://doi.org/10.1109/TVCG.2023.3247062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has attempted to identify methods to mitigate cybersickness and examine its aftereffects. In this direction, this paper examines the effects of cybersickness on cognitive, motor, and reading performance in VR. Also, this paper evaluates the mitigating effects of music on cybersickness, as well as the role of gender, and the computing, VR, and gaming experience of the user. This paper reports two studies. In the 1st study, 92 participants selected the music tracks considered most calming (low valence) or joyful (high valence) to be used in the 2nd study. In the 2nd study, 39 participants performed an assessment four times, once before the rides (baseline), and then once after each ride (3 rides). In each ride either Calming, or Joyful, or No Music was played. During each ride, linear and angular accelerations took place to induce cybersickness in the participants. In each assessment, while immersed in VR, the participants evaluated their cybersickness symptomatology and performed a verbal working memory task, a visuospatial working memory task, and a psychomotor task. While responding to the cybersickness questionnaire (3D UI), eye-tracking was conducted to measure reading time and pupillometry. The results showed that Joyful and Calming music substantially decreased the intensity of nausea-related symptoms. However, only Joyful music significantly decreased the overall cybersickness intensity. Importantly, cybersickness was found to decrease verbal working memory performance and pupil size. Also, it significantly decelerated psychomotor (reaction time) and reading abilities. Higher gaming experience was associated with lower cybersickness. When controlling for gaming experience, there were no significant differences between female and male participants in terms of cybersickness. The outcomes indicated the efficiency of music in mitigating cybersickness, the important role of gaming experience in cybersickness, and the significant effects of cybersickness on pupil size, cognition, psychomotor skills, and reading ability.},
  archive      = {J_TVCG},
  author       = {Panagiotis Kourtesis and Rayaan Amir and Josie Linnell and Ferran Argelaguet and Sarah E. MacPherson},
  doi          = {10.1109/TVCG.2023.3247062},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2326-2336},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cybersickness, cognition, &amp; motor skills: The effects of music, gender, and gaming experience},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FREE-RDW: A multi-user redirected walking method for
supporting non-forward steps. <em>TVCG</em>, <em>29</em>(5), 2315–2325.
(<a href="https://doi.org/10.1109/TVCG.2023.3247107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-user redirected walking (RDW) is widely used in large-scale virtual scenes because it allows more users to move synchronously in both virtual and physical environments. To ensure the freedom of virtual roaming, which can be used in various situations, some redirected algorithms have been dedicated to non-forward movements, such as vertical movement and jumping. However, the existing RDW methods still mainly focus on forward steps, ignoring sideward and backward steps, which are also common and necessary in virtual reality. RDW algorithms for non-forward steps can enrich the movement direction of users&#39; virtual roaming and improve the realism of VR roaming. In addition, the non-forward motions have a larger curvature gain, which can be used to better reduce resets in RDW. Therefore, this paper presents a new method of multi-user redirected walking for supporting non-forward steps (FREE-RDW), which adds the options of sideward and backward steps to extend the VR locomotion. Our method adopts a user collision avoidance strategy based on optimal reciprocal collision avoidance (ORCA) and optimizes it into a linear programming problem to obtain the optimal velocity for users. Furthermore, our method uses APF to expose the user to repulsive forces from other users and walls, thus further reducing potential collisions and improving the utilization of physical space. The experiments show that our method performs well in virtual scenes with forward and non-forward steps. In addition, our method can significantly reduce the number of resets compared with reactive RDW algorithms such as DDB-RDW and APF-RDW in multi-user forward-step virtual scenes.},
  archive      = {J_TVCG},
  author       = {Tianyang Dong and Tieqi Gao and Yinyan Dong and Liming Wang and Kefan Hu and Jing Fan},
  doi          = {10.1109/TVCG.2023.3247107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2315-2325},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FREE-RDW: A multi-user redirected walking method for supporting non-forward steps},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of collaborative training using virtual
co-embodiment on motor skill learning. <em>TVCG</em>, <em>29</em>(5),
2304–2314. (<a href="https://doi.org/10.1109/TVCG.2023.3247112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is a promising tool for motor skill learning. Previous studies have indicated that observing and following a teacher&#39;s movements from a first-person perspective using VR facilitates motor skill learning. Conversely, it has also been pointed out that this learning method makes the learner so strongly aware of the need to follow that it weakens their sense of agency (SoA) for motor skills and prevents them from updating the body schema, thereby preventing long-term retention of motor skills. To address this problem, we propose applying “virtual co-embodiment” to motor skill learning. Virtual co-embodiment is a system in which a virtual avatar is controlled based on the weighted average of the movements of multiple entities. Because users in virtual co-embodiment overestimate their SoA, we hypothesized that learning using virtual co-embodiment with a teacher would improve motor skill retention. In this study, we focused on learning a dual task to evaluate the automation of movement, which is considered an essential element of motor skills. As a result, learning in virtual co-embodiment with the teacher improves motor skill learning efficiency compared with sharing the teacher&#39;s first-person perspective or learning alone.},
  archive      = {J_TVCG},
  author       = {Daiki Kodama and Takato Mizuho and Yuji Hatada and Takuji Narumi and Michitaka Hirose},
  doi          = {10.1109/TVCG.2023.3247112},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2304-2314},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of collaborative training using virtual co-embodiment on motor skill learning},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on remote assistance and training in mixed reality
environments. <em>TVCG</em>, <em>29</em>(5), 2291–2303. (<a
href="https://doi.org/10.1109/TVCG.2023.3247081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a taxonomy based on degree of collaboration, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. All supplemental materials are available at https://augmented-perception.org/publications/2023-training-survey.html.},
  archive      = {J_TVCG},
  author       = {Catarina G. Fidalgo and Yukang Yan and Hyunsung Cho and Maurício Sousa and David Lindlbauer and Joaquim Jorge},
  doi          = {10.1109/TVCG.2023.3247081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2291-2303},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on remote assistance and training in mixed reality environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shadowless projection mapping using retrotransmissive
optics. <em>TVCG</em>, <em>29</em>(5), 2280–2290. (<a
href="https://doi.org/10.1109/TVCG.2023.3247104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a shadowless projection mapping system for interactive applications in which a target surface is frequently occluded from a projector with a user&#39;s body. We propose a delay-free optical solution for this critical problem. Specifically, as the primary technical contribution, we apply a large format retrotransmissive plate to project images onto the target surface from wide viewing angles. We also tackle technical issues unique to the proposed shadowless principle. First, the retrotransmissive optics inevitably suffer from stray light, which leads to significant contrast degradation of the projected result. We propose to block the stray light by covering the retrotransmissive plate with a spatial mask. Because the mask reduces not only the stray light but the achievable luminance of the projected result, we develop a computational algorithm that determines the shape of the mask to balance the image quality. Second, we propose a touch sensing technique by leveraging the optically bidirectional property of the retrotransmissive plate to support interaction between the user and the projected contents on the target object. We implement a proof-of-concept prototype and validate the above-mentioned techniques through experiments.},
  archive      = {J_TVCG},
  author       = {Kosuke Hiratani and Daisuke Iwai and Yuta Kageyama and Parinya Punpongsanon and Takefumi Hiraki and Kosuke Sato},
  doi          = {10.1109/TVCG.2023.3247104},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2280-2290},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shadowless projection mapping using retrotransmissive optics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveling the playing field: A comparative reevaluation of
unmodified eye tracking as an input and interaction modality for VR.
<em>TVCG</em>, <em>29</em>(5), 2269–2279. (<a
href="https://doi.org/10.1109/TVCG.2023.3247058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we establish a much-needed baseline for evaluating eye tracking interactions using an eye tracking enabled Meta Quest 2 VR headset with 30 participants. Each participant went through 1098 targets using multiple conditions representative of AR/VR targeting and selecting tasks, including both traditional standards and those more aligned with AR/VR interactions today. We use circular white world-locked targets, and an eye tracking system with sub-1-degree mean accuracy errors running at approximately 90Hz. In a targeting and button press selection task, we, by design, compare completely unadjusted, cursor-less, eye tracking with controller and head tracking, which both had cursors. Across all inputs, we presented targets in a configuration similar to the ISO 9241–9 reciprocal selection task and another format with targets more evenly distributed near the center. Targets were laid out either flat on a plane or tangent to a sphere and rotated toward the user. Even though we intended this to be a baseline study, we see unmodified eye tracking, without any form of a cursor, or feedback, outperformed the head by 27.9\% and performed comparably to the controller (5.63\% decrease) in throughput. Eye tracking had improved subjective ratings relative to head in Ease of Use, Adoption, and Fatigue (66.4\%, 89.8\%, and 116.1\% improvements, respectively) and had similar ratings relative to the controller (reduction by 4.2\%, 8.9\%, and 5.2\% respectively). Eye tracking had a higher miss percentage than controller and head (17.3\% vs 4.7\% vs 7.2\% respectively). Collectively, the results of this baseline study serve as a strong indicator that eye tracking, with even minor sensible interaction design modifications, has tremendous potential in reshaping interactions in next-generation AR/VR head mounted displays.},
  archive      = {J_TVCG},
  author       = {Ajoy S. Fernandes and T. Scott Murdison and Michael J. Proulx},
  doi          = {10.1109/TVCG.2023.3247058},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2269-2279},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leveling the playing field: A comparative reevaluation of unmodified eye tracking as an input and interaction modality for VR},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). How virtual hand representations affect the perceptions of
dynamic affordances in virtual reality. <em>TVCG</em>, <em>29</em>(5),
2258–2268. (<a href="https://doi.org/10.1109/TVCG.2023.3247041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User representations are critical to the virtual experience, and involve both the input device used to support interactions as well as how the user is virtually represented in the scene. Inspired by previous work that has shown effects of user representations on the perceptions of relatively static affordances, we attempt to investigate how end-effector representations affect the perceptions of affordances that dynamically change over time. Towards this end, we empirically evaluated how different virtual hand representations affect users&#39; perceptions of dynamic affordances in an object retrieval task wherein users were tasked with retrieving a target from a box for a number of trials while avoiding collisions with its moving doors. We employed a 3 (virtual end-effector representation) X 13 (frequency of moving doors) X 2 (target object size) multi-factorial design, manipulating the input modality and its concomitant virtual end-effector representation as a between-subjects factor across three experimental conditions: (1) Controller (using a controller represented as a virtual controller); (2) Controller-hand (using a controller represented as a virtual hand); (3) Glove (using a hand tracked hi-fidelity glove represented as a virtual hand). Results indicated that the controller-hand condition produced lower levels of performance than both the other conditions. Furthermore, users in this condition exhibited a diminished ability to calibrate their performance over trials. Overall, we find that representing the end-effector as a hand tends to increase embodiment but can also come at the cost of performance, or an increased workload due to a discordant mapping between the virtual representation and the input modality used. It follows that VR system designers should carefully consider the priorities and target requirements of the application being developed when choosing the type of end-effector representation for users to embody in immersive virtual experiences.},
  archive      = {J_TVCG},
  author       = {Roshan Venkatakrishnan and Rohith Venkatakrishnan and Balagopal Raveendranath and Christopher C. Pagano and Andrew C. Robb and Wen-Chieh Lin and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2023.3247041},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2258-2268},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How virtual hand representations affect the perceptions of dynamic affordances in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceptually-guided dual-mode virtual reality system for
motion-adaptive display. <em>TVCG</em>, <em>29</em>(5), 2249–2257. (<a
href="https://doi.org/10.1109/TVCG.2023.3247097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal Virtual reality (VR) device should simultaneously provide retina-level resolution, wide field-of-view (FOV), and high refresh rate display, thereby bringing users into a deeply immersive virtual world. However, directly providing such high-quality display poses great challenges for display panel fabrication, real-time rendering, and data transfer. To address this issue, we introduce a dual-mode virtual reality system based on the spatio-temporal perception characteristics of human vision. The proposed VR system has a novel optical architecture. It can switch display modes according to the user&#39;s perceptual requirements for different display scenes to adaptively adjust the display spatial and temporal resolution based on a given display budget, thus providing users with the optimal visual perception quality. In this work, a complete design pipeline for the dual-mode VR optical system is proposed, and a bench-top prototype is built with only off-the-shelf hardware and components to verify its capability. Compared to the conventional VR system, our proposed scheme is more efficient and flexible in utilizing the display budget, and this work is expected to facilitate the development of the VR device based on the human visual system.},
  archive      = {J_TVCG},
  author       = {Hui Zeng and Rong Zhao},
  doi          = {10.1109/TVCG.2023.3247097},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2249-2257},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptually-guided dual-mode virtual reality system for motion-adaptive display},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LFACon: Introducing anglewise attention to no-reference
quality assessment in light field space. <em>TVCG</em>, <em>29</em>(5),
2239–2248. (<a href="https://doi.org/10.1109/TVCG.2023.3247069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging can capture both the intensity information and the direction information of light rays. It naturally enables a six-degrees-of-freedom viewing experience and deep user engagement in virtual reality. Compared to 2D image assessment, light field image quality assessment (LFIQA) needs to consider not only the image quality in the spatial domain but also the quality consistency in the angular domain. However, there is a lack of metrics to effectively reflect the angular consistency and thus the angular quality of a light field image (LFI). Furthermore, the existing LFIQA metrics suffer from high computational costs due to the excessive data volume of LFIs. In this paper, we propose a novel concept of “anglewise attention” by introducing a multihead self-attention mechanism to the angular domain of an LFl. This mechanism better reflects the LFI quality. In particular, we propose three new attention kernels, including anglewise self-attention, anglewise grid attention, and anglewise central attention. These attention kernels can realize angular self-attention, extract multiangled features globally or selectively, and reduce the computational cost of feature extraction. By effectively incorporating the proposed kernels, we further propose our light field attentional convolutional neural network (LFACon) as an LFIQA metric. Our experimental results show that the proposed LFACon metric significantly outperforms the state-of-the-art LFIQA metrics. For the majority of distortion types, LFACon attains the best performance with lower complexity and less computational time.},
  archive      = {J_TVCG},
  author       = {Qiang Qu and Xiaoming Chen and Yuk Ying Chung and Weidong Cai},
  doi          = {10.1109/TVCG.2023.3247069},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2239-2248},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LFACon: Introducing anglewise attention to no-reference quality assessment in light field space},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified egocentric viewpoint for softer seated experience
in virtual reality. <em>TVCG</em>, <em>29</em>(5), 2230–2238. (<a
href="https://doi.org/10.1109/TVCG.2023.3247056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users in a prolonged experience of virtual reality adopt a sitting position according to their task, as they do in the real world. However, inconsistencies in the haptic feedback from a chair they sit on in the real world and that which is expected in the virtual world decrease the feeling of presence. We aimed to change the perceived haptic features of a chair by shifting the position and angle of the users&#39; viewpoints in the virtual reality environment. The targeted features in this study were seat softness and backrest flexibility. To enhance the seat softness, we shifted the virtual viewpoint using an exponential formula soon after a user&#39;s bottom contacted the seat surface. The flexibility of the backrest was manipulated by moving the viewpoint, which followed the tilt of the virtual backrest. These shifts make users feel as if their body moves along with the viewpoint; as a result, they would perceive pseudo-softness or flexibility consistently with the body movement. Based on subjective evaluations, we confirmed that the participants perceived the seat as being softer and the backrest as being more flexible than the actual ones. These results demonstrated that only shifting the viewpoint could change the participants&#39; perceptions of the haptic features of their seats, although significant changes created strong discomfort.},
  archive      = {J_TVCG},
  author       = {Miki Matsumuro and Shohei Mori and Yuta Kataoka and Fumiaki Igarashi and Fumihisa Shibata and Asako Kimura},
  doi          = {10.1109/TVCG.2023.3247056},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2230-2238},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modified egocentric viewpoint for softer seated experience in virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the effects of virtual reality environment
learning on subsequent robot teleoperation in an unfamiliar building.
<em>TVCG</em>, <em>29</em>(5), 2220–2229. (<a
href="https://doi.org/10.1109/TVCG.2023.3247052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a map in an unfamiliar environment requires identifying correspondences between elements of the map&#39;s allocentric representation and elements in egocentric views. Aligning the map with the environment can be challenging. Virtual reality (VR) allows learning about unfamiliar environments in a sequence of egocentric views that correspond closely to the perspectives and views that are experienced in the actual environment. We compared three methods to prepare for localization and navigation tasks performed by teleoperating a robot in an office building: studying a floor plan of the building and two forms of VR exploration. One group of participants studied a building plan, a second group explored a faithful VR reconstruction of the building from a normal-sized avatar&#39;s perspective, and a third group explored the VR from a giant-sized avatar&#39;s perspective. All methods contained marked checkpoints. The subsequent tasks were identical for all groups. The self-localization task required indication of the approximate location of the robot in the environment. The navigation task required navigation between checkpoints. Participants took less time to learn with the giant VR perspective and with the floorplan than with the normal VR perspective. Both VR learning methods significantly outperformed the floorplan in the orientation task. Navigation was performed quicker after learning in the giant perspective compared to the normal perspective and the building plan. We conclude that the normal perspective and especially the giant perspective in VR are viable options for preparing for teleoperation in unfamiliar environments when a virtual model of the environment is available.},
  archive      = {J_TVCG},
  author       = {Karl Eisenträger and Judith Haubner and Jennifer Brade and Wolfgang Einhäuser and Alexandra Bendixen and Sven Winkler and Philipp Klimant and Georg Jahn},
  doi          = {10.1109/TVCG.2023.3247052},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2220-2229},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating the effects of virtual reality environment learning on subsequent robot teleoperation in an unfamiliar building},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Upper body thermal referral and tactile masking for
localized feedback. <em>TVCG</em>, <em>29</em>(5), 2211–2219. (<a
href="https://doi.org/10.1109/TVCG.2023.3247068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the effects of thermal referral and tactile masking illusions to achieve localized thermal feedback on the upper body. Two experiments are conducted. The first experiment uses a 2D array of sixteen vibrotactile actuators $(4\times 4)$ with four thermal actuators to explore the thermal distribution on the user&#39;s back. A combination of thermal and tactile sensations is delivered to establish the distributions of thermal referral illusions with different numbers of vibrotactile cues. The result confirms that localized thermal feedback can be achieved through cross-modal thermo-tactile interaction on the user&#39;s back of the body. The second experiment is conducted to validate our approach by comparing it with thermal-only conditions with an equal and higher number of thermal actuators in VR. The results show that our thermal referral with a tactile masking approach with a lesser number of thermal actuators achieves higher response time and better location accuracy than thermal-only conditions. Our findings can contribute to thermal-based wearable design to achieve greater user performance and experiences.},
  archive      = {J_TVCG},
  author       = {Hyungki Son and Haokun Wang and Yatharth Singhal and Jin Ryong Kim},
  doi          = {10.1109/TVCG.2023.3247068},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2211-2219},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Upper body thermal referral and tactile masking for localized feedback},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-CariGAN: An end-to-end solution to 3D caricature
generation from normal face photos. <em>TVCG</em>, <em>29</em>(4),
2203–2210. (<a href="https://doi.org/10.1109/TVCG.2021.3126659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caricature is a type of artistic style of human faces that attracts considerable attention in the entertainment industry. So far a few 3D caricature generation methods exist and all of them require some caricature information (e.g., a caricature sketch or 2D caricature) as input. This kind of input, however, is difficult to provide by non-professional users. In this paper, we propose an end-to-end deep neural network model that generates high-quality 3D caricatures directly from a normal 2D face photo. The most challenging issue for our system is that the source domain of face photos (characterized by normal 2D faces) is significantly different from the target domain of 3D caricatures (characterized by 3D exaggerated face shapes and textures). To address this challenge, we: (1) build a large dataset of 5,343 3D caricature meshes and use it to establish a PCA model in the 3D caricature shape space; (2) reconstruct a normal full 3D head from the input face photo and use its PCA representation in the 3D caricature shape space to establish correspondences between the input photo and 3D caricature shape; and (3) propose a novel character loss and a novel caricature loss based on previous psychological studies on caricatures. Experiments including a novel two-level user study show that our system can generate high-quality 3D caricatures directly from normal face photos.},
  archive      = {J_TVCG},
  author       = {Zipeng Ye and Mengfei Xia and Yanan Sun and Ran Yi and Minjing Yu and Juyong Zhang and Yu-Kun Lai and Yong-Jin Liu},
  doi          = {10.1109/TVCG.2021.3126659},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2203-2210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D-CariGAN: An end-to-end solution to 3D caricature generation from normal face photos},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey of movement reproduction in immersive virtual
rehabilitation. <em>TVCG</em>, <em>29</em>(4), 2184–2202. (<a
href="https://doi.org/10.1109/TVCG.2022.3142198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has emerged as a powerful tool for rehabilitation. Many effective VR applications have been developed to support motor rehabilitation of people affected by motor issues. Movement reproduction, which transfers users’ movements from the physical world to the virtual environment, is commonly used in VR rehabilitation applications. Three major components are required for movement reproduction in VR: (1) movement input, (2) movement representation, and (3) movement modulation. Until now, movement reproduction in virtual rehabilitation has not yet been systematically studied. This article aims to provide a state-of-the-art review on this subject by focusing on existing literature on immersive motor rehabilitation using VR. In this review, we provided in-depth discussions on the rehabilitation goals and outcomes, technology issues behind virtual rehabilitation, and user experience regarding movement reproduction. Similarly, we present good practices and highlight challenges and opportunities that can form constructive suggestions for the design and development of fit-for-purpose VR rehabilitation applications and can help frame future research directions for this emerging area that combines VR and health.},
  archive      = {J_TVCG},
  author       = {Liu Wang and Mengjie Huang and Rui Yang and Hai-Ning Liang and Ji Han and Ying Sun},
  doi          = {10.1109/TVCG.2022.3142198},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2184-2202},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey of movement reproduction in immersive virtual rehabilitation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analytic review of using augmented reality for situational
awareness. <em>TVCG</em>, <em>29</em>(4), 2166–2183. (<a
href="https://doi.org/10.1109/TVCG.2022.3141585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situational awareness is the perception and understanding of the surrounding environment. Maintaining situational awareness is vital for performance and error prevention in safety critical domains. Prior work has examined applying augmented reality (AR) to the context of improving situational awareness, but has mainly focused on the applicability of using AR rather than on information design. Hence, there is a need to investigate how to design the presentation of information, especially in AR headsets, to increase users’ situational awareness. We conducted a Systematic Literature Review to research how information is currently presented in AR, especially in systems that are being utilized for situational awareness. Comparing current presentations of information to existing design recommendations aided in identifying future areas of design. In addition, this survey further discusses opportunities and challenges in applying AR to increasing users’ situational awareness.},
  archive      = {J_TVCG},
  author       = {Julia Woodward and Jaime Ruiz},
  doi          = {10.1109/TVCG.2022.3141585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2166-2183},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analytic review of using augmented reality for situational awareness},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic review of navigation assistance systems for
people with dementia. <em>TVCG</em>, <em>29</em>(4), 2146–2165. (<a
href="https://doi.org/10.1109/TVCG.2022.3141383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological developments provide solutions to alleviate the tremendous impact on the health and autonomy due to the impact of dementia on navigation abilities. We systematically reviewed the literature on devices tested to provide assistance to people with dementia during indoor, outdoor and virtual navigation (PROSPERO ID number: 215585). Medline and Scopus databases were searched from inception. Our aim was to summarize the results from the literature to guide future developments. Twenty-three articles were included in our study. Three types of information were extracted from these studies. First, the types of navigation advice the devices provided were assessed through: (i) the sensorial modality of presentation, e.g., visual and tactile stimuli, (ii) the navigation content, e.g., landmarks, and (iii) the timing of presentation, e.g., systematically at intersections. Second, we analyzed the technology that the devices were based on, e.g., smartphone. Third, the experimental methodology used to assess the devices and the navigation outcome was evaluated. We report and discuss the results from the literature based on these three main characteristics. Finally, based on these considerations, recommendations are drawn, challenges are identified and potential solutions are suggested. Augmented reality-based devices, intelligent tutoring systems and social support should particularly further be explored.},
  archive      = {J_TVCG},
  author       = {Léa Pillette and Guillaume Moreau and Jean-Marie Normand and Manon Perrier and Anatole Lécuyer and Mélanie Cogné},
  doi          = {10.1109/TVCG.2022.3141383},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2146-2165},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A systematic review of navigation assistance systems for people with dementia},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time lighting estimation for augmented reality via
differentiable screen-space rendering. <em>TVCG</em>, <em>29</em>(4),
2132–2145. (<a href="https://doi.org/10.1109/TVCG.2022.3141943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) applications aim to provide realistic blending between the real-world and virtual objects. One of the important factors for realistic AR is the correct lighting estimation. In this article, we present a method that estimates the real-world lighting condition from a single image in real time, using information from an optional support plane provided by advanced AR frameworks (e.g., ARCore, ARKit, etc.). By analyzing the visual appearance of the real scene, our algorithm can predict the lighting condition from the input RGB photo. In the first stage, we use a deep neural network to decompose the scene into several components: lighting, normal, and Bidirectional Reflectance Distribution Function (BRDF). Then we introduce differentiable screen-space rendering, a novel approach to providing the supervisory signal for regressing lighting, normal, and BRDF jointly. We recover the most plausible real-world lighting condition using Spherical Harmonics and the main directional lighting. Through a variety of experimental results, we demonstrate that our method can provide improved results than prior works quantitatively and qualitatively, and it can enhance the real-time AR experiences.},
  archive      = {J_TVCG},
  author       = {Celong Liu and Lingyu Wang and Zhong Li and Shuxue Quan and Yi Xu},
  doi          = {10.1109/TVCG.2022.3141943},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2132-2145},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time lighting estimation for augmented reality via differentiable screen-space rendering},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Using foliation leaves to extract reeb graphs on surfaces.
<em>TVCG</em>, <em>29</em>(4), 2117–2131. (<a
href="https://doi.org/10.1109/TVCG.2022.3141764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Reeb graph extraction on surfaces, existing methods always use the isolines of a function defined on the surface to detect the surface components and the neighboring relationships between them. Since such detection is unstable, it is still a challenge for the extracted Reeb graphs to stably and concisely encode the topological information of the surface. In this article, we address this challenge by using foliation leaves to extract Reeb graphs. In particular, we employ a method for generating measured harmonic foliations by defining loops for foliation initialization and diffusing leaves from loops over the surface. We demonstrate that when the loops are determined, the neighboring relationships between the leaves from different loops are fixed. Thus, we can use loops to represent surface components for robustly detecting the interrelationships between surface components. As a result, we are able to extract stable and concise Reeb graphs. We developed novel measures for loop determination and improved foliation generation, and our method allows the user to manually prescribe loops for generating Reeb graphs with desired structures. Therefore, the potential of Reeb graphs for representing surfaces is enhanced, including conveniently representing the symmetries of the surface and ignoring topological noise. This is verified by our experimental results which indicate that our Reeb graphs are compact and expressive, promoting shape analysis.},
  archive      = {J_TVCG},
  author       = {Shaodong Wang and Wencheng Wang and Hui Zhao},
  doi          = {10.1109/TVCG.2022.3141764},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2117-2131},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using foliation leaves to extract reeb graphs on surfaces},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARCHIE++: A cloud-enabled framework for conducting AR system
testing in the wild. <em>TVCG</em>, <em>29</em>(4), 2102–2116. (<a
href="https://doi.org/10.1109/TVCG.2022.3141029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present ARCHIE++, a testing framework for conducting AR system testing and collecting user feedback in the wild. Our system addresses challenges in AR testing practices by aggregating usability feedback data (collected in situ ) with system performance data from that same time period. These data packets can then be leveraged to identify edge cases encountered by testers during unconstrained usage scenarios. We begin by presenting a set of current trends in performing human testing of AR systems, identified by reviewing a selection of recent work from leading conferences in mixed reality, human factors, and mobile and pervasive systems. From the trends, we identify a set of challenges to be faced when attempting to adopt these practices to testing in the wild. These challenges are used to inform the design of our framework, which provides a cloud-enabled and device-agnostic way for AR systems developers to improve their knowledge of environmental conditions and to support scalability and reproducibility when testing in the wild. We then present a series of case studies demonstrating how ARCHIE++ can be used to support a range of AR testing scenarios, and demonstrate the limited overhead of the framework through a series of evaluations. We close with additional discussion on the design and utility of ARCHIE++ under various edge conditions.},
  archive      = {J_TVCG},
  author       = {Sarah M. Lehman and Semir Elezovikj and Haibin Ling and Chiu C. Tan},
  doi          = {10.1109/TVCG.2022.3141029},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2102-2116},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ARCHIE++: A cloud-enabled framework for conducting AR system testing in the wild},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multisensory 360° videos under varying resolution levels
enhance presence. <em>TVCG</em>, <em>29</em>(4), 2093–2101. (<a
href="https://doi.org/10.1109/TVCG.2022.3140875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional videos have become a leading multimedia format for Virtual Reality applications. While live 360 $^\circ$ videos offer a unique immersive experience, streaming of omnidirectional content at high resolutions is not always feasible in bandwidth-limited networks. While in the case of flat videos, scaling to lower resolutions works well, 360 $^\circ$ video quality is seriously degraded because of the viewing distances involved in head-mounted displays. Hence, in this article, we investigate first how quality degradation impacts the sense of presence in immersive Virtual Reality applications. Then, we are pushing the boundaries of 360 $^\circ$ technology through the enhancement with multisensory stimuli. 48 participants experimented both 360 $^\circ$ scenarios (with and without multisensory content), while they were divided randomly between four conditions characterised by different encoding qualities (HD, FullHD, 2.5K, 4K). The results showed that presence is not mediated by streaming at a higher bitrate. The trend we identified revealed however that presence is positively and significantly impacted by the enhancement with multisensory content. This shows that multisensory technology is crucial in creating more immersive experiences.},
  archive      = {J_TVCG},
  author       = {Alexandra Covaci and Estêvão B. Saleme and Gebremariam Mesfin and Ioan-Sorin Comsa and Ramona Trestian and Celso A. S. Santos and George Ghinea},
  doi          = {10.1109/TVCG.2022.3140875},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2093-2101},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multisensory 360° videos under varying resolution levels enhance presence},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive optimization algorithm for resetting techniques in
obstacle-ridden environments. <em>TVCG</em>, <em>29</em>(4), 2080–2092.
(<a href="https://doi.org/10.1109/TVCG.2021.3139990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected Walking (RDW) algorithms aim to impose several types of gains on users immersed in Virtual Reality and distort their walking paths in the real world, thus enabling them to explore a larger space. Since collision with physical boundaries is inevitable, a reset strategy needs to be provided to allow users to reset when they hit the boundary. However, most reset strategies are based on simple heuristics by choosing a seemingly suitable solution, which may not perform well in practice. In this article, we propose a novel optimization-based reset algorithm adaptive to different RDW algorithms. Inspired by the approach of finite element analysis, our algorithm splits the boundary of the physical world by a set of endpoints. Each endpoint is assigned a reset vector to represent the optimized reset direction when hitting the boundary. The reset vectors on the edge will be determined by the interpolation between two neighbouring endpoints. We conduct simulation-based experiments for three RDW algorithms with commonly used reset algorithms to compare with. The results demonstrate that the proposed algorithm significantly reduces the number of resets.},
  archive      = {J_TVCG},
  author       = {Song-Hai Zhang and Chia-Hao Chen and Fu Zheng and Yong-Liang Yang and Shi-Min Hu},
  doi          = {10.1109/TVCG.2021.3139990},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2080-2092},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive optimization algorithm for resetting techniques in obstacle-ridden environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EnConVis: A unified framework for ensemble contour
visualization. <em>TVCG</em>, <em>29</em>(4), 2067–2079. (<a
href="https://doi.org/10.1109/TVCG.2021.3140153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble simulation is a crucial method to handle potential uncertainty in modern simulation and has been widely applied in many disciplines. Many ensemble contour visualization methods have been introduced to facilitate ensemble data analysis. On the basis of deep exploration and summarization of existing techniques and domain requirements, we propose a unified framework of ensemble contour visualization, EnConVis (Ensemble Contour Visualization) , which systematically combines state-of-the-art methods. We model ensemble contour visualization as a four-step pipeline consisting of four essential procedures: member filtering, point-wise modeling, uncertainty band extraction, and visual mapping. For each of the four essential procedures, we compare different methods they use, analyze their pros and cons, highlight research gaps, and attempt to fill them. Specifically, we add Kernel Density Estimation in the point-wise modeling procedure and multi-layer extraction in the uncertainty band extraction procedure. This step shows the ensemble data’s details accurately and provides abstract levels. We also analyze existing methods from a global perspective. We investigate their mechanisms and compare their effects, on the basis of which, we offer selection guidelines for them. From the overall perspective of this framework, we find choices and combinations that have not been tried before, which can be well compensated by our method. Synthetic data and real-world data are leveraged to verify the efficacy of our method. Domain experts’ feedback suggests that our approach helps them better understand ensemble data analysis.},
  archive      = {J_TVCG},
  author       = {Mingdong Zhang and Quan Li and Li Chen and Xiaoru Yuan and Junhai Yong},
  doi          = {10.1109/TVCG.2021.3140153},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2067-2079},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EnConVis: A unified framework for ensemble contour visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain and disentangled face manipulation with 3D
guidance. <em>TVCG</em>, <em>29</em>(4), 2053–2066. (<a
href="https://doi.org/10.1109/TVCG.2021.3139913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image manipulation via three-dimensional guidance has been widely applied in various interactive scenarios due to its semantically-meaningful understanding and user-friendly controllability. However, existing 3D-morphable-model-based manipulation methods are not directly applicable to out-of-domain faces, such as non-photorealistic paintings, cartoon portraits, or even animals, mainly due to the formidable difficulties in building the model for each specific face domain. To overcome this challenge, we propose, as far as we know, the first method to manipulate faces in arbitrary domains using human 3DMM. This is achieved through two major steps: 1) disentangled mapping from 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2 [1] that guarantees disentangled and precise controls for each semantic attribute; and 2) cross-domain adaptation that bridges domain discrepancies and makes human 3DMM applicable to out-of-domain faces by enforcing a consistent latent space embedding. Experiments and comparisons demonstrate the superiority of our high-quality semantic manipulation method on a variety of face domains with all major 3D facial attributes controllable – pose, expression, shape, albedo, and illumination. Moreover, we develop an intuitive editing interface to support user-friendly control and instant feedback. Our project page is https://cassiepython.github.io/cddfm3d/index.html .},
  archive      = {J_TVCG},
  author       = {Can Wang and Menglei Chai and Mingming He and Dongdong Chen and Jing Liao},
  doi          = {10.1109/TVCG.2021.3139913},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2053-2066},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cross-domain and disentangled face manipulation with 3D guidance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous crowd simulation using parametric
reinforcement learning. <em>TVCG</em>, <em>29</em>(4), 2036–2052. (<a
href="https://doi.org/10.1109/TVCG.2021.3139031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based synthetic crowd simulation affords the cost-effective large-scale simulation and animation of interacting digital humans. Model-based approaches have successfully generated a plethora of simulators with a variety of foundations. However, prior approaches have been based on statically defined models predicated on simplifying assumptions, limited video-based datasets, or homogeneous policies. Recent works have applied reinforcement learning to learn policies for navigation. However, these approaches may learn static homogeneous rules, are typically limited in their generalization to trained scenarios, and limited in their usability in synthetic crowd domains. In this article, we present a multi-agent reinforcement learning-based approach that learns a parametric predictive collision avoidance and steering policy. We show that training over a parameter space produces a flexible model across crowd configurations. That is, our goal-conditioned approach learns a parametric policy that affords heterogeneous synthetic crowds. We propose a model-free approach without centralization of internal agent information, control signals, or agent communication. The model is extensively evaluated. The results show policy generalization across unseen scenarios, agent parameters, and out-of-distribution parameterizations. The learned model has comparable computational performance to traditional methods. Qualitatively the model produces both expected (laminar flow, shuffling, bottleneck) and unexpected (side-stepping) emergent qualitative behaviours, and quantitatively the approach is performant across measures of movement quality.},
  archive      = {J_TVCG},
  author       = {Kaidong Hu and Brandon Haworth and Glen Berseth and Vladimir Pavlovic and Petros Faloutsos and Mubbasir Kapadia},
  doi          = {10.1109/TVCG.2021.3139031},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2036-2052},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Heterogeneous crowd simulation using parametric reinforcement learning},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A predictive visual analytics system for studying
neurodegenerative disease based on DTI fiber tracts. <em>TVCG</em>,
<em>29</em>(4), 2020–2035. (<a
href="https://doi.org/10.1109/TVCG.2021.3137174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system’s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson’s Progression Markers Initiative.},
  archive      = {J_TVCG},
  author       = {Chaoqing Xu and Tyson Neuroth and Takanori Fujiwara and Ronghua Liang and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2021.3137174},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2020-2035},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A predictive visual analytics system for studying neurodegenerative disease based on DTI fiber tracts},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HaptoMapping: Visuo-haptic augmented reality by embedding
user-imperceptible tactile display control signals in a projected image.
<em>TVCG</em>, <em>29</em>(4), 2005–2019. (<a
href="https://doi.org/10.1109/TVCG.2021.3136214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes HaptoMapping, a projection-based visuo-haptic augmented reality (VHAR) system, that can render visual and haptic content independently and present consistent visuo-haptic sensations on physical surfaces. HaptoMapping controls wearable haptic displays by embedded control signals that are imperceptible to the user in projected images using a pixel-level visible light communication technique. The prototype system is comprised of a high-speed projector and three types of haptic devices—finger worn, stylus, and arm mounted. The finger-worn and stylus devices present vibrotactile sensations to a user&#39;s fingertips. The arm-mounted device presents stroking sensations on a user&#39;s forearm using arrayed actuators with a synchronized hand projection mapping. We identified that the developed system&#39;s maximum latency of haptic from visual sensations was 93.4 ms. We conducted user studies on the latency perception of our VHAR system. The results revealed that the developed haptic devices can present haptic sensations without user-perceivable latencies, and the visual-haptic latency tolerance of our VHAR system was 100, 159, 500 ms for the finger-worn, stylus, and arm-mounted devices, respectively. Another user study with the arm-mounted device discovered that the visuo-haptic stroking system maintained both continuity and pleasantness when the spacing between each substrate was relatively sparse, such as 20 mm, and significantly improved both the continuity and pleasantness at 80 and 150 mm/s when compared to the haptic only stroking system. Lastly, we introduced four potential applications in daily scenes. Our system methodology allows for a wide range of VHAR application design without concern for latency and misalignment effects.},
  archive      = {J_TVCG},
  author       = {Yamato Miyatake and Takefumi Hiraki and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2021.3136214},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2005-2019},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HaptoMapping: Visuo-haptic augmented reality by embedding user-imperceptible tactile display control signals in a projected image},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EHTask: Recognizing user tasks from eye and head movements
in immersive virtual reality. <em>TVCG</em>, <em>29</em>(4), 1992–2004.
(<a href="https://doi.org/10.1109/TVCG.2021.3138902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e., Free viewing , Visual search , Saliency , and Track , in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask – a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of $84.4\%$ versus $62.8\%$ ) and on a real-world dataset ( $61.9\%$ versus $44.1\%$ ). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.},
  archive      = {J_TVCG},
  author       = {Zhiming Hu and Andreas Bulling and Sheng Li and Guoping Wang},
  doi          = {10.1109/TVCG.2021.3138902},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1992-2004},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EHTask: Recognizing user tasks from eye and head movements in immersive virtual reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time globally consistent 3D reconstruction with
semantic priors. <em>TVCG</em>, <em>29</em>(4), 1977–1991. (<a
href="https://doi.org/10.1109/TVCG.2021.3137912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining global consistency continues to be critical for online 3D indoor scene reconstruction. However, it is still challenging to generate satisfactory 3D reconstruction in terms of global consistency for previous approaches using purely geometric analysis, even with bundle adjustment or loop closure techniques. In this article, we propose a novel real-time 3D reconstruction approach which effectively integrates both semantic and geometric cues. The key challenge is how to map this indicative information, i.e., semantic priors, into a metric space as measurable information, thus enabling more accurate semantic fusion leveraging both the geometric and semantic cues. To this end, we introduce a semantic space with a continuous metric function measuring the distance between discrete semantic observations. Within the semantic space, we present an accurate frame-to-model semantic tracker for camera pose estimation, and semantic pose graph equipped with semantic links between submaps for globally consistent 3D scene reconstruction. With extensive evaluation on public synthetic and real-world 3D indoor scene RGB-D datasets, we show that our approach outperforms the previous approaches for 3D scene reconstruction both quantitatively and qualitatively, especially in terms of global consistency.},
  archive      = {J_TVCG},
  author       = {Shi-Sheng Huang and Haoxiang Chen and Jiahui Huang and Hongbo Fu and Shi-Min Hu},
  doi          = {10.1109/TVCG.2021.3137912},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1977-1991},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time globally consistent 3D reconstruction with semantic priors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metric-driven 3D frame field generation. <em>TVCG</em>,
<em>29</em>(4), 1964–1976. (<a
href="https://doi.org/10.1109/TVCG.2021.3136199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling the size and shear of elements is crucial in pure hex or hex-dominant meshing. To this end, non-orthonormal frame fields that are almost everywhere integrable (except for the singularities) can play a key role. However, it is often challenging or impossible to generate such a frame field under the tight control of a general Riemannian metric field. Therefore, we propose to solve a relatively weaker problem, i.e., generating such a frame field for a Riemannian metric field that is flat away from singularities. Such a metric field admits a local isometry to 3D Euclidean space. Applying Cartans first structural equation to the associated rotation field, i.e., the rotation part of the frame field, we show that the rotation field must have zero covariant derivatives under the 3D connection induced by the metric field. This observation leads to a metric-aware smoothness measure, equivalent to local integrability. The use of such a measure can be justified on meshes associated with locally flat metric fields. We also propose a method to generate smooth metric fields under a few intuitive constraints. On cuboid shapes, our method generates singularities aware of the metric fields, which makes the parameterization match the input metric fields better than the conventional methods. For generic shapes, while our method generates visually similar results to those using boundary frame fields to guide the metric field generation, the integrability and consistency of the metric fields are still improved, as reflected by the statistics.},
  archive      = {J_TVCG},
  author       = {Xianzhong Fang and Jin Huang and Yiying Tong and Hujun Bao},
  doi          = {10.1109/TVCG.2021.3136199},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1964-1976},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Metric-driven 3D frame field generation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A variational framework for curve shortening in various
geometric domains. <em>TVCG</em>, <em>29</em>(4), 1951–1963. (<a
href="https://doi.org/10.1109/TVCG.2021.3135021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geodesics measure the shortest distance (either locally or globally) between two points on a curved surface and serve as a fundamental tool in digital geometry processing. Suppose that we have a parameterized path $\gamma (t)=\mathbf {x}(u(t),v(t))$ on a surface $\mathbf {x}=\mathbf {x}(u,v)$ with $\gamma (0)=p$ and $\gamma (1)=q$ . We formulate the two-point geodesic problem into a minimization problem $\int _0^1 H(\Vert \mathbf {x}_uu^{\prime }(t)+\mathbf {x}_vv^{\prime }(t)\Vert)\text{d}t$ , where $H(s)$ satisfies $H(0)=0,H^{\prime }(s)&amp;gt;0$ and $H^{\prime \prime }(s)\geq 0$ for $s&amp;gt;0$ . In our implementation, we choose $H(s)=e^{s^2}-1$ and show that it has several unique advantages over other choices such as $H(s)=s^2$ and $H(s)=s$ . It is also a minimizer of the traditional geodesic length variational and able to guarantee the uniqueness and regularity in terms of curve parameterization. In the discrete setting, we construct the initial path by a sequence of moveable points $\lbrace x_i\rbrace _{i=1}^n$ and minimize $\sum _{i=1}^{n} H(\Vert x_i - x_{i+1}\Vert)$ . The resulting points are evenly spaced along the path. It’s obvious that our algorithm can deal with parametric surfaces. Considering that meshes, point clouds and implicit surfaces can be transformed into a signed distance function (SDF), we also discuss its implementation on a general SDF. Finally, we show that our method can be extended to solve a general least-cost path problem. We validate the proposed algorithm in terms of accuracy, performance and scalability, and demonstrate the advantages by extensive comparisons.},
  archive      = {J_TVCG},
  author       = {Na Yuan and Peihui Wang and Wenlong Meng and Shuangmin Chen and Jian Xu and Shiqing Xin and Ying He and Wenping Wang},
  doi          = {10.1109/TVCG.2021.3135021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1951-1963},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A variational framework for curve shortening in various geometric domains},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effects of virtual and physical elevation on
physiological stress during virtual reality height exposure.
<em>TVCG</em>, <em>29</em>(4), 1937–1950. (<a
href="https://doi.org/10.1109/TVCG.2021.3134412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in virtual reality technology have greatly benefited the acrophobia research field. Virtual reality height exposure is a reliable method of inducing stress with low variance across ages and demographics. When creating a virtual height exposure environment, researchers have often used haptic feedback elements to improve the sense of realism of a virtual environment. While the quality of the rendered for the virtual environment increases over time, the physical environment is often simplified to a conservative passive haptic feedback platform. The impact of the increasing disparity between the virtual and physical environment on the induced stress levels is unclear. This article presents an experiment that explored the effect of combining an elevated physical platform with different levels of virtual heights to induce stress. Eighteen participants experienced four different conditions of varying physical and virtual heights. The measurements included gait parameters, heart rate, heart rate variability, and electrodermal activity. The results show that the added physical elevation at a low virtual height shifts the participant’s walking behaviour and increases the perception of danger. However, the virtual environment still plays an essential role in manipulating height exposure and inducing physiological stress. Another finding is that a person’s behaviour always corresponds to the more significant perceived threat, whether from the physical or virtual environment.},
  archive      = {J_TVCG},
  author       = {Howe Yuan Zhu and Hsiang-Ting Chen and Chin-Teng Lin},
  doi          = {10.1109/TVCG.2021.3134412},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1937-1950},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effects of virtual and physical elevation on physiological stress during virtual reality height exposure},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating the sketchplan: A novel way of identifying
tactical behavior in massive soccer datasets. <em>TVCG</em>,
<em>29</em>(4), 1920–1936. (<a
href="https://doi.org/10.1109/TVCG.2021.3134814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coaches and analysts prepare for upcoming matches by identifying common patterns in the positioning and movement of the competing teams in specific situations. Existing approaches in this domain typically rely on manual video analysis and formation discussion using whiteboards; or expert systems that rely on state-of-the-art video and trajectory visualization techniques and advanced user interaction. We bridge the gap between these approaches by contributing a light-weight, simplified interaction and visualization system, which we conceptualized in an iterative design study with the coaching team of a European first league soccer team. Our approach is walk-up usable by all domain stakeholders, and at the same time, can leverage advanced data retrieval and analysis techniques: a virtual magnetic tactic-board. Users place and move digital magnets on a virtual tactic-board, and these interactions get translated to spatio-temporal queries, used to retrieve relevant situations from massive team movement data. Despite such seemingly imprecise query input, our approach is highly usable, supports quick user exploration, and retrieval of relevant results via query relaxation. Appropriate simplified result visualization supports in-depth analyses to explore team behavior, such as formation detection, movement analysis, and what-if analysis. We evaluated our approach with several experts from European first league soccer clubs. The results show that our approach makes the complex analytical processes needed for the identification of tactical behavior directly accessible to domain experts for the first time, demonstrating our support of coaches in preparation for future encounters.},
  archive      = {J_TVCG},
  author       = {Daniel Seebacher and Tom Polk and Halldor Janetzko and Daniel A. Keim and Tobias Schreck and Manuel Stein},
  doi          = {10.1109/TVCG.2021.3134814},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1920-1936},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the sketchplan: A novel way of identifying tactical behavior in massive soccer datasets},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Editorial a message from the new editor-in-chief.
<em>TVCG</em>, <em>29</em>(4), 1918–1919. (<a
href="https://doi.org/10.1109/TVCG.2023.3241463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Han-Wei Shen},
  doi          = {10.1109/TVCG.2023.3241463},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1918-1919},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editorial a message from the new editor-in-chief},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Editor’s note. <em>TVCG</em>, <em>29</em>(4), 1910–1917.
(<a href="https://doi.org/10.1109/TVCG.2023.3237121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Han-Wei Shen},
  doi          = {10.1109/TVCG.2023.3237121},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {1910-1917},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editor&#39;s note},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large growth deformations of thin tissue using solid-shells.
<em>TVCG</em>, <em>29</em>(3), 1893–1909. (<a
href="https://doi.org/10.1109/TVCG.2022.3217008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating large scale expansion of thin structures, such as in growing leaves, is challenging. Solid-shells have a number of potential advantages over conventional thin-shell methods, but have thus far only been investigated for small plastic deformation cases. In response, we present a new general-purpose FEM growth framework for handling a wide range of challenging growth scenarios using the solid-shell element. Solid-shells are a middle-ground between traditional volume and thin-shell elements where volumetric characteristics are retained while being treatable as a 2D manifold much like thin-shells. These elements are adaptable to accommodate the many techniques that are required for simulating large and intricate plastic deformations, including morphogen diffusion, plastic embedding, strain-aware adaptive remeshing, and collision handling. We demonstrate the capabilities of growing solid-shells in reproducing buckling, rippling, curling, and collision deformations, relevant towards animating growing leaves, flowers, and other thin structures. Solid-shells are compared side-by-side with thin-shells to examine their bending behavior and runtime performance. The experiments demonstrate that solid-shells are a viable alternative to thin-shells for simulating large and intricate growth deformations.},
  archive      = {J_TVCG},
  author       = {Danny Huang and Ian Stavness},
  doi          = {10.1109/TVCG.2022.3217008},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1893-1909},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Large growth deformations of thin tissue using solid-shells},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GUCCI - guided cardiac cohort investigation of blood flow
data. <em>TVCG</em>, <em>29</em>(3), 1876–1892. (<a
href="https://doi.org/10.1109/TVCG.2021.3134083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple data sets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta. The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.},
  archive      = {J_TVCG},
  author       = {Monique Meuschke and Uli Niemann and Benjamin Behrendt and Matthias Gutberlet and Bernhard Preim and Kai Lawonn},
  doi          = {10.1109/TVCG.2021.3134083},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1876-1892},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GUCCI - guided cardiac cohort investigation of blood flow data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nanotilus: Generator of immersive guided-tours in crowded 3D
environments. <em>TVCG</em>, <em>29</em>(3), 1860–1875. (<a
href="https://doi.org/10.1109/TVCG.2021.3133592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This article proposes Nanotilus —a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models – SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.},
  archive      = {J_TVCG},
  author       = {Ruwayda Alharbi and Ondřej Strnad and Laura R. Luidolt and Manuela Waldner and David Kouřil and Ciril Bohak and Tobias Klein and Eduard Gröller and Ivan Viola},
  doi          = {10.1109/TVCG.2021.3133592},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1860-1875},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Nanotilus: Generator of immersive guided-tours in crowded 3D environments},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seamless texture optimization for RGB-d reconstruction.
<em>TVCG</em>, <em>29</em>(3), 1845–1859. (<a
href="https://doi.org/10.1109/TVCG.2021.3134105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring high-fidelity textures for 3D reconstructed models are an increasing demand in AR/VR, cultural heritage protection, entertainment, and other relevant fields. Due to geometric errors and camera pose drifting, existing texture mapping algorithms are either plagued by blurring and ghosting or suffer from undesirable visual seams. In this paper, we propose a novel tri-directional similarity texture synthesis method to eliminate the texture inconsistency in RGB-D 3D reconstruction and generate visually realistic texture mapping results. In addition to RGB color information, we incorporate a novel color image texture detail layer serving as an additional context to improve the effectiveness and robustness of the proposed method. First, we select an optimal texture image for each triangle face of the reconstructed model to avoid texture blurring and ghosting. During the selection procedure, the texture details are weighted to avoid generating texture chart partitions across high-frequency areas. Then, we optimize the camera pose of each texture image to align with the reconstructed 3D shape. Next, we propose a tri-directional similarity function to resynthesize the image context within the boundary stripe of texture charts, which can significantly diminish the occurrence of texture seams. Finally, we introduce a global color harmonization method to address the color inconsistency between texture images captured from different viewpoints. The experimental results demonstrate that the proposed method outperforms state-of-the-art texture mapping methods and effectively overcomes texture tearing, blurring, and ghosting artifacts.},
  archive      = {J_TVCG},
  author       = {Yanping Fu and Qingan Yan and Jie Liao and Huajian Zhou and Jin Tang and Chunxia Xiao},
  doi          = {10.1109/TVCG.2021.3134105},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1845-1859},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seamless texture optimization for RGB-D reconstruction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Labeling out-of-view objects in immersive analytics to
support situated visual searching. <em>TVCG</em>, <em>29</em>(3),
1831–1844. (<a href="https://doi.org/10.1109/TVCG.2021.3133511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ , thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this article, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.},
  archive      = {J_TVCG},
  author       = {Tica Lin and Yalong Yang and Johanna Beyer and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2021.3133511},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1831-1844},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Labeling out-of-view objects in immersive analytics to support situated visual searching},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Neural photometry-guided visual attribute transfer.
<em>TVCG</em>, <em>29</em>(3), 1818–1830. (<a
href="https://doi.org/10.1109/TVCG.2021.3133081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep learning-based method for propagating spatially-varying visual material attributes (e.g., texture maps or image stylizations) to larger samples of the same or similar materials. For training, we leverage images of the material taken under multiple illuminations and a dedicated data augmentation policy, making the transfer robust to novel illumination conditions and affine deformations. Our model relies on a supervised image-to-image translation framework and is agnostic to the transferred domain; we showcase a semantic segmentation, a normal map, and a stylization. Following an image analogies approach, the method only requires the training data to contain the same visual structures as the input guidance. Our approach works at interactive rates, making it suitable for material edit applications. We thoroughly evaluate our learning methodology in a controlled setup providing quantitative measures of performance. Last, we demonstrate that training the model on a single material is enough to generalize to materials of the same type without the need for massive datasets.},
  archive      = {J_TVCG},
  author       = {Carlos Rodriguez-Pardo and Elena Garces},
  doi          = {10.1109/TVCG.2021.3133081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1818-1830},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural photometry-guided visual attribute transfer},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RCMVis: A visual analytics system for route choice modeling.
<em>TVCG</em>, <em>29</em>(3), 1799–1817. (<a
href="https://doi.org/10.1109/TVCG.2021.3131824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers’ route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a $k$ -medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.},
  archive      = {J_TVCG},
  author       = {DongHwa Shin and Jaemin Jo and Bohyoung Kim and Hyunjoo Song and Shin-Hyung Cho and Jinwook Seo},
  doi          = {10.1109/TVCG.2021.3131824},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1799-1817},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RCMVis: A visual analytics system for route choice modeling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STD-net: Structure-preserving and topology-adaptive
deformation network for single-view 3D reconstruction. <em>TVCG</em>,
<em>29</em>(3), 1785–1798. (<a
href="https://doi.org/10.1109/TVCG.2021.3131712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction from single-view images is a long-standing research problem. There have been various methods based on point clouds and volumetric representations. In spite of success in 3D models generation, it is quite challenging for these approaches to deal with models with complex topology and fine geometric details. Thanks to the recent advance of deep shape representations, learning the structure and detail representation using deep neural networks is a promising direction. In this article, we propose a novel approach named STD-Net to reconstruct 3D models utilizing mesh representation that is well suited for characterizing complex structures and geometry details. Our method consists of (1) an auto-encoder network for recovering the structure of an object with bounding box representation from a single-view image; (2) a topology-adaptive GCN for updating vertex position for meshes of complex topology; and (3) a unified mesh deformation block that deforms the structural boxes into structure-aware meshes. Evaluation on ShapeNet and PartNet shows that STD-Net has better performance than state-of-the-art methods in reconstructing complex structures and fine geometric details.},
  archive      = {J_TVCG},
  author       = {Aihua Mao and Canglan Dai and Qing Liu and Jie Yang and Lin Gao and Ying He and Yong-Jin Liu},
  doi          = {10.1109/TVCG.2021.3131712},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1785-1798},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STD-net: Structure-preserving and topology-adaptive deformation network for single-view 3D reconstruction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-calibrated multi-sensor wearable for hand tracking and
modeling. <em>TVCG</em>, <em>29</em>(3), 1769–1784. (<a
href="https://doi.org/10.1109/TVCG.2021.3131230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-sensor system for consistent 3D hand pose tracking and modeling that leverages the advantages of both wearable and optical sensors. Specifically, we employ a stretch-sensing soft glove and three IMUs in combination with an RGB-D camera. Different sensor modalities are fused based on the availability and confidence estimation, enabling seamless hand tracking in challenging environments with partial or even complete occlusion. To maximize the accuracy while maintaining high ease-of-use, we propose an automated user calibration that uses the RGB-D camera data to refine both the glove mapping model and the multi-IMU system parameters. Extensive experiments show that our setup outperforms the wearable-only approaches when the hand is in the field-of-view and outplays the camera-only methods when the hand is occluded.},
  archive      = {J_TVCG},
  author       = {Nikhil Gosala and Fangjinhua Wang and Zhaopeng Cui and Hanxue Liang and Oliver Glauser and Shihao Wu and Olga Sorkine-Hornung},
  doi          = {10.1109/TVCG.2021.3131230},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1769-1784},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-calibrated multi-sensor wearable for hand tracking and modeling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leaning-based interfaces improve ground-based VR locomotion
in reach-the-target, follow-the-path, and racing tasks. <em>TVCG</em>,
<em>29</em>(3), 1748–1768. (<a
href="https://doi.org/10.1109/TVCG.2021.3131422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using standard handheld interfaces for VR locomotion may not provide a believable self-motion experience and can contribute to unwanted side effects such as motion sickness, disorientation, or increased cognitive load. This paper demonstrates how using a seated leaning-based locomotion interface –HeadJoystick– in VR ground-based navigation affects user experience, usability, and performance. In three within-subject studies, we compared controller (touchpad/thumbstick) with a more embodied interface (”HeadJoystick”) where users moved their head and/or leaned in the direction of desired locomotion. In both conditions, users sat on a regular office chair and used it to control virtual rotations. In the first study, 24 participants used HeadJoystick versus Controller in three complementary tasks including reach-the-target, follow-the-path, and racing (dynamic obstacle avoidance). In the second study, 18 participants repeatedly used HeadJoystick versus Controller (8 one-minute trials each) in a reach-the-target task. To evaluate potential benefits of different brake mechanisms, in the third study 18 participants were asked to stop within each target area for one second. All three studies consistently showed advantages of HeadJoystick over Controller: we observed improved performance in all tasks, as well as higher user ratings for enjoyment, spatial presence, immersion, vection intensity, usability, ease of learning, ease of use, and rated potential for daily and long-term use, while reducing motion sickness and task load. Overall, our results suggest that leaning-based interfaces such as HeadJoystick provide an interesting and more embodied alternative to handheld interfaces in driving, reach-the-target, and follow-the-path tasks, and potentially a wider range of scenarios.},
  archive      = {J_TVCG},
  author       = {Abraham M. Hashemian and Ashu Adhikari and Ernst Kruijff and Markus von der Heyde and Bernhard E. Riecke},
  doi          = {10.1109/TVCG.2021.3131422},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1748-1768},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leaning-based interfaces improve ground-based VR locomotion in reach-the-target, follow-the-path, and racing tasks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Molecumentary: Adaptable narrated documentaries using
molecular visualization. <em>TVCG</em>, <em>29</em>(3), 1733–1747. (<a
href="https://doi.org/10.1109/TVCG.2021.3130670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.},
  archive      = {J_TVCG},
  author       = {David Kouřil and Ondřej Strnad and Peter Mindek and Sarkis Halladjian and Tobias Isenberg and M. Eduard Gröller and Ivan Viola},
  doi          = {10.1109/TVCG.2021.3130670},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1733-1747},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Molecumentary: Adaptable narrated documentaries using molecular visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SimuExplorer: Visual exploration of game simulation in table
tennis. <em>TVCG</em>, <em>29</em>(3), 1719–1732. (<a
href="https://doi.org/10.1109/TVCG.2021.3130422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose SimuExplorer, a visualization system to help analysts explore how player behaviors impact scoring rates in table tennis. Such analysis is indispensable for analysts and coaches, who aim to formulate training plans that can help players improve. However, it is challenging to identify the impacts of individual behaviors, as well as to understand how these impacts are generated and accumulated gradually over the course of a game. To address these challenges, we worked closely with experts who work for a top national table tennis team to design SimuExplorer. The SimuExplorer system integrates a Markov chain model to simulate individual and cumulative impacts of particular behaviors. It then provides flow and matrix views to help users visualize and interpret these impacts. We demonstrate the usefulness of the system with case studies and expert interviews. The experts think highly of the system and have obtained insights into players’ behaviors using it.},
  archive      = {J_TVCG},
  author       = {Ji Lan and Zheng Zhou and Jiachen Wang and Hui Zhang and Xiao Xie and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3130422},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1719-1732},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SimuExplorer: Visual exploration of game simulation in table tennis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based information block detection in infographic with
gestalt organization principles. <em>TVCG</em>, <em>29</em>(3),
1705–1718. (<a href="https://doi.org/10.1109/TVCG.2021.3130071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An infographic is a type of visualization chart that displays pieces of information through information blocks. Existing information block detection work utilizes spatial proximity to group elements into several information blocks. However, prior studies ignore the chromatic and structural features of the infographic, resulting in incorrect omissions when detecting information blocks. To alleviate this kind of error, we use a scene graph to represent an infographic and propose a graph-based information block detection model to group elements based on Gestalt Organization Principles (spatial proximity, chromatic similarity, and structural similarity principle). We also construct a new dataset for information block detection. Quantitative and qualitative experiments show that our model can detect the information blocks in the infographic more effectively compared with the spatial proximity-based method.},
  archive      = {J_TVCG},
  author       = {Jie Lin and Yi Cai and Xin Wu and Jianwei Lu},
  doi          = {10.1109/TVCG.2021.3130071},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1705-1718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Graph-based information block detection in infographic with gestalt organization principles},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable comparative visualization of ensembles of call
graphs. <em>TVCG</em>, <em>29</em>(3), 1691–1704. (<a
href="https://doi.org/10.1109/TVCG.2021.3129414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the performance of large-scale parallel codes is critical for efficient utilization of computing resources. Code developers often explore various execution parameters, such as hardware configurations, system software choices, and application parameters, and are interested in detecting and understanding bottlenecks in different executions. They often collect hierarchical performance profiles represented as call graphs, which combine performance metrics with their execution contexts. The crucial task of exploring multiple call graphs together is tedious and challenging because of the many structural differences in the execution contexts and significant variability in the collected performance metrics (e.g., execution runtime). In this paper, we present Ensemble CallFlow to support the exploration of ensembles of call graphs using new types of visualizations, analysis, graph operations, and features. We introduce ensemble-Sankey , a new visual design that combines the strengths of resource-flow (Sankey) and box-plot visualization techniques. Whereas the resource-flow visualization can easily and intuitively describe the graphical nature of the call graph, the box plots overlaid on the nodes of Sankey convey the performance variability within the ensemble. Our interactive visual interface provides linked views to help explore ensembles of call graphs, e.g., by facilitating the analysis of structural differences, and identifying similar or distinct call graphs. We demonstrate the effectiveness and usefulness of our design through case studies on large-scale parallel codes.},
  archive      = {J_TVCG},
  author       = {Suraj P. Kesavan and Harsh Bhatia and Abhinav Bhatele and Stephanie Brink and Olga Pearce and Todd Gamblin and Peer-Timo Bremer and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2021.3129414},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1691-1704},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable comparative visualization of ensembles of call graphs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mesh convolutional networks with face and vertex feature
operators. <em>TVCG</em>, <em>29</em>(3), 1678–1690. (<a
href="https://doi.org/10.1109/TVCG.2021.3129156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have proven effective in many applications, but these implementations mostly apply to data in one or two dimensions. Handling 3D data is more challenging due to its irregularity and complexity, and there is a growing interest in adapting deep learning techniques to the 3D domain. A recent successful approach called MeshCNN consists of a set of convolutional and pooling operators applied to the edges of triangular meshes. While this approach produced superb results in classification and segmentation of 3D shapes, it can only be applied to edges of a mesh, which can constitute a disadvantage for applications where the focuses are other primitives of the mesh. In this study, we propose face-based and vertex-based operators for mesh convolutional networks. We design two novel architectures based on the MeshCNN network that can operate on faces and vertices of a mesh, respectively. We demonstrate that the proposed face-based architecture outperforms the original MeshCNN implementation in mesh classification and mesh segmentation, setting the new state of the art on benchmark datasets. In addition, we extend the vertex-based operator to fit in the Point2Mesh model for mesh reconstruction from clean, noisy, and incomplete point clouds. While no statistically significant performance improvements are observed, the model training and inference time are reduced by the proposed approach by 91\% and 20\%, respectively, as compared with the original Point2Mesh model.},
  archive      = {J_TVCG},
  author       = {Daniel Perez and Yuzhong Shen and Jiang Li},
  doi          = {10.1109/TVCG.2021.3129156},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1678-1690},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh convolutional networks with face and vertex feature operators},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A calibrated force-based model for mixed traffic simulation.
<em>TVCG</em>, <em>29</em>(3), 1664–1677. (<a
href="https://doi.org/10.1109/TVCG.2021.3128286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual traffic benefits a variety of applications, including video games, traffic engineering, autonomous driving, and virtual reality. To date, traffic visualization via different simulation models can reconstruct detailed traffic flows. However, each specific behavior of vehicles is always described by establishing an independent control model. Moreover, mutual interactions between vehicles and other road users are rarely modeled in existing simulators. An all-in-one simulator that considers the complex behaviors of all potential road users in a realistic urban environment is urgently needed. In this work, we propose a novel, extensible, and microscopic method to build heterogeneous traffic simulation using the force-based concept. This force-based approach can accurately replicate the sophisticated behaviors of various road users and their interactions in a simple and unified manner. We calibrate the model parameters using real-world traffic trajectory data. The effectiveness of this approach is demonstrated through many simulation experiments, as well as comparisons to real-world traffic data and popular microscopic simulators for traffic animation.},
  archive      = {J_TVCG},
  author       = {Qianwen Chao and Pengfei Liu and Yi Han and Yingying Lin and Chaoneng Li and Qiguang Miao and Xiaogang Jin},
  doi          = {10.1109/TVCG.2021.3128286},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1664-1677},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A calibrated force-based model for mixed traffic simulation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Volume exploration using multidimensional bhattacharyya
flow. <em>TVCG</em>, <em>29</em>(3), 1651–1663. (<a
href="https://doi.org/10.1109/TVCG.2021.3127918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for volume exploration that is versatile yet effective in isolating semantic structures in both noisy and clean data. Specifically, we describe a hierarchical active contours approach based on Bhattacharyya gradient flow which is easier to control, robust to noise, and can incorporate various types of statistical information to drive an edge-agnostic exploration process. To facilitate a time-bound user-driven volume exploration process that is applicable to a wide variety of data sources, we present an efficient multi-GPU implementation that (1) is approximately 400 times faster than a single thread CPU implementation, (2) allows hierarchical exploration of 2D and 3D images, (3) supports customization through multidimensional attribute spaces, and (4) is applicable to a variety of data sources and semantic structures. The exploration system follows a 2-step process. It first applies active contours to isolate semantically meaningful subsets of the volume. It then applies transfer functions to the isolated regions locally to produce clear and clutter-free visualizations. We show the effectiveness of our approach in isolating and visualizing structures-of-interest without needing any specialized segmentation methods on a variety of data sources, including 3D optical microscopy, multi-channel optical volumes, abdominal and chest CT, micro-CT, MRI, simulation, and synthetic data. We also gathered feedback from a medical trainee regarding the usefulness of our approach and discussion on potential applications in clinical workflows.},
  archive      = {J_TVCG},
  author       = {Shreeraj Jadhav and Mahsa Torkaman and Allen Tannenbaum and Saad Nadeem and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2021.3127918},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1651-1663},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Volume exploration using multidimensional bhattacharyya flow},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining with examples: Lessons learned from crowdsourced
introductory description of information visualizations. <em>TVCG</em>,
<em>29</em>(3), 1638–1650. (<a
href="https://doi.org/10.1109/TVCG.2021.3128157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.},
  archive      = {J_TVCG},
  author       = {Leni Yang and Cindy Xiong and Jason K. Wong and Aoyu Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2021.3128157},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1638-1650},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explaining with examples: Lessons learned from crowdsourced introductory description of information visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuRegenerate: A framework for visualizing
neurodegeneration. <em>TVCG</em>, <em>29</em>(3), 1625–1637. (<a
href="https://doi.org/10.1109/TVCG.2021.3127132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this article, we introduce NeuRegenerate , a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present neuReGANerator , a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) neuroMorph , a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.},
  archive      = {J_TVCG},
  author       = {Saeed Boorboor and Shawn Mathew and Mala Ananth and David Talmage and Lorna W. Role and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2021.3127132},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1625-1637},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeuRegenerate: A framework for visualizing neurodegeneration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ActFloor-GAN: Activity-guided adversarial networks for
human-centric floorplan design. <em>TVCG</em>, <em>29</em>(3),
1610–1624. (<a href="https://doi.org/10.1109/TVCG.2021.3126478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel two-stage approach for automated floorplan design in residential buildings with a given exterior wall boundary. Our approach has the unique advantage of being human-centric, that is, the generated floorplans can be geometrically plausible, as well as topologically reasonable to enhance resident interaction with the environment. From the input boundary, we first synthesize a human-activity map that reflects both the spatial configuration and human-environment interaction in an architectural space. We propose to produce the human-activity map either automatically by a pre-trained generative adversarial network (GAN) model, or semi-automatically by synthesizing it with user manipulation of the furniture. Second, we feed the human-activity map into our deep framework ActFloor-GAN to guide a pixel-wise prediction of room types. We adopt a re-formulated cycle-consistency constraint in ActFloor-GAN to maximize the overall prediction performance, so that we can produce high-quality room layouts that are readily convertible to vectorized floorplans. Experimental results show several benefits of our approach. First, a quantitative comparison with prior methods shows superior performance of leveraging the human-activity map in predicting piecewise room types. Second, a subjective evaluation by architects shows that our results have compelling quality as professionally-designed floorplans and much better than those generated by existing methods in terms of the room layout topology. Last, our approach allows manipulating the furniture placement, considers the human activities in the environment, and enables the incorporation of user-design preferences.},
  archive      = {J_TVCG},
  author       = {Shidong Wang and Wei Zeng and Xi Chen and Yu Ye and Yu Qiao and Chi-Wing Fu},
  doi          = {10.1109/TVCG.2021.3126478},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1610-1624},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ActFloor-GAN: Activity-guided adversarial networks for human-centric floorplan design},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occlusion handling in augmented reality: Past, present and
future. <em>TVCG</em>, <em>29</em>(2), 1590–1609. (<a
href="https://doi.org/10.1109/TVCG.2021.3117866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main goals of many augmented reality applications is to provide a seamless integration of a real scene with additional virtual data. To fully achieve that goal, such applications must typically provide high-quality real-world tracking, support real-time performance and handle the mutual occlusion problem, estimating the position of the virtual data into the real scene and rendering the virtual content accordingly. In this survey, we focus on the occlusion handling problem in augmented reality applications and provide a detailed review of 161 articles published in this field between January 1992 and August 2020. To do so, we present a historical overview of the most common strategies employed to determine the depth order between real and virtual objects, to visualize hidden objects in a real scene, and to build occlusion-capable visual displays. Moreover, we look at the state-of-the-art techniques, highlight the recent research trends, discuss the current open problems of occlusion handling in augmented reality, and suggest future directions for research.},
  archive      = {J_TVCG},
  author       = {Márcio C. F. Macedo and Antônio L. Apolinário},
  doi          = {10.1109/TVCG.2021.3117866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1590-1609},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Occlusion handling in augmented reality: Past, present and future},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principal geodesic analysis of merge trees (and persistence
diagrams). <em>TVCG</em>, <em>29</em>(2), 1573–1589. (<a
href="https://doi.org/10.1109/TVCG.2022.3215001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a computational framework for the Principal Geodesic Analysis of merge trees (MT-PGA), a novel adaptation of the celebrated Principal Component Analysis (PCA) framework (K. Pearson 1901) to the Wasserstein metric space of merge trees (Pont et al. 2022). We formulate MT-PGA computation as a constrained optimization problem, aiming at adjusting a basis of orthogonal geodesic axes, while minimizing a fitting energy. We introduce an efficient, iterative algorithm which exploits shared-memory parallelism, as well as an analytic expression of the fitting energy gradient, to ensure fast iterations. Our approach also trivially extends to extremum persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our approach – with MT-PGA computations in the orders of minutes for the largest examples. We show the utility of our contributions by extending to merge trees two typical PCA applications. First, we apply MT-PGA to data reduction and reliably compress merge trees by concisely representing them by their first coordinates in the MT-PGA basis. Second, we present a dimensionality reduction framework exploiting the first two directions of the MT-PGA basis to generate two-dimensional layouts of the ensemble. We augment these layouts with persistence correlation views, enabling global and local visual inspections of the feature variability in the ensemble. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used to reproduce our results.},
  archive      = {J_TVCG},
  author       = {Mathieu Pont and Jules Vidal and Julien Tierny},
  doi          = {10.1109/TVCG.2022.3215001},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1573-1589},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Principal geodesic analysis of merge trees (and persistence diagrams)},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UnProjection: Leveraging inverse-projections for visual
analytics of high-dimensional data. <em>TVCG</em>, <em>29</em>(2),
1559–1572. (<a href="https://doi.org/10.1109/TVCG.2021.3125576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection techniques are often used to visualize high-dimensional data, allowing users to better understand the overall structure of multi-dimensional spaces on a 2D screen. Although many such methods exist, comparably little work has been done on generalizable methods of inverse-projection – the process of mapping the projected points, or more generally, the projection space back to the original high-dimensional space. In this article we present NNInv, a deep learning technique with the ability to approximate the inverse of any projection or mapping. NNInv learns to reconstruct high-dimensional data from any arbitrary point on a 2D projection space, giving users the ability to interact with the learned high-dimensional representation in a visual analytics system. We provide an analysis of the parameter space of NNInv, and offer guidance in selecting these parameters. We extend validation of the effectiveness of NNInv through a series of quantitative and qualitative analyses. We then demonstrate the method’s utility by applying it to three visualization tasks: interactive instance interpolation, classifier agreement, and gradient visualization.},
  archive      = {J_TVCG},
  author       = {Mateus Espadoto and Gabriel Appleby and Ashley Suh and Dylan Cashman and Mingwei Li and Carlos Scheidegger and Erik W. Anderson and Remco Chang and Alexandru C. Telea},
  doi          = {10.1109/TVCG.2021.3125576},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1559-1572},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UnProjection: Leveraging inverse-projections for visual analytics of high-dimensional data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighbor reweighted local centroid for geometric feature
identification. <em>TVCG</em>, <em>29</em>(2), 1545–1558. (<a
href="https://doi.org/10.1109/TVCG.2021.3124911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying geometric features from sampled surfaces is a significant and fundamental task. The existing curvature-based methods that can identify ridge and valley features are generally sensitive to noise. Without requiring high-order differential operators, most statistics-based methods sacrifice certain extents of the feature descriptive powers in exchange for robustness. However, neither of these types of methods can treat the surface boundary features simultaneously. In this paper, we propose a novel neighbor reweighted local centroid (NRLC) computational algorithm to identify geometric features for point cloud models. It constructs a feature descriptor for the considered point via decomposing each of its neighboring vectors into two orthogonal directions. A neighboring vector starts from the considered point and ends with the corresponding neighbor. The decomposed neighboring vectors are then accumulated with different weights to generate the NRLC. With the defined NRLC, we design a probability set for each candidate feature point so that the convex, concave and surface boundary points can be recognized concurrently. In addition, we introduce a pair of feature operators, including assimilation and dissimilation, to further strengthen the identified geometric features. Finally, we test NRLC on a large body of point cloud models derived from different data sources. Several groups of the comparison experiments are conducted, and the results verify the validity and efficiency of our NRLC method.},
  archive      = {J_TVCG},
  author       = {Tong Liu and Zhenhua Yang and Shaojun Hu and Zhiyi Zhang and Chunxia Xiao and Xiaohu Guo and Long Yang},
  doi          = {10.1109/TVCG.2021.3124911},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1545-1558},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neighbor reweighted local centroid for geometric feature identification},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic mode decomposition for large-scale coherent
structure extraction in shear flows. <em>TVCG</em>, <em>29</em>(2),
1531–1544. (<a href="https://doi.org/10.1109/TVCG.2021.3124729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.},
  archive      = {J_TVCG},
  author       = {Duong B. Nguyen and Panruo Wu and Rodolfo Ostilla Monico and Guoning Chen},
  doi          = {10.1109/TVCG.2021.3124729},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1531-1544},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic mode decomposition for large-scale coherent structure extraction in shear flows},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative analysis of merge trees using local tree edit
distance. <em>TVCG</em>, <em>29</em>(2), 1518–1530. (<a
href="https://doi.org/10.1109/TVCG.2021.3122176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.},
  archive      = {J_TVCG},
  author       = {Raghavendra Sridharamurthy and Vijay Natarajan},
  doi          = {10.1109/TVCG.2021.3122176},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1518-1530},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative analysis of merge trees using local tree edit distance},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TopoCluster: A localized data structure for topology-based
visualization. <em>TVCG</em>, <em>29</em>(2), 1506–1517. (<a
href="https://doi.org/10.1109/TVCG.2021.3121229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The article proposes TopoCluster , a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only a modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.},
  archive      = {J_TVCG},
  author       = {Guoxi Liu and Federico Iuricich and Riccardo Fellegara and Leila De Floriani},
  doi          = {10.1109/TVCG.2021.3121229},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1506-1517},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TopoCluster: A localized data structure for topology-based visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time visualization of large-scale geological models
with nonlinear feature-preserving levels of detail. <em>TVCG</em>,
<em>29</em>(2), 1491–1505. (<a
href="https://doi.org/10.1109/TVCG.2021.3120372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation-invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability. We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.},
  archive      = {J_TVCG},
  author       = {Ronell Sicat and Mohamed Ibrahim and Amani Ageeli and Florian Mannuss and Peter Rautek and Markus Hadwiger},
  doi          = {10.1109/TVCG.2021.3120372},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1491-1505},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time visualization of large-scale geological models with nonlinear feature-preserving levels of detail},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive reset techniques for haptic retargeted interaction.
<em>TVCG</em>, <em>29</em>(2), 1478–1490. (<a
href="https://doi.org/10.1109/TVCG.2021.3120410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a set of adaptive reset techniques for use with haptic retargeting systems focusing on interaction with hybrid virtual reality interfaces that align with a physical interface. Haptic retargeting between changing physical and virtual targets requires a reset where the physical and virtual hand positions are re-aligned. We present a modified Point technique to guide the user in the direction of their next interaction such that the remaining distance to the target is minimized upon completion of the reset. This, along with techniques drawn from existing work are further modified to consider the angular and translational gain of each redirection and identify the optimal position for the reset to take place. When the angular and translational gain is within an acceptable range, the reset can be entirely omitted. This enables continuous retargeting between targets removing interruptions from a sequence of retargeted interactions. These techniques were evaluated in a user study which showed that adaptive reset techniques can provide a significant decrease in task completion time, travel distance, and the number of user errors.},
  archive      = {J_TVCG},
  author       = {Brandon J. Matthews and Bruce H. Thomas and G. Stewart Von Itzstein and Ross T. Smith},
  doi          = {10.1109/TVCG.2021.3120410},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1478-1490},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive reset techniques for haptic retargeted interaction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy spreadsheet: Understanding and exploring uncertainties
in tabular calculations. <em>TVCG</em>, <em>29</em>(2), 1463–1477. (<a
href="https://doi.org/10.1109/TVCG.2021.3119212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, overprecise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses . Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.},
  archive      = {J_TVCG},
  author       = {Vaishali Dhanoa and Conny Walchshofer and Andreas Hinterreiter and Eduard Gröller and Marc Streit},
  doi          = {10.1109/TVCG.2021.3119212},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1463-1477},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fuzzy spreadsheet: Understanding and exploring uncertainties in tabular calculations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural reflectance capture in the view-illumination domain.
<em>TVCG</em>, <em>29</em>(2), 1450–1462. (<a
href="https://doi.org/10.1109/TVCG.2021.3117370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework to efficiently capture the unknown reflectance on a non-planar 3D object, by learning to probe the 4D view-lighting domain with a high-performance illumination multiplexing setup. The core of our framework is a deep neural network, specifically tailored to exploit the multi-view coherence for efficiency. It takes as input the photometric measurements of a surface point under learned lighting patterns at different views, automatically aggregates the information and reconstructs the anisotropic reflectance. We also evaluate the impact of different sampling parameters over our network. The effectiveness of our framework is demonstrated on high-quality reconstructions of a variety of physical objects, with an acquisition efficiency outperforming state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Kaizhang Kang and Minyi Gu and Cihui Xie and Xuanda Yang and Hongzhi Wu and Kun Zhou},
  doi          = {10.1109/TVCG.2021.3117370},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1450-1462},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural reflectance capture in the view-illumination domain},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D talking face with personalized pose dynamics.
<em>TVCG</em>, <em>29</em>(2), 1438–1449. (<a
href="https://doi.org/10.1109/TVCG.2021.3117484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed a boom in applications for 3D talking face generation. However, most existing 3D face generation methods can only generate 3D faces with a static head pose, which is inconsistent with how humans perceive faces. Only a few articles focus on head pose generation, but even these ignore the attribute of personality. In this article, we propose a unified audio-driven approach to endow 3D talking faces with personalized pose dynamics. To achieve this goal, we establish an original person-specific dataset, providing corresponding head poses and face shapes for each video. Our framework is composed of two separate modules: PoseGAN and PGFace. Given an input audio, PoseGAN first produces a head pose sequence for the 3D head, and then, PGFace utilizes the audio and pose information to generate natural face models. With the combination of these two parts, a 3D talking head with dynamic head movement can be constructed. Experimental evidence indicates that our method can generate person-specific head pose sequences that are in sync with the input audio and that best match with the human experience of talking heads.},
  archive      = {J_TVCG},
  author       = {Chenxu Zhang and Saifeng Ni and Zhipeng Fan and Hongbo Li and Ming Zeng and Madhukar Budagavi and Xiaohu Guo},
  doi          = {10.1109/TVCG.2021.3117484},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1438-1449},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D talking face with personalized pose dynamics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DXplorer: A unified visualization framework for interactive
dendritic spine analysis using 3D morphological features. <em>TVCG</em>,
<em>29</em>(2), 1424–1437. (<a
href="https://doi.org/10.1109/TVCG.2021.3116656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer , that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer . We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.},
  archive      = {J_TVCG},
  author       = {JunYoung Choi and Sang-Eun Lee and YeIn Lee and Eunji Cho and Sunghoe Chang and Won-Ki Jeong},
  doi          = {10.1109/TVCG.2021.3116656},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1424-1437},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DXplorer: A unified visualization framework for interactive dendritic spine analysis using 3D morphological features},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from deep stereoscopic attention for simulator
sickness prediction. <em>TVCG</em>, <em>29</em>(2), 1415–1423. (<a
href="https://doi.org/10.1109/TVCG.2021.3115901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulator sickness induced by 360° stereoscopic video contents is a prolonged challenging issue in Virtual Reality (VR) system. Current machine learning models for simulator sickness prediction ignore the underlying interdependencies and correlations across multiple visual features which may lead to simulator sickness. We propose a model for sickness prediction by automatic learning and adaptive integrating multi-level mappings from stereoscopic video features to simulator sickness scores. Firstly, saliency, optical flow and disparity features are extracted from videos to reflect the factors causing simulator sickness, including human attention area, motion velocity and depth information. Then, these features are embedded and fed into a 3-dimensional convolutional neural network (3D CNN) to extract the underlying multi-level knowledge which includes low-level and higher-order visual concepts, and global image descriptor. Finally, an attentional mechanism is exploited to adaptively fuse multi-level information with attentional weights for sickness score estimation. The proposed model is trained by an end-to-end approach and validated over a public dataset. Comparison results with state-of-the-art models and ablation studies demonstrated improved performance in terms of Root Mean Square Error (RMSE) and Pearson Linear Correlation Coefficient.},
  archive      = {J_TVCG},
  author       = {Minghan Du and Hui Cui and Yuan Wang and Henry Been-Lirn Duh},
  doi          = {10.1109/TVCG.2021.3115901},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1415-1423},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning from deep stereoscopic attention for simulator sickness prediction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A music-driven deep generative adversarial model for guzheng
playing animation. <em>TVCG</em>, <em>29</em>(2), 1400–1414. (<a
href="https://doi.org/10.1109/TVCG.2021.3115902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date relatively few efforts have been made on the automatic generation of musical instrument playing animations. This problem is challenging due to the intrinsically complex, temporal relationship between music and human motion as well as the lacking of high quality music-playing motion datasets. In this article, we propose a fully automatic, deep learning based framework to synthesize realistic upper body animations based on novel guzheng music input. Specifically, based on a recorded audiovisual motion capture dataset, we delicately design a generative adversarial network (GAN) based approach to capture the temporal relationship between the music and the human motion data. In this process, data augmentation is employed to improve the generalization of our approach to handle a variety of guzheng music inputs. Through extensive objective and subjective experiments, we show that our method can generate visually plausible guzheng-playing animations that are well synchronized with the input guzheng music, and it can significantly outperform the state-of-the-art methods. In addition, through an ablation study, we validate the contributions of the carefully-designed modules in our framework.},
  archive      = {J_TVCG},
  author       = {Jiali Chen and Changjie Fan and Zhimeng Zhang and Gongzheng Li and Zeng Zhao and Zhigang Deng and Yu Ding},
  doi          = {10.1109/TVCG.2021.3115902},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1400-1414},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A music-driven deep generative adversarial model for guzheng playing animation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ChartStory: Automated partitioning, layout, and captioning
of charts into comic-style narratives. <em>TVCG</em>, <em>29</em>(2),
1384–1399. (<a href="https://doi.org/10.1109/TVCG.2021.3114211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual data storytelling is gaining importance as a means of presenting data-driven information or analysis results, especially to the general public. This has resulted in design principles being proposed for data-driven storytelling, and new authoring tools being created to aid such storytelling. However, data analysts typically lack sufficient background in design and storytelling to make effective use of these principles and authoring tools. To assist this process, we present ChartStory for crafting data stories from a collection of user-created charts, using a style akin to comic panels to imply the underlying sequence and logic of data-driven narratives. Our approach is to operationalize established design principles into an advanced pipeline that characterizes charts by their properties and similarities to each other, and recommends ways to partition, layout, and caption story pieces to serve a narrative. ChartStory also augments this pipeline with intuitive user interactions for visual refinement of generated data comics. We extensively and holistically evaluate ChartStory via a trio of studies. We first assess how the tool supports data comic creation in comparison to a manual baseline tool. Data comics from this study are subsequently compared and evaluated to ChartStory’s automated recommendations by a team of narrative visualization practitioners. This is followed by a pair of interview studies with data scientists using their own datasets and charts who provide an additional assessment of the system. We find that ChartStory provides cogent recommendations for narrative generation, resulting in data comics that compare favorably to manually-created ones.},
  archive      = {J_TVCG},
  author       = {Jian Zhao and Shenyu Xu and Senthil Chandrasegaran and Chris Bryan and Fan Du and Aditi Mishra and Xin Qian and Yiran Li and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2021.3114211},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1384-1399},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChartStory: Automated partitioning, layout, and captioning of charts into comic-style narratives},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exemplar-based 3D portrait stylization. <em>TVCG</em>,
<em>29</em>(2), 1371–1383. (<a
href="https://doi.org/10.1109/TVCG.2021.3114308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exemplar-based portrait stylization is widely attractive and highly desired. Despite recent successes, it remains challenging, especially when considering both texture and geometric styles. In this article, we present the first framework for one-shot 3D portrait style transfer, which can generate 3D face models with both the geometry exaggerated and the texture stylized while preserving the identity from the original content. It requires only one arbitrary style image instead of a large set of training examples for a particular style, provides geometry and texture outputs that are fully parameterized and disentangled, and enables further graphics applications with the 3D representations. The framework consists of two stages. In the first geometric style transfer stage, we use facial landmark translation to capture the coarse geometry style and guide the deformation of the dense 3D face geometry. In the second texture style transfer stage, we focus on performing style transfer on the canonical texture by adopting a differentiable renderer to optimize the texture in a multi-view framework. Experiments show that our method achieves robustly good results on different artistic styles and outperforms existing methods. We also demonstrate the advantages of our method via various 2D and 3D graphics applications.},
  archive      = {J_TVCG},
  author       = {Fangzhou Han and Shuquan Ye and Mingming He and Menglei Chai and Jing Liao},
  doi          = {10.1109/TVCG.2021.3114308},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1371-1383},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exemplar-based 3D portrait stylization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GeoDualCNN: Geometry-supporting dual convolutional neural
network for noisy point clouds. <em>TVCG</em>, <em>29</em>(2),
1357–1370. (<a href="https://doi.org/10.1109/TVCG.2021.3113463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a geometry-supporting dual convolutional neural network (GeoDualCNN) for both point cloud normal estimation and denoising. GeoDualCNN fuses the geometry domain knowledge that the underlying surface of a noisy point cloud is piecewisely smooth with the fact that a point normal is properly defined only when local surface smoothness is guaranteed. Centered around this insight, we define the homogeneous neighborhood (HoNe) which stays clear of surface discontinuities, and associate each HoNe with a point whose geometry and normal orientation is mostly consistent with that of HoNe. Thus, we not only obtain initial estimates of the point normals by performing PCA on HoNes, but also for the first time optimize these initial point normals by learning the mapping from two proposed geometric descriptors to the ground-truth point normals. GeoDualCNN consists of two parallel branches that remove noise using the first geometric descriptor (a homogeneous height map , which encodes the point-position information), while preserving surface features using the second geometric descriptor (a homogeneous normal map , which encodes the point-normal information). Such geometry-supporting network architectures enable our model to leverage previous geometry expertise and to benefit from training data. Experiments with noisy point clouds show that GeoDualCNN outperforms the state-of-the-art methods in terms of both noise-robustness and feature preservation.},
  archive      = {J_TVCG},
  author       = {Mingqiang Wei and Honghua Chen and Yingkui Zhang and Haoran Xie and Yanwen Guo and Jun Wang},
  doi          = {10.1109/TVCG.2021.3113463},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1357-1370},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeoDualCNN: Geometry-supporting dual convolutional neural network for noisy point clouds},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of acceleration structure parameters and hybrid
autotuning for ray tracing. <em>TVCG</em>, <em>29</em>(2), 1345–1356.
(<a href="https://doi.org/10.1109/TVCG.2021.3113499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding optimal parameters for acceleration structures for raytracing is key to improved performance. Previous research has shown that a speedup of over 10\% of rendering time is possible. Some parameters are interdependent which complicates the process of finding an optimal configuration. It is hence interesting to find them efficiently. Autotuning is an automatic optimization scheme able to search for optimal configurations and has been applied successfully to kD-trees in the past, which we apply today on BVHs. The more parameters to optimize, the more difficult it is to find optimal solutions. In this article, we analyze in detail the behavior of the parameters and their impact on acceleration structure building and rendering time. We show the interdependence and context sensitivity (i.e., scene, viewpoint) of the parameters. Based on the use case, this allows to target only crucial parameters. Convergence speed towards an optimal configuration is essential. To find better parameters, the autotuner needs to build the acceleration structure over and over, changing parameters every time. We introduce a hybrid model-based prediction and online autotuning method to address this issue. The prediction model allows for both instantaneous near-optimal configurations when inputs are known or similar, and efficient search of the configuration space when inputs are completely new. Online autotuning outperforms configurations recommended in literature by up to 11\% median. The prediction model achieves 95\% of the maximum speedup of the autotuner while reducing 90\% of its overhead. Thus, hybrid online autonuning enables always-on tuning in ray tracing.},
  archive      = {J_TVCG},
  author       = {Killian Herveau and Philip Pfaffe and Martin Tillmann and Walter F. Tichy and Carsten Dachsbacher},
  doi          = {10.1109/TVCG.2021.3113499},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1345-1356},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis of acceleration structure parameters and hybrid autotuning for ray tracing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balance-aware grid collage for small image collections.
<em>TVCG</em>, <em>29</em>(2), 1330–1344. (<a
href="https://doi.org/10.1109/TVCG.2021.3113031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid collages (GClg) of small image collections are popular and useful in many applications, such as personal album management, online photo posting, and graphic design. In this article, we focus on how visual effects influence individual preferences through various arrangements of multiple images under such scenarios. A novel balance-aware metric is proposed to bridge the gap between multi-image joint presentation and visual pleasure. The metric merges psychological achievements into the field of grid collage. To capture user preference, a bonus mechanism related to a user-specified special location in the grid and uniqueness values of the subimages is integrated into the metric. An end-to-end reinforcement learning mechanism empowers the model without tedious manual annotations. Experiments demonstrate that our metric can evaluate the GClg visual balance in line with human subjective perception, and the model can generate visually pleasant GClg results, which is comparable to manual designs.},
  archive      = {J_TVCG},
  author       = {Yu Song and Fan Tang and Weiming Dong and Feiyue Huang and Tong-Yee Lee and Changsheng Xu},
  doi          = {10.1109/TVCG.2021.3113031},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1330-1344},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Balance-aware grid collage for small image collections},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold-constrained geometric optimization via local
parameterizations. <em>TVCG</em>, <em>29</em>(2), 1318–1329. (<a
href="https://doi.org/10.1109/TVCG.2021.3112896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many geometric optimization problems contain manifold constraints that restrict the optimized vertices on some specified manifold surface. The constraints are highly nonlinear and non-convex, therefore existing methods usually suffer from a breach of condition or low optimization quality. In this article, we present a novel divide-and-conquer methodology for manifold-constrained geometric optimization problems. Central to our methodology is to use local parameterizations to decouple the optimization with hard constraints, which transforms nonlinear constraints into linear constraints. We decompose the input mesh into a set of developable or nearly-developable overlapping patches with disc topology, then flatten each patch into the planar domain with very low isometric distortion, optimize vertices with linear constraints and recover the patch. Finally, we project it onto the constrained manifold surface. We demonstrate the applicability and robustness of our methodology through a variety of geometric optimization tasks. Experimental results show that our method performs much better than existing methods.},
  archive      = {J_TVCG},
  author       = {Bo-Yi Hu and Chunyang Ye and Jian-Ping Su and Ligang Liu},
  doi          = {10.1109/TVCG.2021.3112896},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1318-1329},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Manifold-constrained geometric optimization via local parameterizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale mesh deformation component analysis with
attention-based autoencoders. <em>TVCG</em>, <em>29</em>(2), 1301–1317.
(<a href="https://doi.org/10.1109/TVCG.2021.3112526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformation component analysis is a fundamental problem in geometry processing and shape understanding. Existing approaches mainly extract deformation components in local regions at a similar scale while deformations of real-world objects are usually distributed in a multi-scale manner. In this article, we propose a novel method to exact multiscale deformation components automatically with a stacked attention-based autoencoder. The attention mechanism is designed to learn to softly weight multi-scale deformation components in active deformation regions, and the stacked attention-based autoencoder is learned to represent the deformation components at different scales. Quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods. Furthermore, with the multiscale deformation components extracted by our method, the user can edit shapes in a coarse-to-fine fashion which facilitates effective modeling of new shapes.},
  archive      = {J_TVCG},
  author       = {Jie Yang and Lin Gao and Qingyang Tan and Yi-Hua Huang and Shihong Xia and Yu-Kun Lai},
  doi          = {10.1109/TVCG.2021.3112526},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1301-1317},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiscale mesh deformation component analysis with attention-based autoencoders},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Farewell and new EIC introduction. <em>TVCG</em>,
<em>29</em>(2), 1299–1300. (<a
href="https://doi.org/10.1109/TVCG.2022.3219303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the editorial for this issue of the publication},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3219303},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1299-1300},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Farewell and new EIC introduction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface. <em>TVCG</em>, <em>29</em>(1), xvi–xxiv. (<a
href="https://doi.org/10.1109/TVCG.2022.3211681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the preface to the VIS 2022 conference.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2022.3211681},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xvi-xxiv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Welcome: Message from the VIS 2022 general chairs.
<em>TVCG</em>, <em>29</em>(1), xv. (<a
href="https://doi.org/10.1109/TVCG.2022.3211640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the welcome message from the VIS 2022 General Chairs.},
  archive      = {J_TVCG},
  author       = {Danielle Szafir and David Ebert and Hendrik Strobelt},
  doi          = {10.1109/TVCG.2022.3211640},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Welcome: Message from the VIS 2022 general chairs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Message from the editor-in-chief. <em>TVCG</em>,
<em>29</em>(1), xiv. (<a
href="https://doi.org/10.1109/TVCG.2022.3211639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3211639},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {xiv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-tube attribute visualization for multivariate trajectory
data. <em>TVCG</em>, <em>29</em>(1), 1288–1298. (<a
href="https://doi.org/10.1109/TVCG.2022.3209400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stylized tubes are an established visualization primitive for line data as encountered in many scientific fields, ranging from characteristic lines in flow fields, fiber tracks reconstructed from diffusion tensor imaging, to trajectories of moving objects as they arise from cyber-physical systems in many engineering disciplines. Typical challenges include large data set sizes demanding for efficient rendering techniques as well as a large number of attributes that cannot be mapped simultaneously to the basic visual attributes provided by a tube-based visualization. In this work, we tackle both challenges with a new on-tube visualization approach. We improve recent work on high-quality GPU ray casting of Hermite spline tubes supporting ambient occlusion and extend it by a new layered procedural texturing technique. In the proposed framework, a large number of data set attributes can be mapped simultaneously to a variety of glyphs and plots that are embedded in texture space and organized in layers. Efficient rendering with minimal data transfer is achieved by generating the glyphs procedurally and drawing them in a deferred shading pass. We integrated these techniques in a prototype visualization tool that facilitates flexible mapping of data set attributes to visual tube and glyph attributes. We studied our approach on a variety of example data from different fields and found it to provide a highly adaptable and extensible toolbox to quickly craft tailor-made tube-based trajectory visualizations.},
  archive      = {J_TVCG},
  author       = {Benjamin Russig and David Groß and Raimund Dachselt and Stefan Gumhold},
  doi          = {10.1109/TVCG.2022.3209400},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1288-1298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On-tube attribute visualization for multivariate trajectory data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of spatiotemporal visualizations for 3D urban
analytics. <em>TVCG</em>, <em>29</em>(1), 1277–1287. (<a
href="https://doi.org/10.1109/TVCG.2022.3209474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. Lastly, we report on a series of interviews with four practitioners, and their feedback and suggestions for further work on the visualizations to support 3D spatiotemporal urban data analysis.},
  archive      = {J_TVCG},
  author       = {Roberta Mota and Nivan Ferreira and Julio Daniel Silva and Marius Horga and Marcos Lage and Luis Ceferino and Usman Alim and Ehud Sharlin and Fabio Miranda},
  doi          = {10.1109/TVCG.2022.3209474},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1277-1287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparison of spatiotemporal visualizations for 3D urban analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending the nested model for user-centric XAI: A design
study on GNN-based drug repurposing. <em>TVCG</em>, <em>29</em>(1),
1266–1276. (<a href="https://doi.org/10.1109/TVCG.2022.3209435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whether AI explanations can help users achieve specific tasks efficiently ( i.e. , usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e. , reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators&#39; feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else . DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and Kexin Huang and Payal Chandak and Marinka Zitnik and Nils Gehlenborg},
  doi          = {10.1109/TVCG.2022.3209435},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1266-1276},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extending the nested model for user-centric XAI: A design study on GNN-based drug repurposing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development and evaluation of two approaches of visual
sensitivity analysis to support epidemiological modeling. <em>TVCG</em>,
<em>29</em>(1), 1255–1265. (<a
href="https://doi.org/10.1109/TVCG.2022.3209464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational modeling is a commonly used technology in many scientific disciplines and has played a noticeable role in combating the COVID-19 pandemic. Modeling scientists conduct sensitivity analysis frequently to observe and monitor the behavior of a model during its development and deployment. The traditional algorithmic ranking of sensitivity of different parameters usually does not provide modeling scientists with sufficient information to understand the interactions between different parameters and model outputs, while modeling scientists need to observe a large number of model runs in order to gain actionable information for parameter optimization. To address the above challenge, we developed and compared two visual analytics approaches, namely: algorithm-centric and visualization-assisted , and visualization-centric and algorithm-assisted . We evaluated the two approaches based on a structured analysis of different tasks in visual sensitivity analysis as well as the feedback of domain experts. While the work was carried out in the context of epidemiological modeling, the two approaches developed in this work are directly applicable to a variety of modeling processes featuring time series outputs, and can be extended to work with models with other types of outputs.},
  archive      = {J_TVCG},
  author       = {Erik Rydow and Rita Borgo and Hui Fang and Thomas Torsney-Weir and Ben Swallow and Thibaud Porphyre and Cagatay Turkay and Min Chen},
  doi          = {10.1109/TVCG.2022.3209464},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1255-1265},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Development and evaluation of two approaches of visual sensitivity analysis to support epidemiological modeling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ChartWalk: Navigating large collections of text notes in
electronic health records for clinical chart review. <em>TVCG</em>,
<em>29</em>(1), 1244–1254. (<a
href="https://doi.org/10.1109/TVCG.2022.3209444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient&#39;s electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk , an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.},
  archive      = {J_TVCG},
  author       = {Nicole Sultanum and Farooq Naeem and Michael Brudno and Fanny Chevalier},
  doi          = {10.1109/TVCG.2022.3209444},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1244-1254},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChartWalk: Navigating large collections of text notes in electronic health records for clinical chart review},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Striking a balance: Reader takeaways and preferences when
integrating text and charts. <em>TVCG</em>, <em>29</em>(1), 1233–1243.
(<a href="https://doi.org/10.1109/TVCG.2022.3209383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts containing text with varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways referring to statistics or relational comparisons than text describing elemental or encoded components. Finally, we find different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. We compile these results into four chart design guidelines and discuss future implications for the combination of text and charts.},
  archive      = {J_TVCG},
  author       = {Chase Stokes and Vidya Setlur and Bridget Cogley and Arvind Satyanarayan and Marti A. Hearst},
  doi          = {10.1109/TVCG.2022.3209383},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1233-1243},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Striking a balance: Reader takeaways and preferences when integrating text and charts},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards natural language-based visualization authoring.
<em>TVCG</em>, <em>29</em>(1), 1222–1232. (<a
href="https://doi.org/10.1109/TVCG.2022.3209357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users&#39; visualization editing intents, called editing actions , based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.},
  archive      = {J_TVCG},
  author       = {Yun Wang and Zhitao Hou and Leixian Shen and Tongshuang Wu and Jiaqi Wang and He Huang and Haidong Zhang and Dongmei Zhang},
  doi          = {10.1109/TVCG.2022.3209357},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1222-1232},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards natural language-based visualization authoring},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison conundrum and the chamber of visualizations: An
exploration of how language influences visual design. <em>TVCG</em>,
<em>29</em>(1), 1211–1221. (<a
href="https://doi.org/10.1109/TVCG.2022.3209456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.},
  archive      = {J_TVCG},
  author       = {Aimen Gaba and Vidya Setlur and Arjun Srinivasan and Jane Hoffswell and Cindy Xiong},
  doi          = {10.1109/TVCG.2022.3209456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1211-1221},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison conundrum and the chamber of visualizations: An exploration of how language influences visual design},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlowNL: Asking the flow data in natural languages.
<em>TVCG</em>, <em>29</em>(1), 1200–1210. (<a
href="https://doi.org/10.1109/TVCG.2022.3209453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow visualization is essentially a tool to answer domain experts&#39; questions about flow fields using rendered images. Static flow visualization approaches require domain experts to raise their questions to visualization experts, who develop specific techniques to extract and visualize the flow structures of interest. Interactive visualization approaches allow domain experts to ask the system directly through the visual analytic interface, which provides flexibility to support various tasks. However, in practice, the visual analytic interface may require extra learning effort, which often discourages domain experts and limits its usage in real-world scenarios. In this paper, we propose FlowNL, a novel interactive system with a natural language interface. FlowNL allows users to manipulate the flow visualization system using plain English, which greatly reduces the learning effort. We develop a natural language parser to interpret user intention and translate textual input into a declarative language. We design the declarative language as an intermediate layer between the natural language and the programming language specifically for flow visualization. The declarative language provides selection and composition rules to derive relatively complicated flow structures from primitive objects that encode various kinds of information about scalar fields, flow patterns, regions of interest, connectivities, etc. We demonstrate the effectiveness of FlowNL using multiple usage scenarios and an empirical evaluation.},
  archive      = {J_TVCG},
  author       = {Jieying Huang and Yang Xi and Junnan Hu and Jun Tao},
  doi          = {10.1109/TVCG.2022.3209453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1200-1210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlowNL: Asking the flow data in natural languages},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probablement, wahrscheinlich, likely? A cross-language study
of how people verbalize probabilities in icon array visualizations.
<em>TVCG</em>, <em>29</em>(1), 1189–1199. (<a
href="https://doi.org/10.1109/TVCG.2022.3209367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, $n=50$ participants per language both chose probability expressions — e.g. likely, probable — to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80\% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/ .},
  archive      = {J_TVCG},
  author       = {Noëlle Rakotondravony and Yiren Ding and Lane Harrison},
  doi          = {10.1109/TVCG.2022.3209367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1189-1199},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Probablement, wahrscheinlich, likely? a cross-language study of how people verbalize probabilities in icon array visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual comparison of language model adaptation.
<em>TVCG</em>, <em>29</em>(1), 1178–1188. (<a
href="https://doi.org/10.1109/TVCG.2022.3209458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities) . We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 ( decontextualized ) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.},
  archive      = {J_TVCG},
  author       = {Rita Sevastjanova and Eren Cakmak and Shauli Ravfogel and Ryan Cotterell and Mennatallah El-Assady},
  doi          = {10.1109/TVCG.2022.3209458},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1178-1188},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual comparison of language model adaptation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computing a stable distance on merge trees. <em>TVCG</em>,
<em>29</em>(1), 1168–1177. (<a
href="https://doi.org/10.1109/TVCG.2022.3209395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von Kármán vortex street.},
  archive      = {J_TVCG},
  author       = {Brian Bollen and Pasindu Tennakoon and Joshua A. Levine},
  doi          = {10.1109/TVCG.2022.3209395},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1168-1177},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computing a stable distance on merge trees},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal merge tree maps: A topology-based static
visualization for temporal scalar data. <em>TVCG</em>, <em>29</em>(1),
1157–1167. (<a href="https://doi.org/10.1109/TVCG.2022.3209387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating a static visualization for a time-dependent scalar field is a non-trivial task, yet very insightful as it shows the dynamics in one picture. Existing approaches are based on a linearization of the domain or on feature tracking. Domain linearizations use space-filling curves to place all sample points into a 1D domain, thereby breaking up individual features. Feature tracking methods explicitly respect feature continuity in space and time, but generally neglect the data context in which those features live. We present a feature-based linearization of the spatial domain that keeps features together and preserves their context by involving all data samples. We use augmented merge trees to linearize the domain and show that our linearized function has the same merge tree as the original data. A greedy optimization scheme aligns the trees over time providing temporal continuity. This leads to a static 2D visualization with one temporal dimension, and all spatial dimensions compressed into one. We compare our method against other domain linearizations as well as feature-tracking approaches, and apply it to several real-world data sets.},
  archive      = {J_TVCG},
  author       = {Wiebke Köpp and Tino Weinkauf},
  doi          = {10.1109/TVCG.2022.3209387},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1157-1167},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Temporal merge tree maps: A topology-based static visualization for temporal scalar data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive and visual prompt engineering for ad-hoc task
adaptation with large language models. <em>TVCG</em>, <em>29</em>(1),
1146–1156. (<a href="https://doi.org/10.1109/TVCG.2022.3209479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai ) and our workflow using several real-world use cases.},
  archive      = {J_TVCG},
  author       = {Hendrik Strobelt and Albert Webson and Victor Sanh and Benjamin Hoover and Johanna Beyer and Hanspeter Pfister and Alexander M. Rush},
  doi          = {10.1109/TVCG.2022.3209479},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1146-1156},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive and visual prompt engineering for ad-hoc task adaptation with large language models},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MEDLEY: Intent-based recommendations to support dashboard
composition. <em>TVCG</em>, <em>29</em>(1), 1135–1145. (<a
href="https://doi.org/10.1109/TVCG.2022.3209421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present M edley , a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. M edley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how M edley&#39;s recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.},
  archive      = {J_TVCG},
  author       = {Aditeya Pandey and Arjun Srinivasan and Vidya Setlur},
  doi          = {10.1109/TVCG.2022.3209421},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1135-1145},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MEDLEY: Intent-based recommendations to support dashboard composition},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lotse: A practical framework for guidance in visual
analytics. <em>TVCG</em>, <em>29</em>(1), 1124–1134. (<a
href="https://doi.org/10.1109/TVCG.2022.3209393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-adaptive guidance aims to enable efficient human-machine collaboration in visual analytics, as proposed by multiple theoretical frameworks. This paper bridges the gap between such conceptual frameworks and practical implementation by introducing an accessible model of guidance and an accompanying guidance library, mapping theory into practice. We contribute a model of system-provided guidance based on design templates and derived strategies. We instantiate the model in a library called Lotse that allows specifying guidance strategies in definition files and generates running code from them. Lotse is the first guidance library using such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrate its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse. Further, we analyze our framework&#39;s cognitive dimensions to evaluate its expressiveness and outline a summary of open research questions for aligning guidance practice with its intricate theory.},
  archive      = {J_TVCG},
  author       = {Fabian Sperrle and Davide Ceneda and Mennatallah El-Assady},
  doi          = {10.1109/TVCG.2022.3209393},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1124-1134},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lotse: A practical framework for guidance in visual analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence of visual provenance representations on
strategies in a collaborative hand-off data analysis scenario.
<em>TVCG</em>, <em>29</em>(1), 1113–1123. (<a
href="https://doi.org/10.1109/TVCG.2022.3209495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conducting data analysis tasks rarely occur in isolation. Especially in intelligence analysis scenarios where different experts contribute knowledge to a shared understanding, members must communicate how insights develop to establish common ground among collaborators. The use of provenance to communicate analytic sensemaking carries promise by describing the interactions and summarizing the steps taken to reach insights. Yet, no universal guidelines exist for communicating provenance in different settings. Our work focuses on the presentation of provenance information and the resulting conclusions reached and strategies used by new analysts. In an open-ended, 30-minute, textual exploration scenario, we qualitatively compare how adding different types of provenance information (specifically data coverage and interaction history) affects analysts&#39; confidence in conclusions developed, propensity to repeat work, filtering of data, identification of relevant information, and typical investigation strategies. We see that data coverage (i.e., what was interacted with) provides provenance information without limiting individual investigation freedom. On the other hand, while interaction history (i.e., when something was interacted with) does not significantly encourage more mimicry, it does take more time to comfortably understand, as represented by less confident conclusions and less relevant information-gathering behaviors. Our results contribute empirical data towards understanding how provenance summarizations can influence analysis behaviors.},
  archive      = {J_TVCG},
  author       = {Jeremy E. Block and Shaghayegh Esmaeili and Eric D. Ragan and John R. Goodall and G. David Richardson},
  doi          = {10.1109/TVCG.2022.3209495},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1113-1123},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The influence of visual provenance representations on strategies in a collaborative hand-off data analysis scenario},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A visual analytics system for improving attention-based
traffic forecasting models. <em>TVCG</em>, <em>29</em>(1), 1102–1112.
(<a href="https://doi.org/10.1109/TVCG.2022.3209462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.},
  archive      = {J_TVCG},
  author       = {Seungmin Jin and Hyunwook Lee and Cheonbok Park and Hyeshin Chu and Yunwon Tae and Jaegul Choo and Sungahn Ko},
  doi          = {10.1109/TVCG.2022.3209462},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1102-1112},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics system for improving attention-based traffic forecasting models},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECoalVis: Visual analysis of control strategies in
coal-fired power plants. <em>TVCG</em>, <em>29</em>(1), 1091–1101. (<a
href="https://doi.org/10.1109/TVCG.2022.3209430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the efficiency of coal-fired power plants has numerous benefits. The control strategy is one of the major factors affecting such efficiency. However, due to the complex and dynamic environment inside the power plants, it is hard to extract and evaluate control strategies and their cascading impact across massive sensors. Existing manual and data-driven approaches cannot well support the analysis of control strategies because these approaches are time-consuming and do not scale with the complexity of the power plant systems. Three challenges were identified: a) interactive extraction of control strategies from large-scale dynamic sensor data, b) intuitive visual representation of cascading impact among the sensors in a complex power plant system, and c) time-lag-aware analysis of the impact of control strategies on electricity generation efficiency. By collaborating with energy domain experts, we addressed these challenges with ECoalVis, a novel interactive system for experts to visually analyze the control strategies of coal-fired power plants extracted from historical sensor data. The effectiveness of the proposed system is evaluated with two usage scenarios on a real-world historical dataset and received positive feedback from experts.},
  archive      = {J_TVCG},
  author       = {Shuhan Liu and Di Weng and Yuan Tian and Zikun Deng and Haoran Xu and Xiangyu Zhu and Honglei Yin and Xianyuan Zhan and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209430},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1091-1101},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ECoalVis: Visual analysis of control strategies in coal-fired power plants},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PMU tracker: A visualization platform for epicentric event
propagation analysis in the power grid. <em>TVCG</em>, <em>29</em>(1),
1081–1090. (<a href="https://doi.org/10.1109/TVCG.2022.3209380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as ‘smart’ systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid&#39;s network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram , which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker&#39;s visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.},
  archive      = {J_TVCG},
  author       = {Anjana Arunkumar and Andrea Pinceti and Lalitha Sankar and Chris Bryan},
  doi          = {10.1109/TVCG.2022.3209380},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1081-1090},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PMU tracker: A visualization platform for epicentric event propagation analysis in the power grid},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RISeer: Inspecting the status and dynamics of regional
industrial structure via visual analytics. <em>TVCG</em>,
<em>29</em>(1), 1070–1080. (<a
href="https://doi.org/10.1109/TVCG.2022.3209351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer , which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.},
  archive      = {J_TVCG},
  author       = {Longfei Chen and Yang Ouyang and Haipeng Zhang and Suting Hong and Quan Li},
  doi          = {10.1109/TVCG.2022.3209351},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1070-1080},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RISeer: Inspecting the status and dynamics of regional industrial structure via visual analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The state of the art in BGP visualization tools: A mapping
of visualization techniques to cyberattack types. <em>TVCG</em>,
<em>29</em>(1), 1059–1069. (<a
href="https://doi.org/10.1109/TVCG.2022.3209412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet routing is largely dependent on Border Gateway Protocol (BGP). However, BGP does not have any inherent authentication or integrity mechanisms that help make it secure. Effective security is challenging or infeasible to implement due to high costs, policy employment in these distributed systems, and unique routing behavior. Visualization tools provide an attractive alternative in lieu of traditional security approaches. Several BGP security visualization tools have been developed as a stop-gap in the face of ever-present BGP attacks. Even though the target users, tasks, and domain remain largely consistent across such tools, many diverse visualization designs have been proposed. The purpose of this study is to provide an initial formalization of methods and visualization techniques for BGP cybersecurity analysis. Using PRISMA guidelines, we provide a systematic review and survey of 29 BGP visualization tools with their tasks, implementation techniques, and attacks and anomalies that they were intended for. We focused on BGP visualization tools as the main inclusion criteria to best capture the visualization techniques used in this domain while excluding solely algorithmic solutions and other detection tools that do not involve user interaction or interpretation. We take the unique approach of connecting (1) the actual BGP attacks and anomalies used to validate existing tools with (2) the techniques employed to detect them. In this way, we contribute an analysis of which techniques can be used for each attack type. Furthermore, we can see the evolution of visualization solutions in this domain as new attack types are discovered. This systematic review provides the groundwork for future designers and researchers building visualization tools for providing BGP cybersecurity, including an understanding of the state-of-the-art in this space and an analysis of what techniques are appropriate for each attack type. Our novel security visualization survey methodology—connecting visualization techniques with appropriate attack types—may also assist future researchers conducting systematic reviews of security visualizations. All supplemental materials are available at https://osf.io/tupz6/ .},
  archive      = {J_TVCG},
  author       = {Justin Raynor and Tarik Crnovrsanin and Sara Di Bartolomeo and Laura South and David Saffo and Cody Dunne},
  doi          = {10.1109/TVCG.2022.3209412},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1059-1069},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The state of the art in BGP visualization tools: A mapping of visualization techniques to cyberattack types},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding how designers find and use data visualization
examples. <em>TVCG</em>, <em>29</em>(1), 1048–1058. (<a
href="https://doi.org/10.1109/TVCG.2022.3209490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Examples are useful for inspiring ideas and facilitating implementation in visualization design. However, there is little understanding of how visualization designers use examples, and how computational tools may support such activities. In this paper, we contribute an exploratory study of current practices in incorporating visualization examples. We conducted semi-structured interviews with 15 university students and 15 professional designers. Our analysis focus on two core design activities: searching for examples and utilizing examples. We characterize observed strategies and tools for performing these activities, as well as major challenges that hinder designers&#39; current workflows. In addition, we identify themes that cut across these two activities: criteria for determining example usefulness, curation practices, and design fixation. Given our findings, we discuss the implications for visualization design and authoring tools and highlight critical areas for future research.},
  archive      = {J_TVCG},
  author       = {Hannah K. Bako and Xinyi Liu and Leilani Battle and Zhicheng Liu},
  doi          = {10.1109/TVCG.2022.3209490},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1048-1058},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding how designers find and use data visualization examples},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualization design practices in a crisis: Behind the
scenes with COVID-19 dashboard creators. <em>TVCG</em>, <em>29</em>(1),
1037–1047. (<a href="https://doi.org/10.1109/TVCG.2022.3209493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. Prior work on COVID-19 visualizations has primarily focused on the design and evaluation of specific visualization systems from technology-centered perspectives. However, little is known about what occurs behind the scenes during the visualization creation processes, given the complex sociotechnical contexts in which they are embedded. Yet, such ecological knowledge is necessary to help characterize the nuances and trajectories of visualization design practices in the wild, as well as generate insights into how creators come to understand and approach visualization design on their own terms and for their own situated purposes. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices—from creation to expansion, maintenance, and termination—that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.},
  archive      = {J_TVCG},
  author       = {Yixuan Zhang and Yifan Sun and Joseph D. Gaggiano and Neha Kumar and Clio Andris and Andrea G. Parker},
  doi          = {10.1109/TVCG.2022.3209493},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1037-1047},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization design practices in a crisis: Behind the scenes with COVID-19 dashboard creators},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In defence of visual analytics systems: Replies to critics.
<em>TVCG</em>, <em>29</em>(1), 1026–1036. (<a
href="https://doi.org/10.1109/TVCG.2022.3209360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last decade has witnessed many visual analytics (VA) systems that make successful applications to wide-ranging domains like urban analytics and explainable AI. However, their research rigor and contributions have been extensively challenged within the visualization community. We come in defence of VA systems by contributing two interview studies for gathering critics and responses to those criticisms. First, we interview 24 researchers to collect criticisms the review comments on their VA work. Through an iterative coding and refinement process, the interview feedback is summarized into a list of 36 common criticisms. Second, we interview 17 researchers to validate our list and collect their responses, thereby discussing implications for defending and improving the scientific values and rigor of VA systems. We highlight that the presented knowledge is deep, extensive, but also imperfect, provocative, and controversial, and thus recommend reading with an inclusive and critical eye. We hope our work can provide thoughts and foundations for conducting VA research and spark discussions to promote the research field forward more rigorously and vibrantly.},
  archive      = {J_TVCG},
  author       = {Aoyu Wu and Dazhen Deng and Furui Cheng and Yingcai Wu and Shixia Liu and Huamin Qu},
  doi          = {10.1109/TVCG.2022.3209360},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1026-1036},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {In defence of visual analytics systems: Replies to critics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Thirty-two years of IEEE VIS: Authors, fields of study and
citations. <em>TVCG</em>, <em>29</em>(1), 1016–1025. (<a
href="https://doi.org/10.1109/TVCG.2022.3209422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE VIS Conference (VIS) recently rebranded itself as a unified conference and officially positioned itself within the discipline of Data Science. Driven by this movement, we investigated (1) who contributed to VIS, and (2) where VIS stands in the scientific world. We examined the authors and fields of study of 3,240 VIS publications in the past 32 years based on data collected from OpenAlex and IEEE Xplore, among other sources. We also examined the citation flows from referenced papers (i.e., those referenced in VIS) to VIS, and from VIS to citing papers (i.e., those citing VIS). We found that VIS has been becoming increasingly popular and collaborative. The number of publications, of unique authors, and of participating countries have been steadily growing. Both cross-country collaborations, and collaborations between educational and non-educational affiliations, namely “cross-type collaborations”, are increasing. The dominance of the US is decreasing, and authors from China are now an important part of VIS. In terms of author affiliation types, VIS is increasingly dominated by authors from universities. We found that the topics, inspirations, and influences of VIS research is limited such that (1) VIS, and their referenced and citing papers largely fall into the Computer Science domain, and (2) citations flow mostly between the same set of subfields within Computer Science. Our citation analyses showed that award-winning VIS papers had higher citations. Interactive visualizations, replication data, source code and supplementary material are available at https://32vis.hongtaoh.com and https://osf.io/zkvjm .},
  archive      = {J_TVCG},
  author       = {Hongtao Hao and Yumian Cui and Zhengxiang Wang and Yea-Seul Kim},
  doi          = {10.1109/TVCG.2022.3209422},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1016-1025},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Thirty-two years of IEEE VIS: Authors, fields of study and citations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How do viewers synthesize conflicting information from data
visualizations? <em>TVCG</em>, <em>29</em>(1), 1005–1015. (<a
href="https://doi.org/10.1109/TVCG.2022.3209467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments ( $\mathrm{N}=1166$ ) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants&#39; synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.},
  archive      = {J_TVCG},
  author       = {Prateek Mantri and Hariharan Subramonyam and Audrey L. Michal and Cindy Xiong},
  doi          = {10.1109/TVCG.2022.3209467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1005-1015},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How do viewers synthesize conflicting information from data visualizations?},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geo-storylines: Integrating maps into storyline
visualizations. <em>TVCG</em>, <em>29</em>(1), 994–1004. (<a
href="https://doi.org/10.1109/TVCG.2022.3209480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storyline visualizations are a powerful way to compactly visualize how the relationships between people evolve over time. Real-world relationships often also involve space, for example the cities that two political rivals visited together or alone over the years. By default, Storyline visualizations only show implicitly geospatial co-occurrence between people (drawn as lines), by bringing their lines together. Even the few designs that do explicitly show geographic locations only do so in abstract ways ( e.g. , annotations) and do not communicate geospatial information, such as the direction or extent of their political campains. We introduce Geo-Storylines, a collection of visualisation designs that integrate geospatial context into Storyline visualizations, using different strategies for compositing time and space. Our contribution is twofold. First, we present the results of a sketching workshop with 11 participants, that we used to derive a design space for integrating maps into Storylines. Second, by analyzing the strengths and weaknesses of the potential designs of the design space in terms of legibility and ability to scale to multiple relationships, we extract the three most promising: Time Glyphs, Coordinated Views, and Map Glyphs. We compare these three techniques first in a controlled study with 18 participants, under five different geospatial tasks and two maps of different complexity. We additionally collected informal feedback about their usefulness from domain experts in data journalism. Our results indicate that, as expected, detailed performance depends on the task. Nevertheless, Coordinated Views remain a highly effective and preferred technique across the board.},
  archive      = {J_TVCG},
  author       = {Golina Hulstein and Vanessa Peña-Araya and Anastasia Bezerianos},
  doi          = {10.1109/TVCG.2022.3209480},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {994-1004},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geo-storylines: Integrating maps into storyline visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Erato: Cooperative data story editing via fact
interpolation. <em>TVCG</em>, <em>29</em>(1), 983–993. (<a
href="https://doi.org/10.1109/TVCG.2022.3209428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.},
  archive      = {J_TVCG},
  author       = {Mengdi Sun and Ligan Cai and Weiwei Cui and Yanqiu Wu and Yang Shi and Nan Cao},
  doi          = {10.1109/TVCG.2022.3209428},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {983-993},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Erato: Cooperative data story editing via fact interpolation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaking the fourth wall of data stories through
interaction. <em>TVCG</em>, <em>29</em>(1), 972–982. (<a
href="https://doi.org/10.1109/TVCG.2022.3209409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction is increasingly integrating into data stories to support data exploration and explanation. Interaction can also be combined with the narrative device, breaking the fourth wall (BTFW), to build a deeper connection between readers and data stories. BTFW interaction directly addresses readers by requiring their input. Such user input is then integrated into the narrative or visuals of data stories to encourage readers to inspect the stories more closely. In this work, we explore the design patterns of BTFW interaction commonly used in data stories. Six design patterns were identified through the analysis of 58 high-quality data stories collected from a range of online sources. Specifically, the data stories were categorized using a coding framework, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. To explore the benefits as well as concerns of using BTFW interaction, we conducted a three-session user study including the reading, interview, and recall sessions. The results of our user study suggested that BTFW interaction has a positive impact on self-story connection, user engagement, and information recall. We also discussed design implications to address the possible negative effects on the interactivity-comprehensibility balance, information privacy, and the learning curve of interaction brought by BTFW interaction.},
  archive      = {J_TVCG},
  author       = {Yang Shi and Tian Gao and Xiaohan Jiao and Nan Cao},
  doi          = {10.1109/TVCG.2022.3209409},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {972-982},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Breaking the fourth wall of data stories through interaction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The quest for omnioculars: Embedded visualization for
augmenting basketball game viewing experiences. <em>TVCG</em>,
<em>29</em>(1), 962–971. (<a
href="https://doi.org/10.1109/TVCG.2022.3209353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes&#39; positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans&#39; game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans&#39; in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans&#39; in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.},
  archive      = {J_TVCG},
  author       = {Tica Lin and Zhutian Chen and Yalong Yang and Daniele Chiappalupi and Johanna Beyer and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2022.3209353},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {962-971},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The quest for omnioculars: Embedded visualization for augmenting basketball game viewing experiences},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tac-trainer: A visual analytics system for IoT-based racket
sports training. <em>TVCG</em>, <em>29</em>(1), 951–961. (<a
href="https://doi.org/10.1109/TVCG.2022.3209352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional racket sports training highly relies on coaches&#39; knowledge and experience, leading to biases in the guidance. To solve this problem, smart wearable devices based on Internet of Things technology (IoT) have been extensively investigated to support data-driven training. Considerable studies introduced methods to extract valuable information from the sensor data collected by IoT devices. However, the information cannot provide actionable insights for coaches due to the large data volume and high data dimensions. We proposed an IoT + VA framework, Tac-Trainer, to integrate the sensor data, the information, and coaches&#39; knowledge to facilitate racket sports training. Tac-Trainer consists of four components: device configuration, data interpretation, training optimization, and result visualization. These components collect trainees&#39; kinematic data through IoT devices, transform the data into attributes and indicators, generate training suggestions, and provide an interactive visualization interface for exploration, respectively. We further discuss new research opportunities and challenges inspired by our work from two perspectives, VA for IoT and IoT for VA.},
  archive      = {J_TVCG},
  author       = {Jiachen Wang and Ji Ma and Kangping Hu and Zheng Zhou and Hui Zhang and Xiao Xie and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209352},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {951-961},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tac-trainer: A visual analytics system for IoT-based racket sports training},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RASIPAM: Interactive pattern mining of multivariate event
sequences in racket sports. <em>TVCG</em>, <em>29</em>(1), 940–950. (<a
href="https://doi.org/10.1109/TVCG.2022.3209452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors&#39; playing styles. Many data-driven methods apply pattern mining to racket sports data — which is often recorded as multivariate event sequences — to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.},
  archive      = {J_TVCG},
  author       = {Jiang Wu and Dongyu Liu and Ziyang Guo and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209452},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {940-950},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RASIPAM: Interactive pattern mining of multivariate event sequences in racket sports},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OBTracker: Visual analytics of off-ball movements in
basketball. <em>TVCG</em>, <em>29</em>(1), 929–939. (<a
href="https://doi.org/10.1109/TVCG.2022.3209373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a basketball play, players who are not in possession of the ball (i.e., off-ball players) can still effectively contribute to the team&#39;s offense, such as making a sudden move to create scoring opportunities. Analyzing the movements of off-ball players can thus facilitate the development of effective strategies for coaches. However, common basketball statistics (e.g., points and assists) primarily focus on what happens around the ball and are mostly result-oriented, making it challenging to objectively assess and fully understand the contributions of off-ball movements. To address these challenges, we collaborate closely with domain experts and summarize the multi-level requirements for off-ball movement analysis in basketball. We first establish an assessment model to quantitatively evaluate the offensive contribution of an off-ball movement considering both the position of players and the team cooperation. Based on the model, we design and develop a visual analytics system called OBTracker to support the multifaceted analysis of off-ball movements. OBTracker enables users to identify the frequency and effectiveness of off-ball movement patterns and learn the performance of different off-ball players. A tailored visualization based on the Voronoi diagram is proposed to help users interpret the contribution of off-ball movements from a temporal perspective. We conduct two case studies based on the tracking data from NBA games and demonstrate the effectiveness and usability of OBTracker through expert feedback.},
  archive      = {J_TVCG},
  author       = {Yihong Wu and Dazhen Deng and Xiao Xie and Moqi He and Jie Xu and Hongzeng Zhang and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209373},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {929-939},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OBTracker: Visual analytics of off-ball movements in basketball},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sporthesia: Augmenting sports videos using natural language.
<em>TVCG</em>, <em>29</em>(1), 918–928. (<a
href="https://doi.org/10.1109/TVCG.2022.3209497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural language, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we propose a three-step approach – 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video – and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia&#39;s applicability in two exemplar scenarios, i.e. , authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.},
  archive      = {J_TVCG},
  author       = {Zhutian Chen and Qisen Yang and Xiao Xie and Johanna Beyer and Haijun Xia and Yingcai Wu and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2022.3209497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {918-928},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sporthesia: Augmenting sports videos using natural language},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding barriers to network exploration with
visualization: A report from the trenches. <em>TVCG</em>,
<em>29</em>(1), 907–917. (<a
href="https://doi.org/10.1109/TVCG.2022.3209487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reports on an in-depth study that investigates barriers to network exploration with visualizations. Network visualization tools are becoming increasingly popular, but little is known about how analysts plan and engage in the visual exploration of network data—which exploration strategies they employ, and how they prepare their data, define questions, and decide on visual mappings. Our study involved a series of workshops, interaction logging, and observations from a 6-week network exploration course. Our findings shed light on the stages that define analysts&#39; approaches to network visualization and barriers experienced by some analysts during their network visualization processes. These barriers mainly appear before using a specific tool and include defining exploration goals, identifying relevant network structures and abstractions, or creating appropriate visual mappings for their network data. Our findings inform future work in visualization education and analyst-centered network visualization tool design.},
  archive      = {J_TVCG},
  author       = {Mashael AlKadi and Vanessa Serrano and James Scott-Brown and Catherine Plaisant and Jean-Daniel Fekete and Uta Hinrichs and Benjamin Bach},
  doi          = {10.1109/TVCG.2022.3209487},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {907-917},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding barriers to network exploration with visualization: A report from the trenches},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative evaluation of bipartite, node-link, and
matrix-based network representations. <em>TVCG</em>, <em>29</em>(1),
896–906. (<a href="https://doi.org/10.1109/TVCG.2022.3209427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study ( $\mathrm{n}=150$ ), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.},
  archive      = {J_TVCG},
  author       = {Moataz Abdelaal and Nathan D. Schiele and Katrin Angerbauer and Kuno Kurzhals and Michael Sedlmair and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2022.3209427},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {896-906},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative evaluation of bipartite, node-link, and matrix-based network representations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Taurus: Towards a unified force representation and universal
solver for graph layout. <em>TVCG</em>, <em>29</em>(1), 886–895. (<a
href="https://doi.org/10.1109/TVCG.2022.3209371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few decades, a large number of graph layout techniques have been proposed for visualizing graphs from various domains. In this paper, we present a general framework, Taurus, for unifying popular techniques such as the spring-electrical model, stress model, and maxent-stress model. It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. In addition, we introduce a universal augmented stochastic gradient descent (SGD) optimizer that efficiently finds proper solutions for all layout techniques. To demonstrate the power of our framework, we conduct a comprehensive evaluation of existing techniques on a large number of synthetic and real graphs. We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.},
  archive      = {J_TVCG},
  author       = {Mingliang Xue and Zhi Wang and Fahai Zhong and Yong Wang and Mingliang Xu and Oliver Deussen and Yunhai Wang},
  doi          = {10.1109/TVCG.2022.3209371},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {886-895},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taurus: Towards a unified force representation and universal solver for graph layout},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MosaicSets: Embedding set systems into grid graphs.
<em>TVCG</em>, <em>29</em>(1), 875–885. (<a
href="https://doi.org/10.1109/TVCG.2022.3209485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing sets of elements and their relations is an important research area in information visualization. In this paper, we present MosaicSets : a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. As use case, we consider the research groups of a university faculty as elements, and the departments and joint research projects as sets. We aim at finding a suitable mapping between the research groups and the grid cells such that the department structure forms a base map layout. Our objectives are to optimize both the compactness of the entirety of all cells and of each set by itself. We show that computing the mapping is NP-hard. However, using integer linear programming we can solve real-world instances optimally within a few seconds. Moreover, we propose a relaxation of the contiguity requirement to visualize otherwise non-embeddable set systems. We present and discuss different rendering styles for the set overlays. Based on a case study with real-world data, our evaluation comprises quantitative measures as well as expert interviews.},
  archive      = {J_TVCG},
  author       = {Peter Rottmann and Markus Wallinger and Annika Bonerath and Sven Gedicke and Martin Nöllenburg and Jan-Henrik Haunert},
  doi          = {10.1109/TVCG.2022.3209485},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {875-885},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MosaicSets: Embedding set systems into grid graphs},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing ensemble predictions of music mood.
<em>TVCG</em>, <em>29</em>(1), 864–874. (<a
href="https://doi.org/10.1109/TVCG.2022.3209379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music mood classification has been a challenging problem in comparison with other music classification problems (e.g., genre, composer, or period). One solution for addressing this challenge is to use an ensemble of machine learning models. In this paper, we show that visualization techniques can effectively convey the popular prediction as well as uncertainty at different music sections along the temporal axis while enabling the analysis of individual ML models in conjunction with their application to different musical data. In addition to the traditional visual designs, such as stacked line graph, ThemeRiver, and pixel-based visualization, we introduce a new variant of ThemeRiver, called “dual-flux ThemeRiver”, which allows viewers to observe and measure the most popular prediction more easily than stacked line graph and ThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots can also assist in model-development workflows, in addition to annotating music using ensemble model predictions.},
  archive      = {J_TVCG},
  author       = {Zelin Ye and Min Chen},
  doi          = {10.1109/TVCG.2022.3209379},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {864-874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing ensemble predictions of music mood},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibrate: Interactive analysis of probabilistic model
output. <em>TVCG</em>, <em>29</em>(1), 853–863. (<a
href="https://doi.org/10.1109/TVCG.2022.3209489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier&#39;s predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.},
  archive      = {J_TVCG},
  author       = {Peter Xenopoulos and João Rulff and Luis Gustavo Nonato and Brian Barr and Claudio Silva},
  doi          = {10.1109/TVCG.2022.3209489},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {853-863},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Calibrate: Interactive analysis of probabilistic model output},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SliceTeller: A data slice-driven approach for machine
learning model validation. <em>TVCG</em>, <em>29</em>(1), 842–852. (<a
href="https://doi.org/10.1109/TVCG.2022.3209465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as “ Data Slices ”. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller , a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting , to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development , to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.},
  archive      = {J_TVCG},
  author       = {Xiaoyu Zhang and Jorge Piazentin Ono and Huan Song and Liang Gou and Kwan-Liu Ma and Liu Ren},
  doi          = {10.1109/TVCG.2022.3209465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {842-852},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SliceTeller: A data slice-driven approach for machine learning model validation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConceptExplainer: Interactive explanation for deep neural
networks from a concept perspective. <em>TVCG</em>, <em>29</em>(1),
831–841. (<a href="https://doi.org/10.1109/TVCG.2022.3209384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network&#39;s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate C oncept E xplainer , a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how C oncept E xplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
  archive      = {J_TVCG},
  author       = {Jinbin Huang and Aditi Mishra and Bum Chul Kwon and Chris Bryan},
  doi          = {10.1109/TVCG.2022.3209384},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {831-841},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ConceptExplainer: Interactive explanation for deep neural networks from a concept perspective},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VDL-surrogate: A view-dependent latent-based model for
parameter space exploration of ensemble simulations. <em>TVCG</em>,
<em>29</em>(1), 820–830. (<a
href="https://doi.org/10.1109/TVCG.2022.3209413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. Surrogate-enabled parameter space exploration allows domain scientists to preview simulation results without having to run a large number of computationally costly simulations. Limited by computational resources, however, existing surrogate models may not produce previews with sufficient resolution for visualization and analysis. To improve the efficient use of computational resources and support high-resolution exploration, we perform ray casting from different viewpoints to collect samples and produce compact latent representations. This latent encoding process reduces the cost of surrogate model training while maintaining the output quality. In the model training stage, we select viewpoints to cover the whole viewing sphere and train corresponding VDL-Surrogate models for the selected viewpoints. In the model inference stage, we predict the latent representations at previously selected viewpoints and decode the latent representations to data space. For any given viewpoint, we make interpolations over decoded data at selected viewpoints and generate visualizations with user-specified visual mappings. We show the effectiveness and efficiency of VDL-Surrogate in cosmological and ocean simulations with quantitative and qualitative evaluations. Source code is publicly available at https://github.com/trainsn/VDL-Surrogate .},
  archive      = {J_TVCG},
  author       = {Neng Shi and Jiayi Xu and Haoyu Li and Hanqi Guo and Jonathan Woodring and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3209413},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {820-830},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VDL-surrogate: A view-dependent latent-based model for parameter space exploration of ensemble simulations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DPVisCreator: Incorporating pattern constraints to
privacy-preserving visualizations via differential privacy.
<em>TVCG</em>, <em>29</em>(1), 809–819. (<a
href="https://doi.org/10.1109/TVCG.2022.3209391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users&#39; preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis . A prototype system, DPVisCreator , is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.},
  archive      = {J_TVCG},
  author       = {Jiehui Zhou and Xumeng Wang and Jason K. Wong and Huanliang Wang and Zhongwei Wang and Xiaoyu Yang and Xiaoran Yan and Haozhe Feng and Huamin Qu and Haochao Ying and Wei Chen},
  doi          = {10.1109/TVCG.2022.3209391},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {809-819},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DPVisCreator: Incorporating pattern constraints to privacy-preserving visualizations via differential privacy},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual analysis and detection of contrails in aircraft
engine simulations. <em>TVCG</em>, <em>29</em>(1), 798–808. (<a
href="https://doi.org/10.1109/TVCG.2022.3209356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrails are condensation trails generated from emitted particles by aircraft engines, which perturb Earth&#39;s radiation budget. Simulation modeling is used to interpret the formation and development of contrails. These simulations are computationally intensive and rely on high-performance computing solutions, and the contrail structures are not well defined. We propose a visual computing system to assist in defining contrails and their characteristics, as well as in the analysis of parameters for computer-generated aircraft engine simulations. The back-end of our system leverages a contrail-formation criterion and clustering methods to detect contrails&#39; shape and evolution and identify similar simulation runs. The front-end system helps analyze contrails and their parameters across multiple simulation runs. The evaluation with domain experts shows this approach successfully aids in contrail data investigation.},
  archive      = {J_TVCG},
  author       = {Nafiul Nipu and Carla Floricel and Negar Naghashzadeh and Roberto Paoli and G. Elisabeta Marai},
  doi          = {10.1109/TVCG.2022.3209356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {798-808},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis and detection of contrails in aircraft engine simulations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traveler: Navigating task parallel traces for performance
analysis. <em>TVCG</em>, <em>29</em>(1), 788–797. (<a
href="https://doi.org/10.1109/TVCG.2022.3209375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace —a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler , an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.},
  archive      = {J_TVCG},
  author       = {Sayef Azad Sakin and Alex Bigelow and R. Tohid and Connor Scully-Allison and Carlos Scheidegger and Steven R. Brandt and Christopher Taylor and Kevin A. Huck and Hartmut Kaiser and Katherine E. Isaacs},
  doi          = {10.1109/TVCG.2022.3209375},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {788-797},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Traveler: Navigating task parallel traces for performance analysis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive visual analysis of structure-borne noise data.
<em>TVCG</em>, <em>29</em>(1), 778–787. (<a
href="https://doi.org/10.1109/TVCG.2022.3209478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise sources early in the design and production process is essential for reducing a product&#39;s development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise simulation data for an internal combustion engine.},
  archive      = {J_TVCG},
  author       = {Rainer Splechtna and Denis Gračanin and Goran Todorović and Stanislav Goja and Boris Bedić and Helwig Hauser and Krešimir Matković},
  doi          = {10.1109/TVCG.2022.3209478},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {778-787},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visual analysis of structure-borne noise data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PromotionLens: Inspecting promotion strategies of online
e-commerce via visual analytics. <em>TVCG</em>, <em>29</em>(1), 767–777.
(<a href="https://doi.org/10.1109/TVCG.2022.3209440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens , a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support “what-if” analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens .},
  archive      = {J_TVCG},
  author       = {Chenyang Zhang and Xiyuan Wang and Chuyi Zhao and Yijing Ren and Tianyu Zhang and Zhenhui Peng and Xiaomeng Fan and Xiaojuan Ma and Quan Li},
  doi          = {10.1109/TVCG.2022.3209440},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {767-777},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PromotionLens: Inspecting promotion strategies of online E-commerce via visual analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CohortVA: A visual analytic system for interactive
exploration of cohorts based on historical data. <em>TVCG</em>,
<em>29</em>(1), 756–766. (<a
href="https://doi.org/10.1109/TVCG.2022.3209483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.},
  archive      = {J_TVCG},
  author       = {Wei Zhang and Jason K. Wong and Xumeng Wang and Youcheng Gong and Rongchen Zhu and Kai Liu and Zihan Yan and Siwei Tan and Huamin Qu and Siming Chen and Wei Chen},
  doi          = {10.1109/TVCG.2022.3209483},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {756-766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CohortVA: A visual analytic system for interactive exploration of cohorts based on historical data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting user preferences of dimensionality reduction
embedding quality. <em>TVCG</em>, <em>29</em>(1), 745–755. (<a
href="https://doi.org/10.1109/TVCG.2022.3209449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select “good” and “misleading” views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.},
  archive      = {J_TVCG},
  author       = {Cristina Morariu and Adrien Bibal and Rene Cutura and Benoît Frénay and Michael Sedlmair},
  doi          = {10.1109/TVCG.2022.3209449},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {745-755},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Predicting user preferences of dimensionality reduction embedding quality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive visual cluster analysis by contrastive
dimensionality reduction. <em>TVCG</em>, <em>29</em>(1), 734–744. (<a
href="https://doi.org/10.1109/TVCG.2022.3209423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.},
  archive      = {J_TVCG},
  author       = {Jiazhi Xia and Linquan Huang and Weixing Lin and Xin Zhao and Jing Wu and Yang Chen and Ying Zhao and Wei Chen},
  doi          = {10.1109/TVCG.2022.3209423},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {734-744},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visual cluster analysis by contrastive dimensionality reduction},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporation of human knowledge into data embeddings to
improve pattern significance and interpretability. <em>TVCG</em>,
<em>29</em>(1), 723–733. (<a
href="https://doi.org/10.1109/TVCG.2022.3209382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding is a common technique for analyzing multi-dimensional data. However, the embedding projection cannot always form significant and interpretable visual structures that foreshadow underlying data patterns. We propose an approach that incorporates human knowledge into data embeddings to improve pattern significance and interpretability. The core idea is (1) externalizing tacit human knowledge as explicit sample labels and (2) adding a classification loss in the embedding network to encode samples&#39; classes. The approach pulls samples of the same class with similar data features closer in the projection, leading to more compact (significant) and class-consistent (interpretable) visual structures. We give an embedding network with a customized classification loss to implement the idea and integrate the network into a visualization system to form a workflow that supports flexible class creation and pattern exploration. Patterns found on open datasets in case studies, subjects&#39; performance in a user study, and quantitative experiment results illustrate the general usability and effectiveness of the approach.},
  archive      = {J_TVCG},
  author       = {Jie Li and Chun-qi Zhou},
  doi          = {10.1109/TVCG.2022.3209382},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {723-733},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Incorporation of human knowledge into data embeddings to improve pattern significance and interpretability},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PC-expo: A metrics-based interactive axes reordering method
for parallel coordinate displays. <em>TVCG</em>, <em>29</em>(1),
712–722. (<a href="https://doi.org/10.1109/TVCG.2022.3209392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel coordinate plots (PCPs) have been widely used for high-dimensional (HD) data storytelling because they allow for presenting a large number of dimensions without distortions. The axes ordering in PCP presents a particular story from the data based on the user perception of PCP polylines. Existing works focus on directly optimizing for PCP axes ordering based on some common analysis tasks like clustering, neighborhood, and correlation. However, direct optimization for PCP axes based on these common properties is restrictive because it does not account for multiple properties occurring between the axes, and for local properties that occur in small regions in the data. Also, many of these techniques do not support the human-in-the-loop (HIL) paradigm, which is crucial (i) for explainability and (ii) in cases where no single reordering scheme fits the users&#39; goals. To alleviate these problems, we present PC-Expo, a real-time visual analytics framework for all-in-one PCP line pattern detection and axes reordering. We studied the connection of line patterns in PCPs with different data analysis tasks and datasets. PC-Expo expands prior work on PCP axes reordering by developing real-time, local detection schemes for the 12 most common analysis tasks (properties). Users can choose the story they want to present with PCPs by optimizing directly over their choice of properties. These properties can be ranked, or combined using individual weights, creating a custom optimization scheme for axes reordering. Users can control the granularity at which they want to work with their detection scheme in the data, allowing exploration of local regions. PC-Expo also supports HIL axes reordering via local-property visualization, which shows the regions of granular activity for every axis pair. Local-property visualization is helpful for PCP axes reordering based on multiple properties, when no single reordering scheme fits the user goals. A comprehensive evaluation was done with real users and diverse datasets confirm the efficacy of PC-Expo in data storytelling with PCPs.},
  archive      = {J_TVCG},
  author       = {Anjul Tyagi and Tyler Estro and Geoff Kuenning and Erez Zadok and Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3209392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {712-722},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PC-expo: A metrics-based interactive axes reordering method for parallel coordinate displays},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RankAxis: Towards a systematic combination of projection and
ranking in multi-attribute data exploration. <em>TVCG</em>,
<em>29</em>(1), 701–711. (<a
href="https://doi.org/10.1109/TVCG.2022.3209463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection and ranking are frequently used analysis techniques in multi-attribute data exploration. Both families of techniques help analysts with tasks such as identifying similarities between observations and determining ordered subgroups, and have shown good performances in multi-attribute data exploration. However, they often exhibit problems such as distorted projection layouts, obscure semantic interpretations, and non-intuitive effects produced by selecting a subset of (weighted) attributes. Moreover, few studies have attempted to combine projection and ranking into the same exploration space to complement each other&#39;s strengths and weaknesses. For this reason, we propose RankAxis , a visual analytics system that systematically combines projection and ranking to facilitate the mutual interpretation of these two techniques and jointly support multi-attribute data exploration. A real-world case study, expert feedback, and a user study demonstrate the efficacy of RankAxis .},
  archive      = {J_TVCG},
  author       = {Qiangqiang Liu and Yukun Ren and Zhihua Zhu and Dai Li and Xiaojuan Ma and Quan Li},
  doi          = {10.1109/TVCG.2022.3209463},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {701-711},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RankAxis: Towards a systematic combination of projection and ranking in multi-attribute data exploration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DashBot: Insight-driven dashboard generation based on deep
reinforcement learning. <em>TVCG</em>, <em>29</em>(1), 690–700. (<a
href="https://doi.org/10.1109/TVCG.2022.3209468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.},
  archive      = {J_TVCG},
  author       = {Dazhen Deng and Aoyu Wu and Huamin Qu and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {690-700},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DashBot: Insight-driven dashboard generation based on deep reinforcement learning},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IDLat: An importance-driven latent generation method for
scientific data. <em>TVCG</em>, <em>29</em>(1), 679–689. (<a
href="https://doi.org/10.1109/TVCG.2022.3209419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.},
  archive      = {J_TVCG},
  author       = {Jingyi Shen and Haoyu Li and Jiayi Xu and Ayan Biswas and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3209419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {679-689},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IDLat: An importance-driven latent generation method for scientific data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring effects of spatial visualization and domain on
visualization task performance: A comparative study. <em>TVCG</em>,
<em>29</em>(1), 668–678. (<a
href="https://doi.org/10.1109/TVCG.2022.3209491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding one&#39;s audience is foundational to creating high impact visualization designs. However, individual differences and cognitive abilities influence interactions with information visualization. Different user needs and abilities suggest that an individual&#39;s background could influence cognitive performance and interactions with visuals in a systematic way. This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. We measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law &amp; political science, and math &amp; computer science. We conducted an online study with 90 participants using an established psychometric test to assess spatial visualization ability, and bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. Accuracy and response times varied with domain across chart types and task difficulty. We found that accuracy and time correlate with spatial visualization level, and education in math &amp; computer science can indicate higher spatial visualization. Additionally, we found that motivational differences between domains could contribute to increased levels of accuracy. Our findings indicate discipline not only affects user needs and interactions with data visualization, but also cognitive traits. Our results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.},
  archive      = {J_TVCG},
  author       = {Sara Tandon and Alfie Abdul-Rahman and Rita Borgo},
  doi          = {10.1109/TVCG.2022.3209491},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {668-678},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring effects of spatial visualization and domain on visualization task performance: A comparative study},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual space coupling model guided overlap-free scatterplot.
<em>TVCG</em>, <em>29</em>(1), 657–667. (<a
href="https://doi.org/10.1109/TVCG.2022.3209459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overdraw problem of scatterplots seriously interferes with the visual tasks. Existing methods, such as data sampling, node dispersion, subspace mapping, and visual abstraction, cannot guarantee the correspondence and consistency between the data points that reflect the intrinsic original data distribution and the corresponding visual units that reveal the presented data distribution, thus failing to obtain an overlap-free scatterplot with unbiased and lossless data distribution. A dual space coupling model is proposed in this paper to represent the complex bilateral relationship between data space and visual space theoretically and analytically. Under the guidance of the model, an overlap-free scatterplot method is developed through integration of the following: a geometry-based data transformation algorithm, namely DistributionTranscriptor; an efficient spatial mutual exclusion guided view transformation algorithm, namely PolarPacking; an overlap-free oriented visual encoding configuration model and a radius adjustment tool, namely $f_{r_{draw}}$ . Our method can ensure complete and accurate information transfer between the two spaces, maintaining consistency between the newly created scatterplot and the original data distribution on global and local features. Quantitative evaluation proves our remarkable progress on computational efficiency compared with the state-of-the-art methods. Three applications involving pattern enhancement, interaction improvement, and overdraw mitigation of trajectory visualization demonstrate the broad prospects of our method.},
  archive      = {J_TVCG},
  author       = {Zeyu Li and Ruizhi Shi and Yan Liu and Shizhuo Long and Ziheng Guo and Shichao Jia and Jiawan Zhang},
  doi          = {10.1109/TVCG.2022.3209459},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {657-667},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dual space coupling model guided overlap-free scatterplot},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate probabilistic range queries for scalable
interactive 3D visualization. <em>TVCG</em>, <em>29</em>(1), 646–656.
(<a href="https://doi.org/10.1109/TVCG.2022.3209439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale scientific data, such as weather and climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, in particular logical AND operations, which is challenging for large data and many attributes. Many general data structures for this problem are built for and scale with a fixed number of attributes, and scalability of joint queries with arbitrary attribute subsets remains a significant problem. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection , allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive , mainly scaling with the cardinality of the query result rather than with the input data size. This is particularly important for joint attribute queries, where the query output is usually much smaller than the whole data set. Additionally, our approach can split query evaluation between user interaction and rendering, achieving much better scalability for interactive visualization than the previous state of the art. Furthermore, even when a multi-resolution strategy is used for visualization, queries are jointly evaluated at the finest data granularity, because our framework does not limit query accuracy to a fixed spatial subdivision.},
  archive      = {J_TVCG},
  author       = {Amani Ageeli and Alberto Jaspe-Villanueva and Ronell Sicat and Florian Mannuss and Peter Rautek and Markus Hadwiger},
  doi          = {10.1109/TVCG.2022.3209439},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {646-656},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multivariate probabilistic range queries for scalable interactive 3D visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communicating uncertainty in digital humanities
visualization research. <em>TVCG</em>, <em>29</em>(1), 635–645. (<a
href="https://doi.org/10.1109/TVCG.2022.3209436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their historical nature, humanistic data encompass multiple sources of uncertainty. While humanists are accustomed to handling such uncertainty with their established methods, they are cautious of visualizations that appear overly objective and fail to communicate this uncertainty. To design more trustworthy visualizations for humanistic research, therefore, a deeper understanding of its relation to uncertainty is needed. We systematically reviewed 126 publications from digital humanities literature that use visualization as part of their research process, and examined how uncertainty was handled and represented in their visualizations. Crossing these dimensions with the visualization type and use, we identified that uncertainty originated from multiple steps in the research process from the source artifacts to their datafication. We also noted how besides known uncertainty coping strategies, such as excluding data and evaluating its effects, humanists also embraced uncertainty as a separate dimension important to retain. By mapping how the visualizations encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization. This work contributes with two empirical taxonomies of uncertainty and it&#39;s corresponding coping strategies, as well as with the foundation of a research agenda for uncertainty visualization in the digital humanities. Our findings further the synergy among humanists and visualization researchers, and ultimately contribute to the development of more trustworthy, uncertainty-aware visualizations.},
  archive      = {J_TVCG},
  author       = {Georgia Panagiotidou and Houda Lamqaddam and Jeroen Poblome and Koenraad Brosens and Katrien Verbert and Andrew Vande Moere},
  doi          = {10.1109/TVCG.2022.3209436},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {635-645},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Communicating uncertainty in digital humanities visualization research},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dispersion vs disparity: Hiding variability can encourage
stereotyping when visualizing social outcomes. <em>TVCG</em>,
<em>29</em>(1), 624–634. (<a
href="https://doi.org/10.1109/TVCG.2022.3209377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization research often focuses on perceptual accuracy or helping readers interpret key messages. However, we know very little about how chart designs might influence readers&#39; perceptions of the people behind the data. Specifically, could designs interact with readers&#39; social cognitive biases in ways that perpetuate harmful stereotypes? For example, when analyzing social inequality, bar charts are a popular choice to present outcome disparities between race, gender, or other groups. But bar charts may encourage deficit thinking, the perception that outcome disparities are caused by groups&#39; personal strengths or deficiencies, rather than external factors. These faulty personal attributions can then reinforce stereotypes about the groups being visualized. We conducted four experiments examining design choices that influence attribution biases (and therefore deficit thinking). Crowdworkers viewed visualizations depicting social outcomes that either mask variability in data, such as bar charts or dot plots, or emphasize variability in data, such as jitter plots or prediction intervals. They reported their agreement with both personal and external explanations for the visualized disparities. Overall, when participants saw visualizations that hide within-group variability, they agreed more with personal explanations. When they saw visualizations that emphasize within-group variability, they agreed less with personal explanations. These results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping. Design choices can influence these biases: Hiding variability tends to increase stereotyping while emphasizing variability reduces it.},
  archive      = {J_TVCG},
  author       = {Eli Holder and Cindy Xiong},
  doi          = {10.1109/TVCG.2022.3209377},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {624-634},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dispersion vs disparity: Hiding variability can encourage stereotyping when visualizing social outcomes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fiber uncertainty visualization for bivariate data with
parametric and nonparametric noise models. <em>TVCG</em>,
<em>29</em>(1), 613–623. (<a
href="https://doi.org/10.1109/TVCG.2022.3209424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization and analysis of multivariate data and their uncertainty are top research challenges in data visualization. Constructing fiber surfaces is a popular technique for multivariate data visualization that generalizes the idea of level-set visualization for univariate data to multivariate data. In this paper, we present a statistical framework to quantify positional probabilities of fibers extracted from uncertain bivariate fields. Specifically, we extend the state-of-the-art Gaussian models of uncertainty for bivariate data to other parametric distributions (e.g., uniform and Epanechnikov) and more general nonparametric probability distributions (e.g., histograms and kernel density estimation) and derive corresponding spatial probabilities of fibers. In our proposed framework, we leverage Green&#39;s theorem for closed-form computation of fiber probabilities when bivariate data are assumed to have independent parametric and nonparametric noise. Additionally, we present a nonparametric approach combined with numerical integration to study the positional probability of fibers when bivariate data are assumed to have correlated noise. For uncertainty analysis, we visualize the derived probability volumes for fibers via volume rendering and extracting level sets based on probability thresholds. We present the utility of our proposed techniques via experiments on synthetic and simulation datasets.},
  archive      = {J_TVCG},
  author       = {Tushar M. Athawale and Chris R. Johnson and Sudhanshu Sane and David Pugmire},
  doi          = {10.1109/TVCG.2022.3209424},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {613-623},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fiber uncertainty visualization for bivariate data with parametric and nonparametric noise models},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the use of uncertainty visualisations for
imputations of data missing at random in scatterplots. <em>TVCG</em>,
<em>29</em>(1), 602–612. (<a
href="https://doi.org/10.1109/TVCG.2022.3209348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most real-world datasets contain missing values yet most exploratory data analysis (EDA) systems only support visualising data points with complete cases. This omission may potentially lead the user to biased analyses and insights. Imputation techniques can help estimate the value of a missing data point, but introduces additional uncertainty. In this work, we investigate the effects of visualising imputed values in charts using different ways of representing data imputations and imputation uncertainty— no imputation, mean, 95\% confidence intervals, probability density plots, gradient intervals , and hypothetical outcome plots . We focus on scatterplots, which is a commonly used chart type, and conduct a crowdsourced study with 202 participants. We measure users&#39; bias and precision in performing two tasks—estimating average and detecting trend—and their self-reported confidence in performing these tasks. Our results suggest that, when estimating averages, uncertainty representations may reduce bias but at the cost of decreasing precision. When estimating trend, only hypothetical outcome plots may lead to a small probability of reducing bias while increasing precision. Participants in every uncertainty representation were less certain about their response when compared to the baseline. The findings point towards potential trade-offs in using uncertainty encodings for datasets with a large number of missing values. This paper and the associated analysis materials are available at: https://osf.io/q4y5r/},
  archive      = {J_TVCG},
  author       = {Abhraneel Sarma and Shunan Guo and Jane Hoffswell and Ryan Rossi and Fan Du and Eunyee Koh and Matthew Kay},
  doi          = {10.1109/TVCG.2022.3209348},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {602-612},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating the use of uncertainty visualisations for imputations of data missing at random in scatterplots},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Polyphony: An interactive transfer learning framework for
single-cell data analysis. <em>TVCG</em>, <em>29</em>(1), 591–601. (<a
href="https://doi.org/10.1109/TVCG.2022.3209408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference-based cell-type annotation can significantly reduce time and effort in single-cell analysis by transferring labels from a previously-annotated dataset to a new dataset. However, label transfer by end-to-end computational methods is challenging due to the entanglement of technical ( e.g. , from different sequencing batches or techniques) and biological ( e.g. , from different cellular microenvironments) variations, only the first of which must be removed. To address this issue, we propose Polyphony , an interactive transfer learning (ITL) framework, to complement biologists&#39; knowledge with advanced computational methods. Polyphony is motivated and guided by domain experts&#39; needs for a controllable, interactive, and algorithm-assisted annotation process, identified through interviews with seven biologists. We introduce anchors, i.e. , analogous cell populations across datasets, as a paradigm to explain the computational process and collect user feedback for model improvement. We further design a set of visualizations and interactions to empower users to add, delete, or modify anchors, resulting in refined cell type annotations. The effectiveness of this approach is demonstrated through quantitative experiments, two hypothetical use cases, and interviews with two biologists. The results show that our anchor-based ITL method takes advantage of both human and machine intelligence in annotating massive single-cell datasets.},
  archive      = {J_TVCG},
  author       = {Furui Cheng and Mark S Keller and Huamin Qu and Nils Gehlenborg and Qianwen Wang},
  doi          = {10.1109/TVCG.2022.3209408},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {591-601},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Polyphony: An interactive transfer learning framework for single-cell data analysis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMolBoxes: Dataflow model for molecular dynamics
exploration. <em>TVCG</em>, <em>29</em>(1), 581–590. (<a
href="https://doi.org/10.1109/TVCG.2022.3209411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present sMolBoxes, a dataflow representation for the exploration and analysis of long molecular dynamics (MD) simulations. When MD simulations reach millions of snapshots, a frame-by-frame observation is not feasible anymore. Thus, biochemists rely to a large extent only on quantitative analysis of geometric and physico-chemical properties. However, the usage of abstract methods to study inherently spatial data hinders the exploration and poses a considerable workload. sMolBoxes link quantitative analysis of a user-defined set of properties with interactive 3D visualizations. They enable visual explanations of molecular behaviors, which lead to an efficient discovery of biochemically significant parts of the MD simulation. sMolBoxes follow a node-based model for flexible definition, combination, and immediate evaluation of properties to be investigated. Progressive analytics enable fluid switching between multiple properties, which facilitates hypothesis generation. Each sMolBox provides quick insight to an observed property or function, available in more detail in the bigBox View. The case studies illustrate that even with relatively few sMolBoxes, it is possible to express complex analytical tasks, and their use in exploratory analysis is perceived as more efficient than traditional scripting-based methods.},
  archive      = {J_TVCG},
  author       = {Pavol Ulbrich and Manuela Waldner and Katarína Furmanová and Sérgio M. Marques and David Bednář and Barbora Kozlíková and Jan Byška},
  doi          = {10.1109/TVCG.2022.3209411},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {581-590},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SMolBoxes: Dataflow model for molecular dynamics exploration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GenoREC: A recommendation system for interactive genomics
data visualization. <em>TVCG</em>, <em>29</em>(1), 570–580. (<a
href="https://doi.org/10.1109/TVCG.2022.3209407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/ .},
  archive      = {J_TVCG},
  author       = {Aditeya Pandey and Sehi L&#39;Yi and Qianwen Wang and Michelle A. Borkin and Nils Gehlenborg},
  doi          = {10.1109/TVCG.2022.3209407},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {570-580},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GenoREC: A recommendation system for interactive genomics data visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view design patterns and responsive visualization for
genomics data. <em>TVCG</em>, <em>29</em>(1), 559–569. (<a
href="https://doi.org/10.1109/TVCG.2022.3209398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as “vertically long,” “horizontally wide,” “circular,” and “cross-shaped” compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.},
  archive      = {J_TVCG},
  author       = {Sehi L&#39;Yi and Nils Gehlenborg},
  doi          = {10.1109/TVCG.2022.3209398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {559-569},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-view design patterns and responsive visualization for genomics data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Level set restricted voronoi tessellation for large scale
spatial statistical analysis. <em>TVCG</em>, <em>29</em>(1), 548–558.
(<a href="https://doi.org/10.1109/TVCG.2022.3209473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial statistical analysis of multivariate volumetric data can be challenging due to scale, complexity, and occlusion. Advances in topological segmentation, feature extraction, and statistical summarization have helped overcome the challenges. This work introduces a new spatial statistical decomposition method based on level sets, connected components, and a novel variation of the restricted centroidal Voronoi tessellation that is better suited for spatial statistical decomposition and parallel efficiency. The resulting data structures organize features into a coherent nested hierarchy to support flexible and efficient out-of-core region-of-interest extraction. Next, we provide an efficient parallel implementation. Finally, an interactive visualization system based on this approach is designed and then applied to turbulent combustion data. The combined approach enables an interactive spatial statistical analysis workflow for large-scale data with a top-down approach through multiple-levels-of-detail that links phase space statistics with spatial features.},
  archive      = {J_TVCG},
  author       = {Tyson Neuroth and Martin Rieth and Konduri Aditya and Myoungkyu Lee and Jacqueline H Chen and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3209473},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {548-558},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Level set restricted voronoi tessellation for large scale spatial statistical analysis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quick clusters: A GPU-parallel partitioning for efficient
path tracing of unstructured volumetric grids. <em>TVCG</em>,
<em>29</em>(1), 537–547. (<a
href="https://doi.org/10.1109/TVCG.2022.3209418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6× (1 frame per second (fps) and 1.0 M rays per second (rps) up to now 6 fps and 12.4 M rps , now including volumetric shadows) while simultaneously reducing memory consumption by 3× (33 GB down to 11 GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48 GB of an RTX 8000, we improve the performance of Lander by 17 × (1 fps up to 17 fps, 1.0 M rps up to 35.6 M rps) .},
  archive      = {J_TVCG},
  author       = {Nate Morrical and Alper Sahistan and Uğur Güdükbay and Ingo Wald and Valerio Pascucci},
  doi          = {10.1109/TVCG.2022.3209418},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {537-547},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Quick clusters: A GPU-parallel partitioning for efficient path tracing of unstructured volumetric grids},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRay: Ray casting for visualization and interactive data
exploration of gaussian mixture models. <em>TVCG</em>, <em>29</em>(1),
526–536. (<a href="https://doi.org/10.1109/TVCG.2022.3209374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian mixture model (GMM) describes the distribution of random variables from several different populations. GMMs have widespread applications in probability theory, statistics, machine learning for unsupervised cluster analysis and topic modeling, as well as in deep learning pipelines. So far, few efforts have been made to explore the underlying point distribution in combination with the GMMs, in particular when the data becomes high-dimensional and when the GMMs are composed of many Gaussians. We present an analysis tool comprising various GPU-based visualization techniques to explore such complex GMMs. To facilitate the exploration of high-dimensional data, we provide a novel navigation system to analyze the underlying data. Instead of projecting the data to 2D, we utilize interactive 3D views to better support users in understanding the spatial arrangements of the Gaussian distributions. The interactive system is composed of two parts: (1) raycasting-based views that visualize cluster memberships, spatial arrangements, and support the discovery of new modes. (2) overview visualizations that enable the comparison of Gaussians with each other, as well as small multiples of different choices of basis vectors. Users are supported in their exploration with customization tools and smooth camera navigations. Our tool was developed and assessed by five domain experts, and its usefulness was evaluated with 23 participants. To demonstrate the effectiveness, we identify interesting features in several data sets.},
  archive      = {J_TVCG},
  author       = {Kai Lawonn and Monique Meuschke and Pepe Eulzer and Matthias Mitterreiter and Joachim Giesen and Tobias Günther},
  doi          = {10.1109/TVCG.2022.3209374},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {526-536},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GRay: Ray casting for visualization and interactive data exploration of gaussian mixture models},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FoVolNet: Fast volume rendering using foveated deep neural
networks. <em>TVCG</em>, <em>29</em>(1), 515–525. (<a
href="https://doi.org/10.1109/TVCG.2022.3209498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet—a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user&#39;s focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user&#39;s field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system&#39;s rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.},
  archive      = {J_TVCG},
  author       = {David Bauer and Qi Wu and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2022.3209498},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {515-525},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FoVolNet: Fast volume rendering using foveated deep neural networks},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data hunches: Incorporating personal knowledge into
visualizations. <em>TVCG</em>, <em>29</em>(1), 504–514. (<a
href="https://doi.org/10.1109/TVCG.2022.3209451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trouble with data is that it frequently provides only an imperfect representation of a phenomenon of interest. Experts who are familiar with their datasets will often make implicit, mental corrections when analyzing a dataset, or will be cautious not to be overly confident about their findings if caveats are present. However, personal knowledge about the caveats of a dataset is typically not incorporated in a structured way, which is problematic if others who lack that knowledge interpret the data. In this work, we define such analysts&#39; knowledge about datasets as data hunches . We differentiate data hunches from uncertainty and discuss types of hunches. We then explore ways of recording data hunches, and, based on a prototypical design, develop recommendations for designing visualizations that support data hunches. We conclude by discussing various challenges associated with data hunches, including the potential for harm and challenges for trust and privacy. We envision that data hunches will empower analysts to externalize their knowledge, facilitate collaboration and communication, and support the ability to learn from others&#39; data hunches.},
  archive      = {J_TVCG},
  author       = {Haihan Lin and Derya Akbaba and Miriah Meyer and Alexander Lex},
  doi          = {10.1109/TVCG.2022.3209451},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {504-514},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data hunches: Incorporating personal knowledge into visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seeing what you believe or believing what you see? Belief
biases correlation estimation. <em>TVCG</em>, <em>29</em>(1), 493–503.
(<a href="https://doi.org/10.1109/TVCG.2022.3209405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an analyst or scientist has a belief about how the world works, their thinking can be biased in favor of that belief. Therefore, one bedrock principle of science is to minimize that bias by testing the predictions of one&#39;s belief against objective data. But interpreting visualized data is a complex perceptual and cognitive process. Through two crowdsourced experiments, we demonstrate that supposedly objective assessments of the strength of a correlational relationship can be influenced by how strongly a viewer believes in the existence of that relationship. Participants viewed scatterplots depicting a relationship between meaningful variable pairs (e.g., number of environmental regulations and air quality) and estimated their correlations. They also estimated the correlation of the same scatterplots labeled instead with generic ‘X’ and ‘Y’ axes. In a separate section, they also reported how strongly they believed there to be a correlation between the meaningful variable pairs. Participants estimated correlations more accurately when they viewed scatterplots labeled with generic axes compared to scatterplots labeled with meaningful variable pairs. Furthermore, when viewers believed that two variables should have a strong relationship, they overestimated correlations between those variables by an r-value of about 0.1. When they believed that the variables should be unrelated, they underestimated the correlations by an r-value of about 0.1. While data visualizations are typically thought to present objective truths to the viewer, these results suggest that existing personal beliefs can bias even objective statistical values people extract from data.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Chase Stokes and Yea-Seul Kim and Steven Franconeri},
  doi          = {10.1109/TVCG.2022.3209405},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {493-503},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seeing what you believe or believing what you see? belief biases correlation estimation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified comparison of user modeling techniques for
predicting data interaction and detecting exploration bias.
<em>TVCG</em>, <em>29</em>(1), 483–492. (<a
href="https://doi.org/10.1109/TVCG.2022.3209476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual analytics community has proposed several user modeling algorithms to capture and analyze users&#39; interaction behavior in order to assist users in data exploration and insight generation. For example, some can detect exploration biases while others can predict data points that the user will interact with before that interaction occurs. Researchers believe this collection of algorithms can help create more intelligent visual analytics tools. However, the community lacks a rigorous evaluation and comparison of these existing techniques. As a result, there is limited guidance on which method to use and when. Our paper seeks to fill in this missing gap by comparing and ranking eight user modeling algorithms based on their performance on a diverse set of four user study datasets. We analyze exploration bias detection, data interaction prediction, and algorithmic complexity, among other measures. Based on our findings, we highlight open challenges and new directions for analyzing user interactions and visualization provenance.},
  archive      = {J_TVCG},
  author       = {Sunwoo Ha and Shayan Monadjemi and Roman Garnett and Alvitta Ottley},
  doi          = {10.1109/TVCG.2022.3209476},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {483-492},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A unified comparison of user modeling techniques for predicting data interaction and detecting exploration bias},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D-BIAS: A causality-based human-in-the-loop system for
tackling algorithmic bias. <em>TVCG</em>, <em>29</em>(1), 473–482. (<a
href="https://doi.org/10.1109/TVCG.2022.3209484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.},
  archive      = {J_TVCG},
  author       = {Bhavya Ghai and Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3209484},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {473-482},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {D-BIAS: A causality-based human-in-the-loop system for tackling algorithmic bias},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VACSEN: A visualization approach for noise awareness in
quantum computing. <em>TVCG</em>, <em>29</em>(1), 462–472. (<a
href="https://doi.org/10.1109/TVCG.2022.3209455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing has attracted considerable public attention due to its exponential speedup over classical computing. Despite its advantages, today&#39;s quantum computers intrinsically suffer from noise and are error-prone. To guarantee the high fidelity of the execution result of a quantum algorithm, it is crucial to inform users of the noises of the used quantum computer and the compiled physical circuits. However, an intuitive and systematic way to make users aware of the quantum computing noise is still missing. In this paper, we fill the gap by proposing a novel visualization approach to achieve noise-aware quantum computing. It provides a holistic picture of the noise of quantum computing through multiple interactively coordinated views: a Computer Evolution View with a circuit-like design overviews the temporal evolution of the noises of different quantum computers, a Circuit Filtering View facilitates quick filtering of multiple compiled physical circuits for the same quantum algorithm, and a Circuit Comparison View with a coupled bar chart enables detailed comparison of the filtered compiled circuits. We extensively evaluate the performance of VACSEN through two case studies on quantum algorithms of different scales and in-depth interviews with 12 quantum computing users. The results demonstrate the effectiveness and usability of VACSEN in achieving noise-aware quantum computing.},
  archive      = {J_TVCG},
  author       = {Shaolun Ruan and Yong Wang and Weiwen Jiang and Ying Mao and Qiang Guan},
  doi          = {10.1109/TVCG.2022.3209455},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {462-472},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VACSEN: A visualization approach for noise awareness in quantum computing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoboHapalytics: A robot assisted haptic controller for
immersive analytics. <em>TVCG</em>, <em>29</em>(1), 451–461. (<a
href="https://doi.org/10.1109/TVCG.2022.3209433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user&#39;s fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.},
  archive      = {J_TVCG},
  author       = {Shaozhang Dai and Jim Smiley and Tim Dwyer and Barrett Ens and Lonni Besancon},
  doi          = {10.1109/TVCG.2022.3209433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {451-461},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RoboHapalytics: A robot assisted haptic controller for immersive analytics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of view layout on situated analytics for
multiple-view representations in immersive visualization. <em>TVCG</em>,
<em>29</em>(1), 440–450. (<a
href="https://doi.org/10.1109/TVCG.2022.3209475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.},
  archive      = {J_TVCG},
  author       = {Zhen Wen and Wei Zeng and Luoxuan Weng and Yihan Liu and Mingliang Xu and Wei Chen},
  doi          = {10.1109/TVCG.2022.3209475},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {440-450},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of view layout on situated analytics for multiple-view representations in immersive visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PuzzleFixer: A visual reassembly system for immersive
fragments restoration. <em>TVCG</em>, <em>29</em>(1), 429–439. (<a
href="https://doi.org/10.1109/TVCG.2022.3209388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PuzzleFixer, an immersive interactive system for experts to rectify defective reassembled 3D objects. Reassembling the fragments of a broken object to restore its original state is the prerequisite of many analytical tasks such as cultural relics analysis and forensics reasoning. While existing computer-aided methods can automatically reassemble fragments, they often derive incorrect objects due to the complex and ambiguous fragment shapes. Thus, experts usually need to refine the object manually. Prior advances in immersive technologies provide benefits for realistic perception and direct interactions to visualize and interact with 3D fragments. However, few studies have investigated the reassembled object refinement. The specific challenges include: 1) the fragment combination set is too large to determine the correct matches, and 2) the geometry of the fragments is too complex to align them properly. To tackle the first challenge, PuzzleFixer leverages dimensionality reduction and clustering techniques, allowing users to review possible match categories, select the matches with reasonable shapes, and drill down to shapes to correct the corresponding faces. For the second challenge, PuzzleFixer embeds the object with node-link networks to augment the perception of match relations. Specifically, it instantly visualizes matches with graph edges and provides force feedback to facilitate the efficiency of alignment interactions. To demonstrate the effectiveness of PuzzleFixer, we conducted an expert evaluation based on two cases on real-world artifacts and collected feedback through post-study interviews. The results suggest that our system is suitable and efficient for experts to refine incorrect reassembled objects.},
  archive      = {J_TVCG},
  author       = {Shuainan Ye and Zhutian Chen and Xiangtong Chu and Kang Li and Juntong Luo and Yi Li and Guohua Geng and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {429-439},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PuzzleFixer: A visual reassembly system for immersive fragments restoration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring interactions with printed data visualizations in
augmented reality. <em>TVCG</em>, <em>29</em>(1), 418–428. (<a
href="https://doi.org/10.1109/TVCG.2022.3209386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops ( $\mathrm{N}=20$ ) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter) action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study ( $\mathrm{N}=12$ , HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement “point” for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.},
  archive      = {J_TVCG},
  author       = {Wai Tong and Zhutian Chen and Meng Xia and Leo Yu-Ho Lo and Linping Yuan and Benjamin Bach and Huamin Qu},
  doi          = {10.1109/TVCG.2022.3209386},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {418-428},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring interactions with printed data visualizations in augmented reality},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Studying early decision making with progressive bar charts.
<em>TVCG</em>, <em>29</em>(1), 407–417. (<a
href="https://doi.org/10.1109/TVCG.2022.3209426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct a user study to quantify and compare user performance for a value comparison task using four bar chart designs, where the bars show the mean values of data loaded progressively and updated every second (progressive bar charts). Progressive visualization divides different stages of the visualization pipeline—data loading, processing, and visualization—into iterative animated steps to limit the latency when loading large amounts of data. An animated visualization appearing quickly, unfolding, and getting more accurate with time, enables users to make early decisions. However, intermediate mean estimates are computed only on partial data and may not have time to converge to the true means, potentially misleading users and resulting in incorrect decisions. To address this issue, we propose two new designs visualizing the history of values in progressive bar charts, in addition to the use of confidence intervals. We comparatively study four progressive bar chart designs: with/without confidence intervals, and using near-history representation with/without confidence intervals, on three realistic data distributions. We evaluate user performance based on the percentage of correct answers (accuracy), response time, and user confidence. Our results show that, overall, users can make early and accurate decisions with 92\% accuracy using only 18\% of the data, regardless of the design. We find that our proposed bar chart design with only near-history is comparable to bar charts with only confidence intervals in performance, and the qualitative feedback we received indicates a preference for designs with history.},
  archive      = {J_TVCG},
  author       = {Ameya Patil and Gaëlle Richer and Christopher Jermaine and Dominik Moritz and Jean-Daniel Fekete},
  doi          = {10.1109/TVCG.2022.3209426},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {407-417},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Studying early decision making with progressive bar charts},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scanner deeply: Predicting gaze heatmaps on visualizations
using crowdsourced eye movement data. <em>TVCG</em>, <em>29</em>(1),
396–406. (<a href="https://doi.org/10.1109/TVCG.2022.3209472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a S canner D eeply , a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper&#39;s contribution.},
  archive      = {J_TVCG},
  author       = {Sungbok Shin and Sunghyo Chung and Sanghyun Hong and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2022.3209472},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {396-406},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A scanner deeply: Predicting gaze heatmaps on visualizations using crowdsourced eye movement data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unifying effects of direct and relational associations for
visual communication. <em>TVCG</em>, <em>29</em>(1), 385–395. (<a
href="https://doi.org/10.1109/TVCG.2022.3209443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People have expectations about how colors map to concepts in visualizations, and they are better at interpreting visualizations that match their expectations. Traditionally, studies on these expectations ( inferred mappings ) distinguished distinct factors relevant for visualizations of categorical vs. continuous information. Studies on categorical information focused on direct associations (e.g., mangos are associated with yellows) whereas studies on continuous information focused on relational associations (e.g., darker colors map to larger quantities; dark-is-more bias). We unite these two areas within a single framework of assignment inference. Assignment inference is the process by which people infer mappings between perceptual features and concepts represented in encoding systems. Observers infer globally optimal assignments by maximizing the “merit,” or “goodness,” of each possible assignment. Previous work on assignment inference focused on visualizations of categorical information. We extend this approach to visualizations of continuous data by (a) broadening the notion of merit to include relational associations and (b) developing a method for combining multiple (sometimes conflicting) sources of merit to predict people&#39;s inferred mappings. We developed and tested our model on data from experiments in which participants interpreted colormap data visualizations, representing fictitious data about environmental concepts (sunshine, shade, wild fire, ocean water, glacial ice). We found both direct and relational associations contribute independently to inferred mappings. These results can be used to optimize visualization design to facilitate visual communication.},
  archive      = {J_TVCG},
  author       = {Melissa A. Schoenlein and Johnny Campos and Kevin J. Lande and Laurent Lessard and Karen B. Schloss},
  doi          = {10.1109/TVCG.2022.3209443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {385-395},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unifying effects of direct and relational associations for visual communication},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Photosensitive accessibility for interactive data
visualizations. <em>TVCG</em>, <em>29</em>(1), 374–384. (<a
href="https://doi.org/10.1109/TVCG.2022.3209359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accessibility guidelines place restrictions on the use of animations and interactivity on webpages to lessen the likelihood of webpages inadvertently producing sequences with flashes, patterns, or color changes that may trigger seizures for individuals with photosensitive epilepsy. Online data visualizations often incorporate elements of animation and interactivity to create a narrative, engage users, or encourage exploration. These design guidelines have been empirically validated by perceptual studies in visualization literature, but the impact of animation and interaction in visualizations on users with photosensitivity, who may experience seizures in response to certain visual stimuli, has not been considered. We systematically gathered and tested 1,132 interactive and animated visualizations for seizure-inducing risk using established methods and found that currently available methods for determining photosensitive risk are not reliable when evaluating interactive visualizations, as risk scores varied significantly based on the individual interacting with the visualization. To address this issue, we introduce a theoretical model defining the degree of control visualization designers have over three determinants of photosensitive risk in potentially seizure-inducing sequences: the size, frequency, and color of flashing content. Using an analysis of 375 visualizations hosted on bl.ocks.org, we created a theoretical model of photosensitive risk in visualizations by arranging the photosensitive risk determinants according to the degree of control visualization authors have over whether content exceeds photosensitive accessibility thresholds. We then use this model to propose a new method of testing for photosensitive risk that focuses on elements of visualizations that are subject to greater authorial control - and are therefore more robust to variations in the individual user - producing more reliable risk assessments than existing methods when applied to interactive visualizations. A full copy of this paper and all study materials are available at https://osf.io/8kzmg/ .},
  archive      = {J_TVCG},
  author       = {Laura South and Michelle A. Borkin},
  doi          = {10.1109/TVCG.2022.3209359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {374-384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Photosensitive accessibility for interactive data visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BeauVis: A validated scale for measuring the aesthetic
pleasure of visual representations. <em>TVCG</em>, <em>29</em>(1),
363–373. (<a href="https://doi.org/10.1109/TVCG.2022.3209390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We developed and validated a rating scale to assess the aesthetic pleasure (or beauty ) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, “enjoyable,” “likable,” “pleasing,” “nice,” and “appealing.” Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.},
  archive      = {J_TVCG},
  author       = {Tingying He and Petra Isenberg and Raimund Dachselt and Tobias Isenberg},
  doi          = {10.1109/TVCG.2022.3209390},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {363-373},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {BeauVis: A validated scale for measuring the aesthetic pleasure of visual representations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for multiclass contour visualization.
<em>TVCG</em>, <em>29</em>(1), 353–362. (<a
href="https://doi.org/10.1109/TVCG.2022.3209482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiclass contour visualization is often used to interpret complex data attributes in such fields as weather forecasting, computational fluid dynamics, and artificial intelligence. However, effective and accurate representations of underlying data patterns and correlations can be challenging in multiclass contour visualization, primarily due to the inevitable visual cluttering and occlusions when the number of classes is significant. To address this issue, visualization design must carefully choose design parameters to make visualization more comprehensible. With this goal in mind, we proposed a framework for multiclass contour visualization. The framework has two components: a set of four visualization design parameters, which are developed based on an extensive review of literature on contour visualization, and a declarative domain-specific language (DSL) for creating multiclass contour rendering, which enables a fast exploration of those design parameters. A task-oriented user study was conducted to assess how those design parameters affect users&#39; interpretations of real-world data. The study results offered some suggestions on the value choices of design parameters in multiclass contour visualization.},
  archive      = {J_TVCG},
  author       = {Sihang Li and Jiacheng Yu and Mingxuan Li and Le Liu and Xiaolong Luke Zhang and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2022.3209482},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {353-362},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A framework for multiclass contour visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dashboard design patterns. <em>TVCG</em>, <em>29</em>(1),
342–352. (<a href="https://doi.org/10.1109/TVCG.2022.3209448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces design patterns for dashboards to inform dashboard design processes. Despite a growing number of public examples, case studies, and general guidelines there is surprisingly little design guidance for dashboards. Such guidance is necessary to inspire designs and discuss tradeoffs in, e.g., screenspace, interaction, or information shown. Based on a systematic review of 144 dashboards, we report on eight groups of design patterns that provide common solutions in dashboard design. We discuss combinations of these patterns in “dashboard genres” such as narrative, analytical , or embedded dashboard . We ran a 2-week dashboard design workshop with 23 participants of varying expertise working on their own data and dashboards. We discuss the application of patterns for the dashboard design processes, as well as general design tradeoffs and common challenges. Our work complements previous surveys and aims to support dashboard designers and researchers in co-creation, structured design decisions, as well as future user evaluations about dashboard design guidelines. Detailed pattern descriptions and workshop material can be found online: https://dashboarddesignpatterns.github.io},
  archive      = {J_TVCG},
  author       = {Benjamin Bach and Euan Freeman and Alfie Abdul-Rahman and Cagatay Turkay and Saiful Khan and Yulei Fan and Min Chen},
  doi          = {10.1109/TVCG.2022.3209448},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {342-352},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dashboard design patterns},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaGlyph: Automatic generation of metaphoric glyph-based
visualization. <em>TVCG</em>, <em>29</em>(1), 331–341. (<a
href="https://doi.org/10.1109/TVCG.2022.3209447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.},
  archive      = {J_TVCG},
  author       = {Lu Ying and Xinhuan Shu and Dazhen Deng and Yuchen Yang and Tan Tang and Lingyun Yu and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209447},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {331-341},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MetaGlyph: Automatic generation of metaphoric glyph-based visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DendroMap: Visual exploration of large-scale image datasets
for machine learning with treemaps. <em>TVCG</em>, <em>29</em>(1),
320–330. (<a href="https://doi.org/10.1109/TVCG.2022.3209425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/ .},
  archive      = {J_TVCG},
  author       = {Donald Bertucci and Md Montaser Hamid and Yashwanthi Anand and Anita Ruangrotsakun and Delyar Tabatabai and Melissa Perez and Minsuk Kahng},
  doi          = {10.1109/TVCG.2022.3209425},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {320-330},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DendroMap: Visual exploration of large-scale image datasets for machine learning with treemaps},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HetVis: A visual analysis approach for identifying data
heterogeneity in horizontal federated learning. <em>TVCG</em>,
<em>29</em>(1), 310–319. (<a
href="https://doi.org/10.1109/TVCG.2022.3209347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.},
  archive      = {J_TVCG},
  author       = {Xumeng Wang and Wei Chen and Jiazhi Xia and Zhen Wen and Rongchen Zhu and Tobias Schreck},
  doi          = {10.1109/TVCG.2022.3209347},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {310-319},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HetVis: A visual analysis approach for identifying data heterogeneity in horizontal federated learning},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NAS-navigator: Visual steering for explainable one-shot deep
neural network synthesis. <em>TVCG</em>, <em>29</em>(1), 299–309. (<a
href="https://doi.org/10.1109/TVCG.2022.3209361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.},
  archive      = {J_TVCG},
  author       = {Anjul Tyagi and Cong Xie and Klaus Mueller},
  doi          = {10.1109/TVCG.2022.3209361},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {299-309},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NAS-navigator: Visual steering for explainable one-shot deep neural network synthesis},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual analysis of neural architecture spaces for
summarizing design principles. <em>TVCG</em>, <em>29</em>(1), 288–298.
(<a href="https://doi.org/10.1109/TVCG.2022.3209404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the pairwise distance calculation as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn2N) to O(knN) . Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures.},
  archive      = {J_TVCG},
  author       = {Jun Yuan and Mengchen Liu and Fengyuan Tian and Shixia Liu},
  doi          = {10.1109/TVCG.2022.3209404},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {288-298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of neural architecture spaces for summarizing design principles},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relaxed dot plots: Faithful visualization of samples and
their distribution. <em>TVCG</em>, <em>29</em>(1), 278–287. (<a
href="https://doi.org/10.1109/TVCG.2022.3209429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce relaxed dot plots as an improvement of nonlinear dot plots for unit visualization. Our plots produce more faithful data representations and reduce moiré effects. Their contour is based on a customized kernel frequency estimation to match the shape of the distribution of underlying data values. Previous nonlinear layouts introduce column-centric nonlinear scaling of dot diameters for visualization of high-dynamic-range data with high peaks. We provide a mathematical approach to convert that column-centric scaling to our smooth envelope shape. This formalism allows us to use linear, root, and logarithmic scaling to find ideal dot sizes. Our method iteratively relaxes the dot layout for more correct and aesthetically pleasing results. To achieve this, we modified Lloyd&#39;s algorithm with additional constraints and heuristics. We evaluate the layouts of relaxed dot plots against a previously existing nonlinear variant and show that our algorithm produces less error regarding the underlying data while establishing the blue noise property that works against moiré effects. Further, we analyze the readability of our relaxed plots in three crowd-sourced experiments. The results indicate that our proposed technique surpasses traditional dot plots.},
  archive      = {J_TVCG},
  author       = {Nils Rodrigues and Christoph Schulz and Sören Döring and Daniel Baumgartner and Tim Krake and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2022.3209429},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {278-287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Relaxed dot plots: Faithful visualization of samples and their distribution},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Roboviz: A game-centered project for information
visualization education. <em>TVCG</em>, <em>29</em>(1), 268–277. (<a
href="https://doi.org/10.1109/TVCG.2022.3209402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client–real or simulated–a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.},
  archive      = {J_TVCG},
  author       = {Eytan Adar and Elsie Lee-Robbins},
  doi          = {10.1109/TVCG.2022.3209402},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {268-277},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Roboviz: A game-centered project for information visualization education},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cultivating visualization literacy for children through
curiosity and play. <em>TVCG</em>, <em>29</em>(1), 257–267. (<a
href="https://doi.org/10.1109/TVCG.2022.3209442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fostering data visualization literacy (DVL) as part of childhood education could lead to a more data literate society. However, most work in DVL for children relies on a more formal educational context (i.e., a teacher-led approach) that limits children&#39;s engagement with data to classroom-based environments and, consequently, children&#39;s ability to ask questions about and explore data on topics they find personally meaningful. We explore how a curiosity-driven, child-led approach can provide more agency to children when they are authoring data visualizations. This paper explores how informal learning with crafting physicalizations through play and curiosity may foster increased literacy and engagement with data. Employing a constructionist approach, we designed a do-it-yourself toolkit made out of everyday materials (e.g., paper, cardboard, mirrors) that enables children to create, customize, and personalize three different interactive visualizations (bar, line, pie). We used the toolkit as a design probe in a series of in-person workshops with 5 children (6 to 11-year-olds) and interviews with 5 educators. Our observations reveal that the toolkit helped children creatively engage and interact with visualizations. Children with prior knowledge of data visualization reported the toolkit serving as more of an authoring tool that they envision using in their daily lives, while children with little to no experience found the toolkit as an engaging introduction to data visualization. Our study demonstrates the potential of using the constructionist approach to cultivate children&#39;s DVL through curiosity and play.},
  archive      = {J_TVCG},
  author       = {S. Sandra Bae and Rishi Vanukuru and Ruhan Yang and Peter Gyory and Ran Zhou and Ellen Yi-Luen Do and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2022.3209442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {257-267},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cultivating visualization literacy for children through curiosity and play},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised color-concept association via image
colorization. <em>TVCG</em>, <em>29</em>(1), 247–256. (<a
href="https://doi.org/10.1109/TVCG.2022.3209481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human&#39;s expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover&#39;s Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.},
  archive      = {J_TVCG},
  author       = {Ruizhen Hu and Ziqi Ye and Bin Chen and Oliver van Kaick and Hui Huang},
  doi          = {10.1109/TVCG.2022.3209481},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {247-256},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-supervised color-concept association via image colorization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supporting expressive and faithful pictorial visualization
design with visual style transfer. <em>TVCG</em>, <em>29</em>(1),
236–246. (<a href="https://doi.org/10.1109/TVCG.2022.3209486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.},
  archive      = {J_TVCG},
  author       = {Yang Shi and Pei Liu and Siji Chen and Mengdi Sun and Nan Cao},
  doi          = {10.1109/TVCG.2022.3209486},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {236-246},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supporting expressive and faithful pictorial visualization design with visual style transfer},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KiriPhys: Exploring new data physicalization opportunities.
<em>TVCG</em>, <em>29</em>(1), 225–235. (<a
href="https://doi.org/10.1109/TVCG.2022.3209365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while providing a pleasurable experience that stimulates curiosity and engagement.},
  archive      = {J_TVCG},
  author       = {Foroozan Daneshzand and Charles Perin and Sheelagh Carpendale},
  doi          = {10.1109/TVCG.2022.3209365},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {225-235},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KiriPhys: Exploring new data physicalization opportunities},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ASTF: Visual abstractions of time-varying patterns in radio
signals. <em>TVCG</em>, <em>29</em>(1), 214–224. (<a
href="https://doi.org/10.1109/TVCG.2022.3209469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices. Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses.},
  archive      = {J_TVCG},
  author       = {Ying Zhao and Luhao Ge and Huixuan Xie and Genghuai Bai and Zhao Zhang and Qiang Wei and Yun Lin and Yuchao Liu and Fangfang Zhou},
  doi          = {10.1109/TVCG.2022.3209469},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {214-224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ASTF: Visual abstractions of time-varying patterns in radio signals},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LargeNetVis: Visual exploration of large temporal networks
based on community taxonomies. <em>TVCG</em>, <em>29</em>(1), 203–213.
(<a href="https://doi.org/10.1109/TVCG.2022.3209477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal (or time-evolving) networks are commonly used to model complex systems and the evolution of their components throughout time. Although these networks can be analyzed by different means, visual analytics stands out as an effective way for a pre-analysis before doing quantitative/statistical analyses to identify patterns, anomalies, and other behaviors in the data, thus leading to new insights and better decision-making. However, the large number of nodes, edges, and/or timestamps in many real-world networks may lead to polluted layouts that make the analysis inefficient or even infeasible. In this paper, we propose LargeNetVis, a web-based visual analytics system designed to assist in analyzing small and large temporal networks. It successfully achieves this goal by leveraging three taxonomies focused on network communities to guide the visual exploration process. The system is composed of four interactive visual components: the first (Taxonomy Matrix) presents a summary of the network characteristics, the second (Global View) gives an overview of the network evolution, the third (a node-link diagram) enables community- and node-level structural analysis, and the fourth (a Temporal Activity Map – TAM) shows the community- and node-level activity under a temporal perspective. We demonstrate the usefulness and effectiveness of LargeNetVis through two usage scenarios and a user study with 14 participants.},
  archive      = {J_TVCG},
  author       = {Claudio D. G. Linhares and Jean R. Ponciano and Diogenes S. Pedro and Luis E. C. Rocha and Agma J. M. Traina and Jorge Poco},
  doi          = {10.1109/TVCG.2022.3209477},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {203-213},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LargeNetVis: Visual exploration of large temporal networks based on community taxonomies},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SizePairs: Achieving stable and balanced temporal treemaps
using hierarchical size-based pairing. <em>TVCG</em>, <em>29</em>(1),
193–202. (<a href="https://doi.org/10.1109/TVCG.2022.3209450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SizePairs, a new technique to create stable and balanced treemap layouts that visualize values changing over time in hierarchical data. To achieve an overall high-quality result across all time steps in terms of stability and aspect ratio, SizePairs employs a new hierarchical size-based pairing algorithm that recursively pairs two nodes that complement their size changes over time and have similar sizes. SizePairs maximizes the visual quality and stability by optimizing the splitting orientation of each internal node and flipping leaf nodes, if necessary. We also present a comprehensive comparison of SizePairs against the state-of-the-art treemaps developed for visualizing time-dependent data. SizePairs outperforms existing techniques in both visual quality and stability, while being faster than the local moves technique.},
  archive      = {J_TVCG},
  author       = {Chang Han and Jaemin Jo and Anyi Li and Bongshin Lee and Oliver Deussen and Yunhai Wang},
  doi          = {10.1109/TVCG.2022.3209450},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {193-202},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SizePairs: Achieving stable and balanced temporal treemaps using hierarchical size-based pairing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained dynamic mode decomposition. <em>TVCG</em>,
<em>29</em>(1), 182–192. (<a
href="https://doi.org/10.1109/TVCG.2022.3209437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency-based decomposition of time series data is used in many visualization applications. Most of these decomposition methods (such as Fourier transform or singular spectrum analysis) only provide interaction via pre- and post-processing, but no means to influence the core algorithm. A method that also belongs to this class is Dynamic Mode Decomposition (DMD), a spectral decomposition method that extracts spatio-temporal patterns from data. In this paper, we incorporate frequency-based constraints into DMD for an adaptive decomposition that leads to user-controllable visualizations, allowing analysts to include their knowledge into the process. To accomplish this, we derive an equivalent reformulation of DMD that implicitly provides access to the eigenvalues (and therefore to the frequencies) identified by DMD. By utilizing a constrained minimization problem customized to DMD, we can guarantee the existence of desired frequencies by minimal changes to DMD. We complement this core approach by additional techniques for constrained DMD to facilitate explorative visualization and investigation of time series data. With several examples, we demonstrate the usefulness of constrained DMD and compare it to conventional frequency-based decomposition methods.},
  archive      = {J_TVCG},
  author       = {Tim Krake and Daniel Klötzl and Bernhard Eberhardt and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2022.3209437},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {182-192},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Constrained dynamic mode decomposition},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing the passage of time with video temporal
pyramids. <em>TVCG</em>, <em>29</em>(1), 171–181. (<a
href="https://doi.org/10.1109/TVCG.2022.3209454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What can we learn about a scene by watching it for months or years? A video recorded over a long timespan will depict interesting phenomena at multiple timescales, but identifying and viewing them presents a challenge. The video is too long to watch in full, and some things are too slow to experience in real-time, such as glacial retreat or the gradual shift from summer to fall. Timelapse videography is a common approach to summarizing long videos and visualizing slow timescales. However, a timelapse is limited to a single chosen temporal frequency, and often appears flickery due to aliasing. Also, the length of the timelapse video is directly tied to its temporal resolution, which necessitates tradeoffs between those two facets. In this paper, we propose Video Temporal Pyramids, a technique that addresses these limitations and expands the possibilities for visualizing the passage of time. Inspired by spatial image pyramids from computer vision, we developed an algorithm that builds video pyramids in the temporal domain. Each level of a Video Temporal Pyramid visualizes a different timescale; for instance, videos from the monthly timescale are usually good for visualizing seasonal changes, while videos from the one-minute timescale are best for visualizing sunrise or the movement of clouds across the sky. To help explore the different pyramid levels, we also propose a Video Spectrogram to visualize the amount of activity across the entire pyramid, providing a holistic overview of the scene dynamics and the ability to explore and discover phenomena across time and timescales. To demonstrate our approach, we have built Video Temporal Pyramids from ten outdoor scenes, each containing months or years of data. We compare Video Temporal Pyramid layers to naive timelapse and find that our pyramids enable alias-free viewing of longer-term changes. We also demonstrate that the Video Spectrogram facilitates exploration and discovery of phenomena across pyramid levels, by enabling both overview and detail-focused perspectives.},
  archive      = {J_TVCG},
  author       = {Melissa E. Swift and Wyatt Ayers and Sophie Pallanck and Scott Wehrwein},
  doi          = {10.1109/TVCG.2022.3209454},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {171-181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing the passage of time with video temporal pyramids},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No grammar to rule them all: A survey of JSON-style DSLs for
visualization. <em>TVCG</em>, <em>29</em>(1), 160–170. (<a
href="https://doi.org/10.1109/TVCG.2022.3209460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.},
  archive      = {J_TVCG},
  author       = {Andrew M. McNutt},
  doi          = {10.1109/TVCG.2022.3209460},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {160-170},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {No grammar to rule them all: A survey of JSON-style DSLs for visualization},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Animated vega-lite: Unifying animation with a grammar of
interactive graphics. <em>TVCG</em>, <em>29</em>(1), 149–159. (<a
href="https://doi.org/10.1109/TVCG.2022.3209369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language&#39;s existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel . Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream ; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.},
  archive      = {J_TVCG},
  author       = {Jonathan Zong and Josh Pollock and Dylan Wootton and Arvind Satyanarayan},
  doi          = {10.1109/TVCG.2022.3209369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {149-159},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Animated vega-lite: Unifying animation with a grammar of interactive graphics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HiTailor: Interactive transformation and visualization for
hierarchical tabular data. <em>TVCG</em>, <em>29</em>(1), 139–148. (<a
href="https://doi.org/10.1109/TVCG.2022.3209354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular visualization techniques integrate visual representations with tabular data to avoid additional cognitive load caused by splitting users&#39; attention. However, most of the existing studies focus on simple flat tables instead of hierarchical tables, whose complex structure limits the expressiveness of visualization results and affects users&#39; efficiency in visualization construction. We present HiTailor, a technique for presenting and exploring hierarchical tables. HiTailor constructs an abstract model, which defines row/column headings as biclustering and hierarchical structures. Based on our abstract model, we identify three pairs of operators, Swap/Transpose, ToStacked/ToLinear, Fold/Unfold, for transformations of hierarchical tables to support users&#39; comprehensive explorations. After transformation, users can specify a cell or block of interest in hierarchical tables as a TableUnit for visualization, and HiTailor recommends other related TableUnits according to the abstract model using different mechanisms. We demonstrate the usability of the HiTailor system through a comparative study and a case study with domain experts, showing that HiTailor can present and explore hierarchical tables from different viewpoints. HiTailor is available at https://github.com/bitvis2021/HiTailor .},
  archive      = {J_TVCG},
  author       = {Guozheng Li and Runfei Li and Zicheng Wang and Chi Harold Liu and Min Lu and Guoren Wang},
  doi          = {10.1109/TVCG.2022.3209354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {139-148},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HiTailor: Interactive transformation and visualization for hierarchical tabular data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rigel: Transforming tabular data by declarative mapping.
<em>TVCG</em>, <em>29</em>(1), 128–138. (<a
href="https://doi.org/10.1109/TVCG.2022.3209385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Rigel, an interactive system for rapid transformation of tabular data. Rigel implements a new declarative mapping approach that formulates the data transformation procedure as direct mappings from data to the row, column, and cell channels of the target table. To construct such mappings, Rigel allows users to directly drag data attributes from input data to these three channels and indirectly drag or type data values in a spreadsheet, and possible mappings that do not contradict these interactions are recommended to achieve efficient and straightforward data transformation. The recommended mappings are generated by enumerating and composing data variables based on the row, column, and cell channels, thereby revealing the possibility of alternative tabular forms and facilitating open-ended exploration in many data transformation scenarios, such as designing tables for presentation. In contrast to existing systems that transform data by composing operations (like transposing and pivoting), Rigel requires less prior knowledge on these operations, and constructing tables from the channels is more efficient and results in less ambiguity than generating operation sequences as done by the traditional by-example approaches. User study results demonstrated that Rigel is significantly less demanding in terms of time and interactions and suits more scenarios compared to the state-of-the-art by-example approach. A gallery of diverse transformation cases is also presented to show the potential of Rigel&#39;s expressiveness.},
  archive      = {J_TVCG},
  author       = {Ran Chen and Di Weng and Yanwei Huang and Xinhuan Shu and Jiayi Zhou and Guodao Sun and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {128-138},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rigel: Transforming tabular data by declarative mapping},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revealing the semantics of data wrangling scripts with
comantics. <em>TVCG</em>, <em>29</em>(1), 117–127. (<a
href="https://doi.org/10.1109/TVCG.2022.3209470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop C omantics , a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a “slot filling” strategy. We design experiments to evaluate the performance of C omantics . Further, we assess its flexibility using three example applications in different domains.},
  archive      = {J_TVCG},
  author       = {Kai Xiong and Zhongsu Luo and Siwei Fu and Yongheng Wang and Mingliang Xu and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3209470},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {117-127},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revealing the semantics of data wrangling scripts with comantics},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visinity: Visual spatial neighborhood analysis for
multiplexed tissue imaging data. <em>TVCG</em>, <em>29</em>(1), 106–116.
(<a href="https://doi.org/10.1109/TVCG.2022.3209378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New highly-multiplexed imaging technologies have enabled the study of tissues in unprecedented detail. These methods are increasingly being applied to understand how cancer cells and immune response change during tumor development, progression, and metastasis, as well as following treatment. Yet, existing analysis approaches focus on investigating small tissue samples on a per-cell basis, not taking into account the spatial proximity of cells, which indicates cell-cell interaction and specific biological processes in the larger cancer microenvironment. We present Visinity, a scalable visual analytics system to analyze cell interaction patterns across cohorts of whole-slide multiplexed tissue images. Our approach is based on a fast regional neighborhood computation, leveraging unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. These neighborhoods can be visually analyzed in an exploratory and confirmatory workflow. Users can explore spatial patterns present across tissues through a scalable image viewer and coordinated views highlighting the neighborhood composition and spatial arrangements of cells. To verify or refine existing hypotheses, users can query for specific patterns to determine their presence and statistical significance. Findings can be interactively annotated, ranked, and compared in the form of small multiples. In two case studies with biomedical experts, we demonstrate that Visinity can identify common biological processes within a human tonsil and uncover novel white-blood cell networks and immune-tumor interactions.},
  archive      = {J_TVCG},
  author       = {Simon Warchol and Robert Krueger and Ajit Johnson Nirmal and Giorgio Gaglia and Jared Jessup and Cecily C. Ritch and John Hoffer and Jeremy Muhlich and Megan L. Burger and Tyler Jacks and Sandro Santagata and Peter K. Sorger and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2022.3209378},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {106-116},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visinity: Visual spatial neighborhood analysis for multiplexed tissue imaging data},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diverse interaction recommendation for public users
exploring multi-view visualization using deep learning. <em>TVCG</em>,
<em>29</em>(1), 95–105. (<a
href="https://doi.org/10.1109/TVCG.2022.3209461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users&#39; interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method&#39;s capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.},
  archive      = {J_TVCG},
  author       = {Yixuan Li and Yusheng Qi and Yang Shi and Qing Chen and Nan Cao and Siming Chen},
  doi          = {10.1109/TVCG.2022.3209461},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {95-105},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Diverse interaction recommendation for public users exploring multi-view visualization using deep learning},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A design space for surfacing content recommendations in
visual analytic platforms. <em>TVCG</em>, <em>29</em>(1), 84–94. (<a
href="https://doi.org/10.1109/TVCG.2022.3209445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users&#39; visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.},
  archive      = {J_TVCG},
  author       = {Zhilan Zhou and Wenyuan Wang and Mengtian Guo and Yue Wang and David Gotz},
  doi          = {10.1109/TVCG.2022.3209445},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {84-94},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A design space for surfacing content recommendations in visual analytic platforms},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual concept programming: A visual analytics approach to
injecting human intelligence at scale. <em>TVCG</em>, <em>29</em>(1),
74–83. (<a href="https://doi.org/10.1109/TVCG.2022.3209466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics) . Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code . Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.},
  archive      = {J_TVCG},
  author       = {Md Naimul Hoque and Wenbin He and Arvind Kumar Shekar and Liang Gou and Liu Ren},
  doi          = {10.1109/TVCG.2022.3209466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {74-83},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual concept programming: A visual analytics approach to injecting human intelligence at scale},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MedChemLens: An interactive visual tool to support direction
selection in interdisciplinary experimental research of medicinal
chemistry. <em>TVCG</em>, <em>29</em>(1), 63–73. (<a
href="https://doi.org/10.1109/TVCG.2022.3209434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding “in what direction to proceed” is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.},
  archive      = {J_TVCG},
  author       = {Chuhan Shi and Fei Nie and Yicheng Hu and Yige Xu and Lei Chen and Xiaojuan Ma and Qiong Luo},
  doi          = {10.1109/TVCG.2022.3209434},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {63-73},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MedChemLens: An interactive visual tool to support direction selection in interdisciplinary experimental research of medicinal chemistry},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TrafficVis: Visualizing organized activity and
spatio-temporal patterns for detecting and labeling human trafficking.
<em>TVCG</em>, <em>29</em>(1), 53–62. (<a
href="https://doi.org/10.1109/TVCG.2022.3209403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Law enforcement and domain experts can detect human trafficking (HT) in online escort websites by analyzing suspicious clusters of connected ads. How can we explain clustering results intuitively and interactively, visualizing potential evidence for experts to analyze? We present T raffic V is , the first interface for cluster-level HT detection and labeling. Developed through months of participatory design with domain experts, T raffic V is provides coordinated views in conjunction with carefully chosen backend algorithms to effectively show spatio-temporal and text patterns to a wide variety of anti-HT stakeholders. We build upon state-of-the-art text clustering algorithms by incorporating shared metadata as a signal of connected and possibly suspicious activity, then visualize the results. Domain experts can use T raffic V is to label clusters as HT, or other, suspicious, but non-HT activity such as spam and scam , quickly creating labeled datasets to enable further HT research. Through domain expert feedback and a usage scenario, we demonstrate TRAFFICVIS&#39;s efficacy. The feedback was overwhelmingly positive, with repeated high praises for the usability and explainability of our tool, the latter being vital for indicting possible criminals.},
  archive      = {J_TVCG},
  author       = {Catalina Vajiac and Duen Horng Chau and Andreas Olligschlaeger and Rebecca Mackenzie and Pratheeksha Nair and Meng-Chieh Lee and Yifei Li and Namyong Park and Reihaneh Rabbany and Christos Faloutsos},
  doi          = {10.1109/TVCG.2022.3209403},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {53-62},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TrafficVis: Visualizing organized activity and spatio-temporal patterns for detecting and labeling human trafficking},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ErgoExplorer: Interactive ergonomic risk assessment from
video collections. <em>TVCG</em>, <em>29</em>(1), 43–52. (<a
href="https://doi.org/10.1109/TVCG.2022.3209432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction. ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.},
  archive      = {J_TVCG},
  author       = {Manlio Massiris Fernández and Sanjin Radoš and Krešimir Matković and M. Eduard Gröller and Claudio Delrieux},
  doi          = {10.1109/TVCG.2022.3209432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {43-52},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ErgoExplorer: Interactive ergonomic risk assessment from video collections},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSEUDo: Interactive pattern search in multivariate time
series with locality-sensitive hashing and relevance feedback.
<em>TVCG</em>, <em>29</em>(1), 33–42. (<a
href="https://doi.org/10.1109/TVCG.2022.3209431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PSEUDo, a visual pattern retrieval tool for multivariate time series. It aims to overcome the uneconomic (re-)training problem accompanying deep learning-based methods. Very high-dimensional time series emerge on an unprecedented scale due to increasing sensor usage and data storage. Visual pattern search is one of the most frequent tasks on time series. Automatic pattern retrieval methods often suffer from inefficient training data, a lack of ground truth labels, and a discrepancy between the similarity perceived by the algorithm and required by the user or the task. Our proposal is based on the query-aware locality-sensitive hashing technique to create a representation of multivariate time series windows. It features sub-linear training and inference time with respect to data dimensions. This performance gain allows an instantaneous relevance-feedback-driven adaption to converge to users&#39; similarity notion. We demonstrate PSEUDo&#39;s performance in terms of accuracy, speed, steerability, and usability through quantitative benchmarks with representative time series retrieval methods and a case study. We find that PSEUDo detects patterns in high-dimensional time series efficiently, improves the result with relevance feedback through feature selection, and allows an understandable as well as user-friendly retrieval process.},
  archive      = {J_TVCG},
  author       = {Yuncong Yu and Dylan Kruyff and Jiao Jiao and Tim Becker and Michael Behrisch},
  doi          = {10.1109/TVCG.2022.3209431},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {33-42},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PSEUDo: Interactive pattern search in multivariate time series with locality-sensitive hashing and relevance feedback},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware multidimensional scaling. <em>TVCG</em>,
<em>29</em>(1), 23–32. (<a
href="https://doi.org/10.1109/TVCG.2022.3209420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of multidimensional scaling (MDS) to uncertain data, facilitating uncertainty visualization of multidimensional data. Our approach uses local projection operators that map high-dimensional random vectors to low-dimensional space to formulate a generalized stress. In this way, our generic model supports arbitrary distributions and various stress types. We use our uncertainty-aware multidimensional scaling (UAMDS) concept to derive a formulation for the case of normally distributed random vectors and a squared stress. The resulting minimization problem is numerically solved via gradient descent. We complement UAMDS by additional visualization techniques that address the sensitivity and trustworthiness of dimensionality reduction under uncertainty. With several examples, we demonstrate the usefulness of our approach and the importance of uncertainty-aware techniques.},
  archive      = {J_TVCG},
  author       = {David Hägele and Tim Krake and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2022.3209420},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {23-32},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty-aware multidimensional scaling},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple forecast visualizations (MFVs): Trade-offs in trust
and performance in multiple COVID-19 forecast visualizations.
<em>TVCG</em>, <em>29</em>(1), 12–22. (<a
href="https://doi.org/10.1109/TVCG.2022.3209457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of inadequate SARS-COV-2 (COVID-19) responses may indicate a lack of trust in forecasts and risk communication. However, no work has empirically tested how multiple forecast visualization choices impact trust and task-based performance. The three studies presented in this paper ( $N=1299$ ) examine how visualization choices impact trust in COVID-19 mortality forecasts and how they influence performance in a trend prediction task. These studies focus on line charts populated with real-time COVID-19 data that varied the number and color encoding of the forecasts and the presence of best/worst-case forecasts. The studies reveal that trust in COVID-19 forecast visualizations initially increases with the number of forecasts and then plateaus after 6–9 forecasts. However, participants were most trusting of visualizations that showed less visual information, including a 95\% confidence interval, single forecast, and grayscale encoded forecasts. Participants maintained high trust in intervals labeled with 50\% and 25\% and did not proportionally scale their trust to the indicated interval size. Despite the high trust, the 95\% CI condition was the most likely to evoke predictions that did not correspond with the actual COVID-19 trend. Qualitative analysis of participants&#39; strategies confirmed that many participants trusted both the simplistic visualizations and those with numerous forecasts. This work provides practical guides for how COVID-19 forecast visualizations influence trust, including recommendations for identifying the range where forecasts balance trade-offs between trust and task-based performance.},
  archive      = {J_TVCG},
  author       = {Lace Padilla and Racquel Fygenson and Spencer C. Castro and Enrico Bertini},
  doi          = {10.1109/TVCG.2022.3209457},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {12-22},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiple forecast visualizations (MFVs): Trade-offs in trust and performance in multiple COVID-19 forecast visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affective learning objectives for communicative
visualizations. <em>TVCG</em>, <em>29</em>(1), 1–11. (<a
href="https://doi.org/10.1109/TVCG.2022.3209500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents–those that seek to influence or leverage the audience&#39;s opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived ‘neutrality’ or are ‘political,’ designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions–such as advocacy visualizations or persuasive cartography–we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.},
  archive      = {J_TVCG},
  author       = {Elsie Lee-Robbins and Eytan Adar},
  doi          = {10.1109/TVCG.2022.3209500},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Affective learning objectives for communicative visualizations},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
