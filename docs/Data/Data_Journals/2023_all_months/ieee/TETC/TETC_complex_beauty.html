<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc---85">TETC - 85</h2>
<ul>
<li><details>
<summary>
(2023). Always on voting: A framework for repetitive voting on the
blockchain. <em>TETC</em>, <em>11</em>(4), 1082–1092. (<a
href="https://doi.org/10.1109/TETC.2023.3315748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elections repeat commonly after a fixed time interval, ranging from months to years. This results in limitations on governance since elected candidates or policies are difficult to remove before the next elections, if needed, and allowed by the corresponding law. Participants may decide (through a public deliberation) to change their choices but have no opportunity to vote for these choices before the next elections. Another issue is the peak-end effect, where the judgment of voters is based on how they felt a short time before the elections. To address these issues, we propose Always on Voting (AoV) – a repetitive voting framework that allows participants to vote and change elected candidates or policies without waiting for the next elections. Participants are permitted to privately change their vote at any point in time, while the effect of their change is manifested at the end of each epoch, whose duration is shorter than the time between two main elections. To thwart the problem of peak-end effect in epochs, the ends of epochs are randomized and made unpredictable, while preserved within soft bounds. These goals are achieved using the synergy between a Bitcoin puzzle oracle, verifiable delay function, and smart contracts.},
  archive      = {J_TETC},
  author       = {Sarad Venugopalan and Ivana Stančíková and Ivan Homoliak},
  doi          = {10.1109/TETC.2023.3315748},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1082-1092},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Always on voting: A framework for repetitive voting on the blockchain},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two double-node-upset-hardened flip-flop designs for
high-performance applications. <em>TETC</em>, <em>11</em>(4), 1070–1081.
(<a href="https://doi.org/10.1109/TETC.2023.3317070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous advancement of complementary metal-oxide-semiconductor technologies makes flip-flops (FFs) vulnerable to soft errors. Single-node upsets (SNUs), as well as double-node upsets (DNUs), are typical soft errors. This article proposes two radiation-hardened FF designs, namely DNU-tolerant FF (DUT-FF) and DNU-recoverable FF (DUR-FF). First, the DUT-FF which mainly consists of four dual-interlocked-storage-cells (DICEs) and three 2-input C-elements, is proposed. Then, to provide complete self-recovery from DNUs, the DUR-FF which mainly uses six interlocked DICEs is proposed. They have the following advantages: 1) They can completely protect against SNUs as well as DNUs; 2) the DUT-FF is cost-effective but the DUR-FF can provide complete self-recovery from any DNU. Simulations show the complete SNU/DNU tolerance of DUT-FF and the complete SNU/DNU self-recovery of DUR-FF but at the cost of indispensable area overhead when compared to the SNU hardened FFs. Besides, compared to the FFs of the same-type, the proposed FFs achieve a low delay making them suitable for high-performance applications.},
  archive      = {J_TETC},
  author       = {Aibin Yan and Aoran Cao and Zhengfeng Huang and Jie Cui and Tianming Ni and Patrick Girard and Xiaoqing Wen and Jiliang Zhang},
  doi          = {10.1109/TETC.2023.3317070},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1070-1081},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Two double-node-upset-hardened flip-flop designs for high-performance applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling coflows by online identification in data center
network. <em>TETC</em>, <em>11</em>(4), 1057–1069. (<a
href="https://doi.org/10.1109/TETC.2023.3315512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many scheduling schemes leverage coflows to improve the communication performance of jobs in distributed application frameworks deployed in data center networks, such as MapReduce and Spark. Most of them require application modification to obtain the coflow information such as the coflow ID. The latest work CODA suggests non-intrusively extracting coflow information via an identification method. However, the method depends on the historical traffic information, which may cause the identification accuracy to decrease a lot when traffic varies. To tackle the problem, we present SOCI for Scheduling coflows by the Online Coflow Identification. By observing that flows in a coflow typically communicate with a master process for starting and ending in the up-to-date distributed application frameworks, SOCI uses this characteristic for the online coflow identification. Given identification errors are inevitable, the coflow scheduler in SOCI adopts a Selectively Late Binding (SLB) mechanism, which associates the misclassified flows with coflows according to the estimation on the impact of this association on the average Coflow Completion Time (CCT). The trace-driven simulations show that SOCI can reduce CCT by up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$1.23\times$&lt;/tex-math&gt;&lt;/inline-formula&gt; compared to CODA when the identification accuracy decreases and is comparable to schemes without coflow identification.},
  archive      = {J_TETC},
  author       = {Chang Ruan and Jianxin Wang and Wanchun Jiang and Tao Zhang},
  doi          = {10.1109/TETC.2023.3315512},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1057-1069},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scheduling coflows by online identification in data center network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quadtree-based adaptive spatial decomposition for range
queries under local differential privacy. <em>TETC</em>, <em>11</em>(4),
1045–1056. (<a href="https://doi.org/10.1109/TETC.2023.3317393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, researchers have shown significant interest in geographic location-based spatial data analysis due to its wide range of application scenarios. However, the accuracy of the grid-based quadtree range query (GT-R) algorithm, which utilizes the uniform grid method to divide the data space, is compromised by the excessive noise introduced in the divided area. In addition, the private adaptive grid (PrivAG) algorithm does not adopt any index structure, which leads to inefficient query. To address above issues, this paper presents the Quadtree-based Adaptive Spatial Decomposition (ASDQT) algorithm. ASDQT leverages reservoir sampling technology under local differential privacy (LDP) to extract spatial data as the segmentation object. By setting a reasonable threshold, ASDQT dynamically constructs the tree structure, enabling coarse-grained division of sparse regions and fine-grained division of dense regions. Extensive experiments conducted on two real-world datasets demonstrate the efficacy of ASDQT in handling large-scale spatial datasets with different distributions. The results indicate that ASDQT outperforms existing methods in terms of both accuracy and running efficiency.},
  archive      = {J_TETC},
  author       = {Huiwei Wang and Yaqian Huang and Huaqing Li},
  doi          = {10.1109/TETC.2023.3317393},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1045-1056},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quadtree-based adaptive spatial decomposition for range queries under local differential privacy},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deadline-aware and energy-efficient dynamic task mapping and
scheduling for multicore systems based on wireless network-on-chip.
<em>TETC</em>, <em>11</em>(4), 1031–1044. (<a
href="https://doi.org/10.1109/TETC.2023.3315298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid Wireless Network-on-Chip (HWNoC) architecture has been introduced as a promising communication infrastructure for multicore systems. HWNoC-based multicore systems encounter extremely dynamic application workloads that are submitted at run-time. Mapping and scheduling of these applications are critical for system performance, especially for real-time applications. The existing resource allocation approaches either ignore the use of wireless links in task allocation on cores or ignore the timing characteristic of tasks. In this paper, we propose a new deadline-aware and energy-efficient dynamic task mapping and scheduling approach for the HWNoC-based multicore system. By using of core utilization threshold and tasks laxity time, the proposed approach aims to minimize communication energy consumption and satisfy the deadline of the real-time applications tasks. Through cycle-accurate simulation, the performance of the proposed approach has been compared with state-of-the-art approaches in terms of communication energy consumption, deadline violation rate, communication latency, and runtime overhead. The experimental results confirmed that the proposed approach is a very competitive approach among the alternative approaches.},
  archive      = {J_TETC},
  author       = {Abbas Dehghani and Sadegh Fadaei and Bahman Ravaei and Keyvan RahimiZadeh},
  doi          = {10.1109/TETC.2023.3315298},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1031-1044},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deadline-aware and energy-efficient dynamic task mapping and scheduling for multicore systems based on wireless network-on-chip},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attacks assessment of salient object detection
via symbolic learning. <em>TETC</em>, <em>11</em>(4), 1018–1030. (<a
href="https://doi.org/10.1109/TETC.2023.3316549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers’ attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodology against several adversarial attacks and noise perturbations using standard databases and a real-world problem of a shorebird called the Snowy Plover portraying a visual attention task. We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness. All neural networks suffer significant performance losses, while brain programming stands its ground and remains unaffected. Also, by studying the Snowy Plover, we remark on the importance of security in surveillance activities regarding wildlife protection and conservation.},
  archive      = {J_TETC},
  author       = {Gustavo Olague and Roberto Pineda and Gerardo Ibarra-Vazquez and Matthieu Olague and Axel Martinez and Sambit Bakshi and Jonathan Vargas and Isnardo Reducindo},
  doi          = {10.1109/TETC.2023.3316549},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1018-1030},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Adversarial attacks assessment of salient object detection via symbolic learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Noise-shaping binary-to-stochastic converters for
reduced-length bit-streams. <em>TETC</em>, <em>11</em>(4), 1002–1017.
(<a href="https://doi.org/10.1109/TETC.2023.3299516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic computations have attracted significant attention for applications with moderate fixed-point accuracy requirements, as they offer minimal complexity. In these systems, a stochastic bit-stream encodes a data sample. The derived bit-stream is used for processing. The bit-stream length determines the computation latency for bit-serial implementations and hardware complexity for bit-parallel ones. Noise shaping is a feedback technique that moves the quantization noise outside the bandwidth of interest of a signal. This article proposes a technique that builds on noise shaping and reduces the length of the stochastic bit-stream required to achieve a specific Signal-to-Quantization-Noise Ratio (SQNR). The technique is realized by digital units that encode binary samples into stochastic streams, hereafter called as binary-to-stochastic converters. Furthermore, formulas are derived that relate the bit-stream length reduction to the signal bandwidth. First-order and second-order converters that implement the proposed technique are analyzed. Two architectures are introduced, distinguished by placing a stochastic converter either inside or outside of the noise-shaping loop. The proposed bit-stream length reduction is quantitatively compared to conventional binary-to-stochastic converters for the same signal quality level. Departing from conventional approaches, this article employs bit-stream lengths that are not a power of two, and proposes a modified stochastic-to-binary conversion scheme as a part of the proposed binary-to-stochastic converter. Particularly, SQNR gains of 29.8 dB and 42.1 dB are achieved for the first-order and second-order converters compared to the conventional converters for equal-length bit-streams and low signal bandwidth. The investigated converters are designed and synthesized at a 28-nm FDSOI technology for a range of bit widths.},
  archive      = {J_TETC},
  author       = {Kleanthis Papachatzopoulos and Vassilis Paliouras},
  doi          = {10.1109/TETC.2023.3299516},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {1002-1017},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Noise-shaping binary-to-stochastic converters for reduced-length bit-streams},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An edge-cloud collaboration framework for graph processing
in smart society. <em>TETC</em>, <em>11</em>(4), 985–1001. (<a
href="https://doi.org/10.1109/TETC.2023.3297066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of cloud computing on latency, bandwidth and data confidentiality, edge computing has emerged as a novel location-aware way to provide the capacity-constrained portable terminals with more processing capacity to improve the computing performance and quality of service (QoS) in several typical domains of the human activity in smart society, such as social networks, medical diagnosis, telecommunications, recommendation systems, internal threat detection, transportation, Internet of Things (IoT), etc. These application domains often manage a vast collection of entities with various relationships, which can be naturally represented by the graph data structure. Graph processing is a powerful tool to model and optimize complex problems where graph-based data is involved. In consideration of the relatively insufficient resource provisioning of the edge devices, in this article, for the first time to our knowledge, we propose a reliable edge-cloud collaboration framework that facilitates the graph primitives based on a lightweight interactive graph processing library (GPL), especially for shortest path search (SPS) operations as the demonstrative example. Two types of different practical cases are also presented to show the typical application scenarios of our graph processing strategy. Experimental evaluations indicate that the acceleration rate of performance can reach 6.87x via graph reduction, and less than 3% and 20% extra latency is required for much better user experiences for navigation and pandemic control, respectively, while the online security measures merely consume about 1% extra time of the overall data transmission. Our framework can efficiently execute the applications with considering of user-friendliness, low-latency response, interactions among edge devices, collaboration between edge and cloud, and privacy protection at an acceptable overhead.},
  archive      = {J_TETC},
  author       = {Jun Zhou and Masaaki Kondo},
  doi          = {10.1109/TETC.2023.3297066},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {985-1001},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An edge-cloud collaboration framework for graph processing in smart society},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New construction of balanced codes based on weights of data
for DNA storage. <em>TETC</em>, <em>11</em>(4), 973–984. (<a
href="https://doi.org/10.1109/TETC.2023.3293477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As maintaining a proper balanced GC content is crucial for minimizing errors in DNA storage, constructing GC-balanced DNA codes has become an important research topic. In this article, we propose a novel code construction method based on the weight distribution of the data, which enables us to construct GC-balanced DNA codes. Additionally, we introduce a specific encoding process for both balanced and imbalanced data parts. One of the key differences between the proposed codes and existing codes is that the parity lengths of the proposed codes are variable depending on the data parts, while the parity lengths of existing codes remain fixed. To evaluate the effectiveness of the proposed codes, we compare their average parity lengths to those of existing codes. Our results demonstrate that the proposed codes have significantly shorter average parity lengths for DNA sequences with appropriate GC contents.},
  archive      = {J_TETC},
  author       = {Xiaozhou Lu and Sunghwan Kim},
  doi          = {10.1109/TETC.2023.3293477},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {973-984},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {New construction of balanced codes based on weights of data for DNA storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PISA: A non-volatile processing-in-sensor accelerator for
imaging systems. <em>TETC</em>, <em>11</em>(4), 962–972. (<a
href="https://doi.org/10.1109/TETC.2023.3292251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a Processing-In-Sensor Accelerator, namely PISA, as a flexible, energy-efficient, and high-performance solution for real-time and smart image processing in AI devices. PISA intrinsically implements a coarse-grained convolution operation in Binarized-Weight Neural Networks (BWNNs) leveraging a novel compute-pixel with non-volatile weight storage at the sensor side. This remarkably reduces the power consumption of data conversion and transmission to an off-chip processor. The design is completed with a bit-wise near-sensor in-memory computing unit to process the remaining network layers. Once the object is detected, PISA switches to typical sensing mode to capture the image for a fine-grained convolution using only a near-sensor processing unit. Our circuit-to-application co-simulation results on a BWNN acceleration demonstrate minor accuracy degradation on various image datasets in coarse-grained evaluation compared to baseline BWNN models, while PISA achieves a frame rate of 1000 and efficiency of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\sim$&lt;/tex-math&gt;&lt;/inline-formula&gt; 1.74 TOp/s/W. Lastly, PISA substantially reduces data conversion and transmission energy by &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\sim$&lt;/tex-math&gt;&lt;/inline-formula&gt; 84% compared to a baseline.},
  archive      = {J_TETC},
  author       = {Shaahin Angizi and Sepehr Tabrizchi and David Z. Pan and Arman Roohi},
  doi          = {10.1109/TETC.2023.3292251},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {962-972},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PISA: A non-volatile processing-in-sensor accelerator for imaging systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Resource allocation optimization by quantum computing for
shared use of standalone IRS. <em>TETC</em>, <em>11</em>(4), 950–961.
(<a href="https://doi.org/10.1109/TETC.2023.3292355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent reflecting surfaces (IRSs) have attracted attention as a technology that can considerably improve the energy utilization efficiency of sixth-generation (6G) mobile communication systems. IRSs enable control of propagation characteristics by adjusting the phase shift of each reflective element. However, designing the phase shift requires the acquisition of channel information for each reflective element, which is impractical from an overhead perspective. In addition, for multiple wireless network operators to share an IRS for communication, new infrastructure facilities and operational costs are required at each operator&#39;s end to control the IRS in a coordinated manner. Herein, we propose a wireless communication system using standalone IRSs to solve these problems. The standalone IRSs cover a wide area by periodically switching phase shifts, and each operator allocates radio resources according to their phase-shift switching. Furthermore, we derive a quadratic unconstrained binary optimization equation for the proposed system to optimize radio resource allocation using quantum computing. The results of computer simulations indicate that the proposed system and method can be used to achieve efficient communication in 6G mobile communication systems.},
  archive      = {J_TETC},
  author       = {Takahiro Ohyama and Yuichi Kawamoto and Nei Kato},
  doi          = {10.1109/TETC.2023.3292355},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {950-961},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Resource allocation optimization by quantum computing for shared use of standalone IRS},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FINISH: Efficient and scalable NMF-based federated learning
for detecting malware activities. <em>TETC</em>, <em>11</em>(4),
934–949. (<a href="https://doi.org/10.1109/TETC.2023.3292924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {5G networks with the vast number of devices pose security threats. Manual analysis of such extensive security data is complex. Dark-NMF can detect malware activities by monitoring unused IP address space, i.e., the darknet. However, the challenges of cooperative training for Dark-NMF are immense computational complexity with Big Data, communication overhead, and privacy concern with darknet sensor IP addresses. Darknet sensors can observe multivariate time series of packets from the same hosts, represented as intersecting columns in different data matrices. Previous works do not consider intersecting columns, losing a host&#39;s semantics because they do not aggregate the host&#39;s time series. To solve these problems, we proposed a federated IoT malware detection NMF for intersecting source hosts (FINISH) algorithm for offloading computing tasks to 5G multiaccess edge computing (MEC). The experiments show that FINISH is scalable to a data size with a shorter computational time and has a lower false positive detection performance than Dark-NMF. The comparison results demonstrate that FINISH has better computation and communication efficiency than related works and a short communication time, taking only 1/10 the execution time in a simulated 5G MEC. The experimental results can provide substantial insights into developing federated cybersecurity in the future.},
  archive      = {J_TETC},
  author       = {Yu-Wei Chang and Hong-Yen Chen and Chansu Han and Tomohiro Morikawa and Takeshi Takahashi and Tsung-Nan Lin},
  doi          = {10.1109/TETC.2023.3292924},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {934-949},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FINISH: Efficient and scalable NMF-based federated learning for detecting malware activities},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving authentication protocols for IoT devices
using the SiRF PUF. <em>TETC</em>, <em>11</em>(4), 918–933. (<a
href="https://doi.org/10.1109/TETC.2023.3296016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication between IoT devices is important for maintaining security, trust and data integrity in an edge device ecosystem. The low-power, reduced computing capacity of the IoT device makes public-private, certificate-based forms of authentication impractical, while other lighter-weight, symmetric cryptography-based approaches, such as message authentication codes, are easy to spoof in unsupervised environments where adversaries have direct physical access to the device. Such environments are better served by security primitives rooted in the hardware with capabilities exceeding those available in cryptography-only frameworks. A key foundational hardware security primitive is the physical unclonable function or PUF. PUFs are well known for removing the need to store secrets in secure non-volatile memories, and for providing very large sets of authentication credentials. In this article, we describe two PUF-based mutual authentication protocols rooted in the entropy provided by a strong PUF. The security properties of the authentication protocols, called COBRA and PARCE, are evaluated in hardware experiments on SoC-based FPGAs, and under extended industrial-standard operating conditions. A codesign-based system architecture is presented in which the SiRF PUF and core authentication functions are implemented in the programmable logic as a secure enclave, while network and database operations are implemented in software on an embedded microprocessor.},
  archive      = {J_TETC},
  author       = {Jim Plusquellic and Eirini Eleni Tsiropoulou and Cyrus Minwalla},
  doi          = {10.1109/TETC.2023.3296016},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {918-933},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Privacy-preserving authentication protocols for IoT devices using the SiRF PUF},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph-incorporated latent factor analysis model for
high-dimensional and sparse data. <em>TETC</em>, <em>11</em>(4),
907–917. (<a href="https://doi.org/10.1109/TETC.2023.3292866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A High-dimensional and s parse (HiDS) matrix is frequently encountered in Big Data-related applications such as e-commerce systems or wireless sensor networks. It is of great significance to perform highly accurate representation learning on an HiDS matrix due to the great desires of extracting latent knowledge from it. L atent f actor a nalysis (LFA), which represents an HiDS matrix by learning the low-rank embeddings based on its observed entries only, is one of the most effective and efficient approaches to this issue. However, most existing LFA-based models directly perform such embeddings on an HiDS matrix without exploiting its hidden graph structures, resulting in accuracy loss. To aid this issue, this paper proposes a g raph-incorporated l atent f actor a nalysis (GLFA) model. It adopts two-fold ideas: 1) a graph is constructed for identifying the hidden h igh- o rder i nteraction (HOI) among nodes described by an HiDS matrix, and 2) a recurrent LFA structure is carefully designed with the incorporation of HOI, thereby improving the representation learning ability of a resultant model. Experimental results on three real-world datasets demonstrate that GLFA outperforms six state-of-the-art models in predicting the missing data of an HiDS matrix, which evidently supports its strong representation learning ability to HiDS data.},
  archive      = {J_TETC},
  author       = {Di Wu and Yi He and Xin Luo},
  doi          = {10.1109/TETC.2023.3292866},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {907-917},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A graph-incorporated latent factor analysis model for high-dimensional and sparse data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rei: A reconfigurable interconnection unit for array-based
CNN accelerators. <em>TETC</em>, <em>11</em>(4), 895–906. (<a
href="https://doi.org/10.1109/TETC.2023.3290138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network (CNN) is used in many real-world applications due to its high accuracy. The rapid growth of modern applications based on learning algorithms has increased the importance of efficient implementation of CNNs. The array-type architecture is a well-known platform for the efficient implementation of CNN models, which takes advantage of parallel computation and data reuse. However, accelerators suffer from restricted hardware resources, whereas CNNs involve considerable communication and computation load. Furthermore, since accelerators execute CNN layer by layer, different shapes and sizes of layers lead to suboptimal resource utilization. This problem prevents the accelerator from reaching maximum performance. The increasing scale and complexity of deep learning applications exacerbate this problem. Therefore, the performance of CNN models depends on the hardware&#39;s ability to adapt to different shapes of different layers to increase resource utilization. This work proposes a reconfigurable accelerator that can efficiently execute a wide range of CNNs. The proposed flexible and low-cost reconfigurable interconnect units allow the array to perform CNN faster than fixed-size implementations (by 45.9% for ResNet-18 compared to the baseline). The proposed architecture also reduces the on-chip memory access rate by 36.5% without compromising accuracy.},
  archive      = {J_TETC},
  author       = {Paria Darbani and Hakem Beitollahi and Pejman Lotfi-Kamran},
  doi          = {10.1109/TETC.2023.3290138},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {895-906},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Rei: A reconfigurable interconnection unit for array-based CNN accelerators},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CANNON: Communication-aware sparse neural network
optimization. <em>TETC</em>, <em>11</em>(4), 882–894. (<a
href="https://doi.org/10.1109/TETC.2023.3289778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse deep neural networks (DNNs) have the potential to deliver compelling performance and energy efficiency without significant accuracy loss. However, their benefits can quickly diminish if their training is oblivious to the target hardware. For example, fewer critical connections can have a significant overhead if they translate into long-distance communication on the target hardware. Therefore, hardware-aware sparse training is needed to leverage the full potential of sparse DNNs. To this end, we propose a novel and comprehensive communication-aware sparse DNN optimization framework for tile-based in-memory computing (IMC) architectures. The proposed technique, CANNON first maps the DNN layers onto the tiles of the target architecture. Then, it replaces the fully connected and convolutional layers with communication-aware sparse connections. After that, CANNON optimizes the communication cost with minimal impact on the DNN accuracy. Extensive experimental evaluations with a wide range of DNNs and datasets show up to 3.0× lower communication energy, 3.1× lower communication latency, and 6.8× lower energy-delay product compared to state-of-the-art pruning approaches with a negligible impact on the classification accuracy on IMC-based machine learning accelerators.},
  archive      = {J_TETC},
  author       = {A. Alper Goksoy and Guihong Li and Sumit K. Mandal and Umit Y. Ogras and Radu Marculescu},
  doi          = {10.1109/TETC.2023.3289778},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {882-894},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CANNON: Communication-aware sparse neural network optimization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Construction of a spike-based memory using neural-like logic
gates based on spiking neural networks on SpiNNaker. <em>TETC</em>,
<em>11</em>(4), 868–881. (<a
href="https://doi.org/10.1109/TETC.2023.3281063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic engineering concentrates the efforts of a large number of researchers due to its great potential as a field of research, in a search for the exploitation of the advantages of the biological nervous system and the brain as a whole for the design of more efficient and real-time capable applications. For the development of applications as close to biology as possible, Spiking Neural Networks (SNNs) are used, which are considered biologically-plausible and constitute the third generation of Artificial Neural Networks. This work presents a spiking implementation of a memory, which is one of the most important components in computer architecture. In the process of designing this spiking memory, different intermediate components were also implemented and tested. The tests were carried out on the SpiNNaker neuromorphic platform. This work goes into the development of spiking blocks using a logic gate approach based on previous work and includes a comparison between other works in the state of the art related to spiking memories and the one proposed here. All the implemented blocks and developed tests are available in a public repository.},
  archive      = {J_TETC},
  author       = {Alvaro Ayuso-Martinez and Daniel Casanueva-Morato and J. P. Dominguez-Morales and Angel Jimenez-Fernandez and Gabriel Jimenez-Moreno},
  doi          = {10.1109/TETC.2023.3281063},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {868-881},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Construction of a spike-based memory using neural-like logic gates based on spiking neural networks on SpiNNaker},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-efficient generic accuracy configurable multiplier
based on block-level voltage overscaling. <em>TETC</em>, <em>11</em>(4),
851–867. (<a href="https://doi.org/10.1109/TETC.2023.3279419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voltage Overscaling (VOS) is one of the well-known techniques to increase the energy efficiency of arithmetic units. Also, it can provide significant lifetime improvements, while still meeting the accuracy requirements of inherently error-resilient applications. This paper proposes a generic accuracy-configurable multiplier that employs the VOS at a coarse-grained level (block-level) to reduce the control logic required for applying VOS and its associated overheads, thus enabling a high degree of trade-off between energy consumption and output quality. The proposed configurable Block-Level VOS-based (BL-VOS) multiplier relies on employing VOS in a multiplier composed of smaller blocks, where applying VOS in different blocks results in structures with various output accuracy levels. To evaluate the proposed concept, we implement 8-bit and 16-bit BL-VOS multipliers with various blocks width in a 15-nm FinFET technology. The results show that the proposed multiplier achieves up to 15% lower energy consumption and up to 21% higher output accuracy compared to the state-of-the-art VOS-based multipliers. Also, the effects of Process Variation (PV) and Bias Temperature Instability (BTI) induced delay on the proposed multiplier are investigated. Finally, the effectiveness of the proposed multiplier is studied for two different image processing applications, in terms of quality and energy efficiency.},
  archive      = {J_TETC},
  author       = {Ali Akbar Bahoo and Omid Akbari and Muhammad Shafique},
  doi          = {10.1109/TETC.2023.3279419},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {851-867},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An energy-efficient generic accuracy configurable multiplier based on block-level voltage overscaling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low-cost wireless body area network for human activity
recognition in healthy life and medical applications. <em>TETC</em>,
<em>11</em>(4), 839–850. (<a
href="https://doi.org/10.1109/TETC.2023.3274189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moved by the necessity, also related to the ongoing COVID-19 pandemic, of the design of innovative solutions in the context of digital health, and digital medicine, Wireless Body Area Networks (WBANs) are more and more emerging as a central system for the implementation of solutions for well-being and healthcare. In fact, by elaborating the data collected by a WBAN, advanced classification models can accurately extract health-related parameters, thus allowing, as examples, the implementations of applications for fitness tracking, monitoring of vital signs, diagnosis, and analysis of the evolution of diseases, and, in general, monitoring of human activities and behaviours. Unfortunately, commercially available WBANs present some technological and economic drawbacks from the point of view, respectively, of data fusion and labelling, and cost of the adopted devices. To overcome existing issues, in this article, we present the architecture of a low-cost WBAN, which is built upon accessible off-the-shelf wearable devices and an Android application. Then, we report its technical evaluation concerning resource consumption. Finally, we demonstrate its versatility and accuracy in both medical and well-being application scenarios.},
  archive      = {J_TETC},
  author       = {Florenc Demrozi and Cristian Turetta and Philipp H. Kindt and Fabio Chiarani and Ruggero Angelo Bacchin and Nicola Valè and Francesco Pascucci and Paola Cesari and Nicola Smania and Stefano Tamburin and Graziano Pravadelli},
  doi          = {10.1109/TETC.2023.3274189},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {839-850},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A low-cost wireless body area network for human activity recognition in healthy life and medical applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithm-hardware co-design of split-radix discrete galois
transformation for KyberKEM. <em>TETC</em>, <em>11</em>(4), 824–838. (<a
href="https://doi.org/10.1109/TETC.2023.3270971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {KyberKEM is one of the final round key encapsulation mechanisms in the NIST post-quantum cryptography competition. Number theoretic transform (NTT), as the computing bottleneck of KyberKEM, has been widely studied. Discrete Galois Transformation (DGT) is a variant of NTT that reduces transform length into half but requires more multiplication operations than the latest NTT algorithm in theoretical analysis. This paper proposes the split-radix DGT, a novel DGT variant utilizing the split-radix method, to reduce the computing complexity without compromising the transform length. Specifically, for length-128 polynomial, the split-radix DGT algorithm saves at least 10% multiplication operations compared with the latest NTT algorithm in theoretical analysis. Furthermore, we proposed a unified split-radix DGT processor with the dedicated stream permutation network for KyberKEM and implemented it on the Xilinx Artix-7 FPGA. The processor achieves at least 49.4% faster transformation and 65.3% faster component-wise multiplication, with at most 87% and 32% LUT-NTT area-time product and LUT-CWM area-time product, compared with the state-of-the-art polynomial multipliers in KyberKEM with the same BFU setting on similar platforms. Lastly, we designed a highly efficient KyberKEM architecture using the proposed split-radix DGT processor. The implementation results on Artix-7 FPGA show significant performance improvements over the state-of-the-art KyberKEM designs.},
  archive      = {J_TETC},
  author       = {Guangyan Li and Donglong Chen and Gaoyu Mao and Wangchen Dai and Abdurrashid Ibrahim Sanka and Ray C.C. Cheung},
  doi          = {10.1109/TETC.2023.3270971},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10},
  number       = {4},
  pages        = {824-838},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Algorithm-hardware co-design of split-radix discrete galois transformation for KyberKEM},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimized hardware implementation of modular
multiplication of binary ring LWE. <em>TETC</em>, <em>11</em>(3),
817–821. (<a href="https://doi.org/10.1109/TETC.2023.3280470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing end-to-end security is vital for most networks. Emerging quantum computers make it necessary to design secure crypto-systems against quantum attacks. Binary Ring Learning With Error (Ring-Bin LWE) is a Lattice-based cryptography that is hard to solve by quantum computers. Also, this algorithm does not have costly operations in terms of area, making Ring-Bin LWE a suitable algorithm for resource-constraint devices. This work presents a lightweight hardware implementation of Ring-Bin LWE. In the proposed design, a new multiplication method and design for Ring-Bin LWE is introduced which results in latency reduction by a factor of two. Using column-based multiplication, our design processes two consecutive coefficients in each cycle. The architecture is designed based on the proposed multiplication and contains one specific register bank with two sub-bank registers. The design is implemented on the FPGA platforms. The implementation results show an impressive improvement in execution time and Area-Time metrics over previous similar works.},
  archive      = {J_TETC},
  author       = {Karim Shahbazi and Seok-Bum Ko},
  doi          = {10.1109/TETC.2023.3280470},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {817-821},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An optimized hardware implementation of modular multiplication of binary ring LWE},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low latency approximate adder design based on dual
sub-adders with error recovery. <em>TETC</em>, <em>11</em>(3), 811–816.
(<a href="https://doi.org/10.1109/TETC.2023.3270963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel dual sub-adder based approximate adder that splits a precise adder into two to significantly reduce the latency. The proposed error recovery and reduction technique effectively compensates for the catastrophic accuracy degradation incurred by the split. Implemented in a 65- &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$nm$&lt;/tex-math&gt;&lt;/inline-formula&gt; CMOS technology, our design reduces the delay and energy consumption by up to 68% and 78%, respectively, compared to a traditional adder, and outperforms other existing approximate adders in hardware and accuracy joint metrics. The proposed design&#39;s efficacy is shown through digital image processing applications where our design makes processed output images closest to the one using the accurate adder over other approximate adders. Furthermore, the proposed design is proven to be a general adder model that can be employed in existing approximate adders to enhance hardware resource and error characteristics considerably.},
  archive      = {J_TETC},
  author       = {Hyoju Seo and Yongtae Kim},
  doi          = {10.1109/TETC.2023.3270963},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {811-816},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A low latency approximate adder design based on dual sub-adders with error recovery},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DT2CAM: A decision tree to content addressable memory
framework. <em>TETC</em>, <em>11</em>(3), 805–810. (<a
href="https://doi.org/10.1109/TETC.2023.3261748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are powerful tools for data classification. Accelerating the decision tree search is crucial for on-the-edge applications with limited power and latency budget. In this article, we propose a content-addressable memory compiler for decision tree inference acceleration. We propose a novel ”adaptive-precision” scheme that results in a compact implementation and enables an efficient bijective mapping to ternary content addressable memories while maintaining high inference accuracies. We also develop a resistive-based functional synthesizer to map the decision tree to resistive content addressable memory arrays and perform functional simulations for energy, latency, and accuracy evaluations. We study the decision tree accuracy under hardware non-idealities including device defects, manufacturing variability, and input encoding noise. We test our framework on various decision tree datasets including Give Me Some Credit , Titanic , and COVID-19 . Our results reveal up to 42.4% energy savings and up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$17.8\times$&lt;/tex-math&gt;&lt;/inline-formula&gt; better energy-delay-area product compared to the state-of-art hardware accelerators, and up to 333 million decisions per sec for the pipelined implementation.},
  archive      = {J_TETC},
  author       = {Mariam Rakka and Mohammed E. Fouda and Rouwaida Kanj and Fadi Kurdahi},
  doi          = {10.1109/TETC.2023.3261748},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {805-810},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DT2CAM: A decision tree to content addressable memory framework},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bitwise signature comparison: Enabling more efficient
similarity estimation. <em>TETC</em>, <em>11</em>(3), 798–804. (<a
href="https://doi.org/10.1109/TETC.2022.3221872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the similarity of sets of data is a common operation in computing. Minhash is widely used to estimate similarity by computing a signature for each set and then comparing their signatures. Therefore, signature comparison is an important part of similarity estimation. To make the comparison efficient, the size of the signature components is commonly set to the word size of the processor or to one half or one fourth of it. This enables efficient data manipulation and comparison but is not optimal in terms of storage. For example, 48-bit signatures may be more than enough in many applications but since that size cannot be easily manipulated by most processors, 64-bit signatures are used. This implies a 33.3% memory overhead. In this paper, Bitwise Signature Comparison (BSC), a method that enables the efficient comparison of signature components of any bitwidth is presented and evaluated. The results show that BSC achieves a similar speed to that of the traditional comparison implementation regardless of the size of the signature components. This enables the use of any signature component size enabling better trade-offs in the implementation of similarity estimation sketches.},
  archive      = {J_TETC},
  author       = {Pedro Reviriego and Salvatore Pontarelli and Jorge Martínez},
  doi          = {10.1109/TETC.2022.3221872},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {798-804},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Bitwise signature comparison: Enabling more efficient similarity estimation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Error detection schemes assessed on FPGA for multipliers in
lattice-based key encapsulation mechanisms in post-quantum cryptography.
<em>TETC</em>, <em>11</em>(3), 791–797. (<a
href="https://doi.org/10.1109/TETC.2022.3217006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in quantum computing have brought the need for developing public-key cryptosystems secure against attacks potentially enabled by quantum computers. In late 2017, the National Institute of Standards and Technology (NIST) launched a project to standardize one or more quantum computer-resistant public-key cryptographic algorithms. Among the main post-quantum algorithm classes, lattice-based cryptography is believed to be quantum-resistant. The standardization efforts including that of the NIST which will be concluded in 2022-2024 also affirm the importance of such algorithms. In this work, we propose error detection schemes for lattice-based key encapsulation mechanisms (KEMs). As our case study, we apply such schemes to the hardware accelerators for three post-quantum cryptographic algorithms that have advanced to the third round of the NIST PQC standardization process, i.e., FrodoKEM, Saber, and NTRU. The merit of the proposed schemes is that they can be applied to other applications and cryptographic algorithms that use multiplications in their hardware accelerators. The schemes proposed in this paper are based on recomputing with shifted, negated, and scaled operands. Moreover, we implement our fault detection schemes on field-programmable gate array (FPGA) family Kintex Ultrascale+ device xcku5p-sfvb784-1LV-i to benchmark the overheads induced and the performance degradation of the proposed approaches when added to the original architectures. The results show acceptable overhead and high error coverage for all three studied NIST PQC finalists.},
  archive      = {J_TETC},
  author       = {Alvaro Cintas Canto and Ausmita Sarker and Jasmin Kaur and Mehran Mozaffari Kermani and Reza Azarderakhsh},
  doi          = {10.1109/TETC.2022.3217006},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {791-797},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Error detection schemes assessed on FPGA for multipliers in lattice-based key encapsulation mechanisms in post-quantum cryptography},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation efficiency maximization in multi-UAV-enabled
mobile edge computing systems based on 3D deployment optimization.
<em>TETC</em>, <em>11</em>(3), 778–790. (<a
href="https://doi.org/10.1109/TETC.2023.3268346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been widely devoted to mobile edge computing (MEC) systems that have limited resources to provide high-quality computing and communication services for Internet of Things (IoT) terminals. Energy-efficient computation and resource allocation are key issues for the sustainable operation of the above-mentioned systems. The 3D deployment optimization of multi-UAVs is also crucial to maximizing the system&#39;s computation efficiency. In this paper, we discuss a multi-UAV-enabled MEC system. To maximize the computation efficiency of the terminal system, we consider jointly optimizing the terminal&#39;s CPU frequency, transmission power, offloading correlation decision, and the 3D position and beamwidth of the UAV. Since the original problem is a mixed-integer nonlinear programming (MINLP) problem with a fractional structure, which is difficult to solve directly. Based on Dinkelbach&#39;s method, convex optimization theory, and greedy strategy, we simplify the mathematical model and propose a four-stage alternating iterative computation efficiency maximization algorithm(FICEM) to solve the problem. The simulation results indicate that the algorithm converges fast in various network scenarios and that its computation efficiency is better than that of the baseline algorithm. In addition, the simulation results also manifest the effect of different network parameters on the computation efficiency of the terminal system.},
  archive      = {J_TETC},
  author       = {Xiaoheng Deng and Jiahao Zhao and Zhufang Kuang and Xuechen Chen and Qi Guo and Fengxiao Tang},
  doi          = {10.1109/TETC.2023.3268346},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {778-790},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Computation efficiency maximization in multi-UAV-enabled mobile edge computing systems based on 3D deployment optimization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BIC codes: Bit insertion-based constrained codes with error
correction for DNA storage. <em>TETC</em>, <em>11</em>(3), 764–777. (<a
href="https://doi.org/10.1109/TETC.2023.3268274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new coding algorithm for DNA storage over both error-free and error channels. For the error-free case, we propose a constrained code called bit insertion-based constrained (BIC) code. BIC codes convert a binary data sequence to multiple oligo sequences satisfying the maximum homopolymer run (i.e., run-length (RL)) constraint by inserting dummy bits. We show that the BIC codes nearly achieves the capacity in terms of information density while the simple structure of the BIC codes allows linear-time encoding and fast parallel decoding. Also, by combining a balancing technique with the BIC codes, we obtain the constrained coding algorithm to satisfy the GC-content constraint as well as the RL constraint. Next, for DNA storage channel with errors, we integrate the proposed constrained coding algorithm with a rate-compatible low-density parity-check (LDPC) code to correct errors and erasures. Specifically, we incorporate LDPC codes adopted in the 5 G new radio standard because they have powerful error-correction capability and appealing features for the integration. Simulation results show that the proposed integrated coding algorithm outperforms existing coding algorithms in terms of information density and error correctability.},
  archive      = {J_TETC},
  author       = {Seong-Joon Park and Hosung Park and Hee-Youl Kwak and Jong-Seon No},
  doi          = {10.1109/TETC.2023.3268274},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {764-777},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {BIC codes: Bit insertion-based constrained codes with error correction for DNA storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitask particle swarm optimization with dynamic
transformation. <em>TETC</em>, <em>11</em>(3), 749–763. (<a
href="https://doi.org/10.1109/TETC.2023.3268182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask optimization (MTO) mainly utilizes knowledge transfer among tasks to address multiple optimization problems in parallel. However, the decision space dimensions of different tasks often differ, which leads to the failure of knowledge transfer. Therefore, it is a challenging problem to transfer knowledge among tasks with different dimensions to achieve parallel optimization of multiple tasks. To address this problem, multitask particle swarm optimization with a dynamic transformation strategy (MTPSO-DTS) is proposed to improve the performance of MTO. First, an intertask similarity index based on gradient information and location information is designed to dynamically assess the degree of similarity among different tasks. Then, the intertask similarity is determined to assist the transformation of dimensions. Second, a dynamic transformation strategy based on intertask similarity is developed to achieve a uniform representation of different dimension knowledge, including dimension supplementation and dimension reduction. Then, the MTPSO-DTS algorithm, which takes advantage of the search characteristics of particle swarm optimization, can facilitate knowledge transfer among tasks with different dimensions. Third, the convergence of the MTPSO-DTS algorithm is analyzed to verify the validity theoretically. Finally, numerical comparison experiments on benchmark problems and a practical application are carried out to demonstrate the effectiveness of the MTPSO-DTS algorithm. The results indicate that the proposed MTPSO-DTS algorithm can facilitate knowledge transfer among tasks of different dimensions to promote parallel optimization of multiple tasks.},
  archive      = {J_TETC},
  author       = {Honggui Han and Xing Bai and Hongyan Yang and Ying Hou and Junfei Qiao},
  doi          = {10.1109/TETC.2023.3268182},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {749-763},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Multitask particle swarm optimization with dynamic transformation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible and reliable RRAM-based in-memory computing
architecture for data-intensive applications. <em>TETC</em>,
<em>11</em>(3), 736–748. (<a
href="https://doi.org/10.1109/TETC.2023.3268079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a practical, flexible, and reliable in-memory computing architecture for resistive-memory-based logic designs. Our design uses a new RRAM-based polymorphic in-memory logic gate implementing all 2-input Boolean logic functions to handle real-time applications like search engines. This design reduces the proposed architecture&#39;s power-delay product (PDP) compared to competing designs by utilizing the features of the RRAM device and the proposed reconfigurable sensing amplifier. Additionally, the simulation results on the ISCAS-89 and ITC-99 benchmarks at the 7nm technology node show that the suggested architecture&#39;s PDP is lower than the comparable state-of-the-art designs. A novel sense amplifier supporting all Boolean logic functions is also proposed to handle the applications with resistive-resistive-based inputs, such as search applications. After retrieving data from the main memory, the suggested sense amplifier computes the desired output. To assess the functionality of the suggested design in the presence of process variations, Monte Carlo simulations are conducted to authenticate the robustness and high-performance operation of the proposed design in different classes of resistive-memory-based logic in the presence of process variations. Our results further demonstrate that, compared to its counterpart, the suggested strategy dramatically reduces the energy consumption of the median, Max, and Min filters in real-world applications.},
  archive      = {J_TETC},
  author       = {Nima Eslami and Mohammad Hossein Moaiyeri},
  doi          = {10.1109/TETC.2023.3268079},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {736-748},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A flexible and reliable RRAM-based in-memory computing architecture for data-intensive applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AritPIM: High-throughput in-memory arithmetic.
<em>TETC</em>, <em>11</em>(3), 720–735. (<a
href="https://doi.org/10.1109/TETC.2023.3268137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital processing-in-memory (PIM) architectures are rapidly emerging to overcome the memory-wall bottleneck by integrating logic within memory elements. Such architectures provide vast computational power within the memory itself in the form of parallel bitwise logic operations. We develop novel algorithmic techniques for PIM that, combined with new perspectives on computer arithmetic, extend this bitwise parallelism to the four fundamental arithmetic operations (addition, subtraction, multiplication, and division), for both fixed-point and floating-point numbers, and using both bit-serial and bit-parallel approaches. We propose a state-of-the-art suite of arithmetic algorithms, demonstrating the first algorithm in the literature of digital PIM for a majority of cases – including cases previously considered impossible for digital PIM, such as floating-point addition. Through a case study on memristive PIM, we compare the proposed algorithms to an NVIDIA RTX 3070 GPU and demonstrate significant throughput and energy improvements.},
  archive      = {J_TETC},
  author       = {Orian Leitersdorf and Dean Leitersdorf and Jonathan Gal and Mor Dahan and Ronny Ronen and Shahar Kvatinsky},
  doi          = {10.1109/TETC.2023.3268137},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {720-735},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AritPIM: High-throughput in-memory arithmetic},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource-aware knowledge distillation for federated
learning. <em>TETC</em>, <em>11</em>(3), 706–719. (<a
href="https://doi.org/10.1109/TETC.2023.3252600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of deep learning and the Internet of Things (IoT) has driven a number of smart-world applications, which are mostly deployed in distributed environments. Federated learning, a privacy-preserving collaborative learning paradigm, has shown considerable potential to leverage the rich distributed data at network edges. Nonetheless, the heterogeneity of IoT devices and their connected network environment impedes federated learning applications in IoT systems. Particularly, the stale gradients updated by slower local learners impact the effectiveness of federated learning. Transmitting weight updates with a large number of users leads to network congestion at the edge of the network and incurs unaffordable communication costs. To overcome these challenges, we propose a transfer knowledge based federated learning framework under a resource-limited distributed system. We formulate a knowledge distillation based federated learning optimization problem with the consideration of dynamic local resource. The proposed approach carries out federated learning with the help of knowledge distillation to avoid occupying the expensive network bandwidth or bringing a heavy burden to the network. Theoretical analysis demonstrates convergence of the learning process. The experimental results on three public datasets illustrate that the proposed framework is capable of substantially improving the efficiency of federated learning and outperforming state-of-the-art schemes.},
  archive      = {J_TETC},
  author       = {Zheyi Chen and Pu Tian and Weixian Liao and Xuhui Chen and Guobin Xu and Wei Yu},
  doi          = {10.1109/TETC.2023.3252600},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {706-719},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Resource-aware knowledge distillation for federated learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Path-based delay variation models for parallel-prefix
adders. <em>TETC</em>, <em>11</em>(3), 689–705. (<a
href="https://doi.org/10.1109/TETC.2023.3242555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art static timing analysis algorithms can evaluate worst-case delay in statistical terms. In this paper, a modeling framework is introduced for the evaluation of the maximum-delay Cumulative Density Function (CDF) of an ensemble of parallel-prefix adder topologies. For moderate variations and close-to-nominal supply voltages, the maximum delay of parallel-prefix adders is practically determined by the maximum of a set of near-critical-delay paths around the nominal maximum-delay path. These paths end to the most significant and neighboring bit positions. Matrix-based path delay formulations are derived for the particular set of paths. The introduced matrix formulations are exploited to assess the maximum-delay CDF by means of a multivariate Gaussian CDF. To validate the accuracy of the introduced models, a quantitative comparison of the proposed probabilistic delay models against Spice-level Monte-Carlo simulations is offered for certain parallel-prefix adders. Threshold-voltage variations summarize several process-dependent variation-inducing mechanisms and are modeled as Gaussian variations, introduced to BSIM-4 transistor models for a 16-nm technology node. For the nominal voltage case and 10% threshold-voltage variations, the introduced models estimate the 0.95 timing yield point with a mean absolute error below 1% compared to Spice-level simulations for the 16 bit-length case. Furthermore, an extension of the proposed approach to account for multiple end points is investigated that reduces the error for the estimation of maximum delay, demonstrated for a unit delay model and certain bit-lengths of Kogge-Stone adder.},
  archive      = {J_TETC},
  author       = {Kleanthis Papachatzopoulos and Vassilis Paliouras},
  doi          = {10.1109/TETC.2023.3242555},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {689-705},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Path-based delay variation models for parallel-prefix adders},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Numerical model for 32-bit magnonic ripple carry adder.
<em>TETC</em>, <em>11</em>(3), 679–688. (<a
href="https://doi.org/10.1109/TETC.2023.3238581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In CMOS-based electronics, the most straightforward way to implement a summation operation is to use the ripple carry adder (RCA). Magnonics, the field of science concerned with data processing by spin waves and their quanta magnons, recently proposed a magnonic half-adder that can be considered as the simplest magnonic integrated circuit. Here, we develop a computation model for the magnonic basic blocks to enable the design and simulation of magnonic gates and magnonic circuits of arbitrary complexity and demonstrate its functionality on the example of a 32-bit integrated RCA. It is shown that the RCA requires the utilization of additional regenerators based on magnonic directional couplers with embedded amplifiers to normalize the magnon signals in-between the half-adders. The benchmarking of large-scale magnonic integrated circuits is performed. The energy consumption of 30 nm-based magnonic 32-bit adder can be as low as 961 aJ per operation with taking into account all required amplifiers.},
  archive      = {J_TETC},
  author       = {Umberto Garlando and Qi Wang and Oleksandr V. Dobrovolskiy and Andrii V. Chumak and Fabrizio Riente},
  doi          = {10.1109/TETC.2023.3238581},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {679-688},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Numerical model for 32-bit magnonic ripple carry adder},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdvParams: An active DNN intellectual property protection
technique via adversarial perturbation based parameter encryption.
<em>TETC</em>, <em>11</em>(3), 664–678. (<a
href="https://doi.org/10.1109/TETC.2022.3231012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of Deep Neural Network (DNN) models requires high cost, thus a well-trained DNN model can be considered as intellectual property (IP) of the model owner. To date, many DNN IP protection methods have been proposed, but most of them are watermarking based verification methods where model owners can only verify their ownership passively after the copyright of DNN models has been infringed. In this article, we propose an effective framework to actively protect the DNN IP from infringement. Specifically, we encrypt a small number of model&#39;s parameters by perturbing them with well-crafted adversarial perturbations. With the encrypted parameters, the accuracy of the DNN model drops significantly, which can prevent malicious infringers from using the model. After the encryption, the positions of encrypted parameters and the values of the added adversarial perturbations form a secret key. Authorized user can use the secret key to decrypt the model on Machine Learning as a Service, while unauthorized user cannot use the model. Compared with the existing DNN watermarking methods which passively verify the ownership after the infringement occurs, the proposed method can prevent infringement in advance. Moreover, compared with few existing active DNN IP protection methods, the proposed method does not require additional training process of the model, thus introduces low computational overhead. Experimental results show that, after the encryption, the test accuracy of the model drops by 80.65%, 81.16%, and 87.91% on Fashion-MNIST (DenseNet), CIFAR-10 (ResNet), and GTSRB (AlexNet) datasets, respectively. Moreover, the proposed method only needs to encrypt an extremely low number of parameters. The proportion of the encrypted parameters in all the model&#39;s parameters is as low as 0.000205%. Experimental results also indicate that, the proposed method is robust against model fine-tuning attack, model pruning attack, and the adaptive attack where attackers know the detailed steps of the proposed method and all the parameters of the encrypted model.},
  archive      = {J_TETC},
  author       = {Mingfu Xue and Zhiyu Wu and Yushu Zhang and Jian Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2022.3231012},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {664-678},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AdvParams: An active DNN intellectual property protection technique via adversarial perturbation based parameter encryption},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROSETTA: A resource and energy-efficient inference processor
for recurrent neural networks based on programmable data formats and
fine activation pruning. <em>TETC</em>, <em>11</em>(3), 650–663. (<a
href="https://doi.org/10.1109/TETC.2022.3230961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are extensively employed to perform inference based on the temporal features of the input data. However, their computational workload and power consumption involved in inference are prohibitively high in practice, which may be problematic to achieve a high-speed inference in devices with tight limitations in the available silicon resources and power supply. This paper presents an efficient inference processor for RNNs, named ROSETTA. ROSETTA supports multiple data formats programmable for each vector operand to achieve a wide range or high precision with a limited data size. ROSETTA consistently performs every vector operation based on homogeneous processing units with a high utilization rate. Moreover, ROSETTA skips operations and reduces memory accesses to achieve high energy efficiency by pruning the activation elements in a fine-grained manner. Implemented in a low-cost 28 nm field-programmable gate array, ROSETTA exhibits a resource and energy efficiency as high as 2.51 – 1.14 MOP/s/LUT and 434.01 – 113.29 GOP/s/W, respectively, while producing near-floating-point inference results. The resource and energy efficiency of ROSETTA are higher than those of the previous processor implemented in the same device by up to 206.1% and 304.0%, respectively. The functionality has been verified for several RNN models of various types under a fully-integrated inference system.},
  archive      = {J_TETC},
  author       = {Jiho Kim and Tae-Hwan Kim},
  doi          = {10.1109/TETC.2022.3230961},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {650-663},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ROSETTA: A resource and energy-efficient inference processor for recurrent neural networks based on programmable data formats and fine activation pruning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revealing an inherently limiting factor in human mobility
prediction. <em>TETC</em>, <em>11</em>(3), 635–649. (<a
href="https://doi.org/10.1109/TETC.2022.3229088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting how humans move within space and time is a central topic in many scientific domains such as epidemic propagation, urban planning, and ride-sharing. However, current studies neglect individuals’ preferences to explore and discover new areas. Yet, neglecting novelty-seeking activities at first glance appears to be inconsequential on the ability to understand and predict individuals’ trajectories. We claim and show the opposite in this work: exploration-like visits strongly impact mobility understanding and anticipation. We start by proposing a new approach to identifying exploration visits. Based on that, we construct individuals’ mobility profiles using their exploration inclinations – Scouters (i.e., extreme explorers), Routiners (i.e., extreme returners), and Regulars (i.e., with no extreme behavior). Finally, we evaluate the impacts of novelty-seeking, quality of the data, and the prediction task formulation on the theoretical and practical predictability extents. The results show the validity of our profiling and highlight the obstructive impacts of novelty-seeking activities on the predictability of human trajectories. In particular, in the next-place prediction task, from 40% to 90% of predicted locations are wrong, notably with Scouters .},
  archive      = {J_TETC},
  author       = {Licia Amichi and Aline Viana Carneiro and Mark Crovella and Antonio Loureiro},
  doi          = {10.1109/TETC.2022.3229088},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {635-649},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Revealing an inherently limiting factor in human mobility prediction},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging journaling file system for prompt secure deletion
on interlaced recording drives. <em>TETC</em>, <em>11</em>(3), 619–634.
(<a href="https://doi.org/10.1109/TETC.2022.3226620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing awareness of secure computation, more and more users want to make their digital footprints securely deleted and irrecoverable after updating or removing files on storage devices. To achieve the effect of secure deletion, overwritten-based secure deletion techniques have been proposed to overwrite invalidated storage space with scrambled data content. Nevertheless, overwritten-based secure deletion requires explicit write requests for rewriting invalidated storage space with random data, thus incurring additional internal write traffic on storage devices. Recently, this inefficient aspect of secure deletion is exacerbated by the emergence of high-density interlaced magnetic recording (IMR) technology. IMR technology enhances the storage density of hard disk drives by interlacing narrower top tracks on wider bottom tracks. Owing to the interlaced track layout, securely erasing invalidated data on bottom tracks interferes with adjacent top tracks and track rewrites are required to preserve valid data on top tracks; thus, overwritten-based secure deletion on IMR drives significantly enlarges the internal write traffic and impairs the secure deletion efficiency. In this paper, instead of directly alleviating the constraint induced by the interlaced track layout, we propose leveraging the interlaced track layout of IMR drives in combination with the journaling data stream of journaling file systems to enable prompt secure data deletion while minimizing the write traffic of secure deletion operations. Experimental results suggest that the performance improvement achieve by the proposed prompt secure deletion (P-SD) strategy is 51.84% on average, when compared with the previous TrackLace approach on IMR drives.},
  archive      = {J_TETC},
  author       = {Shuo-Han Chen and Kuo-Hao Huang},
  doi          = {10.1109/TETC.2022.3226620},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {619-634},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Leveraging journaling file system for prompt secure deletion on interlaced recording drives},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A segmented-edit error-correcting code with
re-synchronization function for DNA-based storage systems.
<em>TETC</em>, <em>11</em>(3), 605–618. (<a
href="https://doi.org/10.1109/TETC.2022.3225570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful tool for storing digital information in chemically synthesized molecules, DNA-based data storage has undergone continuous development and received increasingly more attention. Efficiently recovering information from large-scale DNA strands that suffer from insertions, deletions, and substitution errors (collectively referred to as edit errors), is one of the major bottlenecks in DNA-based storage systems. To cope with this challenge, in this paper, we provide a segmented-edit error-correcting code with the re-synchronization function, termed the DNA-LM code. Compared with the previous segmented-error-correcting codes, it has a systematic structure and does not require the endpoint of the received segment as pre-requisite information for decoding. In the case that the number of edit errors exceeds the edit error-correcting capability of a segment, it can easily regain synchronization to ensure that the subsequent decoding continues. Both encoding and decoding complexity is linear in the codeword length. The redundancy of each segment is &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\lceil \log k\rceil +6$&lt;/tex-math&gt;&lt;/inline-formula&gt; quaternary symbols, where &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; is the length of the message segment. We further generalize the decoding algorithm to deal with duplicated DNA strands, whereas it still maintains linear time complexity in the codeword length and the number of duplications. Simulations under a stochastic edit errors model show that, at a low raw error rate of the “next-gen” sequencing, our code can enable error-free decoding by concatenating with the (255,223) RS code.},
  archive      = {J_TETC},
  author       = {Zihui Yan and Cong Liang and Huaming Wu},
  doi          = {10.1109/TETC.2022.3225570},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {605-618},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A segmented-edit error-correcting code with re-synchronization function for DNA-based storage systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Runtime system support for CPS software rejuvenation.
<em>TETC</em>, <em>11</em>(3), 594–604. (<a
href="https://doi.org/10.1109/TETC.2023.3267899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software rejuvenation, which was originally introduced to deal with performance degradation due to software aging, has recently been proposed as a mechanism to provide protection against run-time cyber attacks in cyber-physical systems (CPSs). Experiments have demonstrated that CPSs can be protected from attacks that corrupt run-time code and data by periodically restoring the run-time system with an uncorrupted image. Control theoretic and empirical methods have been developed to determine the timing and mode-switching conditions for CPS software rejuvenation (CPS SR) that will guarantee system safety. This article presents the requirements that need to be met by the run-time system to support CPS SR. It also presents an implementation and demonstration of the run-time system for the PX4 autopilot system for autonomous vehicles.},
  archive      = {J_TETC},
  author       = {Raffaele Romagnoli and Bruce H. Krogh and Dionisio de Niz and Anton D. Hristozov and Bruno Sinopoli},
  doi          = {10.1109/TETC.2023.3267899},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {594-604},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Runtime system support for CPS software rejuvenation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software aging prediction for cloud services using a gate
recurrent unit neural network model based on time series decomposition.
<em>TETC</em>, <em>11</em>(3), 580–593. (<a
href="https://doi.org/10.1109/TETC.2023.3258503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software aging, which is caused by the accumulation of errors in the system and the consumption of computing resources, tends to occur in long-running cloud service software systems. In practice, software aging prediction has proven to be useful in planning the time to trigger rejuvenation because it provides a prior estimate of future resource consumption. However, aging indicators (e.g., physical memory) in cloud may have the characteristics of long-term slow growth, medium-term seasonality variations (alternating peaks and troughs), and short-term irregular fluctuations. Unfortunately, most of the existing aging prediction methods (e.g., a statistical or single machine learning model) only focus on the accuracy of short-term prediction, while lacking the cognition of the medium and long-term variations of aging indicators and their functions in formulating the rejuvenation schedule (e.g., performing rejuvenation when the load is low can minimize interference to users). To address the above problems, this article proposes a novel hybrid aging prediction framework to work on the prediction of memory resource consumption in cloud by applying a seasonal-trend decomposition procedure based on loess (STL) method and Gate Recurrent Unit (GRU) neural network, called the decomposition-based GRU (DGRU). The effectiveness of DGRU mainly has two aspects. One is the STL method as a preprocessing technology that can extract the trend, seasonality, and residual characteristics from memory utilization data. The other is that these characteristics can be well predicted separately by a well-designed GRU model, which can model the time-series relationship between the data. Experimental results show that our DGRU framework has superior performance compared with its competitors, including seven single models and six hybrid models. Our study illustrates that the DGRU is a promising solution for high-precision software aging prediction.},
  archive      = {J_TETC},
  author       = {Kai Jia and Xiao Yu and Chen Zhang and Wenhua Hu and Dongdong Zhao and Jianwen Xiang},
  doi          = {10.1109/TETC.2023.3258503},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {580-593},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Software aging prediction for cloud services using a gate recurrent unit neural network model based on time series decomposition},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting aging-related bugs using network analysis on
aging-related dependency networks. <em>TETC</em>, <em>11</em>(3),
566–579. (<a href="https://doi.org/10.1109/TETC.2023.3279388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software aging, a phenomenon that exhibits an increasing failure rate or progressive performance degradation in long-running software systems, has caused serious cost damage or even loss of human lives. To aid aging-related bug (ARB, whose activation can result in software aging) detection and removal before software release, ARB prediction was proposed. Based on the prediction results, software teams can allocate limited testing resources to ARB-prone modules. Previous research has proposed several methods for both within-project and cross-project ARB prediction. However, they are based on the same set of metrics focusing on the contents of a single module, and only six metrics are aging-related. In this paper, we develop aging-related network measures by constructing an aging-related dependency network to model the flow of aging-related information in the software. Our evaluation on three commonly used open-source projects reveals that aging-related network measures show an inconsistent association with ARB-proneness in three projects, and the performance of aging-related network measures varies under different ARB prediction settings.},
  archive      = {J_TETC},
  author       = {Fangyun Qin and Zheng Zheng and Xiaohui Wan and Zhihao Liu and Zhiping Shi},
  doi          = {10.1109/TETC.2023.3279388},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {566-579},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Predicting aging-related bugs using network analysis on aging-related dependency networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A large scale characterization of device uptimes.
<em>TETC</em>, <em>11</em>(3), 553–565. (<a
href="https://doi.org/10.1109/TETC.2023.3271315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devices ages, also referred to as uptimes, convey information about systems, and are instrumental for patching and rejuvenation purposes. Knowing that a device is up for a long time suggests that it may be at risk or that degradation due to bugs may be in place. Nonetheless, there has been no systematic study of devices uptimes so far. The goal of this paper is to provide a large scale characterization of device uptimes, from the perspectives of a Reddit forum and Shodan. Among our findings, we identified that in the Reddit forum users typically assume that devices lingering up for more than 2 years are assumed to be long-lasting, and 25% of users assume that an uptime of 200 days is considered large. In addition, using Shodan we were able to identify a significant fraction of devices whose uptimes are larger than such threshold, posing security and performance risks to its users. We also found that routers are the devices with the largest uptimes, and that there is a positive correlation between devices tagged by Shodan as compromised and uptimes, with those devices having an average uptime beyond 250 days, that is roughly twice the average uptime in our dataset. In summary, our work suggests that it is viable to characterize uptimes from a system-wide viewpoint, and that such characterization sheds insight on the lifecycle of products and vulnerabilities.},
  archive      = {J_TETC},
  author       = {Mateus Nogueira and Erica da Cunha Ferreira and Pedro Tubenchlak Boechat and Felipe Assis and Estevão Rabello and Rafael Nascimento and Daniel Sadoc Menasché and Geraldo Xexéo and Abhishek Ramchandran and Katinka Wolter},
  doi          = {10.1109/TETC.2023.3271315},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {553-565},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A large scale characterization of device uptimes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special section on applied software aging
and rejuvenation. <em>TETC</em>, <em>11</em>(3), 550–552. (<a
href="https://doi.org/10.1109/TETC.2023.3299150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the publication of the first paper on software aging and rejuvenation by Huang et al. in 1995 [1], considerable research has been devoted to this topic. It deals with the phenomenon that continuously-running software systems may show an increasing failure rate and/or a degrading performance, either because error conditions accumulate inside the running system or because the rate at which faults are activated and errors are propagated is positively correlated with system uptime. Software rejuvenation relates to techniques counteracting aging (for example, by regularly stopping and restarting the software) in order to remove aging effects and to proactively prevent failures from occurring.},
  archive      = {J_TETC},
  author       = {Michael Grottke and Alberto Avritzer and Hironori Washizaki and Kishor Trivedi},
  doi          = {10.1109/TETC.2023.3299150},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7},
  number       = {3},
  pages        = {550-552},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial special section on applied software aging and rejuvenation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FerroCoin: Ferroelectric tunnel junction-based true random
number generator. <em>TETC</em>, <em>11</em>(2), 541–547. (<a
href="https://doi.org/10.1109/TETC.2022.3210401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Ferroelectric Tunnel Junction (FTJ)-based true random number generator (TRNG) that utilizes the stochastic domain switching phenomenon in ferroelectric materials. Ferroelectrics are promising for extracting randomness owing to their innate switching entropy in the multi-domain state. The random numbers generated by the proposed TRNG are shown to pass all the NIST SP 800-22 tests. The robustness of the proposed TRNG is also validated at various temperature and process corners. Important metrics such as power, bit rate, and energy/bit are calculated. This is the first comprehensive work demonstrating a ferroelectric-based TRNG with all these metrics. Compared to state-of-the-art TRNGs using other emerging technologies, we can achieve a higher bit rate with lower power consumption. We also perform material-level optimization with different ferroelectric materials, and showcase the trade-off between the bit rate and the power consumption. The proposed TRNG shows high robustness and reliability, and thus has the potential for implementing a low power on-chip solution.},
  archive      = {J_TETC},
  author       = {Swetaki Chatterjee and Nikhil Rangarajan and Satwik Patnaik and Dinesh Rajasekharan and Ozgur Sinanoglu and Yogesh Singh Chauhan},
  doi          = {10.1109/TETC.2022.3210401},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {541-547},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FerroCoin: Ferroelectric tunnel junction-based true random number generator},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-efficient recurrent neural network with MRAM-based
probabilistic activation functions. <em>TETC</em>, <em>11</em>(2),
534–540. (<a href="https://doi.org/10.1109/TETC.2022.3202112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herein, we develop a programmable energy-efficient hardware implementation for Recurrent Neural Networks (RNNs) with Resistive Random-Access Memory (ReRAM) synapses and ultra-low power, area-efficient spin-based activation functions. To attain high energy-efficiency while maintaining accuracy, a novel Computing-in-Memory (CiM) architecture is proposed to leverage data-level parallelism during the evaluation phase. We employ an MRAM-based Adjustable Probabilistic Activation Function (APAF) via a low-power tunable activation mechanism, providing adjustable levels of accuracy to mimic ideal sigmoid and tanh thresholding along with a matching algorithm to regulate the neuron properties. Our hardware/software cross-layer simulation shows that our proposed design achieves up to 74.5× energy-efficiency with ∼11× area reduction compared to its counterpart designs while keeping the accuracy comparable.},
  archive      = {J_TETC},
  author       = {Shadi Sheikhfaal and Shaahin Angizi and Ronald F. DeMara},
  doi          = {10.1109/TETC.2022.3202112},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {534-540},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy-efficient recurrent neural network with MRAM-based probabilistic activation functions},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-performance and robust spintronic/CNTFET-based
binarized neural network hardware accelerator. <em>TETC</em>,
<em>11</em>(2), 527–533. (<a
href="https://doi.org/10.1109/TETC.2022.3202113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolutional neural network (CNN) is a significant part of the artificial intelligence (AI) systems widely used in different tasks. The binarized neural networks (BNNs) reduce power consumption and hardware overhead to answer the demands for using AI in power-limited applications. In this paper, a BNN hardware accelerator is proposed. The proposed approach is based on a novel nonvolatile XNOR/XOR circuit designed using the magnetic tunnel junction (MTJ) and gate-all-around carbon nanotube field-effect transistor (GAA-CNTFET) devices. The nonvolatility of the proposed design leads to the elimination of external memory access that significantly decreases the data transmission delay and power dissipation. Moreover, it consumes low energy, which is very critical in battery-operated devices. Furthermore, the combinational read circuitry of the proposed design leads to high robustness to process variations. According to the simulation results, our proposed design has a logical error rate of 0.0164%, which is negligible and offers a significantly high network accuracy even in the presence of significant process variations. Our proposed hardware accelerator provides at least 13%, 29%, and 41% improvements regarding power, power delay product (PDP), and area compared to its state-of-the-art counterparts.},
  archive      = {J_TETC},
  author       = {Milad Tanavardi Nasab and Abdolah Amirany and Mohammad Hossein Moaiyeri and Kian Jafari},
  doi          = {10.1109/TETC.2022.3202113},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {527-533},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {High-performance and robust Spintronic/CNTFET-based binarized neural network hardware accelerator},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient vertical federated learning method for ridge
regression of large-scale samples. <em>TETC</em>, <em>11</em>(2),
511–526. (<a href="https://doi.org/10.1109/TETC.2022.3215986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating data from multiple parties to achieve cross-institutional machine learning is an important trend in Industry 4.0 era. However, the privacy risks from sharing data pose a significant challenge to data integration. To integrate data without sharing data and meet large-scale samples’ modeling needs, we propose two vertical federation learning algorithms for ridge regression via least-squares solution for two-party and multi-party scenarios, respectively. Compared with the state-of-the-art algorithms, our algorithms only need one round of calculation for the optimization instead of iteration. Furthermore, our algorithms can effectively handle large-scale samples due to the number of cryptographic operations in our algorithms being independent of the number of samples. Through our proposed the matrix secure agent computing theory and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\delta$&lt;/tex-math&gt;&lt;/inline-formula&gt; -data indistinguishability theory, we provide quantitative theoretical guarantees for the security of our algorithms. Our algorithms satisfy complete data indistinguishability under the “semi-honest” assumption and the quantitative security under the “malicious” assumption. The experiments show that our proposed algorithm takes only about 400 seconds to handle up to 9.6 million large-scale samples, while the state-of-the-art algorithms take close to 1000 seconds to handle every 1000 samples, which embodies the advantage of our algorithms in handling large-scale samples.},
  archive      = {J_TETC},
  author       = {Jianping Cai and Ximeng Liu and Zhiyong Yu and Kun Guo and Jiayin Li},
  doi          = {10.1109/TETC.2022.3215986},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {511-526},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Efficient vertical federated learning method for ridge regression of large-scale samples},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Architecture support for bitslicing. <em>TETC</em>,
<em>11</em>(2), 497–510. (<a
href="https://doi.org/10.1109/TETC.2022.3215480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bitsliced programming model has shown to boost the throughput of software programs. However, on a standard architecture, it exerts a high pressure on register access, causing memory spills and restraining the full potential of bitslicing. In this work, we present architecture support for bitslicing in a System-on-Chip. Our hardware extensions are of two types; internal to the processor core, in the form of custom instructions, and external to the processor, in the form of direct memory access module with support for data transposition. We present a comprehensive performance evaluation of the proposed enhancements in the context of several RISC-V ISA definitions (RV32I, RV64I, RV32B, RV64B). The proposed 14 new custom instructions use 1.5× fewer registers compared to the equivalent functionality expressed using RISC-V instructions. The integration of those custom instructions in a 5-stage pipelined RISC-V RV32I core incurs 10.21% and 12.72% overhead respectively in area and cell count using the SkyWater 130 nm standard cell library. The proposed bitslice transposition unit with DMA provides a further speedup, changing the quadratic increase in execution time of data transposition to linear. Finally, we demonstrate a comprehensive performance evaluation using a set of benchmarks of lightweight and masked ciphers.},
  archive      = {J_TETC},
  author       = {Pantea Kiaei and Thomas Conroy and Patrick Schaumont},
  doi          = {10.1109/TETC.2022.3215480},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {497-510},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Architecture support for bitslicing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuE: Automated neural network ensembles for edge
intelligence. <em>TETC</em>, <em>11</em>(2), 485–496. (<a
href="https://doi.org/10.1109/TETC.2022.3214931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) applications have been established in the mobile industry and are decisively determining the progress in entrepreneurial value creation. This article explores the potential of Edge Computing to enhance the performance of AI applications. In particular, a DNN ensemble formation (DEF) problem is studied which judiciously recruits members for DNN ensembles considering the device heterogeneity, computing resource limitation, and service deadline of edge computing systems, in an attempt to optimize the performance of edge AI services. We design a novel algorithm called Neural Ensemble (NeuE) to solve the DEF problem. NeuE involves an online learning process that learns the in-practice performance of DNN ensembles and adaptively forms DNN ensembles according to the features of admitted tasks. It leverages the framework of contextual multi-armed bandit and follows the constraints of computing resource limitation and service deadline. We also show theoretically that NeuE provides asymptotic optimality. However, NeuE suffers from poor scalability due to exponentially-growing ensemble decision space. We then propose a variant of NeuE, called NeuE-S, to expedite NeuE. NeuE-S identifies representative ensemble decisions using similarities of ensemble decisions and carries out learning with a reduced decision space. We show via theoretical analysis that NeuE-S drastically reduces the computation complexity with negligible performance loss. We implement our method on an edge computing testbed. The results show that our method dramatically improves the performance of edge AI services.},
  archive      = {J_TETC},
  author       = {Yang Bai and Lixing Chen and Jie Xu},
  doi          = {10.1109/TETC.2022.3214931},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {485-496},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {NeuE: Automated neural network ensembles for edge intelligence},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PLSM: A parallelized liquid state machine for unintentional
action detection. <em>TETC</em>, <em>11</em>(2), 474–484. (<a
href="https://doi.org/10.1109/TETC.2022.3211011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir Computing (RC) offers a viable option to deploy AI algorithms on low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired RC model that mimics the cortical microcircuits and uses spiking neural networks (SNN) that can be directly realized on neuromorphic hardware. In this paper, we present a novel Parallelized LSM (PLSM) architecture that incorporates spatio-temporal read-out layer and semantic constraints on model output. To the best of our knowledge, such a formulation has been done for the first time in literature, and it offers a computationally lighter alternative to traditional deep-learning models. Additionally, we also present a comprehensive algorithm for the implementation of parallelizable SNNs and LSMs that are GPU-compatible. We implement the PLSM model to classify unintentional/accidental video clips using the Oops dataset. From the experimental results on detecting unintentional action in a video, it can be observed that our proposed model outperforms a self-supervised model and a fully supervised traditional deep learning model. All the implemented codes can be found in our repository https://github.com/sadimanna/Parallelized_LSM_for_Unintentional_Action_Recognition .},
  archive      = {J_TETC},
  author       = {Siladittya Manna and Dipayan Das and Saumik Bhattacharya and Umapada Pal and Sukalpa Chanda},
  doi          = {10.1109/TETC.2022.3211011},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {474-484},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PLSM: A parallelized liquid state machine for unintentional action detection},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning with sharing: An edge-optimized incremental
learning method for deep neural networks. <em>TETC</em>, <em>11</em>(2),
461–473. (<a href="https://doi.org/10.1109/TETC.2022.3210905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning techniques aim to increase the capability of Deep Neural Network (DNN) model to add new classes in the pre-trained model. However, DNNs suffer from catastrophic forgetting during the incremental learning process. Existing incremental learning techniques try to reduce the effect of catastrophic forgetting by either using previous samples of data while adding new classes in the model or designing complex model architectures. This leads to high design complexity and memory requirements which make incremental learning impossible to implement on edge devices that have limited memory and computation resources. So, we propose a new incremental learning technique Learning with Sharing (LwS) based on the concept of transfer learning. The main aims of LwS are reduction in training complexity and storage memory requirements while achieving high accuracy during the incremental learning process. We perform cloning and sharing of full connected (FC) layers to add new classes in the model incrementally. Our proposed technique can preserve the knowledge of existing classes and add new classes without storing data from the previous classes. We show that our proposed technique outperforms the state-of-the-art techniques in accuracy comparison for Cifar-100, Caltech-101, and UCSD Birds datasets.},
  archive      = {J_TETC},
  author       = {Muhammad Awais Hussain and Shih-An Huang and Tsung-Han Tsai},
  doi          = {10.1109/TETC.2022.3210905},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {461-473},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Learning with sharing: An edge-optimized incremental learning method for deep neural networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning realized with nonlinearly transformed
input space. <em>TETC</em>, <em>11</em>(2), 448–460. (<a
href="https://doi.org/10.1109/TETC.2022.3210568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is concerned with the design of a nonlinear transformation strategy to address the issues caused by the mismatch of marginal probability distributions between the source and target domains in transfer learning. In the process of transfer learning, the existing model (which is regarded as the available source of knowledge) is constructed in the source domain with the assumption that data points are independent and identically distributed. However, this assumption does not hold when the existing model is transferred to a new target domain. As a consequence, the performance of the existing model in the target domain deteriorates. Through mapping the original target space to a new space of the same dimensionality by using nonlinear transformations, the distributions of the data in the source and target domains could well matched each other, which significantly facilitates the transfer of accumulated knowledge to the new domain. No prior knowledge of the data distributions or the detailed information of the existing model is required. Experiments involving both synthetic dataset and real-world datasets are performed to demonstrate the effectiveness of the proposed approach in improving classification accuracy of the exiting model in the target domain.},
  archive      = {J_TETC},
  author       = {Xiubin Zhu and Dan Wang and Witold Pedrycz and Zhiwu Li},
  doi          = {10.1109/TETC.2022.3210568},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {448-460},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Transfer learning realized with nonlinearly transformed input space},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReDeSIGN: Reuse of debug structures for improvement in
performance gain of NoC based MPSoCs. <em>TETC</em>, <em>11</em>(2),
432–447. (<a href="https://doi.org/10.1109/TETC.2022.3203611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-on-Chip (NoC) is considered as a scalable interconnect medium for Multiprocessor System-on-Chip (MPSoC) due to its ability to provide high bandwidth and low latency communication. With the increasing intricacy of the modern-day systems, the state-of-the-art NoCs are becoming extremely complex. Design-for-Debug (DFD) structures are integrated to the system for the validation of such complex modules during post-silicon debug. However, after the system validation and mass production, the DFD hardware remains vestigial on the design. In this context, we propose ReDeSIGN, a framework to reuse the DFD infrastructure during the in-field operation for performance enhancement of the NoC-based MPSoCs. Major contributions of our work include reuse of (i) trace buffer as extended Virtual Channel (VC) for network throughput improvement, (ii) trace prioritization hardware for critical data prioritization, and (iii) packet monitor module for packet starvation control. Experimental evaluations with real benchmarks show an average of 11.46% increase in network throughput, 34.93% decrease in critical data latency, and 19.17% decrease in packet starvation for an 8x8 homogeneous system.},
  archive      = {J_TETC},
  author       = {Sidhartha Sankar Rout and Badri M and Mitali Sinha and Sujay Deb},
  doi          = {10.1109/TETC.2022.3203611},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {432-447},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ReDeSIGN: Reuse of debug structures for improvement in performance gain of NoC based MPSoCs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anatomy of on-chip memory hardware fault effects across the
layers. <em>TETC</em>, <em>11</em>(2), 420–431. (<a
href="https://doi.org/10.1109/TETC.2022.3205808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability evaluation of a microprocessor design may reveal vulnerable silicon areas that require protection against faults, but also hardware structures that are inherently more resilient to faults. In this paper, we revisit the concept of system vulnerability stack by analyzing the anatomy of fault effects across the abstraction layers and identify the sources of error in well-established transient fault vulnerability evaluation methodologies. By providing insights for all distinct system layers, we showcase that the evaluation should take into consideration all faults that arrive from the underlying hardware as well as their distribution. We additionally quantify the level of error in a PVF estimation that can be attributed to the microarchitecture and the architecture. Current established methodologies fail to capture these aspects and can potentially lead to misleading findings. We experimentally show how PVF estimation can follow opposite (and thus misleading) trends compared to the correct, full-stack Architectural Vulnerability Factor (AVF) estimation and explain the reasons why the higher-level methodologies provide diverging results by showing the anatomy of faults manifestation across the layers. Our experiments are performed on an Armv8 microprocessor model, using different input datasets to thoroughly demonstrate our insights.},
  archive      = {J_TETC},
  author       = {George Papadimitriou and Dimitris Gizopoulos},
  doi          = {10.1109/TETC.2022.3205808},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {420-431},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Anatomy of on-chip memory hardware fault effects across the layers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TopSort: A high-performance two-phase sorting accelerator
optimized on HBM-based FPGAs. <em>TETC</em>, <em>11</em>(2), 404–419.
(<a href="https://doi.org/10.1109/TETC.2022.3228575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of high-bandwidth memory (HBM) brings new opportunities to boost the performance of sorting acceleration on FPGAs, which was conventionally bounded by the available off-chip memory bandwidth. However, it is nontrivial for designers to fully utilize this immense bandwidth. First, the existing sorter designs cannot be directly scaled at the increasing rate of available off-chip bandwidth, as the required on-chip resource usage grows at a much faster rate and would bound the sorting performance in turn. Second, designers need an in-depth understanding of HBM&#39;s characteristics to effectively utilize the HBM bandwidth. To tackle these challenges, we present TopSort, a novel two-phase sorting solution optimized for HBM-based FPGAs. In the first phase, 16 merge trees work in parallel to fully utilize 32 HBM channels’ bandwidth. In the second phase, TopSort reuses the logic from phase one to form a wider merge tree to merge the partially sorted results from phase one. TopSort also adopts HBM-specific optimizations to reduce resource overhead and improve bandwidth utilization. TopSort can sort up to 4 GB data using all 32 HBM channels, with an overall sorting performance of 15.6 GB/s. TopSort is 6.7× and 2.7× faster than state-of-the-art CPU and FPGA sorters.},
  archive      = {J_TETC},
  author       = {Weikang Qiao and Licheng Guo and Zhenman Fang and Mau-Chung Frank Chang and Jason Cong},
  doi          = {10.1109/TETC.2022.3228575},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {404-419},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TopSort: A high-performance two-phase sorting accelerator optimized on HBM-based FPGAs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALP: Alleviating CPU-memory data movement overheads in
memory-centric systems. <em>TETC</em>, <em>11</em>(2), 388–403. (<a
href="https://doi.org/10.1109/TETC.2022.3226132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning applications between near-data processing (NDP) and host CPU cores causes inter-segment data movement overhead, which is caused by moving data generated by one segment (e.g., instructions, functions) and used in other consecutive segments. Prior works take two approaches to this problem. The first approach maps segments to NDP or host cores based on the properties of each segment, neglecting the inter-segment data movement overhead. The second approach partitions applications based on the overall memory bandwidth savings, and does not offload each segment to the best-fitting core if they incur high inter-segment data movement. We show that 1) mapping each segment to its best-fitting core ideally can provide substantial benefits, and 2) the inter-segment data movement reduces this benefit significantly. We introduce ALP, a new programmer-transparent technique to alleviate the inter-segment data movement overhead between host and memory in NDP systems. ALP proactively and accurately transfers the required data between the segments based on the key observation that the instructions that generate the inter-segment data stay the same across different executions of a program. ALP uses a compiler pass to identify these instructions and uses specialized hardware to transfer their produced data at runtime. We evaluate ALP across a wide range of workloads and demonstrate 54.3% and 45.4% average speedup over CPU-only and NDP-only executions, respectively.},
  archive      = {J_TETC},
  author       = {Nika Mansouri Ghiasi and Nandita Vijaykumar and Geraldo F. Oliveira and Lois Orosa and Ivan Fernandez and Mohammad Sadrosadati and Konstantinos Kanellopoulos and Nastaran Hajinazar and Juan Gómez Luna and Onur Mutlu},
  doi          = {10.1109/TETC.2022.3226132},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {388-403},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ALP: Alleviating CPU-memory data movement overheads in memory-centric systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuSB: A scalable interconnect architecture for spiking
neuromorphic hardware. <em>TETC</em>, <em>11</em>(2), 373–387. (<a
href="https://doi.org/10.1109/TETC.2023.3238708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic systems are typically designed as a tile-based architecture where inter-tile data communication is facilitated using a shared global interconnect. Congestion on this interconnect can increase both interconnect energy, which increases the total energy consumption of the hardware and latency, which impacts the performance e.g., accuracy of the application that is being executed on the hardware. Mesh-based Network-on-Chip (NoC) that is used in most hardware prototypes is not the optimal interconnect solution for neuromorphic systems. This is because of the following two reasons. First, power consumption and average latency of a NoC increases exponentially with the number of tiles in the hardware. Second, a NoC cannot exploit an application&#39;s data communication pattern efficiently. Once designed for a target hardware, the bandwidth on each NoC link stays the same, independent of the volume of data traffic between different tile pairs of the NoC. In other words, a NoC cannot be customized at a finer granularity based on an individual application running on the hardware. We show that these NoC limitations prevent opportunities to further improve energy and latency of a neuromorphic hardware. To address these limitations, we propose Dynamic Segmented Bus (SB) interconnect for neuromorphic systems. Here, a bus lane is partitioned into segments with each segment connecting a few tiles. Connection of tiles to segments and those between segments are bridged using our novel three-way segmentation switches that are programmed using the software before admitting an application to the hardware. We partition an application by analyzing its workload and place partitions intelligently onto segments. This exploits application characteristics to use the segments without any routing collisions while exploiting the latency and energy savings in the design-time mapping phase. At a high-level, our mapping algorithm places tiles that communicate the most on shorter segments utilizing fewer number of switches, thereby reducing network congestion. It can adjust the bandwidth by controlling the number of segments connected to a destination tile. At run time, our controller dynamically executes the predefined routing paths without requiring any additionally routing decisions, unlike a NoC. This allows us to improve both energy and latency. Using parallel segmented busses, our proposed interconnect architecture can support a large number of tiles without significantly increasing the design cost, energy, and latency. Simulation results show that compared to the most widely-used mesh-based NoC design, our interconnect architecture, which we call NeuSB, reduces the switch area by 20x, average interconnect energy by 6.2x, and latency by 23%.},
  archive      = {J_TETC},
  author       = {Adarsha Balaji and Phu Khanh Huynh and Francky Catthoor and Nikil D. Dutt and Jeffrey L. Krichmar and Anup Das},
  doi          = {10.1109/TETC.2023.3238708},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {373-387},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {NeuSB: A scalable interconnect architecture for spiking neuromorphic hardware},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bit-line computing for CNN accelerators co-design in edge AI
inference. <em>TETC</em>, <em>11</em>(2), 358–372. (<a
href="https://doi.org/10.1109/TETC.2023.3237914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By supporting the access of multiple memory words at the same time, Bit-line Computing (BC) architectures allow the parallel execution of bit-wise operations in-memory. At the array periphery, arithmetic operations are then derived with little additional overhead. Such a paradigm opens novel opportunities for Artificial Intelligence (AI) at the edge, thanks to the massive parallelism inherent in memory arrays and the extreme energy efficiency of computing in-situ, hence avoiding data transfers. Previous works have shown that BC brings disruptive efficiency gains when targeting AI workloads, a key metric in the context of emerging edge AI scenarios. This manuscript builds on these findings by proposing an end-to-end framework that leverages BC-specific optimizations to enable high parallelism and aggressive compression of AI models. Our approach is supported by a novel hardware module performing real-time decoding, as well as new algorithms to enable BC-friendly model compression. Our hardware/software approach results in a 91% energy savings (for a 1% accuracy degradation constraint) regarding state-of-the-art BC computing approaches.},
  archive      = {J_TETC},
  author       = {Marco Rios and Flavio Ponzina and Alexandre Levisse and Giovanni Ansaloni and David Atienza},
  doi          = {10.1109/TETC.2023.3237914},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {358-372},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Bit-line computing for CNN accelerators co-design in edge AI inference},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable reasoning and sensing using processing-in-memory
with hybrid spin/CMOS-based analog/digital blocks. <em>TETC</em>,
<em>11</em>(2), 343–357. (<a
href="https://doi.org/10.1109/TETC.2022.3212341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we leverage in-memory computation for data-intensive applications to surmount the bandwidth restrictions inherent in the Von-Neumann computing paradigm, while addressing transistor technology scaling challenges facing Moore&#39;s Law. We introduce the Spintronically Configurable Analog Processing in-memory Environment (SCAPE) which incorporates top-down architectural approaches along with bottom-up intrinsic device switching behaviors of spin-based post-CMOS devices. SCAPE embeds analog arithmetic capabilities providing a selectable thresholding functionality to realize generalized neuron activation functions that are integrated within the 2-dimensional memory array. Within each module, circuit-switched connections allow in-field configuration of the partly reconfigurable neuron activation function to suit the target application, and the intrinsic computation is performed using spin-based devices. This hybrid-technology design advances in-memory computation beyond previous approaches by integrating analog arithmetic, runtime reconfigurability, and non-volatile devices within a selectable 2-dimensional topology. Simulation results of error rates, power consumption, power-error-product metric, are examined for real-world applications including edge-of-network based Compressive Sensing and Machine Learning use cases, along with process variation analysis. Results show up to 7% improvement in error rate using proposed implementation of enhanced activation function versus baseline conventional sigmoidal activation, whereas realization of AMP signal processing algorithm shows ∼95% reduction in energy consumption at comparable accuracy.},
  archive      = {J_TETC},
  author       = {Mousam Hossain and Adrian Tatulian and Shadi Sheikhfaal and Harshavardhana R. Thummala and Ronald F. DeMara},
  doi          = {10.1109/TETC.2022.3212341},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {343-357},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scalable reasoning and sensing using processing-in-memory with hybrid Spin/CMOS-based Analog/Digital blocks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-efficient computing-in-memory (CiM) scheme using
field-free spin-orbit torque (SOT) magnetic RAMs. <em>TETC</em>,
<em>11</em>(2), 331–342. (<a
href="https://doi.org/10.1109/TETC.2023.3237541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The separation of memory and computing units in the von Neumann architecture leads to undesirable energy consumption due to data movement and insufficient memory bandwidth. Energy-efficient in-memory computing platforms have the potential to address such issues. Due to its non-volatility and advantageous features over CMOS (such as low power, near-zero leakage current and high integration density), spin-based devices have been advocated for in-memory computing. This paper proposes a field-free Spin Orbit Torque (FF-SOT) MRAM based computing-in-memory (CiM) scheme that realizes XNOR/XOR logic and a cascading adder. This novel FF-SOT-CiM design does not require expensive peripheral circuits for computation while using the same memory cell design as a SOT-MRAM. Furthermore, FF-SOT-CiM does not require additional write cycles to save the result of its computations in the memory. The design offers higher write speed and; lower operating energy compared to CiM schemes based on other technologies; it also alleviates the source degeneration effect by leveraging an advanced switching mechanism. Extensive simulation results show that the proposed FF-SOT-CiM achieves up to 3.1x (2.6x) latency (energy) reduction compared to SRAM-based CiM, with negligible hardware overhead when performing in-memory XOR. ADD operations; the proposed FF-SOT-CiM can be to 5.0X and 1.5X faster and 3.4X and 1.1X more energy efficient than existing STT-based and FeFET-based schemes, respectively.},
  archive      = {J_TETC},
  author       = {Bi Wu and Haonan Zhu and Dayane Reis and Zhaohao Wang and Ying Wang and Ke Chen and Weiqiang Liu and Fabrizio Lombardi and Xiaobo Sharon Hu},
  doi          = {10.1109/TETC.2023.3237541},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {331-342},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An energy-efficient computing-in-memory (CiM) scheme using field-free spin-orbit torque (SOT) magnetic RAMs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of MRAM-centric computing: From near memory to in
memory. <em>TETC</em>, <em>11</em>(2), 318–330. (<a
href="https://doi.org/10.1109/TETC.2022.3214833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional von Neumann architecture suffers from bottlenecks in computing performance and power consumption due to frequent data exchange between memory and processing units. To overcome this issue, research on novel computing architectures including near-memory computing (NMC) and in-memory computing (IMC) has been accelerated based on emerging nonvolatile memory devices. Among various potential candidates, spintronic-based magnetic random-access memory (MRAM) has come into a research and development hotspot by its ultralow switching energy, nonvolatility, and superior endurance. This paper outlines the background, trends, and challenges involved in the development of MRAM-centric computing, and highlights the recent prototypes and advances in applications based on MRAM-NMC and MRAM-IMC.},
  archive      = {J_TETC},
  author       = {Yueting Li and Tianshuo Bai and Xinyi Xu and Yundong Zhang and Bi Wu and Hao Cai and Biao Pan and Weisheng Zhao},
  doi          = {10.1109/TETC.2022.3214833},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {318-330},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A survey of MRAM-centric computing: From near memory to in memory},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReaLPrune: ReRAM crossbar-aware lottery ticket pruning for
CNNs. <em>TETC</em>, <em>11</em>(2), 303–317. (<a
href="https://doi.org/10.1109/TETC.2022.3223630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training machine learning (ML) models at the edge (on-chip training on end user devices) can address many pressing challenges including data privacy/security, increase the accessibility of ML applications to different parts of the world by reducing the dependence on the communication fabric and the cloud infrastructure, and meet the real-time requirements of AR/VR applications. However, existing edge platforms do not have sufficient computing capabilities to support complex ML tasks such as training large CNNs. ReRAM-based architectures offer high-performance yet energy efficient computing platforms for on-chip CNN training/inferencing. However, ReRAM-based architectures are not scalable with the size of the CNN. Larger CNNs have more weights, which requires more ReRAM cells that cannot be integrated in a single chip. Moreover, training larger CNNs on-chip will require higher power, which cannot be afforded by these smaller devices. Pruning is an effective way to solve this problem. However, existing pruning techniques are either targeted for inferencing only, or they are not crossbar-aware. This leads to sub-optimal hardware savings and performance benefits for CNN training on ReRAM-based architectures. In this paper, we address this problem by proposing a novel crossbar-aware pruning strategy, referred as ReaLPrune, which can prune more than 90% of CNN weights. The pruned model can be trained from scratch without any accuracy loss. Experimental results indicate that ReaLPrune reduces hardware requirements by 77.2% and accelerates CNN training by ∼20× compared to unpruned CNNs. ReaLPrune also outperforms other crossbar-aware pruning techniques in terms of both performance and hardware savings. In addition, ReaLPrune is equally effective for diverse datasets and more complex CNNs.},
  archive      = {J_TETC},
  author       = {Biresh Kumar Joardar and Janardhan Rao Doppa and Hai Li and Krishnendu Chakrabarty and Partha Pratim Pande},
  doi          = {10.1109/TETC.2022.3223630},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {303-317},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ReaLPrune: ReRAM crossbar-aware lottery ticket pruning for CNNs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A binary-activation, multi-level weight RNN and training
algorithm for ADC-/DAC-free and noise-resilient processing-in-memory
inference with eNVM. <em>TETC</em>, <em>11</em>(2), 292–302. (<a
href="https://doi.org/10.1109/TETC.2023.3241004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for training neural networks with binary activations and multi-level weights, which enables efficient processing-in-memory circuits with embedded nonvolatile memories (eNVM). Binary activations obviate costly DACs and ADCs. Multi-level weights leverage multi-level eNVM cells. Compared to existing algorithms, our method not only works for feed-forward networks (e.g., fully-connected and convolutional), but also achieves higher accuracy and noise resilience for recurrent networks. In particular, we present an RNN-based trigger-word detection PIM accelerator, with detailed hardware noise models and circuit co-design techniques, and validate our algorithm&#39;s high inference accuracy and robustness against a variety of real hardware non-idealities.},
  archive      = {J_TETC},
  author       = {Siming Ma and David Brooks and Gu-Yeon Wei},
  doi          = {10.1109/TETC.2023.3241004},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {292-302},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A binary-activation, multi-level weight RNN and training algorithm for ADC-/DAC-free and noise-resilient processing-in-memory inference with eNVM},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BNN an ideal architecture for acceleration with resistive in
memory computation. <em>TETC</em>, <em>11</em>(2), 281–291. (<a
href="https://doi.org/10.1109/TETC.2023.3237778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary Neural Networks (BNN) have binarized (-1 and 1) weights and feature maps. Achieving smaller model sizes and computational simplicity, they are well suited for edge-AI systems with power and hardware constraints. Recently, memristive crossbar arrays have gained considerable attention from researchers to perform analog in-memory vector-matrix multiplications in machine learning accelerators, with low power and constant computational time. Crossbar arrays suffer from many non-ideal characteristics such as memristor device imperfections, weight noise, device drift, input/output noises, and DAC/ADC overhead. Thus, for analog AI acceleration to become viable, model architectures must be robust against these non-idealities. We propose that BNN&#39;s with their binarized weights, which are ideally mapped to fewer memristive devices with less electrical characteristic issues and higher tolerance to computational noise, are a promising architecture for analog computation. In this work, we examine the viability of deploying state of the art BNNs, with features such as real value residual connections and parametric activations with biases, to analog in-memory computational accelerators. Our simulations show that BNNs are significantly more robust to crossbar non-idealities than full-precision networks, require less chip area, and consume less power on memristive crossbar architectures.},
  archive      = {J_TETC},
  author       = {Andrew Ding and Ye Qiao and Nader Bagherzadeh},
  doi          = {10.1109/TETC.2023.3237778},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {281-291},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {BNN an ideal architecture for acceleration with resistive in memory computation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: IEEE transactions on emerging topics in
computing thematic section on memory- centric designs:
Processing-in-memory, in-memory computing, and near-memory computing for
real-world applications. <em>TETC</em>, <em>11</em>(2), 278–280. (<a
href="https://doi.org/10.1109/TETC.2023.3267909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The von Neumann architecture has been the status quo since the dawn of modern computing. Computers built on the von Neumann architecture are composed of an intelligent master processor (e.g., CPU) and dumb memory/storage devices incapable of computation (e.g., memory and disk). However, the skyrocketing data volume in modern computing is calling such status quo into question. The excessive amounts of data movement between processor and memory/storage in more and more real-world applications (e.g., machine learning and AI applications) have made the processor-centric design a severe power and performance bottleneck. The diminishing Moore&#39;s Law also raises the need for a memory-centric design, which is rising on top of the recent material advancement and manufacturing innovation to open a paradigm shift. By doing computation right inside or near the memory, the memory-centric design promises massive throughput and energy savings.},
  archive      = {J_TETC},
  author       = {Yuan-Hao Chang and Vincenzo Piuri},
  doi          = {10.1109/TETC.2023.3267909},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {278-280},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: IEEE transactions on emerging topics in computing thematic section on memory- centric designs: processing-in-memory, in-memory computing, and near-memory computing for real-world applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Security and approximation: Vulnerabilities in
approximation-aware testing. <em>TETC</em>, <em>11</em>(1), 265–271. (<a
href="https://doi.org/10.1109/TETC.2022.3176761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation-aware testing has been recently proposed because conventional methods cannot take into account the flexibility encountered in approximate circuits for handling errors. However, the security vulnerabilities of approximation-aware testing techniques have not been fully considered. Approximate circuits utilize several schemes with a variety of error patterns, that open new opportunities for attackers. In this paper, the security threats in approximation-aware testing methods are investigated. To the best of the authors’ knowledge, the topic on the security analysis of approximation-aware testing techniques has not been treated in the technical literature. Two malicious tampering attacks are proposed in addition to an attack that can invalidate hardware Trojan detection. The proposed two tampering attacks, namely tampering of the exact netlist (TEN) and tampering of the error metric (TEM), are evaluated with experiments. The results show that original non-acceptable faults can be mistakenly assigned as acceptable faults and that approximation-aware testing techniques can invalidate the detection of hardware Trojans in approximate circuits due to fault classification.},
  archive      = {J_TETC},
  author       = {Yuqin Dou and Chongyan Gu and Chenghua Wang and Weiqiang Liu and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2022.3176761},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {265-271},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Security and approximation: Vulnerabilities in approximation-aware testing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H-BILSTM: A novel bidirectional long short term memory
network based intelligent early warning scheme in mobile edge computing
(MEC). <em>TETC</em>, <em>11</em>(1), 253–264. (<a
href="https://doi.org/10.1109/TETC.2022.3202266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of edge server resources, an efficient offloading strategy is critical for reducing the service latency and energy consumption of terminal devices. However, the efficiency of the resource allocation relies heavily on an accurate evaluation of the load in the edge servers, which is usually unknown and changes periodically. In this paper, we analyse the performance of the resource allocations based on various load prediction algorithm allocations in a mobile edge computing (MEC) system and propose a neural network-based load alert scheme. First, we propose a deep neural network-based MEC load prediction algorithm inspired by a bidirectional long-term and short-term memory neural network (BILSTM) called H-BILSTM to predict the load utilization of MEC servers in future time slots. Second, we correspondingly propose an early warning mechanism based on the predicted load level that can quickly adjust the computing offloading strategy. The simulation results show that both the computing efficiency and prediction accuracy of H-BILSTM are better than those of several baseline prediction methods. In addition, the proposed warning scheme with the early warning unit effectively reduces the response time by 66.5 &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; compared with the baseline.},
  archive      = {J_TETC},
  author       = {Zhibo Li and Yurong Qian and Fengxiao Tang and Ming Zhao and Yusen Zhu},
  doi          = {10.1109/TETC.2022.3202266},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {253-264},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {H-BILSTM: A novel bidirectional long short term memory network based intelligent early warning scheme in mobile edge computing (MEC)},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A web back-end database leakage incident reconstruction
framework over unlabeled logs. <em>TETC</em>, <em>11</em>(1), 237–252.
(<a href="https://doi.org/10.1109/TETC.2022.3198080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a web back-end database leakage incident reconstruction framework (WeB-DLIR) over unlabeled logs, designed to improve the intelligence and automation of reconstructing web back-end database leakage incidents triggered by web-based attacks in unannotated logging environments. Using WeB-DLIR, analysts can reduce the manual workload of tracing and responding to data leakage incidents. Specifically, we first design web front-end and back-end anomaly identification methods based on neural network models with a pruning strategy and fine-grained grouping clustering analysis, respectively, for completely identifying web-related abnormal events in unlabeled logs. To remove redundant abnormal events and reduce subsequent inspection work for false alarm cases, we then propose an anomaly detection result decision fusion method (DFADR). Moreover, to visualize the attack chain reflected by abnormal events, based on the decision fusion results, we propose an attack graph modeling method that can reflect the basic process of data leakage from multiple perspectives. Finally, based on the modeling results, the topology of the data leakage scenario reconstruction can be completed by further auditing the relevant logs. Experimental results using real-world datasets show that the proposed WeB-DLIR is efficient and feasible for practical applications.},
  archive      = {J_TETC},
  author       = {Yanhua Liu and Zhihuang Liu and Ximeng Liu and Wenzhong Guo},
  doi          = {10.1109/TETC.2022.3198080},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {237-252},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A web back-end database leakage incident reconstruction framework over unlabeled logs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast multiple-precision integer division using intel
AVX-512. <em>TETC</em>, <em>11</em>(1), 224–236. (<a
href="https://doi.org/10.1109/TETC.2022.3196147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports on the implementation of a large integer division method that uses Intel Advanced Vector Extensions 512 (AVX-512), which is a 512-bit Single Instruction Multiple Data (SIMD) instruction set, and proposes a modification to a conventional division algorithm that makes it more SIMD instruction-friendly. More specifically, we use the Integer Fused Multiply-Add AVX-512 (AVX-512IFMA) subset, which is an instruction set that works well with large integer multiplication, to compute large integer divisions via a multiplication-based approach with a reciprocal. For the division process, we apply the most basic algorithm and divide-and-conquer methods and then use several techniques to compute efficiently with SIMD instructions in our implementation. We then combine these techniques and methods to implement our division function so that it can flexibly handle various sizes. To evaluate the performance of our proposed implementation, we executed our division program and the GNU Multiple Precision Arithmetic Library (GMP) on a Cannon Lake microarchitecture processor. A comparison of the execution times for our division program and GMP with various sizes showed that our method resulted in performance improvements of 25% to 35% on average, thus indicating that SIMD instructions are effective for fast arbitrary precision integer divisions.},
  archive      = {J_TETC},
  author       = {Takuya Edamatsu and Daisuke Takahashi},
  doi          = {10.1109/TETC.2022.3196147},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {224-236},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fast multiple-precision integer division using intel AVX-512},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling attribute-based access control in NoSQL databases.
<em>TETC</em>, <em>11</em>(1), 208–223. (<a
href="https://doi.org/10.1109/TETC.2022.3193577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NoSQL databases are being increasingly used for efficient management of high volumes of unstructured data in applications like information retrieval, natural language processing, social computing, etc. However, unlike traditional databases, data protection measures such as access control for these databases are still in their infancy, which could lead to significant vulnerabilities and security/privacy issues as their adoption increases. Attribute-based Access Control (ABAC), which provides a flexible and dynamic solution to access control, can be effective for mediating accesses in typical usage scenarios for NoSQL databases. In this paper, we propose a novel methodology for enabling ABAC in NoSQL databases. Specifically we consider MongoDB, which is one of the most popular NoSQL databases in use today. We present an approach to both specify ABAC access control policies and to enforce them when an actual access request has been made. MongoDB Wire Protocol is used for extracting and processing appropriate information from the requests. We also present a method for supporting dynamic access decisions using environmental attributes and handling of ad-hoc access requests through digitally signed user attributes. Results from an extensive set of experiments on the Enron corpus as well as on synthetically generated data demonstrate the scalability of our approach. Finally, we provide details of our implementation on MongoDB and share a Github repository so that any organization can download and deploy the same for enabling ABAC in their own MongoDB installations.},
  archive      = {J_TETC},
  author       = {Eeshan Gupta and Shamik Sural and Jaideep Vaidya and Vijayalakshmi Atluri},
  doi          = {10.1109/TETC.2022.3193577},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {208-223},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Enabling attribute-based access control in NoSQL databases},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A VLSI digital circuit platform for performing deterministic
stochastic computing in the time dimension using fraction operations on
rational numbers. <em>TETC</em>, <em>11</em>(1), 194–207. (<a
href="https://doi.org/10.1109/TETC.2022.3190011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic computing (SC) is a method of expressing values in streams of random bits and performing computation with bitwise operations on the streams. Recently, it was found that SC can be conducted on deterministic bitstreams where a numerical value is represented uniformly by sections of 1s and 0s. This leads to deterministic SC, where the computing result can be 100% accurate. However, at the current stage, its circuit implementation has not reached the level of practicality. Moreover, the expressible values for operands are theoretically limited. An all-digital frequency synthesizer, time-average-frequency direct period synthesis (TAF-DPS), is proposed here as a hardware platform for performing deterministic SC in the time dimension. The TAF-DPS is used as a rational-number-to-time converter (RNTC), transforming a rational number into an electrical waveform. Waveforms of unique frequencies and duty cycles are then generated according to the operand values. Instead of digital bitwise operations, each computation is carried out in the time regime on waveforms. Hence, it can be regarded as an “analog-like” logical operation. This RNTC-based time-domain operation opens up a subfield of SC: rational number stochastic computing (RN-SC). In RN-SC, fraction computation can be executed with logic gates, and the result is 100% accurate. The enabling factor is the improvement in resource utilization since the time dimension is exploited more efficiently. With this extra resource of time, expansion of the scope of SC is expected. In this article, the working principle is elucidated, and the circuit is discussed. Furthermore, a field-programmable gate array (FPGA) prototype is created to validate this platform for SC. The aim of this work is to create a practical tool for broadening the use of SC.},
  archive      = {J_TETC},
  author       = {Xiangye Wei and Liming Xiu},
  doi          = {10.1109/TETC.2022.3190011},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {194-207},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A VLSI digital circuit platform for performing deterministic stochastic computing in the time dimension using fraction operations on rational numbers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The indoor predictability of human mobility: Estimating
mobility with smart home sensors. <em>TETC</em>, <em>11</em>(1),
182–193. (<a href="https://doi.org/10.1109/TETC.2022.3188939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing human mobility patterns is valuable for understanding human behavior and providing location-anticipating services. In this work, we theoretically estimate the predictability of human movement for indoor settings, a problem that has not yet been tackled by the community. To validate the model, we utilize location data collected by ambient sensors in residential settings. The data support the model and allow us to contrast the predictability of various groups, including single-resident homes, homes with multiple residents, and homes with pets.},
  archive      = {J_TETC},
  author       = {Tinghui Wang and Diane J. Cook and Thomas R. Fischer},
  doi          = {10.1109/TETC.2022.3188939},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {182-193},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The indoor predictability of human mobility: Estimating mobility with smart home sensors},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EXSCALATE: An extreme-scale virtual screening platform for
drug discovery targeting polypharmacology to fight SARS-CoV-2.
<em>TETC</em>, <em>11</em>(1), 170–181. (<a
href="https://doi.org/10.1109/TETC.2022.3187134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social and economic impact of the COVID-19 pandemic demands a reduction of the time required to find a therapeutic cure. In this paper, we describe the EXSCALATE molecular docking platform capable to scale on an entire modern supercomputer for supporting extreme-scale virtual screening campaigns. Such virtual experiments can provide in short time information on which molecules to consider in the next stages of the drug discovery pipeline, and it is a key asset in case of a pandemic. The EXSCALATE platform has been designed to benefit from heterogeneous computation nodes and to reduce scaling issues. In particular, we maximized the accelerators’ usage, minimized the communications between nodes, and aggregated the I/O requests to serve them more efficiently. Moreover, we balanced the computation across the nodes by designing an ad-hoc workflow based on the execution time prediction of each molecule. We deployed the platform on two HPC supercomputers, with a combined computational power of 81 PFLOPS, to evaluate the interaction between 70 billion of small molecules and 15 binding-sites of 12 viral proteins of SARS-CoV-2. The experiment lasted 60 hours and it performed more than one trillion ligand-pocket evaluations, setting a new record on the virtual screening scale.},
  archive      = {J_TETC},
  author       = {Davide Gadioli and Emanuele Vitali and Federico Ficarelli and Chiara Latini and Candida Manelfi and Carmine Talarico and Cristina Silvano and Carlo Cavazzoni and Gianluca Palermo and Andrea Rosario Beccari},
  doi          = {10.1109/TETC.2022.3187134},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {170-181},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {EXSCALATE: An extreme-scale virtual screening platform for drug discovery targeting polypharmacology to fight SARS-CoV-2},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Use the spear as a shield: An adversarial example based
privacy-preserving technique against membership inference attacks.
<em>TETC</em>, <em>11</em>(1), 153–169. (<a
href="https://doi.org/10.1109/TETC.2022.3184408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent researches demonstrate that deep learning models are vulnerable to membership inference attacks. Few defenses have been proposed, but suffer from compromising the performance or quality of the target model, or cannot effectively resist membership inference attacks. This paper proposes an adversarial example based privacy-preserving technique (AEPPT), which adds crafted adversarial perturbations to the prediction of the target model to mislead the adversary&#39;s membership inference model. The added adversarial perturbations do not affect the accuracy of the target model, while can prevent the adversary from inferring whether a specific data is in the training set of the target model. Since AEPPT only modifies the original output of the target model, the proposed method does not require to modify or retrain the target model. Experimental results show that the proposed method can reduce the inference accuracy and precision of the membership inference model to around 50%, which is close to a random guess. The recall of the membership inference model drops from 88.24% to 6.48% on TinyImageNet dataset, drops from 98.5% to 17.1% on Purchase dataset, drops from 97.70% to 40.30% on ImageNet dataset, and drops from 88.7% to 51.9% on the CIFAR100 dataset, respectively. Besides, the performances of the proposed method under various factors (i.e., perturbation step size, number of adversary&#39;s data, proportion of member data and non-member data used to train substitute membership inference model, number of target model&#39;s output classes, and different membership inference models) are evaluated, which demonstrate that the proposed method can resist membership inference attacks under different conditions. Moreover, for those adaptive attacks where the adversary knows the defense mechanism, the proposed AEPPT is also demonstrated to be effective. Compared with the state-of-the-art defense methods, the proposed defense can significantly degrade the accuracy and precision of membership inference attacks to 50% (i.e., random guess), while the normal performance and utility of the target model are not affected.},
  archive      = {J_TETC},
  author       = {Mingfu Xue and Chengxiang Yuan and Can He and Yinghao Wu and Zhiyu Wu and Yushu Zhang and Zhe Liu and Weiqiang Liu},
  doi          = {10.1109/TETC.2022.3184408},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {153-169},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Use the spear as a shield: An adversarial example based privacy-preserving technique against membership inference attacks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing the performance of feature selection on regression
problems: A case study on older adults’ functional profile.
<em>TETC</em>, <em>11</em>(1), 137–152. (<a
href="https://doi.org/10.1109/TETC.2022.3181679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare systems are capable of collecting a significant number of patient health-related parameters. Analyzing them to find the reasons that cause a given disease is challenging. Feature Selection techniques have been used to address this issue—reducing these parameters to a smaller set with the most ”determinant” information. However, existing proposals usually focus on classification problems—aimed to detect whether a person is or is not suffering from an illness or from a finite set of illnesses. However, there are many situations in which health professionals need a numerical assessment to quantify the severity of an illness, thus dealing with a regression problem instead. Proposals using Feature Selection here are very limited. This paper examines several Feature Selection techniques to gauge their applicability to the regression-type problems, comparing these techniques by applying them to a real-life scenario on the functional profiles of older adults. Data from 829 functional profiles assessments in 49 residential homes were used in this study. The number of features was reduced from 31 to 25—with a correlation between inputs and outputs of 0.99 according to the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$R^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; score and a Mean Square Error (MSE) of 0.11—or to 14 features—with a correlation of 0.98 and MSE of 5.73.},
  archive      = {J_TETC},
  author       = {Javier Rojo and Lara Guedes de Pinho and César Fonseca and Manuel José Lopes and Sumi Helal and Juan Hernández and Jose Garcia-Alonso and Juan Manuel Murillo},
  doi          = {10.1109/TETC.2022.3181679},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {137-152},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Analyzing the performance of feature selection on regression problems: A case study on older adults’ functional profile},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tractable probabilistic approach to analyze sybil attacks
in sharding-based blockchain protocols. <em>TETC</em>, <em>11</em>(1),
126–136. (<a href="https://doi.org/10.1109/TETC.2022.3179638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain like Bitcoin and Ethereum suffer from scalability issues. Sharding is one of the most promising and leading solutions to scale blockchain. The basic idea behind sharding is to divide the blockchain network into multiple committees, where each processing a separate set of transactions, rather than the entire network processes all transactions. In this paper, we propose a probabilistic approach to analyze the security of sharding-based blockchain protocols. Based on this approach, we investigate the threat of Sybil attacks in these protocols. The key contribution of our paper is a tractable probabilistic approach to accurately compute the failure probability that at least one committee fails and ultimately compute the probability of a successful attack. To show the effectiveness of our approach, we conduct a numerical and comparative analysis of the proposed approach with existing approaches.},
  archive      = {J_TETC},
  author       = {Abdelatif Hafid and Abdelhakim Senhaji Hafid and Mustapha Samih},
  doi          = {10.1109/TETC.2022.3179638},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {126-136},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A tractable probabilistic approach to analyze sybil attacks in sharding-based blockchain protocols},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Targeting DNN inference via efficient utilization of
heterogeneous precision DNN accelerators. <em>TETC</em>, <em>11</em>(1),
112–125. (<a href="https://doi.org/10.1109/TETC.2022.3178730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern applications rely more and more on the simultaneous execution of multiple DNNs, and Heterogeneous DNN Accelerators (HDAs) prevail as a solution to this trend. In this work, we propose, implement, and evaluate low precision Neural Processing Units (NPUs) which serve as building blocks to construct HDAs, to address the efficient deployment of multi-DNN workloads. Moreover, we design and evaluate HDA designs that increase the overall throughput, while reducing the energy consumption during NN inference. At the design time , we implement HDAs inspired by the big.LITTLE computing paradigm, consisting of 8-bit NPUs together with lower precision bit-width NPUs. Additionally, an NN-to-NPU scheduling methodology is implemented to decide at run-time how to map the executed NN to the suitable NPU based on an accuracy drop threshold value. Our hardware/software co-design reduces the energy and response time of NNs by 29% and 10% respectively when compared to state-of-the-art homogeneous architectures. This comes with a negligible accuracy drop of merely 0.5%. Similar to the traditional CPU big.LITTLE, our asymmetric NPU design can open new doors for designing novel DNN accelerator architectures, due to their profound role in increasing the efficiency of DNNs with minimal losses in accuracy.},
  archive      = {J_TETC},
  author       = {Ourania Spantidi and Georgios Zervakis and Sami Alsalamin and Isai Roman-Ballesteros and Jörg Henkel and Hussam Amrouch and Iraklis Anagnostopoulos},
  doi          = {10.1109/TETC.2022.3178730},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {112-125},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Targeting DNN inference via efficient utilization of heterogeneous precision DNN accelerators},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Granularity-driven management for reliable and efficient
skyrmion racetrack memories. <em>TETC</em>, <em>11</em>(1), 95–111. (<a
href="https://doi.org/10.1109/TETC.2022.3171804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyrmion racetrack memory is a rising star of nonvolatile memories due to its outstanding access performance and storage density. However, the working principle of skyrmion racetrack memory introduces unique issues, such as the position error and data representation problem, which considerably impact the data reliability. Although brilliant error correction codes have been proposed to detect and correct bit errors, however, their time-consuming encoding and decoding processes cannot fully match the nanosecond-level access latency of skyrmion racetrack memory. Observing the dilemma among data reliability, access performance, and space utilization, we propose a granularity-driven management scheme for skyrmion racetrack memory. While eliminating the errors incurred due to the position error and data representation problem, the proposed management scheme aims to jointly optimize access performance and space utilization. To achieve this goal, the proposed scheme adaptively selects different combinations of data encoding, layout, and indexing schemes for the data of different granularities. Moreover, we investigate the port selection problem under our proposed data layouts to minimize shift overheads on data accesses. Through analytical and experimental studies, the proposed management scheme is evaluated, and the obtained results are quite encouraging.},
  archive      = {J_TETC},
  author       = {Yun-Shan Hsieh and Po-Chun Huang and Yuan-Hao Chang and Bo-Jun Chen and Wang Kang and Wei-Kuan Shih},
  doi          = {10.1109/TETC.2022.3171804},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {95-111},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Granularity-driven management for reliable and efficient skyrmion racetrack memories},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PlausMal-GAN: Plausible malware training based on generative
adversarial networks for analogous zero-day malware detection.
<em>TETC</em>, <em>11</em>(1), 82–94. (<a
href="https://doi.org/10.1109/TETC.2022.3170544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-day malicious software (malware) refers to a previously unknown or newly discovered software vulnerability. The fundamental objective of this paper is to enhance detection for analogous zero-day malware by efficient learning to plausible generated data. To detect zero-day malware, we proposed a malware training framework based on the generated analogous malware data using generative adversarial networks (PlausMal-GAN). Thus, the PlausMal-GAN can suitably produce analogous zero-day malware images with high quality and high diversity from the existing malware data. The discriminator, as a detector, learns various malware features using both real and generated malware images. In terms of performance, the proposed framework showed higher and more stable performances for the analogous zero-day malware images, which can be assumed to be analogous zero-day malware data. We obtained reliable accuracy performances in the proposed PlausMal-GAN framework with representative GAN models (i.e., deep convolutional GAN, least-squares GAN, Wasserstein GAN with gradient penalty, and evolutionary GAN). These results indicate that the use of the proposed framework is beneficial for the detection and prediction of numerous and analogous zero-day malware data from noted malware when developing and updating malware detection systems.},
  archive      = {J_TETC},
  author       = {Dong-Ok Won and Yong-Nam Jang and Seong-Whan Lee},
  doi          = {10.1109/TETC.2022.3170544},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {82-94},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PlausMal-GAN: Plausible malware training based on generative adversarial networks for analogous zero-day malware detection},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A catalog-based AIG-rewriting approach to the design of
approximate components. <em>TETC</em>, <em>11</em>(1), 70–81. (<a
href="https://doi.org/10.1109/TETC.2022.3170502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computational demand and energy efficiency of computer systems are becoming increasingly relevant requirements, traditional design paradigms are bound to become no longer appropriate, as they cannot guarantee significant improvements. The approximate-computing design paradigm has been introduced as a potential candidate to achieve better performances, by relaxing non-critical functional specifications. Anyway, several challenges need to be addressed in order to exploit its potential. In this paper, we propose a systematic and application-independent approximate design approach suitable to combinational logic circuits. Our approach is based on non-trivial local rewriting of and-inverter graphs (AIG), reducing the number of AIG-nodes and possibly resulting in lower hardware resources requirements. We adopt multi-objective optimization to carefully introduce approximation while aiming at optimal trade-offs between error and hardware-requirements. We evaluate our approach using different benchmarks, and, in order to measure actual gains, we perform actual synthesis of Pareto-optimal approximate configurations. Experimental results show that the proposed approach allows achieving significant savings, since resulting approximate circuits exhibit lower requirements and restrained error w.r.t. their exact couterparts.},
  archive      = {J_TETC},
  author       = {Mario Barbareschi and Salvatore Barone and Nicola Mazzocca and Alberto Moriconi},
  doi          = {10.1109/TETC.2022.3170502},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {70-81},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A catalog-based AIG-rewriting approach to the design of approximate components},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Engaging primary and secondary school students in computer
science through computational thinking training. <em>TETC</em>,
<em>11</em>(1), 56–69. (<a
href="https://doi.org/10.1109/TETC.2022.3163650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Computer Science has grown to become one of the most highly demanded professional careers, every year, only a small percentage of students choose a degree directly related to Computer Science. Perhaps the problem lies in the lack of information that society has about Computer Science itself, and particularly about the work computer scientists do. No one doubts the role of Mathematics or Languages as core subjects in every primary and secondary education syllabus; however, Computer Science plays a negligible role in most current syllabuses. Only in a few countries have governments paid special attention to content related to Computer Science and to learning to analyze and solve problems the way computer scientists do (Computational Thinking). In this article, we present Piens@ Computacion@ULLmente , a project that provides a methodology to promote Computer Science through Computational Thinking activities among primary and secondary education students. The results obtained from an exhaustive statistical analysis of the data we collected demonstrate that the perception of Computer Science that pre-university students have can be improved through specific training. Moreover, we can also confirm that the performance of pre-university students involving Computational Thinking skills is independent of gender, particularly at the primary education level.},
  archive      = {J_TETC},
  author       = {Rafael Herrero-Álvarez and Gara Miranda and Coromoto León and Eduardo Segredo},
  doi          = {10.1109/TETC.2022.3163650},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {56-69},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Engaging primary and secondary school students in computer science through computational thinking training},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WiSE: When learning assists resolving STT-MRAM efficiency
challenges. <em>TETC</em>, <em>11</em>(1), 43–55. (<a
href="https://doi.org/10.1109/TETC.2022.3163438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spin Transfer Torque Magnetic RAM (STT-MRAM) is one of the most promising on-chip technologies, which delivers high density, non-volatility, and near-zero leakage power. However, Spin Transfer Torque Magnetic RAM (STT-MRAM) suffers from three reliability issues, namely, read disturbance, write failure, and retention failure, that present significant challenges to its use as a reliable on-chip memory. All of these three reliability challenges become even more threatening with any increase in STT-MRAM cell temperature. Write operations are regarded as the main source of heat generation and temperature increase in STT-MRAM on-chip memories. This article first presents experiments to show how the heat generated by consecutive writes affects the reliability of an STT-MRAM on-chip cache. Then, it proposes the WiSE framework, an approach to reduce the STT-MRAM-based cache memory temperature and improve its reliability. WiSE utilizes the Reinforcement Learning (Reinforcement Learning (RL)) technique to detect high-density write operation patterns in STT-MRAM cache. To manage the write operations across the STT-MRAM caches, WiSE introduces a new temperature-aware replacement policy. The simulation results show that while WiSE imposes only about 1% performance overhead, it improves retention failure rate, read disturbance rate and write failure rate by 64%, 57%, and 47%, respectively, compared to Least Recently Used (Least Recently Used (LRU)) replacement policy.},
  archive      = {J_TETC},
  author       = {Arash Salahvarzi and Mohsen Khosroanjam and Amir Mahdi Hosseini Monazzah and Hakem Beitollahi and Umit Y. Ogras and Mahdi Fazeli},
  doi          = {10.1109/TETC.2022.3163438},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {43-55},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {WiSE: When learning assists resolving STT-MRAM efficiency challenges},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A technique for approximate communication in
network-on-chips for image classification. <em>TETC</em>,
<em>11</em>(1), 30–42. (<a
href="https://doi.org/10.1109/TETC.2022.3162165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation is an emerging design methodology for reducing power consumption and latency of on-chip communication in many computing applications. However, existing approximation techniques either achieve modest improvements in these metrics or require retraining after approximation. Since classifying many images introduces intensive on-chip communication, reductions in both network latency and power consumption are highly desired. In this paper, we propose an approximate communication technique (ACT) to improve the efficiency of on-chip communications for image classification applications. The proposed technique exploits the error-tolerance of the image classification process to reduce power consumption and latency of on-chip communications, resulting in better overall performance for image classification. This is achieved by incorporating novel quality control and data approximation mechanisms that reduce the packet size. In particular, the proposed quality control mechanisms identify the error-resilient variables and automatically adjust the error thresholds of the variables based on the image classification accuracy. The proposed data approximation mechanisms significantly reduce packet size when the variables are transmitted. The proposed technique reduces the number of flits in each data packet as well as the on-chip communication while maintaining an excellent image classification accuracy. Cycle-accurate simulation results show that ACT achieves 23% in network latency reduction and 24% in dynamic power reduction as compared to existing approximate communication techniques with less than 0.99% classification accuracy loss.},
  archive      = {J_TETC},
  author       = {Yuechen Chen and Shanshan Liu and Fabrizio Lombardi and Ahmed Louri},
  doi          = {10.1109/TETC.2022.3162165},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {30-42},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A technique for approximate communication in network-on-chips for image classification},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Quantum computing based optimization for intelligent
reflecting surface (IRS)-aided cell-free network. <em>TETC</em>,
<em>11</em>(1), 18–29. (<a
href="https://doi.org/10.1109/TETC.2022.3161542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent reflecting surface (IRS) enables the control of propagation characteristics and is attracting considerable attention as a technology to improve energy utilization efficiency in 6th generation mobile communication systems. As cell-free networks with multiple distributed base stations (BSs) can communicate in a coordinated manner, they are being actively researched as a new network architecture to resolve the problem of inter-cell interference in conventional cellular networks. The introduction of the IRS into the cell-free network can avoid shadowing at a lower cost with less power consumption. Thus, in this study, we considered the case of communication with user equipment (UE) in a shadowing environment using IRS in a cell-free network that contained distributed BSs with a single antenna. Moreover, the selection of multiple access methods was derived according to the numbers of BSs, IRSs, and UEs. In addition, we proposed a quadratic unconstrained binary optimization formulation to optimize the IRS reflection coefficient using quantum computing. The simulation results verified that the application of the proposed method resulted in a more efficient communication. Thus, this study clarifies that the optimum control method in every communication environment and aims to act as a stepping stone to optimize the entire cell-free system.},
  archive      = {J_TETC},
  author       = {Takahiro Ohyama and Yuichi Kawamoto and Nei Kato},
  doi          = {10.1109/TETC.2022.3161542},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {18-29},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quantum computing based optimization for intelligent reflecting surface (IRS)-aided cell-free network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-efficient domain-specific architecture for regular
expressions. <em>TETC</em>, <em>11</em>(1), 3–17. (<a
href="https://doi.org/10.1109/TETC.2022.3157948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular Expressions (REs) are a computational kernel widely used for finding patterns in data in compute-intensive tasks such as genomic markers research, signature-based detection, and database query. Although flexible on the set of searched REs, software-based solutions cannot fulfill latency or throughput requirements to analyze massive data volumes at a given power budget. For this reason, many approaches exploit hardware accelerators as an offloading engine for REs matching. Indeed, various solutions rely on FPGA reconfigurability to embed automata into the reconfigurable fabric. However, this approach leads to time-consuming updates of the REs to search. This work exploits REs as sequences of basic instructions and builds a Domain-Specific Architecture (DSA), called TiReX, for RE matching on FPGAs. Our approach enables the user to change the desired RE at run-time, providing software programmability, flexibility, and specialized hardware mechanisms. Our DSA delivers performance in line with other state-of-the-art hardware approaches, while providing remarkable flexibility and we underline the importance of energy efficiency for these computations. We compared with multiple state-of-the-art software obtaining remarkable performance while achieving noticeable results with a better energy efficiency that ranges from 3× to 490× with our multi-core.},
  archive      = {J_TETC},
  author       = {Davide Conficconi and Emanuele Del Sozzo and Filippo Carloni and Alessandro Comodi and Alberto Scolari and Marco Domenico Santambrogio},
  doi          = {10.1109/TETC.2022.3157948},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {3-17},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {An energy-efficient domain-specific architecture for regular expressions},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
