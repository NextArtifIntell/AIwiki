<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PIEEE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pieee---63">PIEEE - 63</h2>
<ul>
<li><details>
<summary>
(2023). Computational imaging and artificial intelligence: The next
revolution of mobile vision. <em>PIEEE</em>, <em>111</em>(12),
1607–1639. (<a
href="https://doi.org/10.1109/JPROC.2023.3338272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signal capture is at the forefront of perceiving and understanding the environment; thus, imaging plays a pivotal role in mobile vision. Recent unprecedented progress in artificial intelligence (AI) has shown great potential in the development of advanced mobile platforms with new imaging devices. Traditional imaging systems based on the “capturing images first and processing afterward” mechanism cannot meet this explosive demand. On the other hand, computational imaging (CI) systems are designed to capture high-dimensional data in an encoded manner to provide more information for mobile vision systems. Thanks to AI, CI can now be used in real-life systems by integrating deep learning algorithms into the mobile vision platform to achieve a closed loop of intelligent acquisition, processing, and decision-making, thus leading to the next revolution of mobile vision. Starting from the history of mobile vision using digital cameras, this work first introduces the advancement of CI in diverse applications and then conducts a comprehensive review of current research topics combining CI and AI. Although new-generation mobile platforms, represented by smart mobile phones, have deeply integrated CI and AI for better image acquisition and processing, most mobile vision platforms, such as self-driving cars and drones only loosely connect CI and AI, and are calling for a closer integration. Motivated by this fact, at the end of this work, we propose some potential technologies and disciplines that aid the deep integration of CI and AI and shed light on new directions in the future generation of mobile vision platforms.},
  archive      = {J_PIEEE},
  author       = {Jinli Suo and Weihang Zhang and Jin Gong and Xin Yuan and David J. Brady and Qionghai Dai},
  doi          = {10.1109/JPROC.2023.3338272},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1607-1639},
  shortjournal = {Proc. IEEE},
  title        = {Computational imaging and artificial intelligence: The next revolution of mobile vision},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on distributed training of graph
neural networks. <em>PIEEE</em>, <em>111</em>(12), 1572–1606. (<a
href="https://doi.org/10.1109/JPROC.2023.3337442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been demonstrated to be a powerful algorithmic model in broad application fields for their effectiveness in learning over graphs. To scale GNN training up for large-scale and ever-growing graphs, the most promising solution is distributed training that distributes the workload of training across multiple computing nodes. At present, the volume of related research on distributed GNN training is exceptionally vast, accompanied by an extraordinarily rapid pace of publication. Moreover, the approaches reported in these studies exhibit significant divergence. This situation poses a considerable challenge for newcomers, hindering their ability to grasp a comprehensive understanding of the workflows, computational patterns, communication strategies, and optimization techniques employed in distributed GNN training. As a result, there is a pressing need for a survey to provide correct recognition, analysis, and comparisons in this field. In this article, we provide a comprehensive survey of distributed GNN training by investigating various optimization techniques used in distributed GNN training. First, distributed GNN training is classified into several categories according to their workflows. In addition, their computational patterns and communication patterns, as well as the optimization techniques proposed by recent work, are introduced. Second, the software frameworks and hardware platforms of distributed GNN training are also introduced for a deeper understanding. Third, distributed GNN training is compared with distributed training of deep neural networks (DNNs), emphasizing the uniqueness of distributed GNN training. Finally, interesting issues and opportunities in this field are discussed.},
  archive      = {J_PIEEE},
  author       = {Haiyang Lin and Mingyu Yan and Xiaochun Ye and Dongrui Fan and Shirui Pan and Wenguang Chen and Yuan Xie},
  doi          = {10.1109/JPROC.2023.3337442},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1572-1606},
  shortjournal = {Proc. IEEE},
  title        = {A comprehensive survey on distributed training of graph neural networks},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A visionary look at the security of reconfigurable cloud
computing. <em>PIEEE</em>, <em>111</em>(12), 1548–1571. (<a
href="https://doi.org/10.1109/JPROC.2023.3330729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-programmable gate arrays (FPGAs) have become critical components in many cloud computing platforms. These devices possess the fine-grained parallelism and specialization needed to accelerate applications ranging from machine learning to networking and signal processing, among many others. Unfortunately, fine-grained programmability also makes FPGAs a security risk. Here, we review the current scope of attacks on cloud FPGAs and their remediation. Many of the FPGA security limitations are enabled by the shared power distribution network in FPGA devices. The simultaneous sharing of FPGAs is a particular concern. Other attacks on the memory, host microprocessor, and input/output channels are also possible. After examining current attacks, we describe trends in cloud architecture and how they are likely to impact possible future attacks. FPGA integration into cloud hypervisors and system software will provide extensive computing opportunities but invite new avenues of attack. We identify a series of system, software, and FPGA architectural changes that will facilitate improved security for cloud FPGAs and the overall systems in which they are located.},
  archive      = {J_PIEEE},
  author       = {Mirjana Stojilović and Kasper Rasmussen and Francesco Regazzoni and Mehdi B. Tahoori and Russell Tessier},
  doi          = {10.1109/JPROC.2023.3330729},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1548-1571},
  shortjournal = {Proc. IEEE},
  title        = {A visionary look at the security of reconfigurable cloud computing},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical tools and methodologies for ultrareliable
low-latency communication—a tutorial. <em>PIEEE</em>, <em>111</em>(11),
1502–1543. (<a
href="https://doi.org/10.1109/JPROC.2023.3328920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrareliable low-latency communication (URLLC) constitutes a key service class of the fifth generation (5G) and beyond cellular networks. Notably, designing and supporting URLLC pose a herculean task due to the fundamental need to identify and accurately characterize the underlying statistical models in which the system operates, e.g., interference statistics, channel conditions, and the behavior of protocols. In general, multilayer end-to-end approaches considering all the potential delay and error sources and proper statistical tools and methodologies are inevitably required for providing strong reliability and latency guarantees. This article contributes to the body of knowledge in the latter aspect by providing a tutorial on several statistical tools and methodologies that are useful for designing and analyzing URLLC systems. Specifically, we overview the frameworks related to the following: 1) reliability theory; 2) short packet communications; 3) inequalities, distribution bounds, and tail approximations; 4) rare-events simulation; 5) queuing theory and information freshness; and 6) large-scale tools, such as stochastic geometry, clustering, compressed sensing, and mean-field (MF) games. Moreover, we often refer to prominent data-driven algorithms within the scope of the discussed tools/methodologies. Throughout this article, we briefly review the state-of-the-art works using the addressed tools and methodologies, and their link to URLLC systems. Moreover, we discuss novel application examples focused on physical and medium access control layers. Finally, key research challenges and directions are highlighted to elucidate how URLLC analysis/design research may evolve in the coming years.},
  archive      = {J_PIEEE},
  author       = {Onel L. A. López and Nurul H. Mahmood and Mohammad Shehab and Hirley Alves and Osmel Martínez Rosabal and Leatile Marata and Matti Latva-Aho},
  doi          = {10.1109/JPROC.2023.3328920},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1502-1543},
  shortjournal = {Proc. IEEE},
  title        = {Statistical tools and methodologies for ultrareliable low-latency Communication—A tutorial},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep-learning-based 3-d surface reconstruction—a survey.
<em>PIEEE</em>, <em>111</em>(11), 1464–1501. (<a
href="https://doi.org/10.1109/JPROC.2023.3321433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, deep learning (DL) has significantly impacted industry and science. Initially largely motivated by computer vision tasks in 2-D imagery, the focus has shifted toward 3-D data analysis. In particular, 3-D surface reconstruction, i.e., reconstructing a 3-D shape from sparse input, is of great interest to a large variety of application fields. DL-based approaches show promising quantitative and qualitative surface reconstruction performance compared to traditional computer vision and geometric algorithms. This survey provides a comprehensive overview of these DL-based methods for 3-D surface reconstruction. To this end, we will first discuss input data modalities, such as volumetric data, point clouds, and RGB, single-view, multiview, and depth images, along with corresponding acquisition technologies and common benchmark datasets. For practical purposes, we also discuss evaluation metrics enabling us to judge the reconstructive performance of different methods. The main part of the document will introduce a methodological taxonomy ranging from point- and mesh-based techniques to volumetric and implicit neural approaches. Recent research trends, both methodological and for applications, are highlighted, pointing toward future developments.},
  archive      = {J_PIEEE},
  author       = {Anis Farshian and Markus Götz and Gabriele Cavallaro and Charlotte Debus and Matthias Nießner and Jón Atli Benediktsson and Achim Streit},
  doi          = {10.1109/JPROC.2023.3321433},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1464-1501},
  shortjournal = {Proc. IEEE},
  title        = {Deep-learning-based 3-D surface Reconstruction—A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ethical considerations on affective computing: An overview.
<em>PIEEE</em>, <em>111</em>(10), 1445–1458. (<a
href="https://doi.org/10.1109/JPROC.2023.3315217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective computing develops systems, which recognize or influence aspects of human life related to emotion, including feelings and attitudes. Significant potential for both good and harm makes it ethically sensitive, and trying to strike sound balances is challenging. Common images of the issues invite oversimplification and offer a limited understanding of the moral consequences and ethical tensions. Considering the state-of-the-art shows how pervasive and complex they are. In many areas, the discipline can potentially bring ethically significant benefits and hence has a duty to try. They include making interactions with machines more effective and less stressful, diagnostic and therapeutic roles in emotion-related disorders, intelligent tutoring, and reducing isolation. However, the limits of recognition technology mean that actions are likely to be based on impoverished representations of people’s affective state, particularly with certain groups; systems are liable to arouse feelings that are positive, but not well grounded in reality, affectively engaging systems can become addictive and manipulative, and they confer dangerous power on those who control the technology. We offer an overview of those and other particular ethical issues, positive and negative, which arise from the current state of affective computing. It aims to reflect the complexities inherent in both the technology and current ethical discussions. Establishing appropriate responses is a challenge for society as a whole, not only the affective computing community.},
  archive      = {J_PIEEE},
  author       = {Laurence Devillers and Roddy Cowie},
  doi          = {10.1109/JPROC.2023.3315217},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1445-1458},
  shortjournal = {Proc. IEEE},
  title        = {Ethical considerations on affective computing: An overview},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affective game computing: A survey. <em>PIEEE</em>,
<em>111</em>(10), 1423–1444. (<a
href="https://doi.org/10.1109/JPROC.2023.3315689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article surveys the current state-of-the-art in affective computing (AC) principles, methods, and tools as applied to games. We review this emerging field, namely affective game computing, through the lens of the four core phases of the affective loop: game affect elicitation, game affect sensing, game affect detection, and game affect adaptation. In addition, we provide a taxonomy of terms, methods, and approaches used across the four phases of the affective game loop and situate the field within this taxonomy. We continue with a comprehensive review of available affect data collection methods with regard to gaming interfaces, sensors, annotation protocols, and available corpora. This article concludes with a discussion on the current limitations of affective game computing and our vision for the most promising future research directions in the field.},
  archive      = {J_PIEEE},
  author       = {Georgios N. Yannakakis and David Melhart},
  doi          = {10.1109/JPROC.2023.3315689},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1423-1444},
  shortjournal = {Proc. IEEE},
  title        = {Affective game computing: A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Engagement detection and its applications in learning: A
tutorial and selective review. <em>PIEEE</em>, <em>111</em>(10),
1398–1422. (<a
href="https://doi.org/10.1109/JPROC.2023.3309560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engagement is critical to satisfaction and performance in a number of domains but is challenging to measure and sustain. Thus, there is considerable interest in developing affective computing technologies to automatically measure and enhance engagement, especially in the wild and at scale. This article provides an accessible introduction to affective computing research on engagement detection and enhancement using educational applications as an application domain. We begin with defining engagement as a multicomponential construct (i.e., a conceptual entity) situated within a context and bounded by time and review how the past six years of research has conceptualized it. Next, we examine traditional and affective computing methods for measuring engagement and discuss their relative strengths and limitations. Then, we move to a review of proactive and reactive approaches to enhancing engagement toward improving the learning experience and outcomes. We underscore key concerns in engagement measurement and enhancement, especially in digitally enhanced learning contexts, and conclude with several open questions and promising opportunities for future work.},
  archive      = {J_PIEEE},
  author       = {Brandon M. Booth and Nigel Bosch and Sidney K. D’Mello},
  doi          = {10.1109/JPROC.2023.3309560},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1398-1422},
  shortjournal = {Proc. IEEE},
  title        = {Engagement detection and its applications in learning: A tutorial and selective review},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social functions of machine emotional expressions.
<em>PIEEE</em>, <em>111</em>(10), 1382–1397. (<a
href="https://doi.org/10.1109/JPROC.2023.3261137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual humans and social robots frequently generate behaviors that human observers naturally see as expressing emotion. In this review article, we highlight that these expressions can have important benefits for human–machine interaction. We first summarize the psychological findings on how emotional expressions achieve important social functions in human relationships and highlight that artificial emotional expressions can serve analogous functions in human–machine interaction. We then review computational methods for determining what expressions make sense to generate within the context of interaction and how to realize those expressions across multiple modalities, such as facial expressions, voice, language, and touch. The use of synthetic expressions raises a number of ethical concerns, and we conclude with a discussion of principles to achieve the benefits of machine emotion in ethical ways.},
  archive      = {J_PIEEE},
  author       = {Celso M. de Melo and Jonathan Gratch and Stacy Marsella and Catherine Pelachaud},
  doi          = {10.1109/JPROC.2023.3261137},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1382-1397},
  shortjournal = {Proc. IEEE},
  title        = {Social functions of machine emotional expressions},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of affective speech synthesis and conversion in
the deep learning era. <em>PIEEE</em>, <em>111</em>(10), 1355–1381. (<a
href="https://doi.org/10.1109/JPROC.2023.3250266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human–computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. However, the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions—aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesized utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology that underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In this overview, we outline ongoing trends and summarize state-of-the-art approaches in an attempt to provide a broad overview of this exciting field.},
  archive      = {J_PIEEE},
  author       = {Andreas Triantafyllopoulos and Björn W. Schuller and Gökçe İymen and Metin Sezgin and Xiangheng He and Zijiang Yang and Panagiotis Tzirakis and Shuo Liu and Silvan Mertes and Elisabeth André and Ruibo Fu and Jianhua Tao},
  doi          = {10.1109/JPROC.2023.3250266},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1355-1381},
  shortjournal = {Proc. IEEE},
  title        = {An overview of affective speech synthesis and conversion in the deep learning era},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Touch technology in affective human–, robot–, and
virtual–human interactions: A survey. <em>PIEEE</em>, <em>111</em>(10),
1333–1354. (<a
href="https://doi.org/10.1109/JPROC.2023.3272780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the importance of affective touch in human interactions, technology designers are increasingly attempting to bring this modality to the core of interactive technology. Advances in haptics and touch-sensing technology have been critical to fostering interest in this area. In this survey, we review how affective touch is investigated to enhance and support the human experience with or through technology. We explore this question across three different research areas to highlight their epistemology, main findings, and the challenges that persist. First, we review affective touch technology through the human–computer interaction literature to understand how it has been applied to the mediation of human–human interaction and its roles in other human interactions particularly with oneself, augmented objects/media, and affect-aware devices. We further highlight the datasets and methods that have been investigated for automatic detection and interpretation of affective touch in this area. In addition, we discuss the modalities of affective touch expressions in both humans and technology in these interactions. Second, we separately review how affective touch has been explored in human–robot and real-human–virtual-human interactions where the technical challenges encountered and the types of experience aimed at are different. We conclude with a discussion of the gaps and challenges that emerge from the review to steer research in directions that are critical for advancing affective touch technology and recognition systems. In our discussion, we also raise ethical issues that should be considered for responsible innovation in this growing area.},
  archive      = {J_PIEEE},
  author       = {Temitayo Olugbade and Liang He and Perla Maiolino and Dirk Heylen and Nadia Bianchi-Berthouze},
  doi          = {10.1109/JPROC.2023.3272780},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1333-1354},
  shortjournal = {Proc. IEEE},
  title        = {Touch technology in affective human–, robot–, and Virtual–Human interactions: A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affective brain–computer interfaces (aBCIs): A tutorial.
<em>PIEEE</em>, <em>111</em>(10), 1314–1332. (<a
href="https://doi.org/10.1109/JPROC.2023.3277471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain–computer interface (BCI) enables a user to communicate directly with a computer using only the central nervous system. An affective BCI (aBCI) monitors and/or regulates the emotional state of the brain, which could facilitate human cognition, communication, decision-making, and health. The last decade has witnessed rapid progress in aBCI research and applications, but there does not exist a comprehensive and up-to-date tutorial on aBCIs. This tutorial fills the gap. It introduces first the basic concepts of BCIs and then, in detail, the individual components in a closed-loop aBCI system, including signal acquisition, signal processing, feature extraction, emotion recognition, and brain stimulation. Next, it describes three representative applications of aBCIs, i.e., cognitive workload recognition, fatigue estimation, and depression diagnosis and treatment. Several challenges and opportunities in aBCI research and applications, including brain signal acquisition, emotion labeling, diversity and size of aBCI datasets, algorithm comparison, negative transfer in emotion recognition, and privacy protection and security of aBCIs, are also explained.},
  archive      = {J_PIEEE},
  author       = {Dongrui Wu and Bao-Liang Lu and Bin Hu and Zhigang Zeng},
  doi          = {10.1109/JPROC.2023.3277471},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1314-1332},
  shortjournal = {Proc. IEEE},
  title        = {Affective Brain–Computer interfaces (aBCIs): A tutorial},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approaches, applications, and challenges in physiological
emotion recognition—a tutorial overview. <em>PIEEE</em>,
<em>111</em>(10), 1287–1313. (<a
href="https://doi.org/10.1109/JPROC.2023.3286445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An automatic emotion recognition system can serve as a fundamental framework for various applications in daily life from monitoring emotional well-being to improving the quality of life through better emotion regulation. Understanding the process of emotion manifestation becomes crucial for building emotion recognition systems. An emotional experience results in changes not only in interpersonal behavior but also in physiological responses. Physiological signals are one of the most reliable means for recognizing emotions since individuals cannot consciously manipulate them for a long duration. These signals can be captured by medical-grade wearable devices, as well as commercial smart watches and smart bands. With the shift in research direction from laboratory to unrestricted daily life, commercial devices have been employed ubiquitously. However, this shift has introduced several challenges, such as low data quality, dependency on subjective self-reports, unlimited movement-related changes, and artifacts in physiological signals. This tutorial provides an overview of practical aspects of emotion recognition, such as experiment design, properties of different physiological modalities, existing datasets, suitable machine learning algorithms for physiological data, and several applications. It aims to provide the necessary psychological and physiological backgrounds through various emotion theories and the physiological manifestation of emotions, thereby laying a foundation for emotion recognition. Finally, the tutorial discusses open research directions and possible solutions.},
  archive      = {J_PIEEE},
  author       = {Yekta Said Can and Bhargavi Mahesh and Elisabeth André},
  doi          = {10.1109/JPROC.2023.3286445},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1287-1313},
  shortjournal = {Proc. IEEE},
  title        = {Approaches, applications, and challenges in physiological emotion Recognition—A tutorial overview},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unlocking the emotional world of visual media: An overview
of the science, research, and impact of understanding emotion.
<em>PIEEE</em>, <em>111</em>(10), 1236–1286. (<a
href="https://doi.org/10.1109/JPROC.2023.3273517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of “emotion,” coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a “Holy Grail” research problem in computing and delineate pivotal directions for future inquiry. Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.},
  archive      = {J_PIEEE},
  author       = {James Z. Wang and Sicheng Zhao and Chenyan Wu and Reginald B. Adams and Michelle G. Newman and Tal Shafir and Rachelle Tsachor},
  doi          = {10.1109/JPROC.2023.3273517},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1236-1286},
  shortjournal = {Proc. IEEE},
  title        = {Unlocking the emotional world of visual media: An overview of the science, research, and impact of understanding emotion},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial micro-expressions: An overview. <em>PIEEE</em>,
<em>111</em>(10), 1215–1235. (<a
href="https://doi.org/10.1109/JPROC.2023.3275192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression (ME) is an involuntary, fleeting, and subtle facial expression. It may occur in high-stake situations when people attempt to conceal or suppress their true feelings. Therefore, MEs can provide essential clues to people’s true feelings and have plenty of potential applications, such as national security, clinical diagnosis, and interrogations. In recent years, ME analysis has gained much attention in various fields due to its practical importance, especially automatic ME analysis in computer vision as MEs are difficult to process by naked eyes. In this survey, we provide a comprehensive review of ME development in the field of computer vision, from the ME studies in psychology and early attempts in computer vision to various computational ME analysis methods and future directions. Four main tasks in ME analysis are specifically discussed, including ME spotting, ME recognition, ME action unit detection, and ME generation in terms of the approaches, advance developments, and challenges. Through this survey, readers can understand MEs in both aspects of psychology and computer vision, and apprehend the future research direction in ME analysis.},
  archive      = {J_PIEEE},
  author       = {Guoying Zhao and Xiaobai Li and Yante Li and Matti Pietikäinen},
  doi          = {10.1109/JPROC.2023.3275192},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1215-1235},
  shortjournal = {Proc. IEEE},
  title        = {Facial micro-expressions: An overview},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward robust facial action units’ detection.
<em>PIEEE</em>, <em>111</em>(10), 1198–1214. (<a
href="https://doi.org/10.1109/JPROC.2023.3257542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action unit (AU) detection plays an important role in performing facial behavioral analysis of raw video inputs. Overall, there are three key factors that contribute toward the optimal performance of AU detectors: 1) being able to capture local AU-centered features; 2) exploiting the fact that some AUs co-occur with others; and 3) utilizing appearance changes across frames. We briefly review current techniques addressing each factor and discuss the challenges they meet. Given that very few works consider how to effectively and efficiently merge them all into a single framework that can be trained in an end-to-end manner, we propose facial AU detection with face alignment(AUNet), a simple yet strong baseline for landmark-based AU detection. AUNet implements the abovementioned key factors by: 1) using the intermediate layers of a pretrained face alignment model to act as our AU features’ space; 2) optimized to satisfy a correlation constraint, derived from the AU labels, and 3) temporal constraint, derived from variations in the contents of consecutive frames in the input videos. The proposed model, with its three key components, remains simple in nature and aligns with the primary AU detection task. Experiments on several benchmarks show that it substantially improves the AU detector’s accuracy and achieves new state-of-the-art AU detection results on popular benchmarks: BP4D and DISFA. Code is available at https://github.com/jingyang2017/AU-Net .},
  archive      = {J_PIEEE},
  author       = {Jing Yang and Yordan Hristov and Jie Shen and Yiming Lin and Maja Pantic},
  doi          = {10.1109/JPROC.2023.3257542},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1198-1214},
  shortjournal = {Proc. IEEE},
  title        = {Toward robust facial action units’ detection},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward label-efficient emotion and sentiment analysis.
<em>PIEEE</em>, <em>111</em>(10), 1159–1197. (<a
href="https://doi.org/10.1109/JPROC.2023.3309299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion and sentiment play a central role in various human activities, such as perception, decision-making, social interaction, and logical reasoning. Developing artificial emotional intelligence (AEI) for machines is becoming a bottleneck in human–computer interaction. The first step of AEI is to recognize the emotion and sentiment that are conveyed in different affective signals. Traditional supervised emotion and sentiment analysis (ESA) methods, especially deep learning-based ones, usually require large-scale labeled training data. However, due to the essential subjectivity, complexity, uncertainty and ambiguity, and subtlety, collecting such annotations is expensive, time-consuming, and difficult in practice. In this article, we introduce label-efficient ESA from the computational perspective. First, we present a hierarchical taxonomy for label-efficient learning based on the availability of sample labels, emotion categories, and data domains during training. Second, for each of the seven paradigms, i.e., unsupervised, semisupervised, weakly supervised, low-shot, incremental, domain-adaptive, and domain-generalizable ESA, we give the definition, summarize existing methods, and present our views on the quantitative and qualitative comparison. Finally, we provide several promising real-world applications, followed by unsolved challenges and potential future directions.},
  archive      = {J_PIEEE},
  author       = {Sicheng Zhao and Xiaopeng Hong and Jufeng Yang and Yanyan Zhao and Guiguang Ding},
  doi          = {10.1109/JPROC.2023.3309299},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1159-1197},
  shortjournal = {Proc. IEEE},
  title        = {Toward label-efficient emotion and sentiment analysis},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An engineering view on emotions and speech: From analysis
and predictive models to responsible human-centered applications.
<em>PIEEE</em>, <em>111</em>(10), 1142–1158. (<a
href="https://doi.org/10.1109/JPROC.2023.3276209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The substantial growth of Internet-of-Things technology and the ubiquity of smartphone devices has increased the public and industry focus on speech emotion recognition (SER) technologies. Yet, conceptual, technical, and societal challenges restrict the wide adoption of these technologies in various domains, including, healthcare, and education. These challenges are amplified when automated emotion recognition systems are called to function “in-the-wild” due to the inherent complexity and subjectivity of human emotion, the difficulty of obtaining reliable labels at high temporal resolution, and the diverse contextual and environmental factors that confound the expression of emotion in real life. In addition, societal and ethical challenges hamper the wide acceptance and adoption of these technologies, with the public raising questions about user privacy, fairness, and explainability. This article briefly reviews the history of affective speech processing, provides an overview of current state-of-the-art approaches to SER, and discusses algorithmic approaches to render these technologies accessible to all, maximizing their benefits and leading to responsible human-centered computing applications.},
  archive      = {J_PIEEE},
  author       = {Chi-Chun Lee and Theodora Chaspari and Emily Mower Provost and Shrikanth S. Narayanan},
  doi          = {10.1109/JPROC.2023.3276209},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1142-1158},
  shortjournal = {Proc. IEEE},
  title        = {An engineering view on emotions and speech: From analysis and predictive models to responsible human-centered applications},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Affective computing [scanning the issue]. <em>PIEEE</em>,
<em>111</em>(10), 1139–1141. (<a
href="https://doi.org/10.1109/JPROC.2023.3318028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special issue cover four major subfields in affective computing, namely affect analysis, affect synthesis, applications, and ethics.},
  archive      = {J_PIEEE},
  author       = {Björn W. Schuller and Matti Pietikäinen},
  doi          = {10.1109/JPROC.2023.3318028},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1139-1141},
  shortjournal = {Proc. IEEE},
  title        = {Affective computing [Scanning the issue]},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trusted AI in multiagent systems: An overview of privacy and
security for distributed learning. <em>PIEEE</em>, <em>111</em>(9),
1097–1132. (<a
href="https://doi.org/10.1109/JPROC.2023.3306773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the advancing computational capacity of distributed end-user equipment (UE), as well as the increasing concerns about sharing private data, there has been considerable recent interest in machine learning (ML) and artificial intelligence (AI) that can be processed on distributed UEs. Specifically, in this paradigm, parts of an ML process are outsourced to multiple distributed UEs. Then, the processed information is aggregated on a certain level at a central server, which turns a centralized ML process into a distributed one and brings about significant benefits. However, this new distributed ML paradigm raises new risks in terms of privacy and security issues. In this article, we provide a survey of the emerging security and privacy risks of distributed ML from a unique perspective of information exchange levels, which are defined according to the key steps of an ML process, i.e., we consider the following levels: 1) the level of preprocessed data; 2) the level of learning models; 3) the level of extracted knowledge; and 4) the level of intermediate results. We explore and analyze the potential of threats for each information exchange level based on an overview of current state-of-the-art attack mechanisms and then discuss the possible defense methods against such threats. Finally, we complete the survey by providing an outlook on the challenges and possible directions for future research in this critical area.},
  archive      = {J_PIEEE},
  author       = {Chuan Ma and Jun Li and Kang Wei and Bo Liu and Ming Ding and Long Yuan and Zhu Han and H. Vincent Poor},
  doi          = {10.1109/JPROC.2023.3306773},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1097-1132},
  shortjournal = {Proc. IEEE},
  title        = {Trusted AI in multiagent systems: An overview of privacy and security for distributed learning},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for smart grid operations:
Algorithms, applications, and prospects. <em>PIEEE</em>,
<em>111</em>(9), 1055–1096. (<a
href="https://doi.org/10.1109/JPROC.2023.3303358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing penetration of renewable energy and flexible loads in smart grids, a more complicated power system with high uncertainty is gradually formed, which brings about great challenges to smart grid operations. Traditional optimization methods usually require accurate mathematical models and parameters and cannot deal well with the growing complexity and uncertainty. Fortunately, the widespread popularity of advanced meters makes it possible for smart grid to collect massive data, which offers opportunities for data-driven artificial intelligence methods to address the optimal operation and control issues. Therein, deep reinforcement learning (DRL) has attracted extensive attention for its excellent performance in operation problems with high uncertainty. To this end, this article presents a comprehensive literature survey on DRL and its applications in smart grid operations. First, a detailed overview of DRL, from fundamental concepts to advanced models, is conducted in this article. Afterward, we review various DRL techniques as well as their extensions developed to cope with emerging issues in the smart grid, including optimal dispatch, operational control, electricity market, and other emerging areas. In addition, an application-oriented survey of DRL in smart grid is presented to identify difficulties for future research. Finally, essential challenges, potential solutions, and future research directions concerning the DRL applications in smart grid are also discussed.},
  archive      = {J_PIEEE},
  author       = {Yuanzheng Li and Chaofan Yu and Mohammad Shahidehpour and Tao Yang and Zhigang Zeng and Tianyou Chai},
  doi          = {10.1109/JPROC.2023.3303358},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1055-1096},
  shortjournal = {Proc. IEEE},
  title        = {Deep reinforcement learning for smart grid operations: Algorithms, applications, and prospects},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training spiking neural networks using lessons from deep
learning. <em>PIEEE</em>, <em>111</em>(9), 1016–1054. (<a
href="https://doi.org/10.1109/JPROC.2023.3308088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This article serves as a tutorial and perspective showing how to apply the lessons learned from several decades of research in deep learning, gradient descent, backpropagation, and neuroscience to biologically plausible spiking neural networks (SNNs). We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to SNNs; the subtle link between temporal backpropagation and spike timing-dependent plasticity; and how deep learning might move toward biologically plausible online learning. Some ideas are well accepted and commonly used among the neuromorphic engineering community, while others are presented or justified for the first time here. A series of companion interactive tutorials complementary to this article using our Python package, snnTorch, are also made available: https://snntorch.readthedocs.io/en/latest/tutorials/index.html .},
  archive      = {J_PIEEE},
  author       = {Jason K. Eshraghian and Max Ward and Emre O. Neftci and Xinxin Wang and Gregor Lenz and Girish Dwivedi and Mohammed Bennamoun and Doo Seok Jeong and Wei D. Lu},
  doi          = {10.1109/JPROC.2023.3308088},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1016-1054},
  shortjournal = {Proc. IEEE},
  title        = {Training spiking neural networks using lessons from deep learning},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Microwave metalens antennas. <em>PIEEE</em>,
<em>111</em>(8), 978–1010. (<a
href="https://doi.org/10.1109/JPROC.2023.3287599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been growing interest in the use of metamaterial (MTM)-based lenses, also known as metalenses, as innovative antenna technology. Increasingly widespread applications of metalenses in modern microwave communication and sensing systems have been found, following the development of the first microwave artificial lens in the 1940s based on the concept of an artificial dielectric, which was later broadly termed an “MTM. ” This article examines the evolution of metalens antennas over the past 80 years and introduces the principles and technologies underlying their design. It then focuses on the latest progress in the research on and applications of MTMs and metasurface (MTS)-based metalens antennas. The principles and basic structures of transmissive focusing metalens antennas in the microwave band are elaborated. Selected metalens antennas are introduced chronologically, starting with metallic waveguide lens antennas, followed by metallic or metal–dielectric Fresnel zone plate lens antennas, transmitarray lens antennas, MTS lens antennas, and flat transformation-optics-based MTM Luneburg lens antennas. The technical merits, challenges, applications, trends, and future research of each type of metalens antenna are also addressed. The information presented in this article will benefit the research, development, and application of metalens antennas in fifth- and beyond-generation communications, Wi-Fi 6 short-range connections, and next-generation microwave sensing and imaging systems.},
  archive      = {J_PIEEE},
  author       = {Zhi Ning Chen and Teng Li and Xianming Qing and Jin Shi and Shunli Li and Yuanyan Su and Wei E. I. Liu and Chunhua Xue and Qun Lou and Zhi Hao Jiang and Ruolei Xu and Peiqin Liu and Huiwen Sheng},
  doi          = {10.1109/JPROC.2023.3287599},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {978-1010},
  shortjournal = {Proc. IEEE},
  title        = {Microwave metalens antennas},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Circuits and antennas incorporating gallium-based liquid
metal. <em>PIEEE</em>, <em>111</em>(8), 955–977. (<a
href="https://doi.org/10.1109/JPROC.2023.3285400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews the application and technology advancement of gallium (Ga)-based liquid metal (LM) in high-frequency circuits and antennas. It discusses the material properties of common LMs, the fluidic channels used to contain LM and their manufacturing techniques, and the actuation techniques, which are all critical for the design and implementation of LM-based devices. LM’s fluidic and pliable nature, together with its excellent electrical, thermal, and rheological (i.e., fluid flow) properties, provides some unique and innovative solutions to flexible/wearable electronics, reconfigurable circuits, and antennas. This article provides a comprehensive review of a wide range of LM-enabled high-frequency circuits and antennas, including interconnects and transitions, reconfigurable passive circuits (such as resonators, filters, and couplers), switches, phase shifters, reconfigurable antennas, flexible and wearable antennas, and metamaterials (i.e., periodic materials with properties not found in nature). This article presents various design concepts and implementation techniques, highlights key capabilities, and discusses the challenges and opportunities with the use of Ga-based LM materials.},
  archive      = {J_PIEEE},
  author       = {Yi-Wen Wu and Shaker Alkaraki and Shi-Yang Tang and Yi Wang and James R. Kelly},
  doi          = {10.1109/JPROC.2023.3285400},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {955-977},
  shortjournal = {Proc. IEEE},
  title        = {Circuits and antennas incorporating gallium-based liquid metal},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coexistence and spectrum sharing above 100 GHz.
<em>PIEEE</em>, <em>111</em>(8), 928–954. (<a
href="https://doi.org/10.1109/JPROC.2023.3286172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electromagnetic spectrum plays a fundamental role in the development of the digital society. It enables wireless communications (either between humans or machines) and sensing (for example, for Earth exploration, radio astronomy, imaging, and radars). While each of these uses benefits from a larger bandwidth, the spectrum is a finite resource. This introduces competing interests among the different stakeholders of the spectrum, which have led—so far—to rigid policies and spectrum allocations. Recently, the spectrum crunch in the sub-6-GHz bands has prompted communication technologies to move to higher carrier frequencies, where future sixth-generation (6G) wireless networks can exploit theoretically very large bandwidths. However, the spectrum above 100 GHz features several narrow, yet numerous subbands that are exclusively allocated for passive sensing applications, e.g., for climate and weather monitoring. This prevents the allocation of large contiguous bands to active users of the spectrum, either being communications (which need tens of gigahertz of bandwidth to target terabit-per-second links) or radars. This article explores how spectrum policy and spectrum technologies can evolve to enable sharing among different stakeholders in the above 100-GHz spectrum, without introducing harmful interference or disrupting either security applications or fundamental science exploration. This portion of the spectrum presents new challenges and opportunities for the design of spectrum sharing schemes, including higher spreading and absorption losses, extremely directional antenna technologies, and ultrahigh data-rate communications, among others. This article provides a tutorial on current regulations above 100 GHz and highlights how sharing is central to allowing each stakeholder to make the most out of this spectrum. It then defines—through detailed simulations based on standard International Telecommunications Union (ITU) channel and antenna models—scenarios in which active users may introduce harmful interference to passive sensing. Based on this evaluation, it reviews a number of promising techniques that can enable active/passive sharing above 100 GHz. The critical review and tutorial on policy and technologies of this article have the potential to kickstart future research and regulations that promote safe coexistence between active and passive users above 100 GHz, further benefiting the development of digital technologies and scientific exploration.},
  archive      = {J_PIEEE},
  author       = {Michele Polese and Xavier Cantos-Roman and Arjun Singh and Michael J. Marcus and Thomas J. Maccarone and Tommaso Melodia and Josep Miquel Jornet},
  doi          = {10.1109/JPROC.2023.3286172},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {928-954},
  shortjournal = {Proc. IEEE},
  title        = {Coexistence and spectrum sharing above 100 GHz},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethink physical security: Protecting vehicles via
battery-enabled sensing and control [point of view]. <em>PIEEE</em>,
<em>111</em>(8), 921–927. (<a
href="https://doi.org/10.1109/JPROC.2023.3285166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyberization is the foundation of vehicle electrification and automation, requiring the deployment of ever-increasing on-board sensing, communication, and computing services. However, vehicle cyberization also introduces new cyber vulnerabilities. In this article, we discuss the opportunities and challenges of using the common 12/24V automotive batteries as sensors and actuators to provide vehicles with three-pronged physical security protection: driver authentication, vehicle access control, and vehicle intrusion detection.},
  archive      = {J_PIEEE},
  author       = {Liang He and Kang G. Shin},
  doi          = {10.1109/JPROC.2023.3285166},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {921-927},
  shortjournal = {Proc. IEEE},
  title        = {Rethink physical security: Protecting vehicles via battery-enabled sensing and control [Point of view]},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling 100% renewable power systems through power
electronic grid-forming converter and control: System integration for
security, stability, and application to europe. <em>PIEEE</em>,
<em>111</em>(7), 891–915. (<a
href="https://doi.org/10.1109/JPROC.2022.3193374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In accordance with European plans, dynamic controls are developed to allow for system-wide integration of 100\% renewable energy sources (RESs) interfaced through power electronic converters while maintaining security. At the heart of the development is the concept of the grid-forming resource (GFR) which brings together both the technologies of renewable energy resource and grid-forming converter. Extending on an extensive review, a grid-forming converter control and protection scheme is presented to ensure operation under small- and large-signal transients as needed for overall system security. The scheme is shown to be practical in offering inertial response emulation and frequency control based on droop characteristics to maintain power balances rapidly, to control voltages, as well as to support transient stability enhancement under grid faults. The integration with the control of wind energy conversion systems (WECSs) creates a grid-forming wind park as the prototype of the GFR. Feedforward signals exchanged between grid and resource-side controls enhance fast overall controllability. The principal claims are substantiated based on a scenario developed in the European project initiative MIGRATE with a focus on the Irish power transmission system. The model comprises more than 2000 individual WECSs grouped into wind parks, where ten wind parks are GFRs. The transient behavior of this scenario comprising 100\% converter-interfaced generation is shown to be superior compared with a counterpart case comprising synchronous machinery. The results validate the fact that GFRs with their proposed controls are expected to be key elements in creating a renewable and secure electric power system.},
  archive      = {J_PIEEE},
  author       = {Kai Strunz and Khaled Almunem and Christoph Wulkow and Maren Kuschke and Marta Valescudero and Xavier Guillaud},
  doi          = {10.1109/JPROC.2022.3193374},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {891-915},
  shortjournal = {Proc. IEEE},
  title        = {Enabling 100\% renewable power systems through power electronic grid-forming converter and control: System integration for security, stability, and application to europe},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disturbance propagation in power grids with high converter
penetration. <em>PIEEE</em>, <em>111</em>(7), 873–890. (<a
href="https://doi.org/10.1109/JPROC.2022.3173813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High penetration of converter-interfaced renewable energy resources will significantly change the swing dynamics between synchronous generators (SGs) in future power systems. This article examines the impact of high converter penetration on wave-like disturbance propagation arising from sudden generator and load losses in radial (1-D) and meshed (2-D) power systems. To keep the uniformity assumption as converters are introduced, the rating of each SG is decreased with a converter resource making up for the reduction. Numerical simulations demonstrate that as the penetration level of constant-power grid-following (GFL) converters increases, the speed of disturbance propagation increases due to the reduced system inertia. Naturally, converters with the capabilities to positively respond to disturbances would in turn reduce the propagation speed. Analytical studies based on continuum models are presented for the 2-D system with SGs and constant-power GFL converters in order to visualize the disturbance propagation and validate numerical simulations based on differential-algebraic equations. In addition, fast active power control of converters can slow down the electromechanical wave (EMW) propagation and even contain it. These concepts are illustrated on the idealized radial and meshed systems and a reduced model of the U.S. eastern interconnection.},
  archive      = {J_PIEEE},
  author       = {Hantao Cui and Stavros Konstantinopoulos and Denis Osipov and Jinning Wang and Fangxing Li and Kevin L. Tomsovic and Joe H. Chow},
  doi          = {10.1109/JPROC.2022.3173813},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {873-890},
  shortjournal = {Proc. IEEE},
  title        = {Disturbance propagation in power grids with high converter penetration},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic performance modeling and analysis of power grids
with high levels of stochastic and power electronic interfaced
resources. <em>PIEEE</em>, <em>111</em>(7), 854–872. (<a
href="https://doi.org/10.1109/JPROC.2023.3284890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines the emerging challenges in modeling and analyzing the electric power system due to the widespread growth of variable renewable energy (VRE), particularly in the form of distributed energy resources (DERs), which are displacing traditional large power plants. Many of these resources are connected to the system through power electronic interfaces, also known as inverter-based resources (IBRs), which are reshaping the system dynamics and lowering the grid strength and inertia. Understanding the dynamic behavior of the power system should be critical to addressing the potential stability concerns, refining the grid requirements, and developing effective and reliable measures among many alternatives. However, conventional methodologies for resource integration and network expansion studies, as well as application-specific electromagnetic transient (EMT) studies, need to be improved. This article thus presents recent academic and industrial efforts to advance the existing approaches, especially by incorporating the uncertainty in model parameters of DERs, variability of VRE, and EMT dynamics of IBRs for the grid planning and operations studies such as the impact of DERs on load modeling and system-wide dynamic performance. In addition, this article showcases recent developments to expand the study boundaries by synergizing the strengths of the industry-accepted approaches along with real system studies for Korea’s electric power systems in particular.},
  archive      = {J_PIEEE},
  author       = {Jae-Kyeong Kim and Jiseong Kang and Jae Woong Shim and Heejin Kim and Jeonghoon Shin and Chongqing Kang and Kyeon Hur},
  doi          = {10.1109/JPROC.2023.3284890},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {854-872},
  shortjournal = {Proc. IEEE},
  title        = {Dynamic performance modeling and analysis of power grids with high levels of stochastic and power electronic interfaced resources},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power system stability with a high penetration of
inverter-based resources. <em>PIEEE</em>, <em>111</em>(7), 832–853. (<a
href="https://doi.org/10.1109/JPROC.2022.3179826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverter-based resources (IBRs) possess dynamics that are significantly different from those of synchronous-generator-based sources and as IBR penetrations grow the dynamics of power systems are changing. This article discusses the characteristics of the new dynamics and examines how they can be accommodated into the long-standing categorizations of power system stability in terms of angle, frequency, and voltage stability. It is argued that inverters are causing the frequency range over which angle, frequency, and voltage dynamics act to extend such that the previously partitioned categories are now coupled and further coupled to new electromagnetic modes. While grid-forming (GFM) inverters share many characteristics with generators, grid-following (GFL) inverters are different. This is explored in terms of similarities and differences in synchronization, inertia, and voltage control. The concept of duality is used to unify the synchronization principles of GFM and GFL inverters and, thus, established the generalized angle dynamics. This enables the analytical study of GFM-GFL interaction, which is particularly important to guide the placement of GFM apparatuses and is even more important if GFM inverters are allowed to fall back to the GFL mode during faults to avoid oversizing to support short-term overload. Both GFL and GFM inverters contribute to voltage strength but with marked differences, which implies new features of voltage stability. Several directions for further research are identified, including: 1) extensions of nonlinear stability analysis to accommodate new inverter behaviors with cross-coupled time frames; 2) establishment of spatial–temporal indices of system strength and stability margin to guide the provision of new stability services; and 3) data-driven approaches to combat increased system complexity and confidentiality of inverter models.},
  archive      = {J_PIEEE},
  author       = {Yunjie Gu and Timothy C. Green},
  doi          = {10.1109/JPROC.2022.3179826},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {832-853},
  shortjournal = {Proc. IEEE},
  title        = {Power system stability with a high penetration of inverter-based resources},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of modeling and applications of energy storage
systems in power grids. <em>PIEEE</em>, <em>111</em>(7), 806–831. (<a
href="https://doi.org/10.1109/JPROC.2022.3158607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the penetration of variable renewable generation increases in power systems, issues, such as grid stiffness, larger frequency deviations, and grid stability, are becoming more relevant, particularly in view of 100\% renewable energy networks, which is the future of smart grids. In this context, energy storage systems (ESSs) are proving to be indispensable for facilitating the integration of renewable energy sources (RESs), are being widely deployed in both microgrids and bulk power systems, and thus will be the hallmark of the clean electrical grids of the future. Hence, this article reviews several energy storage technologies that are rapidly evolving to address the RES integration challenge, particularly compressed air energy storage (CAES), flywheels, batteries, and thermal ESSs, and their modeling and applications in power grids. An overview of these ESSs is provided, focusing on new models and applications in microgrids and distribution and transmission grids for grid operation, markets, stability, and control.},
  archive      = {J_PIEEE},
  author       = {Fabian Calero and Claudio A. Cañizares and Kankar Bhattacharya and Chioma Anierobi and Ivan Calero and Matheus F. Zambroni de Souza and Mostafa Farrokhabadi and Noela Sofia Guzman and William Mendieta and Dario Peralta and Bharatkumar V. Solanki and Nitin Padmanabhan and Walter Violante},
  doi          = {10.1109/JPROC.2022.3158607},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {806-831},
  shortjournal = {Proc. IEEE},
  title        = {A review of modeling and applications of energy storage systems in power grids},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven security and stability rule in high renewable
penetrated power system operation. <em>PIEEE</em>, <em>111</em>(7),
788–805. (<a href="https://doi.org/10.1109/JPROC.2022.3192719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power systems around the world are experiencing an energy revolution that substitutes fossil fuels with renewable energy. Such a transition poses two significant challenges: highly variable generators that add short-term and long-term difficulties for supply–demand balance, and a high proportion of convertor-based devices that may jeopardize power system security and stability. At the same time, machine learning techniques provide more opportunities to study the complex power system security and stability problems. This article summarizes the machine learning framework to embed security rules into power system operation optimization under high renewable energy penetration. First, we explore how high penetration renewable energy impacts power system security and stability. Then, we review how the complex security and stability boundary of power systems is modeled using various machine learning techniques. Finally, we show how the machine learning model is transformed into optimization constraints that can be embedded into the power system operation model. The framework is substantiated through case studies of practical power systems.},
  archive      = {J_PIEEE},
  author       = {Ning Zhang and Hongyang Jia and Qingchun Hou and Ziyang Zhang and Tian Xia and Xiao Cai and Jiaxin Wang},
  doi          = {10.1109/JPROC.2022.3192719},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {788-805},
  shortjournal = {Proc. IEEE},
  title        = {Data-driven security and stability rule in high renewable penetrated power system operation},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Massively digitized power grid: Opportunities and challenges
of use-inspired AI. <em>PIEEE</em>, <em>111</em>(7), 762–787. (<a
href="https://doi.org/10.1109/JPROC.2022.3175070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a use-inspired perspective of the opportunities and challenges in a massively digitized power grid. It argues that the intricate interplay of data availability, computing capability, and artificial intelligence (AI) algorithm development are the three key factors driving the adoption of digitized solutions in the power grid. The impact of these three factors on critical functions of power system operation and planning practices is reviewed and illustrated with industrial practice case studies. Open challenges and research opportunities for data, computing, and AI algorithms are articulated within the context of the power industry’s tremendous decarbonization efforts.},
  archive      = {J_PIEEE},
  author       = {Le Xie and Xiangtian Zheng and Yannan Sun and Tong Huang and Tony Bruton},
  doi          = {10.1109/JPROC.2022.3175070},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {762-787},
  shortjournal = {Proc. IEEE},
  title        = {Massively digitized power grid: Opportunities and challenges of use-inspired AI},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transition to digitalized paradigms for security control and
decentralized electricity market. <em>PIEEE</em>, <em>111</em>(7),
744–761. (<a href="https://doi.org/10.1109/JPROC.2022.3161053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digitalization is one of the key drivers for energy system transformation. The advances in communication technologies and measurement devices render available a large amount of operational data and enable the centralization of such data storage and processing. The greater access to data opens up new opportunities for a more efficient and decentralized management of the energy system. At the distribution level of the energy system, local electricity markets (LEMs) provide new degrees of flexibility by trading and balancing the energy locally and offering ancillary services to the wider transmission and distribution system operators. Maximizing the grid impact from this flexibility calls for novel data analytics and artificial intelligence techniques to enhance the system’s security and reduce the energy costs of local prosumers. At the same time, however, relying on data-based approaches increases the risk of cyberattacks, and robust countermeasures are, therefore, needed as an integral aspect of digitalization efforts. This article discusses the key role of centralized data analytics to fully benefit from the advantages of LEMs in terms of system’s security enhancement and energy costs’ reduction. Data-driven paradigms are investigated that allow for flexibility from decentralized markets, mitigate the physical security risks, and devise defensive strategies shielding the system from cyber threats.},
  archive      = {J_PIEEE},
  author       = {Federica Bellizio and Wangkun Xu and Dawei Qiu and Yujian Ye and Dimitrios Papadaskalopoulos and Jochen L. Cremer and Fei Teng and Goran Strbac},
  doi          = {10.1109/JPROC.2022.3161053},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {744-761},
  shortjournal = {Proc. IEEE},
  title        = {Transition to digitalized paradigms for security control and decentralized electricity market},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DLMP of competitive markets in active distribution networks:
Models, solutions, applications, and visions. <em>PIEEE</em>,
<em>111</em>(7), 725–743. (<a
href="https://doi.org/10.1109/JPROC.2022.3177230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, the electric distribution system operates with uniform energy prices across all system nodes. However, as the adoption of distributed energy resources (DERs) propels a shift from passive to active distribution network (ADN) operation, a distribution-level electricity market has been proposed to manage new complexities efficiently. In addition, distribution locational marginal price (DLMP) has been established in the literature as the primary pricing mechanism. The DLMP inherits the LMP concept in the transmission-level wholesale market but incorporates characteristics of the distribution system, such as high $R/X$ ratios and power losses, system imbalance, and voltage regulation needs. The DLMP provides a solution that can be essential for competitive market operation in future distribution systems. This article first provides an overview of the current distribution-level market architectures and their early implementations. Next, the general clearing model, model relaxations, and DLMP formulation are comprehensively reviewed. The state-of-the-art solution methods for distribution market clearing are summarized and categorized into centralized, distributed, and decentralized methods. Then, DLMP applications for the operation and planning of DERs and distribution system operators (DSOs) are discussed in detail. Finally, visions of future research directions and possible barriers and challenges are presented.},
  archive      = {J_PIEEE},
  author       = {Xiaofei Wang and Fangxing Li and Linquan Bai and Xin Fang},
  doi          = {10.1109/JPROC.2022.3177230},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {725-743},
  shortjournal = {Proc. IEEE},
  title        = {DLMP of competitive markets in active distribution networks: Models, solutions, applications, and visions},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cyber–physical–social perspective on future smart
distribution systems. <em>PIEEE</em>, <em>111</em>(7), 694–724. (<a
href="https://doi.org/10.1109/JPROC.2022.3192535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of distributed energy resources (DERs), such as rooftop photovoltaic (PV), electric vehicles (EVs), and distributed energy storage, are being integrated into the distribution systems. The rise of DERs has come hand-in-hand with large amounts of data generated and explosive growth in data collection, communication, and control devices. In addition, a massive number of consumers are involved in the interaction with the power grid to provide flexibility. Electricity consumers, power networks, and communication networks are three main parts of the distribution systems, which are deeply coupled. In this sense, smart distribution systems can be essentially viewed as cyber–physical–social systems. So far, extensive works have been conducted on the intersection of cyber, physical, and social aspects in distribution systems. These works involve two or three of the cyber, physical, and social aspects. Having a better understanding of how the three aspects are coupled can help to better model, monitor, control, and operate future smart distribution systems. In this regard, this article provides a comprehensive review of the coupling relationships among the cyber, physical, and social aspects of distribution systems. Remarkably, several emerging topics that challenge future cyber–physical–social distribution systems, including applications of 5G communication, the impact of COVID-19, and data privacy issues, are discussed. This article also envisions several future research directions or challenges regarding cyber–physical–social distribution systems.},
  archive      = {J_PIEEE},
  author       = {Yi Wang and Chien-Fei Chen and Peng-Yong Kong and Husheng Li and Qingsong Wen},
  doi          = {10.1109/JPROC.2022.3192535},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {694-724},
  shortjournal = {Proc. IEEE},
  title        = {A Cyber–Physical–Social perspective on future smart distribution systems},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The evolution of smart grids. <em>PIEEE</em>,
<em>111</em>(7), 691–693. (<a
href="https://doi.org/10.1109/JPROC.2023.3284213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception, the smart grid concept has revolutionized power systems worldwide. Concurrently, the energy industry has witnessed significant changes, such as the clean energy transition, digitalization, and the artificial intelligence (AI) revolution. These changes have profoundly impacted power systems technology and energy consumers. As such, power and energy systems have evolved into a multidisciplinary research area encompassing power engineering, information and communication technologies (ICTs), computer and data science, control and optimization theory, and social sciences. In this context, power systems worldwide have moved beyond the smart grid, transforming in terms of technology, physical tructure, and business model. Therefore, this proceeding aims to summarize current developments, recognize new trends, and collect experiences worldwide, focusing on distribution system digitalization and marketization, renewable energy penetration, and electronic device integration.},
  archive      = {J_PIEEE},
  author       = {Chongqing Kang and Daniel Kirschen and Timothy C. Green},
  doi          = {10.1109/JPROC.2023.3284213},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {691-693},
  shortjournal = {Proc. IEEE},
  title        = {The evolution of smart grids},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot and few-shot learning with knowledge graphs: A
comprehensive survey. <em>PIEEE</em>, <em>111</em>(6), 653–685. (<a
href="https://doi.org/10.1109/JPROC.2023.3279374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML), especially deep neural networks, has achieved great success, but many of them often rely on a number of labeled samples for supervision. As sufficient labeled training data are not always ready due to, e.g., continuously emerging prediction targets and costly sample annotation in real-world applications, ML with sample shortage is now being widely investigated. Among all these studies, many prefer to utilize auxiliary information including those in the form of knowledge graph (KG) to reduce the reliance on labeled samples. In this survey, we have comprehensively reviewed over 90 articles about KG-aware research for two major sample shortage settings—zero-shot learning (ZSL) where some classes to be predicted have no labeled samples and few-shot learning (FSL) where some classes to be predicted have only a small number of labeled samples that are available. We first introduce KGs used in ZSL and FSL as well as their construction methods and then systematically categorize and summarize KG-aware ZSL and FSL methods, dividing them into different paradigms, such as the mapping-based, the data augmentation, the propagation-based, and the optimization-based. We next present different applications, including not only KG augmented prediction tasks such as image classification, question answering, text classification, and knowledge extraction but also KG completion tasks and some typical evaluation resources for each task. We eventually discuss some challenges and open problems from different perspectives.},
  archive      = {J_PIEEE},
  author       = {Jiaoyan Chen and Yuxia Geng and Zhuo Chen and Jeff Z. Pan and Yuan He and Wen Zhang and Ian Horrocks and Huajun Chen},
  doi          = {10.1109/JPROC.2023.3279374},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {653-685},
  shortjournal = {Proc. IEEE},
  title        = {Zero-shot and few-shot learning with knowledge graphs: A comprehensive survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bottom-up and top-down approaches for the design of
neuromorphic processing systems: Tradeoffs and synergies between natural
and artificial intelligence. <em>PIEEE</em>, <em>111</em>(6), 623–652.
(<a href="https://doi.org/10.1109/JPROC.2023.3273520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Moore’s law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of alternative brain-inspired computing architectures that aim at achieving the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic engineering represents a paradigm shift in computing based on the implementation of spiking neural network architectures in which processing and memory are tightly colocated. In this article, we provide a comprehensive overview of the field, highlighting the different levels of granularity at which this paradigm shift is realized and comparing design approaches that focus on replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down). First, we present the analog, mixed-signal, and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation, and novel devices. Then, we highlight the key tradeoffs for each of the bottom-up and top-down design approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic systems over conventional machine-learning accelerators in edge computing applications and outline the key ingredients for a framework toward neuromorphic intelligence.},
  archive      = {J_PIEEE},
  author       = {Charlotte Frenkel and David Bol and Giacomo Indiveri},
  doi          = {10.1109/JPROC.2023.3273520},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {623-652},
  shortjournal = {Proc. IEEE},
  title        = {Bottom-up and top-down approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cognitive dynamic systems: A review of theory, applications,
and recent advances. <em>PIEEE</em>, <em>111</em>(6), 575–622. (<a
href="https://doi.org/10.1109/JPROC.2023.3272577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of cognitive dynamic systems (CDSs) is an emerging area of research, whereby engineering learns from neuroscience. Under this framework, engineering systems are configured in a manner that mimics the human brain and improves the utility and performance of traditional systems. In essence, a CDS builds on Fuster’s paradigm of cognition and is fulfilled with the presence of five cognitive processes: the perception-action cycle, memory, attention, intelligence, and language. When augmented with these processes, a system can be classified as a CDS and is afforded the capabilities of processing information and learning from experience through continued interactions with the environment. Tremendous benefit from adopting the CDS framework has been observed in the literature, especially in the fields of cognitive radio and cognitive radar. More recently, the framework has been extended to other areas, such as control theory, risk control, and the Internet of Things; where the potential for drastic performance improvements has been evident in the literature. This comprehensive article presents a thorough background and exposition of the CDS framework and each field where it has been applied. In addition, we provide a comprehensive review of the recent advancements and related works in each domain by summarizing the key facts relating to the methodologies, findings, and limitations of the surveyed papers. Our novel contributions involve being the first source of centralized information on this topic and forming the foundation for future research efforts by presenting suggestions regarding worthwhile avenues for further investigation.},
  archive      = {J_PIEEE},
  author       = {Waleed Hilal and S. Andrew Gadsden and John Yawney},
  doi          = {10.1109/JPROC.2023.3272577},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {575-622},
  shortjournal = {Proc. IEEE},
  title        = {Cognitive dynamic systems: A review of theory, applications, and recent advances},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Micro/nano circuits and systems design and design
automation: Challenges and opportunities. <em>PIEEE</em>,
<em>111</em>(6), 561–574. (<a
href="https://doi.org/10.1109/JPROC.2023.3276941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of design and design automation of micro-/nano-circuits and systems has played a pivotal role in advancing information technologies that are an inseparable part of all our lives. Without the fundamental principles and tools created in this field, modern-day electronic systems that form the foundations of today&#39;s information age would not be a reality. Though the field has achieved tremendous success in the past few decades, it is now facing some unprecedented challenges, stemming from foundational technologies all the way to new applications. Business-as-usual approaches are plateauing. New, fundamental research and innovation are needed to sustain the demanded growth. This paper aims to summarize the key challenges and future research directions in the field of micro/nano circuits and systems design and design automation.},
  archive      = {J_PIEEE},
  author       = {Gert Cauwenberghs and Jason Cong and X. Sharon Hu and Siddharth Joshi and Subhasish Mitra and Wolfgang Porod and H.-S. Philip Wong},
  doi          = {10.1109/JPROC.2023.3276941},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {561-574},
  shortjournal = {Proc. IEEE},
  title        = {Micro/Nano circuits and systems design and design automation: Challenges and opportunities},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overview of megahertz wireless power transfer.
<em>PIEEE</em>, <em>111</em>(5), 528–554. (<a
href="https://doi.org/10.1109/JPROC.2023.3265689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a power supply method with high spatial freedom, megahertz (MHz) wireless power transfer (WPT) has great potential in particular application fields. The role and importance of WPT are described from the perspective of interdisciplinary. This article starts with WPT systems’ performance at different frequencies, emphasizes the basic composition and working principle of MHz systems, and focuses on a comprehensive review of recent years on inverters, rectification, impedance compression, dynamic impedance matching, coupling structure design, and multiload model establishment. Meanwhile, different technologies are summarized, compared, and classified. Current challenges and future development trends are discussed.},
  archive      = {J_PIEEE},
  author       = {Yijie Wang and Zhan Sun and Yueshi Guan and Dianguo Xu},
  doi          = {10.1109/JPROC.2023.3265689},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {528-554},
  shortjournal = {Proc. IEEE},
  title        = {Overview of megahertz wireless power transfer},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resistive neural hardware accelerators. <em>PIEEE</em>,
<em>111</em>(5), 500–527. (<a
href="https://doi.org/10.1109/JPROC.2023.3268092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs), as a subset of machine learning (ML) techniques, entail that real-world data can be learned, and decisions can be made in real time. However, their wide adoption is hindered by a number of software and hardware limitations. The existing general-purpose hardware platforms used to accelerate DNNs are facing new challenges associated with the growing amount of data and are exponentially increasing the complexity of computations. Emerging nonvolatile memory (NVM) devices and the compute-in-memory (CIM) paradigm are creating a new hardware architecture generation with increased computing and storage capabilities. In particular, the shift toward resistive random access memory (ReRAM)-based in-memory computing has great potential in the implementation of area- and power-efficient inference and in training large-scale neural network architectures. These can accelerate the process of IoT-enabled AI technologies entering our daily lives. In this survey, we review the state-of-the-art ReRAM-based DNN many-core accelerators, and their superiority compared to CMOS counterparts was shown. The review covers different aspects of hardware and software realization of DNN accelerators, their present limitations, and prospects. In particular, a comparison of the accelerators shows the need for the introduction of new performance metrics and benchmarking standards. In addition, the major concerns regarding the efficient design of accelerators include a lack of accuracy in simulation tools for software and hardware codesign.},
  archive      = {J_PIEEE},
  author       = {Kamilya Smagulova and Mohammed E. Fouda and Fadi Kurdahi and Khaled N. Salama and Ahmed Eltawil},
  doi          = {10.1109/JPROC.2023.3268092},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {500-527},
  shortjournal = {Proc. IEEE},
  title        = {Resistive neural hardware accelerators},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based deep learning. <em>PIEEE</em>, <em>111</em>(5),
465–499. (<a href="https://doi.org/10.1109/JPROC.2023.3247480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signal processing, communications, and control have traditionally relied on classical statistical modeling techniques. Such model-based methods utilize mathematical formulations that represent the underlying physics, prior information, and additional domain knowledge. Simple classical models are useful but sensitive to inaccuracies and may lead to poor performance when real systems display complex or dynamic behavior. On the other hand, purely data-driven approaches that are model-agnostic are becoming increasingly popular as datasets become abundant and the power of modern deep learning pipelines increases. Deep neural networks (DNNs) use generic architectures that learn to operate from data and demonstrate excellent performance, especially for supervised problems. However, DNNs typically require massive amounts of data and immense computational resources, limiting their applicability for some scenarios. In this article, we present the leading approaches for studying and designing model-based deep learning systems. These are methods that combine principled mathematical models with data-driven systems to benefit from the advantages of both approaches. Such model-based deep learning methods exploit both partial domain knowledge, via mathematical structures designed for specific problems, and learning from limited data. Among the applications detailed in our examples for model-based deep learning are compressed sensing, digital communications, and tracking in state-space models. Our aim is to facilitate the design and study of future systems at the intersection of signal processing and machine learning that incorporate the advantages of both domains.},
  archive      = {J_PIEEE},
  author       = {Nir Shlezinger and Jay Whang and Yonina C. Eldar and Alexandros G. Dimakis},
  doi          = {10.1109/JPROC.2023.3247480},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {465-499},
  shortjournal = {Proc. IEEE},
  title        = {Model-based deep learning},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software-defined imaging: A survey. <em>PIEEE</em>,
<em>111</em>(5), 445–464. (<a
href="https://doi.org/10.1109/JPROC.2023.3266736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huge advancements have been made over the years in terms of modern image-sensing hardware and visual computing algorithms (e.g., computer vision, image processing, and computational photography). However, to this day, there still exists a current gap between the hardware and software design in an imaging system, which silos one research domain from another. Bridging this gap is the key to unlocking new visual computing capabilities for end applications in commercial photography, industrial inspection, and robotics. In this survey, we explore existing works in the literature that can be leveraged to replace conventional hardware components in an imaging system with software for enhanced reconfigurability. As a result, the user can program the image sensor in a way best suited to the end application. We refer to this as software-defined imaging (SDI), where image sensor behavior can be altered by the system software depending on the user’s needs. The scope of our survey covers imaging systems for single-image capture, multi-image, and burst photography, as well as video. We review works related to the sensor primitives, image signal processor (ISP) pipeline, computer architecture, and operating system elements of the SDI stack. Finally, we outline the infrastructure and resources for SDI systems, and we also discuss possible future research directions for the field.},
  archive      = {J_PIEEE},
  author       = {Suren Jayasuriya and Odrika Iqbal and Venkatesh Kodukula and Victor Torres and Robert Likamwa and Andreas Spanias},
  doi          = {10.1109/JPROC.2023.3266736},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {445-464},
  shortjournal = {Proc. IEEE},
  title        = {Software-defined imaging: A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unlocking the hidden capacity of the electrical grid through
smart transformer and smart transmission. <em>PIEEE</em>,
<em>111</em>(4), 421–437. (<a
href="https://doi.org/10.1109/JPROC.2022.3157162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power systems are experiencing a rapid and dramatic transformation driven by the massive integration of nondispatchable renewable energy sources, such as wind and solar, and highly variable loads, such as electric vehicles and air conditioning. This challenges existing grid assets, eventually leading to updating them, which, in turn, increases significantly the costs of sustainable technologies. Power electronics is a pivotal technology for electrical power processing for renewable energies and sustainable transportation. By means of “smart” functionalities, power electronics converters already embedded in such applications can also contribute to guaranteeing the overall system’s stable operation. Anyway, this cooperative contribution from distributed devices may be not enough leading to the need for the voltage transformation and power transmission of “system-level” power electronics solutions. In the case of large charging stations, a smart transformer (ST), while, in the case of large solar and wind parks, integration medium- or high-voltage direct current (HVdc) transmissions are system-level solutions. This article wants to review the potential of using such infrastructures to increase the capacity of existing grid assets, avoiding or deferring their upgrade and, hence, reducing the overall costs of renewables integration and the electrification of the transport sector. In fact, the power converters embedded in ST and HVdc can provide fast frequency and voltage response, and precise control of power flow acting at the system level much more effectively and feasibly for system operators as the distributed power converters embedded in several small sources and users. This article reviews, for the first time, these two key power electronics “system-level” solutions together—ST and HVdc—starting from their basic functionalities and showings how they can go beyond them, showing how, with grid-forming functionalities, they can offer new “smart grid” tools to enhance the capability of the existing electric grid infrastructures.},
  archive      = {J_PIEEE},
  author       = {Marco Liserre and Marcelo A. Perez and Marius Langwasser and Christian A. Rojas and Ziqi Zhou},
  doi          = {10.1109/JPROC.2022.3157162},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {421-437},
  shortjournal = {Proc. IEEE},
  title        = {Unlocking the hidden capacity of the electrical grid through smart transformer and smart transmission},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grid-connected energy storage systems: State-of-the-art and
emerging technologies. <em>PIEEE</em>, <em>111</em>(4), 397–420. (<a
href="https://doi.org/10.1109/JPROC.2022.3183289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High penetration of renewable energy resources in the power system results in various new challenges for power system operators. One of the promising solutions to sustain the quality and reliability of the power system is the integration of energy storage systems (ESSs). This article investigates the current and emerging trends and technologies for grid-connected ESSs. Different technologies of ESSs categorized as mechanical, electrical, electrochemical, chemical, and thermal are briefly explained. Especially, a detailed review of battery ESSs (BESSs) is provided as they are attracting much attention owing, in part, to the ongoing electrification of transportation. Then, the services that grid-connected ESSs provide to the grid are discussed. Grid connection of the BESSs requires power electronic converters. Therefore, a survey of popular power converter topologies, including transformer-based, transformerless with distributed or common dc-link, and hybrid systems, along with some discussions for implementing advanced grid support functionalities in the BESS control, is presented. Furthermore, the requirements of new standards and grid codes for grid-connected BESSs are reviewed for several countries around the globe. Finally, emerging technologies, including flexible power control of photovoltaic systems, hydrogen, and second-life batteries from electric vehicles, are discussed in this article.},
  archive      = {J_PIEEE},
  author       = {Glen G. Farivar and William Manalastas and Hossein Dehghani Tafti and Salvador Ceballos and Alain Sanchez-Ruiz and Emma C. Lovell and Georgios Konstantinou and Christopher D. Townsend and Madhavi Srinivasan and Josep Pou},
  doi          = {10.1109/JPROC.2022.3183289},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {397-420},
  shortjournal = {Proc. IEEE},
  title        = {Grid-connected energy storage systems: State-of-the-art and emerging technologies},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Charging infrastructure and grid integration for
electromobility. <em>PIEEE</em>, <em>111</em>(4), 371–396. (<a
href="https://doi.org/10.1109/JPROC.2022.3216362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicle (EV) charging infrastructure will play a critical role in decarbonization during the next decades, energizing a large share of the transportation sector. This will further increase the enabling role of power electronics converters as an energy transition technology in the widespread adoption of clean energy sources and their efficient use. However, this deep transformation comes with challenges, some of which are already unfolding, such as the slow deployment of charging infrastructure and competing charging standards, and others that will have a long-term impact if not addressed timely, such as the reliability of power converters and power system stability due to loss of system inertia, just to name a few. Nevertheless, the inherent transition toward power systems with higher penetration of power electronics and batteries, together with a layer of communications and information technologies, will also bring opportunities for more flexible and intelligent grid integration and services, which could increase the share of renewable energy in the power grid. This work provides an overview of the existing charging infrastructure ecosystem, covering the different charging technologies for different EV classes, their structure, and configurations, including how they can impact the grid in the future.},
  archive      = {J_PIEEE},
  author       = {Sebastian Rivera and Stefan M. Goetz and Samir Kouro and Peter W. Lehn and Mehanathan Pathmanathan and Pavol Bauer and Rosa Anna Mastromauro},
  doi          = {10.1109/JPROC.2022.3216362},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {371-396},
  shortjournal = {Proc. IEEE},
  title        = {Charging infrastructure and grid integration for electromobility},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The more-electric aircraft and beyond. <em>PIEEE</em>,
<em>111</em>(4), 356–370. (<a
href="https://doi.org/10.1109/JPROC.2022.3152995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aviation is a significant contributor to greenhouse gas (GHG) emissions in the transportation sector. As the adoption of electric cars increases and GHG emissions due to other modes of transport decrease, the impact of air travel on environmental pollution has become even more significant. To reduce pollution and maintenance, and ensure cheaper and more convenient flights, industry and academia have directed their efforts toward aircraft electrification. Considering various types of aircraft, several frameworks have been proposed: more-electric aircraft (MEA), hybrid electric aircraft (HEA), and all-electric aircraft (AEA). In the MEA framework, propulsion is generated by a conventional jet engine; however, all secondary systems (hydraulic, pneumatic, and actuation) are electrified. By further increasing electrification, electric motors can provide propulsion with the electric power supplied by the conventional engine (i.e., HEA) or from electrical energy storage (i.e., AEA). Power electronics and electrical machines play a key role in this scenario in which electric power must be efficiently generated, distributed, and consumed to satisfy extremely high requirements of aviation safety. This article provides an overview of recent advancements in aircraft electrification, and trends and future developments referenced to the global aviation roadmap.},
  archive      = {J_PIEEE},
  author       = {Giampaolo Buticchi and Pat Wheeler and Dushan Boroyevich},
  doi          = {10.1109/JPROC.2022.3152995},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {356-370},
  shortjournal = {Proc. IEEE},
  title        = {The more-electric aircraft and beyond},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power electronics technology for large-scale renewable
energy generation. <em>PIEEE</em>, <em>111</em>(4), 335–355. (<a
href="https://doi.org/10.1109/JPROC.2023.3253165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid integration of renewable energy (REN) requires efficient and reliable power conversion stages, particularly with an increasing demand for high controllability and flexibility seen from the grid side. Underpinned by advanced control and information technologies, power electronics converters play an essential role in large-scale REN generation. However, the use of power converters has also exposed several challenges in conventional power grids, e.g., reducing the system inertia. In this article, grid integration using power electronics is presented for large-scale REN generation. Technical issues and requirements are discussed with a special focus on grid-connected wind, solar photovoltaic, and energy storage systems. In addition, the core of the energy generation and conversion—control for individual power converters (e.g., general current control) and for the system level (e.g., coordinated operation of large-scale energy systems)—is briefly discussed. Future research perspectives are then presented, which further advance large-scale REN generation technologies by incorporating more power electronics systems.},
  archive      = {J_PIEEE},
  author       = {Frede Blaabjerg and Yongheng Yang and Katherine A. Kim and Jose Rodriguez},
  doi          = {10.1109/JPROC.2023.3253165},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {335-355},
  shortjournal = {Proc. IEEE},
  title        = {Power electronics technology for large-scale renewable energy generation},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gallium nitride versus silicon carbide: Beyond the switching
power supply [industry view]. <em>PIEEE</em>, <em>111</em>(4), 322–328.
(<a href="https://doi.org/10.1109/JPROC.2023.3254279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article was jointly produced by IEEE Spectrum and PROCEEDINGS OF THE IEEE with similar versions published in both publications.},
  archive      = {J_PIEEE},
  author       = {Umesh K. Mishra},
  doi          = {10.1109/JPROC.2023.3254279},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {322-328},
  shortjournal = {Proc. IEEE},
  title        = {Gallium nitride versus silicon carbide: Beyond the switching power supply [Industry view]},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radar-based monitoring of vital signs: A tutorial overview.
<em>PIEEE</em>, <em>111</em>(3), 277–317. (<a
href="https://doi.org/10.1109/JPROC.2023.3244362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, substantial attention has been paid to the use of radar systems in health monitoring, due to the availability of both low-cost radar devices and computationally efficient algorithms for processing their measurements. In this article, a tutorial overview of radar-based monitoring of vital signs is provided. More specifically, we first focus on the available radar technologies and the signal processing algorithms developed for the estimation of vital signs. Then, we provide some useful guidelines that should be followed in the selection of radar devices for vital sign monitoring and in their use. Finally, we illustrate various specific applications of radar systems to health monitoring and some relevant research trends in this field.},
  archive      = {J_PIEEE},
  author       = {Giacomo Paterniani and Daria Sgreccia and Alessandro Davoli and Giorgio Guerzoni and Pasquale Di Viesti and Anna Chiara Valenti and Marco Vitolo and Giorgio M. Vitetta and Giuseppe Boriani},
  doi          = {10.1109/JPROC.2023.3244362},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {277-317},
  shortjournal = {Proc. IEEE},
  title        = {Radar-based monitoring of vital signs: A tutorial overview},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection in 20 years: A survey. <em>PIEEE</em>,
<em>111</em>(3), 257–276. (<a
href="https://doi.org/10.1109/JPROC.2023.3238524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today’s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.},
  archive      = {J_PIEEE},
  author       = {Zhengxia Zou and Keyan Chen and Zhenwei Shi and Yuhong Guo and Jieping Ye},
  doi          = {10.1109/JPROC.2023.3238524},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {257-276},
  shortjournal = {Proc. IEEE},
  title        = {Object detection in 20 years: A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of integrated systems and components for 6G
wireless communication in the d-band. <em>PIEEE</em>, <em>111</em>(3),
220–256. (<a href="https://doi.org/10.1109/JPROC.2023.3240127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of wireless communication points to increasing demands on throughput for data-intensive applications in modern society. Integrated millimeter-wave systems with electrical beam-steering capabilities are promising candidates for wireless technologies of the future and are currently the subject of widespread academic and commercial research. The $D$ -band, ranging from 110–170 GHz, offers high aggregate bandwidths (BWs), low atmospheric absorption, and multi-GHz operation at amenable fractional BWs. It, therefore, has the potential to foster efficient, highly integrated wireless-communication systems with data rates approaching 100 Gb/s. This article reviews all aspects of hardware integration against the backdrop of an extensive literature review and outlines the challenges and possible solutions for practical 6G wireless systems in the $D$ -band. To this end, this article covers a number of related topics in depth, which includes system definition, possible radio architectures and array configurations, the scope and potential of integrated circuit (IC) technologies, the design and characterization of key circuit blocks, advances in antenna and packaging technologies for high-frequency systems, and an overview of measurement techniques currently employed at $D$ -band frequencies. A system-level study based on radio-link simulations of different single-carrier quadrature amplitude modulation (QAM) schemes is presented, which quantifies that the impact physical nonidealities, such as signal-to-noise ratio, phase noise, intermodulation distortion, and amplitude and phase imbalances in quadrature signal paths, have on bit-error rates in broadband $D$ -band communication systems. This is followed by a comparative assessment of different arrayed-system configurations that include traditional phased arrays, the use of polarization diversity for the transmission of different or identical data streams, and multiple input multiple output (MIMO) operation. The article also presents an overview of possible transceiver architectures for implementing beam-steering arrays and an outline of the associated tradeoffs. The beam-squinting effect seen in large arrays is also investigated in detail. On the implementation front, we present a comparison between different integrated-circuit technologies for high-frequency applications. These include CMOS and SiGe bipolar complementary metal oxide semiconductor (BiCMOS) heterojunction bipolar transistors (HBTs) in silicon technologies, and MOSFETs, HBTs, and HEMTs in III–V technologies, such as InP and GaAs. Implementation challenges are then addressed, and these include the design of high-frequency circuits in the latest IC technologies, current advances in antenna and packaging technologies, and emerging solutions for hybrid integration. The article also details the design and characterization of critical $D$ -band transceiver circuit blocks, namely, power and low-noise amplifiers, mixers, phase shifters, passive components for quadrature-phase generation, and radiators exploring hybrid antennas, which we have developed over the course of the past five years. These results compliment the literature survey with comparisons with state-of-the-art designs and are applied to radio-link simulations to predict the performance of practicable wireless links.},
  archive      = {J_PIEEE},
  author       = {Tim Maiwald and Teng Li and George-Roberto Hotopan and Katharina Kolb and Karina Disch and Julian Potschka and Alexander Haag and Marco Dietz and Björn Debaillie and Thomas Zwick and Klaus Aufinger and Dieter Ferling and Robert Weigel and Akshay Visweswaran},
  doi          = {10.1109/JPROC.2023.3240127},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {220-256},
  shortjournal = {Proc. IEEE},
  title        = {A review of integrated systems and components for 6G wireless communication in the D-band},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on learning to reject. <em>PIEEE</em>,
<em>111</em>(2), 185–215. (<a
href="https://doi.org/10.1109/JPROC.2023.3238024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to reject is a special kind of self-awareness (the ability to know what you do not know), which is an essential factor for humans to become smarter. Although machine intelligence has become very accurate nowadays, it lacks such kind of self-awareness and usually acts as omniscient, resulting in overconfident errors. This article presents a comprehensive overview of this topic from three perspectives: confidence, calibration, and discrimination. Confidence is an important measurement for the reliability of model predictions. Rejection can be realized by setting thresholds on confidence. However, most models, especially modern deep neural networks, are usually overconfident. Therefore, calibration is a process to ensure confidence matching the actual likelihood of correctness, including two approaches: post-calibration and self-calibration. Calibration reflects the global characteristic of confidence, and the local distinguishing property of confidence is also important. In light of this, discrimination focuses on the performance of accepting positive samples while rejecting negative samples. As a binary classification problem, the challenge of discrimination comes from the missing and nonrepresentativeness of the negative data. Three discrimination tasks are comprehensively analyzed and discussed: failure rejection, unknown rejection, and fake rejection. By rejecting failures, the risk could be controlled especially for mission-critical applications. By rejecting unknowns, the awareness of the knowledge blind zone would be enhanced. By rejecting fakes, security and privacy could be protected. We provide a general taxonomy, organization, and discussion of the methods for solving these problems, which are studied separately in the literature. The connections between different approaches and future directions that are worth further investigation are also presented. With a discriminative and calibrated confidence, learning to reject will let the decision-making process be more practical, reliable, and secure.},
  archive      = {J_PIEEE},
  author       = {Xu-Yao Zhang and Guo-Sen Xie and Xiuli Li and Tao Mei and Cheng-Lin Liu},
  doi          = {10.1109/JPROC.2023.3238024},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {185-215},
  shortjournal = {Proc. IEEE},
  title        = {A survey on learning to reject},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability of HfO2-based ferroelectric FETs: A critical
review of current and future challenges. <em>PIEEE</em>,
<em>111</em>(2), 158–184. (<a
href="https://doi.org/10.1109/JPROC.2023.3234607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ferroelectric transistors (FeFETs) based on doped hafnium oxide (HfO2) have received much attention due to their technological potential in terms of scalability, high-speed, and low-power operation. Unfortunately, however, HfO2-FeFETs also suffer from persistent reliability challenges, specifically affecting retention, endurance, and variability. A deep understanding of the reliability physics of HfO2-FeFETs is an essential prerequisite for the successful commercialization of this promising technology. In this article, we review the literature about the relevant reliability aspects of HfO2-FeFETs. We initially focus on the reliability physics of ferroelectric capacitors, as a prelude to a comprehensive analysis of FeFET reliability. Then, we interpret key reliability metrics of the FeFET at the device level (i.e., retention, endurance, and variability) based on the physical mechanisms previously identified. Finally, we discuss the implications of device-level reliability metrics at both the circuit and system levels. Our integrative approach connects apparently unrelated reliability issues and suggests mitigation strategies at the device, circuit, or system level. We conclude this article by proposing a set of research opportunities to guide future development in this field.},
  archive      = {J_PIEEE},
  author       = {Nicolò Zagni and Francesco Maria Puglisi and Paolo Pavan and Muhammad Ashraful Alam},
  doi          = {10.1109/JPROC.2023.3234607},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {158-184},
  shortjournal = {Proc. IEEE},
  title        = {Reliability of HfO2-based ferroelectric FETs: A critical review of current and future challenges},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed nash equilibrium seeking in games with partial
decision information: A survey. <em>PIEEE</em>, <em>111</em>(2),
140–157. (<a href="https://doi.org/10.1109/JPROC.2023.3234687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nash equilibrium, as an essential strategic profile in game theory, is of both practical relevance and theoretical significance due to its wide penetration into various fields, such as smart grids, wireless communication networks, and networked mobile vehicles. In particular, distributed Nash equilibrium seeking strategies have recently attracted increasing attention because they show remarkable advantages in relaxing the requirement of a central node for information broadcasting or full observation of players’ actions. This article aims to provide a survey of distributed Nash equilibrium seeking in games with partial decision information, in which players can only exchange information with their neighbors and their objective functions may explicitly depend on all players’ actions. First, fundamental problem descriptions on distributed Nash equilibrium seeking are presented. Second, related results on distributed Nash equilibrium seeking in general multiplayer games, aggregative games, and multicluster games are reviewed, respectively, where representative continuous- and discrete-time methods are explained in detail. Third, two practical applications, including collaborative control for a network of mobile sensors and energy consumption control in smart grids, are provided to demonstrate the applicability of distributed Nash equilibrium seeking strategies. Finally, some promising directions are suggested for future research.},
  archive      = {J_PIEEE},
  author       = {Maojiao Ye and Qing-Long Han and Lei Ding and Shengyuan Xu},
  doi          = {10.1109/JPROC.2023.3234687},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {140-157},
  shortjournal = {Proc. IEEE},
  title        = {Distributed nash equilibrium seeking in games with partial decision information: A survey},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The information age and naval command &amp; control.
<em>PIEEE</em>, <em>111</em>(1), 113–131. (<a
href="https://doi.org/10.1109/JPROC.2022.3228637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a condensed version of this article originally prepared for the USNA McMullen Naval History Symposium in 2017 and is a collaboration between David Boslaugh, Peter Marland, and John Vardalas (Stevens Institute of Technology) who have previously written about the postwar development of naval digital systems in their respective countries.},
  archive      = {J_PIEEE},
  author       = {David Boslaugh and Peter Marland and John Vardalas},
  doi          = {10.1109/JPROC.2022.3228637},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {113-131},
  shortjournal = {Proc. IEEE},
  title        = {The information age and naval command &amp; control},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technology prospects for data-intensive computing.
<em>PIEEE</em>, <em>111</em>(1), 92–112. (<a
href="https://doi.org/10.1109/JPROC.2022.3218057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many decades, progress in computing hardware has been closely associated with CMOS logic density, performance, and cost. As such, slowdown in 2-D scaling, frequency saturation in CPUs, and increased cost of design and chip fabrication for advanced technology nodes since the early 2000s have led to concerns about how semiconductor technology may evolve in the future. However, the last two decades have also witnessed a parallel development in the application landscape: the advent of big data and consequent rise of data-intensive computing, using techniques such as machine learning. In this article, we advance the idea that data-intensive computing would further cement semiconductor technology as a foundational technology with multidimensional pathways for growth. Continued progress of semiconductor technology in this new context would require the adoption of a system-centric perspective to holistically harness logic, memory, and packaging resources. After examining the performance metrics for data-intensive computing, we present the historical trends for general-purpose graphics processing unit (GPGPU) as a representative data-intensive computing hardware. Thereon, we estimate the values of the key data-intensive computing parameters for the next decade, and our projections may serve as a precursor for a dedicated technology roadmap. By analyzing the compiled data, we identify and discuss specific opportunities and challenges for data-intensive computing hardware technology.},
  archive      = {J_PIEEE},
  author       = {Kerem Akarvardar and H. -S. Philip Wong},
  doi          = {10.1109/JPROC.2022.3218057},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {92-112},
  shortjournal = {Proc. IEEE},
  title        = {Technology prospects for data-intensive computing},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient acceleration of deep learning inference on
resource-constrained edge devices: A review. <em>PIEEE</em>,
<em>111</em>(1), 42–91. (<a
href="https://doi.org/10.1109/JPROC.2022.3226481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful integration of deep neural networks (DNNs) or deep learning (DL) has resulted in breakthroughs in many areas. However, deploying these highly accurate models for data-driven, learned, automatic, and practical machine learning (ML) solutions to end-user applications remains challenging. DL algorithms are often computationally expensive, power-hungry, and require large memory to process complex and iterative operations of millions of parameters. Hence, training and inference of DL models are typically performed on high-performance computing (HPC) clusters in the cloud. Data transmission to the cloud results in high latency, round-trip delay, security and privacy concerns, and the inability of real-time decisions. Thus, processing on edge devices can significantly reduce cloud transmission cost. Edge devices are end devices closest to the user, such as mobile phones, cyber–physical systems (CPSs), wearables, the Internet of Things (IoT), embedded and autonomous systems, and intelligent sensors. These devices have limited memory, computing resources, and power-handling capability. Therefore, optimization techniques at both the hardware and software levels have been developed to handle the DL deployment efficiently on the edge. Understanding the existing research, challenges, and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence (AI) capability. Mainly, four research directions have been pursued for efficient DL inference on edge devices: 1) novel DL architecture and algorithm design; 2) optimization of existing DL methods; 3) development of algorithm–hardware codesign; and 4) efficient accelerator design for DL deployment. This article focuses on surveying each of the four research directions, providing a comprehensive review of the state-of-the-art tools and techniques for efficient edge inference.},
  archive      = {J_PIEEE},
  author       = {Md. Maruf Hossain Shuvo and Syed Kamrul Islam and Jianlin Cheng and Bashir I. Morshed},
  doi          = {10.1109/JPROC.2022.3226481},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {42-91},
  shortjournal = {Proc. IEEE},
  title        = {Efficient acceleration of deep learning inference on resource-constrained edge devices: A review},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning for emergency management: A survey and
future outlook. <em>PIEEE</em>, <em>111</em>(1), 19–41. (<a
href="https://doi.org/10.1109/JPROC.2022.3223186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency situations encompassing natural and human-made disasters, as well as their cascading effects, pose serious threats to society at large. Machine learning (ML) algorithms are highly suitable for handling the large volumes of spatiotemporal data that are generated during such situations. Hence, over the years, they have been utilized in emergency management to aid first responders and decision-makers in such situations and ultimately improve disaster prevention, preparedness, response, and recovery. In this survey article, we highlight relevant work in this area by first focusing on the commonalities of emergency management applications and key challenges that ML algorithms need to address. Then, we present a categorization of relevant works across all the emergency management phases and operations, highlighting the main algorithms used. Based on our review, we conclude that ML algorithms can provide the basis for tackling different activities across the emergency management phases with a unified algorithmic framework that can solve a large set of problems. Finally, through the systematic literature review, we provide promising future directions for utilizing ML algorithms more effectively in emergency management applications. More importantly, we identify the need for better generalization of algorithms, improved explainability, and trustworthiness of ML algorithms with respect to the emergency management personnel, as well as more efficient ways of addressing the challenges associated with building appropriate datasets.},
  archive      = {J_PIEEE},
  author       = {Christos Kyrkou and Panayiotis Kolios and Theocharis Theocharides and Marios Polycarpou},
  doi          = {10.1109/JPROC.2022.3223186},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {19-41},
  shortjournal = {Proc. IEEE},
  title        = {Machine learning for emergency management: A survey and future outlook},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A perspective vision of micro/nano systems and technologies
as enablers of 6g, super-iot, and tactile internet [point of view].
<em>PIEEE</em>, <em>111</em>(1), 5–18. (<a
href="https://doi.org/10.1109/JPROC.2022.3223791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern research in technology fields, such as electronics, distributed networks of sensing/functional nodes, and wireless and wearable devices, is relentlessly converging around wide application paradigms, such as Internet of Things (IoT) [1] and Internet of Everything (IoE) [2] — Table 1 , at the end of section, offers a full list of used acronyms. From a different perspective, recent advances in electronics, hardware (HW) technologies, information technology (IT), and artificial intelligence (AI) for telecommunication networks, standards, and protocols look to unavoidably fall under the umbrella of fifth generation of mobile communications (5G) [3] . Even though they appear orthogonal to each other, IoT, IoE, and 5G are closely linked together. In a nutshell, IoT and IoE target pervasivity of services, while 5G is the pillar upon which transmission of massive amounts of data and information should lay [4] . As brief recap, 5G poses on the three cornerstone drivers of enhanced mobile broadband (eMBB), massive machine-type communications (mMTCs), and ultrareliable low latency communications (URLLC) [5] , to enable data-centric applications such as machine-to-machine (M2M), vehicle-to-vehicle (V2V), and vehicle-to-everything (V2X) communications, along with virtual reality (VR), augmented reality (AR), and extended reality (XR).},
  archive      = {J_PIEEE},
  author       = {Jacopo Iannacci},
  doi          = {10.1109/JPROC.2022.3223791},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {5-18},
  shortjournal = {Proc. IEEE},
  title        = {A perspective vision of micro/nano systems and technologies as enablers of 6g, super-iot, and tactile internet [point of view]},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scanning the issue. <em>PIEEE</em>, <em>111</em>(1), 2–4.
(<a href="https://doi.org/10.1109/JPROC.2022.3229245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This month’s regular papers issue focuses on machine learning for emergency management, data-intensive computing, and efficient edge inference, a vital element of Edge-AI.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2022.3229245},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {2-4},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {111},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
