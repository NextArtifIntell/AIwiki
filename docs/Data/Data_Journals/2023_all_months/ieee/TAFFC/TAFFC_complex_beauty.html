<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---249">TAFFC - 249</h2>
<ul>
<li><details>
<summary>
(2023). Automated classification of dyadic conversation scenarios
using autonomic nervous system responses. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3388–3395. (<a
href="https://doi.org/10.1109/TAFFC.2023.3236265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Two people&#39;s physiological responses become more similar as those people talk or cooperate, a phenomenon called physiological synchrony. The degree of synchrony correlates with conversation engagement and cooperation quality, and could thus be used to characterize interpersonal interaction. In this study, we used a combination of physiological synchrony metrics and pattern recognition algorithms to automatically classify four different dyadic conversation scenarios: two-sided positive conversation, two-sided negative conversation, and two one-sided scenarios. Heart rate, skin conductance, respiration and peripheral skin temperature were measured from 16 dyads in all four scenarios, and individual as well as synchrony features were extracted from them. A two-stage classifier based on stepwise feature selection and linear discriminant analysis achieved a four-class classification accuracy of 75.0% in leave-dyad-out crossvalidation. Removing synchrony features reduced accuracy to 65.6%, indicating that synchrony is informative. In the future, such classification algorithms may be used to, e.g., provide real-time feedback about conversation mood to participants, with applications in areas such as mental health counseling and education. The approach may also generalize to group scenarios and adjacent areas such as cooperation and competition.},
  archive  = {J},
  author   = {Iman Chatterjee and Maja Goršič and Mohammad S. Hossain and Joshua D. Clapp and Vesna D. Novak},
  doi      = {10.1109/TAFFC.2023.3236265},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3388-3395},
  title    = {Automated classification of dyadic conversation scenarios using autonomic nervous system responses},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the function of expressions in negotiation: The
DyNego-WOZ corpus. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3376–3387. (<a
href="https://doi.org/10.1109/TAFFC.2022.3223030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For affective computing to have an impact outside the laboratory, facial expressions must be studied in rich naturalistic situations. We argue negotiations are one such situation as they are ubiquitous in daily life, often evoke strong emotions, and perceived emotion shapes decisions and outcomes. Negotiations are a growing focus in AI research and applications, including agents that negotiate directly with people and attempt to use affective information. We introduce the DyNego-WOZ Corpus, which includes dyadic negotiation between participants and wizard-controlled virtual humans. We demonstrate the value of this corpus to the affective computing community by examining participants’ facial expressions in response to a virtual human negotiation partner. We show that people&#39;s facial expressions typically co-occur with the end of their partner&#39;s speech (suggesting they reflect a reaction to the content of this speech), that these reactions do not correspond to prototypical emotional expressions, and that these reactions can help predict the expresser&#39;s subsequent action. We highlight challenges in working with such naturalistic data, including difficulties of expression recognition during speech, and the extreme variability of expressions, both across participants and within a negotiation. Our findings reinforce arguments that facial expressions convey more than emotional state but serve important communicative functions.},
  archive  = {J},
  author   = {Jessie Hoegen and David DeVault and Jonathan Gratch},
  doi      = {10.1109/TAFFC.2022.3223030},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3376-3387},
  title    = {Exploring the function of expressions in negotiation: The DyNego-WOZ corpus},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous reinforcement learning network for
aspect-based sentiment classification with external knowledge. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 3362–3375. (<a
href="https://doi.org/10.1109/TAFFC.2022.3233020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment classification aims to automatically predict the sentiment polarity of the specific aspect in a text. However, it is challenging to confirm the mapping between the aspect and the core context since a number of existing methods concentrate on building the global relations of the full context rather than the partial connections based on the aspects. Motivated by the fundamental insights of reinforcement learning, we propose a novel H eterogeneous R einforcement L earning N etwork for aspect-based sentiment analysis (HRLN) to alleviate these issues, which contains two primary components, a heterogeneous network module, and a knowledge graph-based reinforcement learning module consistent with common-sense knowledge and emotional knowledge. To evaluate the effectiveness of HRLN, we conduct extensive experiments on five benchmark datasets, which indicate that HRLN achieves competitive performance and yields state-of-the-art results on all datasets. Additionally, we present an intuitive comprehension of why our HRLN model is more robust for aspect-based sentiment classification via case studies.},
  archive  = {J},
  author   = {Yukun Cao and Yijia Tang and Haizhou Du and Feifei Xu and Ziyue Wei and Chengkun Jin},
  doi      = {10.1109/TAFFC.2022.3233020},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3362-3375},
  title    = {Heterogeneous reinforcement learning network for aspect-based sentiment classification with external knowledge},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dyadic affect in parent-child multimodal interaction:
Introducing the DAMI-P2C dataset and its preliminary analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 3345–3361. (<a
href="https://doi.org/10.1109/TAFFC.2022.3178689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-quality parent-child conversational interactions are crucial for children&#39;s social, emotional, and cognitive development. However, many children have limited exposure to these interactions at home. As increasingly accessible and scalable interventions in child development, interactive technologies, such as social robots, have great potential for facilitating parent-child interactions. However, such technology-based interventions are still underexplored, as the technologies’ limited ability to understand the social-emotional dynamics of human dyadic interactions impedes their effective delivery of timely, adaptive interventions. To advance research on resolving this roadblock, we present a “dyadic affect in multimodal interaction - parent to child” (DAMI-P2C) dataset collected during a study of 34 parent-child pairs, where parents and children (3-7 years old) engaged in reading storybooks together. In contrast to existing public datasets for social-emotional behaviors in dyadic interactions, each instance for both participants in our dataset was annotated for affect by three labelers. Additionally, the dataset contains audiovisual recordings as well as each dyad&#39;s sociodemographic profiles, co-reading behaviors, affect labels, and body joints. We describe the dataset&#39;s main characteristics and provide a preliminary analysis of the interrelations between sociodemographic profiles, co-reading behaviors, and affect labels. The dataset provides us with useful insights into the computing and social science fields.},
  archive  = {J},
  author   = {Huili Chen and Sharifa Alghowinem and Soo Jung Jang and Cynthia Breazeal and Hae Won Park},
  doi      = {10.1109/TAFFC.2022.3178689},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3345-3361},
  title    = {Dyadic affect in parent-child multimodal interaction: Introducing the DAMI-P2C dataset and its preliminary analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social image–text sentiment classification with cross-modal
consistency and knowledge distillation. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3332–3344. (<a
href="https://doi.org/10.1109/TAFFC.2022.3220762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social media sentiment analysis, which aims to evaluate the attitudes of online users based on their posts, has attracted significant research attention due to its successful application in the field of social media monitoring. It is a beneficial way to utilize multimodal information uploaded by users in order to improve sentiment classification ability. However, existing multimodal fusion-based approaches continue to face difficulties due to the issues of between-modality semantic inconsistency and missing modality. To address these issues, we propose a cross-modal consistency modeling-based knowledge distillation framework for image–text sentiment classification of social media data. Specifically, we design a hybrid curriculum learning strategy to measure the semantic consistency of multimodal data, then gradually train all image–text pairs from easy to hard, which can effectively handle the massive amounts of noise caused by inconsistencies between image and text data on social media. Moreover, in order to alleviate the problem of missing images in unimodal posts, we propose a privileged feature distillation method, in which the teacher model additionally considers images as privileged features, to transfer the visual knowledge to the student model, thereby enhancing the accuracy for text sentiment classification. Extensive experiments conducted over three real-world social media datasets demonstrate the effectiveness and superiority of the proposed multimodal sentiment analysis model.},
  archive  = {J},
  author   = {Huan Liu and Ke Li and Jianping Fan and Caixia Yan and Tao Qin and Qinghua Zheng},
  doi      = {10.1109/TAFFC.2022.3220762},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3332-3344},
  title    = {Social Image–Text sentiment classification with cross-modal consistency and knowledge distillation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple but powerful, a language-supervised method for image
emotion classification. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3317–3331. (<a
href="https://doi.org/10.1109/TAFFC.2022.3225049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Image emotion classification is an important computer vision task to extract emotions from images. The methods for image emotion classification (IEC) are primarily based on label or distribution as a supervision signal, which neither has enough accessibility nor diversity, limiting the development of IEC research. Inspired by psychology research and the recent booming of large-scale pretrained language models. We figure out a language-supervised paradigm, which can cleverly combine the features of language and visual emotion to drive the visual model to gain stronger emotional discernment with language prompts. To practice the paradigm, we present a conceptually simple while empirically powerful framework for image emotion classification, SimEmotion. That we propose a prompt-based fine-tuning strategy to learn task-specific representations by composing a template with the emotion-level concept and entity-level information. Evaluations on four widely-used affective datasets, namely, Flickr and Instagram (FI), EmotionROI, Twitter I, and Twitter II, demonstrate that the proposed algorithm outperforms the state-of-the-art methods with a large margin (i.e., $8.42\%$ absolute accuracy gain on EmotionROI) on image emotion classification tasks. Our codes will be publicly available for research purposes.},
  archive  = {J},
  author   = {Sinuo Deng and Lifang Wu and Ge Shi and Lehao Xing and Wenjin Hu and Heng Zhang and Ye Xiang},
  doi      = {10.1109/TAFFC.2022.3225049},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3317-3331},
  title    = {Simple but powerful, a language-supervised method for image emotion classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depression recognition using remote photoplethysmography
from facial videos. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3305–3316. (<a
href="https://doi.org/10.1109/TAFFC.2023.3238641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a mental illness that may be harmful to an individual&#39;s health. The detection of mental health disorders in the early stages and a precise diagnosis are critical to avoid social, physiological, or psychological side effects. This work analyzes physiological signals to observe if different depressive states have a noticeable impact on the blood volume pulse (BVP) and the heart rate variability (HRV) response. Although typically, HRV features are calculated from biosignals obtained with contact-based sensors such as wearables, we propose instead a novel scheme that directly extracts them from facial videos, just based on visual information, removing the need for any contact-based device. Our solution is based on a pipeline that is able to extract complete remote photoplethysmography signals (rPPG) in a fully unsupervised manner. We use these rPPG signals to calculate over 60 statistical, geometrical, and physiological features that are further used to train several machine learning regressors to recognize different levels of depression. Experiments on two benchmark datasets indicate that this approach offers comparable results to other audiovisual modalities based on voice or facial expression, potentially complementing them. In addition, the results achieved for the proposed method show promising and solid performance that outperforms hand-engineered methods and is comparable to deep learning-based approaches.},
  archive  = {J},
  author   = {Constantino Álvarez Casado and Manuel Lage Cañellas and Miguel Bordallo López},
  doi      = {10.1109/TAFFC.2023.3238641},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3305-3316},
  title    = {Depression recognition using remote photoplethysmography from facial videos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid regularizations for multi-aspect category sentiment
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3294–3304. (<a
href="https://doi.org/10.1109/TAFFC.2023.3236948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect level sentiment classification aims to identify the sentiment polarity towards a particular aspect in a sentence. Previous attention-based methods generate an aspect-specific representation for each aspect and employ it to classify the sentiment polarity. However, normalized attention scores scatter over every word in the sentence, resulting in two issues. First, the attention may inherently introduce noise and downgrade the performance. Second, the opinion words may be “diluted” by other words, while the opinion feature should dominate for sentiment analysis. The issues become more severe in multi-aspect sentences. In this paper, we address the above two issues via hybrid regularizations, i.e., aspect-level and task-level regularizations . Concretely, the aspect-level regularizations constrain the attention weights to alleviate noise. Among them, orthogonal regularization is designed for multi-aspect sentences and sparse regularization is for single-aspect sentences. To extract sentiment-dominant features, task-level regularization is proposed by introducing an orthogonal auxiliary task, i.e., aspect category detection. This regularization can allocate task-oriented context information for specific downstream tasks. Extensive experimental results on three public datasets demonstrate the effectiveness of the proposed approach in both single-task and multi-task scenarios.},
  archive  = {J},
  author   = {Mengting Hu and Shiwan Zhao and Honglei Guo and Zhong Su},
  doi      = {10.1109/TAFFC.2023.3236948},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3294-3304},
  title    = {Hybrid regularizations for multi-aspect category sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PIDViT: Pose-invariant distilled vision transformer for
facial expression recognition in the wild. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3281–3293. (<a
href="https://doi.org/10.1109/TAFFC.2022.3220972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many Facial expression recognition methods have achieved great success, but they only considered front facial images or facial images close to the front. Besides, unlike in-the-laboratory datasets, the facial images in the real world (or in the wild) are without lighting and pose control, so that it is a big challenge to recognize these facial expressions. In this paper, the PIDViT (i.e., Pose-Invariant Distilled Vision Transformer) using the teacher-student model for the probability distributions of facial expressions of frontal and multi-pose faces was proposed and solved the pose variance and occlusion issues on expression recognition. First, the multi-pose face dataset FairFace-3D was built from the original FairFace and then used to train pose-invariance on the PIDViT. The PIDViT was trained in two stages; stage 1 is to train the PIDViT to achieve the consistency of facial expressions between frontal faces and multi-pose faces, and stage 2 is to use the student model pre-trained in stage 1 and train facial expressions further on target datasets. Finally, comprehensive experiments were conducted on three in the wild facial expression datasets, and the results validates the generalization of the PIDViT and its superiority over most state-of-the-art models.},
  archive  = {J},
  author   = {Yin-Fu Huang and Chia-Hsin Tsai},
  doi      = {10.1109/TAFFC.2022.3220972},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3281-3293},
  title    = {PIDViT: Pose-invariant distilled vision transformer for facial expression recognition in the wild},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-level contrastive learning for emotion recognition
in conversations. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3269–3280. (<a
href="https://doi.org/10.1109/TAFFC.2023.3243463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A key challenge for Emotion Recognition in Conversations (ERC) is to distinguish semantically similar emotions. Some works utilise Supervised Contrastive Learning (SCL) which uses categorical emotion labels as supervision signals and contrasts in high-dimensional semantic space. However, categorical labels fail to provide quantitative information between emotions. ERC is also not equally dependent on all embedded features in the semantic space, which makes the high-dimensional SCL inefficient. To address these issues, we propose a novel low-dimensional Supervised Cluster-level Contrastive Learning (SCCL) method, which first reduces the high-dimensional SCL space to a three-dimensional affect representation space Valence-Arousal-Dominance (VAD), then performs cluster-level contrastive learning to incorporate measurable emotion prototypes. To help modelling the dialogue and enriching the context, we leverage the pre-trained knowledge adapters to infuse linguistic and factual knowledge. Experiments show that our method achieves new state-of-the-art results with $69.81\%$ on IEMOCAP, $65.7\%$ on MELD, and $62.51\%$ on DailyDialog datasets. The analysis also proves that the VAD space is not only suitable for ERC but also interpretable, with VAD prototypes enhancing its performance and stabilising the training of SCCL. In addition, the pre-trained knowledge adapters benefit the performance of the utterance encoder and SCCL. Our code is available at: https://github.com/SteveKGYang/SCCL},
  archive  = {J},
  author   = {Kailai Yang and Tianlin Zhang and Hassan Alhuzali and Sophia Ananiadou},
  doi      = {10.1109/TAFFC.2023.3243463},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3269-3280},
  title    = {Cluster-level contrastive learning for emotion recognition in conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bias-based soft label learning for facial expression
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3257–3268. (<a
href="https://doi.org/10.1109/TAFFC.2022.3220291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial Expression Recognition (FER) suffers from misrecognition due to the similarities between expressions. To address this issue, popular works replace original annotations with soft labels to reflect expression similarities. However, existing soft label learning (SLL) modules are independent of FER modules. In this article, inspired by automatic control theory, we propose a bias-based soft label learning network for FER named EC-Net. For optimizing FER and SLL modules jointly, EC-Net constitutes the closed-loop feedback between the two modules by designing a module measuring and transmitting the bias between FER module predictions and target labels. Specifically, EC-Net contains three modules: E-subNet, C-subNet, and L-Transmitter. First, E-subNet, i.e., the FER module, attempts to converge to target labels under the supervision of soft labels, acting as the executor. Then, L-Transmitter measures the bias between E-subNet predictions and target labels. It converts multiple discrete biases to the bias-based label through spectral clustering and transmits it to C-subNet. Finally, C-SubNet, i.e., the SLL module, generates soft labels from the bias-based label with a cascaded learner and progressively distinguishes similar expressions. It updates the learned soft labels for E-subNet, performing like the controller. Supervised by the bias-based soft label, E-subNet effectively reduces the dominant bias caused by similar expressions. We conduct extensive experiments on four popular benchmarks, demonstrating the effectiveness of applying closed-loop feedback in the FER task.},
  archive  = {J},
  author   = {Shanmin Wang and Hui Shuai and Chengguang Liu and Qingshan Liu},
  doi      = {10.1109/TAFFC.2022.3220291},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3257-3268},
  title    = {Bias-based soft label learning for facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision transformer with attentive pooling for robust facial
expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3244–3256. (<a
href="https://doi.org/10.1109/TAFFC.2022.3226473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial Expression Recognition (FER) in the wild is an extremely challenging task. Recently, some Vision Transformers (ViT) have been explored for FER, but most of them perform inferiorly compared to Convolutional Neural Networks (CNN). This is mainly because the new proposed modules are difficult to converge well from scratch due to lacking inductive bias and easy to focus on the occlusion and noisy areas. TransFER, a representative transformer-based method for FER, alleviates this with multi-branch attention dropping but brings excessive computations. On the contrary, we present two attentive pooling (AP) modules to pool noisy features directly. The AP modules include Attentive Patch Pooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to emphasize the most discriminative features while reducing the impacts of less relevant features. The proposed APP is employed to select the most informative patches on CNN features, and ATP discards unimportant tokens in ViT. Being simple to implement and without learnable parameters, the APP and ATP intuitively reduce the computational cost while boosting the performance by ONLY pursuing the most discriminative features. Qualitative results demonstrate the motivations and effectiveness of our attentive poolings. Besides, quantitative results on six in-the-wild datasets outperform other state-of-the-art methods.},
  archive  = {J},
  author   = {Fanglei Xue and Qiangchang Wang and Zichang Tan and Zhongsong Ma and Guodong Guo},
  doi      = {10.1109/TAFFC.2022.3226473},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3244-3256},
  title    = {Vision transformer with attentive pooling for robust facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applying segment-level attention on bi-modal transformer
encoder for audio-visual emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3231–3243. (<a
href="https://doi.org/10.1109/TAFFC.2023.3258900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions can be expressed through multiple complementary modalities. This study selected speech and facial expressions as modalities by which to recognize emotions. Current audiovisual emotion recognition models perform supervised learning using signal-level inputs. Such models are presumed to characterize the temporal relationships in signals. In this study, supervised learning was performed on segment-level signals, which are more granular than signal-level signals, to precisely train an emotion recognition model. Effectively fusing multimodal signals is challenging. In this study, sequential segments of audiovisual signals were obtained, and features were extracted and applied to estimate segment-level attention weights according to the emotional consistency of the two modalities using a neural tensor network. A proposed bimodal Transformer Encoder was trained using signal-level and segment-level emotion labels in which temporal context was incorporated into the signals to improve upon existing emotion recognition models. In bimodal emotion recognition, the experimental results demonstrated that the proposed method achieved 74.31% accuracy (3.05% higher than the method of fusing correlation features) on the audio-visual emotion dataset BAUM-1, which is based on fivefold cross-validation, and 76.81% accuracy (2.57% higher than the Multimodal Transformer Encoder) on the multimodal emotion data set CMU-MOSEI, which is composed of training, validation, and testing sets.},
  archive  = {J},
  author   = {Jia-Hao Hsu and Chung-Hsien Wu},
  doi      = {10.1109/TAFFC.2023.3258900},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3231-3243},
  title    = {Applying segment-level attention on bi-modal transformer encoder for audio-visual emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnable hierarchical label embedding and grouping for
visual intention understanding. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3218–3230. (<a
href="https://doi.org/10.1109/TAFFC.2023.3247876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual intention understanding is to mine the potential and subjective intention behind the images, which includes the user&#39;s hidden emotions and perspectives. Due to the label ambiguity, this paper presents a novel learnable Hierarchical Label Embedding and Grouping (HLEG). It is featured in three aspects: 1) For effectively mining the underlying meaning of images, we build a hierarchical transformer structure to model the hierarchy of labels, formulating a multi-level classification scheme. 2) For the label ambiguity issue, we design a novel learnable label embedding with accumulative grouping integrated into the hierarchical structure, which does not require additional annotation. 3) For multi-level classification, we propose a “Hard-First” optimization strategy to adaptively adjust the classification optimization at different levels, avoiding over-classification of the coarse labels. HLEG enhances the F1 score (average +1.24%) and mAP (average +1.48%) on Intentonomy over prominent baseline models. Comprehensive experiments validate the superiority of our proposed method, achieving state-of-the-art performance under various settings. Code is available at https://github.com/ShiQingHongYa/HLEG .},
  archive  = {J},
  author   = {QingHongYa Shi and Mang Ye and Ziyi Zhang and Bo Du},
  doi      = {10.1109/TAFFC.2023.3247876},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3218-3230},
  title    = {Learnable hierarchical label embedding and grouping for visual intention understanding},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personality trait recognition based on smartphone typing
characteristics in the wild. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3207–3217. (<a
href="https://doi.org/10.1109/TAFFC.2023.3253202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As governed by personality trait theory, humans tackle problems differently depending on their long-term behavioral characteristics. Computational awareness of personality traits fuels affective computing research, which investigates how to reliably recognize and utilize personality traits. Applications are diverse, including therapy monitoring, learning assistance, and recommender systems. Data-driven approaches are a promising path forward towards personality-aware human-computer interactions. Thereby, central challenges are the non-disruptive data acquisition, the time frame over which data must be collected before predictions become accurate, and the feature-centered data reduction to train reliable and lightweight machine learning models. In this work, we address these challenges by presenting a fully-automatic feature extraction and machine learning pipeline that makes accurate personality trait predictions for the widely-used Five Factor Model from passively-collected, short-term smartphone typing data collected from 76 participants (68 university students) in the wild. Our model allows for personality trait assessments after one day of data collection, demonstrating that, despite being a long-term behavioral trend, personality traits can be inferred accurately from shorter time periods. We demonstrate that our system can accurately predict personality traits on two levels (low and high) with up to 74.5% accuracy and 0.72 AUC for a single day, and up to 84.5% accuracy and 0.79 AUC after subsequent refinement over 10 weeks.},
  archive  = {J},
  author   = {Nikola Kovačević and Christian Holz and Tobias Günther and Markus Gross and Rafael Wampfler},
  doi      = {10.1109/TAFFC.2023.3253202},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3207-3217},
  title    = {Personality trait recognition based on smartphone typing characteristics in the wild},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-incremental continual learning for mitigating bias in
facial expression and action unit recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3191–3206. (<a
href="https://doi.org/10.1109/TAFFC.2022.3181033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As Facial Expression Recognition (FER) systems become integrated into our daily lives, these systems need to prioritise making fair decisions instead of only aiming at higher individual accuracy scores. From surveillance systems, to monitoring the mental and emotional health of individuals, these systems need to balance the accuracy versus fairness trade-off to make decisions that do not unjustly discriminate against specific under-represented demographic groups. Identifying bias as a critical problem in facial analysis systems, different methods have been proposed that aim to mitigate bias both at data and algorithmic levels. In this work, we propose the novel use of Continual Learning (CL), in particular, using Domain-Incremental Learning (Domain-IL) settings, as a potent bias mitigation method to enhance the fairness of Facial Expression Recognition (FER) systems. We compare different non-CL-based and CL-based methods for their performance and fairness scores on expression recognition and Action Unit (AU) detection tasks using two popular benchmarks, the RAF-DB and BP4D datasets, respectively. Our experimental results show that CL-based methods, on average, outperform other popular bias mitigation techniques on both accuracy and fairness metrics.},
  archive  = {J},
  author   = {Nikhil Churamani and Ozgur Kara and Hatice Gunes},
  doi      = {10.1109/TAFFC.2022.3181033},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3191-3206},
  title    = {Domain-incremental continual learning for mitigating bias in facial expression and action unit recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recognizing, fast and slow: Complex emotion recognition with
facial expression detection and remote physiological measurement.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
3177–3190. (<a
href="https://doi.org/10.1109/TAFFC.2023.3253859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Complex emotion is an aggregate of two or more others which has highly variable appearances, inter-dependence, and affective dynamics.These properties make the recognition hard to handle via existing recognition techniques like action units or valence-arousal detection. In this study, we propose a bionic two-system structure for complex emotion recognition. The structure mimics the working theory of the human brain responding to problems decision-making. System I is a fast compound sensing module. System II is a slower cognitive decision module that processes data more integratively. System I contains one branch for facial expression feature representation including basic emotion, action units, and valence arousal detection and one for physiological measurement which is an image-only implementation for practicality. In System II, a decision module with segmentation is employed to ensure the chosen period including the emotion occurrence and iteratively optimize the emotion information in a given segment via reinforcement learning. The proposed method outperforms state-of-the-art on emotion recognition tasks with an accuracy of 94.15% in basic emotion recognition on the BP4D and an accuracy of 68.75% for binary valence arousal classification on the DEAP. For a subset of complex emotions, the recognition accuracy exceeds 70% on both databases, that is a significant improvement.},
  archive  = {J},
  author   = {Yi-Chiao Wu and Li-Wen Chiu and Chun-Chih Lai and Bing-Fei Wu and Sunny S. J. Lin},
  doi      = {10.1109/TAFFC.2023.3253859},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3177-3190},
  title    = {Recognizing, fast and slow: Complex emotion recognition with facial expression detection and remote physiological measurement},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multitask learning from augmented auxiliary data for
improving speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3164–3176. (<a
href="https://doi.org/10.1109/TAFFC.2022.3221749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems lack generalisation across different conditions. A key underlying reason for poor generalisation is the scarcity of emotion datasets, which is a significant roadblock to designing robust machine learning (ML) models. Recent works in SER focus on utilising multitask learning (MTL) methods to improve generalisation by learning shared representations. However, most of these studies propose MTL solutions with the requirement of meta labels for auxiliary tasks, which limits the training of SER systems. This paper proposes an MTL framework (MTL-AUG) that learns generalised representations from augmented data. We utilise augmentation-type classification and unsupervised reconstruction as auxiliary tasks, which allow training SER systems on augmented data without requiring any meta labels for auxiliary tasks. The semi-supervised nature of MTL-AUG allows for the exploitation of the abundant unlabelled data to further boost the performance of SER. We comprehensively evaluate the proposed framework in the following settings: (1) within corpus, (2) cross-corpus and cross-language, (3) noisy speech, (4) and adversarial attacks. Our evaluations using the widely used IEMOCAP, MSP-IMPROV, and EMODB datasets show improved results compared to existing state-of-the-art methods.},
  archive  = {J},
  author   = {Siddique Latif and Rajib Rana and Sara Khalifa and Raja Jurdak and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2022.3221749},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3164-3176},
  title    = {Multitask learning from augmented auxiliary data for improving speech emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal sentiment analysis based on attentional temporal
convolutional network and multi-layer feature fusion. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 3149–3163. (<a
href="https://doi.org/10.1109/TAFFC.2023.3265653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal sentiment analysis aims to extract and integrate information from different modalities to accurately identify the sentiment expressed in multimodal data. How to effectively capture the relevant information within a specific modality and how to fully exploit the complementary information among multiple modalities are two major challenges in multimodal sentiment analysis. Traditional approaches fail to obtain the global contextual information of long time-series data when extracting unimodal temporal features, and they usually fuse the features from multiple modalities with the same method and ignore the correlation between different modalities when modeling inter-modal interactions. In this paper, we first propose an Attentional Temporal Convolutional Network (ATCN) to extract unimodal temporal features for enhancing the feature representation ability, then introduce a Multi-layer Feature Fusion (MFF) model to improve the effectiveness of multimodal fusion, which fuses the different-level features by different methods according to the correlation coefficient between the features, and cross-modal multi-head attention is used to fully explore the potential relationship between the low-level features. The experimental results on SIMS and CMU-MOSI datasets show that the proposed model achieves superior performance on sentiment analysis tasks compared to state-of-the-art baselines.},
  archive  = {J},
  author   = {Hongju Cheng and Zizhen Yang and Xiaoqi Zhang and Yang Yang},
  doi      = {10.1109/TAFFC.2023.3265653},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3149-3163},
  title    = {Multimodal sentiment analysis based on attentional temporal convolutional network and multi-layer feature fusion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Use of affective visual information for summarization of
human-centric videos. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3135–3148. (<a
href="https://doi.org/10.1109/TAFFC.2022.3222882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The increasing volume of user-generated human-centric video content and its applications, such as video retrieval and browsing, require compact representations addressed by the video summarization literature. Current supervised studies formulate video summarization as a sequence-to-sequence learning problem, and the existing solutions often neglect the surge of the human-centric view, which inherently contains affective content. In this study, we investigate the affective-information enriched supervised video summarization task for human-centric videos. First, we train a visual input-driven state-of-the-art continuous emotion recognition model (CER-NET) on the RECOLA dataset to estimate activation and valence attributes. Then, we integrate the estimated emotional attributes and their high-level embeddings from the CER-NET with the visual information to define the proposed affective video summarization (AVSUM) architectures. In addition, we investigate the use of attention to improve the AVSUM architectures and propose two new architectures based on temporal attention (TA-AVSUM-GRU) and spatial attention (SA-AVSUM-GRU). We conduct video summarization experiments on the TvSum and COGNIMUSE datasets. The proposed temporal attention-based TA-AVSUM architecture attains competitive video summarization performances with strong improvements for the human-centric videos compared to the state-of-the-art in terms of F-score, self-defined face recall, and rank correlation metrics.},
  archive  = {J},
  author   = {Berkay Köprü and Engin Erzin},
  doi      = {10.1109/TAFFC.2022.3222882},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3135-3148},
  title    = {Use of affective visual information for summarization of human-centric videos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Speech synthesis with mixed emotions. <em>IEEE Transactions
on Affective Computing</em>, <em>14</em>(4), 3120–3134. (<a
href="https://doi.org/10.1109/TAFFC.2022.3233324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly focused on imitating an averaged style belonging to a specific emotion type. In this paper, we seek to generate speech with a mixture of emotions at run-time. We propose a novel formulation that measures the relative difference between the speech samples of different emotions. We then incorporate our formulation into a sequence-to-sequence emotional text-to-speech framework. During the training, the framework does not only explicitly characterize emotion styles but also explores the ordinal nature of emotions by quantifying the differences with other emotions. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of the proposed framework. To our best knowledge, this research is the first study on modelling, synthesizing, and evaluating mixed emotions in speech.},
  archive  = {J},
  author   = {Kun Zhou and Berrak Sisman and Rajib Rana and Björn W. Schuller and Haizhou Li},
  doi      = {10.1109/TAFFC.2022.3233324},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3120-3134},
  title    = {Speech synthesis with mixed emotions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain aspect-based sentiment classification by
exploiting domain- invariant semantic-primary feature. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 3106–3119. (<a
href="https://doi.org/10.1109/TAFFC.2023.3239540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment analysis is an important task in fine-grained sentiment analysis, which aims to infer the sentiment towards a given aspect. Previous studies have shown notable success when sufficient labeled training data is available. However, annotating adequate data is labor-intensive, which sets substantial barriers for generalizing the sentiment predictor to the new domain. Two main challenges exist in cross-domain aspect-based sentiment analysis. One challenge is acquiring the domain-invariant knowledge; the other challenge is mining the syntactic-related words towards the aspect-term. In this article, we propose a transformer-based semantic-primary knowledge transferring network (TSPKT) for cross-domain aspect-term sentiment analysis, which utilizes semantic-primary knowledge as a bridge to enable knowledge transfer across domains. Specifically, we first build an S-Graph from external semantic lexicons, and extract the semantic-primary knowledge from the S-Graph. Second, AoaGraphormer is proposed to learn the syntactically relevant words towards the aspect-term. Third, we extend the standard biLSTM classifier to fully integrate the semantic-primary knowledge by adding a novel knowledge-aware memory unit (KAMU) to the biLSTM cell. Extensive experiments on six cross-domain setups demonstrate the superiority of TSPKT against the state-of-the-art baseline methods.},
  archive  = {J},
  author   = {Bowen Zhang and Xianghua Fu and Chuyao Luo and Yunming Ye and Xutao Li and Liwen Jing},
  doi      = {10.1109/TAFFC.2023.3239540},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3106-3119},
  title    = {Cross-domain aspect-based sentiment classification by exploiting domain- invariant semantic-primary feature},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contradicted by the brain: Predicting individual and group
preferences via brain-computer interfacing. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 3094–3105. (<a
href="https://doi.org/10.1109/TAFFC.2022.3225885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We investigate inferring individual preferences and the contradiction of individual preferences with group preferences through direct measurement of the brain. We report an experiment where brain activity collected from 31 participants produced in response to viewing images is associated with their self-reported preferences. First, we show that brain responses present a graded response to preferences, and that brain responses alone can be used to train classifiers that reliably estimate preferences. Second, we show that brain responses reveal additional preference information that correlates with group preference, even when participants self-reported having no such preference. Our analysis of brain responses carries significant implications for researchers in general, as it suggests an individual&#39;s explicit preferences are not always aligned with the preferences inferred from their brain responses. These findings call into question the reliability of explicit and behavioral signals. They also imply that additional, multimodal sources of information may be necessary to infer reliable preference information.},
  archive  = {J},
  author   = {Keith M. Davis and Michiel Spape and Tuukka Ruotsalo},
  doi      = {10.1109/TAFFC.2022.3225885},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3094-3105},
  title    = {Contradicted by the brain: Predicting individual and group preferences via brain-computer interfacing},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-quality landmarked infrared eye video dataset
(IREye4Task): Eye behaviors, insights and benchmarks for wearable mental
state analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3078–3093. (<a
href="https://doi.org/10.1109/TAFFC.2023.3258915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sensing the mental state induced by different task contexts, where cognition is a focus, is as important as sensing the affective state where emotion is induced in the foreground of consciousness, because completing tasks is part of every waking moment of life. However, few datasets are publicly available to advance mental state analysis, especially those using the eye as the sensing modality with detailed ground truth for eye behaviors. In this study, we contribute a high-quality publicly accessible eye video dataset, IREye4Task, where the eyelid, pupil and iris boundary are annotated for each frame to obtain eye behaviors as responses to four different task contexts and two load levels of tasks, over more than a million frames. Meanwhile, we propose a series of eye behavior representations to provide insights into how the eye behaves during different mental states. Finally, we benchmark three mental-state recognition tasks for this dataset to demonstrate the effectiveness of the eye behavior representations. This is the first public wearable eye video dataset for mental state analysis with high quality eye landmarks and a variety of mental states, and is the first study analyzing comprehensive eye behaviors far beyond using pupil size and blink in previous studies.},
  archive  = {J},
  author   = {Siyuan Chen and Julien Epps},
  doi      = {10.1109/TAFFC.2023.3258915},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3078-3093},
  title    = {A high-quality landmarked infrared eye video dataset (IREye4Task): Eye behaviors, insights and benchmarks for wearable mental state analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enroll-to-verify approach for cross-task unseen emotion
class recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3066–3077. (<a
href="https://doi.org/10.1109/TAFFC.2022.3183166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most speech emotion recognition studies often focus on recognizing pre-set emotion classes. However, the task definition may change due to a shift in focus to a previously unseen class in real-world applications. This cross-task modeling has not been addressed previously. Lengthy data re-collection, model retraining, and the traditional adaptation and transfer learning approaches are not applicable to this cross-task setting. This study proposes an enroll-to-verify framework to avoid model retraining and rapidly perform a new task prediction using only a handful of enrolled samples. Specifically, we use negative angular margin prototypical loss in a pretrained multiclass network as an emotion encoder. Then, we enroll a few samples corresponding to emotion classes in the new task definition and simply compare the encoded embedding distance to perform recognition. In the experiments on the IEMOCAP dataset, given a four-class pretrained emotion encoder, we achieved a 71.9% unweighted average recall in the frustration (unseen) recognition task. The MELD dataset was used where the unseen class was surprise, fear, or disgust. The results revealed that enrolling only 20 samples without retraining was comparable to supervised training using the complete dataset. Further analyses were conducted to demonstrate the working mechanism of our proposed enroll-to-verify approach.},
  archive  = {J},
  author   = {Jeng-Lin Li and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2022.3183166},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3066-3077},
  title    = {An enroll-to-verify approach for cross-task unseen emotion class recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning person-specific cognition from facial reactions for
automatic personality recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 3048–3065. (<a
href="https://doi.org/10.1109/TAFFC.2022.3230672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article proposes to recognise the true (self-reported) personality traits from the target subject&#39;s cognition simulated from facial reactions. This approach builds on the following two findings in cognitive science: (i) human cognition partially determines expressed behaviour and is directly linked to true personality traits; and (ii) in dyadic interactions, individuals’ nonverbal behaviours are influenced by their conversational partner&#39;s behaviours. In this context, we hypothesise that during a dyadic interaction, a target subject&#39;s facial reactions are driven by two main factors: their internal (person-specific) cognitive process, and the externalised nonverbal behaviours of their conversational partner. Consequently, we propose to represent the target subject&#39;s (defined as the listener) person-specific cognition in the form of a person-specific CNN architecture that has unique architectural parameters and depth, which takes audio-visual non-verbal cues displayed by the conversational partner (defined as the speaker) as input, and is able to reproduce the target subject&#39;s facial reactions. Each person-specific CNN is explored by the Neural Architecture Search (NAS) and a novel adaptive loss function, which is then represented as a graph representation for recognising the target subject&#39;s true personality. Experimental results not only show that the produced graph representations are well associated with target subjects’ personality traits in both human-human and human-machine interaction scenarios, and outperform the existing approaches with significant advantages, but also demonstrate that the proposed novel strategies help in learning more reliable personality representations.},
  archive  = {J},
  author   = {Siyang Song and Zilong Shao and Shashank Jaiswal and Linlin Shen and Michel Valstar and Hatice Gunes},
  doi      = {10.1109/TAFFC.2022.3230672},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3048-3065},
  title    = {Learning person-specific cognition from facial reactions for automatic personality recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 4DME: A spontaneous 4D micro-expression dataset with
multimodalities. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3031–3047. (<a
href="https://doi.org/10.1109/TAFFC.2022.3182342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions (ME) are a special form of facial expressions which may occur when people try to hide their true feelings for some reasons. MEs are important clues to reveal people&#39;s true feelings, but are difficult or impossible to be captured by ordinary persons with naked-eyes as they are very short and subtle. It is expected that robust computer vision methods can be developed to automatically analyze MEs which requires lots of ME data. The current ME datasets are insufficient, and mostly contain only one single form of 2D color videos. Researches on 4D data of ordinary facial expressions have prospered, but so far no 4D data is available in ME study. In the current study, we introduce the 4DME dataset: a new spontaneous ME dataset which includes 4D data along with three other video modalities. Both micro- and macro-expression clips are labeled out in 4DME, and 22 AU labels and five categories of emotion labels are annotated. Experiments are carried out using three 2D-based methods and one 4D-based method to provide baseline results. The results indicate that the 4D data can potentially benefit ME recognition. The 4DME dataset could be used for developing 4D-based approaches, or exploring fusion of multiple video sources (e.g., texture and depth) for the task of ME analysis in future. Besides, we also emphasize the importance of forming a clear and unified criteria of ME annotation for future ME data collection studies. Several key questions related with ME annotation are listed and discussed in depth, especially about the relationship between AUs and ME emotion categories. A preliminary AU-Emo mapping table is proposed with justified explanations and supportive experimental results. Several unsolved issues are also summarized for future work.},
  archive  = {J},
  author   = {Xiaobai Li and Shiyang Cheng and Yante Li and Muzammil Behzad and Jie Shen and Stefanos Zafeiriou and Maja Pantic and Guoying Zhao},
  doi      = {10.1109/TAFFC.2022.3182342},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3031-3047},
  title    = {4DME: A spontaneous 4D micro-expression dataset with multimodalities},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using affect as a communication modality to improve
human-robot communication in robot-assisted search and rescue scenarios.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
3013–3030. (<a
href="https://doi.org/10.1109/TAFFC.2022.3221922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions can provide a natural communication modality to complement the existing multi-modal capabilities of social robots, such as text and speech, in many domains. We conducted three online studies with 112, 223, and 151 participants, respectively, to investigate the benefits of using emotions as a communication modality for Search And Rescue (SAR) robots. In the first experiment, we investigated the feasibility of conveying information related to SAR situations through robots’ emotions, resulting in mappings from SAR situations to emotions. The second study used Affect Control Theory as an alternative method for deriving such mappings. This method is more flexible, e.g., allows for such mappings to be adjusted for different emotion sets and different robots. In the third experiment, we created affective expressions for an appearance-constrained outdoor field research robot using LEDs as an expressive channel. Using these affective expressions in a variety of simulated SAR situations, we evaluated the effect of these expressions on participants’ (in the role rescue workers) situational awareness. Our results and proposed methodologies (a) provide insights on how emotions could help conveying messages in the context of SAR, and (b) show evidence on the effectiveness of adding emotions as a communication modality in a (simulated) SAR communication context.},
  archive  = {J},
  author   = {Sami Alperen Akgun and Moojan Ghafurian and Mark Crowley and Kerstin Dautenhahn},
  doi      = {10.1109/TAFFC.2022.3221922},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3013-3030},
  title    = {Using affect as a communication modality to improve human-robot communication in robot-assisted search and rescue scenarios},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Teardrops on my face: Automatic weeping detection from
nonverbal behavior. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 3001–3012. (<a
href="https://doi.org/10.1109/TAFFC.2022.3228749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human emotional tears are a powerful socio-emotional signal. Yet, they have received relatively little attention in empirical research compared to facial expressions or body posture. While humans are highly sensitive to others’ tears, to date, no automatic means exist for detecting spontaneous weeping. This article employed facial and postural features extracted using four pre-trained classifiers (FACET, Affdex, OpenFace, OpenPose) to train a Support Vector Machine (SVM) to distinguish spontaneous weepers from non-weepers. Results showed that weeping can be accurately inferred from nonverbal behavior. Importantly, this distinction can be made before the appearance of visible tears on the face. However, features from at least two classifiers need to be combined, with the best models blending three or four classifiers to achieve near-perfect performance (97% accuracy). We discuss how direct and indirect tear detection methods may help to yield important new insights into the antecedents and consequences of emotional tears and how affective computing could benefit from the ability to recognize and respond to this uniquely human signal.},
  archive  = {J},
  author   = {Dennis Küster and Lars Steinert and Marc Baker and Nikhil Bhardwaj and Eva G. Krumhuber},
  doi      = {10.1109/TAFFC.2022.3228749},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {3001-3012},
  title    = {Teardrops on my face: Automatic weeping detection from nonverbal behavior},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computer-aided autism spectrum disorder diagnosis with
behavior signal processing. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2982–3000. (<a
href="https://doi.org/10.1109/TAFFC.2023.3238712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Behavioral observation plays an essential role in the diagnosis of Autism Spectrum Disorder (ASD) by analyzing children&#39;s atypical patterns in social activities (e.g., impaired social interaction, restricted interests, and repetitive behavior). To date, this process still heavily relies on the questionnaire survey, clinical observation, or retrospective video analysis, leading to high demand for professionals with massive labor costs. This article proposes a standardized platform for stimulating, gathering, analyzing, modeling, and interpreting human behavioral data in the application of computer-aided ASD diagnosis. By a structured assessment process, the proposed system can automatically evaluate children&#39;s multiple social interaction skills using the captured audio-visual data and provide the final diagnostic suggestions. We collect a multimodal behavioral database of 95 participants (71 children with ASD and 24 age-matched typical controls) in a real clinic environment, the Third Affiliated Hospital of Sun Yat-sen University, China. On the clinical database, our proposed computer-aided ASD diagnosis system obtains an accuracy of 88.42% for identifying ASD children with an average age of 24 months, representing a performance comparable to top-level human experts. As a unified and replicable solution, it has good potential to be promoted to less developed areas with limited high-quality medical resources.},
  archive  = {J},
  author   = {Ming Cheng and Yingying Zhang and Yixiang Xie and Yueran Pan and Xiao Li and Wenxing Liu and Chengyan Yu and Dong Zhang and Yu Xing and Xiaoqian Huang and Fang Wang and Cong You and Yuanyuan Zou and Yuchong Liu and Fengjing Liang and Huilin Zhu and Chun Tang and Hongzhu Deng and Xiaobing Zou and Ming Li},
  doi      = {10.1109/TAFFC.2023.3238712},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2982-3000},
  title    = {Computer-aided autism spectrum disorder diagnosis with behavior signal processing},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Driver emotion recognition with a hybrid attentional
multimodal fusion framework. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2970–2981. (<a
href="https://doi.org/10.1109/TAFFC.2023.3250460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Negative emotions may induce dangerous driving behaviors leading to extremely serious traffic accidents. Therefore, it is necessary to establish a system that can automatically recognize driver emotions so that some actions can be taken to avoid traffic accidents. Existing studies on driver emotion recognition have mainly used facial data and physiological data. However, there are fewer studies on multimodal data with contextual characteristics of driving. In addition, fully fusing multimodal data in the feature fusion layer to improve the performance of emotion recognition is still a challenge. To this end, we propose to recognize driver emotion using a novel multimodal fusion framework based on convolutional long-short term memory network (ConvLSTM), and hybrid attention mechanism to fuse non-invasive multimodal data of eye, vehicle, and environment. In order to verify the effectiveness of the proposed method, extensive experiments have been carried out on a dataset collected using an advanced driving simulator. The experimental results demonstrate the effectiveness of the proposed method. Finally, a preliminary exploration on the correlation between driver emotion and stress is performed.},
  archive  = {J},
  author   = {Luntian Mou and Yiyuan Zhao and Chao Zhou and Bahareh Nakisa and Mohammad Naim Rastgoo and Lei Ma and Tiejun Huang and Baocai Yin and Ramesh Jain and Wen Gao},
  doi      = {10.1109/TAFFC.2023.3250460},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2970-2981},
  title    = {Driver emotion recognition with a hybrid attentional multimodal fusion framework},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio-visual emotion recognition with preference learning
based on intended and multi-modal perceived labels. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2954–2969. (<a
href="https://doi.org/10.1109/TAFFC.2023.3234777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article introduces a novel preference learning framework that simultaneously considers both the intended and the perceived labels while addressing the mismatches between them. Based on analyzing the discrepancies and agreements between the intended and the perceived labels in different modalities of audio-only, visual-only, and audio-visual, as well as the consistency among the perceptual ratings of all raters, we propose three sets of pair-wise ranking rules to generate multi-scale relevant scores for preference learning, scaling from sketchy manner to detailed manner. Three ranking models with support vector machine (SVM), deep neural networks (DNN), and gradient boosting decision trees (GBDT) are developed. Our results demonstrate that all three preference learning models significantly outperform the conventional classifiers baselines, and the LambdaMART model with gradient boosting decision trees achieves the best performance. The improvement from the preference learning models confirm the benefits of complementary information provided by different types of labels. We also observe additional improvement from the detailed ‘complex ranking rules’, particular with the best LambdaMART model, which suggests that we should treat intended and perceived labels in single-model &amp; multi-modal differently. We further discuss the complementary of different ranking models, and obtain the best overall accuracy of 85.06% on CREMA-D dataset when combining the two best ranking models–LambdaMART and RankNet–together, which is significantly better than the 76.19% accuracy attained by the baseline models. Finally, we perform the cross-corpus emotion recognition experiments by training emotion rankers on CREMA-D dataset and tested the ranking-based emotion classifier on the SAVEE dataset that do not have perceived labels annotated.},
  archive  = {J},
  author   = {Yuanyuan Lei and Houwei Cao},
  doi      = {10.1109/TAFFC.2023.3234777},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2954-2969},
  title    = {Audio-visual emotion recognition with preference learning based on intended and multi-modal perceived labels},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotional contagion-aware deep reinforcement learning for
antagonistic crowd simulation. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2939–2953. (<a
href="https://doi.org/10.1109/TAFFC.2022.3225037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The antagonistic behavior in the crowd usually exacerbates the seriousness of the situation in sudden riots, where the antagonistic emotional contagion and behavioral decision making play very important roles. However, the complex mechanism of antagonistic emotion influencing decision making, especially in the environment of sudden confrontation, has not yet been explored very clearly. In this paper, we propose an Emotional contagion-aware Deep reinforcement learning model for Antagonistic Crowd Simulation (ACSED). First, we build a group emotional contagion module based on the improved Susceptible Infected Susceptible (SIS) infection disease model, and estimate the emotional state of the group at each time step during the simulation. Then, the tendency of crowd antagonistic action is estimated based on Deep Q Network (DQN), where the agent learns the action autonomously, and leverages the mean field theory to quickly calculate the influence of other surrounding individuals on the central one. Finally, the rationality of the predicted actions by DQN is further analyzed in combination with group emotion, and the final action of the agent is determined. The proposed method in this paper is verified through several experiments with different settings. We can conclude antagonistic emotions play a critical role in the decision making of the crowd through influencing the individual behavior in the riot scenario, where individual behaviors are primarily driven by emotions and goals, rather than common rules. The experiment results also prove that the antagonistic emotion has a vital impact on the group combat, and positive emotional states are more conducive to combat. Moreover, by comparing the simulation results with real scenes, the feasibility of our method is further confirmed, which can provide good reference to formulate battle plans and improve the win rate of righteous groups in a variety of situations.},
  archive  = {J},
  author   = {Pei Lv and Qingqing Yu and Boya Xu and Chaochao Li and Bing Zhou and Mingliang Xu},
  doi      = {10.1109/TAFFC.2022.3225037},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2939-2953},
  title    = {Emotional contagion-aware deep reinforcement learning for antagonistic crowd simulation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMPosE: Movie-induced multi-label positive emotion
classification through EEG signals. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2925–2938. (<a
href="https://doi.org/10.1109/TAFFC.2022.3221554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional information plays an important role in various multimedia applications. Movies, as a widely available form of multimedia content, can induce multiple positive emotions and stimulate people&#39;s pursuit of a better life. Different from negative emotions, positive emotions are highly correlated and difficult to distinguish in the emotional space. Since different positive emotions are often induced simultaneously by movies, traditional single-target or multi-class methods are not suitable for the classification of movie-induced positive emotions. In this paper, we propose TransEEG , a model for multi-label positive emotion classification from a viewer&#39;s brain activities when watching emotional movies. The key features of TransEEG include (1) explicitly modeling the spatial correlation and temporal dependencies of multi-channel EEG signals using the Transformer structure based model, which effectively addresses long-distance dependencies, (2) exploiting the label-label correlations to guide the discriminative EEG representation learning, for that we design an Inter-Emotion Mask for guiding the Multi-Head Attention to learn the inter-emotion correlations, and (3) constructing an attention score vector from the representation-label correlation matrix to refine emotion-relevant EEG features. To evaluate the ability of our model for multi-label positive emotion classification, we demonstrate our model on a state-of-the-art positive emotion database CPED. Extensive experimental results show that our proposed method achieves superior performance over the competitive approaches.},
  archive  = {J},
  author   = {Xiaobing Du and Xiaoming Deng and Hangyu Qin and Yezhi Shu and Fang Liu and Guozhen Zhao and Yu-Kun Lai and Cuixia Ma and Yong-Jin Liu and Hongan Wang},
  doi      = {10.1109/TAFFC.2022.3221554},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2925-2938},
  title    = {MMPosE: Movie-induced multi-label positive emotion classification through EEG signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotional expressivity is a reliable signal of surprise.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
2913–2924. (<a
href="https://doi.org/10.1109/TAFFC.2023.3234015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of inferring what happened to a person in a social task from momentary facial reactions. To approach this, we introduce several innovations. First, rather than predicting what (observers think) someone feels, we predict objective features of the event that immediately preceded the facial reactions. Second, we draw on appraisal theory, a key psychological theory of emotion, to characterize features of this immediately-preceded event. Specifically, we explore if facial expressions reveal if the event is expected, goal-congruent, and norm-compatible. Finally, we argue that emotional expressivity serves as a better feature for characterizing momentary expressions than traditional facial features. Specifically, we use supervised machine learning to predict third-party judgments of emotional expressivity with high accuracy, and show this model improves inferences about the nature of the event that preceded an emotional reaction. Contrary to common sense, “genuine smiles” failed to predict if an event advanced a person’s goals. Rather, expressions best revealed if an event violated expectations. We discussed the implications of these findings for the interpretation of facial displays and potential limitations that could impact the generality of these findings.},
  archive  = {J},
  author   = {Su Lei and Jonathan Gratch},
  doi      = {10.1109/TAFFC.2023.3234015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2913-2924},
  title    = {Emotional expressivity is a reliable signal of surprise},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceived conversation quality in spontaneous interactions.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
2901–2912. (<a
href="https://doi.org/10.1109/TAFFC.2023.3233950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The quality of daily spontaneous conversations is of importance towards both our well-being as well as the development of interactive social agents. Prior research directly studying the quality of social conversations has operationalized it in narrow terms, associating greater quality to less small talk. Other works taking a broader perspective of interaction experience have indirectly studied quality through one of the several overlapping constructs such as rapport or engagement, in isolation. In this work we bridge this gap by proposing a holistic conceptualization of conversation quality, building upon the collaborative attributes of cooperative conversation floors. Taking a multilevel perspective of conversation, we develop and validate two instruments for perceived conversation quality (PCQ) at the individual and group levels. Specifically, we motivate capturing external raters’ gestalt impressions of participant experiences from thin slices of behavior, and collect annotations of PCQ on the publicly available MatchNMingle dataset of in-the-wild mingling conversations. Finally, we present an analysis of behavioral features that are predictive of PCQ. We find that for the conversations in MatchNMingle, raters tend to associate smaller group sizes, equitable speaking turns with fewer interruptions, and time taken for synchronous bodily coordination with higher PCQ.},
  archive  = {J},
  author   = {Chirag Raman and Navin Raj Prabhu and Hayley Hung},
  doi      = {10.1109/TAFFC.2023.3233950},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2901-2912},
  title    = {Perceived conversation quality in spontaneous interactions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shared-private memory networks for multimodal sentiment
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2889–2900. (<a
href="https://doi.org/10.1109/TAFFC.2022.3222023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text, visual, and acoustic are usually complementary in the Multimodal Sentiment Analysis (MSA) task. However, current methods primarily concern shared representations while neglecting the critical private aspects of data within individual modalities. In this work, we propose shared-private memory networks based on the recent advances in the attention mechanism, called SPMN, to decouple multimodal representation from shared and private perspectives. It contains three components: a) a shared memory to learn the shared representations of multimodal data; b) three private memories to learn the private representations of individual modalities, respectively; c) and adaptive fusion gates to fuse multimodal private and shared representations. To evaluate the effectiveness of SPMN, we integrate it into different pre-trained language representation models, such as BERT and XLNET, and conduct experiments on two public datasets, CMU-MOSI and CMU-MOSEI. Experimental results indicate that the performances of pre-trained language representation models are significantly improved because of SPMN and demonstrate the superiority of our model compared to the state-of-the-art methods. SPMN&#39;s source code is publicly available at: https://github.com/xiaobaicaihhh/SPMN .},
  archive  = {J},
  author   = {Xianbing Zhao and Yinxin Chen and Sicen Liu and Buzhou Tang},
  doi      = {10.1109/TAFFC.2022.3222023},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2889-2900},
  title    = {Shared-private memory networks for multimodal sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-order networks for action unit detection. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2876–2888. (<a
href="https://doi.org/10.1109/TAFFC.2022.3178524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Action Units (AU) are muscular activations used to describe facial expressions. Therefore accurate AU recognition unlocks unbiaised face representation which can improve face-based affective computing applications. From a learning standpoint AU detection is a multi-task problem with strong inter-task dependencies. To solve such problem, most approaches either rely on weight sharing, or add explicit dependency modelling by decomposing the joint task distribution using Bayes chain rule. If the latter strategy yields comprehensive inter-task relationships modelling, it requires imposing an arbitrary order into an unordered task set. Crucially, this ordering choice has been identified as a source of performance variations. In this paper, we present Multi-Order Network (MONET), a multi-task method with joint task order optimization. MONET uses a differentiable order selection to jointly learn task-wise modules with their optimal chaining order. Furthermore, we introduce warmup and order dropout to enhance order selection by encouraging order exploration. Experimentally, we first demonstrate MONET capacity to retrieve the optimal order in a toy environment. Second, we validate MONET architecture by showing that MONET outperforms existing multi-task baselines on multiple attribute detection problems chosen for their wide range of dependency settings. More importantly, we demonstrate that MONET significantly extends state-of-the-art performance in AU detection.},
  archive  = {J},
  author   = {Gauthier Tallec and Arnaud Dapogny and Kévin Bailly},
  doi      = {10.1109/TAFFC.2022.3178524},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2876-2888},
  title    = {Multi-order networks for action unit detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frustration recognition using spatio temporal data: A novel
dataset and GCN model to recognize in-vehicle frustration. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2864–2875. (<a
href="https://doi.org/10.1109/TAFFC.2022.3229263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Frustration is an unpleasant emotion prevalent in several target applications of affective computing, such as human-machine interaction, learning, (online) customer interaction, and gaming. One idea to redeem this issue is to recognize frustration to offer help or mitigation in real-time, e.g., by a personal assistant. However, the recognition of frustration is not limited to these applied contexts but can also inform emotion research in general. This paper presents a dataset of 43 participants who experienced frustration in driving-related situations in a simulator. The data set contains a continuous subjective label, hand-annotated face and body expressions, facial landmark coordinates of two cameras, and the participants’ age and sex information. In addition, a descriptive analysis and description of the data&#39;s characteristics are provided together with a Graph Convolution Network based model to recognize frustration. Allowing for a tolerance of 10%, the model could correctly identify frustration with a similarity of 79.4 % and a variance of 7.7 %. This work is valuable for researchers of the affective computing community because it provides realistic data with an in-depth description of its characteristics and a benchmark model for automated frustration recognition. Our FRUST-dataset is publicly available under: https://ts.dlr.de/data-lake/frust-dataset/dataset.zip .},
  archive  = {J},
  author   = {Esther Bosch and Raquel Le Houcq Corbí and Klas Ihme and Stefan Hörmann and Meike Jipp and David Käthner},
  doi      = {10.1109/TAFFC.2022.3229263},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2864-2875},
  title    = {Frustration recognition using spatio temporal data: A novel dataset and GCN model to recognize in-vehicle frustration},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart affect monitoring with wearables in the wild: An
unobtrusive mood-aware emotion recognition system. <em>IEEE Transactions
on Affective Computing</em>, <em>14</em>(4), 2851–2863. (<a
href="https://doi.org/10.1109/TAFFC.2022.3232483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective computing strives to recognize a person&#39;s affective state (e.g., emotion, mood) based on what can be observed. However, electroencephalogram (EEG) and video technologies have not been widely adopted for daily life affect monitoring due to obtrusiveness and privacy concerns. Although the connection between affective states and biophysical data collected with unobtrusive wrist-worn wearables in lab settings has been established successfully, the number of studies for affect recognition in the wild is still limited, and current methods have not yet provided the accuracy necessary for robust applications. In this study, we propose a smart mood-aware emotion detection method. The proposed emotion recognition method extracts the most distinctive features from the physiological data and adds the output of the automated mood detection system as an input to improve performance. The effect of the division of self-report scales into emotion classes is also investigated. The proposed system obtained higher emotion recognition accuracies than most in-the-wild studies when we tested it with the daily life data collected from 14 participants for one week.},
  archive  = {J},
  author   = {Yekta Said CAN and Cem ERSOY},
  doi      = {10.1109/TAFFC.2022.3232483},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2851-2863},
  title    = {Smart affect monitoring with wearables in the wild: An unobtrusive mood-aware emotion recognition system},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained domain adaptation for aspect category level
sentiment analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2839–2850. (<a
href="https://doi.org/10.1109/TAFFC.2022.3228695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect category level sentiment analysis aims to identify the sentiment polarities towards the aspect categories discussed in a sentence. It usually suffers from a lack of labeled data. A popular solution is to transfer knowledge from a labeled source domain to an unlabeled target domain by unsupervised domain adaptation. However, most domain adaptation methods in sentiment analysis are coarse-grained, considering the source or target domain as a whole during the adaptation. We argue that these single-source single-target methods are inefficient since they ignore the difference between different aspect categories. In this article, we propose a fine-grained domain adaptation method to address the aspect category level sentiment analysis task by considering the adaptation between subdomains. Specifically, the source/target domain is divided into multiple subdomains according to the hierarchical structure of the aspect categories. We then design a multi-source multi-target transfer network to achieve fine-grained transfer. Extensive experimental results demonstrate the effectiveness of our fine-grained domain adaptation method on aspect category level sentiment analysis.},
  archive  = {J},
  author   = {Mengting Hu and Hang Gao and Yike Wu and Zhong Su and Shiwan Zhao},
  doi      = {10.1109/TAFFC.2022.3228695},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2839-2850},
  title    = {Fine-grained domain adaptation for aspect category level sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-structural interview-based chinese multimodal
depression corpus towards automatic preliminary screening of depressive
disorders. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2823–2838. (<a
href="https://doi.org/10.1109/TAFFC.2022.3181210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a common psychiatric disorder worldwide. However, in China, a considerable number of patients with depression are not diagnosed, and most of them are not aware of their depression. Despite increasing efforts, the goal of automatic depression screening from behavioral indicators has not been achieved. A major limitation is the lack of available multimodal depression corpus in Chinese since linguistic knowledge is crucial in clinical practice. Therefore, we first carried out a comprehensive survey with psychiatrists from a renowned psychiatric hospital to identify key interview topics which are highly related to the diagnosis of depression. Then, a semi-structural interview study was conducted over a year with subjects who have undergone clinical diagnosis and professional assessment. After that, Visual, acoustic, and textual features were extracted and analyzed between the two groups, statistically significant differences were observed in all three modalities. Benchmark evaluations of both single modal and multimodal fusion methods of depression assessment were also performed. A multimodal transformer-based fusion approach achieved the best performance. Finally, the proposed Chinese Multimodal Depression Corpus (CMDC) was made publicly available after de-identification and annotation. Hopefully, the release of this corpus would promote the research progress and practical applications of automatic depression screening.},
  archive  = {J},
  author   = {Bochao Zou and Jiali Han and Yingxue Wang and Rui Liu and Shenghui Zhao and Lei Feng and Xiangwen Lyu and Huimin Ma},
  doi      = {10.1109/TAFFC.2022.3181210},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2823-2838},
  title    = {Semi-structural interview-based chinese multimodal depression corpus towards automatic preliminary screening of depressive disorders},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the uncertainty in emotion class labels with
utterance-specific dirichlet priors. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2810–2822. (<a
href="https://doi.org/10.1109/TAFFC.2022.3221801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition is a key attribute for artificial intelligence systems that need to naturally interact with humans. However, the task definition is still an open problem due to the inherent ambiguity of emotions. In this paper, a novel Bayesian training loss based on per-utterance Dirichlet prior distributions is proposed for verbal emotion recognition, which models the uncertainty in one-hot labels created when human annotators assign the same utterance to different emotion classes. An additional metric is used to evaluate the performance by detecting test utterances with high labelling uncertainty. This removes a major limitation that emotion classification systems only consider utterances with labels where the majority of annotators agree on the emotion class. Furthermore, a frequentist approach is studied to leverage the continuous-valued “soft” labels obtained by averaging the one-hot labels. We propose a two-branch model structure for emotion classification on a per-utterance basis, which achieves state-of-the-art classification results on the widely used IEMOCAP dataset. Based on this, uncertainty estimation experiments were performed. The best performance in terms of the area under the precision-recall curve when detecting utterances with high uncertainty was achieved by interpolating the Bayesian training loss with the Kullback-Leibler divergence training loss for the soft labels. The generality of the proposed approach was verified using the MSP-Podcast dataset which yielded the same pattern of results.},
  archive  = {J},
  author   = {Wen Wu and Chao Zhang and Xixin Wu and Philip C. Woodland},
  doi      = {10.1109/TAFFC.2022.3221801},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2810-2822},
  title    = {Estimating the uncertainty in emotion class labels with utterance-specific dirichlet priors},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MIA-net: Multi-modal interactive attention network for
multi-modal affective analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2796–2809. (<a
href="https://doi.org/10.1109/TAFFC.2023.3259010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When a multi-modal affective analysis model generalizes from a bimodal task to a trimodal or multi-modal task, it is usually transformed into a hierarchical fusion model based on every two pairwise modalities, similar to a binary tree structure. This easily leads to large growth in model parameters and computation as the number of modalities increases, which limits the model&#39;s generalization. Moreover, many multi-modal fusion methods ignore that different modalities contribute differently to affective analysis. To tackle these challenges, this article proposes a general multi-modal fusion model that supports trimodal or multi-modal affective analysis tasks, called Multi-modal Interactive Attention Network (MIA-Net). Instead of treating different modalities equally, MIA-Net takes the modality that contributes the most to emotion as the main modality and the others as auxiliary modalities. MIA-Net introduces multi-modal interactive attention modules to adaptively select the important information of each auxiliary modality one by one to improve the main-modal representation. Moreover, MIA-Net enables quick generalization to trimodal or multi-modal tasks through stacking multiple MIA modules, which maintains efficient training and only requires linear computation and stable parameter counts. Experimental results of the transfer, generalization, and efficiency experiments on the widely-used datasets demonstrate the effectiveness and generalization of the proposed method.},
  archive  = {J},
  author   = {Shuzhen Li and Tong Zhang and Bianna Chen and C. L. Philip Chen},
  doi      = {10.1109/TAFFC.2023.3259010},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2796-2809},
  title    = {MIA-net: Multi-modal interactive attention network for multi-modal affective analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pars-OFF: A benchmark for offensive language detection on
farsi social media. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2787–2795. (<a
href="https://doi.org/10.1109/TAFFC.2022.3219229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing use of social media with its ability for users to share comments immediately, the extent of a system to identify offensive content has become a necessity in all languages. Due to the lack of publicly available resources on offensive language identification for Farsi, which has more than 110 million speakers, we present Pars-OFF, a three-layered annotated corpus for offensive language detection in Farsi to fill the existing gap. The introduced corpus contains 10,563 data samples. The tweets have been collected with a combination of similarity-based and keyword-based data selection techniques to avoid severe unbalancedness. Additionally, as a baseline, this article reports the performance of the traditional machine learning approaches and Transformer based models over the Pars-OFF dataset. The best performance was obtained by the BERT+fastText model, yielding the F1-Macro score of 89.57.},
  archive  = {J},
  author   = {Taha Shangipour Ataei and Kamyar Darvishi and Soroush Javdan and Amin Pourdabiri and Behrouz Minaei-Bidgoli and Mohammad Taher Pilehvar},
  doi      = {10.1109/TAFFC.2022.3219229},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2787-2795},
  title    = {Pars-OFF: A benchmark for offensive language detection on farsi social media},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TensorFormer: A tensor-based multimodal transformer for
multimodal sentiment analysis and depression detection. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2776–2786. (<a
href="https://doi.org/10.1109/TAFFC.2022.3233070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sentiment analysis is an important research field aiming to extract and fuse sentimental information from human utterances. Due to the diversity of human sentiment, analyzing from multiple modalities is usually more accurate than from a single modality. To complement the information between related modalities, one effective approach is performing cross-modality interactions. Recently, Transformer-based frameworks have shown a strong ability to capture long-range dependencies, leading to the introduction of several Transformer-based approaches for multimodal processing. However, due to the built-in attention mechanism of the Transformers, only two modalities can be engaged at once. As a result, the complementary information flow in these Transformer-based techniques is partial and constrained. To mitigate this, we propose, TensorFormer, a tensor-based multimodal Transformer framework that takes into account all relevant modalities for interactions. More precisely, we first construct a tensor utilizing the features extracted from each modality, assuming one modality is the target while the remaining tensors serve as the sources. We can generate the corresponding interacted features by calculating source-target attention. This strategy interacts with all involved modalities and generates complementing global information. Experiments on multimodal sentiment analysis benchmark datasets demonstrated the effectiveness of TensorFormer. In addition, we also evaluate TensorFormer in another related area: depression detection and the results reveal significant improvements when compared to other state-of-the-art methods.},
  archive  = {J},
  author   = {Hao Sun and Yen-Wei Chen and Lanfen Lin},
  doi      = {10.1109/TAFFC.2022.3233070},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2776-2786},
  title    = {TensorFormer: A tensor-based multimodal transformer for multimodal sentiment analysis and depression detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interaction of cognitive and affective load within a virtual
city. <em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
2768–2775. (<a
href="https://doi.org/10.1109/TAFFC.2022.3220953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatial navigation is an important aspect of everyday life but may be negatively impacted by both cognitive and affective load. Cognitive and affective load may be measured via autonomic arousal and increased load may lead to reduced navigational abilities. 53 college students (64.0% female; M age = 19.62) participated in the Virtual Reality Paced Serial Auditory Addition Task (VR-PASAT). Participants followed guides through different areas of the virtual environment (VE). In some areas participants completed the PASAT, high load, and in other areas they simply followed the guides, low load. Some participants were instructed beforehand they would perform a navigate task, increasing load. Results suggested that several psychophysiological measures including skin conductance and inter-beat intervals were impacted by increased load while others were related to the interactions between load and zone order. Awareness of the navigation task led to worse performance on the VR-PASAT, and high load decreased navigational performance. The VR-PASAT was used to implement a VE to manipulate cognitive load. This study may be useful for the creation of adaptive systems because it demonstrates that psychophysiological metrics can assess cognitive and affective load, which may impact navigation within a VE, and navigational task awareness may interact with load.},
  archive  = {J},
  author   = {Thomas D. Parsons and Justin Asbee and Christopher G. Courtney},
  doi      = {10.1109/TAFFC.2022.3220953},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2768-2775},
  title    = {Interaction of cognitive and affective load within a virtual city},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Spatial-temporal graphs plus transformers for
geometry-guided facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 2751–2767. (<a
href="https://doi.org/10.1109/TAFFC.2022.3181736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) is of great interest to the current studies of human-computer interaction. In this paper, we propose a novel geometry-guided facial expression recognition framework, based on graph convolutional networks and transformers, to perform effective emotion recognition from videos. Specifically, we detect and utilize facial landmarks to construct a spatial-temporal graph, based on both the landmark coordinates and local appearance, for representing a facial expression sequence. The graph convolutional blocks and transformer modules are employed to produce high-semantic emotion-related representations from the structured facial graphs, which facilitate the framework to establish both the local and non-local dependency between the vertices. Moreover, spatial and temporal attention mechanisms are introduced into graph-based learning to promote FER reasoning, via the emphasis on the most informative facial components and frames. Extensive experiments demonstrate that the proposed framework achieves promising performance for geometry-based FER and shows great generalization and robustness in real-world applications.},
  archive  = {J},
  author   = {Rui Zhao and Tianshan Liu and Zixun Huang and Daniel P.K. Lun and Kin-Man Lam},
  doi      = {10.1109/TAFFC.2022.3181736},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2751-2767},
  title    = {Spatial-temporal graphs plus transformers for geometry-guided facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based subject-independent emotion recognition using
gated recurrent unit and minimum class confusion. <em>IEEE Transactions
on Affective Computing</em>, <em>14</em>(4), 2740–2750. (<a
href="https://doi.org/10.1109/TAFFC.2022.3179717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic emotion recognition based on electroencephalogram (EEG) has attracted rapidly increasing interests. Due to large inter-subject variabilities, subject-independent emotion recognition faces great challenges. Recently, domain adaptation methods have been successfully applied in this field due to their ability to align features from different subjects. However, since EEG signals corresponding to some emotions have similar oscillation patterns, they are often confused and aligned to the wrong categories, which limits the generalization ability of the model across subjects. Besides, almost all methods only support offline applications, which require collecting a large number of samples of new subjects. To achieve online recognition, a simpler model is needed. In this paper, a novel Gated Recurrent Unit-Minimum Class Confusion (GRU-MCC) model is proposed. Specifically, a simple feature extractor based on gated recurrent unit (GRU) is firstly applied to model the spatial dependence of multiple electrodes and obtain high-level discriminative features. Then, during training, minimum class confusion (MCC) loss is introduced to reduce the confusion between the correct and ambiguous classes for the target subject and increase the transfer gains. We conduct both offline and online experiments on two public datasets: SEED and MPED. The results indicate that our method can obtain the superior performance.},
  archive  = {J},
  author   = {Heng Cui and Aiping Liu and Xu Zhang and Xiang Chen and Jun Liu and Xun Chen},
  doi      = {10.1109/TAFFC.2022.3179717},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2740-2750},
  title    = {EEG-based subject-independent emotion recognition using gated recurrent unit and minimum class confusion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empathetic conversational systems: A review of current
advances, gaps, and opportunities. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2722–2739. (<a
href="https://doi.org/10.1109/TAFFC.2022.3226693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Empathy is a vital factor that contributes to mutual understanding, and joint problem-solving. In recent years, a growing number of studies have recognized the benefits of empathy and started to incorporate empathy in conversational systems. We refer to this topic as empathetic conversational systems. To identify the critical gaps and future opportunities in this topic, this article examines this rapidly growing field using five review dimensions: (i) conceptual empathy models and frameworks, (ii) adopted empathy-related concepts, (iii) datasets and algorithmic techniques developed, (iv) evaluation strategies, and (v) state-of-the-art approaches. The findings show that most studies have centered on the use of the EMPATHETICDIALOGUES dataset, and the text-based modality dominates research in this field. Studies mainly focused on extracting features from the messages of the users and the conversational systems, with minimal emphasis on user modeling and profiling. Notably, studies that have incorporated emotion causes, external knowledge, and affect matching in the response generation models, have obtained significantly better results. For implementation in diverse real-world settings, we recommend that future studies should address key gaps in areas of detecting and authenticating emotions at the entity level, handling multimodal inputs, displaying more nuanced empathetic behaviors, and encompassing additional dialogue system features.},
  archive  = {J},
  author   = {Aravind Sesagiri Raamkumar and Yinping Yang},
  doi      = {10.1109/TAFFC.2022.3226693},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2722-2739},
  title    = {Empathetic conversational systems: A review of current advances, gaps, and opportunities},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion expression in human body posture and movement: A
survey on intelligible motion factors, quantification and validation.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
2697–2721. (<a
href="https://doi.org/10.1109/TAFFC.2022.3226252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many areas in computer science are facing the need to analyze, quantify and reproduce movements expressing emotions. This paper presents a systematic review of the intelligible factors involved in the expression of emotions in human movement and posture. We have gathered the works that have studied and tried to identify these factors by sweeping many disciplinary fields such as psychology, biomechanics, choreography, robotics and computer vision. These researches have each used their own definitions, units and emotions, which prevents a global and coherent vision. We propose a meta-analysis approach that cross-references and aggregates these researches in order to have a unified list of expressive factors quantified for each emotion. A calculation method is then proposed for each of the expressive factors and we extract them from an emotionally annotated animation dataset: Emilya. The comparison between the results of the meta-analysis and the Emilya analysis reveals high correlation rates, which validates the relevance of the quantified values obtained by both methodologies. The analysis of the results raises interesting perspectives for future research in affective computing.},
  archive  = {J},
  author   = {Mehdi-Antoine Mahfoudi and Alexandre Meyer and Thibaut Gaudin and Axel Buendia and Saida Bouakaz},
  doi      = {10.1109/TAFFC.2022.3226252},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2697-2721},
  title    = {Emotion expression in human body posture and movement: A survey on intelligible motion factors, quantification and validation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey on emotion sensing using mobile devices. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2678–2696. (<a
href="https://doi.org/10.1109/TAFFC.2022.3220484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid development and ubiquity of mobile and wearable devices promises to enable researchers to monitor users’ granular emotional data in a less intrusive manner. Researchers have used a wide variety of mobile and wearable devices for this purpose, and have proposed various approaches to sense users’ emotional states. In this survey, we utilise three established digital libraries ( ACM Digital Library , IEEE Xplore Digital Library , and Springer Nature ). We analysed and critically assessed the different approaches used in the three stages (perception, learning, inference) of a typical mobile emotion sensing framework, following a structured paper selection process. The contribution of this survey is three-fold; first, we document all the latest relevant literature on mobile emotion sensing research; second, we describe how mobile and wearable devices use their sensing and computing capabilities to monitor human emotions; third, we discuss challenges and opportunities of mobile emotion sensing to demonstrate the potential of this thriving field of research.},
  archive  = {J},
  author   = {Kangning Yang and Benjamin Tag and Chaofan Wang and Yue Gu and Zhanna Sarsenbayeva and Tilman Dingler and Greg Wadley and Jorge Goncalves},
  doi      = {10.1109/TAFFC.2022.3220484},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2678-2696},
  title    = {Survey on emotion sensing using mobile devices},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based facial affect analysis: A review. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2657–2677. (<a
href="https://doi.org/10.1109/TAFFC.2022.3215918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As one of the most important affective signals, facial affect analysis (FAA) is essential for developing human-computer interaction systems. Early methods focus on extracting appearance and geometry features associated with human affects while ignoring the latent semantic information among individual facial changes, leading to limited performance and generalization. Recent work attempts to establish a graph-based representation to model these semantic relationships and develop frameworks to leverage them for various FAA tasks. This article provides a comprehensive review of graph-based FAA, including the evolution of algorithms and their applications. First, the FAA background knowledge is introduced, especially on the role of the graph. We then discuss approaches widely used for graph-based affective representation in literature and show a trend towards graph construction. For the relational reasoning in graph-based FAA, existing studies are categorized according to their non-deep or deep learning methods, emphasizing the latest graph neural networks. Performance comparisons of the state-of-the-art graph-based FAA methods are also summarized. Finally, we discuss the challenges and potential directions. As far as we know, this is the first survey of graph-based FAA methods. Our findings can serve as a reference for future research in this field.},
  archive  = {J},
  author   = {Yang Liu and Xingming Zhang and Yante Li and Jinzhao Zhou and Xin Li and Guoying Zhao},
  doi      = {10.1109/TAFFC.2022.3215918},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2657-2677},
  title    = {Graph-based facial affect analysis: A review},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A neural predictive model of negative emotions for COVID-19.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(4),
2646–2656. (<a
href="https://doi.org/10.1109/TAFFC.2022.3181671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The long-lasting global pandemic of Coronavirus disease 2019 (COVID-19) has changed our daily life in many ways and put heavy burden on our mental health. Having a predictive model of negative emotions during COVID-19 is of great importance for identifying potential risky population. To establish a neural predictive model achieving both good interpretability and predictivity, we have utilized a large-scale (n = 542) longitudinal dataset, alongside two independent samples for external validation. We built a predictive model based on psychologically meaningful resting state neural activities. The whole-brain resting-state neural activity and social-psychological profile of the subjects were obtained from Sept. to Dec. 2019 (Time 1). Their negative emotions were tracked and re-assessed twice, on Feb 22 (Time 2) and Apr 24 (Time 3), 2020, respectively. We first applied canonical correlation analysis on both the neural profiles and psychological profiles collected on Time 1, this step selects only the psychological meaningful neural patterns for later model construction. We then trained the neural predictive model using those identified features on data obtained on Time 2. It achieved a good prediction performance (r = 0.44, p = 8.13 × 10 -27 ). The two most important neural predictors are associated with self-control and social interaction. This study established an effective neural prediction model of negative emotions, achieving good interpretability and predictivity. It will be useful for identifying potential risky population of emotional disorders related to COVID-19.},
  archive  = {J},
  author   = {Yu Mao and Dongtao Wei and Wenjing Yang and Qunlin Chen and Jiangzhou Sun and Yaxu Yu and Yu Li and Kaixiang Zhuang and Xiaoqin Wang and Li He and Tingyong Feng and Xu Lei and Qinghua He and Hong Chen and Shaozheng Qin and Yunzhe Liu and Jiang Qiu},
  doi      = {10.1109/TAFFC.2022.3181671},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2646-2656},
  title    = {A neural predictive model of negative emotions for COVID-19},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual reality for emotion elicitation – a review. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(4), 2626–2645. (<a
href="https://doi.org/10.1109/TAFFC.2022.3181053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions are multifaceted phenomena that affect our behaviour, perception, and cognition. Increasing evidence indicates that induction mechanisms play a crucial role in triggering emotions by simulating the sensations required for an experimental design. Over the years, many reviews have evaluated a passive elicitation mechanism where the user is an observer, ignoring the importance of self-relevance in emotional experience. So, in response to the gap in the literature, this study intends to explore the possibility of using Virtual Reality (VR) as an active mechanism for emotion induction. VR can simulate controlled environments with high immersion, presence, and interaction to induce intense emotions. Therefore, researchers can evaluate emotional experiences in a realistic context. For the success and quality of research settings, VR must select the appropriate material to effectively evoke emotions. Therefore, in the present review, we evaluated to what extent VR virtual environments, videos, games, tasks, avatar, images, and 360-degree panoramas can elicit emotions. Further, we present public datasets, discuss challenges and recommendations, and review emotion-sensing interfaces related to VR research. The conclusions reveal the VR&#39;s potential to evoke emotions effectively and naturally by generating motivational and empathy mechanisms, which makes it an ecologically valid paradigm to study emotions.},
  archive  = {J},
  author   = {Rukshani Somarathna and Tomasz Bednarz and Gelareh Mohammadi},
  doi      = {10.1109/TAFFC.2022.3181053},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2626-2645},
  title    = {Virtual reality for emotion elicitation – a review},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group synchrony for emotion recognition using physiological
signals. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2614–2625. (<a
href="https://doi.org/10.1109/TAFFC.2023.3265433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {During group interactions, we react and modulate our emotions and behaviour to the group through phenomena including emotion contagion and physiological synchrony. Previous work on emotion recognition through video/image has shown that group context information improves the classification performance. However, when using physiological data, literature mostly focuses on intrapersonal models that leave-out group information, while interpersonal models are unexplored. This paper introduces a new interpersonal Weighted Group Synchrony approach, which relies on Electrodermal Activity (EDA) and Heart-Rate Variability (HRV). We perform an analysis of synchrony metrics applied across diverse data representations (EDA and HRV morphology and features, recurrence plot, spectrogram), to identify which metrics and modalities better characterise physiological synchrony for emotion recognition. We explored two datasets (AMIGOS and K-EmoCon), covering different group sizes (4 vs dyad) and group-based activities (video-watching vs conversation). The experimental results show that integrating group information improves arousal and valence classification, across all datasets, with the exception of K-EmoCon on valence. The proposed method was able to attain mean M-F1 of $\approx$ 72.15% arousal and 81.16% valence for AMIGOS, and M-F1 of $\approx$ 52.63% arousal, 65.09% valence for K-EmoCon, surpassing previous work results for K-EmoCon on arousal, and providing a new baseline on AMIGOS for long-videos.},
  archive  = {J},
  author   = {Patrícia Bota and Tianyi Zhang and Abdallah El Ali and Ana Fred and Hugo Plácido da Silva and Pablo Cesar},
  doi      = {10.1109/TAFFC.2023.3265433},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2614-2625},
  title    = {Group synchrony for emotion recognition using physiological signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two birds with one stone: Knowledge-embedded temporal
convolutional transformer for depression detection and emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(4), 2595–2613. (<a
href="https://doi.org/10.1109/TAFFC.2023.3282704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a critical problem in modern society that affects an estimated 350 million people worldwide, causing feelings of sadness and a lack of interest and pleasure. Emotional disorders are gaining interest and are closely entwined with depression, because one contributes to an understanding of the other. Despite the achievements in the two separate tasks of emotion recognition and depression detection, there has not been much prior effort to build a unified model that can connect these two tasks with different modalities, including multimedia (text, audio, and video) and unobtrusive physiological signals (e.g., electroencephalography). We propose a novel temporal convolutional transformer with knowledge embedding to address the joint task of depression detection and emotion recognition. This approach not only learns multimodal embeddings across domains via the temporal convolutional transformer but also exploits special-domain knowledge from medical knowledge graphs to improve the performance of detection and recognition. It is essential that the features learned by our method can be perceived as a priori and are suitable for increasing the performance of other related tasks. Our method illustrates the case of “two birds with one stone” in the sense that two or more tasks can be efficiently handled with our unique model, which captures effective features. Experimental results on ten real-world datasets show that the proposed approach significantly outperforms other state-of-the-art approaches. On the other hand, experiments in which our methodology is applied to other reasoning tasks show that our approach effectively supports model reasoning related to emotion and improves its performance.},
  archive  = {J},
  author   = {Wenbo Zheng and Lan Yan and Fei-Yue Wang},
  doi      = {10.1109/TAFFC.2023.3282704},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2595-2613},
  title    = {Two birds with one stone: Knowledge-embedded temporal convolutional transformer for depression detection and emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion arousal assessment based on multimodal physiological
signals for game users. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2582–2594. (<a
href="https://doi.org/10.1109/TAFFC.2023.3265008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional arousal, an essential dimension of game users’ experience, plays a crucial role in determining whether a game is successful. Game users’ emotion arousal assessment (GUEA) is of great importance. However, GUEA often faces challenges, such as selecting emotion-inducing games, labeling emotional arousal, and improving accuracy. In this study, the scheme for verifying the effectiveness of emotion-induced games is proposed so that the selected games can induce the target emotions. In addition, the personalized arousal label generation method is developed to reduce the errors caused by individual differences among subjects. Furthermore, to improve the accuracy of GUEA, the Breath Rate Variability (BRV) signal is used as a GUEA indicator along with commonly used physiological signals. Comparative experiments on GUEA based on multimodal physiological signals are conducted. The experimental result shows that the accuracy of GUEA is improved by adding the BRV signal, up to 92%.},
  archive  = {J},
  author   = {Rongyang Li and Jianguo Ding and Huansheng Ning},
  doi      = {10.1109/TAFFC.2023.3265008},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2582-2594},
  title    = {Emotion arousal assessment based on multimodal physiological signals for game users},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WiFE: WiFi and vision based unobtrusive emotion recognition
via gesture and facial expression. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(4), 2567–2581. (<a
href="https://doi.org/10.1109/TAFFC.2023.3285777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion plays a critical role in making the computer more human-like. As the first and most essential step, emotion recognition emerges recently as a hot but relatively nascent topic, i.e., current research mainly focuses on single modality (e.g., facial expression) while human emotion expressions are multi-modal in nature. To this end, we propose an unobtrusive emotion recognition system leveraging two emotion-rich and tightly-coupled modalities, i.e., gesture and facial expression. The system design faces two major challenges, namely, how to capture the emotional expression in both modalities without disturbing the subject and how to leverage the relationship between modalities for recognizing the emotion. For the former, we explore WiFi and vision for unobtrusive and contactless gesture and facial expression sensing, respectively. For the latter, we propose a novel deep learning framework named Multi-Source Learning (MSL) to efficiently exploit both self-correlation in the modality and cross-correlation between modalities for fine-grained emotion recognition. To evaluate the proposed method, we prototype the system on low-cost commodity WiFi and vision devices, build a first-of-its-kind WiFi-Vision emotion dataset, and conduct extensive experiments. Empirical results not only verify the effectiveness of WiFE in emotion recognition, but also confirm the superiority of multi-modality over single-modality.},
  archive  = {J},
  author   = {Yu Gu and Xiang Zhang and Huan Yan and Jingyang Huang and Zhi Liu and Mianxiong Dong and Fuji Ren},
  doi      = {10.1109/TAFFC.2023.3285777},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2567-2581},
  title    = {WiFE: WiFi and vision based unobtrusive emotion recognition via gesture and facial expression},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Special issue on unobtrusive physiological
measurement methods for affective applications. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(4), 2564–2566. (<a
href="https://doi.org/10.1109/TAFFC.2023.3286769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In The formative years of Affective Computing [1], from the late 1990s and into the early 2000s, a significant fraction of research attention was focused on the development of methods for unobtrusive physiological measurement . It quickly became obvious that wiring people with electrodes and strapping cumbersome hardware to their bodies was not only restricting the types of experiments that could be performed but also was not conducive to unbiased observations. For instance, subjects with fingers wrapped with electrodermal activity (EDA) and photoplethysmography (PPG) sensors could hardly type, drive or sleep comfortably. Hence, there was a need for more elegant and scalable physiological measurement methods [2].},
  archive  = {J},
  author   = {Ioannis T. Pavlidis and Theodora Chaspari and Daniel McDuff},
  doi      = {10.1109/TAFFC.2023.3286769},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {2564-2566},
  title    = {Editorial: Special issue on unobtrusive physiological measurement methods for affective applications},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LQGDNet: A local quaternion and global deep network for
facial depression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2557–2563. (<a
href="https://doi.org/10.1109/TAFFC.2021.3139651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent visual-based depression recognition methods mostly use hand-crafted features with information lost in color channels, or deep network features with a limited performance from the finite data. In this paper, we propose a method called Local Quaternion and Global Deep Network (LQGDNet) which can combine advantages from hand-crafted and deep features. Specifically, the Quaternion XOR Asymmetrical Regional Local Gradient Coding (XOR-AR-LGC) is first designed, which encodes the facial images with local textures in the quaternion domain to keep the dependence of color channels, and integrated into the Quaternion Feature Extractor (QFE). To the best of our knowledge, it is the first attempt to use a quaternion-based method for facial depression recognition. Second, we design the Local Quaternion Representation Module (LQRM) composed of Local Deep Feature Extractor (LDFE) and QFE to output local quaternion facial features. Third, global deep facial features are encoded from the Global Deep Representation Module (GDRM) with the deep convolutional neural network. Finally, the LQGDNet integrates LQRM and GDRM with the local quaternion and global deep features and predicts the depression score. The experimental results on AVEC 2013 and AVEC 2014 show the superiority of our method compared to the state-of-the-art approaches.},
  archive  = {J},
  author   = {Yuanyuan Shang and Yuchen Pan and Xiao Jiang and Zhuhong Shao and Guodong Guo and Tie Liu and Hui Ding},
  doi      = {10.1109/TAFFC.2021.3139651},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2557-2563},
  title    = {LQGDNet: A local quaternion and global deep network for facial depression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A computational model of coping and decision making in
high-stress, uncertain situations: An application to hurricane
evacuation decisions. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2539–2556. (<a
href="https://doi.org/10.1109/TAFFC.2022.3173812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {People often encounter highly stressful, emotion-evoking situations. Modeling and predicting people&#39;s behavior in such situations, how they cope, is a critical research topic. To that end, we propose a computational model of coping that casts Lazarus&#39;s theory of coping into a Partially Observable Markov Decision Process (POMDP) framework. This includes an appraisal process that models the factors leading to stress by assessing a person&#39;s relation to the environment and a coping process that models how people seek to reduce stress by directly altering the environment or changing one&#39;s beliefs and goals. We evaluated the model&#39;s assumptions in the context of a high-stress situation, hurricanes. We collected questionnaire data from major U.S. hurricanes in 2018 to evaluate the model&#39;s features for appraisal calculation. We also conducted a series of controlled experiments simulating a hurricane experience to investigate how people change their beliefs and goals to cope with the situation. The results support the model&#39;s assumptions showing that the proposed features are significantly associated with the evacuation decisions and people change their beliefs and goals to cope with the situation.},
  archive  = {J},
  author   = {Nutchanon Yongsatianchot and Stacy Marsella},
  doi      = {10.1109/TAFFC.2022.3173812},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2539-2556},
  title    = {A computational model of coping and decision making in high-stress, uncertain situations: An application to hurricane evacuation decisions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta auxiliary learning for facial action unit detection.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2526–2538. (<a
href="https://doi.org/10.1109/TAFFC.2021.3135516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite the success of deep neural networks on facial action unit (AU) detection, better performance depends on a large number of training images with accurate AU annotations. However, labeling AU is time-consuming, expensive, and error-prone. Considering AU detection and facial expression recognition (FER) are two highly correlated tasks, and facial expression (FE) is relatively easy to annotate, we consider learning AU detection and FER in a multi-task manner. However, the performance of the AU detection task cannot be always enhanced due to the negative transfer in the multi-task scenario. To alleviate this issue, we propose a Meta Auxiliary Learning method (MAL) that automatically selects highly related FE samples by learning adaptative weights for the training FE samples in a meta learning manner. The learned sample weights alleviate the negative transfer from two aspects: 1) balance the loss of each task automatically, and 2) suppress the weights of FE samples that have large uncertainties. Experimental results on several popular AU datasets demonstrate MAL consistently improves the AU detection performance compared with the state-of-the-art multi-task and auxiliary learning methods. MAL automatically estimates adaptive weights for the auxiliary FE samples according to their semantic relevance with the primary AU detection task.},
  archive  = {J},
  author   = {Yong Li and Shiguang Shan},
  doi      = {10.1109/TAFFC.2021.3135516},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2526-2538},
  title    = {Meta auxiliary learning for facial action unit detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GMSS: Graph-based multi-task self-supervised learning for
EEG emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2512–2525. (<a
href="https://doi.org/10.1109/TAFFC.2022.3170428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previous electroencephalogram (EEG) emotion recognition relies on single-task learning, which may lead to overfitting and learned emotion features lacking generalization. In this paper, a graph-based multi-task self-supervised learning model (GMSS) for EEG emotion recognition is proposed. GMSS has the ability to learn more general representations by integrating multiple self-supervised tasks, including spatial and frequency jigsaw puzzle tasks, and contrastive learning tasks. By learning from multiple tasks simultaneously, GMSS can find a representation that captures all of the tasks thereby decreasing the chance of overfitting on the original task, i.e., emotion recognition task. In particular, the spatial jigsaw puzzle task aims to capture the intrinsic spatial relationships of different brain regions. Considering the importance of frequency information in EEG emotional signals, the goal of the frequency jigsaw puzzle task is to explore the crucial frequency bands for EEG emotion recognition. To further regularize the learned features and encourage the network to learn inherent representations, contrastive learning task is adopted in this work by mapping the transformed data into a common feature space. The performance of the proposed GMSS is compared with several popular unsupervised and supervised methods. Experiments on SEED, SEED-IV, and MPED datasets show that the proposed model has remarkable advantages in learning more discriminative and general features for EEG emotional signals.},
  archive  = {J},
  author   = {Yang Li and Ji Chen and Fu Li and Boxun Fu and Hao Wu and Youshuo Ji and Yijin Zhou and Yi Niu and Guangming Shi and Wenming Zheng},
  doi      = {10.1109/TAFFC.2022.3170428},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2512-2525},
  title    = {GMSS: Graph-based multi-task self-supervised learning for EEG emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning of subject-invariant EEG
representations for cross-subject emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 2496–2511. (<a
href="https://doi.org/10.1109/TAFFC.2022.3164516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {EEG signals have been reported to be informative and reliable for emotion recognition in recent years. However, the inter-subject variability of emotion-related EEG signals still poses a great challenge for the practical applications of EEG-based emotion recognition. Inspired by recent neuroscience studies on inter-subject correlation, we proposed a Contrastive Learning method for Inter-Subject Alignment (CLISA) to tackle the cross-subject emotion recognition problem. Contrastive learning was employed to minimize the inter-subject differences by maximizing the similarity in EEG signkal representations across subjects when they received the same emotional stimuli in contrast to different ones. Specifically, a convolutional neural network was applied to learn inter-subject aligned spatiotemporal representations from EEG time series in contrastive learning. The aligned representations were subsequently used to extract differential entropy features for emotion classification. CLISA achieved state-of-the-art cross-subject emotion recognition performance on our THU-EP dataset with 80 subjects and the publicly available SEED dataset with 15 subjects. It could generalize to unseen subjects or unseen emotional stimuli in testing. Furthermore, the spatiotemporal representations learned by CLISA could provide insights into the neural mechanisms of human emotion processing.},
  archive  = {J},
  author   = {Xinke Shen and Xianggen Liu and Xin Hu and Dan Zhang and Sen Song},
  doi      = {10.1109/TAFFC.2022.3164516},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2496-2511},
  title    = {Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint local-global discriminative subspace transfer learning
for facial expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2484–2495. (<a
href="https://doi.org/10.1109/TAFFC.2022.3168834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional facial expression recognition (FER) has achieved satisfactory results to some extent, and most of the current methods are trained and evaluated on a single database. However, in real applications, the training and testing images are often collected in different scenarios, which will lead to performance degeneration. To tackle this problem, in this paper, we propose a novel transfer learning approach, named joint local-global discriminative subspace transfer learning (LGDSTL), for cross-database FER. In LGDSTL, first, we develop a joint local-global graph as the distance metric, in which we not only consider the local discriminative geometric structure for each database, but also consider a global graph to transfer knowledge. In this way, the discrepancy between the two databases will be significantly reduced. Then, we present a pairwise regression function to guide the discriminative subspace transfer learning. Additionally, a data reconstruction constraint is introduced to preserve the main discriminative information. Finally, comparative studies on six popular benchmarks demonstrate the effectiveness of the proposed approach.},
  archive  = {J},
  author   = {Wenjing Zhang and Peng Song and Wenming Zheng},
  doi      = {10.1109/TAFFC.2022.3168834},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2484-2495},
  title    = {Joint local-global discriminative subspace transfer learning for facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion distribution learning based on peripheral
physiological signals. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2470–2483. (<a
href="https://doi.org/10.1109/TAFFC.2022.3163609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion analysis based on peripheral physiological signals has attracted increasing attention recently in affective computing. Previous works usually predict emotional states using a single emotion label for each discrete time. However, in real-world scenarios, it is not sufficient due to the fact that the real-world emotional state is usually a mixture of basic emotions. In this paper, we formulate the emotion analysis as an emotion distribution learning (EDL) problem and make two contributions. First, we establish a standardised dataset containing four negative emotions (anger, disgust, sadness, fear) and three positive emotions (tenderness, joy, amusement), which could be a useful benchmark for the EDL task. Second, we propose an emotion distribution prediction system which has the following distinct characteristics: (1) after processing raw peripheral physiological signals, we compute totally 89 representative features from four channels, i.e., GSR, SKT, ECG and HR, (2) an adaptive feature selection strategy based on recursive feature elimination (RFE) is used to select the most significant features in our EDL task, and (3) we design a dedicated EDL model based on convolution neural networks that takes information from both the feature correlation and the time domain into consideration. Experiments were conducted to validate our proposed system, and the results indicated that (1) the proposed feature selection strategy effectively selects significant features and improves algorithmic performance, and (2) the proposed EDL model can obtain good results in terms of six evaluation measures and outperform existing methods.},
  archive  = {J},
  author   = {Yezhi Shu and Pei Yang and Niqi Liu and Shu Zhang and Guozhen Zhao and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2022.3163609},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2470-2483},
  title    = {Emotion distribution learning based on peripheral physiological signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based emotion recognition with emotion localization via
hierarchical self-attention. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2458–2469. (<a
href="https://doi.org/10.1109/TAFFC.2022.3145623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition based on electroencephalography (EEG) has attracted significant attention due to its wide range of applications, especially in Human-Computer Interaction(HCI). Previous research treats different segments of EEG signals uniformly, ignoring the fact that emotions are unstable and discrete during an extended period. In this paper, we propose a novel two-step spatial-temporal emotion recognition framework. First, considering that the human emotion has not only ”short-term continuity” but also ”long-term similarity”, we propose a hierarchical self-attention network to jointly model local and global temporal information, so as to localize most related segments and reduce the influence of noise at the temporal level. Second, in order to extract discriminative features at the spatial level to enhance the emotion recognition performance, we further employ the squeeze-and-excitation module (SE module) along with the channel correlation loss (CC-Loss) to select the most task-related channels. We also define a new task called emotion localization , which aims to localize fragments with stronger emotions. We evaluate the proposed method on the proposed emotion localization task and typical emotion recognition task with three publicly available datasets, i.e., SEED, DEAP, and MAHNOB-HCI. The experimental results demonstrate that the proposed approach outperforms state-of-the-art methods.},
  archive  = {J},
  author   = {Yuzhe Zhang and Huan Liu and Dalin Zhang and Xuxu Chen and Tao Qin and Qinghua Zheng},
  doi      = {10.1109/TAFFC.2022.3145623},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2458-2469},
  title    = {EEG-based emotion recognition with emotion localization via hierarchical self-attention},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of physiological signals in different types of
multimodal sentiment estimation. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2443–2457. (<a
href="https://doi.org/10.1109/TAFFC.2022.3155604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal sentiment analysis has become a focus of research in recent years. However, most studies of multimodal sentiment analysis have considered only signals that are observable by humans, such as linguistic, audio and visual information, whereas the contribution of the multimodal fusion of such signals with unobservable signals, i.e., physiological signals, has not been comprehensively explored. In this study, we aim to investigate effects of physiological signals in multimodal sentiment analysis by evaluating all of the fusion models for different types of sentiment estimation in naturalistic human-agent interaction settings. Our results suggest that physiological features are effective in the unimodal model and that the fusion of linguistic representations with physiological features provides the best results for estimating self-sentiment labels as annotated by the users themselves. In contrast, the tensor fusion of linguistic representations with audiovisual features is effective for estimating sentiment labels as annotated by a third party in regression tasks, which can be derived from the corresponding signals that are observable by humans. A detailed analysis of the self-sentiment estimation results suggests that different modalities play different roles in sentiment estimation, and corresponding implications are discussed.},
  archive  = {J},
  author   = {Shun Katada and Shogo Okada and Kazunori Komatani},
  doi      = {10.1109/TAFFC.2022.3155604},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2443-2457},
  title    = {Effects of physiological signals in different types of multimodal sentiment estimation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Challenges in evaluating technological interventions for
affect regulation. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2430–2442. (<a
href="https://doi.org/10.1109/TAFFC.2022.3175687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We evaluated whether a vibrotactile breathing pacer would influence two measures of affect during a cognitive stressor. In particular, we examined whether changes in breathing would be evident, and if so, whether these would mediate the effects of breathing pacer on self-report anxiety and skin conductance. Our results were surprising: although we observed the expected effects on breathing, we were unable to demonstrate that changes in breathing parameters were responsible for the observed changes in self-report anxiety. In addition, we failed to show that the pacer had an impact on skin conductance. In this paper, we investigate why we did not observe the expected effects. We believe our negative results have implications for evaluating technological interventions for affect regulation.},
  archive  = {J},
  author   = {Pardis Miri and Horia Margarit and Andero Uusberg and Keith Marzullo and Tali M. Ball and Daniel Yamins and Robert Flory and James J. Gross},
  doi      = {10.1109/TAFFC.2022.3175687},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2430-2442},
  title    = {Challenges in evaluating technological interventions for affect regulation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMIN: Semi-supervised multi-modal interaction network for
conversational emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2415–2429. (<a
href="https://doi.org/10.1109/TAFFC.2022.3141237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conversational emotion recognition is a crucial research topic in human-computer interactions. Due to the heavy annotation cost and inevitable label ambiguity, collecting large amounts of labeled data is challenging and expensive, which restricts the performance of current fully-supervised methods in this domain. To address this problem, researchers attempt to distill knowledge from unlabeled data via semi-supervised learning. However, most of these semi-supervised methods ignore multimodal interactive information, although recent works have proven that such interactive information is essential for emotion recognition. To this end, we propose a novel framework to seamlessly integrate semi-supervised learning with multimodal interactions, called “Semi-supervised Multi-modal Interaction Network (SMIN)”. SMIN contains two well-designed semi-supervised modules, “Intra-modal Interactive Module (IIM)” and “Cross-modal Interactive Module (CIM)” to learn intra- and cross-modal interactions. These two modules leverage additional unlabeled data to extract emotion-salient representations. To capture additional contextual information, we utilize the hierarchical recurrent networks followed with the hybrid fusion strategy to integrate multimodal features. These multimodal features are further utilized for conversational emotion recognition. Experimental results on four benchmark datasets (i.e., IEMOCAP, MELD, CMU-MOSI and CMU-MOSEI) demonstrate that SMIN succeeds over existing state-of-the-art strategies on emotion recognition.},
  archive  = {J},
  author   = {Zheng Lian and Bin Liu and Jianhua Tao},
  doi      = {10.1109/TAFFC.2022.3141237},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2415-2429},
  title    = {SMIN: Semi-supervised multi-modal interaction network for conversational emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting facial expression recognition by a semi-supervised
progressive teacher. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2402–2414. (<a
href="https://doi.org/10.1109/TAFFC.2021.3131621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we aim to improve the performance of in-the-wild Facial Expression Recognition (FER) by exploiting semi-supervised learning. Large-scale labeled data and deep learning methods have greatly improved the performance of image recognition. However, the performance of FER is still not ideal due to the lack of training data and incorrect annotations (e.g., label noises). Among existing in-the-wild FER datasets, reliable ones contain insufficient data to train robust deep models while large-scale ones are annotated in lower quality. To address this problem, we propose a semi-supervised learning algorithm named Progressive Teacher (PT) to utilize reliable FER datasets as well as large-scale unlabeled expression images for effective training. On the one hand, PT introduces semi-supervised learning method to relieve the shortage of data in FER. On the other hand, it selects useful labeled training samples automatically and progressively to alleviate label noise. PT uses selected clean labeled data for computing the supervised classification loss and unlabeled data for unsupervised consistency loss. Experiments on widely-used databases RAF-DB and FERPlus validate the effectiveness of our method, which achieves state-of-the-art performance with accuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate reaches even 30%, the performance of our PT algorithm only degrades by 4.37%.},
  archive  = {J},
  author   = {Jing Jiang and Weihong Deng},
  doi      = {10.1109/TAFFC.2021.3131621},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2402-2414},
  title    = {Boosting facial expression recognition by a semi-supervised progressive teacher},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-invasive measurement of trust in group interactions.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2389–2401. (<a
href="https://doi.org/10.1109/TAFFC.2022.3160132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Trust between group members has many implications for how well a group performs. In this study, we predict perceived trustworthiness of group members when there are subversive group members. We collected multimodal verbal and nonverbal data from a group interaction experiment. During the interaction, we periodically surveyed the group members about their perceptions of trustworthiness of other group members. We used this data to model the relationship between observable behavior and perceptions of trustworthiness. We report the most predictive features and describe them in the context of existing literature on verbal and nonverbal correlates of trust. This research advances the study of behavioral measurement in groups and the role of behavior on perceived trustworthiness.},
  archive  = {J},
  author   = {Lee A. Spitzley and Xinran Wang and Xunyu Chen and Steven J. Pentland and Jay F. Nunamaker and Judee K. Burgoon and Norah E. Dunbar},
  doi      = {10.1109/TAFFC.2022.3160132},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2389-2401},
  title    = {Non-invasive measurement of trust in group interactions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methodology to assess quality, presence, empathy, attitude,
and attention in 360-degree videos for immersive communications.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2375–2388. (<a
href="https://doi.org/10.1109/TAFFC.2022.3149162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes a methodology to assess video quality, spatial and social presence, empathy, attitude, and attention in 360-degree videos for immersive communications. The methodology is validated in an experiment which simulates an immersive communication environment where participants attend three conversations of different genre (everyday conversation, educational, and discussion) and from actor and observer acquisition perspectives. We consider three experimental conditions: (A) visualizing and rating the perceptual quality of contents in a Head-Mounted Display (HMD), (B) visualizing the contents in an HMD, and (C) visualizing the contents in an HMD where participants can see their hands and take notes. In all conditions participants visualize the same 360-degree videos, designed and acquired in the context of international experiences. Fifty-four participants were evenly distributed among A, B, and C conditions taking into account their international experience backgrounds (working or studying in a foreign country), obtaining a balanced and diverse sample of participants. In this paper, video quality is evaluated with Single-Stimulus Discrete Quality Evaluation (SSDQE) methodology. Spatial and social presence are evaluated with questionnaires adapted from the literature. Initial empathy is assessed with Interpersonal Reactivity Index (IRI) and a questionnaire is designed to evaluate the attitude after the visualization of each video. Attention is evaluated with three questions about the conversations of the contents that had pass/fail answers. The results from the subjective test validate the proposed methodology in immersive communications, showing that video quality experiments can be adapted to conditions imposed by experiments focused on the evaluation of socioemotional features in terms of contents of long-duration, different acquisition perspectives, and genre. In addition, the positive results related to the sense of social and spatial presence imply that technology can be relevant in the analyzed use case. Other main result is that the acquisition perspective greatly influences social presence. Finally, the annotated dataset, Student Experiences Around the World dataset (SEAW-dataset), obtained from the experiment is made publicly available for the research community.},
  archive  = {J},
  author   = {Marta Orduna and Pablo Pérez and Jesús Gutiérrez and Narciso García},
  doi      = {10.1109/TAFFC.2022.3149162},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2375-2388},
  title    = {Methodology to assess quality, presence, empathy, attitude, and attention in 360-degree videos for immersive communications},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adversarial training based speech emotion classifier with
isolated gaussian regularization. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2361–2374. (<a
href="https://doi.org/10.1109/TAFFC.2022.3169091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speaker individual bias may cause emotion-related features to form clusters with irregular borders (non-Gaussian distributions), making the model sensitive to local irregularities of pattern distributions, resulting in the model over-fit of the in-domain dataset. This problem may cause a decrease in the validation scores in cross-domain (i.e., speaker-independent, channel-variant) implementation. To mitigate this problem, in this paper, we propose an adversarial training-based classifier to regularize the distribution of latent representations to further smooth the boundaries among different categories. In the regularization phase, the representations are mapped into Gaussian distributions in an unsupervised manner to improve the discriminative ability of the latent representations. A single Gaussian distribution is used for mapping the latent representations in our previous study. In this presented work, we adopt a mixture of isolated Gaussian distributions. Moreover, multi-instance learning was adopted by dividing speech into a bag of segments to capture the most salient part of presenting an emotion. The model was evaluated on the IEMOCAP and MELD datasets with in-corpus speaker-independent sittings. In addition, we investigated the accuracy of cross-corpus sittings in simulating speaker-independent and channel-variants. In the experiment, the proposed model was compared not only with baseline models but also with different configurations of our model. The results show that the proposed model is competitive with respect to the baseline, as demonstrated both by in-corpus and cross-corpus validation.},
  archive  = {J},
  author   = {Changzeng Fu and Chaoran Liu and Carlos Toshinori Ishi and Hiroshi Ishiguro},
  doi      = {10.1109/TAFFC.2022.3169091},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2361-2374},
  title    = {An adversarial training based speech emotion classifier with isolated gaussian regularization},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavior-based ethical understanding in chinese social news.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2349–2360. (<a
href="https://doi.org/10.1109/TAFFC.2022.3160745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ethical understanding aims at morally analyzing and discriminating ethical scenarios described in natural language. By classifying behaviors that occur in ethical scenarios as ethical or unethical, ethical understanding empowers artificial intelligence systems to understand human values so as to discern right from wrong morally. However, most existing ethical understanding methods lack fine-grained analysis and cannot handle the problem that an ethical scenario may contain multiple behaviors with multiple polarities. In this paper, we introduce a novel natural language processing task, behavior-based ethical understanding (BEU), for mining the purpose relation(s) and ethical polarity of a specific behavior from the social news. It contains three subtasks: behavior term extraction (BTE) to extracts behavior terms, purpose relation inference (PRI) to identifies purposive relations among behaviors, and polarity discrimination (PD) to predicts the ethical polarities of behaviors, respectively. To perform this task, we constructed a Chinese BEU dataset, named FG-ETHICS. Besides, we propose a three-stage framework, BEU-BERT, based on the pre-trained language model BERT and deliberately designed downstream models for three subtasks. Experimental results show that the proposed framework achieves the best performance from the BTE and PD tasks, and achieves a promising performance of 75% on the PRI task.},
  archive  = {J},
  author   = {Xuan Feng and Tianlong Gu and Xuguang Bao and Long Li},
  doi      = {10.1109/TAFFC.2022.3160745},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2349-2360},
  title    = {Behavior-based ethical understanding in chinese social news},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). THIN: THrowable information networks and application for
facial expression recognition in the wild. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2336–2348. (<a
href="https://doi.org/10.1109/TAFFC.2022.3144439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For a number of machine learning problems, an exogenous variable can be identified such that it heavily influences the appearance of the different classes, and an ideal classifier should be invariant to this variable. An example of such exogenous variable is identity if facial expression recognition (FER) is considered. In this paper, we propose a dual exogenous/endogenous representation. The former captures the exogenous variable whereas the second one models the task at hand (e.g., facial expression). We design a prediction layer that uses a tree-gated deep ensemble conditioned by the exogenous representation. We also propose an exogenous dispelling loss to remove the exogenous information from the endogenous representation. Thus, the exogenous information is used two times in a throwable fashion, first as a conditioning variable for the target task, and second to create invariance within the endogenous representation. We call this method THIN, standing for THrowable Information Networks. We experimentally validate THIN in several contexts where an exogenous information can be identified, such as digit recognition under large rotations and shape recognition at multiple scales. We also apply it to FER with identity as the exogenous variable. We demonstrate that THIN significantly outperforms state-of-the-art approaches on several challenging datasets.},
  archive  = {J},
  author   = {Estèphe Arnaud and Arnaud Dapogny and Kévin Bailly},
  doi      = {10.1109/TAFFC.2022.3144439},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2336-2348},
  title    = {THIN: THrowable information networks and application for facial expression recognition in the wild},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compound aspect extraction by augmentation and constituency
lattice. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2323–2335. (<a
href="https://doi.org/10.1109/TAFFC.2022.3161683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspects are opinion targets to extract in aspect-based sentiment analysis. While existing methods can already produce satisfactory extraction results, they suffer when faced with compound aspect terms, typically phrase-level aspect terms that have inner structure and occur infrequently in the training set. This issue can be mainly attributed to the scarcity of training examples targeting compound aspect terms and by the neglect of the syntactic structure of a sentence in the modeling process. In this article, we aim to cope with compound aspect extraction by a two-stage hybrid approach. First, we introduce a conditional generation method for data augmentation in a masked sequence-to-sequence framework, which is controllable to preserve original aspects while generating a new sentence. Second, we propose a constituency lattice structure that is induced from the constituency-based parse tree of a sentence. Experimental results on two review datasets show that this approach can greatly improve the effect of compound aspect extraction.},
  archive  = {J},
  author   = {Xiaojun Quan and Zhengcheng Min and Kun Li and Yunyi Yang},
  doi      = {10.1109/TAFFC.2022.3161683},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2323-2335},
  title    = {Compound aspect extraction by augmentation and constituency lattice},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly-supervised learning for fine-grained emotion
recognition using physiological signals. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2304–2322. (<a
href="https://doi.org/10.1109/TAFFC.2022.3158234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Instead of predicting just one emotion for one activity (e.g., video watching), fine-grained emotion recognition enables more temporally precise recognition. Previous works on fine-grained emotion recognition require segment-by-segment, fine-grained emotion labels to train the recognition algorithm. However, experiments to collect these labels are costly and time-consuming compared with only collecting one emotion label after the user watched that stimulus (i.e., the post-stimuli emotion labels). To recognize emotions at a finer granularity level when trained with only post-stimuli labels, we propose an emotion recognition algorithm based on Deep Multiple Instance Learning ( EDMIL ) using physiological signals. EDMIL recognizes fine-grained valence and arousal (V-A) labels by identifying which instances represent the post-stimuli V-A annotated by users after watching the videos. Instead of fully-supervised training, the instances are weakly-supervised by the post-stimuli labels in the training stage. The V-A of instances are estimated by the instance gains, which indicate the probability of instances to predict the post-stimuli labels. We tested EDMIL on three different datasets, CASE , MERCA and CEAP-360VR , collected in three different environments: desktop, mobile and HMD-based Virtual Reality, respectively. Recognition results validated with the fine-grained V-A self-reports show that for subject-independent 3-class classification (high/neutral/low), EDMIL obtains promising recognition accuracies: 75.63% and 79.73% for V-A on CASE , 70.51% and 67.62% for V-A on MERCA and 65.04% and 67.05% for V-A on CEAP-360VR . Our ablation study shows that all components of EDMIL contribute to both the classification and regression tasks. Our experiments also show that (1) compared with fully-supervised learning, weakly-supervised learning can reduce the problem of overfitting caused by the temporal mismatch between fine-grained annotations and physiological signals, (2) instance segment lengths between 1-2 s result in the highest recognition accuracies and (3) EDMIL performs best if post-stimuli annotations consist of less than 30% or more than 60% of the entire video watching.},
  archive  = {J},
  author   = {Tianyi Zhang and Abdallah El Ali and Chen Wang and Alan Hanjalic and Pablo Cesar},
  doi      = {10.1109/TAFFC.2022.3158234},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2304-2322},
  title    = {Weakly-supervised learning for fine-grained emotion recognition using physiological signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modelling stochastic context of audio-visual expressive
behaviour with affective processes. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2290–2303. (<a
href="https://doi.org/10.1109/TAFFC.2022.3157141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognising apparent emotion from audio-visual signals in naturalistic conditions remains an open problem. Existing methods that build on recurrent models, or in the modelling of contextual dependencies at the feature level using self-attention fail to model the long-term dependencies that subtly occur at different levels of abstraction. Affective Processes have emerged as a novel paradigm to the modelling of temporal dynamics through a probabilistic global latent variable that captures context and induces dependencies in the outputs, showing superior performance with little complexity. Despite its impressive results on visual data, Affective Processes remain unexplored in the domain of audio data, known to crucially influence the perception of emotions. In this paper, we first revisit and extend Affective Processes to the speech domain, identifying the key components and learning procedures for their efficient training. We then extend Affective Processes to audio-visual affect recognition, using modality-specific context encoders. Finally, we propose a novel application of Affective Processes in the domain of Cooperative Machine Learning for propagating affect labels in videos using sparse human supervision. We conduct extensive ablation studies, identifying the main components behind the success of Affective Processes, as well as comparisons against existing works in a variety of datasets.},
  archive  = {J},
  author   = {Mani Kumar Tellamekala and Timo Giesbrecht and Michel Valstar},
  doi      = {10.1109/TAFFC.2022.3157141},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2290-2303},
  title    = {Modelling stochastic context of audio-visual expressive behaviour with affective processes},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid contrastive learning of tri-modal representation for
multimodal sentiment analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2276–2289. (<a
href="https://doi.org/10.1109/TAFFC.2022.3172360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The wide application of smart devices enables the availability of multimodal data, which can be utilized in many tasks. In the field of multimodal sentiment analysis, most previous works focus on exploring intra- and inter-modal interactions. However, training a network with cross-modal information (language, audio and visual) is still challenging due to the modality gap. Besides, while learning dynamics within each sample draws great attention, the learning of inter-sample and inter-class relationships is neglected. Moreover, the size of datasets limits the generalization ability of the models. To address the afore-mentioned issues, we propose a novel framework HyCon for hybrid contrastive learning of tri-modal representation. Specifically, we simultaneously perform intra-/inter-modal contrastive learning and semi-contrastive learning, with which the model can fully explore cross-modal interactions, learn inter-sample and inter-class relationships, and reduce the modality gap. Besides, refinement term and modality margin are introduced to enable a better learning of unimodal pairs. Moreover, we devise pair selection mechanism to identify and assign weights to the informative negative and positive pairs. HyCon can naturally generate many training pairs for better generalization and reduce the negative effect of limited datasets. Extensive experiments demonstrate that our method outperforms baselines on multimodal sentiment analysis and emotion recognition.},
  archive  = {J},
  author   = {Sijie Mai and Ying Zeng and Shuangjia Zheng and Haifeng Hu},
  doi      = {10.1109/TAFFC.2022.3172360},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2276-2289},
  title    = {Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic state-space modeling with factorial memories in
temporal dominance of sensations, emotions and temporal liking. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 2266–2275. (<a
href="https://doi.org/10.1109/TAFFC.2022.3149133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There are few mathematical models that describe how human perceptual and affective responses evolve with time. We developed a state-space model of the interrelationships based on time-evolving perceptual and affective responses acquired by the temporal dominance (TD) method. We defined and computed the state variables that endow the system with memory using canonical variate analysis. Furthermore, we determined the model parameters on the basis of a cross-validation approach such that the observed and estimated changes in the affective responses are highly correlated. We applied this method to the TD curves of responses to eating strawberries and plum pickles, reported in our previous work. The model describes what happens during food intake in terms of a few state variables that summarize a large number of observable responses reductively, thus helping to make the perceptual and affective dynamics understandable.},
  archive  = {J},
  author   = {Kyoichi Tachi and Shogo Okamoto},
  doi      = {10.1109/TAFFC.2022.3149133},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2266-2275},
  title    = {Dynamic state-space modeling with factorial memories in temporal dominance of sensations, emotions and temporal liking},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of depression severity based on the prosodic and
semantic features with bidirectional LSTM and time distributed CNN.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2251–2265. (<a
href="https://doi.org/10.1109/TAFFC.2022.3154332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is increasingly impacting individuals both physically and psychologically worldwide. It has become a global major public health problem and attracts attention from various research fields. Traditionally, the diagnosis of depression is formulated through semi-structured interviews and supplementary questionnaires, which makes the diagnosis heavily relying on physicians’ experience and is subject to bias. However, since the pathogenic mechanism of depression is still under investigation, it is difficult for physicians to diagnose and treat, especially in the early clinical stage. As smart devices and artificial intelligence advance rapidly, understanding how depression associates with daily behaviors can be beneficial for the early stage depression diagnosis, which reduces labor costs and the likelihood of clinical mistakes as well as physicians bias. Furthermore, mental health monitoring and cloud-based remote diagnosis can be implemented through an automated depression diagnosis system. In this article, we propose an attention-based multimodality speech and text representation for depression prediction. Our model is trained to estimate the depression severity of participants using the Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) dataset. For the audio modality, we use the collaborative voice analysis repository (COVAREP) features provided by the dataset and employ a Bidirectional Long Short-Term Memory Network (Bi-LSTM) followed by a Time-distributed Convolutional Neural Network (T-CNN). For the text modality, we use global vectors for word representation (GloVe) to perform word embeddings and the embeddings are fed into the Bi-LSTM network. Results show that both audio and text models perform well on the depression severity estimation task, with best sequence level $F_{1}$ score of 0.9870 and patient-level $F_{1}$ score of 0.9074 for the audio model over five classes (healthy, mild, moderate, moderately severe, and severe), as well as sequence level $F_{1}$ score of 0.9709 and patient-level $F_{1}$ score of 0.9245 for the text model over five classes. Results are similar for the multimodality fused model, with the highest $F_{1}$ score of 0.9580 on the patient-level depression detection task over five classes. Experiments show statistically significant improvements over previous works.},
  archive  = {J},
  author   = {Kaining Mao and Wei Zhang and Deborah Baofeng Wang and Ang Li and Rongqi Jiao and Yanhui Zhu and Bin Wu and Tiansheng Zheng and Lei Qian and Wei Lyu and Minjie Ye and Jie Chen},
  doi      = {10.1109/TAFFC.2022.3154332},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2251-2265},
  title    = {Prediction of depression severity based on the prosodic and semantic features with bidirectional LSTM and time distributed CNN},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TSception: Capturing temporal dynamics and spatial asymmetry
from EEG for emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2238–2250. (<a
href="https://doi.org/10.1109/TAFFC.2022.3169001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The high temporal resolution and the asymmetric spatial activations are essential attributes of electroencephalogram (EEG) underlying emotional processes in the brain. To learn the temporal dynamics and spatial asymmetry of EEG towards accurate and generalized emotion recognition, we propose TSception, a multi-scale convolutional neural network that can classify emotions from EEG. TSception consists of dynamic temporal, asymmetric spatial, and high-level fusion layers, which learn discriminative representations in the time and channel dimensions simultaneously. The dynamic temporal layer consists of multi-scale 1D convolutional kernels whose lengths are related to the sampling rate of EEG, which learns the dynamic temporal and frequency representations of EEG. The asymmetric spatial layer takes advantage of the asymmetric EEG patterns for emotion, learning the discriminative global and hemisphere representations. The learned spatial representations will be fused by a high-level fusion layer. Using more generalized cross-validation settings, the proposed method is evaluated on two publicly available datasets DEAP and MAHNOB-HCI. The performance of the proposed network is compared with prior reported methods such as SVM, KNN, FBFgMDM, FBTSC, Unsupervised learning, DeepConvNet, ShallowConvNet, and EEGNet. TSception achieves higher classification accuracies and F1 scores than other methods in most of the experiments. The codes are available at: https://github.com/yi-ding-cs/TSception},
  archive  = {J},
  author   = {Yi Ding and Neethu Robinson and Su Zhang and Qiuhao Zeng and Cuntai Guan},
  doi      = {10.1109/TAFFC.2022.3169001},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2238-2250},
  title    = {TSception: Capturing temporal dynamics and spatial asymmetry from EEG for emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stress detection during motor activity: Comparing
neurophysiological indices in older adults. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2224–2237. (<a
href="https://doi.org/10.1109/TAFFC.2022.3148234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The effects of cognitive stress are complex and multi-dimensional with nuanced neural and physiological representations across our lifespan. Chronic and instantaneous stressors are known to alter both executive function and motor performance — a particularly challenging prospect for older adults. Age, sex, and motor activity are critical yet under-represented dimensions in the domain of stress detection. Through the present work, we explore a subset of these variables and the relevance of brain hemodynamics and heart rate variability (HR/V) as biomarkers of stress in an aging population. We rely on a multimodal, sex-balanced, motor-stress data set (N = 59) and an exhaustive machine learning workflow to operationalize the unique neurophysiological states that form the human stress response. We found that a quadratic discriminant was sufficient to separate the two states across feature, demographic, and activity variables. We report a stress detection accuracy between $78-98\%$ when using models trained independently on each feature-set. However, these models were highly sensitive to sex, and activity differences — with distinct regions, and features implicated in stress recognition. Both HR/V and fNIRS based features were excellent indices of cognitive stress, however neither generalized to a degree beneficial toward operational use. Our observations underscore the importance of task-context, age, and sex as factors in modeling stress detection tools for older adults.},
  archive  = {J},
  author   = {Rohith Karthikeyan and Anthony D. McDonald and Ranjana K. Mehta},
  doi      = {10.1109/TAFFC.2022.3148234},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2224-2237},
  title    = {Stress detection during motor activity: Comparing neurophysiological indices in older adults},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to learn better unimodal representations via
adaptive multimodal meta-learning. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2209–2223. (<a
href="https://doi.org/10.1109/TAFFC.2022.3178231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal sentiment analysis is an emerging field of artificial intelligence. The most predominant approaches have made notable progress by designing sophisticated fusion architectures, exploring inter-modal interactions between modalities. However, these works tend to utilize a uniform optimization strategy for each modality, so that only sub-optimal unimodal representations are obtained for multimodal fusion. To address this issue, we propose a novel meta-learning based paradigm that can retain the advantages of unimodal existence and further boost the performance of multimodal fusion. Specifically, we introduce the Adaptive Multimodal Meta-Learning (AMML) to meta-learn the unimodal networks and adapt them for multimodal inference. AMML can (1) effectively obtain more optimized unimodal representation via meta-training on unimodal tasks, which adaptively adjusts the learning rate and assigns a more specific optimization procedure for each modality; (2) and adapt the optimized unimodal representations for multimodal fusion via meta-testing on multimodal tasks. Considering multimodal fusion often suffers from the distributional mismatches between features of different modalities due to heterogeneous nature of the signals, we implement a distribution transformation layer on unimodal representations to regularize the unimodal distributions. In this way, distribution gaps can be reduced to achieve a better effect of fusion. Extensive experiments on two widely-used datasets demonstrate that AMML achieves state-of-the-art performance.},
  archive  = {J},
  author   = {Ya Sun and Sijie Mai and Haifeng Hu},
  doi      = {10.1109/TAFFC.2022.3178231},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2209-2223},
  title    = {Learning to learn better unimodal representations via adaptive multimodal meta-learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio-visual gated-sequenced neural networks for affect
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2193–2208. (<a
href="https://doi.org/10.1109/TAFFC.2022.3156026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The interest in automatic emotion recognition and the larger field of Affective Computing has recently gained momentum. The current emergence of large, video-based affect datasets offering rich multi-modal inputs facilitates the development of deep learning-based models for automatic affect analysis that currently holds the state of the art. However, recent approaches to process these modalities cannot fully exploit them due to the use of oversimplified fusion schemes. Furthermore, the efficient use of temporal information inherent to these huge data are also largely unexplored hindering their potential progress. In this work, we propose a multi-modal, sequence-based neural network with gating mechanisms for Valence and Arousal based affect recognition. Our model consists of three major networks: Firstly, a latent-feature generator that extracts compact representations from both modalities that have been artificially degraded to add robustness. Secondly, a multi-task discriminator that estimates both input identity and a first step emotion quadrant estimation. Thirdly, a sequence-based predictor with attention and gating mechanisms that effectively merges both modalities and uses this information through sequence modelling. In our experiments on the SEMAINE and SEWA affect datasets, we observe the impact of both proposed methods with progressive increase in accuracy. We further show in our ablation studies how the internal attention weight and gating coefficient impact our models’ estimates quality. Finally, we demonstrate state of the art accuracy through comparisons with current alternatives on both datasets.},
  archive  = {J},
  author   = {Decky Aspandi and Federico Sukno and Björn W. Schuller and Xavier Binefa},
  doi      = {10.1109/TAFFC.2022.3156026},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2193-2208},
  title    = {Audio-visual gated-sequenced neural networks for affect recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SocialInteractionGAN: Multi-person interaction sequence
generation. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2182–2192. (<a
href="https://doi.org/10.1109/TAFFC.2022.3171719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Prediction of human actions in social interactions has important applications in the design of social robots or artificial avatars. In this paper, we focus on a unimodal representation of interactions and propose to tackle interaction generation in a data-driven fashion. In particular, we model human interaction generation as a discrete multi-sequence generation problem and present SocialInteractionGAN, a novel adversarial architecture for conditional interaction generation. Our model builds on a recurrent encoder-decoder generator network and a dual-stream discriminator, that jointly evaluates the realism of interactions and individual action sequences and operates at different time scales. Crucially, contextual information on interacting participants is shared among agents and reinjected in both the generation and the discriminator evaluation processes. Experiments show that albeit dealing with low dimensional data, SocialInteractionGAN succeeds in producing high realism action sequences of interacting people, comparing favorably to a diversity of recurrent and convolutional discriminator baselines, and we argue that this work will constitute a first stone towards higher dimensional and multimodal interaction generation. Evaluations are conducted using classical GAN metrics, that we specifically adapt for discrete sequential data. Our model is shown to properly learn the dynamics of interaction sequences, while exploiting the full range of available actions.},
  archive  = {J},
  author   = {Louis Airale and Dominique Vaufreydaz and Xavier Alameda-Pineda},
  doi      = {10.1109/TAFFC.2022.3171719},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2182-2192},
  title    = {SocialInteractionGAN: Multi-person interaction sequence generation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual and joint body movement assessed by wearable
sensing as a predictor of attraction in speed dates. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 2168–2181. (<a
href="https://doi.org/10.1109/TAFFC.2021.3138349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interpersonal attraction is known to motivate behavioral responses in the person experiencing this subjective phenomenon. Such responses may involve the imitation of behavior, as in mirroring or mimicry of postures or gestures, which have been found to be associated with the desire to be liked by an interlocutor. Speed dating provides a unique opportunity for the study of such behavioral manifestations of interpersonal attraction through the elimination of barriers to initiating communication, while maintaining significant ecological validity. In this paper we investigate the relationship between body movement, measured via accelerometer sensors, and self-reports or ratings of attraction and affiliation in a dataset of 399 speed dates between 72 subjects. Through machine learning experiments, we found that both features derived from a single individual&#39;s body movement and features designed to measure aspects of synchrony and convergence of the couple&#39;s body movement signals were predictive of different attraction ratings. Our statistical analysis revealed that the overall increase or decrease in an individual&#39;s body movement throughout an interaction is a potential indicator of friendly intentions, possibly related to the desire to affiliate.},
  archive  = {J},
  author   = {Jose Vargas-Quiros and Öykü Kapcak and Hayley Hung and Laura Cabrera-Quiros},
  doi      = {10.1109/TAFFC.2021.3138349},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2168-2181},
  title    = {Individual and joint body movement assessed by wearable sensing as a predictor of attraction in speed dates},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep multi-modal network based automated depression severity
estimation. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 2153–2167. (<a
href="https://doi.org/10.1109/TAFFC.2022.3179478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a severe mental illness that impairs a person&#39;s capacity to function normally in personal and professional life. The assessment of depression usually requires a comprehensive examination by an expert professional. Recently, machine learning-based automatic depression assessment has received considerable attention for a reliable and efficient depression diagnosis. Various techniques for automated depression detection were developed; however, certain concerns still need to be investigated. In this work, we propose a novel deep multi-modal framework that effectively utilizes facial and verbal cues for an automated depression assessment. Specifically, we first partition the audio and video data into fixed-length segments. Then, these segments are fed into the Spatio-Temporal Networks as input, which captures both spatial and temporal features as well as assigns higher weights to the features that contribute most. In addition, Volume Local Directional Structural Pattern (VLDSP) based dynamic feature descriptor is introduced to extract the facial dynamics by encoding the structural aspects. Afterwards, we employ the Temporal Attentive Pooling (TAP) approach to summarize the segment-level features for audio and video data. Finally, the multi-modal factorized bilinear pooling (MFB) strategy is applied to fuse the multi-modal features effectively. An extensive experimental study reveals that the proposed method outperforms state-of-the-art approaches.},
  archive  = {J},
  author   = {Md Azher Uddin and Joolekha Bibi Joolee and Kyung-Ah Sohn},
  doi      = {10.1109/TAFFC.2022.3179478},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2153-2167},
  title    = {Deep multi-modal network based automated depression severity estimation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing interaction quality in long duration (simulated)
space missions with wearables. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2139–2152. (<a
href="https://doi.org/10.1109/TAFFC.2022.3176967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Space exploration is evolving with the recent increase in interest and investment. For the success of planned long-duration crewed missions, good interpersonal interactions between crew members are crucial. In this study, we evaluate the use of wearables for detection and estimation of the quality of each social interaction participants have throughout a long mission rather than aggregate measures of interactions. Our proposed method utilizes Temporal Convolutional Networks(TCNs) for extracting individual representations from acceleration and audio streams and learnable pooling layers(NetVLAD) to aggregate these representations into fixed-size representations. Use of NetVLAD layers provides an intelligent alternative to simple aggregation for handling variable-sized interactions and interactions with missing data. We evaluate our method on a 4-month simulated space mission where 5 participants wore Sociometric Badges and provided reports on their interactions in terms of effectiveness, frustration, and satisfaction. Our method provides an average ROC-AUC score of 0.64. Since we are not aware of any comparable baselines, we compare our method to hand-crafted features formerly utilized for cohesion estimation in similar scenarios and show it significantly outperforms them. We also present ablation studies where we replace the components in our approach with well-known alternatives and show that they provide better performance than their respective counterparts.},
  archive  = {J},
  author   = {Ekin Gedik and Jeffrey Olenick and Chu-Hsiang Chang and Steve W.J. Kozlowski and Hayley Hung},
  doi      = {10.1109/TAFFC.2022.3176967},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2139-2152},
  title    = {Capturing interaction quality in long duration (Simulated) space missions with wearables},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). I enjoy writing and playing, do you?: A personalized and
emotion grounded dialogue agent using generative adversarial network.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2127–2138. (<a
href="https://doi.org/10.1109/TAFFC.2022.3155105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social chatbots have gained immense popularity, and their appeal lies in their capacity to respond to diverse requests, but also in their ability to develop an emotional connection with users. To develop and promote social chatbots, we need to concentrate on increasing user interaction and consider both the intellectual and emotional quotient in conversational agents. In this work, we propose the task of empathetic, personalized dialogue generation giving the machine the capability to respond emotionally and in accordance with the persona of the user. We design a generative adversarial framework, named EP-GAN (Empathy and Persona aware Generative Adversarial Network) to generate responses that are sensitive to the emotion of the user and corresponds to the persona. The persona information is encoded as memory vectors that, along with the dialogue history, are fed to the decoder for generation. An interactive adversarial learning framework is implemented to verify whether the generated responses elicit the emotion and persona in dialogues. Experimental results show that the EP-GAN framework significantly outperforms the existing baselines for both automatic and manual evaluation. We achieve an improvement of 1 point BLEU-4 and 2 points in the emotion accuracy metric compared to the best performing baseline.},
  archive  = {J},
  author   = {Mauajama Firdaus and Naveen Thangavelu and Asif Ekbal and Pushpak Bhattacharyya},
  doi      = {10.1109/TAFFC.2022.3155105},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2127-2138},
  title    = {I enjoy writing and playing, do you?: A personalized and emotion grounded dialogue agent using generative adversarial network},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Altered brain dynamics and their ability for major
depression detection using EEG microstates analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 2116–2126. (<a
href="https://doi.org/10.1109/TAFFC.2021.3139104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Major depressive disorder (MDD) may be driven by dysfunction in intrinsic dynamic properties of the brain, and EEG microstate is a promising method for analyzing brain dynamics. However, the alterations in EEG microstate is still not entirely clear, and its ability for MDDs detection is worth probing. Moreover, the mechanism behind the neural networks contributing to microstates remains poorly understood in MDDs. Therefore, we applied microstate analysis and Topographic Electrophysiological State Source-imaging (TESS) on EEG data of 27 MDDs and 28 healthy controls (HCs). Compared to HCs, MDDs had apparent increase in microstate C and decrease in microstate D. Furthermore, TESS results showed that the underlying network of microstate C in MDDs overlapped with the anterior cingulate cortex and left insula gyrus, whereas main source of microstate D was in the orbital part of inferior frontal gyrus. The reduced transition probability from C to D in MDDs may reveal an imbalance between the networks of microstates. The microstate parameters as features reached good performance in identifying MDD (89.09% accuracy, 92.86% sensitivity, 85.19% specificity), indicating their potential as biomarkers of depression pathology. Collectively, these results highlight alteration of brain activity patterns and provide new insights into abnormal EEG dynamics in MDDs.},
  archive  = {J},
  author   = {Jianxiu Li and Nan Li and Xuexiao Shao and Junhao Chen and Yanrong Hao and Xiaowei Li and Bin Hu},
  doi      = {10.1109/TAFFC.2021.3139104},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2116-2126},
  title    = {Altered brain dynamics and their ability for major depression detection using EEG microstates analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual information based fusion model (MIBFM): Mild
depression recognition using EEG and pupil area signals. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 2102–2115. (<a
href="https://doi.org/10.1109/TAFFC.2022.3171782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The detection of mild depression is conducive to the early intervention and treatment of depression. This study explored the fusion of electroencephalography (EEG) and pupil area signals to build an effective and convenient mild depression recognition model. We proposed Mutual Information Based Fusion Model (MIBFM), which innovatively used pupil area signals to select EEG electrodes based on mutual information. Then we extracted features from EEG and pupil area signals in different bands, and fused bimodal features using the denoising autoencoder. Experimental results showed that MIBFM could obtain the highest accuracy of 87.03%. And MIBFM exhibited better performance than other existing methods. Our findings validate the effectiveness of the use of pupil area as signals, which makes eye movement signals can be easily obtained using high resolution camera, and the EEG electrode selection scheme based on mutual information is also proved to be an applicable solution for data dimension reduction and multimodal complementary information screening. This study casts a new light for mild depression recognition using multimodal data of EEG and pupil area signals, and provides a theoretical basis for the development of portable and universal application systems.},
  archive  = {J},
  author   = {Jing Zhu and Changlin Yang and Xiannian Xie and Shiqing Wei and Yizhou Li and Xiaowei Li and Bin Hu},
  doi      = {10.1109/TAFFC.2022.3171782},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2102-2115},
  title    = {Mutual information based fusion model (MIBFM): Mild depression recognition using EEG and pupil area signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel markovian framework for integrating absolute and
relative ordinal emotion information. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2089–2101. (<a
href="https://doi.org/10.1109/TAFFC.2022.3159782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There is growing interest in affective computing for the representation and prediction of emotions along ordinal scales. However, the term ordinal emotion label has been used to refer to both absolute notions such as low or high arousal, as well as relation notions such as arousal is higher at one instance compared to another. In this paper, we introduce the terminology absolute and relative ordinal labels to make this distinction clear and investigate both with a view to integrate them and exploit their complementary nature. We propose a Markovian framework referred to as Dynamic Ordinal Markov Model (DOMM) that makes use of both absolute and relative ordinal information, to improve speech based ordinal emotion prediction. Finally, the proposed framework is validated on two speech corpora commonly used in affective computing, the RECOLA and the IEMOCAP databases, across a range of system configurations. The results consistently indicate that integrating relative ordinal information improves absolute ordinal emotion prediction.},
  archive  = {J},
  author   = {Jingyao Wu and Ting Dang and Vidhyasaharan Sethu and Eliathamby Ambikairajah},
  doi      = {10.1109/TAFFC.2022.3159782},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2089-2101},
  title    = {A novel markovian framework for integrating absolute and relative ordinal emotion information},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personal-zscore: Eliminating individual difference for
EEG-based cross-subject emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2077–2088. (<a
href="https://doi.org/10.1109/TAFFC.2021.3137857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It was observed that accuracy of the Subject-Dependent emotion recognition model was much higher than that of the Subject-Independent modelÂ in the field of electroencephalogram (EEG) based affective computing. This phenomenon is mainly caused by the individual difference of EEG, which is the key issue to be solved for the application of emotion recognition. In this work, 14 subjects from the SEED were selected for individual difference analysis. Through individual aggregation features evaluation, sample space visualization, and correlation analysis, we proposed four quantification indicators to analyze individual difference phenomenon. Finally, we presented the Personal-Zscore (PZ) feature processing method, and it was found that the data set processed with PZ method could represent emotion better than the original data set, and the conventional model with the PZ method was more robust. The accuracies of emotion recognition models trained with PZ processing have been improved to some extent, which showed that the PZ method could effectively eliminate the individual aggregation of feature space and improve the emotional representation ability of data sets. Hence, our findings may provide a new insight into the foundation for universal implementation of EEG-based application, and the Personal-Zscore feature processing method is of great significance for the development of effective emotion recognition system.},
  archive  = {J},
  author   = {Huayu Chen and Shuting Sun and Jianxiu Li and Ruilan Yu and Nan Li and Xiaowei Li and Bin Hu},
  doi      = {10.1109/TAFFC.2021.3137857},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2077-2088},
  title    = {Personal-zscore: Eliminating individual difference for EEG-based cross-subject emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial image-based automatic assessment of equine pain.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
2064–2076. (<a
href="https://doi.org/10.1109/TAFFC.2022.3177639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognition of pain in animals is essential for their welfare. However, since there is no verbal communication, this assessment depends solely on the ability of the observer to locate visible or audible signs of pain. The use of grimace scales is proven to be efficient in detecting the pain visually, but the assessment quality depends on the level of training of the assessor and the validity is not easily ensured. There is a clear need for automating the pain assessment process. This work provides a system for pain prediction in horses, based on grimace scales. The pipeline automatically determines the quantitative pose of the equine head and finds facial landmarks before classification, proposing a novel scale-normalisation approach for equine heads. The pain estimation is achieved for each facial region of interest separately, following the clinical pain estimation procedure. We introduce a database of horse images, annotated by professional veterinarians for training and assessment. We also propose a data augmentation method to alleviate the data scarcity issues, which relies on generating realistic 3D equine face models based on 2D annotated images. We show that the data augmentation method improves the performance of both quantitative pose estimation and landmark detection. Our results establish a strong baseline for automatic equine pain estimation.},
  archive  = {J},
  author   = {Francisca Pessanha and Albert Ali Salah and Thijs van Loon and Remco Veltkamp},
  doi      = {10.1109/TAFFC.2022.3177639},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2064-2076},
  title    = {Facial image-based automatic assessment of equine pain},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GANSER: A self-supervised data augmentation framework for
EEG-based emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 2048–2063. (<a
href="https://doi.org/10.1109/TAFFC.2022.3170369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG)-based affective computing has a scarcity problem. As a result, it is difficult to build effective, highly accurate and stable models using machine learning algorithms, especially deep learning models. Data augmentation has recently shown performance improvements in deep learning models with increased accuracy, stability and reduced overfitting. In this paper, we propose a novel data augmentation framework, named the generative adversarial network-based self-supervised data augmentation (GANSER). As the first to combine adversarial training with self-supervised learning for EEG-based emotion recognition, the proposed framework generates high-quality and high-diversity simulated EEG samples. In particular, we utilize adversarial training to learn an EEG generator and force the generated EEG signals to approximate the distribution of real samples, ensuring the quality of the augmented samples. A transformation operation is employed to mask parts of the EEG signals and force the generator to synthesize potential EEG signals based on the unmasked parts to produce a wide variety of samples. A masking possibility during transformation is introduced as prior knowledge to generalize the classifier for the augmented sample space. Finally, numerous experiments demonstrate that our proposed method can improve emotion recognition with an increase in performance and achieve state-of-the-art results.},
  archive  = {J},
  author   = {Zhi Zhang and Yan Liu and Sheng-hua Zhong},
  doi      = {10.1109/TAFFC.2022.3170369},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2048-2063},
  title    = {GANSER: A self-supervised data augmentation framework for EEG-based emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region attentive action unit intensity estimation with
uncertainty weighted multi-task learning. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2033–2047. (<a
href="https://doi.org/10.1109/TAFFC.2021.3139101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial action units (AUs) refer to a comprehensive set of atomic facial muscle movements. Recent works have focused on exploring complementary information by learning the relationships among AUs. Most existing approaches process AU co-occurrence and enhance AU recognition by learning the dependencies among AUs from labels, however, the complementary information among features of different AUs are ignored. Moreover, ground truth annotations suffer from a large intra-class variance and their associated intensity levels may vary depending on the annotators’ experience. In this paper, we propose the Region Attentive AU intensity estimation method with Uncertainty Weighted Multi-task Learning (RA-UWML). A RoI-Net is first used to extract features from the pre-defined facial patches where the AUs locate. Then, we use the co-occurrence of AUs using both within patch and between patches representation learning. Within a given patch, we propose sharing representation learning in a multi-task manner. To achieve complementarity and avoid redundancy between different image patches, we propose to use a multi-head self-attention mechanism to adaptively and attentively encode each patch specific representation. Moreover, the AU intensity is represented as a Gaussian distribution, instead of a single value, where the mean value indicates the most likely AU intensity and the variance indicates the uncertainty of the estimated AU intensity. The estimated variances are leveraged to automatically weight the loss of each AU in the multitask learning model. In extensive experiments on the Disfa, Fera2015 and Feafa benchmarks, it is shown that the proposed AU intensity estimation model achieves better results compared to the state-of-the-art models.},
  archive  = {J},
  author   = {Haifeng Chen and Dongmei Jiang and Yong Zhao and Xiaoyong Wei and Ke Lu and Hichem Sahli},
  doi      = {10.1109/TAFFC.2021.3139101},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2033-2047},
  title    = {Region attentive action unit intensity estimation with uncertainty weighted multi-task learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial emotional intelligence in socially assistive
robots for older adults: A pilot study. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 2020–2032. (<a
href="https://doi.org/10.1109/TAFFC.2022.3143803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents our recent research on integrating artificial emotional intelligence in a social robot (Ryan) and studies the robot&#39;s effectiveness in engaging older adults. Ryan is a socially assistive robot designed to provide companionship for older adults with depression and dementia through conversation. We used two versions of Ryan for our study, empathic and non-empathic. The empathic Ryan utilizes a multimodal emotion recognition algorithm and a multimodal emotion expression system. Using different input modalities for emotion, i.e., facial expression and speech sentiment, the empathic Ryan detects users’ emotional state and utilizes an affective dialogue manager to generate a response. On the other hand, the non-empathic Ryan lacks facial expression and uses scripted dialogues that do not factor in the users’ emotional state. We studied these two versions of Ryan with 10 older adults living in a senior care facility. The statistically significant improvement in the users’ reported face-scale mood measurement indicates an overall positive effect from the interaction with both the empathic and non-empathic versions of Ryan. However, the number of spoken words measurement and the exit survey analysis suggest that the users perceive the empathic Ryan as more engaging and likable.},
  archive  = {J},
  author   = {Hojjat Abdollahi and Mohammad H. Mahoor and Rohola Zandie and Jarid Siewierski and Sara H. Qualls},
  doi      = {10.1109/TAFFC.2022.3143803},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2020-2032},
  title    = {Artificial emotional intelligence in socially assistive robots for older adults: A pilot study},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using subgroup discovery to relate odor pleasantness and
intensity to peripheral nervous system reactions. <em>IEEE Transactions
on Affective Computing</em>, <em>14</em>(3), 2005–2019. (<a
href="https://doi.org/10.1109/TAFFC.2022.3173403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Activation of the autonomic nervous system is a primary characteristic of human hedonic responses to sensory stimuli. For smells, general tendencies of physiological reactions have been described using classical statistics. However, these physiological variations are generally not quantified precisely; each psychophysiological parameter has very often been studied separately and individual variability was not systematically considered. The current study presents an innovative approach based on data mining, whose goal is to extract knowledge from a dataset. This approach uses a subgroup discovery algorithm which allows extraction of rules that apply to as many olfactory stimuli and individuals as possible. These rules are described by intervals on a set of physiological attributes. Results allowed both quantifying how each physiological parameter relates to odor pleasantness and perceived intensity but also describing the participation of each individual to these rules. This approach can be applied to other fields of affective sciences characterized by complex and heterogeneous datasets.},
  archive  = {J},
  author   = {Maëlle Moranges and Marc Plantevit and Moustafa Bensafi},
  doi      = {10.1109/TAFFC.2022.3173403},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {2005-2019},
  title    = {Using subgroup discovery to relate odor pleasantness and intensity to peripheral nervous system reactions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised cross-corpus speech emotion recognition using a
multi-source cycle-GAN. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 1991–2004. (<a
href="https://doi.org/10.1109/TAFFC.2022.3146325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER) plays a crucial role in understanding user feelings when developing artificial intelligence services. However, the data mismatch and label distortion between the training (source) set and the testing (target) set significantly degrade the performances when developing the SER systems. Additionally, most emotion-related speech datasets are highly contextualized and limited in size. The manual annotation cost is often too high leading to an active investigation of unsupervised cross-corpus SER techniques. In this paper, we propose a framework in unsupervised cross-corpus emotion recognition using multi-source corpus in a data augmentation manner. We introduced Corpus-Aware Emotional CycleGAN (CAEmoCyGAN) including a corpus-aware attention mechanism to aggregate each source datasets to generate the synthetic target sample. We choose the widely used speech emotion corpora the IEMOCAP and the VAM as sources and the MSP-Podcast as the target. By generating synthetic target-aware samples to augment source datasets and by directly training on this augmented dataset, our proposed multi-source target-aware augmentation method outperforms other baseline models in activation and valence classification.},
  archive  = {J},
  author   = {Bo-Hao Su and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2022.3146325},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1991-2004},
  title    = {Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-GAN},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactual representation augmentation for cross-domain
sentiment analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 1979–1990. (<a
href="https://doi.org/10.1109/TAFFC.2022.3158843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-domain sentiment analysis aims to adapt sentiment classification models trained on one or more source domains to the target domain, which can effectively alleviate the problem of insufficient labelled data in specific domains. Unlike most previous methods of adjusting models based on observed data, we propose a novel counterfactual representation augmentation (CRA) method, which aims to improve target-domain generalization by constructing new counterfactual representations for training. Specifically, we train a domain discriminator to learn the domain discrepancy between the source and target domains on unlabeled data, and use a gradient editing method to directly construct counterfactual representations, which reduces the inductive bias of the source domain and augments the training data. Moreover, we further leverage an ensemble-based training method to indirectly encourage the target-domain classifier to rely more on robust features for prediction. Extensive experiments on a widely-used cross-domain sentiment classification benchmark dataset show that our method consistently surpasses different baseline methods on different tasks, demonstrating the strong ability to improve domain generalization. We also find that our model can effectively adjust the decision boundary to make the classifier more robust and generalized, through extensive qualitative and quantitative analysis.},
  archive  = {J},
  author   = {Ke Wang and Xiaojun Wan},
  doi      = {10.1109/TAFFC.2022.3158843},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1979-1990},
  title    = {Counterfactual representation augmentation for cross-domain sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical interactive multimodal transformer for
aspect-based multimodal sentiment analysis. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 1966–1978. (<a
href="https://doi.org/10.1109/TAFFC.2022.3171091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based multimodal sentiment analysis (ABMSA) aims to determine the sentiment polarities of each aspect or entity mentioned in a multimodal post or review. Previous studies to ABMSA can be summarized into two subtasks: aspect-term based multimodal sentiment classification (ATMSC) and aspect-category based multimodal sentiment classification (ACMSC). However, these existing studies have three shortcomings: (1) ignoring the object-level semantics in images; (2) primarily focusing on aspect-text and aspect-image interactions; (3) failing to consider the semantic gap between text and image representations. To tackle these issues, we propose a general Hierarchical Interactive Multimodal Transformer (HIMT) model for ABMSA. Specifically, we extract salient features with semantic concepts from images via an object detection method, and then propose a hierarchical interaction module to first model the aspect-text and aspect-image interactions, followed by capturing the text-image interactions. Moreover, an auxiliary reconstruction module is devised to largely eliminate the semantic gap between text and image representations. Experimental results show that our HIMT model significantly outperforms state-of-the-art methods on two benchmarks for ATMSC and one benchmark for ACMSC.},
  archive  = {J},
  author   = {Jianfei Yu and Kai Chen and Rui Xia},
  doi      = {10.1109/TAFFC.2022.3171091},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1966-1978},
  title    = {Hierarchical interactive multimodal transformer for aspect-based multimodal sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual attention and element recalibration networks for
automatic depression level prediction. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 1954–1965. (<a
href="https://doi.org/10.1109/TAFFC.2022.3177737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Physiological studies have identified that facial dynamics can be considered as biomarkers to analyze depression severity. This paper accordingly develops a Dual Attention and Element Recalibration (DAER) network to extract facial changes to predict the depression level. In this model, we propose two blocks: a Dual Attention (DA) block and Element Recalibration (ER) block. The DA block uses the self-attention to investigate the dynamic changes in the representation sequence of a facial video segment. It further examines the influence of feature components of the representation sequence on depression level prediction through bilinear-attention. Moreover, to improve the representation ability of network, the ER block is used to obtain the global information to recalibrate each element of the tensor. Adopting this approach, for the depression level prediction task, we first divide the long-term video into fixed-length segments and use the trained ResNet50 to encode each frame to generate the representation sequences of video segments. Second, the representation sequences are input into DAER network to obtain the depression level scores. Finally, the average of these scores yields the prediction result corresponding to the long-term video. Experiments on publicly available AVEC 2013 and AVEC 2014 depression databases illustrate the effectiveness of our method.},
  archive  = {J},
  author   = {Mingyue Niu and Ziping Zhao and Jianhua Tao and Ya Li and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2022.3177737},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1954-1965},
  title    = {Dual attention and element recalibration networks for automatic depression level prediction},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection and identification of choking under pressure in
college tennis based upon physiological parameters, performance
patterns, and game statistics. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 1942–1953. (<a
href="https://doi.org/10.1109/TAFFC.2022.3165139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Choking under pressure, or underperformance in pressure situations, is a phenomenon feared by many athletes. Psychology research is heavily invested in studying the phenomenon; however, a universal structural approach to objectively analyze choking is yet to be established. The purpose of this work was to study relationships between choking in college tennis and selected physiological, performance, and game statistic variables in order to design a choking detection model. Choking was identified as a newly introduced variable called the choking moment. The introduced variables were measured with wearable sensors and observed from video recordings of college tennis matches. Swing speed and game statistic variables showed strong relationships to choking, and thus appear practical for the proposed choking detection model. Heartrate and footwork performance variables displayed weaker, but still some, patterns potentially related to choking, and thus could serve as supporting parameters for improvements of the choking detection mechanism. This study suggested unique techniques for identification and detection of choking in college tennis by introducing tennis specific variables, data collection framework, and data analysis procedures. The impact of this study is a contribution to the existing choking research by proposing an objective approach to analyze the subjective phenomenon of choking under pressure.},
  archive  = {J},
  author   = {Stepan Vancurik and Dale W. Callahan},
  doi      = {10.1109/TAFFC.2022.3165139},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1942-1953},
  title    = {Detection and identification of choking under pressure in college tennis based upon physiological parameters, performance patterns, and game statistics},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic attribute tree structured convolutional neural
networks for facial expression recognition in the wild. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1927–1941. (<a
href="https://doi.org/10.1109/TAFFC.2022.3156920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Very recent work has demonstrated tremendous improvements in facial expression recognition (FER) on laboratory-controlled datasets. However, recognizing facial expressions under in-the-wild conditions still remains challenging, especially on unseen subjects due to high inter-subject variations. In this paper, we propose a novel Probabilistic Attribute Tree Convolutional Neural Network (PAT-CNN) to explicitly deal with large intra-class variations caused by identity-related attributes, e.g., age, race, and gender. Specifically, a PAT module with an associated PAT loss is proposed to learn features in a hierarchical tree structure organized according to identity-related attributes, where the final features are less affected by the attributes. Then, expression-related features are extracted from leaf nodes. Samples are probabilistically assigned to tree nodes at different levels such that expression-related features can be learned from all samples weighted by probabilities. Furthermore, the proposed PAT-CNN can be learned from limited attribute-annotated samples to make the best use of available data. Experimental results on four spontaneous facial expression datasets, i.e., RAF-DB, SFEW, ExpW, and FER-2013, have demonstrated that the proposed PAT-CNN achieves the best performance when compared to state-of-the-art methods by explicitly modeling attributes. Impressively, a single model PAT-CNN achieves the best performance on the SFEW test dataset when compared to the state-of-the-art methods using an ensemble of hundreds of CNNs.},
  archive  = {J},
  author   = {Jie Cai and Zibo Meng and Ahmed Shehab Khan and Zhiyuan Li and James O’Reilly and Yan Tong},
  doi      = {10.1109/TAFFC.2022.3156920},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1927-1941},
  title    = {Probabilistic attribute tree structured convolutional neural networks for facial expression recognition in the wild},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Self supervised adversarial domain adaptation for
cross-corpus and cross-language speech emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1912–1926. (<a
href="https://doi.org/10.1109/TAFFC.2022.3167013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite the recent advancement in speech emotion recognition (SER) within a single corpus setting, the performance of these SER systems degrades significantly for cross-corpus and cross-language scenarios. The key reason is the lack of generalisation in SER systems towards unseen conditions, which causes them to perform poorly in cross-corpus and cross-language settings. Recent studies focus on utilising adversarial methods to learn domain generalised representation for improving cross-corpus and cross-language SER to address this issue. However, many of these methods only focus on cross-corpus SER without addressing the cross-language SER performance degradation due to a larger domain gap between source and target language data. This contribution proposes an adversarial dual discriminator (ADDi) network that uses the three-players adversarial game to learn generalised representations without requiring any target data labels. We also introduce a self-supervised ADDi (sADDi) network that utilises self-supervised pre-training with unlabelled data. We propose synthetic data generation as a pretext task in sADDi, enabling the network to produce emotionally discriminative and domain invariant representations and providing complementary synthetic data to augment the system. The proposed model is rigorously evaluated using five publicly available datasets in three languages and compared with multiple studies on cross-corpus and cross-language SER. Experimental results demonstrate that the proposed model achieves improved performance compared to the state-of-the-art methods.},
  archive  = {J},
  author   = {Siddique Latif and Rajib Rana and Sara Khalifa and Raja Jurdak and Björn Schuller},
  doi      = {10.1109/TAFFC.2022.3167013},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1912-1926},
  title    = {Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-sensitive boosting pruning trees for depression
detection on twitter. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 1898–1911. (<a
href="https://doi.org/10.1109/TAFFC.2022.3145634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is one of the most common mental health disorders, and a large number of depressed people commit suicide each year. Potential depression sufferers usually do not consult psychological doctors because they feel ashamed or are unaware of any depression, which may result in severe delay of diagnosis and treatment. In the meantime, evidence shows that social media data provides valuable clues about physical and mental health conditions. In this paper, we argue that it is feasible to identify depression at an early stage by mining online social behaviours. Our approach, which is innovative to the practice of depression detection, does not rely on the extraction of numerous or complicated features to achieve accurate depression detection. Instead, we propose a novel classifier, namely, Cost-sensitive Boosting Pruning Trees (CBPT), which demonstrates a strong classification ability on two publicly accessible Twitter depression detection datasets. To comprehensively evaluate the classification capability of CBPT, we use additional three datasets from the UCI machine learning repository and CBPT obtains appealing classification results against several state of the arts boosting algorithms. Finally, we comprehensively explore the influence factors for the model prediction, and the results manifest that our proposed framework is promising for identifying Twitter users with depression.},
  archive  = {J},
  author   = {Lei Tong and Zhihua Liu and Zheheng Jiang and Feixiang Zhou and Long Chen and Jialin Lyu and Xiangrong Zhang and Qianni Zhang and Abdul Sadka and Yinhai Wang and Ling Li and Huiyu Zhou},
  doi      = {10.1109/TAFFC.2022.3145634},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1898-1911},
  title    = {Cost-sensitive boosting pruning trees for depression detection on twitter},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion recognition for everyday life using physiological
signals from wearables: A systematic literature review. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1876–1897. (<a
href="https://doi.org/10.1109/TAFFC.2022.3176135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Smart wearables, equipped with sensors monitoring physiological parameters, are becoming an integral part of our life. In this work, we investigate the possibility of utilizing such wearables to recognize emotions in the wild. In most reviewed papers, the authors apply a similar procedure consisting of participant recruitment, stimuli preparation and annotation, signal collection and processing, self-assessment, and machine learning model learning and validation. Besides, we identified seven emotion recognition scenarios and analyzed the transition from psychological models to machine learning tasks. Even though the majority of the research was performed in the laboratory environment, we conclude that studies in the field are feasible. They require especially: (1) new self-assessment and triggering procedures adjusted to a real-life scenario, (2) more attention to the machine learning process, including suitable deep learning architectures, revision of the data imbalance problem, and subject-specific data processing, (3) adequate validation procedures, (4) consideration of the model generalizability versus personalizability, (5) comfortable devices able to provide reliable measurements in motion. Additionally, more large-scale studies are necessary to increase result credibility. We also postulate actions toward replicability and comparability of the research.},
  archive  = {J},
  author   = {Stanisław Saganowski and Bartosz Perz and Adam G. Polak and Przemysław Kazienko},
  doi      = {10.1109/TAFFC.2022.3176135},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1876-1897},
  title    = {Emotion recognition for everyday life using physiological signals from wearables: A systematic literature review},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of facial micro-expression analysis: Data,
methodology and challenge. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 1857–1875. (<a
href="https://doi.org/10.1109/TAFFC.2022.3143100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial micro-expressions indicate brief and subtle facial movements that appear during emotional communication. In comparison to macro-expressions, micro-expressions are more challenging to be analyzed due to the short span of time and the fine-grained changes. In recent years, micro-expression recognition (MER) has drawn much attention because it can benefit a wide range of applications, e.g., police interrogation, clinical diagnosis, depression analysis, and business negotiation. In this survey, we offer a fresh overview to discuss new research directions and challenges these days for MER tasks. For example, we review MER approaches from three novel aspects: macro-to-micro adaptation, recognition based on key apex frames, and recognition based on facial action units. Moreover, to mitigate the problem of limited and biased ME data, synthetic data generation is surveyed for the diversity enrichment of micro-expression data. Since micro-expression spotting can boost micro-expression analysis, the state-of-the-art spotting works are also introduced in this paper. At last, we discuss the challenges in MER research and provide potential solutions as well as possible directions for further investigation.},
  archive  = {J},
  author   = {Hong-Xia Xie and Ling Lo and Hong-Han Shuai and Wen-Huang Cheng},
  doi      = {10.1109/TAFFC.2022.3143100},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1857-1875},
  title    = {An overview of facial micro-expression analysis: Data, methodology and challenge},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoML-emo: Automatic knowledge selection using congruent
effect for emotion identification in conversations. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1845–1856. (<a
href="https://doi.org/10.1109/TAFFC.2022.3232166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversations (ERC) has wide applications in medical care, human-computer interaction, and other fields. Unlike the general task of emotion analysis, humans usually rely on context and commonsense knowledge to convey emotions in conversations. Only when the model can connect and fully utilize a large-scale commonsense knowledge base, it can better understand latent contents in conversations. Unfortunately, there is no available knowledge selection mechanism to address such knowledge needs and to make sure the system is not flooded with irrelevant commonsense knowledge. Therefore, we propose an AutoML strategy based on emotion congruent effect to select suitable knowledge and models, called AutoML-Emo. Global exploration and local exploitation-based selection mechanisms (G&amp;LESM) are used for automatic knowledge selection. The transformer-based architecture search (TAS) is applied to model selection, the selected transformer-based model is employed to incorporate knowledge and capture context information in conversations. The experimental results show that AutoML-Emo can effectively enhance external knowledge in different sizes and domain datasets. Moreover, the selected transformer-based model derived from TAS is superior to the most advanced models.},
  archive  = {J},
  author   = {Dazhi Jiang and Runguo Wei and Jintao Wen and Geng Tu and Erik Cambria},
  doi      = {10.1109/TAFFC.2022.3232166},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1845-1856},
  title    = {AutoML-emo: Automatic knowledge selection using congruent effect for emotion identification in conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal emotion-cause pair extraction in conversations.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
1832–1844. (<a
href="https://doi.org/10.1109/TAFFC.2022.3226559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conversation is an important form of human communication and contains a large number of emotions. It is interesting to discover emotions and their causes in conversations. Conversation in its natural form is multimodal. Many studies have been carried out on multimodal emotion recognition in conversations, yet there is still a lack of work on multimodal emotion cause analysis. In this article, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly extract emotions and the corresponding causes from conversations reflected in multiple modalities (i.e., text, audio and video). We accordingly construct a multimodal conversational emotion cause dataset, Emotion-Cause-in-Friends, which contains 9,794 multimodal emotion-cause pairs among 13,619 utterances in the Friends sitcom. We benchmark the task by establishing two baseline systems including a heuristic approach considering inherent patterns in the location of causes and emotions and a deep learning approach that incorporates multimodal features for emotion-cause pair extraction, and conduct the human performance test for comparison. Furthermore, we investigate the effect of multimodal information, explore the potential of incorporating commonsense knowledge, and perform the task under both Static and Real-time settings.},
  archive  = {J},
  author   = {Fanfan Wang and Zixiang Ding and Rui Xia and Zhaoyu Li and Jianfei Yu},
  doi      = {10.1109/TAFFC.2022.3226559},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1832-1844},
  title    = {Multimodal emotion-cause pair extraction in conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PerceptSent - exploring subjectivity in a novel dataset for
visual sentiment analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 1817–1831. (<a
href="https://doi.org/10.1109/TAFFC.2022.3225238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual sentiment analysis is a challenging problem. Many datasets and approaches have been designed to foster breakthroughs in this trending research topic. However, most works scrutinize only subsymbolic models through visual attributes of the evaluated images, paying less attention to the subjectivity of viewers’ perceptions as a basis for neuro-symbolic systems. Aiming to fill this gap, we present PerceptSent, a novel dataset for visual sentiment analysis that spans 5,000 images shared by users on social networks. Besides the sentiment opinion (positive, slightly positive, neutral, slightly negative, negative) expressed by every evaluator about each image analyzed, the dataset contains evaluator&#39;s metadata (age, gender, socioeconomic status, education, and psychological hints) as well as perceptions observed by the evaluator about the image — such as the presence of nature, violence, lack of maintenance, etc. Deep architectures and different problem formulations are explored using our dataset to combine visual and extra attributes (external knowledge) for automatic sentiment analysis. We show evidence that evaluator&#39;s perceptionss, when correctly employed, are crucial in visual sentiment analysis, improving the F-score performance from 61% to an impressive rate above 97%. Although, at this point, we do not have automatic approaches to capture these perceptions, our results open up new investigation avenues.},
  archive  = {J},
  author   = {Cesar Rafael Lopes and Rodrigo Minetto and Myriam Regattieri Delgado and Thiago H Silva},
  doi      = {10.1109/TAFFC.2022.3225238},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1817-1831},
  title    = {PerceptSent - exploring subjectivity in a novel dataset for visual sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment- emotion- and context-guided knowledge selection
framework for emotion recognition in conversations. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1803–1816. (<a
href="https://doi.org/10.1109/TAFFC.2022.3223517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversations (ERC) needs to detect the emotion of each utterance in conversations. However, it is difficult for machines to recognize the emotion of utterances like humans, partly because of the lack of commonsense knowledge. Despite existing efforts gradually incorporate knowledge in ERC, they can not adaptively adjust knowledge according to different utterances and their context. In this article, we propose a knowledge selection framework SKSEC ( S elect K nowledge in light of S entiment E motion and C ontext). In the SKSEC framework, first, external knowledge is eliminated by three Knowledge Elimination (KE) modules. More concretely, In word-level KE, the concept knowledge different from the sentiment corresponding to the word in utterances is randomly eliminated. In utterance- or context-level KE, If the similarity between the knowledge representation and the emotion label representation of the current utterance or its context is less than the preset threshold, the knowledge will be eliminated. Then we refine the weight of knowledge using two Graph ATtention (GAT) mechanisms. Specifically, In Sentics GAT, we employ a dimensional emotion model to measure words in utterances and their corresponding knowledge and adjust the weight of knowledge according to their emotional similarity. In Semantics GAT, the weight of knowledge is adjusted according to the semantic similarity between context and incorporated knowledge. Finally, we feed the selected knowledge to the most advanced models to evaluate the quality of knowledge. The experimental results show that the SKSEC framework can effectively improve the performance of the model by eliminating and refining external knowledge in different size and domain datasets.},
  archive  = {J},
  author   = {Geng Tu and Bin Liang and Dazhi Jiang and Ruifeng Xu},
  doi      = {10.1109/TAFFC.2022.3223517},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1803-1816},
  title    = {Sentiment- emotion- and context-guided knowledge selection framework for emotion recognition in conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classifying suicide-related content and emotions on twitter
using graph convolutional neural networks. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(3), 1791–1802. (<a
href="https://doi.org/10.1109/TAFFC.2022.3221683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent work in Natural Language Processing has increasingly focused on detecting suicidal intent in textual data, where the main aim is to detect expressions in a binary setting. However, previous research has shown that search results and other mentions of suicide online are not only limited to expressions of suicidal intent. Therefore, previously proposed algorithms and datasets might for example struggle to distinguish between suicidal intent of a user and suicide mentioned in a humorous context. In this article we introduce a new dataset called TWISCO , which proposes an alternative approach to classifying expressions of suicidality online. For this, we use a coding framework developed in Psychology to distinguish between different mentions of suicide. Next, we present a variety of machine and deep learning baselines in three different classification settings ( text only , features only and text and features ). Furthermore, we introduce a Feature GCN that improves performance over the GCN baseline. Finally, we investigate the hypothesis that feelings of dominance are correlated with users expressing their own suicidality. We provide an in-depth discussion of the trade-offs in classifying suicidal intent online.},
  archive  = {J},
  author   = {Annika M Schoene and Lana Bojanić and Minh-Quoc Nghiem and Isabelle M Hunt and Sophia Ananiadou},
  doi      = {10.1109/TAFFC.2022.3221683},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1791-1802},
  title    = {Classifying suicide-related content and emotions on twitter using graph convolutional neural networks},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reinforcement learning based two-stage model for emotion
cause pair extraction. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(3), 1779–1790. (<a
href="https://doi.org/10.1109/TAFFC.2022.3218648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, many efforts have been devoted to promoting the Emotion-Cause Pair Extraction (ECPE) task, as jointly extracting emotions and their causes is considered more helpful than only identifying the emotions in many applications. Among the existing efforts, end-to-end approaches are getting popular as the main trend, while others like pipeline models have been overlooked due to their potential issues of cascading errors. Nevertheless, the advantages of the pipeline models, such as logically dividing a complicated task into multiple easier subtasks, are underestimated and not well exploited. Moreover, the existing end-to-end approaches fail to capture the implicit co-occurrence or exclusion patterns between multiple pairs of emotions and causes since they are extracted independently. In view of these limitations, we propose a novel two-stage model to address the ECPE task and incorporate reinforcement learning (RL) to tackle the cascading error issue. In particular, our two-stage model first detects emotion clauses and then recognizes cause clauses for each detected emotion clause sequentially. By representing the error of each decision as an explicit reward, our model clearly knows how the error at each stage affects the final performance, hence the model can adjust itself for better performance. Furthermore, the sequential prediction enables our model to use the results achieved in the previous stages as auxiliary information in the subsequent stages. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our proposed two-stage model, and the ablation comparison shows the promising effect of reducing cascading errors by incorporating RL.},
  archive  = {J},
  author   = {Xinhong Chen and Qing Li and Zongxi Li and Haoran Xie and Fu Lee Wang and Jianping Wang},
  doi      = {10.1109/TAFFC.2022.3218648},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1779-1790},
  title    = {A reinforcement learning based two-stage model for emotion cause pair extraction},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective 3D text recurrent voting generator for
metaverse. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 1766–1778. (<a
href="https://doi.org/10.1109/TAFFC.2022.3216782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Metaverse is a novel innovative platform that connects users worldwide in the distributed virtual environment. People share their interests, opinions, and resources on this virtual reality platform. With this, we come to know that besides other fundamental techniques, the language generation method is also a necessity to regulate the VR environment. There are several types of language generation methods in 3D, including neural learning, such as GRU, RNN, and GPT-3, and transfer learning. This paper proposes a recurrent voting generator (RVG) system that understands the 3D text of a book and performs emotional analytics within a metaverse space. The proposed model RVG evaluates emotions through three algorithms such as the first module is a recurrent sentiment generator (RSG) that analyzes emotions and calculates and generates the distributions. The second module is the sentiment decomposition (SD) that optimizes higher dimensions in Big Data. And, the third module is the compound voting learning (CVL) module that performs calculations with an emphasis on optimal performance. The dataset used to evaluate RVG is based on the content of movie reviews and books. The performance evaluation shows that the proposed approach outperforms better compared to the existing 2D RNN models in the metaverse.},
  archive  = {J},
  author   = {Woo Hyun Park and Nawab Muhammad Faseeh Qureshi and Dong Ryeol Shin},
  doi      = {10.1109/TAFFC.2022.3216782},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1766-1778},
  title    = {An effective 3D text recurrent voting generator for metaverse},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). ECPEC: Emotion-cause pair extraction in conversations.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
1754–1765. (<a
href="https://doi.org/10.1109/TAFFC.2022.3216551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conversational sentiment analysis (CSA) and emotion-cause pair extraction (ECPE) tasks have attracted increasing attention in recent years. The former aims to predict the sentiment states of speakers in a conversation, and the latter is about extracting emotion-cause clauses in a document. However, one drawback of CSA is that it cannot model the causal reasoning among emotion and neutral utterances from different speakers. In this work, we propose a new task: emotion-cause pair extraction in conversations (ECPEC), which aims to extract pairs of emotional utterances and corresponding cause utterances in conversations. The utterance-level ECPEC task is more challenging since the distance between emotion and cause utterances is larger than that of the clause-level ECPE task. To this end, we build a novel dataset ConvECPE and propose a specifically designed two-step framework for the new ECPEC task. Experimental results on ConvECPE dataset demonstrate the feasibility of the ECPEC task as well as the effectiveness of our framework.},
  archive  = {J},
  author   = {Wei Li and Yang Li and Vlad Pandelea and Mengshi Ge and Luyao Zhu and Erik Cambria},
  doi      = {10.1109/TAFFC.2022.3216551},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1754-1765},
  title    = {ECPEC: Emotion-cause pair extraction in conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The biases of pre-trained language models: An empirical
study on prompt-based sentiment analysis and emotion detection. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(3), 1743–1753. (<a
href="https://doi.org/10.1109/TAFFC.2022.3204972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, e.g., sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word mapping templates. Prompt-based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. However, there still remain many unclear issues related to the mechanisms of PLMs and prompt-based classification. We conduct a systematic empirical study on prompt-based sentiment analysis and emotion detection to study the biases of PLMs towards affective computing. We find that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons.},
  archive  = {J},
  author   = {Rui Mao and Qian Liu and Kai He and Wei Li and Erik Cambria},
  doi      = {10.1109/TAFFC.2022.3204972},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1743-1753},
  title    = {The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-based self-training and re-weighting for aspect-based
sentiment analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 1731–1742. (<a
href="https://doi.org/10.1109/TAFFC.2022.3202831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment analysis (ABSA) means to identify fine-grained aspects, opinions, and sentiment polarities. Recent ABSA research focuses on utilizing multi-task learning (MTL) to achieve less computational costs and better performance. However, there are certain limits in MTL-based ABSA. For example, unbalanced labels and sub-task learning difficulties may result in the biases that some labels and sub-tasks are overfitting, while the others are underfitting. To address these issues, inspired by neuro-symbolic learning systems, we propose a meta-based self-training method with a meta-weighter (MSM). We believe that a generalizable model can be achieved by appropriate symbolic representation selection (in-domain knowledge) and effective learning control (regulation) in a neural system. Thus, MSM trains a teacher model to generate in-domain knowledge (e.g., unlabeled data selection and pseudo-label generation), where the generated pseudo-labels are used by a student model for supervised learning. Then, the meta-weighter of MSM is jointly trained with the student model to provide each instance with sub-task-specific weights to coordinate their convergence rates, balancing class labels, and alleviating noise impacts introduced from self-training. The following experiments indicate that MSM can utilize 50% labeled data to achieve comparable results to state-of-arts models in ABSA and outperform them with all labeled data.},
  archive  = {J},
  author   = {Kai He and Rui Mao and Tieliang Gong and Chen Li and Erik Cambria},
  doi      = {10.1109/TAFFC.2022.3202831},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1731-1742},
  title    = {Meta-based self-training and re-weighting for aspect-based sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TSSRD: A topic sentiment summarization framework based on
reaching definition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(3), 1716–1730. (<a
href="https://doi.org/10.1109/TAFFC.2022.3186015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exposure to massive information in daily lives makes it necessary for people to obtain major points efficiently, promoting the development of text summarization technology. However, existing sentiment-based text summarization methods only pay attention to the sentiment polarity of either a single sentence or a whole document, ignoring changes of sentiments along with sentences or sentiment flow across the whole document. To incorporate the above two aspects into the summarization process to generate high-quality summaries, we propose a topic sentiment summarization framework based on reaching definition (TSSRD). In the framework, we first use topic models to model documents and calculate topic sentiment embeddings. Then, we analyze document structures from different perspectives to design data flow diagrams, in which improved reaching definition is used to analyze sentiment changes and sentiment flow. Finally, topic sentiment summaries are generated based on sentiments in steady states of the reaching definition. To evaluate our summarization framework, we introduce an extrinsic evaluation method. In this method, a sentiment classifier is trained by the topic sentiment summaries, and accuracy of the sentiment classification is used as a quality score. Experimental results demonstrate that our summarization framework is at least 2.32% better than baselines on IMDb and Amazon datasets.},
  archive  = {J},
  author   = {Xiaodong Li and Chenxin Zou and Pangjing Wu and Qing Li},
  doi      = {10.1109/TAFFC.2022.3186015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1716-1730},
  title    = {TSSRD: A topic sentiment summarization framework based on reaching definition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial neurosymbolic AI for sentiment analysis.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(3),
1711–1715. (<a
href="https://doi.org/10.1109/TAFFC.2023.3310856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural network-based methods, especially deep learning, have been a burgeoning area in AI research and have been successful in tackling the expanding data volume as we move into a digital age. Today, the neural network-based methods are not only used for low-level cognitive tasks, such as recognizing objects and spotting keywords, but they have also been deployed in various industrial information systems to assist high-level decision-making. In natural language processing, there have been two milestones for the past decade: one is word2vec [1], a group of neural models that learn word embeddings (vector representations of words) from large datasets; and one is the most recent GPT-based models [2], which combine reinforcement learning with a generative transformer in order to enable multi-round end-to-end conversations. While producing highly accurate predictions on datasets and generating human-like utterances, those neural network-based artifacts provide little understanding of the internal features and representations of the data. Many problems and concerns subsequently emerge from this black-box issue. Because some of the problems and concerns are also relevant in the context of sentiment analysis.},
  archive  = {J},
  author   = {Frank Xing and Björn Schuller and Iti Chaturvedi and Erik Cambria and Amir Hussain},
  doi      = {10.1109/TAFFC.2023.3310856},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {1711-1715},
  title    = {Guest editorial neurosymbolic AI for sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A re-analysis and synthesis of data on affect dynamics in
learning. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1696–1710. (<a
href="https://doi.org/10.1109/TAFFC.2021.3086118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affect dynamics, the study of how affect develops and manifests over time, has become a popular area of research in affective computing for learning. In this article, we first provide a detailed analysis of prior affect dynamics studies, elaborating both their findings and the contextual and methodological differences between these studies. We then address methodological concerns that have not been previously addressed in the literature, discussing how various edge cases should be treated. Next, we present mathematical evidence that several past studies applied the transition metric (L) incorrectly - leading to invalid conclusions of statistical significance - and provide a corrected method. Using this corrected analysis method, we reanalyze ten past affect datasets collected in diverse contexts and synthesize the results, determining that the findings do not match the most popular theoretical model of affect dynamics. Instead, our results highlight the need to focus on cultural factors in future affect dynamics research.},
  archive  = {J},
  author   = {Shamya Karumbaiah and Ryan S. Baker and Jaclyn Ocumpaugh and Juliana Ma. Alexandra L. Andres},
  doi      = {10.1109/TAFFC.2021.3086118},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1696-1710},
  title    = {A re-analysis and synthesis of data on affect dynamics in learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic emotion recognition in clinical scenario: A
systematic review of methods. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1675–1695. (<a
href="https://doi.org/10.1109/TAFFC.2021.3128787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {BACKGROUND - Automatic emotion recognition has powerful and interesting opportunities in the clinical field, but several critical aspects are still open, such as heterogeneity of methodologies or technologies tested mainly on healthy people. This systematic review aims to survey automatic emotion recognition systems applied in real clinical contexts (i.e., on a population of people with a pathology). METHODS - The literature review was conducted on the following scientific databases: IEEE Xplore ®, ScienceDirect®, Scopus®, PubMed®, ACM®. Inclusion criteria were the presence of an automatic emotion recognition algorithm and the enrollment of at least 2 patients in the experimental protocol. The review process followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Moreover, the works were analysed according to a reference model in the form of a class diagram, to highlight the most important clinical and technical aspects and relationships among them. RESULTS - 52 scientific papers passed the inclusion criteria. Based on our findings, most clinical applications involved neuro-developmental, neurological and psychiatric disorders with the aims of diagnosing, monitoring, or treating emotional symptoms. The study design seems to be mostly related to the aim of the study (it is generally observational for monitoring and diagnosis, interventional for treatment), the most adopted signals are video and audio, and supervised shallow learning emerged as most used approach for emotion recognition algorithm. DISCUSSION - Tiny samples, absence of a control group and of tests in real-life conditions emerged as important clinical limitations. Under a technical point of view, a great heterogeneity of performance metrics, datasets and algorithms challenges the comparability, robustness, reliability and reproducibility of results. Suggested guidelines are identified and discussed to help scientific community to overcome limitations and provide direction for future works.},
  archive  = {J},
  author   = {Lucia Pepa and Luca Spalazzi and Marianna Capecci and Maria Gabriella Ceravolo},
  doi      = {10.1109/TAFFC.2021.3128787},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1675-1695},
  title    = {Automatic emotion recognition in clinical scenario: A systematic review of methods},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of affective computing research based on
function-component-representation framework. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1655–1674. (<a
href="https://doi.org/10.1109/TAFFC.2021.3104512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective computing (AC), a field that bridges the gap between human affect and computational technology, has witnessed remarkable technical advancement. However, theoretical underpinnings of affective computing are rarely discussed and reviewed. This paper provides a thorough conceptual analysis of the literature to understand theoretical questions essential to affective computing and current answers. Inspired by emotion theories, we proposed the function-component-representation (FCR) framework to organize different conceptions of affect along three dimensions that each address an important question: function of affect (why compute affect), component of affect (how to compute affect), and representation of affect (what affect to compute). We coded each paper by its underlying conception of affect and found preferences towards affect detection, behavioral component, and categorical representation. We also observed coupling of certain conceptions. For example, papers using the behavioral component tend to adopt the categorical representation, whereas papers using the physiological component tend to adopt the dimensional representation. The FCR framework is not only the first attempt to organize different theoretical perspectives in a systematic and quantitative way, but also a blueprint to help conceptualize an AC project and pinpoint new possibilities. Future work may explore how the identified frequencies of FCR framework combinations may be applied in practice.},
  archive  = {J},
  author   = {Haiwei Ma and Svetlana Yarosh},
  doi      = {10.1109/TAFFC.2021.3104512},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1655-1674},
  title    = {A review of affective computing research based on function-component-representation framework},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey of deep representation learning for speech emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1634–1654. (<a
href="https://doi.org/10.1109/TAFFC.2021.3114365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditionally, speech emotion recognition (SER) research has relied on manually handcrafted acoustic features using feature engineering. However, the design of handcrafted features for complex SER tasks requires significant manual effort, which impedes generalisability and slows the pace of innovation. This has motivated the adoption of representation learning techniques that can automatically learn an intermediate representation of the input signal without any manual feature engineering. Representation learning has led to improved SER performance and enabled rapid innovation. Its effectiveness has further increased with advances in deep learning (DL), which has facilitated deep representation learning where hierarchical representations are automatically learned in a data-driven manner. This article presents the first comprehensive survey on the important topic of deep representation learning for SER. We highlight various techniques, related challenges and identify important future areas of research. Our survey bridges the gap in the literature since existing surveys either focus on SER with hand-engineered features or representation learning in the general setting without focusing on SER.},
  archive  = {J},
  author   = {Siddique Latif and Rajib Rana and Sara Khalifa and Raja Jurdak and Junaid Qadir and Björn Schuller},
  doi      = {10.1109/TAFFC.2021.3114365},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1634-1654},
  title    = {Survey of deep representation learning for speech emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning in emotion recognition of spontaneous
speech using a siamese neural network with adaptive sample pair
formation. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1627–1633. (<a
href="https://doi.org/10.1109/TAFFC.2021.3109485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech-based machine learning (ML) has been heralded as a promising solution for tracking prosodic and spectrotemporal patterns in real-life that are indicative of emotional changes, providing a valuable window into one&#39;s cognitive and mental state. Yet, the scarcity of labelled data in ambulatory studies prevents the reliable training of ML models, which usually rely on “data-hungry” distribution-based learning. Leveraging the abundance of labelled speech data from acted emotions, this paper proposes a few-shot learning approach for automatically recognizing emotion in spontaneous speech from a small number of labelled samples. Few-shot learning is implemented via a metric learning approach through a siamese neural network, which models the relative distance between samples rather than relying on learning absolute patterns of the corresponding distributions of each emotion. Results indicate the feasibility of the proposed metric learning in recognizing emotions from spontaneous speech in four datasets, even with a small amount of labelled samples. They further demonstrate superior performance of the proposed metric learning compared to commonly used adaptation methods, including network fine-tuning and adversarial learning. Findings from this work provide a foundation for the ambulatory tracking of human emotion in spontaneous speech contributing to the real-life assessment of mental health degradation.},
  archive  = {J},
  author   = {Kexin Feng and Theodora Chaspari},
  doi      = {10.1109/TAFFC.2021.3109485},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1627-1633},
  title    = {Few-shot learning in emotion recognition of spontaneous speech using a siamese neural network with adaptive sample pair formation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A region group adaptive attention model for subtle
expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1613–1626. (<a
href="https://doi.org/10.1109/TAFFC.2021.3133429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition has received extensive attention in recent years due to its important applications in many fields. Most expression samples used in research are relatively easy to analyze emotions because they have explicit expressions with strong intensities. However, in situations such as video question and answer, business negotiation, polygraph detection in the security field, autism treatment and medical escort, emotions are expressed in suppressed manners with low intensive expression or subtle expressions, making it difficult to estimate emotions accurately. In these situations, how to effectively extract expression features from facial expression images is a critical problem that affects the accuracy of subtle expression recognition. To address this problem, we propose an end-to-end group adaptive attention model for subtle expression recognition. Cropping an image into several regions of interest (ROI) according to the correlations between facial skeleton and emotions, the proposed model analyses the relationship among regions of interest, and mutual relations between local regions and the holistic region. Using the region group adaptive attention mechanism, the model effectively trains the convolutional neural network to efficiently extract facial expressions representing features and increases the accuracy and robustness of the recognition, particularly in some subtle facial expression circumstances. To improve the ability of different regional features to discriminate expressions, a group adaptive loss function is introduced to verify and improve estimation accuracy. Extensive experiments are conducted on the existing public face datasets CK+, JAFFE, KDEF and the self-collected subtle expression dataset SFER. Results show that the proposed model achieves accuracies of 99.59%, 95.20%, and 93.47% with datasets CK+, JAFFE, and KDEF, respectively. The proposed model thus generally achieves better performance in facial expression recognition than other methods.},
  archive  = {J},
  author   = {Gan Chen and Junjie Peng and Wenqiang Zhang and Kanrun Huang and Feng Cheng and Haochen Yuan and Yansong Huang},
  doi      = {10.1109/TAFFC.2021.3133429},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1613-1626},
  title    = {A region group adaptive attention model for subtle expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discerning affect from touch and gaze during interaction
with a robot pet. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1598–1612. (<a
href="https://doi.org/10.1109/TAFFC.2021.3094894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Practical affect recognition needs to be efficient and unobtrusive in interactive contexts. One approach to a robust realtime system is to sense and automatically integrate multiple nonverbal sources. We investigated how users’ touch , and secondarily gaze , perform as affect-encoding modalities during physical interaction with a robot pet, in comparison to more-studied biometric channels. To elicit authentically experienced emotions, participants recounted two intense memories of opposing polarity in Stressed - Relaxed or Depressed - Excited conditions. We collected data (N=30) from a touch sensor embedded under robot fur (force magnitude and location), a robot-adjacent gaze tracker (location), and biometric sensors (skin conductance, blood volume pulse, respiration rate). Cross-validation of Random Forest classifiers achieved best-case accuracy for combined touch-with-gaze approaching that of biometric results: where training and test sets include adjacent temporal windows, subject-dependent prediction was 94% accurate. In contrast, subject-independent Leave-One-participant-Out predictions resulted in 30% accuracy (chance 25%). Performance was best where participant information was available in both training and test sets. Addressing computational robustness for dynamic, adaptive realtime interactions, we analyzed subsets of our multimodal feature set, varying sample rates and window sizes. We summarize design directions based on these parameters for this touch-based, affective, and hard, realtime robot interaction application.},
  archive  = {J},
  author   = {Xi Laura Cang and Paul Bucci and Jussi Rantala and Karon E. MacLean},
  doi      = {10.1109/TAFFC.2021.3094894},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1598-1612},
  title    = {Discerning affect from touch and gaze during interaction with a robot pet},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based emotional video classification via learning
connectivity structure. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1586–1597. (<a
href="https://doi.org/10.1109/TAFFC.2021.3126263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG) is a useful way to implicitly monitor the user&#39;s perceptual state during multimedia consumption. One of the primary challenges for the practical use of EEG-based monitoring is to achieve a satisfactory level of accuracy in EEG classification. Connectivity between different brain regions is an important property for the classification of EEG. However, how to define the connectivity structure for a given task is still an open problem, because there is no ground truth about how the connectivity structure should be in order to maximize the classification performance. In this paper, we propose an end-to-end neural network model for EEG-based emotional video classification, which can extract an appropriate multi-layer graph structure and signal features directly from a set of raw EEG signals and perform classification using them. Experimental results demonstrate that our method yields improved performance in comparison to the existing approaches where manually defined connectivity structures and signal features are used. Furthermore, we show that the graph structure extraction process is reliable in terms of consistency, and the learned graph structures make much sense in the viewpoint of emotional perception occurring in the brain.},
  archive  = {J},
  author   = {Soobeom Jang and Seong-Eun Moon and Jong-Seok Lee},
  doi      = {10.1109/TAFFC.2021.3126263},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1586-1597},
  title    = {EEG-based emotional video classification via learning connectivity structure},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The mediating effect of emotions on trust in the context of
automated system usage. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1572–1585. (<a
href="https://doi.org/10.1109/TAFFC.2021.3094883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Safety-critical systems are often equipped with warning mechanisms to alert users regarding imminent system failures. However, they can suffer from false alarms, and affect users’ emotions and trust in the system negatively. While providing feedback could be an effective way to calibrate trust under such scenarios, the effects of feedback and warning reliability on users’ emotions, trust, and compliance behavior are not clear. This article investigates this by designing a 2 (feedback: present/absent) × 2 (warning reliability: high/low) × 4 (sessions) mixed design study where participants interacted with a simulated unmanned aerial vehicle (UAV) system to identify and neutralize enemy targets. Results indicated that feedback containing both correctness and affective components decreased users’ positive emotions and trust in the system, and increased loneliness and hostility (negative) emotions. Emotions were found to mediate the relationship between feedback and trust. Implications of our findings for designing feedback and calibration of trust are discussed in the paper.},
  archive  = {J},
  author   = {Md Abdullah Al Fahim and Mohammad Maifi Hasan Khan and Theodore Jensen and Yusuf Albayram and Emil Coman and Ross Buck},
  doi      = {10.1109/TAFFC.2021.3094883},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1572-1585},
  title    = {The mediating effect of emotions on trust in the context of automated system usage},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-task and cross-participant classification of cognitive
load in an emergency simulation game. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1558–1571. (<a
href="https://doi.org/10.1109/TAFFC.2021.3098237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Assessment of cognitive load is a major step towards adaptive interfaces. However, non-invasive assessment is rather subjective as well as task specific and generalizes poorly, mainly due to methodological limitations. Additionally, it heavily relies on performance data like game scores or test results. In this study, we present an eye-tracking approach that circumvents these shortcomings and allows for effective generalizing across participants and tasks. First, we established classifiers for predicting cognitive load individually for a typical working memory task (n-back), which we then applied to an emergency simulation game by considering the similar ones and weighting their predictions. Standardization steps helped achieve high levels of cross-task and cross-participant classification accuracy between 63.78 and 67.25 percent for the distinction between easy and hard levels of the emergency simulation game. These very promising results could pave the way for novel adaptive computer-human interaction across domains and particularly for gaming and learning environments.},
  archive  = {J},
  author   = {Tobias Appel and Peter Gerjets and Stefan Hoffmann and Korbinian Moeller and Manuel Ninaus and Christian Scharinger and Natalia Sevcenko and Franz Wortha and Enkelejda Kasneci},
  doi      = {10.1109/TAFFC.2021.3098237},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1558-1571},
  title    = {Cross-task and cross-participant classification of cognitive load in an emergency simulation game},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the contextual factors affecting multimodal
emotion recognition in videos. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1547–1557. (<a
href="https://doi.org/10.1109/TAFFC.2021.3071503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional expressions form a key part of user behavior on today&#39;s digital platforms. While multimodal emotion recognition techniques are gaining research attention, there is a lack of deeper understanding on how visual and non-visual features can be used to better recognize emotions in certain contexts, but not others. This study analyzes the interplay between the effects of multimodal emotion features derived from facial expressions, tone and text in conjunction with two key contextual factors: i) gender of the speaker, and ii) duration of the emotional episode. Using a large public dataset of 2,176 manually annotated YouTube videos, we found that while multimodal features consistently outperformed bimodal and unimodal features, their performance varied significantly across different emotions, gender and duration contexts. Multimodal features performed particularly better for male speakers in recognizing most emotions. Furthermore, multimodal features performed particularly better for shorter than for longer videos in recognizing neutral and happiness, but not sadness and anger. These findings offer new insights towards the development of more context-aware emotion recognition and empathetic systems.},
  archive  = {J},
  author   = {Prasanta Bhattacharya and Raj Kumar Gupta and Yinping Yang},
  doi      = {10.1109/TAFFC.2021.3071503},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1547-1557},
  title    = {Exploring the contextual factors affecting multimodal emotion recognition in videos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User state modeling based on the arousal-valence plane:
Applications in customer satisfaction and health-care. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1533–1546. (<a
href="https://doi.org/10.1109/TAFFC.2021.3112543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The acoustic analysis helps to discriminate emotions according to non-verbal information, while linguistics aims to capture verbal information from written sources. Acoustic and linguistic analyses can be addressed for different applications, where information related to emotions, mood, or affect are involved. The Arousal-Valence plane is commonly used to model emotional states in a multidimensional space. This study proposes a methodology focused on modeling the user’s state based on the Arousal-Valence plane in different scenarios. Acoustic and linguistic information are used as input to feed different deep learning architectures mainly based on convolutional and recurrent neural networks, which are trained to model the Arousal-Valence plane. The proposed approach is used for the evaluation of customer satisfaction in call-centers and for health-care applications in the assessment of depression in Parkinson’s disease and the discrimination of Alzheimer’s disease. F-scores of up to 0.89 are obtained for customer satisfaction, of up to 0.82 for depression in Parkinson’s patients, and of up to 0.80 for Alzheimer’s patients. The proposed approach confirms that there is information embedded in the Arousal-Valence plane that can be used for different purposes.},
  archive  = {J},
  author   = {Paula Andrea Pérez-Toro and Juan Camilo Vásquez-Correa and Tobias Bocklet and Elmar Nöth and Juan Rafael Orozco-Arroyave},
  doi      = {10.1109/TAFFC.2021.3112543},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1533-1546},
  title    = {User state modeling based on the arousal-valence plane: Applications in customer satisfaction and health-care},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AT2GRU: A human emotion recognition model with mitigated
device heterogeneity. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1520–1532. (<a
href="https://doi.org/10.1109/TAFFC.2021.3114123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Device heterogeneity can cause a detrimental impact on the classification of healthcare data. In this work, we propose the Maximum Difference-based Heterogeneity Mitigation (MDHM) method to address device heterogeneity. Mitigating heterogeneity increases the reliability of using multiple devices from different manufacturers for measuring a particular physiological signal. Further, we propose an attention-based bilevel GRU (Gated Recurrent Unit) model, abbreviated as AT2GRU, to classify multi-modal healthcare time-series data for human emotion recognition. The physiological signals of Electroencephalogram (EEG) and Electrocardiogram (ECG) for twenty-three persons are leveraged from the DREAMER dataset for emotion recognition. Also, from the DEAP dataset, the biosignals namely EEG, Galvanic Skin Response (GSR), Respiration Amplitude (RA), Skin Temperature (ST), Blood Volume (BV), Electromyogram (EMG) and Electrooculogram (EOG) of thirty-two persons are used for emotion recognition. The EEG and the other biosignals are denoised by the wavelet filters for enhancing the model&#39;s classification accuracy. A multi-class classification is carried out considering valence, arousal, and dominance for each person in the datasets. The classification accuracy is validated against the self-assessment obtained from the respective person after watching a movie/video. The proposed AT2GRU model surpasses the other sequential models namely Long Short Term Memory (LSTM) and GRU in performance.},
  archive  = {J},
  author   = {Pritam Khan and Priyesh Ranjan and Sudhir Kumar},
  doi      = {10.1109/TAFFC.2021.3114123},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1520-1532},
  title    = {AT2GRU: A human emotion recognition model with mitigated device heterogeneity},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Indirect identification of perinatal psychosocial risks from
natural language. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1506–1519. (<a
href="https://doi.org/10.1109/TAFFC.2021.3079282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {During the perinatal period, psychosocial health risks, including depression and intimate partner violence, are associated with serious adverse health outcomes for birth parents and children. To appropriately intervene, healthcare professionals must first identify those at risk, yet stigma often prevents people from directly disclosing the information needed to prompt an assessment. In this research we use short diary entries to indirectly elicit information that could indicate psychosocial risks, then examine patterns that emerge in the language of those at risk. We find that diary entries exhibit consistent themes, extracted using topic modeling, and emotional perspective, drawn from dictionary-informed sentiment features. Using these features, we use regularized regression to predict screening measures for depression and psychological aggression by an intimate partner. Journal text entries quantified through topic models and sentiment features show promise for depression prediction, corresponding with self-reported screening measures almost as well as closed-form questions. Text-based features are less useful in predicting intimate partner violence, but topic models generate themes that align with known risk correlates. The indirect features uncovered in this research could aid in the detection and analysis of stigmatized risks.},
  archive  = {J},
  author   = {Kristen C. Allen and Alex Davis and Tamar Krishnamurti},
  doi      = {10.1109/TAFFC.2021.3079282},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1506-1519},
  title    = {Indirect identification of perinatal psychosocial risks from natural language},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding needles in a haystack: Recognizing emotions just
from your heart. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1488–1505. (<a
href="https://doi.org/10.1109/TAFFC.2021.3096542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion plays an important role in human cognition and behavior. How to recognize emotions based on physiological signals has attracted increasing research interests worldwide up to date. Both traditional eastern medicine and modern western medicine have confirmed the existence of relationship between human emotionality and heart activity. However, in practice, emotion recognition only using Electrocardiogram (ECG) signals seems quite challenging, not only due to the severe noise interferences and the serious data variations, but also because of the ambiguous relationship between emotional states and ECG data. Such difficulty can even be compared to finding needles in a haystack. As an innovative endeavor to deal with the issue of only-ECG-based emotion recognition, this paper has proposed a novel solution from the perspective of weak signal classification. The proposed solution extracts the static-dynamic representation under the principle of Yin-Yang balance from the heartbeat data, and then utilizes the set-based collaborative measurement upon the thought of data coopetition to classify these features for recognizing emotions. Experimental results have demonstrated the effectiveness, efficiency and adaptiveness of the solution for uncovering the potential relationship between emotions and ECG. Thus, this proposal has also illuminated a promising research direction for the general problem of weak signal classification.},
  archive  = {J},
  author   = {Wei Li},
  doi      = {10.1109/TAFFC.2021.3096542},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1488-1505},
  title    = {Finding needles in a haystack: Recognizing emotions just from your heart},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EmoNet: A transfer learning framework for multi-corpus
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1472–1487. (<a
href="https://doi.org/10.1109/TAFFC.2021.3135152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this manuscript, the topic of multi-corpus Speech Emotion Recognition (SER) is approached from a deep transfer learning perspective. A large corpus of emotional speech data, EmoSet , is assembled from a number of existing Speech Emotion Recognition (SER) corpora. In total, EmoSet contains 84 181 audio recordings from 26 SER corpora with a total duration of over 65 hours . The corpus is then utilised to create a novel framework for multi-corpus SER and general audio recognition, namely EmoNet . A combination of a deep ResNet architecture and residual adapters is transferred from the field of multi-domain visual recognition to multi-corpus SER on EmoSet . The introduced residual adapter approach enables parameter efficient training of a multi-domain SER model on all 26 corpora. A shared model with only 3.5 times the number of parameters of a model trained on a single database leads to increased performance for 21 of the 26 corpora in EmoSet . Using repeated training runs and Almost Stochastic Order with significance level of $\alpha = 0.05$ , these improvements are further significant for 15 datasets while there are just three corpora that see only significant decreases across the residual adapter transfer experiments. Finally, we make our EmoNet framework publicly available for users and developers at https://github.com/EIHW/EmoNet .},
  archive  = {J},
  author   = {Maurice Gerczuk and Shahin Amiriparian and Sandra Ottl and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2021.3135152},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1472-1487},
  title    = {EmoNet: A transfer learning framework for multi-corpus speech emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving humanness of virtual agents and users’ cooperation
through emotions. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1461–1471. (<a
href="https://doi.org/10.1109/TAFFC.2021.3096831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we analyze the performance of an agent developed according to a well-accepted appraisal theory of human emotion with respect to how it modulates play in the context of a social dilemma. We ask if the agent will be capable of generating interactions that are considered to be more human-like than machine-like. We conducted an experiment with 117 participants and show how participants rated our agent on dimensions of human-uniqueness (separating humans from animals) and human-nature (separating humans from machines). We show that our appraisal theoretic agent is perceived to be more human-like than the baseline models, by significantly improving both human-nature and human-uniqueness aspects of the intelligent agent. We also show that perception of humanness positively affects enjoyment and cooperation in the social dilemma, and discuss consequences for the task duration recall.},
  archive  = {J},
  author   = {Moojan Ghafurian and Neil Budnarain and Jesse Hoey},
  doi      = {10.1109/TAFFC.2021.3096831},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1461-1471},
  title    = {Improving humanness of virtual agents and users’ cooperation through emotions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Examining emotion perception agreement in live music
performance. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1442–1460. (<a
href="https://doi.org/10.1109/TAFFC.2021.3093787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current music emotion recognition (MER) systems rely on emotion data averaged across listeners and over time to infer the emotion expressed by a musical piece, often neglecting time- and listener-dependent factors. These limitations can restrict the efficacy of MER systems and cause misjudgements. We present two exploratory studies on music emotion perception. First, in a live music concert setting, fifteen audience members annotated perceived emotion in the valence-arousal space over time using a mobile application. Analyses of inter-rater reliability yielded widely varying levels of agreement in the perceived emotions. A follow-up lab-based study to uncover the reasons for such variability was conducted, where twenty-one participants annotated their perceived emotions whilst viewing and listening to a video recording of the original performance and offered open-ended explanations. Thematic analysis revealed salient features and interpretations that help describe the cognitive processes underlying music emotion perception. Some of the results confirm known findings of music perception and MER studies. Novel findings highlight the importance of less frequently discussed musical attributes, such as musical structure, performer expression, and stage setting, as perceived across audio and visual modalities. Musicians are found to attribute emotion change to musical harmony, structure, and performance technique more than non-musicians. We suggest that accounting for such listener-informed music features can benefit MER in helping to address variability in emotion perception by providing reasons for listener similarities and idiosyncrasies.},
  archive  = {J},
  author   = {Simin Yang and Courtney N. Reed and Elaine Chew and Mathieu Barthet},
  doi      = {10.1109/TAFFC.2021.3093787},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1442-1460},
  title    = {Examining emotion perception agreement in live music performance},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MERASTC: Micro-expression recognition using effective
feature encodings and 2D convolutional neural network. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1431–1441. (<a
href="https://doi.org/10.1109/TAFFC.2021.3061967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial micro-expression (ME) can disclose genuine and concealed human feelings. It makes MEs extensively useful in real-world applications pertaining to affective computing and psychology. Unfortunately, they are induced by subtle facial movements for a short duration of time, which makes the ME recognition, a highly challenging problem even for human beings. In automatic ME recognition, the well-known features encode either incomplete or redundant information, and there is a lack of sufficient training data. The proposed method, Micro-Expression Recognition by Analysing Spatial and Temporal Characteristics, $MERASTC$ mitigates these issues for improving the ME recognition. It compactly encodes the subtle deformations using action units (AUs), landmarks, gaze, and appearance features of all the video frames while preserving most of the relevant ME information. Furthermore, it improves the efficacy by introducing a novel neutral face normalization for ME and initiating the utilization of gaze features in deep learning-based ME recognition. The features are provided to the 2D convolutional neural network that jointly analyses the spatial and temporal behavior for correct ME classification. Experimental results 1 on publicly available datasets indicate that the proposed method exhibits better performance than the well-known methods.},
  archive  = {J},
  author   = {Puneet Gupta},
  doi      = {10.1109/TAFFC.2021.3061967},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1431-1441},
  title    = {MERASTC: Micro-expression recognition using effective feature encodings and 2D convolutional neural network},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CPED: A chinese positive emotion database for emotion
elicitation and analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1417–1430. (<a
href="https://doi.org/10.1109/TAFFC.2021.3088523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Positive emotions are of great significance to people&#39;s daily life, such as human-computer/robot interaction. However, the structure of extensive positive emotions is not clear yet and effective standardized inducing materials containing as many positive emotional categories as possible are lacking. Thus, this article aims to establish a Chinese positive emotion database (CPED) to (1) effectively elicit positive emotion categories as many as possible, (2) provide both the subjective feelings of different positive emotions and a corresponding peripheral physiological database, and (3) explore the structure and framework of positive emotion categories. 42 video clips of 16 positive emotion categories were screened from 1000+ online clips. Then a total of 312 participants watched and rated these video clips during which GSR and PPG signals were recorded. 34 video clips that met hit rate and intensity standards were systemically clustered into four emotion categories (empathy, fun, creativity and esteem). Eventually, 22 film clips of these four major categories formed the CPED database. A total of 84 features from GSR and PPG signals were extracted and entered into RF, SVM, DBN and LSTM classifiers that serves as baseline classification methods. A classification accuracy of 44.66 percent for four major categories of positive emotions was achieved.},
  archive  = {J},
  author   = {Yulin Zhang and Guozhen Zhao and Yezhi Shu and Yan Ge and Dan Zhang and Yong-Jin Liu and Xianghong Sun},
  doi      = {10.1109/TAFFC.2021.3088523},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1417-1430},
  title    = {CPED: A chinese positive emotion database for emotion elicitation and analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual learning for joint facial landmark detection and action
unit recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1404–1416. (<a
href="https://doi.org/10.1109/TAFFC.2021.3114158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial landmark detection and action unit (AU) recognition are two essential tasks in facial analysis. Previous works rarely consider the relationship between these complementary tasks. In this article, we introduce a novel multi-task dual learning framework to exploit the relationship between facial landmark detection and AU recognition while simultaneously addressing both tasks. When both tasks share middle-level features, common patterns can be exploited and middle- and high-level features can be used to perform facial landmark detection and AU recognition, respectively. In addition, a dual learning mechanism is designed to convert the predicted landmarks and AUs of the label space to the corresponding facial image of the image space, further exploring the strong correlations between the tasks. By jointly training the proposed method at both the feature and label levels, each task improves the other. Experiments on two benchmark databases demonstrate that the proposed method can leverage dependencies to boost the generalization of both tasks.},
  archive  = {J},
  author   = {Shangfei Wang and Yanan Chang and Can Wang},
  doi      = {10.1109/TAFFC.2021.3114158},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1404-1416},
  title    = {Dual learning for joint facial landmark detection and action unit recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal affective states recognition based on multiscale
CNNs and biologically inspired decision fusion model. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1391–1403. (<a
href="https://doi.org/10.1109/TAFFC.2021.3093923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There has been an encouraging progress in the affective states recognition models based on the single-modality signals as electroencephalogram (EEG) signals or peripheral physiological signals in recent years. However, multimodal physiological signals-based affective states recognition methods have not been thoroughly exploited yet. Here we propose Multiscale Convolutional Neural Networks (Multiscale CNNs) and a biologically inspired decision fusion model for multimodal affective states recognition. First, the raw signals are pre-processed with baseline signals. Then, the High Scale CNN and Low Scale CNN in Multiscale CNNs are utilized to predict the probability of affective states output for EEG and each peripheral physiological signal respectively. Finally, the fusion model calculates the reliability of each single-modality signals by the euclidean distance between various class labels and the classification probability from Multiscale CNNs, and the decision is made by the more reliable modality information while other modalities information is retained. We use this model to classify four affective states from the arousal valence plane in the DEAP and AMIGOS dataset. The results show that the fusion model improves the accuracy of affective states recognition significantly compared with the result on single-modality signals, and the recognition accuracy of the fusion result achieve 98.52 and 99.89 percent in the DEAP and AMIGOS dataset respectively.},
  archive  = {J},
  author   = {Yuxuan Zhao and Xinyan Cao and Jinlong Lin and Dunshan Yu and Xixin Cao},
  doi      = {10.1109/TAFFC.2021.3093923},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1391-1403},
  title    = {Multimodal affective states recognition based on multiscale CNNs and biologically inspired decision fusion model},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying emotional similarity in speech. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1376–1390. (<a
href="https://doi.org/10.1109/TAFFC.2021.3127390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study proposes the novel formulation of measuring emotional similarity between speech recordings. This formulation explores the ordinal nature of emotions by comparing emotional similarities instead of predicting an emotional attribute, or recognizing an emotional category. The proposed task determines which of two alternative samples has the most similar emotional content to the emotion of a given anchor. This task raises some interesting questions. Which is the emotional descriptor that provide the most suitable space to assess emotional similarities? Can deep neural networks (DNNs) learn representations to robustly quantify emotional similarities? We address these questions by exploring alternative emotional spaces created with attribute-based descriptors and categorical emotions. We create the representation using a DNN trained with the triplet loss function, which relies on triplets formed with an anchor, a positive example, and a negative example. We select a positive sample that has similar emotion content to the anchor, and a negative sample that has dissimilar emotion to the anchor. The task of our DNN is to identify the positive sample. The experimental evaluations demonstrate that we can learn a meaningful embedding to assess emotional similarities, achieving higher performance than human evaluators asked to complete the same task.},
  archive  = {J},
  author   = {John Harvill and Seong-Gyun Leem and Mohammed AbdelWahab and Reza Lotfian and Carlos Busso},
  doi      = {10.1109/TAFFC.2021.3127390},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1376-1390},
  title    = {Quantifying emotional similarity in speech},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal sarcasm detection and humor classification in
code-mixed conversations. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1363–1375. (<a
href="https://doi.org/10.1109/TAFFC.2021.3083522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sarcasm detection and humor classification are inherently subtle problems, primarily due to their dependence on the contextual and non-verbal information. Furthermore, existing studies in these two topics are usually constrained in non-English languages such as Hindi, due to the unavailability of qualitative annotated datasets. In this work, we make two major contributions considering the above limitations: (1) we develop a Hindi-English code-mixed dataset, MaSaC , 1 for the multi-modal sarcasm detection and humor classification in conversational dialog, which to our knowledge is the first dataset of its kind; (2) we propose MSH-COMICS , 2 a novel attention-rich neural architecture for the utterance classification. We learn efficient utterance representation utilizing a hierarchical attention mechanism that attends to a small portion of the input sentence at a time. Further, we incorporate dialog-level contextual attention mechanism to leverage the dialog history for the multi-modal classification. We perform extensive experiments for both the tasks by varying multi-modal inputs and various submodules of MSH-COMICS . We also conduct comparative analysis against existing approaches. We observe that MSH-COMICS attains superior performance over the existing models by $&amp;gt;$ 1 F1-score point for the sarcasm detection and 10 F1-score points in humor classification. We diagnose our model and perform thorough analysis of the results to understand the superiority and pitfalls.},
  archive  = {J},
  author   = {Manjot Bedi and Shivani Kumar and Md Shad Akhtar and Tanmoy Chakraborty},
  doi      = {10.1109/TAFFC.2021.3083522},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1363-1375},
  title    = {Multi-modal sarcasm detection and humor classification in code-mixed conversations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). You’re not you when you’re angry: Robust emotion features
emerge by recognizing speakers. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1351–1362. (<a
href="https://doi.org/10.1109/TAFFC.2021.3086050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The robustness of an acoustic emotion recognition system hinges on first having access to features that represent an acoustic input signal. These representations should abstract extraneous low-level variations present in acoustic signals and only capture speaker characteristics relevant for emotion recognition. Previous research has demonstrated that, in other classification tasks, when large labeled datasets are available, neural networks trained on these data learn to extract robust features from the input signal. However, the datasets used for developing emotion recognition systems remain significantly smaller than those used for developing other speech systems. Thus, acoustic emotion recognition systems remain in need of robust feature representations. In this article, we study the utility of speaker embeddings, representations extracted from a trained speaker recognition network, as robust features for detecting emotions. We first study the relationship between emotions and speaker embeddings and demonstrate how speaker embeddings highlight the differences that exist between neutral speech and emotionally expressive speech. We quantify the modulations that variations in emotional expression incur on speaker embeddings and show how these modulations are greater than those incurred from lexical variations in an utterance. Finally, we demonstrate how speaker embeddings can be used as a replacement for traditional low-level acoustic features for emotion recognition.},
  archive  = {J},
  author   = {Zakaria Aldeneh and Emily Mower Provost},
  doi      = {10.1109/TAFFC.2021.3086050},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1351-1362},
  title    = {You&#39;re not you when you&#39;re angry: Robust emotion features emerge by recognizing speakers},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The multimodal sentiment analysis in car reviews (MuSe-CaR)
dataset: Collection, insights and improvements. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1334–1350. (<a
href="https://doi.org/10.1109/TAFFC.2021.3097002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Truly real-life data presents a strong, but exciting challenge for sentiment and emotion research. The high variety of possible ‘in-the-wild’ properties makes large datasets such as these indispensable with respect to building robust machine learning models. A sufficient quantity of data covering a deep variety in the challenges of each modality to force the exploratory analysis of the interplay of all modalities has not yet been made available in this context. In this contribution, we present MuSe-CaR, a first of its kind multimodal dataset. The data is publicly available as it recently served as the testing bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on the tasks of emotion, emotion-target engagement, and trustworthiness recognition by means of comprehensively integrating the audio-visual and language modalities. Furthermore, we give a thorough overview of the dataset in terms of collection and annotation, including annotation tiers not used in this year&#39;s MuSe 2020. In addition, for one of the sub-challenges – predicting the level of trustworthiness – no participant outperformed the baseline model, and so we propose a simple, but highly efficient Multi-Head-Attention network that exceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 percent improvement).},
  archive  = {J},
  author   = {Lukas Stappen and Alice Baird and Lea Schumann and Björn Schuller},
  doi      = {10.1109/TAFFC.2021.3097002},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1334-1350},
  title    = {The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning transferable sparse representations for
cross-corpus facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1322–1333. (<a
href="https://doi.org/10.1109/TAFFC.2021.3077489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An assumption widely used in traditional facial expression recognition algorithms is that the training and testing are conducted on the same dataset. However, this assumption does not hold in practice, in which the training data and testing data are often from different datasets. In this scenario, directly deploying these algorithms would lead to severe information loss and performance degradation due to the domain shift. To address this challenging problem, in this article, we propose a novel transferable sparse subspace representation method (TSSR) for cross-corpus facial expression recognition. Specifically, in order to reduce the cross-corpus mismatch, inspired by sparse subspace clustering, we advocate reconstructing the source and target samples using the source data points based on $\ell _1-$ norm sparse representation. Each data point in source and target corpora can be ideally represented as a combination of a few other source points from its own subspace. Moreover, we take into account the local geometrical information within the cross-corpus data by adopting a graph Laplacian regularizer, which can efficiently preserve the local manifold structure and better transfer knowledge between two corpora. Finally, extensive experiments on several facial expression datasets are conducted to evaluate the recognition performance of TSSR. Experimental results demonstrate the superiority of the proposed method over some state-of-the-art methods.},
  archive  = {J},
  author   = {Dongliang Chen and Peng Song and Wenming Zheng},
  doi      = {10.1109/TAFFC.2021.3077489},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1322-1333},
  title    = {Learning transferable sparse representations for cross-corpus facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning enhanced acoustic latent representation for small
scale affective corpus with adversarial cross corpora integration.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(2),
1308–1321. (<a
href="https://doi.org/10.1109/TAFFC.2021.3126145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Achieving robust cross contexts speech emotion recognition (SER) has become a critical next direction of research for wide adoption of SER technology. The core challenge is in the large variability of affective speech that is highly contextualized. Prior works have worked on this as a transfer learning problem that mostly focuses on developing domain adaptation strategy. However, many of the existing speech emotion corpora, even those considered as large scale, are still limited in size resulting in an unsatisfactory transfer result. On the other hand, directly collecting context-specific corpus often results in an even smaller data size leading to an inevitably non-robust accuracy. In order to mitigate this issue, we propose the concept of enhancing the affect-related variability when learning the in-context acoustic latent representation by integrating out-of-context emotion data. Specifically, we utilize adversarial autoencoder network as our backbone with multiple out-of-context emotion labels derived for each in-context samples that serve as an auxiliary constraint in learning the latent representation. We extensively evaluate our framework using three in-context databases with three out-of-context databases. In this work, we demonstrate not only an improved recognition accuracy but also a comprehensive analysis on the effectiveness of this representation learning strategy.},
  archive  = {J},
  author   = {Chun-Min Chang and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2021.3126145},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1308-1321},
  title    = {Learning enhanced acoustic latent representation for small scale affective corpus with adversarial cross corpora integration},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving textual emotion recognition based on intra- and
inter-class variations. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1297–1307. (<a
href="https://doi.org/10.1109/TAFFC.2021.3104720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual Emotion Recognition (TER) is an important task in Natural Language Processing (NLP), due to its high impact in real-world applications. Prior research has tackled the automatic classification of emotion expressions in text by maximising the probability of the correct emotion class using cross-entropy loss. However, this approach does not account for intra- and inter-class variations within and between emotion classes. To overcome this problem, we introduce a variant of triplet centre loss as an auxiliary task to emotion classification. This allows TER models to learn compact and discriminative features. Furthermore, we introduce a method for evaluating the impact of intra- and inter-class variations on each emotion class. Experiments performed on three datasets demonstrate the effectiveness of our method when applied to each emotion class in comparison to previous approaches. Finally, we present analyses that illustrate the benefits of our method in terms of improving the prediction scores as well as producing discriminative features.},
  archive  = {J},
  author   = {Hassan Alhuzali and Sophia Ananiadou},
  doi      = {10.1109/TAFFC.2021.3104720},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1297-1307},
  title    = {Improving textual emotion recognition based on intra- and inter-class variations},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning users inner thoughts and emotion changes for social
media based suicide risk detection. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1280–1296. (<a
href="https://doi.org/10.1109/TAFFC.2021.3116026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Suicide has become a serious problem, hurting the well-being of human society. Thanks to social media, from people&#39;s linguistic posts, suicide risk detection has achieved good performance. The aim of this article is to investigate whether more significant accuracy could be achieved. Motivated by the observation that the prior solutions strived to detect suicide risk based on users explicit outer post expressions on social media, and no attempt was made to infer users’ inner true thoughts and emotion changes from their normal open posts for suicide risk detection, we propose to first learn the correlations between user&#39;s normal open posts and hidden comments, trying to understand user&#39;s inner true thoughts and emotion changes from the open posts, and then detect user&#39;s suicide risk upon the generated intermediate results. The better detection performance on the microblog dataset (3,652 at-risk microblog users and 3,652 ordinary microblog users) and forum dataset (392 at-risk forum users and 108 ordinary forum users) verifies the insight that it is more effective to learn users’ inner thoughts and emotion changes for social media-based suicide risk detection.},
  archive  = {J},
  author   = {Lei Cao and Huijun Zhang and Xin Wang and Ling Feng},
  doi      = {10.1109/TAFFC.2021.3116026},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1280-1296},
  title    = {Learning users inner thoughts and emotion changes for social media based suicide risk detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact of facial landmark localization on facial expression
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1267–1279. (<a
href="https://doi.org/10.1109/TAFFC.2021.3124142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although facial landmark localization (FLL) approaches are becoming increasingly accurate in identifying facial components, one question remains unanswered: what is the impact of these approaches on subsequent, related tasks? In this paper, we focus on facial expression recognition (FER), where facial landmarks are used for face registration, which is a common usage. Since the common datasets for facial landmark localization do not allow for a proper measurement of performance according to the different difficulties (e.g., pose, expression, illumination, occlusion, motion blur), we also quantify the performance of recent approaches in the presence of head pose variations and facial expressions. Finally, we conduct a study of the impact of these approaches on FER. We show that the landmark accuracy achieved so far by optimizing the euclidean distance does not necessarily guarantee a gain in performance for FER. To deal with this issue, we propose a new evaluation metric for FLL that is more relevant to FER.},
  archive  = {J},
  author   = {Romain Belmonte and Benjamin Allaert and Pierre Tirilly and Ioan Marius Bilasco and Chaabane Djeraba and Nicu Sebe},
  doi      = {10.1109/TAFFC.2021.3124142},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1267-1279},
  title    = {Impact of facial landmark localization on facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collecting mementos: A multimodal dataset for
context-sensitive modeling of affect and memory processing in responses
to videos. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1249–1266. (<a
href="https://doi.org/10.1109/TAFFC.2021.3089584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article we introduce Mementos : the first multimodal corpus for computational modeling of affect and memory processing in response to video content. It was collected online via crowdsourcing and captures 1995 individual responses collected from 297 unique viewers responding to 42 different segments of music videos. Apart from webcam recordings of their upper-body behavior (totaling 2012 minutes) and self-reports of their emotional experience, it contains detailed descriptions of the occurrence and content of 989 personal memories triggered by the video content. Finally, the dataset includes self-report measures related to individual differences in participants’ background and situation ( Demographics , Personality , and Mood ), thereby facilitating the exploration of important contextual factors in research using the dataset. We describe 1) the construction and contents of the corpus itself, 2) analyse the validity of its content by investigating biases and consistency with existing research on affect and memory processing, 3) review previously published work that demonstrates the usefulness of the multimodal data in the corpus for research on automated detection and prediction tasks, and 4) provide suggestions for how the dataset can be used in future research on modeling Video-Induced Emotions , Memory-Associated Affect , and Memory Evocation .},
  archive  = {J},
  author   = {Bernd Dudzik and Hayley Hung and Mark Neerincx and Joost Broekens},
  doi      = {10.1109/TAFFC.2021.3089584},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1249-1266},
  title    = {Collecting mementos: A multimodal dataset for context-sensitive modeling of affect and memory processing in responses to videos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition with visual transformers and
attentional selective fusion. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1236–1248. (<a
href="https://doi.org/10.1109/TAFFC.2021.3122146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.},
  archive  = {J},
  author   = {Fuyan Ma and Bin Sun and Shutao Li},
  doi      = {10.1109/TAFFC.2021.3122146},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1236-1248},
  title    = {Facial expression recognition with visual transformers and attentional selective fusion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation via face morphing for recognizing
intensities of facial emotions. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1228–1235. (<a
href="https://doi.org/10.1109/TAFFC.2021.3096922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Being able to recognize emotional intensity is a desirable feature for a facial emotional recognition (FER) system. However, the development of such a feature is hindered by the paucity of intensity-labeled data for model training. To ameliorate the situation, the present study proposes using face morphing as a novel way of data augmentation to synthesize faces that express different degrees of a designated emotion. Such an approach has been successfully validated on humans and machines. Specifically, humans indeed perceived different levels of intensified emotions in these parametrically synthesized faces, and FER systems based on neural networks indeed showed improved sensitivities to intensities of different emotions when additionally trained on the synthesized faces. Overall, the proposed data augmentation method is not only simple and effective but also useful for building FER systems that recognize facial expressions of mixed emotions.},
  archive  = {J},
  author   = {Tsung-Ren Huang and Shin-Min Hsu and Li-Chen Fu},
  doi      = {10.1109/TAFFC.2021.3096922},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1228-1235},
  title    = {Data augmentation via face morphing for recognizing intensities of facial emotions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chunk-level speech emotion recognition: A general framework
of sequence-to-one dynamic temporal modeling. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1215–1227. (<a
href="https://doi.org/10.1109/TAFFC.2021.3083821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A critical issue of current speech-based sequence-to-one learning tasks, such as speech emotion recognition (SER), is the dynamic temporal modeling for speech sentences with different durations. The goal is to extract an informative representation vector of the sentence from acoustic feature sequences with varied length. Traditional methods rely on static descriptions such as statistical functions or a universal background model (UBM), which are not capable of characterizing dynamic temporal changes. Recent advances in deep learning architectures provide promising results, directly extracting sentence-level representations from frame-level features. However, conventional cropping and padding techniques that deal with varied length sequences are not optimal, since they truncate or artificially add sentence-level information. Therefore, we propose a novel dynamic chunking approach, which maps the original sequences of different lengths into a fixed number of chunks that have the same duration by adjusting their overlap. This simple chunking procedure creates a flexible framework that can incorporate different feature extractions and sentence-level temporal aggregation approaches to cope, in a principled way, with different sequence-to-one tasks. Our experimental results based on three databases demonstrate that the proposed framework provides: 1) improvement in recognition accuracy, 2) robustness toward different temporal length predictions, and 3) high model computational efficiency advantages.},
  archive  = {J},
  author   = {Wei-Cheng Lin and Carlos Busso},
  doi      = {10.1109/TAFFC.2021.3083821},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1215-1227},
  title    = {Chunk-level speech emotion recognition: A general framework of sequence-to-one dynamic temporal modeling},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Werewolf-XL: A database for identifying spontaneous affect
in large competitive group interactions. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1201–1214. (<a
href="https://doi.org/10.1109/TAFFC.2021.3101563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective computing and natural human-computer interaction, which would be capable of interpreting and responding intelligently to the social cues of interaction in crowds, are more needed than ever as an individual&#39;s affective experience is often related to others in group activities. To develop the next-generation intelligent interactive systems, we require numerous human facial expressions with accurate annotations. However, existing databases usually consider nonspontaneous human behavior (posed or induced), individual or dyadic setting, and a single type of emotion annotation. To address this need, we created the Werewolf-XL database, which contains a total of 890 minutes of spontaneous audio-visual recordings of 129 subjects in a group interaction of nine individuals playing a conversational role-playing game called Werewolf. We provide 131,688 individual utterance-level video clips with internal self-assessment of 18 non-prototypical emotional categories and external assessment of pleasure, arousal, and dominance, including 14,632 speakers&#39; samples and the rest of listeners&#39; samples. Besides, the results of the annotation agreement analysis show fair reliability and validity. Role information and outcomes of the game are also recorded. Furthermore, we provided extensive benchmarks of unimodal and multimodal emotional recognition results. The database is made publicly available.},
  archive  = {J},
  author   = {Kejun Zhang and Xinda Wu and Xinhang Xie and Xiaoran Zhang and Hui Zhang and Xiaoyu Chen and Lingyun Sun},
  doi      = {10.1109/TAFFC.2021.3101563},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1201-1214},
  title    = {Werewolf-XL: A database for identifying spontaneous affect in large competitive group interactions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic estimation of action unit intensities and
inference of emotional appraisals. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1188–1200. (<a
href="https://doi.org/10.1109/TAFFC.2021.3077590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The development of a two-stage approach for appraisal inference from automatically detected Action Unit (AU) intensities in recordings of human faces is described. AU intensity estimation is based on a hybrid approach fusing information from an individually fitted mesh model of the faces and texture information. Evaluation results for two datasets and a comparison against a state-of-the-art system, namely OpenFace are provided. In the second stage, the emotional appraisals novelty, valence and control are predicted from estimated AU intensities by linear regressions. Prediction performance is evaluated based on face recordings from a market research study, which were rated by human observers in terms of perceived appraisals. Predictions of valence and control from automatically estimated AU intensities closely match those obtained from manually coded AUs in terms of agreement with human observers, while novelty predictions lag somewhat behind. Overall, results highlight the flexibility and interpretability of a two-stage approach to emotion inference.},
  archive  = {J},
  author   = {Dominik Seuss and Teena Hassan and Anja Dieckmann and Matthias Unfried and Klaus R. Scherer and Marcello Mortillaro and Jens Garbas},
  doi      = {10.1109/TAFFC.2021.3077590},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1188-1200},
  title    = {Automatic estimation of action unit intensities and inference of emotional appraisals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Looking at the body: Automatic analysis of body gestures and
self-adaptors in psychological distress. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1175–1187. (<a
href="https://doi.org/10.1109/TAFFC.2021.3101698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Psychological distress is a significant and growing issue in society. In particular, depression and anxiety are leading causes of disability that often go undetected or late-diagnosed. Automatic detection, assessment, and analysis of behavioural markers of psychological distress can help improve identification and support prevention and early intervention efforts. Compared to modalities such as face, head, and vocal, research investigating the use of the body modality for these tasks is relatively sparse, which is partly due to the limited available datasets and difficulty in automatically extracting useful body features. To enable our research, we have collected and analyzed a new dataset containing full body videos for interviews and self-reported distress labels. We propose a novel approach to automatically detect self-adaptors and fidgeting, a subset of self-adaptors that has been shown to correlate with psychological distress. We perform analysis on statistical body gestures and fidgeting features to explore how distress levels affect behaviors. We then propose a multi-modal approach that combines different feature representations using Multi-modal Deep Denoising Auto-Encoders and Improved Fisher Vector Encoding. We demonstrate that our proposed model, combining audio-visual features with detected fidgeting behavioral cues, can successfully predict depression and anxiety in the dataset.},
  archive  = {J},
  author   = {Weizhe Lin and Indigo Orton and Qingbiao Li and Gabriela Pavarini and Marwa Mahmoud},
  doi      = {10.1109/TAFFC.2021.3101698},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1175-1187},
  title    = {Looking at the body: Automatic analysis of body gestures and self-adaptors in psychological distress},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Geometry-aware facial expression recognition via attentive
graph convolutional networks. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1159–1174. (<a
href="https://doi.org/10.1109/TAFFC.2021.3088895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Learning discriminative representations with good robustness from facial observations serves as a fundamental step towards intelligent facial expression recognition (FER). In this article, we propose a novel geometry-aware FER framework to boost the FER performance based on both the geometric and appearance knowledge. Specifically, we propose an encoding strategy for facial landmarks, and adopt a graph convolutional network (GCN) to fully explore the structural information of the facial components behind different expressions. A convolutional neural network (CNN) is further applied to the whole facial observation to learn the global characteristics of different expressions. The features from these two networks are fused into a comprehensive high-semantic representation, which promotes the FER reasoning from both visual and structural perspectives. Moreover, to facilitate the networks to concentrate on the most informative facial regions and components, we introduce multi-level attention mechanisms into the proposed framework, which enhance the reliability of the learned representations for effective FER. Experiments on two challenging FER benchmarks demonstrate that the attentive graph-based learning on the facial geometry boosts the FER accuracy. Furthermore, the insensitivity of the geometric information to the appearance variations also improves the generalization of the proposed framework.},
  archive  = {J},
  author   = {Rui Zhao and Tianshan Liu and Zixun Huang and Daniel P.K. Lun and Kin-Man Lam},
  doi      = {10.1109/TAFFC.2021.3088895},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1159-1174},
  title    = {Geometry-aware facial expression recognition via attentive graph convolutional networks},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep siamese neural networks for facial expression
recognition in the wild. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1148–1158. (<a
href="https://doi.org/10.1109/TAFFC.2021.3077248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article introduces an algorithm for facial expression recognition (FER) using deep Siamese Neural Networks (SNNs) that preserve the local structure of images in the embedding similarity space. We designed the network to reveal the input pairs similarity by comparing features through a designed metric. Furthermore, we developed a novel image pairing (i.e., positive and negative pairs) strategy technique to train our Siamese model. Our Siamese model comprises of a verification framework and an identification framework to learn a joint embedding space. The verification path reduces the intra-class variations by minimizing the distance between the extracted features from the same class, while the identification path increases the inter-class variations by maximizing the distance between the features extracted from different classes. We apply transfer learning to only use the identification model for facial expression classification. We evaluated our algorithm using AffectNet, FER2013, and Compound Facial Expressions of Emotion (CFEE) datasets, where better results are achieved compared to other deep learning-based approaches.},
  archive  = {J},
  author   = {Wassan Hayale and Pooran Singh Negi and Mohammad H. Mahoor},
  doi      = {10.1109/TAFFC.2021.3077248},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1148-1158},
  title    = {Deep siamese neural networks for facial expression recognition in the wild},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The rhythm of flow: Detecting facial expressions of flow
experiences using CNNs. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1138–1147. (<a
href="https://doi.org/10.1109/TAFFC.2021.3087222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, the flow state, a state in which individuals perform at the peak of their ability and are completely immersed in the task while experiencing a state of elatedness, has been the subject of active research. We introduce a novel approach of using convolutional neural networks to recognize flow in live performing musicians from analyzing their facial expression. A modified and partially re-trained version of the popular ResNet-50 architecture is employed for binary classification of flow, achieving a detection accuracy of 77.55 percent. This is done on labelled YouTube video-data of musicians with a labeling strategy that was verified through a perception experiment. Maximum accuracy within a 5-fold cross-validation is 74.98 percent with the mean exhibiting an accuracy of 65.10 percent. The results indicate that the state of flow is indeed recognizable through facial expressions of musicians. In addition, the utility of the presented model is demonstrated in two exemplary applications: Predicting the popularity of YouTube videos based on flow recognized in the faces through our system and correlating flow and six discrete emotions (neutral, happy, angry, fear, disgust, surprise).},
  archive  = {J},
  author   = {Lukas Humpe and Simon Friedrich Murillo and Janik Muires and Peter Gloor},
  doi      = {10.1109/TAFFC.2021.3087222},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1138-1147},
  title    = {The rhythm of flow: Detecting facial expressions of flow experiences using CNNs},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Typical facial expression network using a facial feature
decoupler and spatial-temporal learning. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1125–1137. (<a
href="https://doi.org/10.1109/TAFFC.2021.3102245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) accuracy is often affected by an individual’s unique facial characteristics. Recognition performance can be improved if the influence from these physical characteristics is minimized. Using video instead of single image for FER provides better results but requires extracting temporal features and the spatial structure of facial expressions in an integrated manner. We propose a new network called Typical Facial Expression Network (TFEN) to address both challenges. TFEN uses two deep two-dimensional (2D) convolutional neural networks (CNNs) to extract facial and expression features from input video. A facial feature decoupler decouples facial features from expression features to minimize the influence from inter-subject face variations. These networks combine with a 3D CNN and form a spatial-temporal learning network to jointly explore the spatial-temporal features in a video. A facial recognition network works as an adversarial network to refine the facial feature decoupler and the network performance by minimizing the residual influence of facial features after decoupling. The whole network is trained with an adversarial algorithm to improve FER performance. TFEN was evaluated on four popular dynamic FER datasets. Experimental results show TFEN achieves or outperforms the recognition accuracy of state-of-the-art approaches.},
  archive  = {J},
  author   = {Jianing Teng and Dong Zhang and Wei Zou and Ming Li and Dah-Jye Lee},
  doi      = {10.1109/TAFFC.2021.3102245},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1125-1137},
  title    = {Typical facial expression network using a facial feature decoupler and spatial-temporal learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative few shot learning of facial dynamics in
interview videos for autism trait classification. <em>IEEE Transactions
on Affective Computing</em>, <em>14</em>(2), 1110–1124. (<a
href="https://doi.org/10.1109/TAFFC.2022.3178946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autism is a prevalent neurodevelopmental disorder characterized by impairments in social and communicative behaviors. Possible connections between autism and facial expression recognition have recently been studied in the literature. However, most works are based on facial images or short videos. Few works aim at Autism Diagnostic Observation Schedule (ADOS) videos due to their complexity (e.g., interaction between interviewer and interviewee) and length (e.g., usually last for hours). In this paper, we attempt to fill this gap by developing a novel discriminative few shot learning method to analyze hour-long video data and exploring the fusion of facial dynamics for the trait classification of ASD. Leveraging well-established computer vision tools from spatio-temporal feature extraction and marginal fisher analysis to few-shot learning and scene-level fusion, we have constructed a three-category system to classify an individual into Autism, Autism Spectrum, and Non-Spectrum. For the first time, we have shown that certain interview scenes carry more discriminative information for ASD trait classification than others. Experimental results are reported to demonstrate the potential of the proposed automatic ASD trait classification system (achieving 91.72% accuracy on the Caltech ADOS video dataset) and the benefits of few-shot learning and scene-level fusion strategy by extensive ablation studies.},
  archive  = {J},
  author   = {Na Zhang and Mindi Ruan and Shuo Wang and Lynn Paul and Xin Li},
  doi      = {10.1109/TAFFC.2022.3178946},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1110-1124},
  title    = {Discriminative few shot learning of facial dynamics in interview videos for autism trait classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enforcing semantic consistency for cross corpus emotion
prediction using adversarial discrepancy learning in emotion. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1098–1109. (<a
href="https://doi.org/10.1109/TAFFC.2021.3111110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mismatch between databases entails a challenge in performing emotion recognition on a practical-condition unlabeled database with labeled source data. The alignment between the source and target is crucial for conventional neural network; therefore, many studies have mapped two domains in a common feature space. However, the effect of distortion in emotion semantics across different conditions has been neglected in such work, and a sample from the target may be considered a high emotional annotation in the target but as low in the source. In this article, we propose the maximum regression discrepancy (MRD) network, which enforces semantic consistency in a source and target by adjusting the acoustic feature encoder to minimize discrepancy in maximally distorted samples through adversarial training. We show our framework in several experiments using three databases (the USC IEMOCAP, MSP-Improv, and MSP-Podcast) for cross corpus emotion prediction. Compared to the Source-only neural network and DANN, MRD network demonstrates a significant improvement between 5% and 10% in the concordance correlation coefficient (CCC) in cross-corpus prediction and between 3% and 10% for evaluation on MSP-PODCAST. We also visualize the effect of MRD on feature representation to shows the efficacy of the MRD structure we designed.},
  archive  = {J},
  author   = {Chun-Min Chang and Gao-Yi Chao and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2021.3111110},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1098-1109},
  title    = {Enforcing semantic consistency for cross corpus emotion prediction using adversarial discrepancy learning in emotion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavioral and physiological signals-based deep multimodal
approach for mobile emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 1082–1097. (<a
href="https://doi.org/10.1109/TAFFC.2021.3100868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rapid development of mobile and wearable devices, it is increasingly possible to access users’ affective data in a more unobtrusive manner. On this basis, researchers have proposed various systems to recognize user’s emotional states. However, most of these studies rely on traditional machine learning techniques and a limited number of signals, leading to systems that either do not generalize well or would frequently lack sufficient information for emotion detection in realistic scenarios. In this paper, we propose a novel attention-based LSTM system that uses a combination of sensors from a smartphone (front camera, microphone, touch panel) and a wristband (photoplethysmography, electrodermal activity, and infrared thermopile sensor) to accurately determine user’s emotional states. We evaluated the proposed system by conducting a user study with 45 participants. Using collected behavioral (facial expression, speech, keystroke) and physiological (blood volume, electrodermal activity, skin temperature) affective responses induced by visual stimuli, our system was able to achieve an average accuracy of 89.2 percent for binary positive and negative emotion classification under leave-one-participant-out cross-validation. Furthermore, we investigated the effectiveness of different combinations of data signals to cover different scenarios of signal availability.},
  archive  = {J},
  author   = {Kangning Yang and Chaofan Wang and Yue Gu and Zhanna Sarsenbayeva and Benjamin Tag and Tilman Dingler and Greg Wadley and Jorge Goncalves},
  doi      = {10.1109/TAFFC.2021.3100868},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1082-1097},
  title    = {Behavioral and physiological signals-based deep multimodal approach for mobile emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling multiple temporal scales of full-body movements for
emotion classification. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 1070–1081. (<a
href="https://doi.org/10.1109/TAFFC.2021.3095425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article investigates classification of emotions from full-body movements by using a novel Convolutional Neural Network-based architecture. The model is composed of two shallow networks processing in parallel when the 8-bit RGB images obtained from time intervals of 3D-positional data are the inputs. One network performs a coarse-grained modelling in the time domain while the other one applies a fine-grained modelling. We show that combining different temporal scales into a single architecture improves the classification results of a dataset composed of short excerpts of the performances of professional dancers who interpreted four affective states: anger, happiness, sadness, and insecurity. Additionally, we investigate the effect of data chunk duration, overlapping, the size of the input images and the contribution of several data augmentation strategies for our proposed method. Better recognition results were obtained when the duration of a data chunk was longer, and this was further improved by applying balanced data augmentation. Moreover, we test our method on other existing motion capture datasets and compare the results with prior art. In all experiments, our results surpassed the state-of-the-art approaches, showing that this method generalizes across diverse settings and contexts.},
  archive  = {J},
  author   = {Cigdem Beyan and Sukumar Karumuri and Gualtiero Volpe and Antonio Camurri and Radoslaw Niewiadomski},
  doi      = {10.1109/TAFFC.2021.3095425},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1070-1081},
  title    = {Modeling multiple temporal scales of full-body movements for emotion classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio-visual automatic group affect analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1056–1069. (<a
href="https://doi.org/10.1109/TAFFC.2021.3104170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective computing has progressed well due to methods, which can identify a person’s posed and spontaneous perceived affect with high accuracy. This paper focuses on group-level affect analysis on videos, which is one of the first few multimodal group-level affect analysis studies. There are many challenges on video-based group-level affect analysis as most of the work is focused on either a single person&#39;s affect recognition or image-based group affect analysis. To address this, first, we present an audio-visual perceived group affect dataset - ‘Video-level Group AFfect (VGAF)’. VGAF is a large-scale dataset consisting of 4,183 group videos. The videos are collected from YouTube with large variations in the keywords for collecting data across different genders, group settings, group sizes, illuminations and poses. The variety within the dataset will help the study of perception of group affect in a real environment. The data is manually annotated for three group affect classes - positive, neutral, and negative. Further, a fusion based audio-visual method is proposed to set a benchmark performance on the proposed dataset. The experimental results show the effectiveness of facial, holistic and speech features for group-level affect analysis. The baseline code, dataset, and pre-trained models are available at [LINK].},
  archive  = {J},
  author   = {Garima Sharma and Abhinav Dhall and Jianfei Cai},
  doi      = {10.1109/TAFFC.2021.3104170},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1056-1069},
  title    = {Audio-visual automatic group affect analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Receiving a mediated touch from your partner vs. A male
stranger: How visual feedback of touch and its sender influence touch
experience. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1044–1055. (<a
href="https://doi.org/10.1109/TAFFC.2021.3085185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social touch is essential to human development and communication. Mediated social touch is suggested as a solution for circumstances where distance prevents skin-to-skin contact. However, past research aimed at demonstrating efficacy of mediated touch in reducing stress and promoting helping have produced mixed findings. These inconsistent findings could possibly be due to insufficient control of contextual factors combined with unnatural interaction scenarios. For example, touch occurs less frequently among strangers and is often accompanied with nonverbal visual cues. We investigated how visual presentation of touch, and interpersonal relationship to the sender influence perception, affective experiences, and autonomic responses the touch evoke. Fifty couples of mixed gender were recruited. A mediated touch was repeatedly applied by either the male partner or male confederate to female participants. The latter witnessed through a webcam as the sender caressed a rubber hand or touchpad to send the touch. Following our hypotheses, touch sent by one&#39;s partner was perceived softer and more comforting than stranger touch. The partner&#39;s touch also resulted in weaker skin conductance responses, particularly when sent by touching a touchpad. In sum, how a mediated touch is experienced depends both on who is touching, and on how the touch is visually represented.},
  archive  = {J},
  author   = {Sima Ipakchian Askari and Ville J. Harjunen and Michiel M. Spapé and Antal Haans and Niklas Ravaja and Wijnand A. IJsselsteijn},
  doi      = {10.1109/TAFFC.2021.3085185},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1044-1055},
  title    = {Receiving a mediated touch from your partner vs. a male stranger: How visual feedback of touch and its sender influence touch experience},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DBATES: Dataset for discerning benefits of audio, textual,
and facial expression features in competitive debate speeches. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 1028–1043. (<a
href="https://doi.org/10.1109/TAFFC.2021.3103442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we present a database of multimodal communication features extracted from debate speeches in the 2019 North American Universities Debate Championships (NAUDC). Feature sets were extracted from the visual (facial expression, gaze, and head pose), audio (PRAAT), and textual (word sentiment and linguistic category) modalities of raw video recordings of competitive collegiate debaters (N=716 6-minute recordings from 140 unique debaters). Each speech has an associated competition debate score (range: 67-96) from experienced judges as well as competitor demographic and per-round reflection surveys. We observe the fully multimodal model performs best in comparison to models trained on various compositions of individual modalities. We also find that the weights of some features (such as the expression of joy and the use of the word ”we”) change in direction between the aforementioned models. We use these results to highlight the value of a multimodal dataset for studying competitive, collegiate debate.},
  archive  = {J},
  author   = {Taylan K. Sen and Gazi Naven and Luke Gerstner and Daryl Bagley and Raiyan Abdul Baten and Wasifur Rahman and Md Kamrul Hasan and Kurtis Haut and Abdullah Al Mamun and Samiha Samrose and Anne Solbu and R. Eric Barnes and Mark G. Frank and Ehsan Hoque},
  doi      = {10.1109/TAFFC.2021.3103442},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1028-1043},
  title    = {DBATES: Dataset for discerning benefits of audio, textual, and facial expression features in competitive debate speeches},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal engagement analysis from facial videos in the
classroom. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 1012–1027. (<a
href="https://doi.org/10.1109/TAFFC.2021.3127692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Student engagement is a key component of learning and teaching, resulting in a plethora of automated methods to measure it. Whereas most of the literature explores student engagement analysis using computer-based learning often in the lab, we focus on using classroom instruction in authentic learning environments. We collected audiovisual recordings of secondary school classes over a one and a half month period, acquired continuous engagement labeling per student (N=15) in repeated sessions, and explored computer vision methods to classify engagement from facial videos. We learned deep embeddings for attentional and affective features by training Attention-Net for head pose estimation and Affect-Net for facial expression recognition using previously-collected large-scale datasets. We used these representations to train engagement classifiers on our data, in individual and multiple channel settings, considering temporal dependencies. The best performing engagement classifiers achieved student-independent AUCs of .620 and .720 for grades 8 and 12, respectively, with attention-based features outperforming affective features. Score-level fusion either improved the engagement classifiers or was on par with the best performing modality. We also investigated the effect of personalization and found that only 60 seconds of person-specific data, selected by margin uncertainty of the base classifier, yielded an average AUC improvement of .084.},
  archive  = {J},
  author   = {Ömer Sümer and Patricia Goldberg and Sidney D’Mello and Peter Gerjets and Ulrich Trautwein and Enkelejda Kasneci},
  doi      = {10.1109/TAFFC.2021.3127692},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {1012-1027},
  title    = {Multimodal engagement analysis from facial videos in the classroom},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neurofeedback training with an electroencephalogram-based
brain-computer interface enhances emotion regulation. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 998–1011. (<a
href="https://doi.org/10.1109/TAFFC.2021.3134183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion regulation plays a vital role in human beings daily lives by helping them deal with social problems and protects mental and physical health. However, objective evaluation of the efficacy of emotion regulation and assessment of the improvement in emotion regulation ability at the individual level remain challenging. In this study, we leveraged neurofeedback training to design a real-time EEG-based brain-computer interface (BCI) system for users to effectively regulate their emotions. Twenty healthy subjects performed 10 BCI-based neurofeedback training sessions to regulate their emotion towards a specific emotional state (positive, negative, or neutral), while their EEG signals were analyzed in real time via machine learning to predict their emotional states. The prediction results were presented as feedback on the screen to inform the subjects of their immediate emotional state, based on which the subjects could update their strategies for emotion regulation. The experimental results indicated that the subjects improved their ability to regulate these emotions through our BCI neurofeedback training. Further EEG-based spectrum analysis revealed how each emotional state was related to specific EEG patterns, which were progressively enhanced through long-term training. These results together suggested that long-term EEG-based neurofeedback training could be a promising tool for helping people with emotional or mental disorders.},
  archive  = {J},
  author   = {Weichen Huang and Wei Wu and Molly V. Lucas and Haiyun Huang and Zhenfu Wen and Yuanqing Li},
  doi      = {10.1109/TAFFC.2021.3134183},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {998-1011},
  title    = {Neurofeedback training with an electroencephalogram-based brain-computer interface enhances emotion regulation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distilling region-wise and channel-wise deep structural
facial relationships for FAU (DSR-FAU) intensity estimation. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 986–997. (<a
href="https://doi.org/10.1109/TAFFC.2021.3129065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial emotions are expressed through a combination of facial muscle movements, namely, the Facial Action Units (FAUs). FAU intensity estimation aims to estimate the intensity of a set of structurally dependent FAUs. Contrary to the existing works that focus on improving FAU intensity estimation performance, this study investigates how knowledge distillation (KD) incorporated into a training model can improve FAU intensity estimation efficiency while achieving the comparable level of performance. Given the intrinsic structural characteristics of FAU, it is desirable to distill deep structural relationships, namely, DSR-FAU, using heatmap regression. Our methodology is as follows: First, a feature map-level distillation loss is applied to ensure that the student network and the teacher network share similar feature distributions. Second, the region-wise and channel-wise relationship distillation loss functions are introduced to penalize the difference in structural relationships. Specifically, the region-wise relationship can be represented by the structural correlations across the facial features, whereas the channel-wise relationship is represented by the implicit FAU co-occurrence dependencies. Third, we compare the model performance of DSR-FAU with the state-of-the-art models, based on two benchmarking datasets. It is shown that our model achieves comparable performance, with a lower number of model parameters and lower computation complexities.},
  archive  = {J},
  author   = {Yingruo Fan and Jacqueline C.K. Lam and Victor O.K. Li},
  doi      = {10.1109/TAFFC.2021.3129065},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {986-997},
  title    = {Distilling region-wise and channel-wise deep structural facial relationships for FAU (DSR-FAU) intensity estimation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal hierarchical attention neural network: Looking
for candidates behaviour which impact recruiter’s decision. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 969–985. (<a
href="https://doi.org/10.1109/TAFFC.2021.3113159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic analysis of job interviews has gained in interest amongst academic and industrial research. The particular case of asynchronous video interviews allows to collect vast corpora of videos where candidates answer standardized questions in monologue videos, enabling the use of deep learning algorithms. On the other hand, state-of-the-art approaches still face some obstacles, among which the fusion of information from multiple modalities and the interpretability of the predictions. We study the task of predicting candidates performance in asynchronous video interviews using three modalities (verbal content, prosody and facial expressions) independently or simultaneously, using data from real interviews which take place in real conditions. We propose a sequential and multimodal deep neural network model, called Multimodal HireNet. We compare this model to state-of-the-art approaches and show a clear improvement of the performance. Moreover, the architecture we propose is based on attention mechanism, which provides interpretability about which questions, moments and modalities contribute the most to the output of the network. While other deep learning systems use attention mechanisms to offer a visualization of moments with attention values, the proposed methodology enables an in-depth interpretation of the predictions by an overall analysis of the features of social signals contained in these moments.},
  archive  = {J},
  author   = {Léo Hemamou and Arthur Guillon and Jean-Claude Martin and Chloé Clavel},
  doi      = {10.1109/TAFFC.2021.3113159},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {969-985},
  title    = {Multimodal hierarchical attention neural network: Looking for candidates behaviour which impact recruiter&#39;s decision},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based emotion recognition via neural architecture
search. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 957–968. (<a
href="https://doi.org/10.1109/TAFFC.2021.3130387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the flourishing development of deep learning (DL) and the convolution neural network (CNN), electroencephalogram-based (EEG) emotion recognition is occupying an increasingly crucial part in the field of brain-computer interface (BCI). However, currently employed architectures have mostly been designed manually by human experts, which is a time-consuming and labor-intensive process. In this paper, we proposed a novel neural architecture search (NAS) framework based on reinforcement learning (RL) for EEG-based emotion recognition, which can automatically design network architectures. The proposed NAS mainly contains three parts: search strategy, search space, and evaluation strategy. During the search process, a recurrent network (RNN) controller is used to select the optimal network structure in the search space. We trained the controller with RL to maximize the expected reward of the generated models on a validation set and force parameter sharing among the models. We evaluated the performance of NAS on the DEAP and DREAMER dataset. On the DEAP dataset, the average accuracies reached 97.94%, 97.74%, and 97.82% on arousal, valence, and dominance respectively. On the DREAMER dataset, average accuracies reached 96.62%, 96.29% and 96.61% on arousal, valence, and dominance, respectively. The experimental results demonstrated that the proposed NAS outperforms the state-of-the-art CNN-based methods.},
  archive  = {J},
  author   = {Chang Li and Zhongzhen Zhang and Rencheng Song and Juan Cheng and Yu Liu and Xun Chen},
  doi      = {10.1109/TAFFC.2021.3130387},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {957-968},
  title    = {EEG-based emotion recognition via neural architecture search},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep multimodal learning approach to perceive basic needs
of humans from instagram profile. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 944–956. (<a
href="https://doi.org/10.1109/TAFFC.2021.3090809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, a significant part of our time is spent sharing multimodal data on social media sites such as Instagram, Facebook and Twitter. The particular way through which users present themselves to social media can provide useful insights into their behaviours, personalities, perspectives, motives and needs. This article proposes to use multimodal data collected from Instagram accounts to predict the five basic prototypical needs described in Glasser&#39;s choice theory (i.e., Survival , Power , Freedom , Belonging , and Fun ). We automate the identification of the unconsciously perceived needs from Instagram profiles by using both visual and textual contents. The proposed approach aggregates the visual and textual features extracted using deep learning and constructs a homogeneous representation for each profile through the proposed Bag-of-Content . Finally, we perform multi-label classification on the fusion of both modalities. We validate our proposal on a large database, consensually annotated by two expert psychologists, with more than 30,000 images, captions and comments. Experiments show promising accuracy and complementary information between visual and textual cues.},
  archive  = {J},
  author   = {Mohammad Mahdi Dehshibi and Bita Baiani and Gerard Pons and David Masip},
  doi      = {10.1109/TAFFC.2021.3090809},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {944-956},
  title    = {A deep multimodal learning approach to perceive basic needs of humans from instagram profile},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A media-guided attentive graphical network for personality
recognition using physiology. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 931–943. (<a
href="https://doi.org/10.1109/TAFFC.2021.3090040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Physiological automatic personality recognition has been largely developed to model an individual’s personality trait from a variety of signals. However, few studies have tackled the problems of integration methodology from multiple observations into a single personality prediction. In this study, we focus on finding a novel learning architecture to model the personality trait under a Many-to-One scenario. We propose to integrate not only the information on the user but also consider the effect of the affective multimedia stimulus. Specifically, we present a novel Acoustic-Visual Guided Attentive Graph Convolutional Network for enhanced personality recognition. The emotional multimedia content guides the formation of the physiological responses into a graph-like structure to integrate latent inter-correlation among all responses toward affective multimedia. Then these graphs would be further processed by the Graph Convolutional Network (GCN) to jointly model instances and inter-correlation levels of the subject’s responses. We show that our model outperforms the current state of the art on two large public corpora for personality recognition. Further analysis reveals that there indeed exists a multimedia preference for inferring personality from physiology, and several frequency-domain descriptors in ECG and the tonic component in EDA are shown to be robust for automatic personality recognition.},
  archive  = {J},
  author   = {Hao-Chun Yang and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2021.3090040},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {931-943},
  title    = {A media-guided attentive graphical network for personality recognition using physiology},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring complexity of facial dynamics in autism spectrum
disorder. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 919–930. (<a
href="https://doi.org/10.1109/TAFFC.2021.3113876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Atypical facial expression is one of the early symptoms of autism spectrum disorder (ASD) characterized by reduced regularity and lack of coordination of facial movements. Automatic quantification of these behaviors can offer novel biomarkers for screening, diagnosis, and treatment monitoring of ASD. In this work, 40 toddlers with ASD and 396 typically developing toddlers were shown developmentally-appropriate and engaging movies presented on a smart tablet during a well-child pediatric visit. The movies consisted of social and non-social dynamic scenes designed to evoke certain behavioral and affective responses. The front-facing camera of the tablet was used to capture the toddlers’ face. Facial landmarks’ dynamics were then automatically computed using computer vision algorithms. Subsequently, the complexity of the landmarks’ dynamics was estimated for the eyebrows and mouth regions using multiscale entropy. Compared to typically developing toddlers, toddlers with ASD showed higher complexity (i.e., less predictability) in these landmarks’ dynamics. This complexity in facial dynamics contained novel information not captured by traditional facial affect analyses. These results suggest that computer vision analysis of facial landmark movements is a promising approach for detecting and quantifying early behavioral symptoms associated with ASD.},
  archive  = {J},
  author   = {Pradeep Raj Krishnappa Babu and J. Matias Di Martino and Zhuoqing Chang and Sam Perochon and Kimberly L. H. Carpenter and Scott Compton and Steven Espinosa and Geraldine Dawson and Guillermo Sapiro},
  doi      = {10.1109/TAFFC.2021.3113876},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {919-930},
  title    = {Exploring complexity of facial dynamics in autism spectrum disorder},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating multisensory integration in emotion
recognition through bio-inspired computational models. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(2), 906–918. (<a
href="https://doi.org/10.1109/TAFFC.2021.3106254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion understanding represents a core aspect of human communication. Our social behaviours are closely linked to expressing our emotions and understanding others’ emotional and mental states through social signals. The majority of the existing work proceeds by extracting meaningful features from each modality and applying fusion techniques either at a feature level or decision level. However, these techniques are incapable of translating the constant talk and feedback between different modalities. Such constant talk is particularly important in continuous emotion recognition, where one modality can predict, enhance and complement the other. This article proposes three multisensory integration models, based on different pathways of multisensory integration in the brain; that is, integration by convergence, early cross-modal enhancement, and integration through neural synchrony. The proposed models are designed and implemented using third-generation neural networks, Spiking Neural Networks (SNN). The models are evaluated using widely adopted, third-party datasets and compared to state-of-the-art multimodal fusion techniques, such as early, late and deep learning fusion. Evaluation results show that the three proposed models have achieved comparable results to the state-of-the-art supervised learning techniques. More importantly, this article demonstrates plausible ways to translate constant talk between modalities during the training phase, which also brings advantages in generalisation and robustness to noise.},
  archive  = {J},
  author   = {Esma Mansouri Benssassi and Juan Ye},
  doi      = {10.1109/TAFFC.2021.3106254},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {906-918},
  title    = {Investigating multisensory integration in emotion recognition through bio-inspired computational models},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long short-term memory network based unobtrusive workload
monitoring with consumer grade smartwatches. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(2), 895–905. (<a
href="https://doi.org/10.1109/TAFFC.2021.3110211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continuous high perceived workload has a negative impact on the individual&#39;s well-being. Prior works focused on detecting the workload with medical-grade wearable systems in restricted settings, and the effect of applying deep learning techniques for perceived workload detection in the wild settings is not investigated. We present an unobtrusive, comfortable, pervasive, and affordable Long Short-Term Memory Network based continuous workload monitoring system based on a smartwatch application that monitors the perceived workload of individuals in the wild. We have recorded physiological data from daily life with perceived workload questionnaires from subjects in their real-life environments over a month. The model was trained and evaluated with the daily-life physiological data coming from different days, which makes it robust to daily changes in the heart rate variability that we use with accelerometer features to assess low and high workload. Our system has the capability of detecting perceived workload by using traditional and deep classifiers. We discussed the problems related to ’in the wild’ applications with the consumer-grade smartwatches. We showed that Long Short-Term Memory Network with feature extraction outperforms traditional classifiers and Convolutional Neural Networks on discrimination of low and high perceived workload with smartwatches in the wild.},
  archive  = {J},
  author   = {Deniz Ekiz and Yekta Said Can and Cem Ersoy},
  doi      = {10.1109/TAFFC.2021.3110211},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {895-905},
  title    = {Long short-term memory network based unobtrusive workload monitoring with consumer grade smartwatches},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression animation by landmark guided residual
module. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(2), 878–894. (<a
href="https://doi.org/10.1109/TAFFC.2021.3100352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the problem of facial expression animation from a still image according to a driving video. This is a challenging task as expression motions are non-rigid and very subtle to be captured. Existing methods mostly fail to model these subtle expression motions, leading to the lack of details in their animation results. In this paper, we propose a novel facial expression animation method based on generative adversarial learning. To capture the subtle expression motions, Landmark guided Residual Module (LRM) is proposed to model detailed facial expression features. Specifically, residual learning is conducted at both coarse and fine levels conditioned on facial landmark heatmaps and landmark points respectively. Furthermore, we employ a consistency discriminator to ensure the temporal consistency of the generated video sequence. In addition, a novel metric named Emotion Consistency Metric is proposed to evaluate the consistency of facial expressions in the generated sequences with those in the driving videos. Experiments on MUG-Face, Oulu-CASIA and CAER datasets show that the proposed method can generate arbitrary expression motions on the source still image effectively, which are more photo-realistic and consistent with the driving video compared with results of state-of-the-art methods.},
  archive  = {J},
  author   = {Xueping Wang and Yunhong Wang and Weixin Li and Zhengyin Du and Di Huang},
  doi      = {10.1109/TAFFC.2021.3100352},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {878-894},
  title    = {Facial expression animation by landmark guided residual module},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E-key: An EEG-based biometric authentication and driving
fatigue detection system. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(2), 864–877. (<a
href="https://doi.org/10.1109/TAFFC.2021.3133443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to the increasing number of fatal traffic accidents, there are strong desire for more effective and convenient techniques for driving fatigue detection. Here, we propose a unified framework – E-Key to simultaneously perform personal identification (PI) and driving fatigue detection using a convolutional neural network and attention (CNN-Attention) structure. The performance was assessed using EEG data collected through a wearable dry-sensor system from 31 healthy subjects undergoing a 90-min simulated driving task. In comparison with three widely-used competitive models (including CNN, CNN-LSTM, and Attention), the proposed scheme achieved the best (p $&amp;lt;$ 0.01) performance in both PI (98.5%) and fatigue detection (97.8%). Besides, the spatial-temporal structure of the proposed framework exhibits an optimal balance between classification performance and computational efficiency. Additional validation analyses were conducted to assess the reliability and practicability of the model via re-configuring the kernel size and manipulating the input data, showing that it can achieve a satisfactory performance using a subset of the input data. In sum, these findings would pave the way for further practical implementation of in-vehicle expert system, showing great potential in autonomous driving and car-sharing where currently monitoring of PI and driving fatigue are of particular interest.},
  archive  = {J},
  author   = {Tao Xu and Hongtao Wang and Guanyong Lu and Feng Wan and Mengqi Deng and Peng Qi and Anastasios Bezerianos and Cuntai Guan and Yu Sun},
  doi      = {10.1109/TAFFC.2021.3133443},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {864-877},
  title    = {E-key: An EEG-based biometric authentication and driving fatigue detection system},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Morality classification in natural language text. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 857–863. (<a
href="https://doi.org/10.1109/TAFFC.2020.3034050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The language employed by an individual when discussing topics of a moral nature (of the kind typically found in, e.g., social media) is revealing not only of the text affective contents itself, but also of the individual who wrote the text in the first place. Based on these observations, this work intends to illustrate how two kinds of morality-related information may be inferred from text by presenting a number of shallow and deep learning models of moral stance and moral foundations classification. In doing so, we introduce a novel corpus of texts labelled with moral foundation scores, and a novel approach to fine-grained, human-centric moral foundations classification that is, to the best of our knowledge, among the first NLP studies of this kind.},
  archive  = {J},
  author   = {Matheus Camasmie Pavan and Vitor Garcia dos Santos and Alex Gwo Jen Lan and João Trevisan Martins and Wesley Ramos dos Santos and Caio Deutsch and Pablo Botton da Costa and Fernando Chiu Hsieh and Ivandré Paraboni},
  doi      = {10.1109/TAFFC.2020.3034050},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {857-863},
  title    = {Morality classification in natural language text},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating affective taste experience using combined
implicit behavioral and neurophysiological measures. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 849–856. (<a
href="https://doi.org/10.1109/TAFFC.2020.3032236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We trained a model to distinguish an extreme high arousal, unpleasant drink from regular drinks based on a range of implicit behavioral and physiological responses to naturalistic tasting. The trained model predicted arousal ratings of regular drinks, highlighting the possibility to estimate affective experience without having to rely on subjective ratings.},
  archive  = {J},
  author   = {A.-M. Brouwer and T. J. van den Broek and M. A. Hogervorst and D. Kaneko and A. Toet and V. Kallen and J. B. F. van Erp},
  doi      = {10.1109/TAFFC.2020.3032236},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {849-856},
  title    = {Estimating affective taste experience using combined implicit behavioral and neurophysiological measures},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion-regularized conditional variational autoencoder for
emotional response generation. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 842–848. (<a
href="https://doi.org/10.1109/TAFFC.2021.3073809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents an emotion-regularized conditional variational autoencoder (Emo-CVAE) model for generating emotional conversation responses. In conventional CVAE-based emotional response generation, emotion labels are simply used as additional conditions in prior, posterior and decoder networks. Considering that emotion styles are naturally entangled with semantic contents in the language space, the Emo-CVAE model utilizes emotion labels to regularize the CVAE latent space by introducing an extra emotion prediction network. In the training stage, the estimated latent variables are required to predict the emotion labels and token sequences of the input responses simultaneously. Experimental results show that our Emo-CVAE model can learn a more informative and structured latent space than a conventional CVAE model and output responses with better content and emotion performance than baseline CVAE and sequence-to-sequence (Seq2Seq) models.},
  archive  = {J},
  author   = {Yu-Ping Ruan and Zhen-Hua Ling},
  doi      = {10.1109/TAFFC.2021.3073809},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {842-848},
  title    = {Emotion-regularized conditional variational autoencoder for emotional response generation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated autism detection based on characterizing
observable patterns from photos. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 836–841. (<a
href="https://doi.org/10.1109/TAFFC.2020.3035088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autism spectrum disorder (ASD) is a developmental disorder that affects the communication and behavior. People with ASD show atypical attentions to social stimuli and gaze at human faces and complex scenes in an unusual way, and their facial expressions are often atypical as well. This article investigates the feasibility of developing an automated method to analyze the visual cues of autism using the photos taken by people with ASD, comparing to photos taken by people without ASD, in different scenarios. It was inspired by a recent study based on manual inspection of the photos. The key challenge is what and how to characterize the photos taken by people with ASD, to facilitate an automated separation from normal people. Several features are proposed to characterize the observable behaviors for ASD with experimental validations. This is the first work to perform an automatic analysis of the photos taken by people with ASD, achieving a prediction accuracy of 85.8 percent.},
  archive  = {J},
  author   = {Alice Z. Guo},
  doi      = {10.1109/TAFFC.2020.3035088},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {836-841},
  title    = {Automated autism detection based on characterizing observable patterns from photos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AffectON: Incorporating affect into dialog generation.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
823–835. (<a href="https://doi.org/10.1109/TAFFC.2020.3043067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to its expressivity, natural language is paramount for explicit and implicit affective state communication among humans. The same linguistic inquiry (e.g., How are you? ) might induce responses with different affects depending on the affective state of the conversational partner(s) and the context of the conversation. Yet, most dialog systems do not consider affect as constitutive aspect of response generation. In this article, we introduce AffectON , an approach for generating affective responses during inference. For generating language in a targeted affect, our approach leverages a probabilistic language model and an affective space. AffectON is language model agnostic, since it can work with probabilities generated by any language model (e.g., sequence-to-sequence models, neural language models, n-grams). Hence, it can be employed for both affective dialog and affective language generation. We experimented with affective dialog generation and evaluated the generated text objectively and subjectively. For the subjective part of the evaluation, we designed a custom user interface for rating and provided recommendations for the design of such interfaces. The results, both subjective and objective demonstrate that our approach is successful in pulling the generated language toward the targeted affect, with little sacrifice in syntactic coherence.},
  archive  = {J},
  author   = {Zana Buçinca and Yücel Yemez and Engin Erzin and Metin Sezgin},
  doi      = {10.1109/TAFFC.2020.3043067},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {823-835},
  title    = {AffectON: Incorporating affect into dialog generation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local temporal pattern and data augmentation for spotting
micro-expressions. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 811–822. (<a
href="https://doi.org/10.1109/TAFFC.2020.3023821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions (MEs) are very important nonverbal communication clues. However, due to their local and short nature, spotting them is challenging. In this article, we address this problem by using a dedicated local and temporal pattern (LTP) of facial movement. This pattern has a specific shape (an S-pattern) when MEs are displayed. Thus, by using a classic classification algorithm (SVM), MEs can be distinguished from other facial movements. We also propose a global final fusion analysis covering the whole face to improve the distinction between ME (local) and head (global) movements. However, the learning of S-patterns is limited by the small number of ME databases and the low volume of ME samples. Hammerstein models (HMs) are known to effectively approximate muscle movements. By approximating each S-pattern with an HM, we can both filter out outliers and generate new similar S-patterns. In this way, we augment the dataset for S-pattern training and improve the ability to differentiate MEs from other movements. The spotting results, performed in the CASMEI and CASMEII databases, show that our proposed LTP outperforms the most popular spotting method in terms of the F1-score. Adding a fusion process and data augmentation improves the spotting performance even further.},
  archive  = {J},
  author   = {Jingting Li and Catherine Soladié and Renaud Séguier},
  doi      = {10.1109/TAFFC.2020.3023821},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {811-822},
  title    = {Local temporal pattern and data augmentation for spotting micro-expressions},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STCAM: Spatial-temporal and channel attention module for
dynamic facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 800–810. (<a
href="https://doi.org/10.1109/TAFFC.2020.3027340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Capturing the dynamics of facial expression progression in video is an essential and challenging task for facial expression recognition (FER). In this article, we propose an effective framework to address this challenge. We develop a C3D-based network architecture, 3D-Inception-ResNet, to extract spatial-temporal features from the dynamic facial expression image sequence. A Spatial-Temporal and Channel Attention Module (STCAM) is proposed to explicitly exploit the holistic spatial-temporal and channel-wise correlations among the extracted features. Specifically, the proposed STCAM calculates a channel-wise and a spatial-temporal-wise attention map to enhance the features along the corresponding feature dimensions for more representative features. We evaluate our method on three popular dynamic facial expression recognition datasets, CK+, Oulu-CASIA, and MMI. Experimental results show that our method achieves better or comparable performance compared to the state-of-the-art approaches.},
  archive  = {J},
  author   = {Weicong Chen and Dong Zhang and Ming Li and Dah-Jye Lee},
  doi      = {10.1109/TAFFC.2020.3027340},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {800-810},
  title    = {STCAM: Spatial-temporal and channel attention module for dynamic facial expression recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Acute stress state classification based on electrodermal
activity modeling. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 788–799. (<a
href="https://doi.org/10.1109/TAFFC.2021.3055294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Acute stress is a physiological condition that may induce several neural dysfunctions with a significant impact on life quality. Accordingly, it would be important to monitor stress in everyday life unobtrusively and inexpensively. In this paper, we presented a new methodological pipeline to recognize acute stress conditions using electrodermal activity (EDA) exclusively. Particularly, we combined a rigorous and robust model (cvxEDA) for EDA processing and decomposition, with an algorithm based on a support vector machine to classify the stress state at a single-subject level. Indeed, our method, based on a single sensor, is robust to noise, applies a rigorous phasic decomposition, and implements an unbiased multiclass classification. To this end, we analyzed the EDA of 65 volunteers subjected to different acute stress stimuli induced by a modified version of the Trier Social Stress Test. Our results show that stress is successfully detected with an average accuracy of 94.62 percent. Besides, we proposed a further 4-class pattern recognition system able to distinguish between non-stress condition and three different stressful stimuli achieving an average accuracy as high as 75.00 percent. These results, obtained under controlled conditions, are the first step towards applications in ecological scenarios.},
  archive  = {J},
  author   = {Alberto Greco and Gaetano Valenza and Jesús Lázaro and Jorge Mario Garzón-Rey and Jordi Aguiló and Concepcion de la Cámara and Raquel Bailón and Enzo Pasquale Scilingo},
  doi      = {10.1109/TAFFC.2021.3055294},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {788-799},
  title    = {Acute stress state classification based on electrodermal activity modeling},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards participant-independent stress detection using
instrumented peripherals. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 773–787. (<a
href="https://doi.org/10.1109/TAFFC.2021.3061417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Methods to measure work stress generally rely on subjective measures from questionnaires or require dedicated sensors that are cumbersome to wear and interfere with the task. To address this problem, we propose a method to detect stress unobtrusively using commodity devices (keyboards, mice) instrumented with pressure sensors. We propose a minimalist design that can be easily replicated by other researchers using off-the-shelf and low-cost hardware. We validate the design in a laboratory experiment that simulates office tasks and mild stressors while avoiding methodological limitations of previous studies. We compare stress-detection performance when using conventional features reported in the literature (keystroke dynamics, mouse trajectories) augmented with information from pressure sensors. Our results indicate that pressure provides additional information for stress discrimination; adding pressure information to keystroke dynamics and mouse trajectories improves classification performance by 6% and 3%, respectively. These results show how devices that are already part of the modern workplace may be used and enhanced to automatically and unobtrusively detect stress.},
  archive  = {J},
  author   = {Dennis R. da C. Silva and Zelun Wang and Ricardo Gutierrez-Osuna},
  doi      = {10.1109/TAFFC.2021.3061417},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {773-787},
  title    = {Towards participant-independent stress detection using instrumented peripherals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multidimensional culturally adapted representation of
emotions for affective computational simulation and recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
761–772. (<a href="https://doi.org/10.1109/TAFFC.2020.3030586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the main challenges in affective computing is the development of models to represent the information that is inherent to emotions. It is necessary to consider that the terms used by humans to name emotions depend on the culture and language used. This article presents an experiment-based method to represent and adapt emotion terms to different cultural environments. We propose using circular boxplots to analyze the distribution of emotions in the Pleasure-Arousal space. From the results of this analysis, we define a new cross-cultural representation model of emotions in which each emotion term is assigned to an area in the Pleasure-Arousal space. An emotion is represented by a vector in which the direction indicates the type, and the module indicates the intensity of the emotion. We propose two methods based on fuzzy logic to represent and express emotions: the emotion representation process in which the term associated with the recognized emotion is defuzzified and projected as a vector in the Pleasure-Arousal space; and the emotion expression process in which a fuzzification of the vector is produced, generating a fuzzy emotion term that is adapted to the culture and language in which the emotion will be used.},
  archive  = {J},
  author   = {Joaquin Taverner and Emilio Vivancos and Vicente Botti},
  doi      = {10.1109/TAFFC.2020.3030586},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {761-772},
  title    = {A multidimensional culturally adapted representation of emotions for affective computational simulation and recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spontaneous driver emotion facial expression (DEFE)
dataset for intelligent vehicles: Emotions triggered by video-audio
clips in driving scenarios. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 747–760. (<a
href="https://doi.org/10.1109/TAFFC.2021.3063387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, a new dataset, the driver emotion facial expression (DEFE) dataset for drivers’ spontaneous emotions analysis is introduced. The dataset includes facial expression recordings from 60 participants during driving. After watching a selected video-audio clip to elicit a speciﬁc emotion, each participant completed the driving tasks in the same driving scenario and rated his/her emotional responses during the driving processes from the aspects of dimensional emotion method and discrete emotion method. The study also conducted classiﬁcation experiments to recognize the scales of arousal, valence, dominance, as well as the emotion category and intensity to establish baseline results for the proposed dataset. Furthermore, this paper compared emotion recognition results difference through facial expressions between dynamic driving and static life scenarios. The results showed that dynamic driving and static life datasets were different in emotion recognition results. To further explore the reasons for the difference in emotion recognition results, the analysis from the AU (action unit) presence perspective was studied. The results showed significant differences in the AUs presence of facial expressions between dynamic driving and static life scenarios, indicating that drivers’ facial expressions may be affected by the driving task to influence the recognition of drivers’ emotions through facial expressions. Therefore, to accurately recognize the drivers’ emotions to establish a reliable emotion-aware human-machine interaction system, thereby improving driving safety and comfort, publishing a human emotion dataset speciﬁcally for the driver is necessary. The proposed dataset will be publicly available so that researchers worldwide can use it to develop and examine their driver emotion analysis methods. To the best of our knowledge, this is currently the only public driver facial expression dataset.},
  archive  = {J},
  author   = {Wenbo Li and Yaodong Cui and Yintao Ma and Xingxin Chen and Guofa Li and Guanzhong Zeng and Gang Guo and Dongpu Cao},
  doi      = {10.1109/TAFFC.2021.3063387},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {747-760},
  title    = {A spontaneous driver emotion facial expression (DEFE) dataset for intelligent vehicles: Emotions triggered by video-audio clips in driving scenarios},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When is a haptic message like an inside joke? Digitally
mediated emotive communication builds on shared history. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 732–746. (<a
href="https://doi.org/10.1109/TAFFC.2023.3244520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Touch is valued for supporting emotional bonds. How can people access its warmth and nuance remotely, when tech-mediated proxies are so different from direct touch? We assessed the viability of haptic animations as affect-embedded tactile messages, highlighting findings which demonstrate how crucial relationship and shared history is in influencing these expressions in design and interpretation. To investigate haptic messaging, we first identified a set of 10 common emotion-imbued scenarios by surveying 201 people in distance relationships. Then, using a novel prototype of a wearable spatial vibrotactile display, 10 intimate dyads designed 167 haptic encodings matching the provided scenarios plus 17 user-defined “wildcards”. A week later, 21 individuals interpreted sentiment from encodings designed by themselves, a partner or a stranger. We examined design strategies, engagement, and compared human versus machine interpretation accuracy. A striking finding was participants’ facile use of shared context when it was available, building on “inside stories” to communicate subtle meanings with high effectiveness despite the unfamiliar medium, and doing so with evident fun. We analyze recognition accuracy and share insights on what it might take to make interpersonal haptic messaging work.},
  archive  = {J},
  author   = {Xi Laura Cang and Ali Israr and Karon E. MacLean},
  doi      = {10.1109/TAFFC.2023.3244520},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {732-746},
  title    = {When is a haptic message like an inside joke? digitally mediated emotive communication builds on shared history},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised cross-view facial expression image generation
and recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 718–731. (<a
href="https://doi.org/10.1109/TAFFC.2020.3029531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose an unsupervised cross-view facial expression adaptation network (UCFEAN) to simultaneously generate and recognize cross-view facial expressions in images in an unsupervised manner. The main idea of UCFEAN is to convert the unsupervised domain adaptation between two image spaces with different appearance into semi-supervised learning (SSL) in feature spaces with the same semantic content. The cyclic image generation of cross-view facial expressions based on the generative adversarial network (GAN) is carried out to project unlabelled target images and labelled source images to the corresponding feature spaces with the same semantic content. This helps realize the unsupervised feature learning of the target image. Labels of facial expressions represented in the projected target features can then be learned using the projected source features, because the distributions of the projected features in the two domains are close enough for knowledge transfer by using SSL. Three techniques are developed to train UCFEAN in an effective and stable manner. Extensive experiments are conducted to evaluate the UCFEAN on two multi-view facial expression image databases including RaFD and Multi-PIE. The results show that the proposed method can generate realistic target images of the facial expression and recognize cross-view facial expressions with high precision.},
  archive  = {J},
  author   = {Ning Sun and Qingyi Lu and Wenming Zheng and Jixin Liu and Guang Han},
  doi      = {10.1109/TAFFC.2020.3029531},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {718-731},
  title    = {Unsupervised cross-view facial expression image generation and recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic detection of emotional changes induced by social
support loss using fMRI. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 706–717. (<a
href="https://doi.org/10.1109/TAFFC.2021.3059965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose using fMRI to study emotional changes related to social-support. In this respect, a Social Support fMRI task, which triggers emotional changes was designed and implemented. The detection of emotional changes from fMRI signals has significant importance in understanding the underlying mechanisms of social-support. Unfortunately, acquired signals exhibit a very low signal-to-noise ratio and strong inter-subject variations, which render the detection process a very challenging task. For this purpose, a three-phase detection system is designed. First, possible emotional change intervals are classified to isolate trivial samples and further process the challenging ones. Second, a new denoising strategy is proposed to preserve the structural waveform properties of the emotional changes, while removing noise. Third, fMRI signals are synthesized using trapezoidal modeling and a novel feature set is extracted to characterize the varying social-support levels. The analysis shows that emotional changes can be detected automatically up to a requisite level. Despite the results cannot be generalized for the entire population due to its small sample size, our findings are meaningful and suggest further research with larger datasets. The introduced task may enable further research and the proposed system may be used as a tool for social neuroscience studies.},
  archive  = {J},
  author   = {Cemre Candemir and Ali Saffet Gonul and M. Alper Selver},
  doi      = {10.1109/TAFFC.2021.3059965},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {706-717},
  title    = {Automatic detection of emotional changes induced by social support loss using fMRI},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective connectivity based EEG revealing the inhibitory
deficits for distracting stimuli in major depression disorders. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 694–705. (<a
href="https://doi.org/10.1109/TAFFC.2021.3054953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional conflict control is impaired in major depression disorders (MDDs) and affects decision-making with further consequent social interactions dysfunction. However, neural correlates of conflict monitoring processes being modulated by different affective distractor stimuli are not clear in MDDs. In this article, we investigated abnormal neural basis of conflict monitoring processes in MDD patients by applying dynamic causal modeling (DCM) technique on electroencephalography (EEG). The results indicated that MDD patients showed lower N2 amplitudes regardless of stimulus conditions, and reduced activation within ACC region for incongruent stimuli, relative to healthy controls. Especially, MDDs had more negative N2 amplitudes to happy incongruent trials than happy congruent trials. Source localization analyses revealed that MDD patients had significantly enhanced left inferior temporal gyrus (ITG) activation, which is involved in written words processing. Further DCM analysis provided abnormal neural correlates through greater backward connections (fusiform→ITG, amygdala→ITG) on happy incongruent trials than happy congruent trials in MDD group. These findings indicate that only sad words induce significantly greater interference effects to positive target faces in MDD patients, which may be associated with ITG activity dysfunction. The findings may share new insights into the neural mechanisms of emotional conflict processing in MDDs.},
  archive  = {J},
  author   = {Jianxiu Li and Yanrong Hao and Wei Zhang and Xiaowei Li and Bin Hu},
  doi      = {10.1109/TAFFC.2021.3054953},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {694-705},
  title    = {Effective connectivity based EEG revealing the inhibitory deficits for distracting stimuli in major depression disorders},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The pixels and sounds of emotion: General-purpose
representations of arousal in games. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 680–693. (<a
href="https://doi.org/10.1109/TAFFC.2021.3060877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {What if emotion could be captured in a general and subject-agnostic fashion? Is it possible, for instance, to design general-purpose representations that detect affect solely from the pixels and audio of a human-computer interaction video? In this article we address the above questions by evaluating the capacity of deep learned representations to predict affect by relying only on audiovisual information of videos. We assume that the pixels and audio of an interactive session embed the necessary information required to detect affect. We test our hypothesis in the domain of digital games and evaluate the degree to which deep classifiers and deep preference learning algorithms can learn to predict the arousal of players based only on the video footage of their gameplay. Our results from four dissimilar games suggest that general-purpose representations can be built across games as the arousal models obtain average accuracies as high as 85 percent using the challenging leave-one-video-out cross-validation scheme. The dissimilar audiovisual characteristics of the tested games showcase the strengths and limitations of the proposed method.},
  archive  = {J},
  author   = {Konstantinos Makantasis and Antonios Liapis and Georgios N. Yannakakis},
  doi      = {10.1109/TAFFC.2021.3060877},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {680-693},
  title    = {The pixels and sounds of emotion: General-purpose representations of arousal in games},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward automated classroom observation: Multimodal machine
learning to estimate CLASS positive climate and negative climate.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
664–679. (<a href="https://doi.org/10.1109/TAFFC.2021.3059209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article we present a multi-modal machine learning-based system, which we call ACORN, to analyze videos of school classrooms for the Positive Climate (PC) and Negative Climate (NC) dimensions of the CLASS [1] observation protocol that is widely used in educational research. ACORN uses convolutional neural networks to analyze spectral audio features, the faces of teachers and students, and the pixels of each image frame, and then integrates this information over time using Temporal Convolutional Networks. The audiovisual ACORN&#39;s PC and NC predictions have Pearson correlations of 0.55 and 0.63 with ground-truth scores provided by expert CLASS coders on the UVA Toddler dataset (cross-validation on $n=300$ 15-min video segments), and a purely auditory ACORN predicts PC and NC with correlations of 0.36 and 0.41 on the MET dataset (test set of $n=2000$ videos segments). These numbers are similar to inter-coder reliability of human coders. Finally, using Graph Convolutional Networks we make early strides (AUC=0.70) toward predicting the specific moments (45-90sec clips) when the PC is particularly weak/strong. Our findings inform the design of automatic classroom observation and also more general video activity recognition and summary recognition systems.},
  archive  = {J},
  author   = {Anand Ramakrishnan and Brian Zylich and Erin Ottmar and Jennifer LoCasale-Crouch and Jacob Whitehill},
  doi      = {10.1109/TAFFC.2021.3059209},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {664-679},
  title    = {Toward automated classroom observation: Multimodal machine learning to estimate CLASS positive climate and negative climate},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human emotion recognition with relational region-level
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 650–663. (<a
href="https://doi.org/10.1109/TAFFC.2021.3064918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing the emotional state of a person within the image in real-world scenarios is a key problem in affective computing and has various promising applications. Local regions in the image, including different objects in the background scene and parts within the foreground body, usually have different contributions to emotion perception of the target person. This, however, has not been well exploited in most existing methods. In this article, we propose to make relational region-level analysis to account for the different contributions of different regions to emotion recognition. For the background scene, we propose a Body-Object Attention (BOA) module to estimate the contributions of background objects to emotion recognition given the target foreground body. Within the foreground body, we propose a Body Part Attention (BPA) module to recalibrate the channel-wise body feature responses to attend on body parts that are more important. Moreover, we propose to model the emotion label dependency in real-world images, considering both the semantic meanings of these labels and their co-occurrence patterns. We evaluate the proposed method on the EMOTIC and CAER-S datasets, and experimental results show the superiority of our method compared with the state-of-the-art algorithms.},
  archive  = {J},
  author   = {Weixin Li and Xuan Dong and Yunhong Wang},
  doi      = {10.1109/TAFFC.2021.3064918},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {650-663},
  title    = {Human emotion recognition with relational region-level analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain-computer interface for generating personally
attractive images. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 637–649. (<a
href="https://doi.org/10.1109/TAFFC.2021.3059043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While we instantaneously recognize a face as attractive, it is much harder to explain what exactly defines personal attraction. This suggests that attraction depends on implicit processing of complex, culturally and individually defined features. Generative adversarial neural networks (GANs), which learn to mimic complex data distributions, can potentially model subjective preferences unconstrained by pre-defined model parameterization. Here, we present generative brain-computer interfaces (GBCI), coupling GANs with brain-computer interfaces. GBCI first presents a selection of images and captures personalized attractiveness reactions toward the images via electroencephalography. These reactions are then used to control a GAN model, finding a representation that matches the features constituting an attractive image for an individual. We conducted an experiment (N = 30) to validate GBCI using a face-generating GAN and producing images that are hypothesized to be individually attractive. In double-blind evaluation of the GBCI-produced images against matched controls, we found GBCI yielded highly accurate results. Thus, the use of EEG responses to control a GAN presents a valid tool for interactive information-generation. Furthermore, the GBCI-derived images visually replicated known effects from social neuroscience, suggesting that the individually responsive, generative nature of GBCI provides a powerful, new tool in mapping individual differences and visualizing cognitive-affective processing.},
  archive  = {J},
  author   = {Michiel Spapé and Keith M. Davis and Lauri Kangassalo and Niklas Ravaja and Zania Sovijärvi-Spapé and Tuukka Ruotsalo},
  doi      = {10.1109/TAFFC.2021.3059043},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {637-649},
  title    = {Brain-computer interface for generating personally attractive images},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UBFC-phys: A multimodal database for psychophysiological
studies of social stress. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 622–636. (<a
href="https://doi.org/10.1109/TAFFC.2021.3056960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As humans, we experience social stress in countless everyday-life situations. Giving a speech in front of an audience, passing a job interview, and similar experiences all lead us to go through stress states that impact both our psychological and physiological states. Therefore, studying the link between stress and physiological responses had become a critical societal issue, and recently, research in this field has grown in popularity. However, publicly available datasets have limitations. In this article, we propose a new dataset, UBFC-Phys, collected with and without contact from participants living social stress situations. A wristband was used to measure contact blood volume pulse (BVP) and electrodermal activity (EDA) signals. Video recordings allowed to compute remote pulse signals, using remote photoplethysmography (RPPG), and facial expression features. Pulse rate variability (PRV) was extracted from BVP and RPPG signals. Our dataset permits to evaluate the possibility of using video-based physiological measures compared to more conventional contact-based modalities. The goal of this article is to present both the dataset, which we make publicly available, and experimental results of contact and non-contact data comparison, as well as stress recognition. We obtained a stress state recognition accuracy of 85.48 percent, achieved by remote PRV features.},
  archive  = {J},
  author   = {Rita Meziati Sabour and Yannick Benezeth and Pierre De Oliveira and Julien Chappé and Fan Yang},
  doi      = {10.1109/TAFFC.2021.3056960},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {622-636},
  title    = {UBFC-phys: A multimodal database for psychophysiological studies of social stress},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating forthcoming events and personality traits in
social media based stress prediction. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 603–621. (<a
href="https://doi.org/10.1109/TAFFC.2021.3076294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting the forthcoming stress is critical for stress management. In this article, we consider not only one’s posts on social media, but also learn to understand the influence of stressor/uplift events and individual&#39;s reactions to the events by constructing an event-post correlation memory network, which evolves dynamically along with the change of events impact and one’s response reflected from their posts. We further build a joint memory network for modeling the dynamics of one’s emotions incurred by stressor/uplift events, and learn one&#39;s personality traits based on linguistic words and a fuzzy neural network. We finally predict one&#39;s future stress level based on a fully-connected network with attention, where personality traits, social activeness features, and forthcoming possible events are incorporated. We construct a dataset consisting of 1138 strongly-stressed and 985 weakly-stressed users on microblog. Experimental results show that: (1) our method outperformed the baseline, delivering 81.03 percent of prediction accuracy; (2) integrating the personality traits helped increase the prediction accuracy by 3.97 percent; (3) considering forthcoming events enabled to improve the prediction accuracy by 5.81 percent; (4) strongly-stressed users tended to be more neurotic and less active on social media, complying with psychological studies; (5) data scarcity had negative influence on stress prediction and (6) the dataset that is biased towards female made the model have a better prediction accuracy on female users.},
  archive  = {J},
  author   = {Ningyun Li and Huijun Zhang and Ling Feng},
  doi      = {10.1109/TAFFC.2021.3076294},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {603-621},
  title    = {Incorporating forthcoming events and personality traits in social media based stress prediction},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An emotion recognition method for game evaluation based on
electroencephalogram. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 591–602. (<a
href="https://doi.org/10.1109/TAFFC.2020.3023966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Players-based emotion recognition can help the understanding game players’ emotional states, contributing to the improvement of the game&#39;s quality and value. This article develops a hybrid neural network learning framework called convolutional smooth feedback fuzzy network (CSFFN) to detect a player&#39;s emotional states in real-time during a gaming process based on electroencephalogram (EEG) signals. Specifically, CSFFN rationally combines a convolutional neural network (CNN), a fuzzy neural network (FNN), and a recurrent neural network (RNN). CNN not only captures spatial characteristics between EEG signals from different channels but also eliminates noise from EEG signals, improving the accuracy and anti-noise performance in game emotion recognition. FNN extracts the membership degree of a player&#39;s different emotional states, further improving the emotion recognition accuracy. Since a player&#39;s current emotional state is influenced by the previous emotional states during the game process, RNN is employed to capture the temporal characteristics of EEG signals, better improving the emotion recognition accuracy. Experimental results show that CSFFN has higher recognition accuracy and noise resistance in identifying four emotional states (happiness, sadness, superiority, and anger) compared to support vector machine (SVM) with different kernels, linear discrimination analysis (LDA), AlexNet, and VGG16 methods.},
  archive  = {J},
  author   = {Guanglong Du and Wenpei Zhou and Chunquan Li and Di Li and Peter X. Liu},
  doi      = {10.1109/TAFFC.2020.3023966},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {591-602},
  title    = {An emotion recognition method for game evaluation based on electroencephalogram},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDN: A deep maximization-differentiation network for
spatio-temporal depression detection. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 578–590. (<a
href="https://doi.org/10.1109/TAFFC.2021.3072579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning (DL) models have been successfully applied in video-based affective computing, allowing, for instance, to recognize emotions and mood, or to estimate the intensity of pain or stress of individuals based on their facial expressions. Despite the recent advances with state-of-the-art DL models for spatio-temporal recognition of facial expressions associated with depressive behaviour, some key challenges remain in the cost-effective application of 3D-CNNs: (1) 3D convolutions usually employ structures with fixed temporal depth that decreases the potential to extract discriminative representations due to the usually small difference of spatio-temporal variations along different depression levels; and (2) the computational complexity of these models with consequent susceptibility to overfitting. To address these challenges, we propose a novel DL architecture called the Maximization and Differentiation Network (MDN) in order to effectively represent facial expression variations that are relevant for depression assessment. The MDN, operating without 3D convolutions, explores multiscale temporal information using a maximization block that captures smooth facial variations and a difference block that encodes sudden facial variations. Extensive experiments using our proposed MDN with models with 100 and 152 layers result in improved performance while reducing the number of parameters by more than $3\times$ when compared with 3D ResNet models. Our model also outperforms other 3D models and achieves state-of-the-art results for depression detection. Code available at: https://github.com/wheidima/MDN .},
  archive  = {J},
  author   = {Wheidima Carneiro de Melo and Eric Granger and Miguel Bordallo López},
  doi      = {10.1109/TAFFC.2021.3072579},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {578-590},
  title    = {MDN: A deep maximization-differentiation network for spatio-temporal depression detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ordinal logistic regression with partial proportional odds
for depression prediction. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 563–577. (<a
href="https://doi.org/10.1109/TAFFC.2020.3031300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Like many psychological scales, depression scales are ordinal in nature. Depression prediction from behavioral signals has so far been posed either as classification or regression problems. However, these naive approaches have fundamental issues because they are not focused on ranking, unlike ordinal regression, which is the most appropriate approach. Ordinal regression to date has comparatively few methods when compared with other branches in machine learning, and its usage is limited to specific research domains. Ordinal logistic regression (OLR) is one such method, which is an extension for ordinal data of the well-known logistic regression, but is not familiar in speech processing, affective computing or depression prediction. The primary aim of this article is to investigate proportionality structures and model selection for the design of ordinal regression systems within the logistic regression framework. A new greedy-based algorithm for partial proportional odds model selection (GREP) is proposed that allows the parsimonious design of effective ordinal logistic regression models, which avoids an exhaustive search and outperforms model selection using the Brant test. Evaluations on the DAIC-WOZ and AViD depression corpora show that OLR models exploiting GREP can outperform two competitive baseline systems (GSR and CNN), in terms of both RMSE and Spearman correlation.},
  archive  = {J},
  author   = {Sadari Jayawardena and Julien Epps and Eliathamby Ambikairajah},
  doi      = {10.1109/TAFFC.2020.3031300},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {563-577},
  title    = {Ordinal logistic regression with partial proportional odds for depression prediction},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regional attention network (RAN) for head pose and
fine-grained gesture recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 549–562. (<a
href="https://doi.org/10.1109/TAFFC.2020.3031841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affect is often expressed via non-verbal body language such as actions/gestures, which are vital indicators for human behaviors. Recent studies on recognition of fine-grained actions/gestures in monocular images have mainly focused on modeling spatial configuration of body parts representing body pose, human-objects interactions and variations in local appearance. The results show that this is a brittle approach since it relies on accurate body parts/objects detection. In this work, we argue that there exist local discriminative semantic regions, whose “informativeness” can be evaluated by the attention mechanism for inferring fine-grained gestures/actions. To this end, we propose a novel end-to-end regional attention network (RAN) , which is a fully convolutional neural network (CNN) to combine multiple contextual regions through attention mechanism, focusing on parts of the images that are most relevant to a given task. Our regions consist of one or more consecutive cells and are adapted from the strategies used in computing HOG (Histogram of Oriented Gradient) descriptor. The model is extensively evaluated on ten datasets belonging to 3 different scenarios: 1) head pose recognition, 2) drivers state recognition, and 3) human action and facial expression recognition. The proposed approach outperforms the state-of-the-art by a considerable margin in different metrics.},
  archive  = {J},
  author   = {Ardhendu Behera and Zachary Wharton and Yonghuai Liu and Morteza Ghahremani and Swagat Kumar and Nik Bessis},
  doi      = {10.1109/TAFFC.2020.3031841},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {549-562},
  title    = {Regional attention network (RAN) for head pose and fine-grained gesture recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SparseDGCNN: Recognizing emotion from multichannel EEG
signals. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 537–548. (<a
href="https://doi.org/10.1109/TAFFC.2021.3051332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition from EEG signals has attracted much attention in affective computing. Recently, a novel dynamic graph convolutional neural network (DGCNN) model was proposed, which simultaneously optimized the network parameters and a weighted graph $G$ characterizing the strength of functional relation between each pair of two electrodes in the EEG recording equipment. In this article, we propose a sparse DGCNN model which modifies DGCNN by imposing a sparseness constraint on $G$ and improves the emotion recognition performance. Our work is based on an important observation: the tomography study reveals that different brain regions sampled by EEG electrodes may be related to different functions of the brain and then the functional relations among electrodes are possibly highly localized and sparse. However, introducing sparseness constraint into the graph $G$ makes the loss function of sparse DGCNN non-differentiable at some singular points. To ensure that the training process of sparse DGCNN converges, we apply the forward-backward splitting method. To evaluate the performance of sparse DGCNN, we compare it with four representative recognition methods (SVM, DBN, GELM and DGCNN). In addition to comparing different recognition methods, our experiments also compare different features and spectral bands, including EEG features in time-frequency domain (DE, PSD, DASM, RASM, ASM and DCAU on different bands) extracted from four representative EEG datasets (SEED, DEAP, DREAMER, and CMEED). The results show that (1) sparse DGCNN has consistently better accuracy than representative methods and has a good scalability, and (2) DE, PSD, and ASM features on $\gamma$ band convey most discriminative emotional information, and fusion of separate features and frequency bands can improve recognition performance.},
  archive  = {J},
  author   = {Guanhua Zhang and Minjing Yu and Yong-Jin Liu and Guozhen Zhao and Dan Zhang and Wenming Zheng},
  doi      = {10.1109/TAFFC.2021.3051332},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {537-548},
  title    = {SparseDGCNN: Recognizing emotion from multichannel EEG signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active learning with complementary sampling for instructing
class-biased multi-label text emotion classification. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 523–536. (<a
href="https://doi.org/10.1109/TAFFC.2020.3038401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-quality corpora have been very scarce for the text emotion research. Existing corpora with multi-label emotion annotations have been either too small or too class-biased to properly support a supervised emotion learning. In this article, we propose a novel active learning method for efficiently instructing the human annotations for a less-biased and high-quality multi-label emotion corpus. Specifically, to compensate annotation for the minority-class examples, we propose a complementary sampling strategy based on unlabeled resources by measuring a probabilistic distance between the expected emotion label distribution in a temporary corpus and an uniform distribution. Qualitative evaluations are also given to the unlabeled examples, in which we evaluate the model uncertainties for multi-label emotion predictions, their syntactic representativeness for the other unlabeled examples, and their diverseness to the labeled examples, for a high-quality sampling. Through active learning, a supervised emotion classifier gets progressively improved by learning from these new examples. Experiment results suggest that by following these sampling strategies we can develop a corpus of high-quality examples with significantly relieved bias for emotion classes. Compared to the learning procedures based on traditional active learning algorithms, our learning procedure indicates the most efficient learning curve and estimates the best multi-label emotion predictions.},
  archive  = {J},
  author   = {Xin Kang and Xuefeng Shi and Yunong Wu and Fuji Ren},
  doi      = {10.1109/TAFFC.2020.3038401},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {523-536},
  title    = {Active learning with complementary sampling for instructing class-biased multi-label text emotion classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shyness trait recognition for schoolchildren via multi-view
features of online writing. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 509–522. (<a
href="https://doi.org/10.1109/TAFFC.2021.3077410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Shyness trait is a double-edged personality trait, and could pose a risk in early childhood for later adjustment difficulties. Therefore, it is necessary to pay attention to children with shyness trait and properly supervise them in early education, where the key problem is shyness trait recognition. Although some psychological methods of shyness measurement have been presented, they are high-cost and can only get a one-sided result limited by the targeted subjects. To develop an automated method of shyness trait recognition, we collect a dataset, containing online writing data of 1,754 schoolchildren from an educational website and ground truth labels obtained by a professional scale. The natural implicitness makes shyness trait difficult to be observed from a single point of view and the class imbalanced problem increases the challenge. In this article, a novel shyness trait recognition framework is proposed, which extracts multi-view features of online writing, including document-view, sentence-view and temporality-view ones. Different strategies with different features are applied to each single-view prediction and the multi-view prediction is made by a weighted voting ensemble. To verify the effectiveness, extensive experiments are conducted on the real-world dataset, demonstrating that the multi-view prediction significantly outperforms each single-view prediction and some advanced models of multi-view learning.},
  archive  = {J},
  author   = {Xuetao Tian and Liping Jing and Fang Luo and Sheng Zhang},
  doi      = {10.1109/TAFFC.2021.3077410},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {509-522},
  title    = {Shyness trait recognition for schoolchildren via multi-view features of online writing},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation of sensory-affective relationships depending on
material categories of pictorial stimuli. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 498–508. (<a
href="https://doi.org/10.1109/TAFFC.2020.3039684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exposure to stimuli gives rise to sensory and affective experiences. Computing the relationships between these experiences is instrumental to designing affectively appealing products and understanding human experiences. Hierarchical structures of sensory and affective responses, which can be built through adjective rating tasks, are regarded as an effective means for expressing relationships between sensory and affective responses. Naturally, these hierarchical structures depend on the type of stimulus; however, so far, their dependencies on material categories have yet to be examined. We therefore investigated how hierarchical structures of affective and sensory responses depend on seven material categories: fabric, leather, wood, paper, foliage, stone, and glass. Each material category had 100 visual representations selected from the Flickr Material Database. Thirty-nine participants were asked to rate 368 pictures across a set of materials. The questionnaire adjectives included visually- and haptically-centered items although the stimuli were purely visual. We found that the structures differed substantially among the material categories, although there were some commonalities. Particularly, the positions of polysemic or multimodal adjectives such as “light” and “uncomfortable” in the hierarchy were highly dependent on the material category. For example, “light” has both physical (lightweight) and psychological (e.g., non-solemn, cheerful) meanings. For stone and glass (generally considered to be of heavy weight), the psychological meanings were primarily considered. Conversely, the physical meanings were predominant for fabric, leather, wood, paper, and foliage, for which weight is a factor in judging quality. The present study helps interpret affective descriptors whose meanings vary among types of stimuli.},
  archive  = {J},
  author   = {Shogo Okamoto and Kohta Wakamatsu and Shigeki Nakauchi and Jinhwan Kwon and Maki Sakamoto},
  doi      = {10.1109/TAFFC.2020.3039684},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {498-508},
  title    = {Computation of sensory-affective relationships depending on material categories of pictorial stimuli},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auditory feedback of false heart rate for video game
experience improvement. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 487–497. (<a
href="https://doi.org/10.1109/TAFFC.2020.3039874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Changes in emotions affect our physiological responses, and perhaps vice versa. We investigate a new game interaction system that uses false heart rate (fHR) feedback to improve the player experience (PX). The fHR feedback presents false HR information to players so that they perceive changes in the presented HR as being a result of alteration in PX. We introduced auditory fHR feedback into game interaction and investigated its effects through an experiment. Participants repeated gameplay of an action game while hearing heartbeat-like sounds and answered questionnaires regarding PX. Some participants heard the heartbeat-like sounds synchronized with their actual HR, whereas others heard the heartbeat-like sounds whose tempo became gradually faster or slower than their actual HR. The results indicated that an accelerating fHR feedback pattern with +5 bpm/min was appropriate for improving PX; participants were able to maintain their motivation to continue the game. The experiment also indicated that it is necessary for participants to perceive the presented heartbeat-like sounds as reflecting their actual HR. Participants did not maintain their motivation when they were told that the presented sounds were not correlated with their actual HR. The present work provides new principles for video game interaction design based on physiological measurements.},
  archive  = {J},
  author   = {Sayaka Ogawa and Koichi Fujiwara and Manabu Kano},
  doi      = {10.1109/TAFFC.2020.3039874},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {487-497},
  title    = {Auditory feedback of false heart rate for video game experience improvement},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-label emotion detection via emotion-specified feature
extraction and emotion correlation learning. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 475–486. (<a
href="https://doi.org/10.1109/TAFFC.2020.3034215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual emotion detection is an attractive task while previous studies mainly focused on polarity or single-emotion classification. However, human expressions are complex, and multiple emotions often co-occur with non-negligible emotion correlations. In this paper, a Multi-label Emotion Detection Architecture (MEDA) is proposed to detect all associated emotions expressed in a given piece of text. MEDA is mainly composed of two modules: Multi-Channel Emotion-Specified Feature Extractor (MC-ESFE) and Emotion Correlation Learner (ECorL). MEDA captures underlying emotion-specified features through MC-ESFE module, which is composed of multiple channel-wise ESFE networks. Each channel in MC-ESFE is devoted to the feature extraction of a specified emotion from sentence-level to context-level through a hierarchical structure. With underlying features, emotion correlation learning is implemented through an emotion sequence predictor in ECorL. Furthermore, we define a new loss function: multi-label focal loss. With this loss function, the model can focus more on misclassified positive-negative emotion pairs and improve the overall performance by balancing the prediction of positive and negative emotions. The evaluation of proposed MEDA architecture is carried out on emotional corpus: RenCECps and NLPCC2018 datasets. The experimental results indicate that the proposed method can achieve better performance than state-of-the-art methods in this task.},
  archive  = {J},
  author   = {Jiawen Deng and Fuji Ren},
  doi      = {10.1109/TAFFC.2020.3034215},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {475-486},
  title    = {Multi-label emotion detection via emotion-specified feature extraction and emotion correlation learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). To be or not to be in flow at work: Physiological
classification of flow using machine learning. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 463–474. (<a
href="https://doi.org/10.1109/TAFFC.2020.3045269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The focal role of flow in promoting desirable outcomes in companies, such as increased employees’ well-being and performance, led scholars to study flow in the context of work. However, current measurement approaches which assess flow via self-report scales after task execution are limited due to obtrusiveness and a lack of real-time support. Hence, new measurement approaches must be created to overcome these limitations. In this article, we use cardiac features (heart rate variability; HRV) and a Random Forest classifier to distinguish high and low flow. Our results from a large-scale lab experiment with 158 participants and a field study with nine participants reveal, that with HRV features alone, flow-classifiers can be built with an accuracy of 68.5 percent (lab) and 70.6 percent (field). Our research contributes to the challenge of developing a less obtrusive, real-time measurement method of flow based on physiological features and to investigate flow from a physiological perspective. Our findings may serve as foundation for future work aiming to build physio-adaptive systems which can improve employee&#39;s performance. For instance, these systems could ensure that no notifications are forwarded to employees when they are ‘sensing’ flow.},
  archive  = {J},
  author   = {Raphael Rissler and Mario Nadj and Maximilian X. Li and Nico Loewe and Michael T. Knierim and Alexander Maedche},
  doi      = {10.1109/TAFFC.2020.3045269},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {463-474},
  title    = {To be or not to be in flow at work: Physiological classification of flow using machine learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition in the wild using multi-level
features and attention mechanisms. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 451–462. (<a
href="https://doi.org/10.1109/TAFFC.2020.3031602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Learning discriminative features is of vital importance for automatic facial expression recognition (FER) in the wild. In this article, we propose a novel Slide-Patch and Whole-Face Attention model with SE blocks (SPWFA-SE), which jointly perceives the discriminative locality characteristics and informative global features of the face for effective FER. Specifically, the well-designed slide patches are proposed to extract local features. Different from the existing methods, our slide patches not only can maintain the information at the edge area of patches, but also do not need to detect facial landmarks. Moreover, to make the model adaptively focus on the distinguishable regions, an attention module is proposed in the patch level to learn the weight of each patch. Furthermore, squeeze-and-excitation blocks are explored in the channel level to learn the weight of each channel. As such, the proposed multi-level feature extraction and attention mechanisms can enhance the representative ability of the learned features. Extensive experiments on five challenging datasets demonstrate that our method can achieve state-of-the-art performance. Cross database experiments on another three databases show the superior generalization performance of our model. Furthermore, complexity analysis results show that our model contains fewer parameters with fast training advantages than other competing models.},
  archive  = {J},
  author   = {Yingjian Li and Guangming Lu and Jinxing Li and Zheng Zhang and David Zhang},
  doi      = {10.1109/TAFFC.2020.3031602},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {451-462},
  title    = {Facial expression recognition in the wild using multi-level features and attention mechanisms},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A transfer learning approach to heatmap regression for
action unit intensity estimation. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 436–450. (<a
href="https://doi.org/10.1109/TAFFC.2021.3061605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Action Units (AUs) are geometrically-based atomic facial muscle movements known to produce appearance changes at specific facial locations. Motivated by this observation we propose a novel AU modelling problem that consists of jointly estimating their localisation and intensity. To this end, we propose a simple yet efficient approach based on Heatmap Regression that merges both problems into a single task. A Heatmap models whether an AU occurs or not at a given spatial location. To accommodate the joint modelling of AUs intensity, we propose variable size heatmaps, with their amplitude and size varying according to the labelled intensity. Using Heatmap Regression, we can inherit from the progress recently witnessed in facial landmark localisation. Building upon the similarities between both problems, we devise a transfer learning approach where we exploit the knowledge of a network trained on large-scale facial landmark datasets. In particular, we explore different alternatives for transfer learning through a) fine-tuning, b) adaptation layers, c) attention maps, and d) reparametrisation. Our approach effectively inherits the rich facial features produced by a strong face alignment network, with minimal extra computational cost. We empirically validate that our system sets a new state-of-the-art on three popular datasets, namely BP4D, DISFA, and FERA2017.},
  archive  = {J},
  author   = {Ioanna Ntinou and Enrique Sanchez and Adrian Bulat and Michel Valstar and Georgios Tzimiropoulos},
  doi      = {10.1109/TAFFC.2021.3061605},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {436-450},
  title    = {A transfer learning approach to heatmap regression for action unit intensity estimation},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG feature selection via global redundancy minimization for
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 421–435. (<a
href="https://doi.org/10.1109/TAFFC.2021.3068496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A common drawback of EEG-based emotion recognition is that volume conduction effects of the human head introduce interchannel dependence and result in highly correlated information among most EEG features. These highly correlated EEG features cannot provide extra useful information, and they actually reduce the performance of emotion recognition. However, the existing feature selection methods, commonly used to remove redundant EEG features for emotion recognition, ignore the correlation between the EEG features or utilize a greedy strategy to evaluate the interdependence, which leads to the algorithms retaining the correlated and redundant features with similar feature scores in the EEG feature subset. To solve this problem, we propose a novel EEG feature selection method for emotion recognition, termed global redundancy minimization in orthogonal regression (GRMOR). GRMOR can effectively evaluate the dependence among all EEG features from a global view and then select a discriminative and nonredundant EEG feature subset for emotion recognition. To verify the performance of GRMOR, we utilized three EEG emotional data sets (DEAP, SEED, and HDED) with different numbers of channels (32, 62, and 128). The experimental results demonstrate that GRMOR is a promising tool for redundant feature removal and informative feature selection from highly correlated EEG features.},
  archive  = {J},
  author   = {Xueyuan Xu and Tianyuan Jia and Qing Li and Fulin Wei and Long Ye and Xia Wu},
  doi      = {10.1109/TAFFC.2021.3068496},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {421-435},
  title    = {EEG feature selection via global redundancy minimization for emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does visual self-supervision improve learning of speech
representations for emotion recognition? <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 406–420. (<a
href="https://doi.org/10.1109/TAFFC.2021.3062406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Self-supervised learning has attracted plenty of recent research interest. However, most works for self-supervision in speech are typically unimodal and there has been limited work that studies the interaction between audio and visual modalities for cross-modal self-supervision. This article (1) investigates visual self-supervision via face reconstruction to guide the learning of audio representations; (2) proposes an audio-only self-supervision approach for speech representation learning; (3) shows that a multi-task combination of the proposed visual and audio self-supervision is beneficial for learning richer features that are more robust in noisy conditions; (4) shows that self-supervised pretraining can outperform fully supervised training and is especially useful to prevent overfitting on smaller sized datasets. We evaluate our learned audio representations for discrete emotion recognition, continuous affect recognition and automatic speech recognition. We outperform existing self-supervised methods for all tested downstream tasks. Our results demonstrate the potential of visual self-supervision for audio feature learning and suggest that joint visual and audio self-supervision leads to more informative audio representations for speech and emotion recognition.},
  archive  = {J},
  author   = {Abhinav Shukla and Stavros Petridis and Maja Pantic},
  doi      = {10.1109/TAFFC.2021.3062406},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {406-420},
  title    = {Does visual self-supervision improve learning of speech representations for emotion recognition?},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based online regulation of difficulty in simulated
flying. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 394–405. (<a
href="https://doi.org/10.1109/TAFFC.2021.3059688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Adaptively increasing the difficulty level in learning was shown to be beneficial than increasing the level after some fixed time intervals. To efficiently adapt the level, we aimed at decoding the subjective difficulty level based on Electroencephalography (EEG) signals. We designed a visuomotor learning task that one needed to pilot a simulated drone through a series of waypoints of different sizes, to investigate the effectiveness of the EEG decoder. The EEG decoder was compared with another condition that the subjects decided when to increase the difficulty level. We examined the decoding performance together with behavioral outcomes. The online accuracies were higher than the chance level for 16 out of 26 cases, and the behavioral results, such as task scores, skill curves, and learning patterns, of EEG condition were similar to the condition based on manual regulation of difficulty.},
  archive  = {J},
  author   = {Ping-Keng Jao and Ricardo Chavarriaga and José del R. Millán},
  doi      = {10.1109/TAFFC.2021.3059688},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {394-405},
  title    = {EEG-based online regulation of difficulty in simulated flying},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based emotion recognition via channel-wise attention and
self attention. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 382–393. (<a
href="https://doi.org/10.1109/TAFFC.2020.3025777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition based on electroencephalography (EEG) is a significant task in the brain-computer interface field. Recently, many deep learning-based emotion recognition methods are demonstrated to outperform traditional methods. However, it remains challenging to extract discriminative features for EEG emotion recognition, and most methods ignore useful information in channel and time. This article proposes an attention-based convolutional recurrent neural network (ACRNN) to extract more discriminative features from EEG signals and improve the accuracy of emotion recognition. First, the proposed ACRNN adopts a channel-wise attention mechanism to adaptively assign the weights of different channels, and a CNN is employed to extract the spatial information of encoded EEG signals. Then, to explore the temporal information of EEG signals, extended self-attention is integrated into an RNN to recode the importance based on intrinsic similarity in EEG signals. We conducted extensive experiments on the DEAP and DREAMER databases. The experimental results demonstrate that the proposed ACRNN outperforms state-of-the-art methods.},
  archive  = {J},
  author   = {Wei Tao and Chang Li and Rencheng Song and Juan Cheng and Yu Liu and Feng Wan and Xun Chen},
  doi      = {10.1109/TAFFC.2020.3025777},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {382-393},
  title    = {EEG-based emotion recognition via channel-wise attention and self attention},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-target positive emotion recognition from EEG signals.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
370–381. (<a href="https://doi.org/10.1109/TAFFC.2020.3043135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Compared with the widely studied negative emotions in which different classes are easy to distinguish, nowadays less attention is paid to the recognition of positive emotions that are not fully independent. In this article, we propose to recognize multiple continuous positive emotions that exhibit statistical dependencies using multi-target regression — by analyzing brain activities when an individual watches emotional film clips — and explore the neural representation of different positive emotions. Thirty-seven participants volunteered to participate in our study, in which their brain activities were recorded when watching five selected film clips (corresponding to five positive emotions: amusement, happiness, romance, tenderness and warmth). First, 150 well-known power features extracted from Electroencephalography (EEG) signals and 105 multimedia content analysis features were collected as the pool of candidate features. Second, based on the collected features, we propose to use a linear model (linear regression) and a nonlinear model (long short-term memory network, LSTM) to predict the percentage of five positive emotions. Then, percentage values were converted to ranking numbers and Kendall rank correlation coefficients were calculated. Our results showed that (1) ensemble of regressor chains (ERC) using LSTM as unit regressor obtained both the best regression results (with lowest RMSE = 8.325 and highest $\text{R }^{2} = 0.346$ ) and the best Kendall rank correlation coefficient (0.165) on EEG features merely, and (2) selective features from alpha frequency bands of EEG signals could represent different positive emotions. These results demonstrate the effectiveness of selective EEG features on recognizing different positive emotions.},
  archive  = {J},
  author   = {Guozhen Zhao and Yulin Zhang and Guanhua Zhang and Dan Zhang and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2020.3043135},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {370-381},
  title    = {Multi-target positive emotion recognition from EEG signals},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotional attention detection and correlation exploration
for image emotion distribution learning. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 357–369. (<a
href="https://doi.org/10.1109/TAFFC.2021.3071131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current works on image emotion distribution learning typically extract visual representations from the holistic image or explore emotion-related regions in the image from a global-wise perspective. However, different regions of an image contribute differently to the arousal of each emotion. Existing works do not deeply explore corresponding emotion-aware regions of each emotion in the image, nor do they fully capture the relationship between each emotion-aware region and the emotion labels. In this article, we propose a novel attention based emotion distribution learning method, which can explore the emotion-related regions of images from the perspective of each emotion category, and can conduct region relationship learning. Specifically, we introduce a semantic guided attention detection network to generate class-wise attention maps for each emotion and a global-wise attention map for the holistic image. Meanwhile, an emotional graph-based network is adopted to capture the correlation between each region and the emotion distribution. Experiments on several benchmark datasets demonstrate the superiority of the proposed method compared to related works.},
  archive  = {J},
  author   = {Zhiwei Xu and Shangfei Wang},
  doi      = {10.1109/TAFFC.2021.3071131},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {357-369},
  title    = {Emotional attention detection and correlation exploration for image emotion distribution learning},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational instance-adaptive graph for EEG emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 343–356. (<a
href="https://doi.org/10.1109/TAFFC.2021.3064940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The individual differences and the dynamic uncertain relationships among different electroencephalogram (EEG) regions are essential factors that limit EEG emotion recognition. To address these issues, in this article, we propose a variational instance-adaptive graph method (V-IAG) that simultaneously captures the individual dependencies among different EEG electrodes and estimates the underlying uncertain information. Specifically, we employ two branches, i.e., instance-adaptive branch and variational branch, to construct the graph. Inspired by the attention mechanism, the instance-adaptive branch generates the graph based on the input so as to characterize the individual dependencies among EEG channels. The variational branch generates the probabilistic graph, which quantifies the uncertainties. We combine these two types of graphs to extract more discriminative features. To present more precise graph representation, we propose a new operation named the multi-level and multi-graph convolution operation, which aggregates the features of EEG channels from different frequencies with different graphs. Furthermore, we design the graph coarsening and employ the sparse constraint to obtain more robust features. We conduct extensive experiments on three widely-used EEG emotion recognition databases, i.e., SJTU emotion EEG dataset (SEED), multi-modal physiological emotion recognition dataset (MPED) and DREAMER. The results demonstrate that the proposed model achieves the-state-of-the-art performance.},
  archive  = {J},
  author   = {Tengfei Song and Suyuan Liu and Wenming Zheng and Yuan Zong and Zhen Cui and Yang Li and Xiaoyan Zhou},
  doi      = {10.1109/TAFFC.2021.3064940},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {343-356},
  title    = {Variational instance-adaptive graph for EEG emotion recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Touching virtual humans: Haptic responses reveal the
emotional impact of affective agents. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 331–342. (<a
href="https://doi.org/10.1109/TAFFC.2020.3038137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interpersonal touch is critical for social-emotional development and presents a powerful modality for communicating emotions. Virtual agents of the future could capitalize on touch to establish social bonds with humans and facilitate cooperation in virtual reality (VR). We studied whether the emotional expression of a virtual agent would affect the way humans touch the agent. Participants were asked to hold a pressure-sensing tube presented as the agent’s arm in VR. Upon seeing the agent’s emotional expression change, participants briefly squeezed the arm. The effect of emotional expressions on affective state was measured using self-reported valence and arousal as well as physiology-based indices. Onset, duration, and intensity of the squeeze were recorded to examine the haptic responses. Emotional expression of agents affected squeeze intensity and duration through changes in emotional perception and experience. Haptic responses may thus provide an implicit measure of persons’ experience towards their virtual companion.},
  archive  = {J},
  author   = {Imtiaj Ahmed and Ville J. Harjunen and Giulio Jacucci and Niklas Ravaja and Tuukka Ruotsalo and Michiel M. Spapé},
  doi      = {10.1109/TAFFC.2020.3038137},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {331-342},
  title    = {Touching virtual humans: Haptic responses reveal the emotional impact of affective agents},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of interbeat interval time-series using
attention entropy. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 321–330. (<a
href="https://doi.org/10.1109/TAFFC.2020.3031004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Classification of interbeat interval time-series which fluctuates in an irregular and complex manner is very challenging. Typically, entropy methods are employed to quantify the complexity of the time-series for classifying. Traditional entropy methods focus on the frequency distribution of all the observations in a time-series. This requires a relatively long time-series with at least a couple of thousands of data points, which limits their usages in practical applications. The methods are also sensitive to the parameter settings. In this paper, we propose a conceptually new approach called attention entropy , which pays attention only to the key observations. Instead of counting the frequency of all observations, it analyzes the frequency distribution of the intervals between the key observations in a time-series. Attention entropy does not need any parameter to tune, it is robust to the time-series length, and requires only linear time to compute. Experiments show that it outperforms fourteen state-of-the-art entropy methods evaluated by real-world datasets. It achieves average classification accuracy of AUC = 0.71 while the second-best method, multiscale entropy, achieves AUC = 0.62 when classifying four groups of people with a time-series length of 100.},
  archive  = {J},
  author   = {Jiawei Yang and Gulraiz I. Choudhary and Susanto Rahardja and Pasi Fränti},
  doi      = {10.1109/TAFFC.2020.3031004},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {321-330},
  title    = {Classification of interbeat interval time-series using attention entropy},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When and why static images are more effective than videos.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
308–320. (<a href="https://doi.org/10.1109/TAFFC.2020.3040399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {People often prefer videos over images in research and applications, believing that videos are more effective for eliciting human emotions and building machine intelligence. However, our research shows that this assumption is not always correct when it comes to evoking emotions in human observers. In this article, we compare thirteen emotions and two perceptions elicited by short videos (2-6 second, silent video clips) versus static frames extracted from the videos. We show that static frames and videos elicit most emotions similarly, but static frames elicit negative emotions more strongly than videos. We test two complementary explanations: differential activation of suspense and the peak-end rule. These findings help us to computationally model human reactions more faithfully with fewer video frames. Our interdisciplinary results have important implications for methods, theory, and applications in diverse fields, including social psychology, computer vision, mass media, and marketing.},
  archive  = {J},
  author   = {Shaojing Fan and Zhiqi Shen and Bryan L. Koenig and Tian-Tsong Ng and Mohan S. Kankanhalli},
  doi      = {10.1109/TAFFC.2020.3040399},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {308-320},
  title    = {When and why static images are more effective than videos},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal spatiotemporal representation for automatic
depression level detection. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 294–307. (<a
href="https://doi.org/10.1109/TAFFC.2020.3031345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Physiological studies have shown that there are some differences in speech and facial activities between depressive and healthy individuals. Based on this fact, we propose a novel spatio-temporal attention (STA) network and a multimodal attention feature fusion (MAFF) strategy to obtain the multimodal representation of depression cues for predicting the individual depression level. Specifically, we first divide the speech amplitude spectrum/video into fixed-length segments and input these segments into the STA network, which not only integrates the spatial and temporal information through attention mechanism, but also emphasizes the audio/video frames related to depression detection. The audio/video segment-level feature is obtained from the output of the last full connection layer of the STA network. Second, this article employs the eigen evolution pooling method to summarize the changes of each dimension of the audio/video segment-level features to aggregate them into the audio/video level feature. Third, the multimodal representation with modal complementary information is generated using the MAFF and inputs into the support vector regression predictor for estimating depression severity. Experimental results on the AVEC2013 and AVEC2014 depression databases illustrate the effectiveness of our method.},
  archive  = {J},
  author   = {Mingyue Niu and Jianhua Tao and Bin Liu and Jian Huang and Zheng Lian},
  doi      = {10.1109/TAFFC.2020.3031345},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {294-307},
  title    = {Multimodal spatiotemporal representation for automatic depression level detection},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedding refinement framework for targeted aspect-based
sentiment analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 279–293. (<a
href="https://doi.org/10.1109/TAFFC.2021.3071388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The state-of-the-art approaches to targeted aspect-based sentiment analysis (TABSA) are mostly built on deep neural networks with attention mechanisms. One problem is that embeddings of targets and aspects are either pre-trained from large external corpora or randomly initialized. We argue that affective commonsense knowledge and words indicative of sentiment could be used to learn better target and aspect embeddings. We therefore propose an embedding refinement framework called RAEC ( R efining A ffective E mbedding from C ontext), in which sentiment concepts extracted from affective commonsense knowledge and word relative location information are incorporated to derive context-affective embeddings. Furthermore, a sparse coefficient vector is exploited in refining the embeddings of targets and aspects separately. In this way, embeddings of targets and aspects can capture the highly relevant affective words. Experimental results on two benchmark datasets show that our framework can be easily integrated with existing embedding-based TABSA models and achieves state-of-the-art results compared to models relying on pre-trained word embeddings or built on other embedding refinement methods.},
  archive  = {J},
  author   = {Bin Liang and Rongdi Yin and Jiachen Du and Lin Gui and Yulan He and Min Yang and Ruifeng Xu},
  doi      = {10.1109/TAFFC.2021.3071388},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {279-293},
  title    = {Embedding refinement framework for targeted aspect-based sentiment analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Silicon coppélia and the formalization of the affective
process. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 255–278. (<a
href="https://doi.org/10.1109/TAFFC.2020.3048587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {After 20 years of testing a framework for affective user responses to artificial agents and robots, we compiled a full formalization of our findings so to make the agent respond affectively to its user. Silicon Coppélia as we dubbed our system works from the features of the observed other, appraises these in various domains (e.g., ethics and affordances), then compares them to goals and concerns of the agent, to finally reach a response that includes intentions to work with the user as well as a level of being engaged with the user. This ultimately results in an action that adds to or changes the situation both agencies are in. Unlike many other systems, Silicon Coppélia can deal with ambiguous emotions of its user and has ambiguous ‘feelings’ of its own, which makes its decisions quite human-like. In the current paper, we advance a fuzzy-sets approach and show the inner workings of our system through an elaborate example. We present a number of simulation experiments, one of which showed decision behaviors based on biases when agent goals had low priorities. Silicon Coppélia is open to scrutiny and experimentation by way of an open-source implementation in Ptolemy.},
  archive  = {J},
  author   = {Johan F. Hoorn and Thomas Baier and Jeroen van Maanen and Jeroen Wester},
  doi      = {10.1109/TAFFC.2020.3048587},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {255-278},
  title    = {Silicon coppélia and the formalization of the affective process},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FENP: A database of neonatal facial expression for pain
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 245–254. (<a
href="https://doi.org/10.1109/TAFFC.2020.3030296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we introduce a new neonatal facial expression database for pain analysis. This database, called facial expression of neonatal pain (FENP), contains 11,000 neonatal facial expression images associated with 106 Chinese neonates from two children&#39;s hospitals, i.e., the Children&#39;s Hospital Affiliated to Nanjing Medical University and Second Affiliated Hospital Affiliated to Nanjing Medical University in China. The facial expression images cover four categories of facial expressions, i.e., severe pain expression, mild pain expression, crying expression and calmness expression, where each category contains 2750 neonatal facial expression images. Based on this database, we also investigate the pain facial expression recognition problem using several state-of-the-art facial expression features and expression recognition methods, such as Gabor+SVM, LBP+SVM, HOG+SVM, LBP+HOG+SVM, and several Convolutional Neural Network (CNN) methods (including AlexNet, VGGNet, GoogLeNet, ResNet and DenseNet). The experimental results indicate that the proposed neonatal pain facial expression database is very suitable for the study of both neonatal pain and facial expression recognition. Moreover, the FENP database is publicly available after signing a license agreement (the users can contact Jingjie Yan (yanjingjie@njupt.edu.cn), Guanming Lu (lugm@njupt.edu.cn)) or Xiaonan Li (xnli@njmu.edu.cn).},
  archive  = {J},
  author   = {Jingjie Yan and Guanming Lu and Xiaonan Li and Wenming Zheng and Chengwei Huang and Zhen Cui and Yuan Zong and Mengying Chen and Qiang Hao and Yi Liu and Jindu Zhu and Haibo Li},
  doi      = {10.1109/TAFFC.2020.3030296},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {245-254},
  title    = {FENP: A database of neonatal facial expression for pain analysis},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-modal stacked ensemble model for bipolar disorder
classification. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 236–244. (<a
href="https://doi.org/10.1109/TAFFC.2020.3047582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose an automatic ternary classification model for Bipolar Disorder (BD) states. As input information, the model uses speech signals from patients’ audio-visual recordings of structured interviews. The model classifies the patient&#39;s clinical state as Mania, Hypo-Mania, or Remission. We capture Mel-Frequency Cepstral Coefficients (MFCCs) and Geneva Minimalistic Acoustic Parameter Set (GeMAPS) as audio features. We compute linguistic and sentiment features for each subject&#39;s transcript. We present a stacked ensemble classifier to classify all fused features after feature selection. A set of three homogeneous Convolutional Neural Networks (CNNs) and a Multi Layer Perceptron (MLP) construct the first-level and second-level of the stacked ensemble classifier respectively. Moreover, we use the Neural Architecture Search (NAS) reinforcement learning strategy to optimize the networks and their hyperparameters. We show that our stacked ensemble framework outperforms existing models on the BD Turkish corpus with a $ 59.3\%$ Unweighted Average Unit (UAR) on the test set. To the best of our knowledge, this is the highest UAR achieved on this dataset.},
  archive  = {J},
  author   = {Niloufar AbaeiKoupaei and Hussein Al Osman},
  doi      = {10.1109/TAFFC.2020.3047582},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {236-244},
  title    = {A multi-modal stacked ensemble model for bipolar disorder classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel computational linguistic measures, dialogue system and
the development of SOPHIE: Standardized online patient for healthcare
interaction education. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 223–235. (<a
href="https://doi.org/10.1109/TAFFC.2021.3054717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we describe the iterative participatory design of SOPHIE, an online virtual patient for feedback-based practice of sensitive patient-physician conversations, and discuss an initial qualitative evaluation of the system by professional end users. The design of SOPHIE was motivated from a computational linguistic analysis of the transcripts of 383 patient-physician conversations from an essential office visit of late stage cancer patients with their oncologists. We developed methods for the automatic detection of two behavioral paradigms, lecturing and positive language usage patterns (sentiment trajectory of conversation), that are shown to be significantly associated with patient prognosis understanding. These automated metrics associated with effective communication were incorporated into SOPHIE, and a pilot user study identified that SOPHIE was favorably reviewed by a user group of practicing physicians.},
  archive  = {J},
  author   = {Mohammad Rafayet Ali and Taylan Sen and Benjamin Kane and Shagun Bose and Thomas M Carroll and Ronald Epstein and Lenhart Schubert and Ehsan Hoque},
  doi      = {10.1109/TAFFC.2021.3054717},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {223-235},
  title    = {Novel computational linguistic measures, dialogue system and the development of SOPHIE: Standardized online patient for healthcare interaction education},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting mental disorders in social media through emotional
patterns - the case of anorexia and depression. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 211–222. (<a
href="https://doi.org/10.1109/TAFFC.2021.3075638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Millions of people around the world are affected by one or more mental disorders that interfere in their thinking and behavior. A timely detection of these issues is challenging but crucial, since it could open the possibility to offer help to people before the illness gets worse. One alternative to accomplish this is to monitor how people express themselves, that is for example what and how they write, or even a step further, what emotions they express in their social media communications. In this article, we analyze two computational representations that aim to model the presence and changes of the emotions expressed by social media users. In our evaluation we use two recent public data sets for two important mental disorders: Depression and Anorexia. The obtained results suggest that the presence and variability of emotions, captured by the proposed representations, allow to highlight important information about social media users suffering from depression or anorexia. Furthermore, the fusion of both representations can boost the performance, equalling the best reported approach for depression and barely behind the top performer for anorexia by only 1 percent. Moreover, these representations open the possibility to add some interpretability to the results.},
  archive  = {J},
  author   = {Mario Ezra Aragón and Adrian Pastor López-Monroy and Luis Carlos González-Gurrola and Manuel Montes-y-Gómez},
  doi      = {10.1109/TAFFC.2021.3075638},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {211-222},
  title    = {Detecting mental disorders in social media through emotional patterns - the case of anorexia and depression},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting dependency-related sentiment features for
aspect-level sentiment classification. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 196–210. (<a
href="https://doi.org/10.1109/TAFFC.2021.3063259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-level sentiment classification aims to determine the sentiment polarity of a sentence toward a given aspect term or aspect category. For sentiment classification toward a given aspect term, some opinions may exist that are not the given aspect term&#39;s modifiers because a sentence may contain more than one aspect term. Hence, It is necessary to capture relevant opinion for a certain aspect term. To capture the nearest opinion of the aspect term, researchers have used the relative distance between an aspect term and all other words in a sentence. However, this can be infeasible when the sentence has a complex syntactic structure. In this paper, we introduce dependency relation to detect the dependency-related sentiment feature for the aspect term in the dependency parse tree, and integrate this relationship into the convolutional neural network and bidirectional long short-term memory. Experiments show that the related sentiment features for an aspect term help models discriminate its sentiment polarity. The proposed models achieve state-of-the-art results among neural networks. The codes and datasets are released on https://github.com/LittleSummer114/DW-CNN .},
  archive  = {J},
  author   = {Xing Zhang and Jingyun Xu and Yi Cai and Xingwei Tan and Changxi Zhu},
  doi      = {10.1109/TAFFC.2021.3063259},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {196-210},
  title    = {Detecting dependency-related sentiment features for aspect-level sentiment classification},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of person-specific facial dynamics
for automatic personality recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 178–195. (<a
href="https://doi.org/10.1109/TAFFC.2021.3064601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article aims to solve two important issues that frequently occur in existing automatic personality analysis systems: 1. Attempting to use very short video segments or even single frames, rather than long-term behaviour, to infer personality traits; 2. Lack of methods to encode person-specific facial dynamics for personality recognition. To deal with these issues, this paper first proposes a novel Rank Loss which utilizes the natural temporal evolution of facial actions, rather than personality labels, for self-supervised learning of facial dynamics. Our approach first trains a generic U-net style model that can infer general facial dynamics learned from a set of unlabelled face videos. Then, the generic model is frozen, and a set of intermediate filters are incorporated into this architecture. The self-supervised learning is then resumed with only person-specific videos. This way, the learned filters’ weights are person-specific, making them a valuable source for modeling person-specific facial dynamics. We then propose to concatenate the weights of the learned filters as a person-specific representation, which can be directly used to predict the personality traits without needing other parts of the network. We evaluate the proposed approach on both self-reported personality and apparent personality datasets. In addition to achieving promising results in the estimation of personality trait scores from videos, we show that the tasks conducted by the subject in the video matters, that fusion of a combination of tasks reaches highest accuracy, and that multi-scale dynamics are more informative than single-scale dynamics.},
  archive  = {J},
  author   = {Siyang Song and Shashank Jaiswal and Enrique Sanchez and Georgios Tzimiropoulos and Linlin Shen and Michel Valstar},
  doi      = {10.1109/TAFFC.2021.3064601},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {178-195},
  title    = {Self-supervised learning of person-specific facial dynamics for automatic personality recognition},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The analysis of driver’s behavioral tendency under different
emotional states based on a bayesian network. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 165–177. (<a
href="https://doi.org/10.1109/TAFFC.2020.3027720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective factors have been associated with an array of driving behaviors. However, the mechanism by which emotion influences driving behaviors remains largely unknown. In the present study, a probabilistic approach for characterizing the emotional influence on driving behavioral decision-making was proposed. First, a series of experiments were designed to obtain the human-vehicle-environment data when the drivers, whose emotional states were effectively induced, were driving a vehicle in the car-following scene. Next, a Bayesian Network (BN) was developed to model the causal relationships between driving behavioral tendency, driver&#39;s emotion and other indicators related to vehicle driving. Finally, the driver&#39;s different tendencies in driving behavior caused by emotional factors were analyzed through calculating the probability distribution of driving behavioral tendency under different emotional conditions in the BN. The research results are not only beneficial to improve the accuracy of driving behavior identification and prediction which is of great significance for vehicle active safety, but also have a potential promoting effect on human-computer harmonious interaction of vehicle system.},
  archive  = {J},
  author   = {Ya-Qi Liu and Xiao-Yuan Wang},
  doi      = {10.1109/TAFFC.2020.3027720},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {165-177},
  title    = {The analysis of driver&#39;s behavioral tendency under different emotional states based on a bayesian network},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multiscale recurrent neural networks for
detecting suicide notes. <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 153–164. (<a
href="https://doi.org/10.1109/TAFFC.2021.3057105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent statistics in suicide prevention show that people are increasingly posting their last words online and with the unprecedented availability of textual data from social media platforms researchers have the opportunity to analyse such data. Furthermore, psychological studies have shown that our state of mind can manifest itself in the linguistic features we use to communicate. In this article, we investigate whether it is possible to automatically identify suicide notes from other types of social media blogs in two document-level classification tasks. The first task aims to identify suicide notes from depressed and blog posts in a balanced dataset, whilst the second experiment looks at how well suicide notes can be classified when there is a vast amount of neutral text data, which makes the task more applicable to real-world scenarios. Furthermore, we perform a linguistic analysis using LIWC (Linguistic Inquiry and Word Count). We present a learning model for modelling long sequences in two experiment series. We achieve an f1-score of 88.26 percent over the baselines of 0.60 in experiment 1 and 96.1 percent over the baseline in experiment 2. Finally, we show through visualisations which features the learning model identifies, these include emotions such as love and personal pronouns.},
  archive  = {J},
  author   = {Annika Marie Schoene and Alexander P. Turner and Geeth De Mel and Nina Dethlefs},
  doi      = {10.1109/TAFFC.2021.3057105},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {153-164},
  title    = {Hierarchical multiscale recurrent neural networks for detecting suicide notes},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretation of depression detection models via feature
selection methods. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 133–152. (<a
href="https://doi.org/10.1109/TAFFC.2020.3035535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Given the prevalence of depression worldwide and its major impact on society, several studies employed artificial intelligence modelling to automatically detect and assess depression. However, interpretation of these models and cues are rarely discussed in detail in the AI community, but have received increased attention lately. In this article, we aim to analyse the commonly selected features using a proposed framework of several feature selection methods and their effect on the classification results, which will provide an interpretation of the depression detection model. The developed framework aggregates and selects the most promising features for modelling depression detection from 38 feature selection algorithms of different categories. Using three real-world depression datasets, 902 behavioural cues were extracted from speech behaviour, speech prosody, eye movement and head pose. To verify the generalisability of the proposed framework, we applied the entire process to depression datasets individually and when combined. The results from the proposed framework showed that speech behaviour features (e.g. pauses) are the most distinctive features of the depression detection model. From the speech prosody modality, the strongest feature groups were F0, HNR, formants, and MFCC, while for the eye activity modality they were left-right eye movement and gaze direction, and for the head modality it was yaw head movement. Modelling depression detection using the selected features (even though there are only 9 features) outperformed using all features in all the individual and combined datasets. Our feature selection framework did not only provide an interpretation of the model, but was also able to produce a higher accuracy of depression detection with a small number of features in varied datasets. This could help to reduce the processing time needed to extract features and creating the model.},
  archive  = {J},
  author   = {Sharifa Alghowinem and Tom Gedeon and Roland Goecke and Jeffrey F. Cohn and Gordon Parker},
  doi      = {10.1109/TAFFC.2020.3035535},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {133-152},
  title    = {Interpretation of depression detection models via feature selection methods},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beneath the tip of the iceberg: Current challenges and new
directions in sentiment analysis research. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 108–132. (<a
href="https://doi.org/10.1109/TAFFC.2020.3038167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sentiment analysis as a field has come a long way since it was first introduced as a task nearly 20 years ago. It has widespread commercial applications in various domains like marketing, risk management, market research, and politics, to name a few. Given its saturation in specific subtasks — such as sentiment polarity classification — and datasets, there is an underlying perception that this field has reached its maturity. In this article, we discuss this perception by pointing out the shortcomings and under-explored, yet key aspects of this field necessary to attain true sentiment understanding. We analyze the significant leaps responsible for its current relevance. Further, we attempt to chart a possible course for this field that covers many overlooked and unanswered questions.},
  archive  = {J},
  author   = {Soujanya Poria and Devamanyu Hazarika and Navonil Majumder and Rada Mihalcea},
  doi      = {10.1109/TAFFC.2020.3038167},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {108-132},
  title    = {Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic emotion recognition for groups: A review. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 89–107. (<a
href="https://doi.org/10.1109/TAFFC.2021.3065726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article aims to summarize and describe research on the topic of automatic group emotion recognition. In recent years, the topic of emotion analysis of groups or crowds has gained interest, with studies performing emotion detection in different contexts, using different datasets and modalities (such as images, video, audio, social media messages), and taking different approaches. Articles are included after an innovative search method, including Dense Query Extraction and automatic cross-referencing. Discussed are the types of groups and emotion models considered in automatic emotion recognition research, common datasets for all modalities, general approaches taken, and reported performances. These performances are discussed, followed by an analysis of the application possibilities of the discussed methods. To ensure clear, replicable, and comparable studies, we suggest research should test on multiple, common datasets and report on multiple metrics, when possible. Implementation details and code should be made available where possible. An area of interest for future work is to build systems with more real-world application possibilities, coping with changing group sizes, different emotional subgroups, and changing emotions over time, while having a higher robustness and working with datasets with reduced biases.},
  archive  = {J},
  author   = {Emmeke A. Veltmeijer and Charlotte Gerritsen and Koen V. Hindriks},
  doi      = {10.1109/TAFFC.2021.3065726},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {89-107},
  title    = {Automatic emotion recognition for groups: A review},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio features for music emotion recognition: A survey.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
68–88. (<a href="https://doi.org/10.1109/TAFFC.2020.3032373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The design of meaningful audio features is a key need to advance the state-of-the-art in music emotion recognition (MER). This article presents a survey on the existing emotionally-relevant computational audio features, supported by the music psychology literature on the relations between eight musical dimensions (melody, harmony, rhythm, dynamics, tone color, expressivity, texture and form) and specific emotions. Based on this review, current gaps and needs are identified and strategies for future research on feature engineering for MER are proposed, namely ideas for computational audio features that capture elements of musical form, texture and expressivity that should be further researched. Previous MER surveys offered broad reviews, covering topics such as emotion paradigms, approaches for the collection of ground-truth data, types of MER problems and overviewing different MER systems. On the contrary, our approach is to offer a deep and specific review on one key MER problem: the design of emotionally-relevant audio features.},
  archive  = {J},
  author   = {Renato Panda and Ricardo Malheiro and Rui Pedro Paiva},
  doi      = {10.1109/TAFFC.2020.3032373},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {68-88},
  title    = {Audio features for music emotion recognition: A survey},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A survey of textual emotion recognition and its challenges.
<em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1),
49–67. (<a href="https://doi.org/10.1109/TAFFC.2021.3053275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual language is the most natural carrier of human emotion. In natural language processing, textual emotion recognition (TER) has become an important topic due to its significant academic and commercial potential. With the advanced development of deep learning technologies, TER has attracted growing attention and has significantly been promoted in recent years. This article provides a systematic survey of the latest TER advances, focusing on approaches using deep neural networks. According to how deep learning works at each stage, TER approaches are reviewed on word embedding, architecture, and training levels, respectively. We discussed the remaining challenges and opportunities from four aspects: the shortage of large-scale and high-quality datasets, fuzzy emotional boundaries, incomplete extractable emotional information in texts, and TER in dialogue. This article creates a systematic and in-depth overview of deep TER technologies. It provides the necessary knowledge and new insights for relevant researchers to understand better the research state, remaining challenges, and future directions in this field.},
  archive  = {J},
  author   = {Jiawen Deng and Fuji Ren},
  doi      = {10.1109/TAFFC.2021.3053275},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {49-67},
  title    = {A survey of textual emotion recognition and its challenges},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Emotion intensity and its control for emotional voice
conversion. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 31–48. (<a
href="https://doi.org/10.1109/TAFFC.2022.3175578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional voice conversion (EVC) seeks to convert the emotional state of an utterance while preserving the linguistic content and speaker identity. In EVC, emotions are usually treated as discrete categories overlooking the fact that speech also conveys emotions with various intensity levels that the listener can perceive. In this paper, we aim to explicitly characterize and control the intensity of emotion. We propose to disentangle the speaker style from linguistic content and encode the speaker style into a style embedding in a continuous space that forms the prototype of emotion embedding. We further learn the actual emotion encoder from an emotion-labelled database and study the use of relative attributes to represent fine-grained emotion intensity. To ensure emotional intelligibility, we incorporate emotion classification loss and emotion embedding similarity loss into the training of the EVC network. As desired, the proposed network controls the fine-grained emotion intensity in the output speech. Through both objective and subjective evaluations, we validate the effectiveness of the proposed network for emotional expressiveness and emotion intensity control.},
  archive  = {J},
  author   = {Kun Zhou and Berrak Sisman and Rajib Rana and Björn W. Schuller and Haizhou Li},
  doi      = {10.1109/TAFFC.2022.3175578},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {31-48},
  title    = {Emotion intensity and its control for emotional voice conversion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The acoustically emotion-aware conversational agent with
speech emotion recognition and empathetic responses. <em>IEEE
Transactions on Affective Computing</em>, <em>14</em>(1), 17–30. (<a
href="https://doi.org/10.1109/TAFFC.2022.3205919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion is important for the conversational user interface. In prior research, conversational agents (CAs) employ natural language process techniques to create affective interaction based on text. However, the use of acoustic features of speech for voice-based CAs is under exploration. This work presents an acoustically emotion-aware CA that enables speech emotion recognition and stylizes responses with empathetic feedback and interjections. We conducted an experiment with 75 participants to evaluate their perceived emotional intelligence (PEI) after interacting with the CA. Our results show that the acoustical emotion-awareness increased the participants’ PEI of the CA, and the empathetic responses from the CA helped alleviate some participants’ negative emotions. Our work provides implications for designing future CAs with better PEI.},
  archive  = {J},
  author   = {Jiaxiong Hu and Yun Huang and Xiaozhu Hu and Yingqing Xu},
  doi      = {10.1109/TAFFC.2022.3205919},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {17-30},
  title    = {The acoustically emotion-aware conversational agent with speech emotion recognition and empathetic responses},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hidden bawls, whispers, and yelps: Can text convey the sound
of speech, beyond words? <em>IEEE Transactions on Affective
Computing</em>, <em>14</em>(1), 6–16. (<a
href="https://doi.org/10.1109/TAFFC.2022.3174721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Whether a word was bawled, whispered, or yelped, captions will typically represent it in the same way. If they are your only way to access what is being said, subjective nuances expressed in the voice will be lost. Since so much of communication is carried by these nuances, we posit that if captions are to be used as an accurate representation of speech, embedding visual representations of paralinguistic qualities into captions could help readers use them to better understand speech beyond its mere textual content. This paper presents a model for processing vocal prosody (its loudness, pitch, and duration) and mapping it into visual dimensions of typography (respectively, font-weight, baseline shift, and letter-spacing), creating a visual representation of these lost vocal subtleties that can be embedded directly into the typographical form of text. An evaluation was carried out where participants were exposed to this speech-modulated typography and asked to match it to its originating audio, presented between similar alternatives. Participants (n=117) were able to correctly identify the original audios with an average accuracy of 65%, with no significant difference when showing them modulations as animated or static text. Additionally, participants’ comments showed their mental models of speech-modulated typography varied widely.},
  archive  = {J},
  author   = {Caluã de Lacerda Pataca and Paula Dornhofer Paro Costa},
  doi      = {10.1109/TAFFC.2022.3174721},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {6-16},
  title    = {Hidden bawls, whispers, and yelps: Can text convey the sound of speech, beyond words?},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on affective speech and
language synthesis, generation, and conversion. <em>IEEE Transactions on
Affective Computing</em>, <em>14</em>(1), 3–5. (<a
href="https://doi.org/10.1109/TAFFC.2022.3233120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special section focus on affective speech and language synthesis, generation, and conversion. As an inseparable and crucial part of spoken language, emotions play a substantial role in human-human and human-technology conversation. They convey information about a person’s needs, how one feels about the objectives of a conversation, the trustworthiness of one’s verbal communication, and more. Accordingly, substantial efforts have been made to generate affective text and speech for conversational AI, artificial storytelling, and machine translation. Similarly, there is a push for converting the affect in text and speech, ideally, in real-time and fully preserving intelligibility, e. g., to hide one’s emotion, for creative applications and in entertainment, or even to augment training data for affect analyzing AI.},
  archive  = {J},
  author   = {Shahin Amiriparian and Björn W. Schuller and Nabiha Asghar and Heiga Zen and Felix Burkhardt},
  doi      = {10.1109/TAFFC.2022.3233120},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {3-5},
  title    = {Guest editorial: Special issue on affective speech and language synthesis, generation, and conversion},
  volume   = {14},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial transactions on affective computing–news on the
journal. <em>IEEE Transactions on Affective Computing</em>,
<em>14</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TAFFC.2023.3236419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Elisabeth André},
  doi     = {10.1109/TAFFC.2023.3236419},
  journal = {IEEE Transactions on Affective Computing},
  month   = {1},
  number  = {1},
  pages   = {1-2},
  title   = {Editorial transactions on affective Computing–News on the journal},
  volume  = {14},
  year    = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
