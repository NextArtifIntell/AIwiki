<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---72">DMKD - 72</h2>
<ul>
<li><details>
<summary>
(2023). Column-coherent matrix decomposition. <em>DMKD</em>,
<em>37</em>(6), 2564–2588. (<a
href="https://doi.org/10.1007/s10618-023-00954-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix decomposition is a widely used tool in machine learning with many applications such as dimension reduction or visualization. In this paper we consider decomposing X, a matrix of size $$n \times m$$ , to a product WS where we require that S, a matrix of size $$n \times k$$ , needs to have consecutive ones property. More specifically, we require that each row of S needs to be in the form of $$0, \ldots , 0, 1, \ldots , 1, 0, \ldots , 0$$ . Such decompositions are particularly meaningful if X is a matrix where each row represents a time series; in such a case the ones in each row in S represent a time segment. We show that the optimization problem is inapproximable. To solve the problem we propose 5 different algorithms. The first two algorithms are based on solving iteratively S while keeping W fixed and then solving W while keeping S fixed. The next two algorithms are based on greedily optimizing a single row in S and the corresponding column in W. The last algorithm first finds the optimal decomposition of with $$2k - 1$$ non-overlapping rows, and then greedily combines the rows until k rows remain. We compare the algorithms experimentally, focusing on the quality of the decomposition as well as the computational time. We show experimentally that our algorithms yield interpretable results in practical time.},
  archive      = {J_DMKD},
  author       = {Tatti, Nikolaj},
  doi          = {10.1007/s10618-023-00954-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2564-2588},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Column-coherent matrix decomposition},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable contextual anomaly detection using quantile
regression forests. <em>DMKD</em>, <em>37</em>(6), 2517–2563. (<a
href="https://doi.org/10.1007/s10618-023-00967-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.},
  archive      = {J_DMKD},
  author       = {Li, Zhong and van Leeuwen, Matthijs},
  doi          = {10.1007/s10618-023-00967-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2517-2563},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Explainable contextual anomaly detection using quantile regression forests},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). I-align: An interpretable knowledge graph alignment model.
<em>DMKD</em>, <em>37</em>(6), 2494–2516. (<a
href="https://doi.org/10.1007/s10618-023-00963-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are becoming essential resources for many downstream applications. However, their incompleteness may limit their potential. Thus, continuous curation is needed to mitigate this problem. One of the strategies to address this problem is KG alignment, i.e., forming a more complete KG by merging two or more KGs. This paper proposes i-Align, an interpretable KG alignment model. Unlike the existing KG alignment models, i-Align provides an explanation for each alignment prediction while maintaining high alignment performance. Experts can use the explanation to check the correctness of the alignment prediction. Thus, the high quality of a KG can be maintained during the curation process (e.g., the merging process of two KGs). To this end, a novel Transformer-based Graph Encoder (Trans-GE) is proposed as a key component of i-Align for aggregating information from entities’ neighbors (structures). Trans-GE uses Edge-gated Attention that combines the adjacency matrix and the self-attention matrix to learn a gating mechanism to control the information aggregation from the neighboring entities. It also uses historical embeddings, allowing Trans-GE to be trained over mini-batches, or smaller sub-graphs, to address the scalability issue when encoding a large KG. Another component of i-Align is a Transformer encoder for aggregating entities’ attributes. This way, i-Align can generate explanations in the form of a set of the most influential attributes/neighbors based on attention weights. Extensive experiments are conducted to show the power of i-Align. The experiments include several aspects, such as the model’s effectiveness for aligning KGs, the quality of the generated explanations, and its practicality for aligning large KGs. The results show the effectiveness of i-Align in these aspects.},
  archive      = {J_DMKD},
  author       = {Trisedya, Bayu Distiawan and Salim, Flora D. and Chan, Jeffrey and Spina, Damiano and Scholer, Falk and Sanderson, Mark},
  doi          = {10.1007/s10618-023-00963-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2494-2516},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {I-align: An interpretable knowledge graph alignment model},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the core resilience of real-world hypergraphs.
<em>DMKD</em>, <em>37</em>(6), 2438–2493. (<a
href="https://doi.org/10.1007/s10618-023-00958-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions that involve a group of people or objects are omnipresent in practice. Some examples include the list of recipients of an email, the group of co-authors of a publication, and the users participating in online discussion threads. These interactions are modeled as hypergraphs in which each hyperedge is a set of nodes constituting an interaction. In a hypergraph, the k-core is the sub-hypergraph within which the degree of each node is at least k. Investigating the k-core structures is valuable in revealing some properties of the hypergraph, one of which is the network behavior when facing attacks. Networks in practice are often prone to attacks by which the attacker removes a portion of the nodes or hyperedges to weaken some properties of the networks. The resilience of the k-cores is an indicator of the robustness of the network against such attacks. In this work, we investigate the core resilience of real-world hypergraphs against deletion attacks. How robust are the core structures of real-world hypergraphs in these attack scenarios? Given the complexity of a real-world hypergraph, how should we supplement the hypergraph with augmented hyperedges to enhance its core resilience? In light of several empirical observations regarding core resilience, we present a two-step method that preserves and strengthens the core structures of the hypergraphs.},
  archive      = {J_DMKD},
  author       = {Do, Manh Tuan and Shin, Kijung},
  doi          = {10.1007/s10618-023-00958-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2438-2493},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving the core resilience of real-world hypergraphs},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypercore decomposition for non-fragile hyperedges:
Concepts, algorithms, observations, and applications. <em>DMKD</em>,
<em>37</em>(6), 2389–2437. (<a
href="https://doi.org/10.1007/s10618-023-00956-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs are a powerful abstraction for modeling high-order relations, which are ubiquitous in many fields. A hypergraph consists of nodes and hyperedges (i.e., subsets of nodes); and there have been a number of attempts to extend the notion of $${\varvec{k}}$$ -cores, which proved useful with numerous applications for pairwise graphs, to hypergraphs. However, the previous extensions are based on an unrealistic assumption that hyperedges are fragile, i.e., a high-order relation becomes obsolete as soon as a single member leaves it.In this work, we propose a new substructure model, called $${\varvec{(k,t)}}$$ -hypercore, based on the assumption that high-order relations remain as long as at least t fraction of the members remains. Specifically, it is defined as the maximal subhypergraph where (1) every node is contained in at least $${\varvec{k}}$$ hyperedges in it and (2) at least $${\varvec{t}}$$ fraction of the nodes remain in every hyperedge. We first prove that, given $${\varvec{t}}$$ (or $${\varvec{k}}$$ ), finding the $${\varvec{(k,t)}}$$ -hypercore for every possible $${\varvec{k}}$$ (or $${\varvec{t}}$$ ) can be computed in time linear w.r.t the sum of the sizes of hyperedges. Then, we demonstrate that real-world hypergraphs from the same domain share similar $${\varvec{(k,t)}}$$ -hypercore structures, which capture different perspectives depending on $${\varvec{t}}$$ . Lastly, we show the successful applications of our model in identifying influential nodes, dense substructures, and vulnerability in hypergraphs.},
  archive      = {J_DMKD},
  author       = {Bu, Fanchen and Lee, Geon and Shin, Kijung},
  doi          = {10.1007/s10618-023-00956-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2389-2437},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hypercore decomposition for non-fragile hyperedges: Concepts, algorithms, observations, and applications},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reciprocity in directed hypergraphs: Measures, findings, and
generators. <em>DMKD</em>, <em>37</em>(6), 2330–2388. (<a
href="https://doi.org/10.1007/s10618-023-00955-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group interactions are prevalent in a variety of areas. Many of them, including email exchanges, chemical reactions, and bitcoin transactions, are directional, and thus they are naturally modeled as directed hypergraphs, where each hyperarc consists of the set of source nodes and the set of destination nodes. For directed graphs, which are a special case of directed hypergraphs, reciprocity has played a key role as a fundamental graph statistic in revealing organizing principles of graphs and in solving graph learning tasks. For general directed hypergraphs, however, even no systematic measure of reciprocity has been developed. In this work, we investigate the reciprocity of 11 real-world hypergraphs. To this end, we first introduce eight axioms that any reasonable measure of reciprocity should satisfy. Second, we propose HyperRec, a family of principled measures of hypergraph reciprocity that satisfy all the axioms. Third, we develop FastHyperRec, a fast and exact algorithm for computing the measures. Fourth, using them, we examine 11 real-world hypergraphs and discover patterns that distinguish them from random hypergraphs. Lastly, we propose ReDi, an intuitive generative model for directed hypergraphs exhibiting the patterns.},
  archive      = {J_DMKD},
  author       = {Kim, Sunwoo and Choe, Minyoung and Yoo, Jaemin and Shin, Kijung},
  doi          = {10.1007/s10618-023-00955-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2330-2388},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Reciprocity in directed hypergraphs: Measures, findings, and generators},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trie-nlg: Trie context augmentation to improve personalized
query auto-completion for short and unseen prefixes. <em>DMKD</em>,
<em>37</em>(6), 2306–2329. (<a
href="https://doi.org/10.1007/s10618-023-00966-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches, and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge $$\sim$$ 57% and $$\sim$$ 14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively. We make our code publicly available at https://github.com/kaushal0494/Trie-NLG .},
  archive      = {J_DMKD},
  author       = {Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar and Gupta, Manish and Agrawal, Puneet},
  doi          = {10.1007/s10618-023-00966-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2306-2329},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Trie-nlg: Trie context augmentation to improve personalized query auto-completion for short and unseen prefixes},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An alternative for data visualization using space-filling
curve. <em>DMKD</em>, <em>37</em>(6), 2281–2305. (<a
href="https://doi.org/10.1007/s10618-023-00943-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction helps data analysts and machine learning designers to visualize in low dimension structures lying in high dimension. This is a basic but crucial operation, to discover relationship between variables, considering the difficulties to tweek machine learning algorithm. The data have not to be consider as a black-box but can be visualized, leading to better decision making. Inspired from previous works, this article proposes to create a dimensionality reduction method based on space-filling curves (SFCs). Of course, the Hilbert curve was considered (guided by reflected binary gray code pattern) but also alternative high locality SFCs, recently identified. Mapping algorithms working with alternative curves are provided, and illustrated through a numerical example. Mapping a D-dimensional point to a 1D index is usual but developing an algorithm for reverse mapping, i.e. from 1D index to 2D or 3D point is more original and can allow the visualization of data. The work position is specified and justifications are given. A discussion on the choice of parameters (order of curves n and $$n&#39;$$ ) is led in order to guide the user to select good parameters (to define a bijection between original data space and projected space). Experiments are conducted to compare our proposition to state of the art approaches (PCA, MDS, t-SNE, UMAP) over seven dataset involving from 3D to 16D and covering diverse topologies. The results show interesting ability on data visualization. Compare to standard techniques, the time computing is low, which is an interesting property in regards to the amount of data today created.},
  archive      = {J_DMKD},
  author       = {Owczarek, Valentin and Franco, Patrick and Mullot, Rémy},
  doi          = {10.1007/s10618-023-00943-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2281-2305},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An alternative for data visualization using space-filling curve},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Symmetry properties and asymmetry evaluation of bayesian
confirmation measures. <em>DMKD</em>, <em>37</em>(6), 2255–2280. (<a
href="https://doi.org/10.1007/s10618-023-00942-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Confirmation Measures (BCMs) are used to assess the degree to which an evidence (or premise) E supports or contradicts an hypothesis (or conclusion) H, making use of prior probability $$Pr(H)$$ , posterior probability $$Pr(H|E)$$ and of probability of evidence $$Pr(E)$$ . In the literature many BCMs have been defined with the consequent need for their comparison. For this purpose, various criteria have been proposed and some of these refer to symmetry properties. We relate the set of possible symmetries of BCMs, via an isomorphism, to the dihedral group of symmetries of the square. In this way it is possible to identify 10 subsets of symmetries that can coexist, for each subset we suggest a representative BCM, defining at this aim two new BCMs. The structure of the subgroups of the dihedral group allows also to provide an algorithm that simplifies the verification of the symmetry properties. Addressing the debate on which symmetry properties should be considered as desirable and which should not, we define asymmetry measures for BCMs. In fact, different BCMs that do not satisfy a specific symmetry property may exhibit different levels of asymmetry, this way resulting more (less) desirable. The evidence for the practical use of the approach is given through the numerical evaluation of the asymmetry degrees of some BCMs, showing this way how it is possible to discover some of their characteristics, similarities and differences.},
  archive      = {J_DMKD},
  author       = {Celotto, Emilio and Ellero, Andrea and Ferretti, Paola},
  doi          = {10.1007/s10618-023-00942-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2255-2280},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Symmetry properties and asymmetry evaluation of bayesian confirmation measures},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Datasets, tasks, and training methods for large-scale
hypergraph learning. <em>DMKD</em>, <em>37</em>(6), 2216–2254. (<a
href="https://doi.org/10.1007/s10618-023-00952-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relations among multiple entities are prevalent in many fields, and hypergraphs are widely used to represent such group relations. Hence, machine learning on hypergraphs has received considerable attention, and especially much effort has been made in neural network architectures for hypergraphs (a.k.a., hypergraph neural networks). However, existing studies mostly focused on small datasets for a few single-entity-level downstream tasks and overlooked scalability issues, although most real-world group relations are large-scale. In this work, we propose new tasks, datasets, and scalable training methods for addressing these limitations. First, we introduce two pair-level hypergraph-learning tasks to formulate a wide range of real-world problems. Then, we build and publicly release two large-scale hypergraph datasets with tens of millions of nodes, rich features, and labels. After that, we propose PCL, a scalable learning method for hypergraph neural networks. To tackle scalability issues, PCL splits a given hypergraph into partitions and trains a neural network via contrastive learning. Our extensive experiments demonstrate that hypergraph neural networks can be trained for large-scale hypergraphs by PCL while outperforming 16 baseline models. Specifically, the performance is comparable, or surprisingly even better than that achieved by training hypergraph neural networks on the entire hypergraphs without partitioning.},
  archive      = {J_DMKD},
  author       = {Kim, Sunwoo and Lee, Dongjin and Kim, Yul and Park, Jungho and Hwang, Taeho and Shin, Kijung},
  doi          = {10.1007/s10618-023-00952-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2216-2254},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Datasets, tasks, and training methods for large-scale hypergraph learning},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast block-wise partitioning for extreme multi-label
classification. <em>DMKD</em>, <em>37</em>(6), 2192–2215. (<a
href="https://doi.org/10.1007/s10618-023-00945-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme multi-label classification aims to learn a classifier that annotates an instance with a relevant subset of labels from an extremely large label set. Many existing solutions embed the label matrix to a low-dimensional linear subspace, or examine the relevance of a test instance to every label via a linear scan. In practice, however, those approaches can be computationally exorbitant. To alleviate this drawback, we propose a Block-wise Partitioning (BP) pretreatment that divides all instances into disjoint clusters, to each of which the most frequently tagged label subset is attached. One multi-label classifier is trained on one pair of instance and label clusters, and the label set of a test instance is predicted by first delivering it to the most appropriate instance cluster. Experiments on benchmark multi-label data sets reveal that BP pretreatment significantly reduces prediction time, and retains almost the same level of prediction accuracy.},
  archive      = {J_DMKD},
  author       = {Liang, Yuefeng and Hsieh, Cho-Jui and Lee, Thomas C. M.},
  doi          = {10.1007/s10618-023-00945-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2192-2215},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fast block-wise partitioning for extreme multi-label classification},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interplay between topology and edge weights in real-world
graphs: Concepts, patterns, and an algorithm. <em>DMKD</em>,
<em>37</em>(6), 2139–2191. (<a
href="https://doi.org/10.1007/s10618-023-00940-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What are the relations between the edge weights and the topology in real-world graphs? Given only the topology of a graph, how can we assign realistic weights to its edges based on the relations? Several trials have been done for edge-weight prediction where some unknown edge weights are predicted with most edge weights known. There are also existing works on generating both topology and edge weights of weighted graphs. Differently, we are interested in generating edge weights that are realistic in a macroscopic scope, merely from the topology, which is unexplored and challenging. To this end, we explore and exploit the patterns involving edge weights and topology in real-world graphs. Specifically, we divide each graph into layers where each layer consists of the edges with weights at least a threshold. We observe consistent and surprising patterns appearing in multiple layers: the similarity between being adjacent and having high weights, and the nearly-linear growth of the fraction of edges having high weights with the number of common neighbors. We also observe a power-law pattern that connects the layers. Based on the observations, we propose PEAR, an algorithm assigning realistic edge weights to a given topology. The algorithm relies on only two parameters, preserves all the observed patterns, and produces more realistic weights than the baseline methods with more parameters.},
  archive      = {J_DMKD},
  author       = {Bu, Fanchen and Kang, Shinhwan and Shin, Kijung},
  doi          = {10.1007/s10618-023-00940-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2139-2191},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interplay between topology and edge weights in real-world graphs: Concepts, patterns, and an algorithm},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph embedding methods for entity alignment:
Experimental review. <em>DMKD</em>, <em>37</em>(5), 2070–2137. (<a
href="https://doi.org/10.1007/s10618-023-00941-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity, a task largely known as the Entity Alignment. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods’ effectiveness and efficiency.},
  archive      = {J_DMKD},
  author       = {Fanourakis, Nikolaos and Efthymiou, Vasilis and Kotzinos, Dimitris and Christophides, Vassilis},
  doi          = {10.1007/s10618-023-00941-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2070-2137},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Knowledge graph embedding methods for entity alignment: Experimental review},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MrTF: Model refinery for transductive federated learning.
<em>DMKD</em>, <em>37</em>(5), 2046–2069. (<a
href="https://doi.org/10.1007/s10618-023-00946-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a real-world scenario in which a newly-established pilot project needs to make inferences for newly-collected data with the help of other parties under privacy protection policies. Current federated learning (FL) paradigms are devoted to solving the data heterogeneity problem without considering the to-be-inferred data. We propose a novel learning paradigm named transductive federated learning to simultaneously consider the structural information of the to-be-inferred data. On the one hand, the server could use the pre-available test samples to refine the aggregated models for robust model fusion, which tackles the data heterogeneity problem in FL. On the other hand, the refinery process incorporates test samples into training and could generate better predictions in a transductive manner. We propose several techniques including stabilized teachers, rectified distillation, and clustered label refinery to facilitate the model refinery process. Abundant experimental studies verify the superiorities of the proposed Model refinery framework for Transductive Federated learning. The source code is available at https://github.com/lxcnju/MrTF .},
  archive      = {J_DMKD},
  author       = {Li, Xin-Chun and Yang, Yang and Zhan, De-Chuan},
  doi          = {10.1007/s10618-023-00946-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2046-2069},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MrTF: Model refinery for transductive federated learning},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameterizing the cost function of dynamic time warping
with application to time series classification. <em>DMKD</em>,
<em>37</em>(5), 2024–2045. (<a
href="https://doi.org/10.1007/s10618-023-00926-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic time warping (DTW) is a popular time series distance measure that aligns the points in two series with one another. These alignments support warping of the time dimension to allow for processes that unfold at differing rates. The distance is the minimum sum of costs of the resulting alignments over any allowable warping of the time dimension. The cost of an alignment of two points is a function of the difference in the values of those points. The original cost function was the absolute value of this difference. Other cost functions have been proposed. A popular alternative is the square of the difference. However, to our knowledge, this is the first investigation of both the relative impacts of using different cost functions and the potential to tune cost functions to different time series classification tasks. We do so in this paper by using a tunable cost function $$\lambda _{\gamma }$$ with parameter $$\gamma $$ . We show that higher values of $$\gamma $$ place greater weight on larger pairwise differences, while lower values place greater weight on smaller pairwise differences. We demonstrate that training $$\gamma $$ significantly improves the accuracy of both the $${ DTW }$$ nearest neighbor and Proximity Forest classifiers.},
  archive      = {J_DMKD},
  author       = {Herrmann, Matthieu and Tan, Chang Wei and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-023-00926-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2024-2045},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Parameterizing the cost function of dynamic time warping with application to time series classification},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair detection of poisoning attacks in federated learning on
non-i.i.d. data. <em>DMKD</em>, <em>37</em>(5), 1998–2023. (<a
href="https://doi.org/10.1007/s10618-022-00912-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconciling machine learning with individual privacy is one of the main motivations behind federated learning (FL), a decentralized machine learning technique that aggregates partial models trained by clients on their own private data to obtain a global deep learning model. Even if FL provides stronger privacy guarantees to the participating clients than centralized learning collecting the clients’ data in a central server, FL is vulnerable to some attacks whereby malicious clients submit bad updates in order to prevent the model from converging or, more subtly, to introduce artificial bias in the classification (poisoning). Poisoning detection techniques compute statistics on the updates to identify malicious clients. A downside of anti-poisoning techniques is that they might lead to discriminate minority groups whose data are significantly and legitimately different from those of the majority of clients. This would not only be unfair, but would yield poorer models that would fail to capture the knowledge in the training data, especially when data are not independent and identically distributed (non-i.i.d.). In this work, we strive to strike a balance between fighting poisoning and accommodating diversity to help learning fairer and less discriminatory federated learning models. In this way, we forestall the exclusion of diverse clients while still ensuring detection of poisoning attacks. Empirical work on three data sets shows that employing our approach to tell legitimate from malicious updates produces models that are more accurate than those obtained with state-of-the-art poisoning detection techniques. Additionally, we explore the impact of our proposal on the performance of models on non-i.i.d local training data.},
  archive      = {J_DMKD},
  author       = {Singh, Ashneet Khandpur and Blanco-Justicia, Alberto and Domingo-Ferrer, Josep},
  doi          = {10.1007/s10618-022-00912-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1998-2023},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fair detection of poisoning attacks in federated learning on non-i.i.d. data},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient algorithms for fair clustering with a new notion
of fairness. <em>DMKD</em>, <em>37</em>(5), 1959–1997. (<a
href="https://doi.org/10.1007/s10618-023-00928-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the problem of fair clustering, first introduced by Chierichetti et al. (Fair clustering through fairlets, 2017), which requires each protected attribute to have approximately equal representation in every cluster, i.e., a Balance property. Existing solutions to fair clustering are either not scalable or do not achieve an optimal trade-off between clustering objectives and fairness. In this paper, we propose a new notion of fairness which we call $$\varvec{\tau }$$ -ratio fairness, that strictly generalizes the Balance property and enables a fine-grained efficiency vs. fairness trade-off. Furthermore, we show that a simple greedy round-robin-based algorithm achieves this trade-off efficiently. Under a more general setting of multi-valued protected attributes, we rigorously analyze the theoretical properties of the proposed algorithm, the Fair Round-Robin Algorithm for Clustering Over-End ( $${\textsc {FRAC}}_{OE}$$ ). We also propose a heuristic algorithm, Fair Round-Robin Algorithm for Clustering (FRAC), that applies round-robin allocation at each iteration of a vanilla clustering algorithm. Our experimental results suggest that both FRAC and $${\textsc {FRAC}}_{OE}$$ outperform all the state-of-the-art algorithms and work exceptionally well even for a large number of clusters.},
  archive      = {J_DMKD},
  author       = {Gupta, Shivam and Ghalme, Ganesh and Krishnan, Narayanan C. and Jain, Shweta},
  doi          = {10.1007/s10618-023-00928-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1959-1997},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient algorithms for fair clustering with a new notion of fairness},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enforcing fairness using ensemble of diverse pareto-optimal
models. <em>DMKD</em>, <em>37</em>(5), 1930–1958. (<a
href="https://doi.org/10.1007/s10618-023-00922-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges of machine learning is to ensure that its applications do not generate or propagate unfair discrimination based on sensitive characteristics such as gender, race, and ethnicity. Research in this area typically limits models to a level of discrimination quantified by an equity metric (usually the “benefit” discrepancy between privileged and non-privileged groups). However, when models reduce bias, they may also reduce their performance (e.g., accuracy, F1 score). Therefore, we have to optimize contradictory metrics (performance and fairness) at the same time. This problem is well characterized as a multi-objective optimization (MOO) problem. In this study, we use MOO methods to minimize the difference between groups, maximize the benefits for each group, and preserve performance. We search for the best trade-off models in binary classification problems and aggregate them using ensemble filtering and voting procedures. The aggregation of models with different levels of benefits for each group improves robustness regarding performance and fairness. We compared our approach with other known methodologies, using logistic regression as a benchmark for comparison. The proposed methods obtained interesting results: (i) multi-objective training found models that are similar to or better than the adversarial methods and are more diverse in terms of fairness and accuracy metrics, (ii) multi-objective selection was able to improve the balance between fairness and accuracy compared to selection with a single metric, and (iii) the final predictor found models with higher fairness without sacrificing much accuracy.},
  archive      = {J_DMKD},
  author       = {Guardieiro, Vitória and Raimundo, Marcos M. and Poco, Jorge},
  doi          = {10.1007/s10618-023-00922-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1930-1958},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Enforcing fairness using ensemble of diverse pareto-optimal models},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bias characterization, assessment, and mitigation in
location-based recommender systems. <em>DMKD</em>, <em>37</em>(5),
1885–1929. (<a
href="https://doi.org/10.1007/s10618-022-00913-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-Based Social Networks stimulated the rise of services such as Location-based Recommender Systems. These systems suggest to users points of interest (or venues) to visit when they arrive in a specific city or region. These recommendations impact various stakeholders in society, like the users who receive the recommendations and venue owners. Hence, if a recommender generates biased or polarized results, this affects in tangible ways both the experience of the users and the providers’ activities. In this paper, we focus on four forms of polarization, namely venue popularity, category popularity, venue exposure, and geographical distance. We characterize them on different families of recommendation algorithms when using a realistic (temporal-aware) offline evaluation methodology while assessing their existence. Besides, we propose two automatic approaches to mitigate those biases. Experimental results on real-world data show that these approaches are able to jointly improve the recommendation effectiveness, while alleviating these multiple polarizations.},
  archive      = {J_DMKD},
  author       = {Sánchez, Pablo and Bellogín, Alejandro and Boratto, Ludovico},
  doi          = {10.1007/s10618-022-00913-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1885-1929},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bias characterization, assessment, and mitigation in location-based recommender systems},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social norm bias: Residual harms of fairness-aware
algorithms. <em>DMKD</em>, <em>37</em>(5), 1858–1884. (<a
href="https://doi.org/10.1007/s10618-022-00910-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern machine learning algorithms mitigate bias by enforcing fairness constraints across coarsely-defined groups related to a sensitive attribute like gender or race. However, these algorithms seldom account for within-group heterogeneity and biases that may disproportionately affect some members of a group. In this work, we characterize Social Norm Bias (SNoB), a subtle but consequential type of algorithmic discrimination that may be exhibited by machine learning models, even when these systems achieve group fairness objectives. We study this issue through the lens of gender bias in occupation classification. We quantify SNoB by measuring how an algorithm’s predictions are associated with conformity to inferred gender norms. When predicting if an individual belongs to a male-dominated occupation, this framework reveals that “fair” classifiers still favor biographies written in ways that align with inferred masculine norms. We compare SNoB across algorithmic fairness techniques and show that it is frequently a residual bias, and post-processing approaches do not mitigate this type of bias at all.},
  archive      = {J_DMKD},
  author       = {Cheng, Myra and De-Arteaga, Maria and Mackey, Lester and Kalai, Adam Tauman},
  doi          = {10.1007/s10618-022-00910-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1858-1884},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Social norm bias: Residual harms of fairness-aware algorithms},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TIRPClo: Efficient and complete mining of time
intervals-related patterns. <em>DMKD</em>, <em>37</em>(5), 1806–1857.
(<a href="https://doi.org/10.1007/s10618-023-00944-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent Time Intervals-Related Patterns (TIRPs) from series of symbolic time intervals offers a comprehensive framework for heterogeneous, multivariate temporal data analysis in various application domains. While gaining a growing interest in recent decades, the efficient mining of frequent TIRPs is still a high computational challenge which has also not yet been investigated in its full complexity. The majority of previous methods discover only the first instances of the TIRPs within each series of symbolic time intervals, whereas their re-occurring instances are ignored. This eventually results in an incomplete discovery of frequent TIRPs, a problem that lies also in the challenge of mining only the frequent closed TIRPs, which was only recently investigated for the first time. In this paper, we introduce TIRPClo—an efficient algorithm for the complete mining of either the entire set of frequent TIRPs, or only the frequent closed TIRPs. The algorithm proposes a non-ambiguous sequential representation of symbolic time intervals series through the intervals’ end-points, as well as a memory-efficient index and a novel method for data projection, due to which it is the first algorithm to guarantee a complete discovery of frequent closed TIRPs. The experimental evaluation conducted on eleven real-world and four synthetic datasets demonstrates that TIRPClo is up to 10 times faster when mining the entire set of frequent TIRPs, and up to more than 100 times faster when mining only the frequent closed TIRPs compared to four state-of-the-art methods, while also reporting lower memory measurements.},
  archive      = {J_DMKD},
  author       = {Harel, Omer and Moskovitch, Robert},
  doi          = {10.1007/s10618-023-00944-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1806-1857},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TIRPClo: Efficient and complete mining of time intervals-related patterns},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hydra: Competing convolutional kernels for fast and accurate
time series classification. <em>DMKD</em>, <em>37</em>(5), 1779–1805.
(<a href="https://doi.org/10.1007/s10618-023-00939-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate a simple connection between dictionary methods for time series classification, which involve extracting and counting symbolic patterns in time series, and methods based on transforming input time series using convolutional kernels, namely Rocket and its variants. We show that by adjusting a single hyperparameter it is possible to move by degrees between models resembling dictionary methods and models resembling Rocket. We present Hydra, a simple, fast, and accurate dictionary method for time series classification using competing convolutional kernels, combining key aspects of both Rocket and conventional dictionary methods. Hydra is faster and more accurate than the most accurate existing dictionary methods, achieving similar accuracy to several of the most accurate current methods for time series classification. Hydra can also be combined with Rocket and its variants to significantly improve the accuracy of these methods.},
  archive      = {J_DMKD},
  author       = {Dempster, Angus and Schmidt, Daniel F. and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-023-00939-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1779-1805},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hydra: Competing convolutional kernels for fast and accurate time series classification},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking and survey of explanation methods for black box
models. <em>DMKD</em>, <em>37</em>(5), 1719–1778. (<a
href="https://doi.org/10.1007/s10618-023-00933-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.},
  archive      = {J_DMKD},
  author       = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
  doi          = {10.1007/s10618-023-00933-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1719-1778},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Benchmarking and survey of explanation methods for black box models},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROhAN: Row-order agnostic null models for
statistically-sound knowledge discovery. <em>DMKD</em>, <em>37</em>(4),
1692–1718. (<a
href="https://doi.org/10.1007/s10618-023-00938-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel class of null models for the statistical validation of results obtained from binary transactional and sequence datasets. Our null models are Row-Order Agnostic (ROA), i.e., do not consider the order of rows in the observed dataset to be fixed, in stark contrast with previous null models, which are Row-Order Enforcing (ROE). We present ROhAN, an algorithmic framework for efficiently sampling datasets from ROA models according to user-specified distributions, which is a necessary step for the resampling-based statistical hypothesis tests employed to validate the results. ROhAN uses Metropolis-Hastings or rejection sampling to build on top of existing or future ROE sampling procedures. Our experimental evaluation shows that ROA models are very different from ROE ones, impacting the statistical validation, and that ROhAN is efficient, mixes fast, and scales well as the dataset grows.},
  archive      = {J_DMKD},
  author       = {Abuissa, Maryam and Lee, Alexander and Riondato, Matteo},
  doi          = {10.1007/s10618-023-00938-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1692-1718},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ROhAN: Row-order agnostic null models for statistically-sound knowledge discovery},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A combinatorial multi-armed bandit approach to correlation
clustering. <em>DMKD</em>, <em>37</em>(4), 1630–1691. (<a
href="https://doi.org/10.1007/s10618-023-00937-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph whose edges are assigned positive-type and negative-type weights, the problem of correlation clustering aims at grouping the graph vertices so as to minimize (resp. maximize) the sum of negative-type (resp. positive-type) intra-cluster weights plus the sum of positive-type (resp. negative-type) inter-cluster weights. In correlation clustering, it is typically assumed that the weights are readily available. This is a rather strong hypothesis, which is unrealistic in several scenarios. To overcome this limitation, in this work we focus on the setting where edge weights of a correlation-clustering instance are unknown, and they have to be estimated in multiple rounds, while performing the clustering. The clustering solutions produced in the various rounds provide a feedback to properly adjust the weight estimates, and the goal is to maximize the cumulative quality of the clusterings. We tackle this problem by resorting to the reinforcement-learning paradigm, and, specifically, we design for the first time a Combinatorial Multi-Armed Bandit (CMAB) framework for correlation clustering. We provide a variety of contributions, namely (1) formulations of the minimization and maximization variants of correlation clustering in a CMAB setting; (2) adaptation of well-established CMAB algorithms to the correlation-clustering context; (3) regret analyses to theoretically bound the accuracy of these algorithms; (4) design of further (heuristic) algorithms to have the probability constraint satisfied at every round (key condition to soundly adopt efficient yet effective algorithms for correlation clustering as CMAB oracles); (5) extensive experimental comparison among a variety of both CMAB and non-CMAB approaches for correlation clustering.},
  archive      = {J_DMKD},
  author       = {Gullo, F. and Mandaglio, D. and Tagarelli, A.},
  doi          = {10.1007/s10618-023-00937-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1630-1691},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A combinatorial multi-armed bandit approach to correlation clustering},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust and sparse multinomial regression in high dimensions.
<em>DMKD</em>, <em>37</em>(4), 1609–1629. (<a
href="https://doi.org/10.1007/s10618-023-00936-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust and sparse estimator for multinomial regression is proposed for high dimensional data. Robustness of the estimator is achieved by trimming the observations, and sparsity of the estimator is obtained by the elastic net penalty. In contrast to multi-group classifiers based on dimension reduction, this model is very appealing in terms of interpretation, since one obtains estimated coefficients individually for every group, and also the sparsity of the coefficients is group specific. Simulation studies are conducted to show the performance in comparison to the non-robust version of the multinomial regression estimator, and some real data examples underline the usefulness of this robust estimator particularly in terms of result interpretation and model diagnostics.},
  archive      = {J_DMKD},
  author       = {Kurnaz, Fatma Sevinç and Filzmoser, Peter},
  doi          = {10.1007/s10618-023-00936-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1609-1629},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Robust and sparse multinomial regression in high dimensions},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot relational learning for extrapolation reasoning on
temporal knowledge graphs. <em>DMKD</em>, <em>37</em>(4), 1591–1608. (<a
href="https://doi.org/10.1007/s10618-023-00935-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, temporal knowledge graph reasoning has been a critical task in natural language processing. Temporal knowledge graphs store temporal facts that model dynamic relationships or interactions between entities along the timeline. Most existing temporal knowledge graph reasoning methods need a large number of training instances (i.e. support entity facts) for each relation. However, the same as traditional knowledge graphs, temporal knowledge graphs also exhibit long-tailed relational frequency distribution, in which most relationships often do not have many support entity pairs for training. To address this problem, in this paper, we propose a one-shot learning framework (OSLT) applied to temporal knowledge graph link prediction, which aims to predict new relational facts with only one support instance. Specifically, OSLT employs an fact encoder based on Temporal Convolutional Network to encode historical information and model connection of facts at the same timestamp by the aggregator with an attention mechanism. After that, a matching network is employed to compute the similarity score between support fact and query fact. Experiments show that the proposed method outperforms the state-of-the-art baselines on two benchmark datasets.},
  archive      = {J_DMKD},
  author       = {Ma, Ruixin and Mei, Biao and Ma, Yunlong and Zhang, Hongyan and Liu, Meihong and Zhao, Liang},
  doi          = {10.1007/s10618-023-00935-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1591-1608},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {One-shot relational learning for extrapolation reasoning on temporal knowledge graphs},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private tree-based redescription mining.
<em>DMKD</em>, <em>37</em>(4), 1548–1590. (<a
href="https://doi.org/10.1007/s10618-023-00934-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy provides a strong form of privacy and allows preserving most of the original characteristics of the dataset. Utilizing these benefits requires one to design specific differentially private data analysis algorithms. In this work, we present three tree-based algorithms for mining redescriptions while preserving differential privacy. Redescription mining is an exploratory data analysis method for finding connections between two views over the same entities, such as phenotypes and genotypes of medical patients, for example. It has applications in many fields, including some, like health care informatics, where privacy-preserving access to data is desired. Our algorithms are the first tree-based differentially private redescription mining algorithms, and we show via experiments that, despite the inherent noise in differential privacy, it can return trustworthy results even in smaller datasets where noise typically has a stronger effect.},
  archive      = {J_DMKD},
  author       = {Mihelčić, Matej and Miettinen, Pauli},
  doi          = {10.1007/s10618-023-00934-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1548-1590},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Differentially private tree-based redescription mining},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph convolutional fusion model for community detection
in multiplex networks. <em>DMKD</em>, <em>37</em>(4), 1518–1547. (<a
href="https://doi.org/10.1007/s10618-023-00932-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is to partition a network into several components, each of which contains densely connected nodes with some structural similarities. Recently, multiplex networks, each layer consisting of a same node set but with a different topology by a unique edge type, have been proposed to model real-world multi-relational networks. Although some heuristic algorithms have been extended into multiplex networks, little work on neural models have been done so far. In this paper, we propose a graph convolutional fusion model (GCFM) for community detection in multiplex networks, which takes account of both intra-layer structural and inter-layer relational information for learning node representation in an interwoven fashion. In particular, we first develop a graph convolutional auto-encoder for each network layer to encode neighbor-aware intra-layer structural features under different convolution scales. We next design a multiscale fusion network to learn a holistic version of nodes’ representations by fusing nodes’ encodings at different layers and different scales. Finally, a self-training mechanism is used to train our model and output community divisions. Experiment results on both synthetic and real-world datasets indicate that the proposed GCFM outperforms the state-of-the-art techniques in terms of better detection performances.},
  archive      = {J_DMKD},
  author       = {Cai, Xiang and Wang, Bang},
  doi          = {10.1007/s10618-023-00932-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1518-1547},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A graph convolutional fusion model for community detection in multiplex networks},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the evaluation of outlier detection and one-class
classification: A comparative study of algorithms, model selection, and
ensembles. <em>DMKD</em>, <em>37</em>(4), 1473–1517. (<a
href="https://doi.org/10.1007/s10618-023-00931-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that unsupervised outlier detection methods can be adapted to the one-class classification problem (Janssens and Postma, in: Proceedings of the 18th annual Belgian-Dutch on machine learning, pp 56–64, 2009; Janssens et al. in: Proceedings of the 2009 ICMLA international conference on machine learning and applications, IEEE Computer Society, pp 147–153, 2009. https://doi.org/10.1109/ICMLA.2009.16 ). In this paper, we focus on the comparison of one-class classification algorithms with such adapted unsupervised outlier detection methods, improving on previous comparison studies in several important aspects. We study a number of one-class classification and unsupervised outlier detection methods in a rigorous experimental setup, comparing them on a large number of datasets with different characteristics, using different performance measures. In contrast to previous comparison studies, where the models (algorithms, parameters) are selected by using examples from both classes (outlier and inlier), here we also study and compare different approaches for model selection in the absence of examples from the outlier class, which is more realistic for practical applications since labeled outliers are rarely available. Our results showed that, overall, SVDD and GMM are top-performers, regardless of whether the ground truth is used for parameter selection or not. However, in specific application scenarios, other methods exhibited better performance. Combining one-class classifiers into ensembles showed better performance than individual methods in terms of accuracy, as long as the ensemble members are properly selected.},
  archive      = {J_DMKD},
  author       = {Marques, Henrique O. and Swersky, Lorne and Sander, Jörg and Campello, Ricardo J. G. B. and Zimek, Arthur},
  doi          = {10.1007/s10618-023-00931-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1473-1517},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On the evaluation of outlier detection and one-class classification: A comparative study of algorithms, model selection, and ensembles},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatiotemporal deep neural network for fine-grained
multi-horizon wind prediction. <em>DMKD</em>, <em>37</em>(4), 1441–1472.
(<a href="https://doi.org/10.1007/s10618-023-00929-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, multi-horizon spatiotemporal network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal module fuses coarse-grained forecasts derived by numerical weather prediction (NWP) and historical on-site observation data at stations so as to leverage both global and local atmospheric information. Second, a spatial module exploits spatial correlation by modeling the joint representation of all stations. Third, an ensemble module weighs the above two modules for final predictions. Furthermore, a covariate selection module automatically choose influential meteorological variables as initial input. MHSTN is already integrated into the scheduling platform of one of the busiest international airports of China. The evaluation results demonstrate that our model outperforms competitors by a significant margin.},
  archive      = {J_DMKD},
  author       = {Huang, Fanling and Deng, Yangdong},
  doi          = {10.1007/s10618-023-00929-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1441-1472},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A spatiotemporal deep neural network for fine-grained multi-horizon wind prediction},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Making clusterings fairer by post-processing: Algorithms,
complexity results and experiments. <em>DMKD</em>, <em>37</em>(4),
1404–1440. (<a
href="https://doi.org/10.1007/s10618-022-00893-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While existing fairness work typically focuses on fair-by-design algorithms, here we consider making a fairness-unaware algorithm’s output fairer. Specifically, we explore the area of fairness in clustering by modifying clusterings produced by existing algorithms to make them fairer whilst retaining their quality. We formulate the minimal cluster modification for fairness (MCMF) problem, where the input is a given partitional clustering and the goal is to minimally change it so that the clustering is still of good quality but fairer. We show that for a single binary protected status variable, the problem is efficiently solvable (i.e., in the class P) by proving that the constraint matrix for an integer linear programming formulation is totally unimodular. Interestingly, we show that even for a single protected variable, the addition of simple pairwise guidance for clustering (to say ensure individual-level fairness) makes the MCMF problem computationally intractable (i.e., NP-hard). Experimental results using Twitter, Census and NYT data sets show that our methods can modify existing clusterings for data sets in excess of 100,000 instances within minutes on laptops and find clusterings that are as fair but are of higher quality than those produced by fair-by-design clustering algorithms. Finally, we explore a challenging practical problem of making a historical clustering (i.e., zipcodes clustered into California’s congressional districts) fairer using a new multi-faceted benchmark data set.},
  archive      = {J_DMKD},
  author       = {Davidson, Ian and Bai, Zilong and Tran, Cindy Mylinh and Ravi, S. S.},
  doi          = {10.1007/s10618-022-00893-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1404-1440},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Making clusterings fairer by post-processing: Algorithms, complexity results and experiments},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FiSH: Fair spatial hot spots. <em>DMKD</em>, <em>37</em>(4),
1374–1403. (<a
href="https://doi.org/10.1007/s10618-022-00887-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasiveness of tracking devices and enhanced availability of spatially located data has deepened interest in using them for various policy interventions, through computational data analysis tasks such as spatial hot spot detection. In this paper, we consider, for the first time to our best knowledge, fairness in detecting spatial hot spots. We motivate the need for ensuring fairness through statistical parity over the collective population covered across chosen hot spots. We then characterize the task of identifying a diverse set of solutions in the noteworthiness-fairness trade-off spectrum, to empower the user to choose a trade-off justified by the policy domain. Being a novel task formulation, we also develop a suite of evaluation metrics for fair hot spots, motivated by the need to evaluate pertinent aspects of the task. We illustrate the computational infeasibility of identifying fair hot spots using naive and/or direct approaches and devise a method, codenamed FiSH, for efficiently identifying high-quality, fair and diverse sets of spatial hot spots. FiSH traverses the tree-structured search space using heuristics that guide it towards identifying noteworthy and fair sets of spatial hot spots. Through an extensive empirical analysis over a real-world dataset from the domain of human development, we illustrate that FiSH generates high-quality solutions at fast response times. Towards assessing the relevance of FiSH in real-world context, we also provide a detailed discussion of how it could fit within the current practice of hot spots policing, as read within the historical context of the evolution of the practice.},
  archive      = {J_DMKD},
  author       = {P., Deepak and Sundaram, Sowmya S.},
  doi          = {10.1007/s10618-022-00887-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1374-1403},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {FiSH: Fair spatial hot spots},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Making individually fair predictions with causal pathways.
<em>DMKD</em>, <em>37</em>(4), 1327–1373. (<a
href="https://doi.org/10.1007/s10618-022-00885-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is being increasingly used to make algorithmic decisions that have strong societal impact on people’s lives. Due to their huge societal impact, such algorithmic decisions need to be accurate and fair with respect to sensitive features, including race, gender, religion, and sexual orientation. To achieve a good balance between prediction accuracy and fairness, causality-based methods have been proposed, which utilize a causal graph with unfair pathways. However, none of these methods can ensure fairness for each individual without making restrictive functional assumptions about the data generating processes, which are not satisfied in many cases. In this paper, we propose a far more practical causality-based framework for learning an individually fair classifier. To avoid impractical functional assumptions, we introduce a new criterion, the probability of individual unfairness, and derive its upper bound that can be estimated from data. We then train a classifier by solving an optimization problem where the upper bound value is forced to be close to zero. We elucidate why solving such an optimization problem can guarantee fairness for each individual. Moreover, we provide two extensions for dealing with challenging real-world scenarios where there are unobserved variables called latent confounders, and the true causal graph is uncertain. Experimental results show that our method can learn an individually fair classifier at a slight cost of prediction accuracy.},
  archive      = {J_DMKD},
  author       = {Chikahara, Yoichi and Sakaue, Shinsaku and Fujino, Akinori and Kashima, Hisashi},
  doi          = {10.1007/s10618-022-00885-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1327-1373},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Making individually fair predictions with causal pathways},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-step anomaly detection based method for PU
classification in imbalanced data sets. <em>DMKD</em>, <em>37</em>(3),
1301–1325. (<a
href="https://doi.org/10.1007/s10618-023-00925-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several machine learning applications, including genetics and fraud detection, suffer from incomplete label information. In such applications, a classifier can only train from positive and unlabeled (PU) examples in which the unlabeled data consist of both positive and negative examples. Despite a substantial presence of PU learning in the literature, few works have considered a class imbalance setting. Hence, we propose a novel two-step method that exploits anomaly detection to identify hidden positives within the unlabeled data. Our method allows the end-user to choose the anomaly detector depending on preference or domain knowledge. Moreover, we introduce Nearest-Neighbor Isolation Forest (NNIF), a novel semi-supervised anomaly detector based on the Isolation Forest. In contrast to unsupervised anomaly detectors, NNIF can utilize all available label information. Empirical analysis shows that our method generally outperforms, using NNIF as the anomaly detector, state-of-the-art PU learning methods for imbalanced data sets under different labeling mechanisms. Further experiments suggest that our two-step method shows strong robustness to wrong class prior estimates.},
  archive      = {J_DMKD},
  author       = {Ortega Vázquez, Carlos and vanden Broucke, Seppe and De Weerdt, Jochen},
  doi          = {10.1007/s10618-023-00925-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1301-1325},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A two-step anomaly detection based method for PU classification in imbalanced data sets},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ClaSP: Parameter-free time series segmentation.
<em>DMKD</em>, <em>37</em>(3), 1262–1300. (<a
href="https://doi.org/10.1007/s10618-023-00923-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of natural and human-made processes often results in long sequences of temporally-ordered values, aka time series (TS). Such processes often consist of multiple states, e.g. operating modes of a machine, such that state changes in the observed processes result in changes in the distribution of shape of the measured values. Time series segmentation (TSS) tries to find such changes in TS post-hoc to deduce changes in the data-generating process. TSS is typically approached as an unsupervised learning problem aiming at the identification of segments distinguishable by some statistical property. Current algorithms for TSS require domain-dependent hyper-parameters to be set by the user, make assumptions about the TS value distribution or the types of detectable changes which limits their applicability. Common hyper-parameters are the measure of segment homogeneity and the number of change points, which are particularly hard to tune for each data set. We present ClaSP, a novel, highly accurate, hyper-parameter-free and domain-agnostic method for TSS. ClaSP hierarchically splits a TS into two parts. A change point is determined by training a binary TS classifier for each possible split point and selecting the one split that is best at identifying subsequences to be from either of the partitions. ClaSP learns its main two model-parameters from the data using two novel bespoke algorithms. In our experimental evaluation using a benchmark of 107 data sets, we show that ClaSP outperforms the state of the art in terms of accuracy and is fast and scalable. Furthermore, we highlight properties of ClaSP using several real-world case studies.},
  archive      = {J_DMKD},
  author       = {Ermshaus, Arik and Schäfer, Patrick and Leser, Ulf},
  doi          = {10.1007/s10618-023-00923-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1262-1300},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ClaSP: Parameter-free time series segmentation},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint leaf-refinement and ensemble pruning through <span
class="math display"><em>L</em><sub>1</sub></span> regularization.
<em>DMKD</em>, <em>37</em>(3), 1230–1261. (<a
href="https://doi.org/10.1007/s10618-023-00921-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles are among the state-of-the-art in many machine learning applications. With the ongoing integration of ML models into everyday life, e.g., in the form of the Internet of Things, the deployment and continuous application of models become more and more an important issue. Therefore, small models that offer good predictive performance and use small amounts of memory are required. Ensemble pruning is a standard technique for removing unnecessary classifiers from a large ensemble that reduces the overall resource consumption and sometimes improves the performance of the original ensemble. Similarly, leaf-refinement is a technique that improves the performance of a tree ensemble by jointly re-learning the probability estimates in the leaf nodes of the trees, thereby allowing for smaller ensembles while preserving their predictive performance. In this paper, we develop a new method that combines both approaches into a single algorithm. To do so, we introduce $$L_1$$ regularization into the leaf-refinement objective, which allows us to jointly prune and refine trees at the same time. In an extensive experimental evaluation, we show that our approach not only offers statistically significantly better performance than the state-of-the-art but also offers a better accuracy-memory trade-off. We conclude our experimental evaluation with a case study showing the effectiveness of our method in a real-world setting.},
  archive      = {J_DMKD},
  author       = {Buschjäger, Sebastian and Morik, Katharina},
  doi          = {10.1007/s10618-023-00921-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1230-1261},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Joint leaf-refinement and ensemble pruning through $$L_1$$ regularization},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AA-forecast: Anomaly-aware forecast for extreme events.
<em>DMKD</em>, <em>37</em>(3), 1209–1229. (<a
href="https://doi.org/10.1007/s10618-023-00919-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series models often are impacted by extreme events and anomalies, both prevalent in real-world datasets. Such models require careful probabilistic forecasts, which is vital in risk management for extreme events such as hurricanes and pandemics. However, it’s challenging to automatically detect and learn from extreme events and anomalies for large-scale datasets which often results in extra manual efforts. Here, we propose an anomaly-aware forecast framework that leverages the effects of anomalies to improve its prediction accuracy during the presence of extreme events. Our model has trained to extract anomalies automatically and incorporates them through an attention mechanism to increase the accuracy of forecasts during extreme events. Moreover, the framework employs a dynamic uncertainty optimization algorithm that reduces the uncertainty of forecasts in an online manner. The proposed framework demonstrated consistent superior accuracy with less uncertainty on three datasets with different varieties of anomalies over the current prediction models.},
  archive      = {J_DMKD},
  author       = {Farhangi, Ashkan and Bian, Jiang and Huang, Arthur and Xiong, Haoyi and Wang, Jun and Guo, Zhishan},
  doi          = {10.1007/s10618-023-00919-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1209-1229},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {AA-forecast: Anomaly-aware forecast for extreme events},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view metro station clustering based on passenger
flows: A functional data-edged network community detection approach.
<em>DMKD</em>, <em>37</em>(3), 1154–1208. (<a
href="https://doi.org/10.1007/s10618-023-00916-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at metro station clustering based on passenger flow data. Compared with existing clustering methods that only use boarding or alighting data of each station separately, we focus on higher granularity origin-destination (O-D) path flow data, and provide more flexible and insightful clustering results. In particular, we regard the metro system as a network, with each station as a node. The real-time passenger flows over time between different O-D paths serve as directed edges between nodes. Compared with traditional networks, our edges are temporal curves, and can be regarded as functional data. For this functional data-edged graph, we are the first to develop a novel community detection approach for node clustering. Our method is based on functional factorization. First a dual time-warped sparse nonnegative functional factorization is proposed for extracting patterns of the functional edges. Then the passenger flow of each O-D path can be regarded as a linear combination of different extracted passenger flow patterns. Based on it, we construct a multi-view directed and weighted network, where each view represents one particular pattern, and the factorization coefficient of each O-D path on this pattern is treated as the weight of this directed edge in this particular view. Then a novel community detection algorithm based on nonnegative matrix tri-factorization is constructed according to the topological structure of the multi-view network. The fusion of different views can be either subjectively determined or objectively learnt in a data-driven way, which gives flexibility of the clustering algorithm to emphasize on different travel patterns. Two real datasets of Singapore and Hong Kong metro systems are used to validate the proposed method.},
  archive      = {J_DMKD},
  author       = {Zhang, Chen and Zheng, Baihua and Tsung, Fugee},
  doi          = {10.1007/s10618-023-00916-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1154-1208},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multi-view metro station clustering based on passenger flows: A functional data-edged network community detection approach},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting sensor data in professional road cycling:
Personalized data-driven approach for frequent fitness monitoring.
<em>DMKD</em>, <em>37</em>(3), 1125–1153. (<a
href="https://doi.org/10.1007/s10618-022-00905-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a personalized approach for frequent fitness monitoring in road cycling solely relying on sensor data collected during bike rides and without the need for maximal effort tests. We use competition and training data of three world-class cyclists of Team Jumbo–Visma to construct personalised heart rate models that relate the heart rate during exercise to the pedal power signal. Our model captures the non-trivial dependency between exertion and corresponding response of the heart rate, which we show can be effectively estimated by an exponential kernel. To construct the daily heart rate models that are required for day-to-day fitness estimation, we aggregate all sessions in the previous week and apply sampling. On average, the explained variance of our models is 0.86, which we demonstrate is more than twice as large as for models that ignore the temporal integration involved in the heart’s response to exercise. We show that the fitness of a cyclist can be monitored by tracking developments of parameters of our heart rate models. In particular, we monitor the decay constant of the kernel involved, and also analytically determine virtual aerobic and anaerobic thresholds. We demonstrate that our findings for the virtual anaerobic threshold on average agree with the results of exercise tests. We believe this work is an important step forward in performance optimization by opening up avenues for switching to adaptive training programs that take into account the current physiological state of an athlete.},
  archive      = {J_DMKD},
  author       = {de Leeuw, Arie-Willem and Heijboer, Mathieu and Verdonck, Tim and Knobbe, Arno and Latré, Steven},
  doi          = {10.1007/s10618-022-00905-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1125-1153},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exploiting sensor data in professional road cycling: Personalized data-driven approach for frequent fitness monitoring},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hyperbolic approach for learning communities on graphs.
<em>DMKD</em>, <em>37</em>(3), 1090–1124. (<a
href="https://doi.org/10.1007/s10618-022-00902-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting communities on graphs has received significant interest in recent literature. Current state-of-the-art approaches tackle this problem by coupling Euclidean graph embedding with community detection. Considering the success of hyperbolic representations of graph-structured data in the last years, an ongoing challenge is to set up a hyperbolic approach to the community detection problem. The present paper meets this challenge by introducing a Riemannian geometry based framework for learning communities on graphs. The proposed methodology combines graph embedding on hyperbolic spaces with Riemannian K-means or Riemannian mixture models to perform community detection. The usefulness of this framework is illustrated through several experiments on generated community graphs and real-world social networks as well as comparisons with the most powerful baselines. The code implementing hyperbolic community embedding is available online https://www.github.com/tgeral68/HyperbolicGraphAndGMM .},
  archive      = {J_DMKD},
  author       = {Gerald, Thomas and Zaatiti, Hadi and Hajri, Hatem and Baskiotis, Nicolas and Schwander, Olivier},
  doi          = {10.1007/s10618-022-00902-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1090-1124},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A hyperbolic approach for learning communities on graphs},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KNN matrix profile for knowledge discovery from time series.
<em>DMKD</em>, <em>37</em>(3), 1055–1089. (<a
href="https://doi.org/10.1007/s10618-022-00883-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix Profile (MP) has been proposed as a powerful technique for knowledge extraction from time series. Several algorithms have been proposed for computing it, e.g., STAMP and STOMP. Currently, MP is computed based on 1NN search in all subsequences of the time series. In this paper, we claim that a kNN MP can be more useful than the 1NN MP for knowledge extraction, and propose an efficient technique to compute such a MP. We also propose an algorithm for parallel execution of kNN MP by using multiple cores of an off-the-shelf computer. We evaluated the performance of our solution by using multiple real datasets. The results illustrate the superiority of kNN MP for knowledge discovery compared to 1NN MP.},
  archive      = {J_DMKD},
  author       = {Mondal, Tanmoy and Akbarinia, Reza and Masseglia, Florent},
  doi          = {10.1007/s10618-022-00883-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1055-1089},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {KNN matrix profile for knowledge discovery from time series},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable classifier-agnostic channel selection for
multivariate time series classification. <em>DMKD</em>, <em>37</em>(2),
1010–1054. (<a
href="https://doi.org/10.1007/s10618-022-00909-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy is a key focus of current work in time series classification. However, speed and data reduction are equally important in many applications, especially when the data scale and storage requirements rapidly increase. Current multivariate time series classification (MTSC) algorithms need hundreds of compute hours to complete training and prediction. This is due to the nature of multivariate time series data which grows with the number of time series, their length and the number of channels. In many applications, not all the channels are useful for the classification task, hence we require methods that can efficiently select useful channels and thus save computational resources. We propose and evaluate two methods for channel selection. Our techniques work by representing each class by a prototype time series and performing channel selection based on the prototype distance between classes. The main hypothesis is that useful channels enable better separation between classes; hence, channels with a larger distance between class prototypes are more useful. On the UEA MTSC benchmark, we show that these techniques achieve significant data reduction and classifier speedup for similar levels of classification accuracy. Channel selection is applied as a pre-processing step before training state-of-the-art MTSC algorithms and saves about 70% of computation time and data storage with preserved accuracy. Furthermore, our methods enable efficient classifiers, such as ROCKET, to achieve better accuracy than using no selection or greedy forward channel selection. To further study the impact of our techniques, we present experiments on classifying synthetic multivariate time series datasets with more than 100 channels, as well as a real-world case study on a dataset with 50 channels. In both cases, our channel selection methods result in significant data reduction with preserved or improved accuracy.},
  archive      = {J_DMKD},
  author       = {Dhariyal, Bhaskar and Le Nguyen, Thach and Ifrim, Georgiana},
  doi          = {10.1007/s10618-022-00909-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1010-1054},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Scalable classifier-agnostic channel selection for multivariate time series classification},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized density attractor clustering for incomplete
data. <em>DMKD</em>, <em>37</em>(2), 970–1009. (<a
href="https://doi.org/10.1007/s10618-022-00904-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean shift is a popular and powerful clustering method for implementing density attractor clustering (DAC). However, DAC is underdeveloped in terms of modeling definitions and methods for incomplete data. Due to DAC’s importance, solving this common issue is crucial. This work makes DAC more versatile by making it applicable to incomplete data: First, using formal modeling definitions, we propose a unifying framework for DAC. Second, we propose new methods that implement the definitions and perform DAC for incomplete data more efficiently and stably than others. We discuss and compare our methods and the closest competitor using theoretical analyses. We quantify the performance of our methods using synthetic datasets with known structures and real-life business data for three missing value types. Finally, we analyze Stack Overflow’s 2021 survey to extract clusters of programmers from India and the USA. The experiments verify our methods’ superiority to six alternatives. Code, Data: https://bit.ly/genDAC},
  archive      = {J_DMKD},
  author       = {Leibrandt, Richard and Günnemann, Stephan},
  doi          = {10.1007/s10618-022-00904-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {970-1009},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Generalized density attractor clustering for incomplete data},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Difference embedding for recommender systems. <em>DMKD</em>,
<em>37</em>(2), 948–969. (<a
href="https://doi.org/10.1007/s10618-022-00899-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel and straightforward pointwise training strategy, namely difference embedding (DifE), for recommender systems that capture the personalized information retained in pairwise preference differences while simply using effective and efficient pointwise training. Specifically, a function was designed to capture and emphasize pairwise preference differences. Then, a novel projection was used to construct a new space in which pairwise information is preserved, and the newly proposed pointwise loss function is sufficient to learn a better embedding. To verify the superiority and generality of the proposed strategy, we integrate the proposed loss function with four state-of-the-art recommenders and obtain four corresponding optimized models namely MF-DifE, NeuMF-DifE, GCN-DifE, and SGL-DifE. Comprehensive and comparative experiment results on three public datasets show that these optimized models achieve significant improvement compared to their corresponding baselines and outperform various recent recommendation methods, which indicates the excellence and generality of the proposed loss function.},
  archive      = {J_DMKD},
  author       = {Yi, Peng and Cai, Xiongcai and Li, Ziteng},
  doi          = {10.1007/s10618-022-00899-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {948-969},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Difference embedding for recommender systems},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph convolutional networks for traffic forecasting with
missing values. <em>DMKD</em>, <em>37</em>(2), 913–947. (<a
href="https://doi.org/10.1007/s10618-022-00903-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting has attracted widespread attention recently. In reality, traffic data usually contains missing values due to sensor or communication errors. The Spatio-temporal feature in traffic data brings more challenges for processing such missing values, for which the classic techniques (e.g., data imputations) are limited: (1) in temporal axis, the values can be randomly or consecutively missing; (2) in spatial axis, the missing values can happen on one single sensor or on multiple sensors simultaneously. Recent models powered by Graph Neural Networks achieved satisfying performance on traffic forecasting tasks. However, few of them are applicable to such a complex missing-value context. To this end, we propose GCN-M, a Graph Convolutional Network model with the ability to handle the complex missing values in the Spatio-temporal context. Particularly, we jointly model the missing value processing and traffic forecasting tasks, considering both local Spatio-temporal features and global historical patterns in an attention-based memory network. We propose as well a dynamic graph learning module based on the learned local-global features. The experimental results on real-life datasets show the reliability of our proposed method.},
  archive      = {J_DMKD},
  author       = {Zuo, Jingwei and Zeitouni, Karine and Taher, Yehia and Garcia-Rodriguez, Sandra},
  doi          = {10.1007/s10618-022-00903-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {913-947},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Graph convolutional networks for traffic forecasting with missing values},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and robust video-based exercise classification via body
pose tracking and scalable multivariate time series classifiers.
<em>DMKD</em>, <em>37</em>(2), 873–912. (<a
href="https://doi.org/10.1007/s10618-022-00895-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advancements have spurred the usage of machine learning based applications in sports science and healthcare. Using wearable sensors and video cameras to analyze and improve the performance of athletes, has become widely popular. Physiotherapists, sports coaches and athletes actively look to incorporate the latest technologies in order to further improve performance and avoid injuries. While wearable sensors are very popular, their use is hindered by constraints on battery power and sensor calibration, especially for use cases which require multiple sensors to be placed on the body. Hence, there is renewed interest in video-based data capture and analysis for sports science. In this paper, we present the application of classifying strength and conditioning exercises using video. We focus on the popular Military Press exercise, where the execution is captured with a video-camera using a mobile device, such as a mobile phone, and the goal is to classify the execution into different types. Since video recordings need a lot of storage and computation, this use case requires data reduction, while preserving the classification accuracy and enabling fast prediction. To this end, we propose an approach named BodyMTS to turn video into time series by employing body pose tracking, followed by training and prediction using multivariate time series classifiers. We analyze the accuracy and robustness of BodyMTS and show that it is robust to different types of noise caused by either video quality or pose estimation factors. We compare BodyMTS to state-of-the-art deep learning methods which classify human activity directly from videos and show that BodyMTS achieves similar accuracy, but with reduced running time and model engineering effort. Finally, we discuss some of the practical aspects of employing BodyMTS in this application in terms of accuracy and robustness under reduced data quality and size. We show that BodyMTS achieves an average accuracy of 87%, which is significantly higher than the accuracy of human domain experts.},
  archive      = {J_DMKD},
  author       = {Singh, Ashish and Bevilacqua, Antonio and Nguyen, Thach Le and Hu, Feiyan and McGuinness, Kevin and O’Reilly, Martin and Whelan, Darragh and Caulfield, Brian and Ifrim, Georgiana},
  doi          = {10.1007/s10618-022-00895-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {873-912},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fast and robust video-based exercise classification via body pose tracking and scalable multivariate time series classifiers},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methods for explaining top-n recommendations through
subgroup discovery. <em>DMKD</em>, <em>37</em>(2), 833–872. (<a
href="https://doi.org/10.1007/s10618-022-00897-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) has received a lot of attention over the past decade, with the proposal of many methods explaining black box classifiers such as neural networks. Despite the ubiquity of recommender systems in the digital world, only few researchers have attempted to explain their functioning, whereas one major obstacle to their use is the problem of societal acceptability and trustworthiness. Indeed, recommender systems direct user choices to a large extent and their impact is important as they give access to only a small part of the range of items (e.g., products and/or services), as the submerged part of the iceberg. Consequently, they limit access to other resources. The potentially negative effects of these systems have been pointed out as phenomena like echo chambers and winner-take-all effects, because the internal logic of these systems is to likely enclose the consumer in a “déjà vu” loop. Therefore, it is crucial to provide explanations of such recommender systems and to identify the user data that led the respective system to make the individual recommendations. This then makes it possible to evaluate recommender systems not only regarding their effectiveness (i.e., their capability to recommend an item that was actually chosen by the user), but also with respect to the diversity, relevance and timeliness of the active data used for the recommendation. In this paper, we propose a deep analysis of two state-of-the-art models learnt on four datasets based on the identification of the items or the sequences of items actively used by the models. Our proposed methods are based on subgroup discovery with different pattern languages (i.e., itemsets and sequences). Specifically, we provide interpretable explanations of the recommendations of the Top-N items, which are useful to compare different models. Ultimately, these can then be used to present simple and understandable patterns to explain the reasons behind a generated recommendation to the user.},
  archive      = {J_DMKD},
  author       = {Iferroudjene, Mouloud and Lonjarret, Corentin and Robardet, Céline and Plantevit, Marc and Atzmueller, Martin},
  doi          = {10.1007/s10618-022-00897-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {833-872},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Methods for explaining top-N recommendations through subgroup discovery},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecast evaluation for data scientists: Common pitfalls and
best practices. <em>DMKD</em>, <em>37</em>(2), 788–832. (<a
href="https://doi.org/10.1007/s10618-022-00894-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent trends in the Machine Learning (ML) and in particular Deep Learning (DL) domains have demonstrated that with the availability of massive amounts of time series, ML and DL techniques are competitive in time series forecasting. Nevertheless, the different forms of non-stationarities associated with time series challenge the capabilities of data-driven ML models. Furthermore, due to the domain of forecasting being fostered mainly by statisticians and econometricians over the years, the concepts related to forecast evaluation are not the mainstream knowledge among ML researchers. We demonstrate in our work that as a consequence, ML researchers oftentimes adopt flawed evaluation practices which results in spurious conclusions suggesting methods that are not competitive in reality to be seemingly competitive. Therefore, in this work we provide a tutorial-like compilation of the details associated with forecast evaluation. This way, we intend to impart the information associated with forecast evaluation to fit the context of ML, as means of bridging the knowledge gap between traditional methods of forecasting and adopting current state-of-the-art ML techniques.We elaborate the details of the different problematic characteristics of time series such as non-normality and non-stationarities and how they are associated with common pitfalls in forecast evaluation. Best practices in forecast evaluation are outlined with respect to the different steps such as data partitioning, error calculation, statistical testing, and others. Further guidelines are also provided along selecting valid and suitable error measures depending on the specific characteristics of the dataset at hand.},
  archive      = {J_DMKD},
  author       = {Hewamalage, Hansika and Ackermann, Klaus and Bergmeir, Christoph},
  doi          = {10.1007/s10618-022-00894-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {788-832},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Forecast evaluation for data scientists: Common pitfalls and best practices},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BDRI: Block decomposition based on relational interaction
for knowledge graph completion. <em>DMKD</em>, <em>37</em>(2), 767–787.
(<a href="https://doi.org/10.1007/s10618-023-00918-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are large-scale semantic networks designed to describe real-world facts. However, existing KGs typically contain only a small subset of all possible facts. Knowledge graph completion (KGC) is a task of inferring missing facts based on existing facts, which can help KGs become more complete. Tensor decomposition algorithms have proved promising for KGC problems. In this paper, we propose block decomposition based on relational interaction for knowledge graph completion (BDRI), a novel and robust model based on block term decomposition of the binary tensor representation of knowledge graph triples. Further, BDRI considers that the inverse relation, as one of the most important relation types, not only occupies a large proportion in real-world facts but also has an impact on other relation types. Although some existing models also take into account the importance of inverse relations, it is not enough to learn inverse relations independently. BDRI strengthens the fusion of forward relations and inverse relations by introducing inverse relations into the model in an enhanced way. We prove BDRI is full expressiveness and derive the bound on its entity and relation embedding dimensionality and smaller than the bound of SimplE and ComplEx. Experimental results on five public datasets show the effectiveness of BDRI.},
  archive      = {J_DMKD},
  author       = {Yu, Mei and Guo, Jiujiang and Yu, Jian and Xu, Tianyi and Zhao, Mankun and Liu, Hongwei and Li, Xuewei and Yu, Ruiguo},
  doi          = {10.1007/s10618-023-00918-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {767-787},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {BDRI: Block decomposition based on relational interaction for knowledge graph completion},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring uplift modeling with high class imbalance.
<em>DMKD</em>, <em>37</em>(2), 736–766. (<a
href="https://doi.org/10.1007/s10618-023-00917-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uplift modeling refers to individual level causal inference. Existing research on the topic ignores one prevalent and important aspect: high class imbalance. For instance in online environments uplift modeling is used to optimally target ads and discounts, but very few users ever end up clicking an ad or buying. One common approach to deal with imbalance in classification is by undersampling the dataset. In this work, we show how undersampling can be extended to uplift modeling. We propose four undersampling methods for uplift modeling. We compare the proposed methods empirically and show when some methods have a tendency to break down. One key observation is that accounting for the imbalance is particularly important for uplift random forests, which explains the poor performance of the model in earlier works. Undersampling is also crucial for class-variable transformation based models.},
  archive      = {J_DMKD},
  author       = {Nyberg, Otto and Klami, Arto},
  doi          = {10.1007/s10618-023-00917-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {736-766},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exploring uplift modeling with high class imbalance},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual projection for quantile regression in vertically
partitioned big data. <em>DMKD</em>, <em>37</em>(2), 710–735. (<a
href="https://doi.org/10.1007/s10618-022-00914-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard regression techniques model only the mean of the response variable. Quantile regression (QR) is more powerful in that it depicts a comprehensive relationship between the response variable and independent covariates at different quantiles. It is particularly useful for non-normally distributed data with skewness or heterogeneity, which appear routinely in many scientific fields, such as economics, finance, public health and biology. Although its theory has been well developed in the literature, its computation in big data still faces multiple challenges, especially for vertically stored big data in modern distributed environments, where communication efficiency and security are usually the primary considerations. While the popular alternating direction method of multipliers (ADMM) provides a general computational solution, its slow convergence becomes a bottleneck when communication cost dominates local computational consumption, such as Internet of Things (IoT) networks. Motivated by the residual projection technique, in this paper we propose an innovative iterative parallel framework, PIQR, that converges faster and has a more secure data transmission plan, and establish its convergence property. This framework is further extended to composite quantile regression (CQR), a modified QR technique that improves estimation efficiency at extreme quantiles. Simulation studies show that both the ADMM-based method and the PIQR enjoy favorable estimation accuracy in distributed environments. While PIQR is inferior to the ADMM-based method at local computation, it requires much fewer iterations to achieve convergence, and hence significantly improves the overall computational efficiency when communication cost is the dominating factor. Moreover, PIQR transmits only data involving the residual information between different machines, and can better prevent the leakage of important data information compared with the ADMM-based method.},
  archive      = {J_DMKD},
  author       = {Fan, Ye and Li, Jr-Shin and Lin, Nan},
  doi          = {10.1007/s10618-022-00914-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {710-735},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Residual projection for quantile regression in vertically partitioned big data},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MERLIN++: Parameter-free discovery of time series anomalies.
<em>DMKD</em>, <em>37</em>(2), 670–709. (<a
href="https://doi.org/10.1007/s10618-022-00876-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The burgeoning age of IoT has reinforced the need for robust time series anomaly detection. While there are hundreds of anomaly detection methods in the literature, one definition, time series discords, has emerged as a competitive and popular choice for practitioners. Time series discords are subsequences of a time series that are maximally far away from their nearest neighbors. Perhaps the most attractive feature of discords is their simplicity. Unlike many of the parameter-laden methods proposed, discords require only a single parameter to be set by the user: the subsequence length. We believe that the utility of discords is reduced by sensitivity to even this single user choice. The obvious solution to this problem, computing discords of all lengths then selecting the best anomalies (under some measure), appears at first glance to be computationally untenable. However, in this work we discuss MERLIN, a recently introduced algorithm that can efficiently and exactly find discords of all lengths in massive time series archives. By exploiting computational redundancies, MERLIN is two orders of magnitude faster than comparable algorithms. Moreover, we show that by exploiting a little-known indexing technique called Orchard’s algorithm, we can create a new algorithm called MERLIN++, which is an order of magnitude faster than MERLIN, yet produces identical results. We demonstrate the utility of our ideas on a large and diverse set of experiments and show that MERLIN++ can discover subtle anomalies that defy existing algorithms or even careful human inspection. We further compare to five state-of-the-art rival methods, on the largest benchmark dataset for this task, and show that MERLIN++ is superior in terms of accuracy and speed.},
  archive      = {J_DMKD},
  author       = {Nakamura, Takaaki and Mercer, Ryan and Imamura, Makoto and Keogh, Eamonn},
  doi          = {10.1007/s10618-022-00876-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {670-709},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MERLIN++: Parameter-free discovery of time series anomalies},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAMP: Accurate time series anomaly detection on trillions of
datapoints and ultra-fast arriving data streams. <em>DMKD</em>,
<em>37</em>(2), 627–669. (<a
href="https://doi.org/10.1007/s10618-022-00911-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series anomaly detection is one of the most active areas of research in data mining, with dozens of new approaches been suggested each year. In spite of all these creative solutions proposed for this problem, recent empirical evidence suggests that the time series discord, a relatively simple twenty-year old distance-based technique, remains among the state-of-art techniques. While there are many algorithms for computing the time series discords, they all have limitations. First, they are limited to the batch case, whereas the online case is more actionable. Second, these algorithms exhibit poor scalability beyond tens of thousands of datapoints. In this work we introduce DAMP, a novel algorithm that addresses both these issues. DAMP computes exact left-discords on fast arriving streams, at up to 300,000 Hz using a commodity desktop. This allows us to find time series discords in datasets with trillions of datapoints for the first time. We will demonstrate the utility of our algorithm with the most ambitious set of time series anomaly detection experiments ever conducted. We will further show that our speedup improvements can be applied in the multidimensional case.},
  archive      = {J_DMKD},
  author       = {Lu, Yue and Wu, Renjie and Mueen, Abdullah and Zuluaga, Maria A. and Keogh, Eamonn},
  doi          = {10.1007/s10618-022-00911-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {627-669},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {DAMP: Accurate time series anomaly detection on trillions of datapoints and ultra-fast arriving data streams},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On computing exact means of time series using the
move-split-merge metric. <em>DMKD</em>, <em>37</em>(2), 595–626. (<a
href="https://doi.org/10.1007/s10618-022-00908-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing an accurate mean of a set of time series is a critical task in applications like nearest-neighbor classification and clustering of time series. While there are many distance functions for time series, the most popular distance function used for the computation of time series means is the non-metric dynamic time warping (DTW) distance. A recent algorithm for the exact computation of a DTW-Mean has a running time of $${\mathcal {O}}(n^{2k+1}2^kk)$$ , where k denotes the number of time series and n their maximum length. In this paper, we study the mean problem for the move-split-merge (MSM) metric that not only offers high practical accuracy for time series classification but also carries of the advantages of the metric properties that enable further diverse applications. The main contribution of this paper is an exact and efficient algorithm for the MSM-Mean problem of time series. The running time of our algorithm is $${\mathcal {O}}(n^{k+3}2^k k^3 )$$ , and thus better than the previous DTW-based algorithm. The results of an experimental comparison confirm the running time superiority of our algorithm in comparison to the DTW-Mean competitor. Moreover, we introduce a heuristic to improve the running time significantly without sacrificing much accuracy.},
  archive      = {J_DMKD},
  author       = {Holznigenkemper, Jana and Komusiewicz, Christian and Seeger, Bernhard},
  doi          = {10.1007/s10618-022-00908-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {595-626},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On computing exact means of time series using the move-split-merge metric},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional classwise principal component analysis: A
classification framework for functional data analysis. <em>DMKD</em>,
<em>37</em>(2), 552–594. (<a
href="https://doi.org/10.1007/s10618-022-00898-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, functional data analysis has been successfully applied in the field of high dimensional data classification. In this paper, we present a classification framework using functional data and classwise Principal Component Analysis (PCA). Our proposed method can be used in high dimensional time series data which typically suffers from small sample size problem. Our method extracts a piecewise linear functional feature space and is particularly suitable for hard classification problems. The proposed framework converts time series data into functional data and uses classwise functional PCA for feature extraction followed by classification using a Bayesian linear classifier. We demonstrate the efficacy of our proposed method by applying it to both synthetic data sets and real time series data from diverse fields including but not limited to neuroscience, food science, medical sciences and chemometrics.},
  archive      = {J_DMKD},
  author       = {Chatterjee, Avishek and Mazumder, Satyaki and Das, Koel},
  doi          = {10.1007/s10618-022-00898-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {552-594},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Functional classwise principal component analysis: A classification framework for functional data analysis},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HARPA: Hierarchical attention with relation paths for
knowledge graph embedding adversarial learning. <em>DMKD</em>,
<em>37</em>(2), 521–551. (<a
href="https://doi.org/10.1007/s10618-022-00888-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) aims to map the knowledge graph into a low-dimensional continuous vector space and provide a unified underlying representation for downstream tasks. Recently, graph neural network (GNN) has been widely used in knowledge graph embedding because of its powerful feature extraction ability, and most KGE models based on GNN use aggregation operations to extract potential information from the triples. Unfortunately, they only emphasize entity embedding and use shallow operations to update relations. As a result, the learning of relation embedding is relatively simple. And they ignore the rich inference information contained in the multi-hop paths. In addition, their complex network structure lacks regularization constraint, which is prone to the over-fitting problem. Therefore, this paper proposes a novel hierarchical attention with relation paths model for knowledge graph embedding adversarial learning (HARPA). HARPA constructs a two-layer attention encoder to learn the information of triples and neighborhoods at the triples-level and further utilizes the rich inference information of paths to deeply learn relation embedding at the paths-level. Besides, HARPA proposes an improved generative adversarial network (GAN) named I-GAN as the regularization term of the model, which imposes constraints on the process of learning embedding and enables the model to learn high-quality and robust embedding. The link prediction experiments on four general knowledge graphs show that the HARPA model outperforms state-of-the-art methods.},
  archive      = {J_DMKD},
  author       = {Zhang, Naixin and Wang, Jinmeng and He, Jieyue},
  doi          = {10.1007/s10618-022-00888-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {521-551},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {HARPA: Hierarchical attention with relation paths for knowledge graph embedding adversarial learning},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid bayesian network discovery with latent variables by
scoring multiple interventions. <em>DMKD</em>, <em>37</em>(1), 476–520.
(<a href="https://doi.org/10.1007/s10618-022-00882-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian Networks (BNs), the direction of edges is crucial for causal reasoning and inference. However, Markov equivalence class considerations mean it is not always possible to establish edge orientations, which is why many BN structure learning algorithms cannot orientate all edges from purely observational data. Moreover, latent confounders can lead to false positive edges. Relatively few methods have been proposed to address these issues. In this work, we present the hybrid mFGS-BS (majority rule and Fast Greedy equivalence Search with Bayesian Scoring) algorithm for structure learning from discrete data that involves an observational data set and one or more interventional data sets. The algorithm assumes causal insufficiency in the presence of latent variables and produces a Partial Ancestral Graph (PAG). Structure learning relies on a hybrid approach and a novel Bayesian scoring paradigm that calculates the posterior probability of each directed edge being added to the learnt graph. Experimental results based on well-known networks of up to 109 variables and 10 k sample size show that mFGS-BS improves structure learning accuracy relative to the state-of-the-art and it is computationally efficient.},
  archive      = {J_DMKD},
  author       = {Chobtham, Kiattikun and Constantinou, Anthony C. and Kitson, Neville K.},
  doi          = {10.1007/s10618-022-00882-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {476-520},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hybrid bayesian network discovery with latent variables by scoring multiple interventions},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized impurity reduction: Accurate decision trees with
complexity guarantees. <em>DMKD</em>, <em>37</em>(1), 434–475. (<a
href="https://doi.org/10.1007/s10618-022-00884-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are popular classification models, providing high accuracy and intuitive explanations. However, as the tree size grows the model interpretability deteriorates. Traditional tree-induction algorithms, such as C4.5 and CART, rely on impurity-reduction functions that promote the discriminative power of each split. Thus, although these traditional methods are accurate in practice, there has been no theoretical guarantee that they will produce small trees. In this paper, we justify the use of a general family of impurity functions, including the popular functions of entropy and Gini-index, in scenarios where small trees are desirable, by showing that a simple enhancement can equip them with complexity guarantees. We consider a general setting, where objects to be classified are drawn from an arbitrary probability distribution, classification can be binary or multi-class, and splitting tests are associated with non-uniform costs. As a measure of tree complexity, we adopt the expected cost to classify an object drawn from the input distribution, which, in the uniform-cost case, is the expected number of tests. We propose a tree-induction algorithm that gives a logarithmic approximation guarantee on the tree complexity. This approximation factor is tight up to a constant factor under mild assumptions. The algorithm recursively selects a test that maximizes a greedy criterion defined as a weighted sum of three components. The first two components encourage the selection of tests that improve the balance and the cost-efficiency of the tree, respectively, while the third impurity-reduction component encourages the selection of more discriminative tests. As shown in our empirical evaluation, compared to the original heuristics, the enhanced algorithms strike an excellent balance between predictive accuracy and tree complexity.},
  archive      = {J_DMKD},
  author       = {Zhang, Guangyi and Gionis, Aristides},
  doi          = {10.1007/s10618-022-00884-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {434-475},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Regularized impurity reduction: Accurate decision trees with complexity guarantees},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using differential evolution for an attribute-weighted
inverted specific-class distance measure for nominal attributes.
<em>DMKD</em>, <em>37</em>(1), 409–433. (<a
href="https://doi.org/10.1007/s10618-022-00881-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metrics are central to many machine learning algorithms. Improving their measurement performance can greatly affect the classification result of these algorithms. The inverted specific-class distance measure (ISCDM) is effective in handling nominal attributes rather than numeric ones, especially if a training set contains missing values and non-class attribute noise. However, similar to many other distance metrics, this method is still based on the attribute independence assumption, which is obviously infeasible for many real-world datasets. In this study, we focus on establishing an improved ISCDM by using an attribute weighting scheme to address its attribute independence assumption. We use a differential evolution (DE) algorithm to determine better attribute weights for our improved ISCDM, which is thus denoted as DE-AWISCDM. We experimentally tested our DE-AWISCDM on 29 UCI datasets, and find that it significantly outperforms the original ISCDM and other state-of-the-art methods with respect to negative conditional log likelihood and root relative squared error.},
  archive      = {J_DMKD},
  author       = {Gong, Fang and Guo, Xingfeng and Wang, Dianhong},
  doi          = {10.1007/s10618-022-00881-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {409-433},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Using differential evolution for an attribute-weighted inverted specific-class distance measure for nominal attributes},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical message-passing graph neural networks.
<em>DMKD</em>, <em>37</em>(1), 381–408. (<a
href="https://doi.org/10.1007/s10618-022-00890-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have become a prominent approach to machine learning with graphs and have been increasingly applied in a multitude of domains. Nevertheless, since most existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled: (i) they are costly in encoding long-range information spanning the graph structure; (ii) they are failing to encode features in the high-order neighbourhood in the graphs as they only perform information aggregation across the observed edges in the original graph. To deal with these two issues, we propose a novel Hierarchical Message-passing Graph Neural Networks framework. The key idea is generating a hierarchical structure that re-organises all nodes in a flat graph into multi-level super graphs, along with innovative intra- and inter-level propagation manners. The derived hierarchy creates shortcuts connecting far-away nodes so that informative long-range interactions can be efficiently accessed via message passing and incorporates meso- and macro-level semantics into the learned node representations. We present the first model to implement this framework, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), with the assistance of a hierarchical community detection algorithm. The theoretical analysis illustrates HC-GNN’s remarkable capacity in capturing long-range information without introducing heavy additional computation complexity. Empirical experiments conducted on 9 datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection. Moreover, the model analysis further demonstrates HC-GNN’s robustness facing graph sparsity and the flexibility in incorporating different GNN encoders.},
  archive      = {J_DMKD},
  author       = {Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun},
  doi          = {10.1007/s10618-022-00890-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {381-408},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hierarchical message-passing graph neural networks},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment analysis in tweets: An assessment study from
classical to modern word representation models. <em>DMKD</em>,
<em>37</em>(1), 318–380. (<a
href="https://doi.org/10.1007/s10618-022-00853-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of social media networks, such as Twitter, plenty of user-generated data emerge daily. The short texts published on Twitter – the tweets – have earned significant attention as a rich source of information to guide many decision-making processes. However, their inherent characteristics, such as the informal, and noisy linguistic style, remain challenging to many natural language processing (NLP) tasks, including sentiment analysis. Sentiment classification is tackled mainly by machine learning-based classifiers. The literature has adopted different types of word representation models to transform tweets to vector-based inputs to feed sentiment classifiers. The representations come from simple count-based methods, such as bag-of-words, to more sophisticated ones, such as BERTweet, built upon the trendy BERT architecture. Nevertheless, most studies mainly focus on evaluating those models using only a small number of datasets. Despite the progress made in recent years in language modeling, there is still a gap regarding a robust evaluation of induced embeddings applied to sentiment analysis on tweets. Furthermore, while fine-tuning the model from downstream tasks is prominent nowadays, less attention has been given to adjustments based on the specific linguistic style of the data. In this context, this study fulfills an assessment of existing neural language models in distinguishing the sentiment expressed in tweets, by using a rich collection of 22 datasets from distinct domains and five classification algorithms. The evaluation includes static and contextualized representations. Contexts are assembled from Transformer-based autoencoder models that are also adapted based on the masked language model task, using a plethora of strategies.},
  archive      = {J_DMKD},
  author       = {Barreto, Sérgio and Moura, Ricardo and Carvalho, Jonnathan and Paes, Aline and Plastino, Alexandre},
  doi          = {10.1007/s10618-022-00853-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {318-380},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sentiment analysis in tweets: An assessment study from classical to modern word representation models},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural iterative lexicographic autoencoded node
representation. <em>DMKD</em>, <em>37</em>(1), 289–317. (<a
href="https://doi.org/10.1007/s10618-022-00880-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning approaches are effective to automatically extract relevant hidden features from graphs. Previous related work in graph representation learning can be divided into connectivity and structural-based. Connectivity-based representation learning methods work on the assumption that neighboring nodes should have similar representations. While structural node representation learning assumes that nodes with the same structure should have identical representations; structural representation learning is suitable for node classification and regression tasks. Possible drawbacks of current structural node representation learning approaches are prohibitive execution time complexity and the inability to entirely preserve structural information. In this work, we propose SILA, a Structural Iterative Lexicographic Autoencoded approach for node representation learning. This new iterative approach presents a small number of iterations, and compared with the method presented in the literature, shows better performance in preserving structural information for both classification and regression tasks.},
  archive      = {J_DMKD},
  author       = {Joaristi, Mikel and Serra, Edoardo},
  doi          = {10.1007/s10618-022-00880-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {289-317},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Structural iterative lexicographic autoencoded node representation},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving embedded knowledge graph multi-hop question
answering by introducing relational chain reasoning. <em>DMKD</em>,
<em>37</em>(1), 255–288. (<a
href="https://doi.org/10.1007/s10618-022-00891-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) aims to answer user-questions from a knowledge graph (KG) by identifying the reasoning relations between topic entity and answer. As a complex branch task of KGQA, multi-hop KGQA requires reasoning over the multi-hop relational chain preserved in KG to arrive at the right answer. Despite recent successes, the existing works on answering multi-hop complex questions still face the following challenges: (i) The absence of an explicit relational chain order reflected in user-question stems from a misunderstanding of a user’s intentions. (ii) Incorrectly capturing relational types on weak supervision of which dataset lacks intermediate reasoning chain annotations due to expensive labeling cost. (iii) Failing to consider implicit relations between the topic entity and the answer implied in structured KG because of limited neighborhoods size constraint in subgraph retrieval-based algorithms. To address these issues in multi-hop KGQA, we propose a novel model herein, namely Relational Chain based Embedded KGQA (Rce-KGQA), which simultaneously utilizes the explicit relational chain revealed in natural language question and the implicit relational chain stored in structured KG. Our extensive empirical study on three open-domain benchmarks proves that our method significantly outperforms the state-of-the-art counterparts like GraftNet, PullNet and EmbedKGQA. Comprehensive ablation experiments also verify the effectiveness of our method on the multi-hop KGQA task. We have made our model’s source code available at github: https://github.com/albert-jin/Rce-KGQA .},
  archive      = {J_DMKD},
  author       = {Jin, Weiqiang and Zhao, Biao and Yu, Hang and Tao, Xi and Yin, Ruiping and Liu, Guizhong},
  doi          = {10.1007/s10618-022-00891-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {255-288},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Improving embedded knowledge graph multi-hop question answering by introducing relational chain reasoning},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Informative pseudo-labeling for graph neural networks with
few labels. <em>DMKD</em>, <em>37</em>(1), 228–254. (<a
href="https://doi.org/10.1007/s10618-022-00879-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved state-of-the-art results for semi-supervised node classification on graphs. Nevertheless, the challenge of how to effectively learn GNNs with very few labels is still under-explored. As one of the prevalent semi-supervised methods, pseudo-labeling has been proposed to explicitly address the label scarcity problem. It is the process of augmenting the training set with pseudo-labeled unlabeled nodes to retrain a model in a self-training cycle. However, the existing pseudo-labeling approaches often suffer from two major drawbacks. First, these methods conservatively expand the label set by selecting only high-confidence unlabeled nodes without assessing their informativeness. Second, these methods incorporate pseudo-labels to the same loss function with genuine labels, ignoring their distinct contributions to the classification task. In this paper, we propose a novel informative pseudo-labeling framework (InfoGNN) to facilitate learning of GNNs with very few labels. Our key idea is to pseudo-label the most informative nodes that can maximally represent the local neighborhoods via mutual information maximization. To mitigate the potential label noise and class-imbalance problem arising from pseudo-labeling, we also carefully devise a generalized cross entropy with a class-balanced regularization to incorporate pseudo-labels into model retraining. Extensive experiments on six real-world graph datasets validate that our proposed approach significantly outperforms state-of-the-art baselines and competitive self-supervised methods on graphs.},
  archive      = {J_DMKD},
  author       = {Li, Yayong and Yin, Jie and Chen, Ling},
  doi          = {10.1007/s10618-022-00879-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {228-254},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Informative pseudo-labeling for graph neural networks with few labels},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiated matching for individual and average treatment
effect estimation. <em>DMKD</em>, <em>37</em>(1), 205–227. (<a
href="https://doi.org/10.1007/s10618-022-00886-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental problem of causal inference is estimating treatment eect with observational data where variables are confounded. The traditional way of controlling the confounding bias is to match units with different treatments but similar variables. However, traditional matching methods fail on selection and differentiation among the pool of numerous potential confounders, leading to possible under-performance. In this paper, we give a theoretical analysis of confounder differentiation and propose a novel Differentiated Matching (DM) algorithm for both individual and average treatment effect estimation by learning confounder weights for variable differentiation and unit matching. To address the distribution shift in confounder weights learning, we further propose a Propensity Score based DM (PSDM) algorithm by weighted regression with the inverse of the propensity score. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed algorithms achieve better performance than other matching methods on treatment effect estimation.},
  archive      = {J_DMKD},
  author       = {Ziyu, Zhao and Kuang, Kun and Li, Bo and Cui, Peng and Wu, Runze and Xiao, Jun and Wu, Fei},
  doi          = {10.1007/s10618-022-00886-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {205-227},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Differentiated matching for individual and average treatment effect estimation},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A methodology for refined evaluation of neural code
completion approaches. <em>DMKD</em>, <em>37</em>(1), 167–204. (<a
href="https://doi.org/10.1007/s10618-022-00866-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code completion has become an indispensable feature of modern Integrated Development Environments. In recent years, many approaches have been proposed to tackle this task. However, it is hard to compare between the models without explicitly re-evaluating them due to the differences of used benchmarks (e.g. datasets and evaluation metrics). Besides, almost all of these works report the accuracy of the code completion models as aggregated metrics averaged over all types of code tokens. Such evaluations make it difficult to assess the potential improvements for particularly relevant types of tokens (i.e. method or variable names), and blur the differences between the performance of the methods. In this paper, we propose a methodology called Code Token Type Taxonomy (CT3) to address the issue of using aggregated metrics. We identify multiple dimensions relevant for code prediction (e.g. syntax type, context, length), partition the tokens into meaningful types along each dimension, and compute individual accuracies by type. We illustrate the utility of this methodology by comparing the code completion accuracy of a Transformer-based model in two variants: with closed, and with open vocabulary. Our results show that the refined evaluation provides a more detailed view of the differences and indicates where further work is needed. We also survey the state-of-the-art of Machine Learning-based code completion models to illustrate that there is a demand for a set of standardized benchmarks for code completion approaches. Furthermore, we find that the open vocabulary model is significantly more accurate for relevant code token types such as usage of (defined) variables and literals.},
  archive      = {J_DMKD},
  author       = {Le, Kim Tuyen and Rashidi, Gabriel and Andrzejak, Artur},
  doi          = {10.1007/s10618-022-00866-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {167-204},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A methodology for refined evaluation of neural code completion approaches},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConvMOS: Climate model output statistics with deep learning.
<em>DMKD</em>, <em>37</em>(1), 136–166. (<a
href="https://doi.org/10.1007/s10618-022-00877-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate models are the tool of choice for scientists researching climate change. Like all models they suffer from errors, particularly systematic and location-specific representation errors. One way to reduce these errors is model output statistics (MOS) where the model output is fitted to observational data with machine learning. In this work, we assess the use of convolutional Deep Learning climate MOS approaches and present the ConvMOS architecture which is specifically designed based on the observation that there are systematic and location-specific errors in the precipitation estimates of climate models. We apply ConvMOS models to the simulated precipitation of the regional climate model REMO, showing that a combination of per-location model parameters for reducing location-specific errors and global model parameters for reducing systematic errors is indeed beneficial for MOS performance. We find that ConvMOS models can reduce errors considerably and perform significantly better than three commonly used MOS approaches and plain ResNet and U-Net models in most cases. Our results show that non-linear MOS models underestimate the number of extreme precipitation events, which we alleviate by training models specialized towards extreme precipitation events with the imbalanced regression method DenseLoss. While we consider climate MOS, we argue that aspects of ConvMOS may also be beneficial in other domains with geospatial data, such as air pollution modeling or weather forecasts.},
  archive      = {J_DMKD},
  author       = {Steininger, Michael and Abel, Daniel and Ziegler, Katrin and Krause, Anna and Paeth, Heiko and Hotho, Andreas},
  doi          = {10.1007/s10618-022-00877-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {136-166},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ConvMOS: Climate model output statistics with deep learning},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ContE: Contextualized knowledge graph embedding for circular
relations. <em>DMKD</em>, <em>37</em>(1), 110–135. (<a
href="https://doi.org/10.1007/s10618-022-00851-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding has been proposed to embed entities and relations into continuous vector spaces, which can benefit various downstream tasks, such as question answering and recommender systems, etc. A common assumption of existing knowledge graph embedding models is that the relation is a translation vector connecting the embedded head entity and tail entity. However, based on this assumption, the same relation connecting multiple entities may form a circle and lead to mistakes during computing process. To solve this so-called circular relation problem that has been ignored previously, we propose a novel method called ContE (Contextualized Embedding) for knowledge graphs by exploring collaborative relations. Specifically, each collaborative relation combines an explicit relation and a latent relation, where the explicit one is the original relation between two entities, and the latent one is introduced to capture the implicit interactions obtained via the context information of the two entities. With the collaborative relations, the same relations will be embedded varying across different contexts, and the sharing of similar semantics will be guaranteed as well. We conduct extensive experiments in link prediction and triple classification tasks on five benchmark datasets. The experimental results demonstrate that the proposed ContE achieves improvements over several baselines.},
  archive      = {J_DMKD},
  author       = {Ma, Ting and Li, Mingming and Lv, Shangwen and Zhu, Fuqing and Huang, Longtao and Hu, Songlin},
  doi          = {10.1007/s10618-022-00851-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {110-135},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ContE: Contextualized knowledge graph embedding for circular relations},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large scale k-means clustering using GPUs. <em>DMKD</em>,
<em>37</em>(1), 67–109. (<a
href="https://doi.org/10.1007/s10618-022-00869-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-means algorithm is widely used for clustering, compressing, and summarizing vector data. We present a fast and memory-efficient GPU-based algorithm for exact k-means, Asynchronous Selective Batched K-means (ASB K-means). Unlike most GPU-based k-means algorithms that require loading the whole dataset onto the GPU for clustering, the amount of GPU memory required to run our algorithm can be chosen to be much smaller than the size of the whole dataset. Thus, our algorithm can cluster datasets whose size exceeds the available GPU memory. The algorithm works in a batched fashion and applies the triangle inequality in each k-means iteration to omit a data point if its membership assignment, i.e., the cluster it belongs to, remains unchanged, thus significantly reducing the number of data points that need to be transferred between the CPU’s RAM and the GPU’s global memory and enabling the algorithm to very efficiently process large datasets. Our algorithm can be substantially faster than a GPU-based implementation of standard k-means even in situations when application of the standard algorithm is feasible because the whole dataset fits into GPU memory. Experiments show that ASB K-means can run up to 15x times faster than a standard GPU-based implementation of k-means, and it also outperforms the GPU-based k-means implementation in NVIDIA’s open-source RAPIDS machine learning library on all the datasets used in our experiments.},
  archive      = {J_DMKD},
  author       = {Li, Mi and Frank, Eibe and Pfahringer, Bernhard},
  doi          = {10.1007/s10618-022-00869-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {67-109},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Large scale K-means clustering using GPUs},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Category tree distance: A taxonomy-based transaction
distance for web user analysis. <em>DMKD</em>, <em>37</em>(1), 39–66.
(<a href="https://doi.org/10.1007/s10618-022-00874-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of webpage services, huge amounts of customer transaction data are flooded in cyberspace, which are getting more and more useful for profiling users and making recommendations. Since web user transaction data are usually multi-modal, heterogeneous and large-scale, the traditional data analysis methods meet new challenges. One of the challenges is the distance definition on two transaction data or two web users. The distance definition takes an important role in further analysis, such as the cluster analysis or k-nearest neighbor query. We introduce a category tree distance in this paper, which makes use of the product taxonomy information to convert the user transaction data to vectors. Then, the similarity between web users can be evaluated by the vectors from their transaction data. The properties of the distance like upper and lower bounds and the complexity analysis are also given in the paper. To investigate the performance of the proposal, we conduct experiments on real web user transaction data. The results show that the proposed distance outperforms the other distances on user transaction analysis.},
  archive      = {J_DMKD},
  author       = {Zhang, Yinjia and Zhao, Qinpei and Shi, Yang and Li, Jiangfeng and Rao, Weixiong},
  doi          = {10.1007/s10618-022-00874-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {39-66},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Category tree distance: A taxonomy-based transaction distance for web user analysis},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical joint human-machine exploration of industrial time
series using the matrix profile. <em>DMKD</em>, <em>37</em>(1), 1–38.
(<a href="https://doi.org/10.1007/s10618-022-00871-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements and widespread adaptation of new technology in industry have made industrial time series data more available than ever before. With this development grows the need for versatile methods for mining industrial time series data. This paper introduces a practical approach for joint human-machine exploration of industrial time series data using the Matrix Profile, and presents some challenges involved. The approach is demonstrated on three real-life industrial data sets to show how it enables the user to quickly extract semantic information, detect cycles, find deviating patterns, and gain a deeper understanding of the time series. A benchmark test is also presented on ECG (electrocardiogram) data, showing that the approach works well in comparison to previously suggested methods for extracting relevant time series motifs.},
  archive      = {J_DMKD},
  author       = {Nilsson, Felix and Bouguelia, Mohamed-Rafik and Rögnvaldsson, Thorsteinn},
  doi          = {10.1007/s10618-022-00871-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Practical joint human-machine exploration of industrial time series using the matrix profile},
  volume       = {37},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
