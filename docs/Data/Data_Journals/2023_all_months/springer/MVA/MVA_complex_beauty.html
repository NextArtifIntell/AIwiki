<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---130">MVA - 130</h2>
<ul>
<li><details>
<summary>
(2023). Similarity contrastive estimation for image and video soft
contrastive self-supervised learning. <em>MVA</em>, <em>34</em>(6),
1–19. (<a href="https://doi.org/10.1007/s00138-023-01444-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks. Source code is available here: https://github.com/juliendenize/eztorch .},
  archive      = {J_MVA},
  author       = {Denize, Julien and Rabarisoa, Jaonary and Orcesi, Astrid and Hérault, Romain},
  doi          = {10.1007/s00138-023-01444-9},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Similarity contrastive estimation for image and video soft contrastive self-supervised learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized few-shot learning under large scope by using
episode-wise regularizing imprinting. <em>MVA</em>, <em>34</em>(6),
1–20. (<a href="https://doi.org/10.1007/s00138-023-01445-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning explores machine learning tasks under input scarce conditions. In the past few years, meta-learning has demonstrated some advantages, but most of the meta-learning-based methods proposed in this field can only be applied to the restricted version of the problem, namely, the small-scope novel-only classification. It faces serious challenges when extending the problem from two practical direction. Firstly, the meta-learning conducted only improves sensitivity on a few novel target classes but do not maintain accuracy on all-class-oriented situation or more complicated visual learning tasks. Secondly, meta-learning performs poorly when recognizing objects from a large scope either based-classes are involved or not. In this paper, we focus on metric-based meta-learning. A key characteristic of this branch of models is its training stage relies on support-set to generate classifier on-the-fly in each iteration, which in turn limited their application on more complicated tasks. To overcome these limitations, we introduce a method that accomplishes few-shot oriented learning by iteratively using a traditional training routine and a parameter imprinting routine. Our approach closes the gap between small-scope and large-scope few-shot classifier by boosting the performances of previous approaches under large-scope settings. This is the first approach that achieves the effect of meta-learning by using traditional learning routings. The proposed approach is comparatively evaluated with a number of recent approaches on popular few-shot classification benchmarks and demonstrates better performance consistently.},
  archive      = {J_MVA},
  author       = {Sun, Nan and Tang, Yunxuan and Wang, Kun},
  doi          = {10.1007/s00138-023-01445-8},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Generalized few-shot learning under large scope by using episode-wise regularizing imprinting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CMNet: A novel model and design rationale based on
comparison studies and synergy of CNN and MetaFormer. <em>MVA</em>,
<em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01446-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional- and Transformer-based backbone architecture are two dominant, widely accepted, models in computer vision. Nevertheless, it is still a challenge, thus a focus of research, to decide which backbone architecture performs better, and under which circumstances. In this paper, we conduct an in-depth investigation into the differences of the macroscopic backbone design of the CNN and Transformer models with the ultimate purpose of developing new models to combine the strengths of both types of architectures for effective image classification. Specifically, we first analyze the model structures of both models and identified four main differences, then we design four sets of ablation experiments using the ImageNet-1K dataset with an image classification problem as an example to study the impacts of these four differences on model performance. Based on the experimental results, we derive four observations as rules of thumb for designing a vision model backbone architecture. Informed by the experiment findings, we then conceive a novel model called CMNet which marries the experiment-proved best design practices of CNN and Transformer architectures. Finally, we carry out extensive experiments on CMNet using the same dataset against baseline classifiers. Initial results prove CMNet achieves the highest top-1 accuracy of 80.08% on the ImageNet-1K validation set, this is a very competitive value compared to previous classical models with similar computational complexity. Details of the implementation, algorithms and codes, are publicly available on Github: https://github.com/Arwin-Yu/CMNet .},
  archive      = {J_MVA},
  author       = {Yu, Haowen and Chen, Liming},
  doi          = {10.1007/s00138-023-01446-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CMNet: A novel model and design rationale based on comparison studies and synergy of CNN and MetaFormer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multilayer human motion prediction perceptron by
aggregating repetitive motion. <em>MVA</em>, <em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01447-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction aims to forecast future human poses given a historical motion. Current state-of-the-art approaches rely on deep learning architectures of arbitrary complexity, such as Recurrent Neural Networks (RNN), Graph Convolutional Networks (GCN), and typically requires multiple training stages and more parameters. In addition, existing learning-based methods fail to model the observation that human motion tends to repeat itself. In summary, to address the problem of the existing methods neglecting the repetitive nature of human motion, we first introduced a Multi-level Attention Mechanism (MAM) that explicitly leverages this observation to find relevant historical information for predicting future motion. Instead of modeling frame-wise attention via pose similarity, the motion attention was extracted to capture the similarity between the current motion context and the historical motion sub-sequences. In this context, the use of different types of attention, computed at joint, body part, and full pose levels was studied. Furthermore, to address the complexity of existing algorithms based on deep learning architectures, a Fully connected Transpose MLP (FTMLP) model was introduced. By combining a MLP network with a fully connected and transposed layer to process the aggregated relevant past movements, the patterns of motion from the long-term history can be quickly and efficiently used to predict the future poses. The experimental results on standard motion prediction benchmark datasets Human3.6 M and CMU motion capture dataset show that our model is able to make accurate short- and long-term predictions.},
  archive      = {J_MVA},
  author       = {Geng, Lei and Yang, Wenzhu and Jiao, Yanyan and Zeng, Shuang and Chen, Xinting},
  doi          = {10.1007/s00138-023-01447-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A multilayer human motion prediction perceptron by aggregating repetitive motion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance benchmark of deep learning human pose estimation
for UAVs. <em>MVA</em>, <em>34</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s00138-023-01448-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is a computer vision application that estimates human body joints from images. It gives machines the capability to better understand the interaction between humans and the environment. For this accomplishment, many HPE methods have been deployed in robots, vehicles, and unmanned aerial vehicles (UAVs). This effort raised the challenge of balance between algorithm performance and efficiency, especially in UAVs, where computational resources are limited for saving battery power. Despite the considerable progress in the HPE problem, there are very few methods that are proposed to face this challenge. To highlight the severity of this fact, the proposed paper presents a brief review and an HPE benchmark from the aspect of algorithms performance and efficiency under UAV operation. More specifically, the contribution of HPE methods in the last 22 years is covered, along with the variety of methods that exist. The benchmark consists of 36 pose estimation models in 3 known datasets with metrics that fulfill the paper aspect. From the results, MobileNet-based models achieved competitive performance and the lowest computational cost, in comparison with ResNet-based models. Finally, benchmark results are projected in edge devices hardware specifications to analyze the appropriateness of these algorithms for UAV deployment.},
  archive      = {J_MVA},
  author       = {Kalampokas, Theofanis and Krinidis, Stelios and Chatzis, Vassilios and Papakostas, George A.},
  doi          = {10.1007/s00138-023-01448-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Performance benchmark of deep learning human pose estimation for UAVs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward phytoplankton parasite detection using autoencoders.
<em>MVA</em>, <em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-023-01450-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phytoplankton parasites are largely understudied microbial components with a potentially significant ecological influence on phytoplankton bloom dynamics. To better understand the impact of phytoplankton parasites, improved detection methods are needed to integrate phytoplankton parasite interactions into monitoring of aquatic ecosystems. Automated imaging devices commonly produce vast amounts of phytoplankton image data, but the occurrence of anomalous phytoplankton data in such datasets is rare. Thus, we propose an unsupervised anomaly detection system based on the similarity between the original and autoencoder-reconstructed samples. With this approach, we were able to reach an overall F1 score of 0.75 in nine phytoplankton species, which could be further improved by species-specific fine-tuning. The proposed unsupervised approach was further compared with the supervised Faster R-CNN-based object detector. Using this supervised approach and the model trained on plankton species and anomalies, we were able to reach a highest F1 score of 0.86. However, the unsupervised approach is expected to be more universal as it can also detect unknown anomalies and it does not require any annotated anomalous data that may not always be available in sufficient quantities. Although other studies have dealt with plankton anomaly detection in terms of non-plankton particles or air bubble detection, our paper is, according to our best knowledge, the first that focuses on automated anomaly detection considering putative phytoplankton parasites or infections.},
  archive      = {J_MVA},
  author       = {Bilik, Simon and Batrakhanov, Daniel and Eerola, Tuomas and Haraguchi, Lumi and Kraft, Kaisa and Van den Wyngaert, Silke and Kangas, Jonna and Sjöqvist, Conny and Madsen, Karin and Lensu, Lasse and Kälviäinen, Heikki and Horak, Karel},
  doi          = {10.1007/s00138-023-01450-x},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Toward phytoplankton parasite detection using autoencoders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of probabilistic subspaces for
multi-spectral and multi-temporal image-based disaster mapping.
<em>MVA</em>, <em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01451-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely identification of regions damaged by a natural disaster is critical for assessing the damages and reducing the human life cost. The increasing availability of satellite imagery and other remote sensing data has triggered research activities on development of algorithms for detection and monitoring of natural events. Here, we introduce an unsupervised subspace learning-based methodology that uses multi-temporal and multi-spectral satellite images to identify regions damaged by natural disasters. It first performs region delineation, matching, and fusion. Next, it applies subspace learning in the joint regional space to produce a change map. It identifies the damaged regions by estimating probabilistic subspace distances and rejecting the non-disaster changes. We evaluated the performance of our method on seven disaster datasets including four wildfire events, two flooding events, and a earthquake/tsunami event. We validated our results by calculating the dice similarity coefficient (DSC), and accuracy of classification between our disaster maps and ground-truth data. Our method produced average DSC values of 0.833 and 0.736, for wildfires and floods, respectively, and overall DSC of 0.855 for the tsunami event. The evaluation results support the applicability of our method to multiple types of natural disasters.},
  archive      = {J_MVA},
  author       = {Okorie, Azubuike and Kambhamettu, Chandra and Makrogiannnis, Sokratis},
  doi          = {10.1007/s00138-023-01451-w},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Unsupervised learning of probabilistic subspaces for multi-spectral and multi-temporal image-based disaster mapping},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable interactive projections of images. <em>MVA</em>,
<em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01452-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reductions (DR) help people make sense of image collections by organizing images in the 2D space based on similarities. However, they provide little support for explaining why images were placed together or apart in the 2D space. Additionally, they do not provide support for modifying and updating the 2D representation to explore new relationships and organizations of images. To address these problems, we present an interactive DR method for images that uses visual features extracted by a deep neural network to project the images into 2D space and provides visual explanations of image features that contributed to the 2D location. In addition, it allows people to directly manipulate the 2D projection space to define alternative relationships and explore subsequent projections of the images. With an iterative cycle of semantic interaction and explainable-AI feedback, people can explore complex visual relationships in image data. Our approach to human–AI interaction integrates visual knowledge from both human-mental models and pre-trained deep neural models to explore image data. We demonstrate our method through examples with collaborators in agricultural science and other applications. Additionally, we present a quantitative evaluation that assesses how well our method captures and incorporates feedback.},
  archive      = {J_MVA},
  author       = {Han, Huimin and Faust, Rebecca and Keith Norambuena, Brian Felipe and Lin, Jiayue and Li, Song and North, Chris},
  doi          = {10.1007/s00138-023-01452-9},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Explainable interactive projections of images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuro-augmented vision for evolutionary robotics.
<em>MVA</em>, <em>34</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s00138-023-01453-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents neuro-augmented vision for evolutionary robotics (NAVER), which aims to address the two biggest challenges in camera-equipped robot evolutionary controllers. The first challenge is that camera images typically require many inputs from the controller, which greatly increases the complexity of optimising the search space. The second challenge is that evolutionary controllers often cannot bridge the reality gap between simulation and the real world. This method utilises a variational autoencoder to compress the camera image into smaller input vectors that are easier to manage, while still retaining the relevant information of the original image. Automatic encoders are also used to remove unnecessary details from real-world images, in order to better align with images generated by simple visual simulators. NAVER is used to evolve the controller of a robot, which only uses camera inputs to navigate the maze based on visual cues and avoid collisions. The experimental results indicate that the controller evolved in simulation and transferred to the physical robot, where it successfully performed the same navigation task. The controller can navigate the maze using only visual information. The controller responds to visual cues and changes its behaviour accordingly. NAVER has shown great potential as it has successfully completed (so far) the most complex vision-based task controller in evolutionary robotics literature.},
  archive      = {J_MVA},
  author       = {Watt, Nathan and du Plessis, Mathys C.},
  doi          = {10.1007/s00138-023-01453-8},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Neuro-augmented vision for evolutionary robotics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised anomaly detection via knowledge distillation
with non-directly-coupled student block fusion. <em>MVA</em>,
<em>34</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01454-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, knowledge distillation has achieved excellent results in unsupervised anomaly detection. The representation difference of anomalies between teacher and student model is an essential basis for unsupervised anomaly detection. To fully exploit the diversity of anomaly representations, a novel distillation network is proposed for unsupervised anomaly detection, consisting of a complete teacher network and a set of non-directly-coupled student blocks. Instead of taking a complete network as a student which sequentially inherits the distilled knowledge from the previous layer, the student blocks are specifically designed, which independently take features of each layer of teacher network as their input and target to recover the multi-scale representation of the teacher. For each block, an adaptive weighted multi-branch feature extraction strategy is presented to enable the blocks to better focus on key messages from the teacher model. In addition, a feature reunion technique is given during distillation to make multi-scale features more robust to noisy input. The experimental results indicate that the proposed method achieves an outstanding performance on MVTec AD dataset. Compared with the baseline method, the proposed method improves by 2.21% at ROC-AUC of image level and improves by 1.00 and 2.22% for both ROC-AUC and PRO-AUC at pixel level.},
  archive      = {J_MVA},
  author       = {Feng, Zhiyuan and Chen, Ying and Xie, Linbo},
  doi          = {10.1007/s00138-023-01454-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Unsupervised anomaly detection via knowledge distillation with non-directly-coupled student block fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A late fusion scheme for multi-graph regularized NMF.
<em>MVA</em>, <em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01455-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph regularized non-negative matrix factorization (GNMF), which is an important extension of NMF, shows good clustering performance on some datasets. However, GNMF only uses one graph to simulate the manifold structure of the data, which may be not accurate enough. Then, some scholars proposed multi-graph regularized NMF(MGNMF). MGNMF first combines multiple graphs into one graph through a linear combination method, and then obtains a low-dimensional representation of the data. However, the representation ability of MGNMF is limited and cannot fully make use of the multi-graph information since MGNMF first combines multiple graphs into one graph and then obtains only one low-dimensional representation of the data, which makes clustering performance unsatisfactory enough. Therefore, we propose an innovative method, i.e., a late fusion scheme for multi-graph regularized NMF(LFS/MGNMF). Different from the existing algorithms, LFS/MGNMF does not directly combine multiple graphs into a graph, but first obtains their own low-dimensional representation matrices, and then uses self-expressiveness property of data to obtain the self-representation matrices of all the low-dimensional representations and removes noise simultaneously. In addition, by using the tensor low-rank constraint, i.e., tensor nuclear norm constraint, LFS/MGNMF can explore higher-order information among different self-representation matrices. Finally, LFS/MGNMF fuses the self-representation matrices for clustering. Therefore, we believe the proposed method is a late fusion scheme and can make full use of the multi-graph information. As far as we know, LFS/MGNMF is the first late fusion method for multi-graph regularized methods. The Augmented Lagrange Multiplier method is exploited to solve LFS/MGNMF, and the experimental results on four datasets are very promising and fully demonstrate the superiority of LFS/MGNMF.},
  archive      = {J_MVA},
  author       = {Ji, Guangyan and Lu, Gui-Fu},
  doi          = {10.1007/s00138-023-01455-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A late fusion scheme for multi-graph regularized NMF},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). S-pad: Self-learning padding mechanism. <em>MVA</em>,
<em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01456-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Padding is used to maintain the size of the feature map and reduce information bias against boundaries. The extra information added by these schemes at the boundary implicitly affects the feature extraction. Zero-padding introduces weakly correlated information resulting in weak boundary information, but provides location encoded information. Various padding produces weight asymmetries due to the uneven application in subsampling. With the lack of a universally superior method, it is necessary to manually determine whether the padding method is suitable for a given task. In this paper, we propose a self-learning padding mechanism, S-Pad, which extends the value by learning the boundary information of the image and provides the network with a set of $$1\times 1$$ filters. Following the training process, tailored boundary rules adapted to the specific task can be obtained. S-Pad, in turn, effectively mitigates the weakening of boundary information as well as weight asymmetries. Our study is dedicated to probing the effectiveness of S-Pad in the domains of object detection and classification. Through our validation process, we establish the enhanced efficacy of S-Pad, resulting in an overall performance improvement for both tasks.},
  archive      = {J_MVA},
  author       = {Ke, Xiao and Lin, Xinru and Guo, Wenzhong},
  doi          = {10.1007/s00138-023-01456-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {S-pad: Self-learning padding mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascading spatio-temporal attention network for real-time
action detection. <em>MVA</em>, <em>34</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01457-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting human actions in video has many applications, such as video surveillance and somatosensory games. In this paper, we propose a spatial-aware attention module (SAM) and a temporal-aware attention module (TAM) for spatio-temporal action detection in videos. SAM first concatenates the feature maps of consecutive frames on the channel and then uses dilated convolutional layer followed by a sigmoid function to generate a spatial attention map. The resulting attention map contains spatial information from consecutive frames, so it helps the detector focus on salient spatial features to achieve more accurate localization of action instances in consecutive frames. TAM deploys several fully connected layers to generate a temporal attention map. The temporal attention map focuses on the temporal association of each spatial feature; it can capture the temporal association of action instances, thereby improving the detector to track actions. To evaluate the effectiveness of SAM and TAM, we build an efficient and strong anchor-free action detector, cascading spatio-temporal attention network, equipped with a 2D backbone and SAM and TAM modules. Extensive experiments on two benchmarks, JHMDB and UCF101-24, demonstrate the preferable performance of SAM and TAM.},
  archive      = {J_MVA},
  author       = {Yang, Jianhua and Wang, Ke and Li, Ruifeng and Perner, Petra},
  doi          = {10.1007/s00138-023-01457-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cascading spatio-temporal attention network for real-time action detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond a strong baseline: Cross-modality contrastive
learning for visible-infrared person re-identification. <em>MVA</em>,
<em>34</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01458-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality pedestrian image matching, which entails the matching of visible and infrared images, is a vital area in person re-identification (reID) due to its potential to facilitate person retrieval across a spectrum of lighting conditions. Despite its importance, this task presents considerable challenges stemming from two significant areas: cross-modality discrepancies due to the different imaging principles of spectrum cameras and within-class variations caused by the diverse viewpoints of large-scale distributed surveillance cameras. Unfortunately, the existing literature provides limited insights into effectively mitigating these issues, signifying a crucial research gap. In response to this, the present paper makes two primary contributions. First, we conduct a comprehensive study of training methodologies and subsequently present a strong baseline network designed specifically to address the complexities of the visible-infrared person reID task. This strong baseline network is paramount to the advancement of the field and to ensure the fair evaluation of algorithmic effectiveness. Second, we propose the Cross-Modality Contrastive Learning (CMCL) scheme, a novel approach to address the cross-modality discrepancies and enhance the quality of image embeddings across both modalities. CMCL incorporates intra-modality and inter-modality contrastive loss components, designed to improve the matching quality across the modalities. Thorough experiments show the superior performance of the baseline network, and the proposed CMCL can further bring performance over the baselines, outperforming the state-of-the-art methods considerably.},
  archive      = {J_MVA},
  author       = {Fang, Pengfei and Zhang, Yukang and Lan, Zhenzhong},
  doi          = {10.1007/s00138-023-01458-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Beyond a strong baseline: Cross-modality contrastive learning for visible-infrared person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of a robust cascaded architecture for
intelligent robot grasping using limited labelled data. <em>MVA</em>,
<em>34</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01459-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grasping objects intelligently is a challenging task even for humans, and we spend a considerable amount of time during our childhood to learn how to grasp objects correctly. In the case of robots, we cannot afford to spend that much time on making it to learn how to grasp objects effectively. Therefore, in the present research we propose an efficient learning architecture based on VQVAE so that robots can be taught with sufficient data corresponding to correct grasping. However, getting sufficient labelled data is extremely difficult in the robot grasping domain. To help solve this problem, a semi-supervised learning-based model, which has much more generalization capability even with limited labelled data set, has been investigated. Its performance shows 6% improvement when compared with existing state-of-the-art models including our earlier model. During experimentation, it has been observed that our proposed model, RGGCNN2, performs significantly better, both in grasping isolated objects as well as objects in a cluttered environment, compared to the existing approaches which do not use unlabelled data for generating grasping rectangles. To the best of our knowledge, developing an intelligent robot grasping model (based on semi-supervised learning) trained through representation learning and exploiting the high-quality learning ability of GGCNN2 architecture with the limited number of labelled dataset together with the learned latent embeddings, can be used as a de-facto training method which has been established and also validated in this paper through rigorous hardware experimentations using Baxter (Anukul) research robot ( Video demonstration ).},
  archive      = {J_MVA},
  author       = {Shukla, Priya and Kushwaha, Vandana and Nandi, G. C.},
  doi          = {10.1007/s00138-023-01459-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Development of a robust cascaded architecture for intelligent robot grasping using limited labelled data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global attention guided multi-scale network for face image
super-resolution. <em>MVA</em>, <em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01460-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image super-resolution (FSR) is a subtask of image super-resolution that aims to enhance the resolution of facial images. Previous FSR methods have leveraged facial prior information, such as parsing maps and landmarks, to improve their performance. However, these methods have not fully utilized the potential of this prior information, as they typically use the same network structure for different types of facial information. To address this limitation, we propose a new network structure called GAMFSR, which incorporates parsing map prior information and includes a global attention module (GAM) to improve the utilization of the parsing map. Additionally, we developed a multistage super-resolution network for preprocessing, which further improves the prediction accuracy. We conducted ablation studies to investigate the effectiveness of GAM and the impact of different prior information. Our experimental results demonstrate that our approach significantly enhances the guidance of parsing map features and achieves better performance with less prior information.},
  archive      = {J_MVA},
  author       = {Zhang, Jinlu and Liu, Mingliang and Wang, Xiaohang},
  doi          = {10.1007/s00138-023-01460-9},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Global attention guided multi-scale network for face image super-resolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting facial recognition capability for faces wearing
masks using attention augmented residual model with quadruplet loss.
<em>MVA</em>, <em>34</em>(6), 1–45. (<a
href="https://doi.org/10.1007/s00138-023-01461-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked face recognition gained importance after the outbreak of COVID-19 when people started wearing facial masks to protect themselves against disease. The performance of existing face recognition systems is declined on the images of persons wearing face masks because the discriminative facial features are hidden under the mask. In addition, face recognition algorithms proposed for handling masked face recognition scenarios exhibit degraded performance in unmasked face recognition settings. Therefore, there is a need for a solution that can perform well in masked-face scenarios and maintains the same performance while recognizing the unmasked faces. In this paper, we propose a Masked Face Unveiling Model (MFUM), which can be added to the backbone of existing facial recognition systems and improves the performance of existing approaches in masked face settings without the need to retrain the current models. The MFUM utilizes the masked face embedding produced by the existing face recognition model backbone and uses attention augmented residual model for recognition. It enhances the similarity of masked face embedding with the unmasked face embedding of the same identity and diminishes the similarity with the unmasked facial embedding of the different identities. We experimented with different face recognition models’ backbone, MFUM alternative architectures, and attention mechanisms on the MFR2 dataset with real masked faces and the LFW dataset with synthetic masks. The results show that the proposed Masked Face Unveiling-Attention Augmented Dense Residual Unit trained with quadruplet loss outperformed not only other MFUM architectures and losses but also exhibited superior performance as compared to other state-of-the-art algorithms.},
  archive      = {J_MVA},
  author       = {Nawshad, Muhammad Aasharib and Saadat, Ahsan and Fraz, Muhammad Moazam},
  doi          = {10.1007/s00138-023-01461-8},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-45},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting facial recognition capability for faces wearing masks using attention augmented residual model with quadruplet loss},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group attention retention network for co-salient object
detection. <em>MVA</em>, <em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01462-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The co-salient object detection (Co-SOD) aims to discover common, salient objects from a group of images. With the development of convolutional neural networks, the performance of Co-SOD methods has been significantly improved. However, some models cannot construct collaborative relationships across images optimally and lack effective retention of collaborative features in the top-down decoding process. In this paper, we propose a novel group attention retention network (GARNet), which captures excellent collaborative features and retains them. First, a group attention module is designed to construct the inter-image relationships. Second, an attention retention module and a spatial attention module are designed to retain inter-image relationships for protecting them from being diluted and filter out the cluttered context during feature fusion, respectively. Finally, considering the intra-group consistency and inter-group separability of images, an embedding loss is additionally designed to discriminate between real collaborative objects and distracting objects. The experiments on four datasets (iCoSeg, CoSal2015, CoSoD3k, and CoCA) show that our GARNet outperforms previous state-of-the-art methods. The source code is available at https://github.com/TJUMMG/GARNet .},
  archive      = {J_MVA},
  author       = {Liu, Jing and Wang, Jiaxiang and Fan, Zhiwei and Yuan, Min and Wang, Weikang and Yu, Jiexiao},
  doi          = {10.1007/s00138-023-01462-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Group attention retention network for co-salient object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-aware cross-domain target detection based on cityscape
in fog. <em>MVA</em>, <em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01463-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of unsupervised simulation to real-world adjustment (USRA) is designed to improve the training of simulation data in a real-world environment. In practical applications, such as robotic vision and autonomous driving, this could save the cost of manually annotating data. Regular USRA&#39;s are often assumed to include large samples of unla-Beled&#39;s real-world data for training purposes. However, this assumption is incorrect because of the difficulties of collection and, in practice, data on some practices is still lacking. Therefore, our aim is to reduce the need for large amounts of real data, in the case of unsupervised simulation-real-world domain adaptability (USDA) and generalization (USDG) issues, which only exist in the real world. In order to make up for the limited actual data, this paper first constructs a pseudo-target domain, using a real data to achieve the simulation data style. Based on this method, this paper proposes a cross-domain interdomain randomization method based on class perception to extract domain invariant knowledge from simulated objects and virtual objects. We will demonstrate the effectiveness of our approach in USDA and USDG, such as Cityscapes and Foggy Cityscapes, which are far superior to existing technological means.},
  archive      = {J_MVA},
  author       = {Gan, Linfeng and Liu, Hu and Chen, Aoran and Xu, Xibin and Zhang, Xuebiao},
  doi          = {10.1007/s00138-023-01463-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Class-aware cross-domain target detection based on cityscape in fog},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real estate pricing prediction via textual and visual
features. <em>MVA</em>, <em>34</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01464-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real estate industry relies heavily on accurately predicting the price of a house based on numerous factors such as size, location, amenities, and season. In this study, we explore the use of machine learning techniques for predicting house prices by considering both visual cues and estate attributes. We collected a dataset (REPD-3000) of 3000 houses across 74 cities in the USA and annotated 14 estate attributes and five visual images for each house&#39;s exterior, interior-living room, kitchen, bedroom, and bathroom. We extracted features from the input images using convolutional neural network (CNN) and fed them along with the estate attributes into a multi-kernel deep learning regression model to predict the house price. Our model outperformed baseline models in extensive experiments, achieving the best result with a mean absolute error (MAE) of 16.60. We compared our model with a multi-kernel support vector regression and analyzed the impact of incorporating individual feature sets. In future, we plan to address class imbalance by having the same number of houses in each class and explore feature engineering for improving the model&#39;s performance.},
  archive      = {J_MVA},
  author       = {Yousif, Amira and Baraheem, Samah and Vaddi, Sai Surya and Patel, Vatsa S. and Shen, Ju and Nguyen, Tam V.},
  doi          = {10.1007/s00138-023-01464-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real estate pricing prediction via textual and visual features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A transformer-based neural ODE for dense prediction.
<em>MVA</em>, <em>34</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01465-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural ordinary differential equations (ODEs) represent an emergent class of deep learning models exhibiting continuous depth. While they have shown promising results across various machine learning tasks, existing methods for dense prediction tasks have not fully harnessed their potential, often due to employing sub-optimal architectural components and limited dataset studies. To address this, our paper introduces a robust neural ODE architecture specifically tailored for dense prediction tasks and performs an extensive evaluation across a broad range of datasets. Our approach draws upon proven design elements from top-performing networks, integrating transformer blocks as core building blocks. Unique to our design is the retention of multiple concurrent representations at varying resolutions throughout the network. These representations continually exchange information, ensuring they remain updated. Our network achieves unrivaled performance in tasks such as image classification, semantic segmentation, and answer grounding. We conduct several ablation studies to shed light on the impacts of various design parameters. Our results affirm the effectiveness of our approach and its potential for further advancements in dense prediction tasks.},
  archive      = {J_MVA},
  author       = {Khoshsirat, Seyedalireza and Kambhamettu, Chandra},
  doi          = {10.1007/s00138-023-01465-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A transformer-based neural ODE for dense prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction: Unsupervised single-shot depth estimation using
perceptual reconstruction. <em>MVA</em>, <em>34</em>(6), 1. (<a
href="https://doi.org/10.1007/s00138-023-01466-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Angermann, Christoph and Schwab, Matthias and Haltmeier, Markus and Laubichler, Christian and Jónsson, Steinbjörn},
  doi          = {10.1007/s00138-023-01466-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction: Unsupervised single-shot depth estimation using perceptual reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gait recognition using free-area transformer networks.
<em>MVA</em>, <em>34</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01467-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A gait is a unique biological feature that can be identified in long distances. CNN-based gait recognition methods have achieved good recognition results. However, they perform poorly in occlusion situations. We established that different regions of the image have different numbers of features, and the higher the extraction efficiency, the sparser the features. Changing the area of different regions of the image affects the feature extraction efficiency of the corresponding regions and improves the overall recognition accuracy. In this study, we propose a free-area transformer network and introduce free-form deformation to adaptively adjust the area of different regions of the gait image. We used GaitSet and GaitPart as our recognition network to achieve state-of-the-art accuracies in the experiment. The experimental results on the CASIA-B and OU-MVLP datasets proved that our method can be effectively generalized, as compared to other silhouette-based methods.},
  archive      = {J_MVA},
  author       = {Chen, Guannan and Wei, Shimin},
  doi          = {10.1007/s00138-023-01467-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Gait recognition using free-area transformer networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distortion diminishing with vulnerability filters pruning.
<em>MVA</em>, <em>34</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s00138-023-01468-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overparameterization of convolutional neural networks allows model compression, and model pruning algorithms have attracted much interest due to their practical acceleration effects. The pruning algorithm should remove as many redundant structures as possible while maintaining the original accuracy to maximize the acceleration effect. However, prior works utilize benign samples to evaluate the loss of accuracy but ignore the vulnerability to malicious adversarial samples, which brings potential security risks. To tackle this limitation, we propose the distortion diminishing with vulnerability filters pruning (DD-VFP) method that simultaneously improves the compressed model’s classification accuracy and adversarial defense capabilities. Specifically, we define adversarial vulnerability for filters by computing the distortion of their latent features. We then propose a new pruning framework, vulnerability filter pruning (VFP), to remove filters with higher adversarial vulnerability during training. We propose a new loss for adversarial training, distortion diminishing (DD) loss, which directly suppresses the model’s dependence on non-robust features by reducing the latent feature distortion under adversarial perturbation. Experiments on multiple benchmark datasets prove the effectiveness of our method, which not only achieves state-of-the-art adversarial defense capabilities but also removes more model parameters. An interesting phenomenon is that although the DD-VFP method slightly loses accuracy after pruning, the trade-off between accuracy and adversarial defense capabilities is significantly reduced, which proves that our method successfully removes non-robust features.},
  archive      = {J_MVA},
  author       = {Huang, Hengyi and Wu, Pingfan and Xia, Shifeng and Liu, Ningzhong},
  doi          = {10.1007/s00138-023-01468-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Distortion diminishing with vulnerability filters pruning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple and effective complementary label learning based on
mean square error loss. <em>MVA</em>, <em>34</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s00138-023-01469-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complementary label specifies one of the classes that an instance does not belong to. Complementary label learning only uses training instances each assigned a complementary label to train a classifier that can predict a ground-truth label for each testing instance. Though many surrogate loss functions have been proposed for complementary label learning, the mean square error (MSE) surrogate loss function, widely used in the standard classification paradigm, cannot provide classifier consistency in complementary label learning. However, classifier consistency not only guarantees the converged model is the optimal classifier that can be found in the searching space but also indicates that standard backpropagation is enough to search for the optimal classifier without needing model selection. This paper designs an effective square loss for complementary label learning under unbiased and biased assumptions. We also theoretically demonstrate that our method assurances that the optimal classifier under complementary labels is also the optimal classifier under ordinary labels. Finally, we test our method on different benchmark datasets with biased and unbiased assumptions to verify the effectiveness of our method.},
  archive      = {J_MVA},
  author       = {Wang, Chenggang and Xu, Xiong and Liu, Defu and Niu, Xinyu and Han, Shijiao},
  doi          = {10.1007/s00138-023-01469-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Simple and effective complementary label learning based on mean square error loss},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot action recognition by clustered representation
with redundancy-free features. <em>MVA</em>, <em>34</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-023-01470-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot action recognition (ZSAR) is a practical and challenging issue, which compensates for the shortcomings of existing action recognition by being able to recognize those action classes that don’t have visual representation during training. However, existing zero-shot action recognition doesn’t focus on the fact that the generated features have many outliers, which harms the recognition. A new method for zero-shot action recognition is proposed, which suppresses this defect by clustered representation with redundancy-free features. In addition, a generative adversarial network (GAN) with gradient penalty is trained to synthesize stable features, solving the problem of data imbalance and alleviating the bottleneck of unstable features generated in existing methods. To reduce the dimension and the subsequent computation, a redundancy-free feature is introduced into the ZSAR. Experiments performed on Olympic Sports, HMDB51, and UCF101 public datasets prove that our method outperforms the state-of-the-art approaches with absolute gains of 1.8%, 0.3%, and 1.7%, respectively, in zero-shot action recognition.},
  archive      = {J_MVA},
  author       = {Xia, Limin and Wen, Xin},
  doi          = {10.1007/s00138-023-01470-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Zero-shot action recognition by clustered representation with redundancy-free features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Position puzzle network and augmentation: Localizing human
keypoints beyond the bounding box. <em>MVA</em>, <em>34</em>(6), 1–14.
(<a href="https://doi.org/10.1007/s00138-023-01471-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating human pose with a partial image of a person, we, humans, do not confine the spatial range of our estimation to the given image and can readily localize keypoints outside of the image by referring to visual clues such as the body size. However, computational methods for human pose estimation do not consider those keypoints outside and focus only on the bounded area of a given image. In this paper, we propose a neural network and a data augmentation method to extend the range of human pose estimation beyond the bounding box. While our Position Puzzle Network expands the spatial range of keypoint localization by refining the position and the size of the target’s bounding box, Position Puzzle Augmentation enables the keypoint detector to estimate keypoints not only within, but also beyond the input image. We show that the proposed method enhances the baseline keypoint detectors by 39.5% and 30.5% on average in mAP and mAR, respectively, by enabling the localization of keypoints out of the bounding box using a cropped image dataset prepared for proper evaluation. Additionally, we verify that the proposed method does not degrade the performance under the original benchmarks and instead, improves the performance by alleviating false-positive errors.},
  archive      = {J_MVA},
  author       = {Park, Soonchan and Park, Jinah},
  doi          = {10.1007/s00138-023-01471-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Position puzzle network and augmentation: Localizing human keypoints beyond the bounding box},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage structural information enhancement for source-free
domain adaptation. <em>MVA</em>, <em>34</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01472-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) uses models trained from source domains to solve similar tasks in unlabeled domains, without accessing source domain data. Existing SFDA methods have not been able to learn spatial and semantic structural information of target domains simultaneously, making them insufficient and inefficient for target domain exploration. To fully explore the target domain structural information, we propose a novel representation learning framework, called structural information enhancement (SIE). SIE has a two-stage approach that, in the first stage, clusters local neighbors and pushes away global non-neighbors in the feature space to obtain spatial structural information. In the second stage, SIE fine-tunes the clustered model using a semantic structure consistency strategy that exploits semantic structural information by mutual learning interpolated sample pairs. Our extensive experiments demonstrate the superiority of our method, and our method can serve as a strong baseline for future SFDA research.},
  archive      = {J_MVA},
  author       = {Chen, Sijie and Shao, Mingwen and Zhang, Lixu and Bao, Zhiyuan},
  doi          = {10.1007/s00138-023-01472-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Two-stage structural information enhancement for source-free domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A camera style-invariant learning and channel interaction
enhancement fusion network for visible-infrared person
re-identification. <em>MVA</em>, <em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01473-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality visible-infrared person re-identification (VI-ReID) aims to match visible and infrared pedestrian images from different cameras in various scenarios. However, most existing VI-ReID methods only focus on eliminating the modality discrepancy while ignoring the intra-class discrepancy caused by different camera styles. In addition, some feature fusion-based VI-ReID methods try to improve the discriminative capability of pedestrian representations by fusing pedestrian features from different convolutional layers or branches. However, most of them only implement feature fusion by simple operations, such as summation or concatenation, and ignore the interaction between different feature maps. To this end, we propose a camera style-invariant learning and channel interaction enhancement fusion network for VI-ReID. In particular, we design a channel interaction enhancement fusion module. It first computes and utilizes the channel-level similarity matrix of two feature maps to obtain two corresponding weighted feature maps that enhance the common concern information of the original two feature maps. Then, it obtains more discriminative pedestrian features by fusing the two weighted feature maps and mining their complementary information. Furthermore, in order to weaken the impact of camera style discrepancy of pedestrian images, we design a camera style-invariant feature-level adversarial learning strategy to ensure that the feature extraction network can extract camera style-invariant pedestrian features by the adversarial learning between the feature extraction network and the camera style classifier. Extensive experimental results on the two benchmark datasets, SYSU-MM01 and RegDB, demonstrate that the performance of CC-Net achieves the recent advanced level.},
  archive      = {J_MVA},
  author       = {Du, Haishun and Hao, Xinxin and Ye, Yanfang and He, Linbing and Guo, Jiangtao},
  doi          = {10.1007/s00138-023-01473-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A camera style-invariant learning and channel interaction enhancement fusion network for visible-infrared person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on human pose estimation and
its applications. <em>MVA</em>, <em>34</em>(6), 1–4. (<a
href="https://doi.org/10.1007/s00138-023-01474-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Tang, Wei and Ren, Zhou and Wang, Jingdong},
  doi          = {10.1007/s00138-023-01474-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-4},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Guest editorial: Special issue on human pose estimation and its applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision-based approach to assess performance levels while
eating. <em>MVA</em>, <em>34</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01475-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The elderly population is increasing at a rapid rate, and the need for effectively supporting independent living has become crucial. Wearable sensors can be helpful, but these are intrusive as they require adherence by the elderly. Thus, a semi-anonymous (no image records) vision-based non-intrusive monitoring system might potentially be the answer. As everyone has to eat, we introduce a first investigation into how eating behavior might be used as an indicator of performance changes. This study aims to provide a comprehensive model of the eating behavior of individuals. This includes creating a visual representation of the different actions involved in the eating process, in the form of a state diagram, as well as measuring the level of performance or decay over time during eating. Also, in studies that involve humans, getting a generalized model across numerous human subjects is challenging, as indicative features that parametrize decay/performance changes vary significantly from person to person. We present a two-step approach to get a generalized model using distinctive micro-movements, i.e., (1) get the best features across all subjects (all features are extracted from 3D poses of subjects) and (2) use an uncertainty-aware regression model to tackle the problem. Moreover, we also present an extended version of EatSense, a dataset that explores eating behavior and quality of motion assessment while eating.},
  archive      = {J_MVA},
  author       = {Raza, Muhammad Ahmed and Fisher, Robert B.},
  doi          = {10.1007/s00138-023-01475-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based approach to assess performance levels while eating},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CCTV-calib: A toolbox to calibrate surveillance cameras
around the globe. <em>MVA</em>, <em>34</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s00138-023-01476-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CCTV camera calibration and geolocalization are essential for the deployment of smart-city applications. Despite the pressing need for practical and accurate surveillance camera calibration, most existing techniques are cumbersome and rely on restrictive assumptions on the camera’s or the scene’s geometry. In this context, we propose CCTV-Calib, a light and user-friendly toolbox to calibrate traffic cameras using satellite views. Unlike other surveillance camera calibration techniques, CCTV-Calib can estimate the intrinsic and extrinsic parameters as well as the GPS location of one or multiple CCTV cameras in a few clicks. For this purpose, we propose a simple yet effective two-stage pipeline to ensure a robust, repeatable, and easy calibration process. In the first stage, we perform automated keypoint matching between the CCTV image and the satellite views to compute an initial estimation of the camera’s parameters. In the second stage, this rough initialization guides a fine matching strategy, further improving calibration accuracy. This novel calibration pipeline is integrated into an easy-to-use GUI, making traffic camera calibration accessible to non-computer vision experts. To qualitatively and quantitatively evaluate the accuracy and relevance of our technique, we propose a novel dataset composed of synthetic and real images captured around the globe. Finally, in order to illustrate the pertinence of our calibration strategy, we demonstrate its applicability to 3D vehicle geolocalization. We made this toolbox and datasets freely available: https://github.com/rameau-fr/CCTV-Calib .},
  archive      = {J_MVA},
  author       = {Rameau, Francois and Choe, Jaesung and Pan, Fei and Lee, Seokju and Kweon, In So},
  doi          = {10.1007/s00138-023-01476-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CCTV-calib: A toolbox to calibrate surveillance cameras around the globe},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CGA-net: Channel-wise gated attention network for improved
super-resolution in remote sensing imagery. <em>MVA</em>,
<em>34</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s00138-023-01477-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution (SR) is a powerful technique for enhancing the quality of remote sensing imagery, which in turn can improve the accuracy of various computer vision tasks, such as object detection, classification, and segmentation. Deep convolutional neural networks (CNNs) have demonstrated significant progress in this field, and attention mechanisms are widely adopted in deep CNNs as they allow the models to assign weights to important areas within the feature map. In this paper, we propose the channel-wise gated attention (CGA) module, which integrates attention across the feature map channels and scales the resulting feature map through a gating parameter, leading to performance improvements. Furthermore, we present an SR framework that employs multiple attention blocks, with the CGA module serving as the core of each block, to enhance the spatial resolution of remote sensing imagery. Our proposed network, the channel-wise gated attention Network (CGA-Net), outperforms other attention-based deep SR models for 4 $$\times $$ - and 8 $$\times $$ -upsampling on two remote sensing datasets: Satellite Imagery Multi-Vehicles Dataset (SIMD), consisting of 5000 high-resolution remote sensing images, and DOTA, a large-scale satellite imagery dataset. We conduct several experiments to evaluate the effectiveness of our SR framework for object detection on the SIMD dataset. The code and trained weights for the proposed framework can be found at this link: https://github.com/Vision-At-SEECS/CGA-Net .},
  archive      = {J_MVA},
  author       = {Khan, Bostan and Mumtaz, Adeel and Zafar, Zuhair and Sedkey, Mohamed and Benkhelifa, Elhadj and Fraz, Muhammad Moazam},
  doi          = {10.1007/s00138-023-01477-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CGA-net: Channel-wise gated attention network for improved super-resolution in remote sensing imagery},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep retinex network for underwater low-light image
enhancement. <em>MVA</em>, <em>34</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01478-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images suffer from color cast and low contrast due to the light absorption and scattering. Especially when natural light is not sufficient, large dark areas appear in the captured image, making it impossible to understand the image content. To address this issue, we propose an underwater low-light enhancement method based on Retinex theory. Our model is an end-to-end trainable. The decomposition network decomposes the raw image into reflectance and illumination according to Retinex theory. In the reflectance enhancement network, cross-residual blocks and dense connections can improve the efficiency of feature utilization and the hybrid attention concentrate on the regions of interest in feature maps from different perspectives. The illumination adjustment network utilizes adaptive frequency convolutional blocks to generate additional band information, which reconstructs the more natural illumination. In order to preserve the color consistency of the enhanced image with the reference image, we project the HSV space into the Cartesian coordinate system and use the Euclidean distance as the color cast loss to constrain the enhancement network. Qualitative and quantitative evaluations on different underwater datasets indicate that our method has the excellent performance and can achieve delightful visual enhancements.},
  archive      = {J_MVA},
  author       = {Ji, Kai and Lei, Weimin and Zhang, Wei},
  doi          = {10.1007/s00138-023-01478-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A deep retinex network for underwater low-light image enhancement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recent progress in sign language recognition: A review.
<em>MVA</em>, <em>34</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s00138-023-01479-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language is a predominant form of communication among a large group of society. The nature of sign languages is visual, making them distinct from spoken languages. Unfortunately, very few able people can understand sign language making communication with the hearing-impaired infeasible. Research in the field of sign language recognition (SLR) can help reduce the barrier between deaf and able people. Despite having tremendous advances in SLR, unfortunately, this form of recognition is still at least a decade behind speech recognition. There has been a gradual transition from static to isolated to continuous SLR, but still the research is scattered, limited to very small vocabularies, and only suitable for tailor-made conditions. This paper aims to compile recent progress in SLR and presents a comprehensive review of the emerging SLR frameworks and algorithms. We have categorized SLR based on the unit of written text, i.e., letters or alphabets, words and sentences. This review also includes a study-wise summary of the datasets used in different research conducted during the last few years. We identify state-of-the-art techniques for each category. We also suggest novel research directions for future work, and highlight several primary factors contributing to SLR’s inability to achieve improved practical outcomes.},
  archive      = {J_MVA},
  author       = {Wali, Aamir and Shariq, Roha and Shoaib, Sajdah and Amir, Sukhan and Farhan, Asma Ahmad},
  doi          = {10.1007/s00138-023-01479-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Recent progress in sign language recognition: A review},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LTM: Efficient learning with triangular topology constraint
for feature matching with heavy outliers. <em>MVA</em>, <em>34</em>(6),
1–16. (<a href="https://doi.org/10.1007/s00138-023-01482-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image feature matching, which aims to establish correspondence between two images, is an important task in computer vision. Among image feature matching, the removal of mismatches is crucial to ensure the correctness of the matches. In recent years, machine learning has become a new perspective for mismatch removal. However, existing learning-based methods require a large amount of image data for training, which shows a lack of generalizability and is hard to deal with cases with high mismatch ratio. In this paper, we induce the triangular topology constraint into machine learning, where topology constraints around the matching points are summarized; combining with the idea of sampling, we achieve the task of removing mismatches. Topology constraints are studied in spite of the image input; our LTM (learning topology for matching) just needs fewer than 20 parameters as input, so that only ten training image pairs from four image sets involving about 3,000 matches are employed to train; it still achieves promising results on various datasets with different machine learning approaches. The experimental results of this study also demonstrate the superior performance of our LTM over existing methods.},
  archive      = {J_MVA},
  author       = {Shen, Chentao and He, Zaixing and Zhao, Xinyue and Cui, Wenfeng and Shen, Huarong},
  doi          = {10.1007/s00138-023-01482-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {LTM: Efficient learning with triangular topology constraint for feature matching with heavy outliers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Unsupervised single-shot depth estimation using perceptual
reconstruction. <em>MVA</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01410-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time estimation of actual object depth is an essential module for various autonomous system tasks such as 3D reconstruction, scene understanding and condition assessment. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks has yielded approaches that succeed in achieving realistic depth synthesis out of a simple RGB modality. Most of these models are based on paired RGB-depth data and/or the availability of video sequences and stereo images. However, the lack of RGB-depth pairs, video sequences, or stereo images makes depth estimation a challenging task that needs to be explored in more detail. This study builds on recent advances in the field of generative neural networks in order to establish fully unsupervised single-shot depth estimation. Two generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance, a novel perceptual reconstruction term, and hand-crafted image filters. We comprehensively evaluate the models using a custom-generated industrial surface depth data set as well as the Texas 3D Face Recognition Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset that records body depth. For each evaluation dataset, the proposed method shows a significant increase in depth accuracy compared to state-of-the-art single-image transfer methods.},
  archive      = {J_MVA},
  author       = {Angermann, Christoph and Schwab, Matthias and Haltmeier, Markus and Laubichler, Christian and Jónsson, Steinbjörn},
  doi          = {10.1007/s00138-023-01410-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Unsupervised single-shot depth estimation using perceptual reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic sign detection and recognition under low
illumination. <em>MVA</em>, <em>34</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s00138-023-01417-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problems such as the difficulty of traffic sign detection and recognition under low illumination, a new low illumination traffic sign detection and recognition algorithm is proposed. The algorithm firstly uses an illumination judgement algorithm to filter out low-illumination images, then uses a New Illumination Enhancement algorithm to adjust the brightness and contrast of the low-illumination images, and finally uses mask RCNN (mask region-based convolutional neural network, mask RCNN) to detect and recognize traffic signs. The New Illumination Enhancement Algorithm is based on Illumination-Reflection model, firstly converting the image RGB space into HSV space, applying guided filtering to the V channel to obtain the illumination component, using the illumination component to extract the reflection component, and adjusting the reflection component by linear pull-up. Next, the distribution characteristics of the illumination component are used to adjust the 2D gamma function and obtain the optimized illumination component. Subsequently, the illumination component is used to obtain the detail component. Finally, a hybrid spatial enhancement method is used to obtain the enhanced V-channel and reconstruct the image. The experimental results show that the New Illumination Enhancement algorithm can effectively improve image brightness and sharpness in both low illumination traffic scenes, ensure that the image is not distorted, retain image information and enhance the prominence of traffic signs in traffic scenes. In the ZCTSDB-lightness test set, the combined algorithm of new light image enhancement and Mask RCNN improved object detection $${\varvec{mAP}}^{{{\varvec{bb}}}}$$ and instance segmentation $${\varvec{mAP}}^{{{\varvec{seg}}}}$$ by 2.810% and 1.176%, respectively, compared to Mask RCNN. In the ZCTSDB test set, the performance of the new low illumination traffic sign detection and recognition algorithm outperformed all other algorithms.},
  archive      = {J_MVA},
  author       = {Yao, Jiana and Huang, Bingqiang and Yang, Song and Xiang, Xinjian and Lu, Zhigang},
  doi          = {10.1007/s00138-023-01417-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traffic sign detection and recognition under low illumination},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-level feature channel attention UNet network for
cholangiocarcinoma segmentation from microscopy hyperspectral images.
<em>MVA</em>, <em>34</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01418-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathological diagnosis is the gold standard for the diagnosis of cholangiocarcinoma. The manual segmentation of pathology sections is time-consuming. Automatic segmentation has become a clinical requirement. Recently, the UNet network has been widely used in automatic segmentation; however, due to the complex structure and diverse shapes of pathological slices of cholangiocarcinoma, the segmentation ability of UNet is insufficient. In addition, traditional RGB images cannot reflect the spectral characteristics of cancerous tissue. Therefore, in this paper, a high-level feature channel attention UNet (HLCA-UNet) network is proposed to segment cholangiocarcinoma using microscopy hyperspectral images. Compared with the original UNet, HLCA-UNet has the following improvements: (1) a new path consisting of hierarchical feature extraction and channel attention mechanism designed to deliver encoder features to decoder, in which the hierarchical feature extraction module is able to extract high-resolution high-level features and reduce semantic gaps while channel attention mechanism establishes a deep correlation between the low-level and high-level features, (2) we introduce bilinear interpolation technology to replace the transpose convolution so as to achieve a smooth segmentation boundary. We have evaluated the performance of the proposed HLCA-UNet model and compared it with other excellent models using cholangiocarcinoma microscopy hyperspectral images. Experiment results show that HLCA-UNet gains a superior performance, with accuracy, precision, and recall of 82.84%, 69.60%, and 77.99%, respectively. In general, our study provides new ideas for future pathological diagnosis of cholangiocarcinoma and makes recommendations for future researchers.},
  archive      = {J_MVA},
  author       = {Gao, Hongmin and Yang, Mengran and Cao, Xueying and Liu, Qin and Xu, Peipei},
  doi          = {10.1007/s00138-023-01418-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A high-level feature channel attention UNet network for cholangiocarcinoma segmentation from microscopy hyperspectral images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PerSnake: A real-time pedestrian instance segmentation
network using contour representation. <em>MVA</em>, <em>34</em>(5),
1–10. (<a href="https://doi.org/10.1007/s00138-023-01419-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an intelligent transportation system, pedestrian identification is an indispensable security link. This paper aims to explore a high precision and real-time pedestrian recognition method. Most of the existing pedestrian recognition methods are usually pixel-level and framed. However, these methods cannot be able to make an accurate or real-time judgment of the pedestrian position. To this end, this paper introduces a novel contour-based segmentation network (PerSnake) for real-time pedestrian detection in autonomous driving. We design an octagon contour specifically for pedestrians by using a YOLO-V4 detector as the initial pedestrian contour, and a contour feature aggregation module is proposed to aggregate the multi-level pedestrian contour features. To construct pedestrian contour labels, we annotate the ground truth of the pedestrian contour on Penn-Fudan and Citypersons datasets by using edge detection. Substantial experiments are conducted on the CityPersons, and the Penn-Fudan database. The results demonstrate that our PerSnake is capable of achieving real-time pedestrian identification with a speed of 37.8 frame per second and the average precision reaches 36.4%. Compared with the existing methods, our PerSnake exhibits competitive advantages in terms of segmentation speed and precision.},
  archive      = {J_MVA},
  author       = {Guo, Zhiyang and Wang, Xincan and Zhang, Zhihua and Huang, Yingping and Ma, Xueyin},
  doi          = {10.1007/s00138-023-01419-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {PerSnake: A real-time pedestrian instance segmentation network using contour representation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When dual contrastive learning meets disentangled features
for unpaired image deraining. <em>MVA</em>, <em>34</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01421-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the basis work of image processing, rain removal from a single image has always been an important and challenging problem. Due to the lack of real rain images and corresponding clean images, most rain removal networks are trained by synthetic datasets, which makes the output images unsatisfactory in practical applications. In this work, we propose a new feature decoupling network for unsupervised image rain removal. Its purpose is to decompose the rain image into two distinguishable layers: clean image layer and rain layer. In order to fully decouple the features of different attributes, we use contrastive learning to constrain this process. Specifically, the image patch with similarity is pulled together as a positive sample, while the rain layer patch is pushed away as a negative sample. We not only make use of the inherent self-similarity within the sample, but also make use of the mutual exclusion between the two layers, so as to better distinguish the rain layer from the clean image. We implicitly constrain the embedding of different samples in the depth feature space to better promote rainline removal and image restoration. Our method achieves a PSNR of 25.80 on Test100, surpassing other unsupervised methods.},
  archive      = {J_MVA},
  author       = {Wang, Tianming and Wang, Kaige and Li, Qing},
  doi          = {10.1007/s00138-023-01421-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {When dual contrastive learning meets disentangled features for unpaired image deraining},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occlusion recovery face recognition based on information
reconstruction. <em>MVA</em>, <em>34</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01423-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face data in the wild have a face occlusion, complex background, and inestimable posture, leading to more sample classification errors. To alleviate the problem of face occlusion, we apply facial information reconstruction to occluded face recognition. Our model processes face images with variable poses and reconstructs face information to compensate for information loss due to occlusion. In this context, we propose an unconstrained face recognition method based on information reconstruction and occlusion dictionary learning. One is that 3D facial information reconstruction makes up for the lost part of self-occluded face images. The other is that the Gabor-based occlusion dictionary learning method increases feature diversity and better represents occluded faces. We conduct experiments on the occluded AR dataset to verify the robustness of occluded dictionary learning. Compared with the classical algorithm, the accuracy of the CAS-PEAL dataset improves by 40.3%. The experimental results of the LFW dataset are 5.57–12.21% higher than those of the state-of-the-art algorithm, which indicates that the proposed method realizes effective occlusion face recognition.},
  archive      = {J_MVA},
  author       = {He, Huanjie and Liang, Jiuzhen and Hou, Zhenjie and Liu, Hao and Zhou, Xinwen},
  doi          = {10.1007/s00138-023-01423-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Occlusion recovery face recognition based on information reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic multi-modal depth estimation based on
camera–LiDAR sensor fusion. <em>MVA</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01426-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal depth estimation is one of the key challenges for endowing autonomous machines with robust robotic perception capabilities. There have been outstanding advances in the development of uni-modal depth estimation techniques based on either monocular cameras, because of their rich resolution, or LiDAR sensors, due to the precise geometric data they provide. However, each of these suffers from some inherent drawbacks, such as high sensitivity to changes in illumination conditions in the case of cameras and limited resolution for the LiDARs. Sensor fusion can be used to combine the merits and compensate for the downsides of these two kinds of sensors. Nevertheless, current fusion methods work at a high level. They process the sensor data streams independently and combine the high-level estimates obtained for each sensor. In this paper, we tackle the problem at a low level, fusing the raw sensor streams, thus obtaining depth estimates which are both dense and precise, and can be used as a unified multi-modal data source for higher-level estimation problems. This work proposes a conditional random field model with multiple geometry and appearance potentials. It seamlessly represents the problem of estimating dense depth maps from camera and LiDAR data. The model can be optimized efficiently using the conjugate gradient squared algorithm. The proposed method was evaluated and compared with the state of the art using the commonly used KITTI benchmark dataset.},
  archive      = {J_MVA},
  author       = {Obando-Ceron, Johan S. and Romero-Cano, Victor and Monteiro, Sildomar},
  doi          = {10.1007/s00138-023-01426-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Probabilistic multi-modal depth estimation based on camera–LiDAR sensor fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust vehicle tracking in low-altitude UAV videos.
<em>MVA</em>, <em>34</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s00138-023-01427-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we concentrate on solving variations in scale, aspect ratio, rotation, visual model and target motion problems for vehicle tracking in low-altitude UAV videos. The contributions of this work are threefold: 1. By introducing a particle rescaling mechanism where each particle is resized with different aspect ratios, tracking under scale and aspect ratio variations is improved. 2. By fully integrating a particle filter with a convolutional neural network, a new structure, which acts as an auxiliary particle filter, is developed. This new structure improves the estimation of the states, namely the location and the velocity of the target, and the dimensions of the bounding boxes, thus enables tracking under fast motion. 3. By introducing a unified multi-part vehicle tracking framework, robust tracking is achieved against scale change, aspect ratio, visual model variations and sudden rotations. The processing of multiple parts, independently, improves the tracking under sudden aspect ratio and rotation changes compared to tracking the vehicle as a whole. In this study, without loss of generality, the number of independent parts is taken as two and the proposed method is tested for image sequences from UAV dataset with various visual problems. The comparisons with the state-of-the-art trackers show that the proposed method achieves good precision and success scores, and outperforms most of the state-of-the-art trackers.},
  archive      = {J_MVA},
  author       = {Maraş, Bahri and Arica, Nafiz and Ertüzün, Ayşın},
  doi          = {10.1007/s00138-023-01427-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A robust vehicle tracking in low-altitude UAV videos},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pakistan sign language recognition: Leveraging deep learning
models with limited dataset. <em>MVA</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01429-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language is the predominant form of communication among a large group of society. The nature of sign languages is visual. This makes them very different from spoken languages. Unfortunately, very few able people can understand sign language making communication with the hearing-impaired extremely difficult. Research in the field of sign language recognition can help reduce the barrier between deaf and able people. A lot of work has been done on sign language recognition for numerous languages such as American sign language and Chinese sign language. Unfortunately, very little to no work has been done for Pakistan Sign Language. Any contribution in Pakistan Sign Language recognition is limited to static images instead of gestures. Furthermore, the dataset available for this language is very small in terms of the number of examples per word which makes it very difficult to train deep networks that require a considerable amount of training data. Data Augmentation techniques help the network generalize better by providing more variety in the training data. In this paper, a pipeline for the Pakistan Sign Language recognition system is proposed that incorporates an augmentation unit. To validate the effectiveness of the proposed pipeline, three deep learning models, C3D, I3D, and TSM are used. Results show that translation and rotation are the two best augmentation techniques for the Pakistan Sign Language dataset. The models trained using our data-augment-supported pipeline outperform other methods that only used the original data. The most suitable model is C3D which not only produced an accuracy of 93.33% but also has a low training time as compared to other models.},
  archive      = {J_MVA},
  author       = {Hamza, Hafiz Muhammad and Wali, Aamir},
  doi          = {10.1007/s00138-023-01429-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pakistan sign language recognition: Leveraging deep learning models with limited dataset},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFEMANet: An effective disaster image classification
approach for practical risk assessment. <em>MVA</em>, <em>34</em>(5),
1–23. (<a href="https://doi.org/10.1007/s00138-023-01430-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emergency risk assessment by collecting disaster-affected images via unmanned aerial vehicles is the current norm. Reasonable rescue planning and resource allocation depend on a quick and precise semantic interpretation of natural disaster images. However, the poor image quality produced by various technological and environmental factors and complex scenarios associated with disaster-affected regions makes the classification operation challenging. In order to get in-depth spatial features for decoding the intricate textures associated with catastrophe images, this study proposes an implementation of the CNN-based multibranch feature extraction technique. An advanced mixed-attention mechanism is exploited to extract the highly essential features. This mixed-attention mechanism effectively overcomes the flaws generated by traditional convolution by neglecting the global information and focusing on local key features. An SRGAN-based super-resolution method is utilized to acquire high-resolution images with rich spatial details to enhance the quality of aerial images. Besides, we experiment with several existing image classification algorithms, such as the ensemble model of pre-trained networks, the capsule network model, and the stacked autoencoder. Finally, we perform a comparative analysis between all the deployed models to obtain the best-performing classifier. Our proposed multibranch feature extraction with mixed-attention mechanism-based network performs more superiorly among the four models due to its ability to extract highly relevant features from disaster images. Generated super-resolution images effectively increase the classification performance. Our research findings and approaches accommodate quality resources for disaster image quality enhancement and classification activities.},
  archive      = {J_MVA},
  author       = {Bhadra, Payal and Balabantaray, Avijit and Pasayat, Ajit Kumar},
  doi          = {10.1007/s00138-023-01430-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MFEMANet: An effective disaster image classification approach for practical risk assessment},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automated framework to evaluate soft skills using posture
and disfluency detection. <em>MVA</em>, <em>34</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s00138-023-01431-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft skills are interpersonal skills that define the personality trait of an individual. These skills influence the social interactions of a person and are described using non-verbal and verbal factors like posture, gesture, vocal tone, etc. Throughout the literature, many attempts have been made to evaluate the soft skills of an individual using computer-based algorithms. Most of these attempts focus on only one of the two factors, i.e., either verbal or non-verbal evaluation. The non-verbal evaluation algorithms mainly consist of lateral body posture evaluation or the study of kinematic behavior using specialized hardware, whereas the verbal evaluation typically includes in-person interviews. Hence, a computerized soft skills evaluation system is required that can automatically evaluate the soft skills of an individual. This study proposes a way to evaluate soft skills using automated frontal posture evaluation and vocal assessment of an individual. The proposed methodology estimates the non-verbal and verbal confidence score of an individual. The non-verbal confidence score is estimated using the combination of the MoveNet-thunder skeleton estimation algorithm and the posture angle evaluation system. This system evaluates the frontal posture of an individual and provides a confidence score based on various posture angles like shoulder alignment, neck bent angle, arm abduction, etc. The verbal confidence score is estimated using pause detection, filler word detection, and continuous word repetition estimation models. The confidence scores generated from these two estimation pipelines are combined to form the overall soft skills confidence score of an individual. This score is compared with a threshold value to validate the results. The threshold value depicts the average and natural soft skills confidence score of an individual. This threshold value was estimated using the natural speech part of the standard Librispeech dataset and the bio-mechanic studies for posture estimation. The proposed model only supports single-user visual input, and it can be improved using cloud implementations and real-time data input considerations.},
  archive      = {J_MVA},
  author       = {Gulati, Vaibhav and Dwivedi, Srijan and Kumar, Deepika and Wadhwa, Jatin and Dhingra, Devaansh and Hemanth, Jude D.},
  doi          = {10.1007/s00138-023-01431-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An automated framework to evaluate soft skills using posture and disfluency detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable exponent diffusion for image detexturing.
<em>MVA</em>, <em>34</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01432-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a variational approach to the problem of structure + texture decomposition (also known as cartoon + texture decomposition). As usual for many variational problems in image analysis and processing, the energy we minimize consists of two terms: a data-fitting term and a regularization term. The main feature of our approach consists of choosing parameters in the regularization term adaptively. Namely, the regularization term is given by a weighted $$p(\cdot )$$ -Dirichlet-based energy $$\int \!a({\varvec{x}}){\,\!|\,\!}\nabla u {\,\!|\,\!}^{p({\varvec{x}})}$$ , where the weight and exponent functions are determined from an analysis of the spectral content of the image curvature. Our numerical experiments, both qualitative and quantitative, suggest that the proposed approach delivers better results than state-of-the-art methods for extracting the structure from textured and mosaic images, as well as competitive results on image enhancement problems.},
  archive      = {J_MVA},
  author       = {Fayolle, Pierre-Alain and Belyaev, Alexander G.},
  doi          = {10.1007/s00138-023-01432-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Variable exponent diffusion for image detexturing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLAVR: Flow-free architecture for fast video frame
interpolation. <em>MVA</em>, <em>34</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s00138-023-01433-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern frame interpolation approaches rely on explicit bidirectional optical flows between adjacent frames, thus are sensitive to the accuracy of underlying flow estimation in handling occlusions while additionally introducing computational bottlenecks unsuitable for efficient deployment. In this paper, we propose a flow-free approach that is completely end-to-end trainable for multi-frame video interpolation. Our method, FLAVR, leverages 3D spatio-temporal kernels to directly learn motion properties from unlabeled videos and greatly simplifies the process of training, testing and deploying frame interpolation models. As a result, FLAVR delivers up to $$6\times $$ speed up compared to the current state-of-the-art methods for multi-frame interpolation while consistently demonstrating superior qualitative and quantitative results compared with prior methods on popular benchmarks including Vimeo-90K, SNU-Film, and GoPro. Additionally, we show that frame interpolation is a competitive self-supervised pre-training task for videos by demonstrating various novel applications of FLAVR including action recognition, optical flow estimation, and video object tracking. Code and trained models are available in the project page: https://tarun005.github.io/FLAVR/ . We have provided additional qualitative results for frame interpolation as well as downstream applications in our video available using this web link https://paperid1300.s3.us-west-2.amazonaws.com/FLAVRVideo.mp4 (link).},
  archive      = {J_MVA},
  author       = {Kalluri, Tarun and Pathak, Deepak and Chandraker, Manmohan and Tran, Du},
  doi          = {10.1007/s00138-023-01433-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FLAVR: Flow-free architecture for fast video frame interpolation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FFA-net: Fast feature aggregation network for 3D point cloud
segmentation. <em>MVA</em>, <em>34</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01434-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications like autonomous driving and robot navigation, the large-scale 3D point cloud segmentation method is required to be both fast and efficient. In this paper, a fast and efficient large-scale point cloud semantic segmentation network, namely FFA-Net, is proposed. We used U-Net architecture as the backbone architecture, and used the random sampling algorithm to down sample the 3D points faster. To maintain the speed advantage of random sampling and protect integral information of 3D point cloud, we designed a lightweight operator, called fast feature aggregation (FFA) operator, to learn local features of 3D point cloud efficiently, and we equipped this FFA operator in a dilated residual block to aggregate local features within a larger receptive field hierarchically. This operator contains only minimal of learnable parameters, which makes the segmentation network not only improved in segmentation performance, but also in computation speed. Extensive experimental results on three large-scale 3D point cloud benchmarks have verified the effectiveness of our method in both segmentation performances and speed.},
  archive      = {J_MVA},
  author       = {Cheng, Ruting and Zeng, Hui and Zhang, Baoqing and Wang, Xuan and Zhao, Tianmeng},
  doi          = {10.1007/s00138-023-01434-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FFA-net: Fast feature aggregation network for 3D point cloud segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGB-LiDAR fusion for accurate 2D and 3D object detection.
<em>MVA</em>, <em>34</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01435-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective detection of road objects in diverse environmental conditions is a critical requirement for autonomous driving systems. Multi-modal sensor fusion is a promising approach for improving perception, as it enables the combination of information from multiple sensor streams in order to optimize the integration of their respective data. Fusion operators are employed within fully convolutional architectures to combine features derived from different modalities. In this research, we present a framework that utilizes early fusion mechanisms to train and evaluate 2D object detection algorithms. Our evaluation shows that sensor fusion outperforms RGB-only detection methods, yielding a boost of +15.07% for car detection, +10.81% for pedestrian detection, and +19.86% for cyclist detection. In our comparative study, we evaluated three arithmetic-based fusion operators and two learnable fusion operators. Furthermore, we conducted a performance comparison between early- and mid-level fusion techniques and investigated the effects of early fusion on state-of-the-art 3D object detectors. Lastly, we provide a comprehensive analysis of the computational complexity of our proposed framework, along with an ablation study.},
  archive      = {J_MVA},
  author       = {Mousa-Pasandi, Morteza and Liu, Tianran and Massoud, Yahya and Laganière, Robert},
  doi          = {10.1007/s00138-023-01435-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RGB-LiDAR fusion for accurate 2D and 3D object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual shuffle attention network for image
super-resolution. <em>MVA</em>, <em>34</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01436-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image super-resolution reconstruction methods based on deep learning achieve satisfactory visual quality; however, the majority are difficult to be directly deployed to mobile or embedded devices due to the model complexity. This paper introduces a lightweight residual shuffle attention network for image super-resolution task. Among them, a residual shuffle attention block (RSAB) that fully integrates the information distillation mechanism is designed to extract deep features, which consists of multiple enhanced residual blocks (MERB) and shuffle attention. The MERB is capable of boosting the feature representation, and the shuffle attention can capture critical information extracted by grouping features. Furthermore, the RSAB utilizes multiple skip connection to build the module structure. Extensive experimental results have demonstrated that the network model proposed in this paper outperforms state-of-the-art methods on several benchmarks with acceptable complexity.},
  archive      = {J_MVA},
  author       = {Li, Xuanyi and Shao, Zhuhong and Li, Bicao and Shang, Yuanyuan and Wu, Jiasong and Duan, Yuping},
  doi          = {10.1007/s00138-023-01436-9},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Residual shuffle attention network for image super-resolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling driving task-relevant attention for intelligent
vehicles using triplet ranking. <em>MVA</em>, <em>34</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01437-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the driving task-relevant attention (i.e. when to pay more attention?) is beneficial for improved safety in intelligent vehicles. Modeling driving task-relevant attention is challenging because it requires a collective understanding of multiple environmental risk factors in a given traffic scene. We formulate this research problem as a learning to rank task when given a traffic scene from a vehicle-mounted camera, we output an attention score that represents the required driver attention level. In this manner, we explicitly enforce the inherent ordering present in the different required attention levels in addition to a clear separation of attention levels. First, we learn a ranking function by contrasting two traffic scenes at a time using a pairwise ranking loss. Then, we introduce a novel triplet ranking architecture to model driving task-relevant attention with improved accuracy and train time. We evaluate our proposed method using traffic scenes from the Berkeley DeepDrive dataset. Experimental results demonstrate that the proposed method outperforms the existing classifier-based methods by a significant margin.},
  archive      = {J_MVA},
  author       = {Withanawasam, Jayani and Kamijo, Shunsuke},
  doi          = {10.1007/s00138-023-01437-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Modeling driving task-relevant attention for intelligent vehicles using triplet ranking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Representing dynamic textures based on polarized gradient
features. <em>MVA</em>, <em>34</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s00138-023-01438-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently representing dynamic textures (DTs) is one of the significant challenges for video understanding in real implementations of computer vision applications. In this work, an efficient approach for DT description is introduced by addressing the following prominent concepts. Firstly, high-order 2D/3D Gaussian-gradient filtering kernels are used for filtering a given video to obtain its Gaussian-gradient-filtered images/volumes. Secondly, taking advantage of the polarizing property of these responses, we propose a competent model of decomposition to decompose them into corresponding robust collections of separately polarized outcomes, which are complementary to DT representation. Finally, a simple variant of local binary patterns (LBPs) is applied to extract local polarized Gaussian-gradient features from the complemented collections for constructing discriminative local-based descriptors. Experimental results for DT recognition on benchmark datasets have remarkably validated the efficacy of our proposal.},
  archive      = {J_MVA},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong and Bouchara, Frédéric},
  doi          = {10.1007/s00138-023-01438-7},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Representing dynamic textures based on polarized gradient features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AFC-net: Adjacent feature complementary for crowded
pedestrian detection. <em>MVA</em>, <em>34</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01439-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, despite the significant performance improvement for pedestrian detection algorithms in crowded scenes, an imbalance between detection accuracy and speed still exists. To address this issue, we propose an adjacent features complementary network for crowded pedestrian detection based on one-stage anchor-free detector, which is called AFC-Net. Firstly, deep dilated convolution (DDC) is invoked in the backbone to expand receptive fields, so that the feature map can remain its original size with feature spatial sensitivity enhanced. Secondly, hierarchical feature extraction (HFE) is designed to extract feature information pertinently according to the feature properties from different layers. Specifically, multi-scale feature extractor and channel attention mechanism are employed to extract contextual information among features on high-level features. Spatial attention mechanism is applied to filter background information on low-level features. Finally, adjacent feature integration (AFI) is proposed to aggregate the correlative features of adjacent layers so as to make expressive ability of features more comprehensive, thus improving the pedestrian detection results. In the challenging CityPersons dataset and CrowdHuman dataset, the crowded scene pedestrian detection network with complementary adjacent features has achieved great results in pedestrian detection. The result achieved from the experiment shows that the proposed algorithm can still maintain the comparability and stability of detection accuracy, while the network parameters are greatly reduced and the speed is effectively improved.},
  archive      = {J_MVA},
  author       = {Wang, Jing and Zhao, Cailing and Liu, Zhiqiang and Huo, Zhanqiang},
  doi          = {10.1007/s00138-023-01439-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {AFC-net: Adjacent feature complementary for crowded pedestrian detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance-dimension dual contrastive learning of visual
representations. <em>MVA</em>, <em>34</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01440-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing contrastive methods usually learn visual representations either by maximizing instance contrast or by minimizing dimension redundancy separately, and fail to make full use of data information. In this paper, we propose an instance-dimension dual contrastive method named IDDCLR to thoroughly mine the intrinsic knowledge underlying data. It jointly optimizes the instance contrast and the dimension redundancy to learn better visual representations. Specifically, we employ the normalized temperature scaled cross entropy (NT-Xent) to formulate the instance contrast loss, and propose a dimension contrast loss function that also takes the form of NT-Xent, resulting in symmetric form of the whole loss. The significance of minimizing the loss is twofold: On the one hand, it learns effective visual representations in the latent space, where the agreement between differently augmented views of the same instance is maximized. On the other hand, it minimizes the redundancy among feature dimensions, consequently being capable of avoiding trivial embeddings. Experimental results show that IDDCLR outperforms state-of-the-art self-supervised methods on classification tasks, and performs comparably on transfer learning tasks.},
  archive      = {J_MVA},
  author       = {Liu, Qingrui and Wang, Liantao and Wang, Qinxu and Zhang, Jinxia},
  doi          = {10.1007/s00138-023-01440-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Instance-dimension dual contrastive learning of visual representations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editor’s note: Special issue from winter conference on
applications of computer vision - WACV 2023. <em>MVA</em>,
<em>34</em>(5), 1. (<a
href="https://doi.org/10.1007/s00138-023-01441-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  doi          = {10.1007/s00138-023-01441-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editor’s note: Special issue from winter conference on applications of computer vision - WACV 2023},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-path u-net for pulmonary vessel segmentation method
based on lightweight 3D attention. <em>MVA</em>, <em>34</em>(5), 1–12.
(<a href="https://doi.org/10.1007/s00138-023-01442-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, pulmonary vessel segmentation has aroused widespread interest in medical image analysis. However, the structure of pulmonary vessels is complex and CT images have a lot of noise. Therefore, it is a difficult task to extract pulmonary vessels accurately. In terms of pulmonary vessel segmentation, medical image segmentation methods based on deep learning only utilize single volume data in CT image, but cannot fully fuse multiple volume data, resulting in the accuracy of pulmonary vessel segmentation is low. In order to fully utilize the complementary advantages of multiple volume data, we propose a DS-ResUNet to segment pulmonary vessels in multi-view. The DS-ResUNet uses feature fusion module to fuse 2  mm and 5 mm volume data and makes full use of the detailed vessel textures in 2 mm volume data and coarse vessel structures in 5 mm volume data. By this multi-view fusion module, the accuracy of pulmonary vessel segmentation can be effectively improved. In addition, in order to strengthen the spatial weight of vessels and reduce the model parameters, we design a lightweight 3D axial attention module by separable convolution. To confirm the improved performance, we design some comparison experiments with the state-of-the-art segmentation methods. As a result, our DS-ResUNet has a better performance than other state-of-the-art methods on pulmonary vessel segmentation, but also has fewer parameters.},
  archive      = {J_MVA},
  author       = {Wu, Rencheng and Xin, Yu and Dong, Yihong and Qian, Jiangbo},
  doi          = {10.1007/s00138-023-01442-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A dual-path U-net for pulmonary vessel segmentation method based on lightweight 3D attention},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Highly transparent material classification using the
refractive index, reflectivity, and transmissivity features from an
imaging model of a time-of-flight camera. <em>MVA</em>, <em>34</em>(5),
1–14. (<a href="https://doi.org/10.1007/s00138-023-01443-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly transparent material classification can play an important role in the field of computer vision to classify glass or plastics for recycling and for home service robots to recognize transparent material. In these areas, there is a need to classify materials that are more than 73% transparent, but current transparent material classification methods cannot classify materials with full transparency levels. This paper proposes a highly transparent material classification method based on the refractive index, reflectivity, and transmissivity features from an imaging model of a time-of-flight (ToF) camera as the classification feature. First, we use the ToF camera to collect the depth and light intensity of the transparent material, as well as the scene information. The acquisition depth is distorted owing to the material characteristics of transparent materials. Second, we estimate the refractive index, reflectance, and transmittance from the depth distortion and IR (infrared rays) image. Finally, we choose a classifier that conforms to the nonlinear characteristics of the data to achieve transparent material classification. The method’s classification accuracy reached 94.1% in an experiment, indicating that our method considers the unique phenomenon of highly transparent materials reflecting against the background, incorporates this phenomenon into the ToF distance model, it can extract material features that express the characteristics of highly transparent materials, making it applicable to the classification of transparent materials at all levels of transparency.},
  archive      = {J_MVA},
  author       = {Lang, Shinan and Chen, Fangyi and Cai, Yiheng},
  doi          = {10.1007/s00138-023-01443-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Highly transparent material classification using the refractive index, reflectivity, and transmissivity features from an imaging model of a time-of-flight camera},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-distribution fitting for multi-view stereo.
<em>MVA</em>, <em>34</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01449-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-view stereo network based on multi-distribution fitting (MDF-Net), which achieves high-resolution depth map prediction with low memory and high efficiency. This method adopts a four-stage cascade structure, which mainly has the following three contributions. First, view cost regularization is proposed to weaken the influence of matching noise on building the cost volume. Second, it is suggested to adaptively calculate the depth refinement interval using multi-distribution fitting (MDF). Gaussian distribution fitting is used to refine and correct depth within a large interval, and then Laplace distribution fitting is used to accurately estimate depth within a small interval. Third, the lightweight image super-resolution network is applied to upsample the depth map in the fourth stage to reduce running time and memory requirements. The experimental results on the DTU dataset indicate that MDF-Net has achieved the most advanced results. It has the lowest memory consumption and running time among the high-resolution reconstruction methods, requiring only approximately 4.29G memory for predicting a depth map with the resolution of 1600 × 1184. In addition, we validate the generalization ability on Tanks and Temples dataset, achieving very competitive performance. The code has been released at https://github.com/zongh5a/MDF-Net .},
  archive      = {J_MVA},
  author       = {Chen, Jinguang and Yu, Zonghua and Ma, Lili and Zhang, Kaibing},
  doi          = {10.1007/s00138-023-01449-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-distribution fitting for multi-view stereo},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowded pose-guided multi-task learning for instance-level
human parsing. <em>MVA</em>, <em>34</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01392-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-level human parsing remains challenging due to the similarity between human instances and background, complex interactions, and various poses. Aiming at assigning each human-related pixel a semantic label and associate each label with the corresponding instance simultaneously, a new top-down method based on multi-task learning guided by crowded pose estimation is proposed to learn instance-level human semantic part information. Firstly, we introduce a path attention feature pyramid to learn more robust multi-scale shared semantic features by changing the feature propagation to concatenation and increasing channel attention at each layer in order to solve the problem of complex background. Secondly, by improving the learned shared features via spatial attention and RC-ASPP, we design an instance-agnostic human parsing module to learn body part segmentation and edge information. In addition, we design a Mask-RCNN-based crowded pose estimation module that uses D-SPPE and hierarchical association rules to obtain pose information. Finally, we define fusion strategy and multi-task learning loss to fuse different semantic features and instance features, which can learn the final instance-level human parsing results in an end-to-end manner. Extensive experimental results on PASCAL-Person-Part and MHPv2.0 dataset verify the effectiveness of our proposed method that outperforms most of state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Wei, Yong and Liu, Li and Fu, Xiaodong and Liu, LiJun and Peng, Wei},
  doi          = {10.1007/s00138-023-01392-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Crowded pose-guided multi-task learning for instance-level human parsing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient abnormality detection using patch-based 3D
convolution with recurrent model. <em>MVA</em>, <em>34</em>(4), 1–23.
(<a href="https://doi.org/10.1007/s00138-023-01397-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the intelligence video monitoring system have received widespread attention for the detection of anomalous human behavior in crowded scenes. Due to the varying crowd densities, low-resolution videos, inter-object occlusions, and complex human crowds, the detection of abnormalities from human activities is extremely challenging. Hence, automatic analysis of behavioral patterns is necessary for accurately modeling crowd behavior and alerting human operators about suspicious activities on the scene. In response to these concerns, we propose a two-stream multi-scale patch-based pyramidal dilated 3D fully connected network (FCN) with attentive bidirectional long short-term memory (2MPD-3DFCN-AttBiDLSTM) for detecting and locating abnormal activities in the frame. This model effectively captures the spatial–temporal features with a dilated convolution network, and thus the motion and optical flow information features are exploited from the continuous frame, which improves the detection accuracy. Also, we introduce a parallel weighted skip connection into the residual learning framework that preserves the rich characteristics of the input data to be learned without the loss of effective features. Based on the attentive mechanism in the bidirectional LSTM model, two directions of temporal and global representations are extracted that enhance the classification of unusual and normal activity in the visual sequences. Experimental analysis is performed with the two publicly available datasets and evaluated in terms of the equal error rate, precision–recall curve, receiver operating characteristic curve, and area under the curve metrics measures. The result shows that the proposed model outperforms the existing model and achieves high detection results in the video surveillance monitoring system.},
  archive      = {J_MVA},
  author       = {Kokila, M. L. Sworna and Christopher, V. Bibin and Sajan, R. Isaac and Akhila, T. S. and Kavitha, M. Joselin},
  doi          = {10.1007/s00138-023-01397-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Efficient abnormality detection using patch-based 3D convolution with recurrent model},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VGT-MOT: Visibility-guided tracking for online
multiple-object tracking. <em>MVA</em>, <em>34</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01398-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is an important task of computer vision which has a wide range of applications. Existing multi-object tracking methods mostly employ the Kalman filter to predict the object location in the next frame. However, if the video is captured by a camera with significant motion variation or contains objects moving at non-constant speed, the Kalman filter may fail. In addition, although object occlusion has been studied extensively in MOT, it has not been well addressed yet. To deal with these problems, a joint detection and tracking method named visibility-guided tracking for MOT (VGT-MOT) is proposed in this paper. Specifically, to cope with the difficulty of accurate object position estimation caused by drastic camera or object motion variation, VGT-MOT utilizes an adjacent-frame object location prediction network with inter-frame attention to predict the target position in the next frame. To handle object occlusion, VGT-MOT employs the object visibility as a dynamic weight to adaptively fuse the motion and appearance similarities and update the object appearance representation. The proposed VGT-MOT has been evaluated on the MOT16, MOT17 and MOT20 datasets. The results show that VGT-MOT compares favorably against state-of-the-art MOT approaches. The source code of the proposed method is available at https://github.com/wang-ironman/VGT-MOT.},
  archive      = {J_MVA},
  author       = {Wang, Shuai and Li, Wei-Xi and Wang, Lu and Xu, Li-Sheng and Deng, Qing-Xu},
  doi          = {10.1007/s00138-023-01398-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {VGT-MOT: Visibility-guided tracking for online multiple-object tracking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal dance style transfer. <em>MVA</em>,
<em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01399-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper first presents CycleDance, a novel dance style transfer system that transforms an existing motion clip in one dance style into a motion clip in another dance style while attempting to preserve the motion context of the dance. CycleDance extends existing CycleGAN architectures with multimodal transformer encoders to account for the music context. We adopt a sequence length-based curriculum learning strategy to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. Building upon CycleDance, we further propose StarDance, which enables many-to-many mappings across different styles using a single generator network. Additionally, we introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. To evaluate the performance of our approach, we perform an extensive ablation study and a human study with 30 participants, each with 5 or more years of dance experience. Our experimental results show that our approach can generate realistic movements with the target style, outperforming the baseline CycleGAN and its variants on naturalness, transfer strength, and content preservation. Our proposed approach has potential applications in choreography, gaming, animation, and tool development for artistic and scientific innovations in the field of dance.},
  archive      = {J_MVA},
  author       = {Yin, Wenjie and Yin, Hang and Baraka, Kim and Kragic, Danica and Björkman, Mårten},
  doi          = {10.1007/s00138-023-01399-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multimodal dance style transfer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). X-align++: Cross-modal cross-view alignment for
bird’s-eye-view segmentation. <em>MVA</em>, <em>34</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01400-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bird’s-eye-view (BEV) grid is a typical representation of the perception of road components, e.g., drivable area, in autonomous driving. Most existing approaches rely on cameras only to perform segmentation in BEV space, which is fundamentally constrained by the absence of reliable depth information. The latest works leverage both camera and LiDAR modalities but suboptimally fuse their features using simple, concatenation-based mechanisms. In this paper, we address these problems by enhancing the alignment of the unimodal features in order to aid feature fusion, as well as enhancing the alignment between the cameras’ perspective view (PV) and BEV representations. We propose X-Align, a novel end-to-end cross-modal and cross-view learning framework for BEV segmentation consisting of the following components: (i) a novel Cross-Modal Feature Alignment (X-FA) loss, (ii) an attention-based Cross-Modal Feature Fusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an auxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA) losses to improve the PV-to-BEV transformation. We evaluate our proposed method across two commonly used benchmark datasets, i.e., nuScenes and KITTI-360. Notably, X-Align significantly outperforms the state-of-the-art by 3 absolute mIoU points on nuScenes. We also provide extensive ablation studies to demonstrate the effectiveness of the individual components.},
  archive      = {J_MVA},
  author       = {Borse, Shubhankar and Klingner, Marvin and Ravi, Varun and Cai, Hong and Almuzairee, Abdulaziz and Yogamani, Senthil and Porikli, Fatih},
  doi          = {10.1007/s00138-023-01400-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {X-align++: Cross-modal cross-view alignment for bird’s-eye-view segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WideCaps: A wide attention-based capsule network for image
classification. <em>MVA</em>, <em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01401-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capsule network is a distinct and promising segment of the neural network family that has drawn attention due to its unique ability to maintain equivariance by preserving spatial relationships among the features. The capsule network has attained unprecedented success in image classification with datasets such as MNIST and affNIST by encoding the characteristic features into capsules and building a parse-tree structure. However, on datasets involving complex foreground and background regions, such as CIFAR-10 and CIFAR-100, the performance of the capsule network is suboptimal due to its naive data routing policy and incompetence in extracting complex features. This paper proposes a new design strategy for capsule network architectures for efficiently dealing with complex images. The proposed method incorporates the optimal placement of the novel wide bottleneck residual block and squeeze and excitation Attention Blocks into the capsule network upheld by the modified factorized machines routing algorithm to address the defined problem. This setup allows channel interdependencies at almost no computational cost, thereby enhancing the representation ability of capsules on complex images. We extensively evaluate the performance of the proposed model on the five publicly available datasets, namely the CIFAR-10, Fashion MNIST, Brain Tumor, SVHN, and the CIFAR-100 datasets. The proposed method outperformed the top-5 capsule network-based methods on Fashion MNIST, CIFAR-10, SVHN, Brain Tumor, and gave a highly competitive performance on the CIFAR-100 datasets.},
  archive      = {J_MVA},
  author       = {Pawan, S. J. and Sharma, Rishi and Reddy, Hemanth and Vani, M. and Rajan, Jeny},
  doi          = {10.1007/s00138-023-01401-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WideCaps: A wide attention-based capsule network for image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature refinement with multi-level context for object
detection. <em>MVA</em>, <em>34</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01402-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust multi-scale object detection is challenging as it requires both spatial details and semantic knowledge to deal with problems including high scale variation and cluttered background. Appropriate fusion of high-resolution features with deep semantic features is the key issue to achieve better performance. Different approaches have been developed to extract and combine deep features with shallow layer spatial features, such as feature pyramid network. However, high-resolution feature maps contain noisy and distractive features. Directly combines shallow features with semantic features might degrade detection accuracy. Besides, contextual information is also important for multi-scale object detection. In this work, we present a feature refinement scheme to tackle the feature fusion problem. The proposed feature refinement module increases feature resolution and refine feature maps progressively with the guidance from deep features. Meanwhile, we propose a context extraction method to capture global and local contextual information. The method utilizes a multi-level cross-pooling unit to extract global context and a cascaded context module to extract local context. The proposed object detection framework has been evaluated on PASCAL VOC and MS COCO datasets. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art approaches.},
  archive      = {J_MVA},
  author       = {Ma, Yingdong and Wang, Yanan},
  doi          = {10.1007/s00138-023-01402-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature refinement with multi-level context for object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The general framework for few-shot learning by kernel
HyperNetworks. <em>MVA</em>, <em>34</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01403-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot models aim at making predictions using a minimal number of labeled examples from a given task. The main challenge in this area is the one-shot setting, where only one element represents each class. We propose the general framework for few-shot learning via kernel HyperNetworks—the fusion of kernels and hypernetwork paradigm. Firstly, we introduce the classical realization of this framework, dubbed HyperShot. Compared to reference approaches that apply a gradient-based adjustment of the parameters, our models aim to switch the classification module parameters depending on the task’s embedding. In practice, we utilize a hypernetwork, which takes the aggregated information from support data and returns the classifier’s parameters handcrafted for the considered problem. Moreover, we introduce the kernel-based representation of the support examples delivered to hypernetwork to create the parameters of the classification module. Consequently, we rely on relations between the support examples’ embeddings instead of the backbone models’ direct feature values. Thanks to this approach, our model can adapt to highly different tasks. While such a method obtains very good results, it is limited by typical problems such as poorly quantified uncertainty due to limited data size. We further show that incorporating Bayesian neural networks into our general framework, an approach we call BayesHyperShot, solves this issue.},
  archive      = {J_MVA},
  author       = {Sendera, Marcin and Przewiȩźlikowski, Marcin and Miksa, Jan and Rajski, Mateusz and Karanowski, Konrad and Ziȩba, Maciej and Tabor, Jacek and Spurek, Przemysław},
  doi          = {10.1007/s00138-023-01403-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {The general framework for few-shot learning by kernel HyperNetworks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint self-supervised learning and adversarial adaptation
for monocular depth estimation from thermal image. <em>MVA</em>,
<em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01404-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from thermal images is one potential solution to achieve reliability and robustness against diverse weather, lighting, and environmental conditions. Also, a self-supervised training method further boosts its scalability to various scenarios, which are usually impossible to collect ground-truth labels, such as GPS-denied and LiDAR-denied conditions. However, self-supervision from thermal images is usually insufficient to train networks due to the thermal image properties, such as low-contrast and textureless properties. Introducing additional self-supervision sources (e.g., RGB images) also introduces further hardware and software constraints, such as complicated multi-sensor calibration and synchronized data acquisition. Therefore, this manuscript proposes a novel training framework combining self-supervised learning and adversarial feature adaptation to leverage additional modality information without such constraints. The framework aims to train a network that estimates a monocular depth map from a thermal image in a self-supervised manner. In the training stage, the framework utilizes two self-supervisions; image reconstruction of unpaired RGB-thermal images and adversarial feature adaptation between unpaired RGB-thermal features. Based on the proposed method, the trained network achieves state-of-the-art quantitative results and edge-preserved depth estimation results compared to previous methods. Our source code is available at www.github.com/ukcheolshin/SelfDepth4Thermal},
  archive      = {J_MVA},
  author       = {Shin, Ukcheol and Park, Kwanyong and Lee, Kyunghyun and Lee, Byeong-Uk and Kweon, In So},
  doi          = {10.1007/s00138-023-01404-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Joint self-supervised learning and adversarial adaptation for monocular depth estimation from thermal image},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Saliency prediction based on multi-channel models of visual
processing. <em>MVA</em>, <em>34</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-023-01405-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention is one of the most significant characteristics for selecting and understanding the outside redundancy world. The human vision system cannot process all information simultaneously due to the visual information bottleneck. In order to reduce the redundant input of visual information, the human visual system mainly focuses on dominant parts of scenes. This is commonly known as visual saliency map prediction. This paper proposed a new psychophysical oriented saliency prediction architecture, which inspired by multi-channel model of visual cortex functioning in humans. The model consists of opponent color channels, wavelet transform, wavelet energy map, and contrast sensitivity function for extracting low-level image features and providing a maximum approximation to the low-level human visual system. The proposed model is evaluated using several datasets, including the MIT1003, MIT300, TORONTO, SID4VAM, and UCF Sports datasets. We also quantitatively and qualitatively compare the saliency prediction performance with that of other state-of-the-art models. Our model achieved strongly stable and better performance with different metrics on natural images, psychophysical synthetic images and dynamic videos. Additionally, we suggested that Fourier and spectral-inspired saliency prediction models outperformed other state-of-the-art non-neural network and even deep neural network models on psychophysical synthetic images. In the meantime, we suggest that deep neural networks need specific architectures and goals to be able to predict salient performance on psychophysical synthetic images better and more reliably. Finally, the proposed model could be used as a computational model of primate low-level vision system and help us understand mechanism of primate low-level vision system. The project page can be available at: https://sinodanishspain.github.io/HVS_SaliencyModel/ .},
  archive      = {J_MVA},
  author       = {Li, Qiang},
  doi          = {10.1007/s00138-023-01405-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Saliency prediction based on multi-channel models of visual processing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Foreground enhancement network for object detection in sonar
images. <em>MVA</em>, <em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01406-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special detection task, sonar image object detection has been suffering from two main problems: the widespread noise and the lack of high-frequency information. In this paper, we propose two independent modules to solve the above two problems. For the widespread noise, we propose the foreground semantic enhancement module. Different from simple feature fusion, this module creatively associates the semantic map with features from each feature level, thus increasing the foreground–background distance and highlighting the object information. To solve the problem of insufficient high-frequency information, we propose the foreground edge enhancement module. This module inventively combines RNN networks to enhance edges by spatial semantic information from different directions as a way to improve the feature representation of foreground objects. Based on the above two modules, we design a novel detection architecture, foreground enhancement network (FEN), which enhances the features of a single point to make the classification more powerful and the localization more accurate. Through extensive experimental validation, our FEN network achieves high-performance improvement when combined with different detectors, and achieves the highest 10 $$\%$$ mAP performance improvement when combined with a single-stage detector (FCOS).},
  archive      = {J_MVA},
  author       = {Yang, Chao and Li, Yongpeng and Jiang, Longyu and Huang, Jianxing},
  doi          = {10.1007/s00138-023-01406-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Foreground enhancement network for object detection in sonar images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape description losses for medical image segmentation.
<em>MVA</em>, <em>34</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01407-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of deep learning in medical image segmentation, most deep learning-based segmentation methods utilize pixel-wise losses as their training objectives, which focus on the local information around each pixel but lack global context. In this paper, we propose a set of shape description losses to supervise the training of the segmentation networks by extracting and quantifying geometric features of the targets, including volume, surface area, center of mass, and bounding box. We believe that these loss functions can introduce global geometrical and statistical constraints into the training procedure. The proposed shape losses are differentiable and can be computed efficiently. We demonstrate that using the proposed shape losses alone for weakly supervised learning can obtain promising performance. The results of our study illustrate the effectiveness of the shape descriptions with physical meanings on segmenting different organs from various modalities, including liver from computed tomography, left atrium from magnetic resonance imaging, and prostate from ultrasound volume.},
  archive      = {J_MVA},
  author       = {Fang, Xi and Xu, Xuanang and Xia, James J. and Sanford, Thomas and Turkbey, Baris and Xu, Sheng and Wood, Bradford J. and Yan, Pingkun},
  doi          = {10.1007/s00138-023-01407-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Shape description losses for medical image segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text-driven object affordance for guiding grasp-type
recognition in multimodal robot teaching. <em>MVA</em>, <em>34</em>(4),
1–11. (<a href="https://doi.org/10.1007/s00138-023-01408-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In robot teaching, the grasping strategies taught to robots by users are critical information, because these strategies contain the implicit knowledge necessary to successfully perform a series of manipulations; however, limited practical knowledge exists on how to utilize linguistic information for supporting grasp-type recognition in multimodal teaching. This study focused on the effects of text-driven object affordance—a prior distribution of grasp types for each object—on image-based grasp-type recognition. To this end, we created the datasets of first-person grasping-hand images labeled with grasp types and object names and tested if the object affordance enhanced the performance of image-based recognition. We evaluated two scenarios with real and illusory objects to be grasped, considering a teaching condition in mixed reality, where the lack of visual object information can make image-based recognition challenging. The results show that object affordance guided the image-based recognition in two scenarios, that is, increasing the recognition accuracy by (1) excluding the unlikely grasp types from the candidates and (2) enhancing the likely grasp types. Additionally, the “enhancing effect” was more pronounced with greater grasp-type bias for each object in a test dataset. These results indicate the effectiveness of object affordance for guiding grasp-type recognition in multimodal robot teaching applications. The contributions of this study are (1) demonstrating the effectiveness of object affordance in guiding grasp-type recognition both with and without the real objects in images, (2) demonstrating the conditions under which the merits of object affordance are pronounced, and (3) providing a dataset of first-person grasping images labeled with possible grasp types for each object.},
  archive      = {J_MVA},
  author       = {Wake, Naoki and Saito, Daichi and Sasabuchi, Kazuhiro and Koike, Hideki and Ikeuchi, Katsushi},
  doi          = {10.1007/s00138-023-01408-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Text-driven object affordance for guiding grasp-type recognition in multimodal robot teaching},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese tracker with temporal information based on
transformer-like feature fusion mechanism. <em>MVA</em>, <em>34</em>(4),
1–10. (<a href="https://doi.org/10.1007/s00138-023-01409-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The position of the target in video trackers based on the Siamese network is usually obtained by calculating the similarity score of features between the target template and the predicted region. This method performs poorly in complex scenarios due to such problems as the deformation of the target and motion blur. To solve these problems, this paper proposes a transformer-like feature fusion mechanism to fuse the temporal information of consecutive frames of the video. We separate the encoder and decoder into two parallel branches to accommodate the characteristics of Siamese networks. The features of the target template are enhanced by a transformer-like encoder while temporal feature-related information is fused by using a transformer-like decoder. The results of experiments on the standard OTB100, VOT2018, UAV123, and NFS datasets showed that the proposed network outperforms most mainstream algorithms in the area.},
  archive      = {J_MVA},
  author       = {Shi, Yuexiang and Wu, Ziping and Chen, Yangzhuo and Dong, Jinlong},
  doi          = {10.1007/s00138-023-01409-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Siamese tracker with temporal information based on transformer-like feature fusion mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLDM: Convolutional layer dropout module. <em>MVA</em>,
<em>34</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s00138-023-01411-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) are prone to overfitting due to their overparameterization. Structural dropout methods such as weighted channel dropout alleviate this problem by dropping continuous regions based on the importance degrees computed from the average activation values of each channel in the feature map. However, there is insufficient evidence supporting the mean value as a representative evaluation method of importance degree. Additionally, the importance degree of a channel may also be related to kernel information. To better represent the importance degree of channels, this work proposes using the variance instead of the mean as the importance evaluation method of regions in structural dropout methods and proposes a convolutional layer dropout module (CLDM), which utilizes the variance values of both the kernel and feature map to determine the regions that can be dropped. CLDM is a parameter-free plug-and-play module used for regularizing various deep CNNs without any additional computational cost during the test phase. Extensive experimental results on various datasets demonstrate that the proposed CLDM outperforms other state-of-the-art structural dropout methods and proves the effectiveness of the variance evaluation method and the benefit of introducing kernel information in the dropout process, respectively.},
  archive      = {J_MVA},
  author       = {Zhao, Jiafeng and Ye, Xiang and Yue, Tan and Li, Yong},
  doi          = {10.1007/s00138-023-01411-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CLDM: Convolutional layer dropout module},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using synthesized facial views for active face recognition.
<em>MVA</em>, <em>34</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01412-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active perception/vision exploits the ability of robots to interact with their environment, for example move in space, towards increasing the quantity or quality of information obtained through their sensors and, thus, improving their performance in various perception tasks. Active face recognition is largely understudied in recent literature. Attempting to tackle this situation, in this paper, we propose an active approach that utilizes facial views produced by photorealistic facial image rendering. Essentially, the robot that performs the recognition selects the best among a number of candidate movements around the person of interest by simulating their results through view synthesis. This is accomplished by feeding the robot’s face recognizer with a real-world facial image acquired in the current position, generating synthesized views that differ by $$\pm \theta ^\circ $$ from the current view and deciding, based on the confidence of the recognizer, whether to stay in place or move to the position that corresponds to one of the two synthesized views, in order to acquire a new real image with its sensor. Experimental results in three datasets verify the superior performance of the proposed method compared to the respective “static” approach, approaches based on the same face recognizer that involve synthetic face frontalization and synthesized views, random direction robot movement, robot movement towards a frontal location based on view angle estimation, as well as a state of the art active method. Results from a proof of concept simulation in a robotic simulator are also provided.},
  archive      = {J_MVA},
  author       = {Kakaletsis, Efstratios and Nikolaidis, Nikos},
  doi          = {10.1007/s00138-023-01412-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Using synthesized facial views for active face recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic apraxia detection using deep convolutional neural
networks and similarity methods. <em>MVA</em>, <em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01413-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dementia represents one of the great problems to be solved in medicine for a society that is becoming increasingly long-lived. One of the main causes of dementia is Alzheimer’s disease, which accounts for 80% of cases. There is currently no cure for this disease, although there are treatments to try to alleviate its effects, which is why detecting Alzheimer’s disease in its early stages is crucial to slow down its evolution and thus help sufferers. One of the symptoms of the disease that manifests in its early stages is apraxia, difficulties in carrying out voluntary movements. In the clinical setting, apraxia is typically assessed by asking the patient to imitate hand gestures that are performed by the examiner. To automate this test, this paper proposes a system that, based on a video of the patient making the gesture, evaluates its execution. This evaluation is done in two steps, first extracting the skeleton of the hands and then using a similarity function to obtain an objective score of the execution of the gesture. The results obtained in an experiment with several patients performing different gestures are shown, showing the effectiveness of the proposed method. The system is intended to serve as a diagnostic tool, enabling medical experts to detect possible mobility impairments in patients that may have signs of Alzheimer’s disease.},
  archive      = {J_MVA},
  author       = {Vicedo, Cristina and Nieto-Reyes, Alicia and Bringas, Santos and Duque, Rafael and Lage, Carmen and Montaña, José Luis},
  doi          = {10.1007/s00138-023-01413-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic apraxia detection using deep convolutional neural networks and similarity methods},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persistent animal identification leveraging non-visual
markers. <em>MVA</em>, <em>34</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-023-01414-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our objective is to locate and provide a unique identifier for each mouse in a cluttered home-cage environment through time, as a precursor to automated behaviour recognition for biological research. This is a very challenging problem due to (i) the lack of distinguishing visual features for each mouse, and (ii) the close confines of the scene with constant occlusion, making standard visual tracking approaches unusable. However, a coarse estimate of each mouse’s location is available from a unique RFID implant, so there is the potential to optimally combine information from (weak) tracking with coarse information on identity. To achieve our objective, we make the following key contributions: (a) the formulation of the object identification problem as an assignment problem (solved using Integer Linear Programming), (b) a novel probabilistic model of the affinity between tracklets and RFID data, and (c) a curated dataset with per-frame BB and regularly spaced ground-truth annotations for evaluating the models. The latter is a crucial part of the model, as it provides a principled probabilistic treatment of object detections given coarse localisation. Our approach achieves 77% accuracy on this animal identification problem, and is able to reject spurious detections when the animals are hidden.},
  archive      = {J_MVA},
  author       = {Camilleri, Michael P. J. and Zhang, Li and Bains, Rasneer S. and Zisserman, Andrew and Williams, Christopher K. I.},
  doi          = {10.1007/s00138-023-01414-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Persistent animal identification leveraging non-visual markers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-atlas subcortical segmentation: An orchestration of 3D
fully convolutional network and generalized mixture function.
<em>MVA</em>, <em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01415-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately segment subcortical structures and therefore profit for numerous neuroimaging applications, we proposed a multi-atlas subcortical segmentation method by orchestrating a 3D fully convolutional network and a generalized mixture function. Template atlases were first aligned to the target image. Then, target image patches and several most similar atlas patches were extracted from the transformed template atlases by employing a proposed similar atlas selection network and fed into the proposed multi-atlas driven 3D fully convolutional neural network. To sufficiently extract the subcortical features and improve the segmentation performance, a restricted region thought as a bounding box was utilized to roughly locate the subcortical structures. Additionally, a generalized mixture function was introduced to reduce the impact of the size and stride in 3D patch extraction. Two datasets consisting of 16 and 18 T1-weighted magnetic resonance images images (MRIs) were included to evaluate the proposed method, respectively. The results showed significantly higher segmentation accuracy than several state-of-the-art subcortical segmentation approaches for most subcortical structures. Furthermore, the proposed method achieved notable higher mean Dice similarity coefficients being, respectively, 0.915 and 0.869. The proposed method automatically and accurately segments subcortical structures in MRIs, which may assist the artificial diagnosis of brain disorders.},
  archive      = {J_MVA},
  author       = {Wu, Jiong and He, Shuan and Zhou, Shuang},
  doi          = {10.1007/s00138-023-01415-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-atlas subcortical segmentation: An orchestration of 3D fully convolutional network and generalized mixture function},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parametric loss-based super-resolution for scene text
recognition. <em>MVA</em>, <em>34</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01416-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text image super-resolution (STISR) is regarded as the process of improving the image quality of low-resolution scene text images to improve text recognition accuracy. Recently, a text attention network was introduced to reconstruct high-resolution scene text images; the backbone method involved the convolutional neural network-based and transformer-based architecture. Although it can deal with rotated and curved-shaped texts, it still cannot properly handle images containing improper-shaped texts and blurred text regions. This can lead to incorrect text predictions during the text recognition step. In this study, we propose the application of multiple parametric regularizations and parametric weight parameters to the loss function of the STISR method to improve scene text image quality and text recognition accuracy. We design and extend it into three types of methods: adding multiple parametric regularizations, modifying parametric weight parameters, and combining parametric weights and multiple parametric regularizations. Experiments were conducted and compared with state-of-the-art models. The results showed a significant improvement for every proposed method. Moreover, our methods generated clearer and sharper edges than the baseline with a better-quality image score.},
  archive      = {J_MVA},
  author       = {Viriyavisuthisakul, Supatta and Sanguansat, Parinya and Racharak, Teeradaj and Nguyen, Minh Le and Kaothanthong, Natsuda and Haruechaiyasak, Choochart and Yamasaki, Toshihiko},
  doi          = {10.1007/s00138-023-01416-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Parametric loss-based super-resolution for scene text recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online continual learning with saliency-guided experience
replay using tiny episodic memory. <em>MVA</em>, <em>34</em>(4), 1–14.
(<a href="https://doi.org/10.1007/s00138-023-01420-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps, which provide visual explanations for the model’s decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model’s prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. We perform a detailed study to show the effectiveness of zero-padded patch replay compared to the other candidate approaches. Moreover, with qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase and hence performs well with small episodic memory.},
  archive      = {J_MVA},
  author       = {Saha, Gobinda and Roy, Kaushik},
  doi          = {10.1007/s00138-023-01420-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Online continual learning with saliency-guided experience replay using tiny episodic memory},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective long-term tracking with contrast optimizer.
<em>MVA</em>, <em>34</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-023-01422-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenge of long-term tracking includes data uncertainty in long-term observations. Previous methods tackle the long-term tracking task by online update-based trackers. However, sophisticated online update strategies of these trackers are usually with a considerable computational burden. In this work, a contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. Specifically, the optimizer first perceives the similarity between distractors and positive samples through metric learning. Next, the contrastive learning between target anchors and hard negative samples forces the optimizer to notice the difference between targets and distractors. Finally, the optimizer will learn a binary output to assist the tracker updating. The proposed optimizer can be easily integrated into other online trackers with little impact on their running speed. Extensive experimental results show that the method achieves state-of-the-art performance on the VOT2018LT, VOT2019LT, OxUvA, and LaSOT benchmarks while running at real-time speed on GPU.},
  archive      = {J_MVA},
  author       = {Han, Yongbo and Liang, Yitao},
  doi          = {10.1007/s00138-023-01422-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Effective long-term tracking with contrast optimizer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alternate guidance network for boundary-aware camouflaged
object detection. <em>MVA</em>, <em>34</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01424-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object, similar to the background, shows indistinct boundaries and low-contrast features, which brings great challenges to the detection task. Moreover, existing models still suffer from coarse object boundaries. Motivated by the complementary relationship between boundaries and camouflaged object regions, we propose an alternate guidance network named AGNet for better interaction between them. Specifically, we first propose feature selective module to select high discriminative features and simultaneously filter out noisy background features. The proposed AGNet follows a locate and refine manner, where multi-scale convolution is applied to expand receptive field for accurate initial coarse localization. Finally, a novel alternate guidance module is designed and embedded into each side-output to refine the previous localization progressively. Contributed by it, the complementary characteristic between the region and boundary features can be well captured, which is beneficial to obtain more complete detection. Experimental results on five COD datasets prove the effectiveness of our model, and it is superior to existing state-of-the-art models in object accuracy and boundary accuracy.},
  archive      = {J_MVA},
  author       = {Yu, Jinhao and Chen, Shuhan and Lu, Lu and Chen, Zeyu and Xu, Xiuqi and Hu, Xuelong and Zhu, Jinrong},
  doi          = {10.1007/s00138-023-01424-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Alternate guidance network for boundary-aware camouflaged object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MÆIDM: Multi-scale anomaly embedding inpainting and
discrimination for surface anomaly detection. <em>MVA</em>,
<em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01425-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of anomalous structures in natural image data plays a crucial role in numerous tasks in the field of computer vision. Methods based on image reconstruction or inpainting are trained on images with no anomalies or artificial anomalies; anomaly detection and localization are achieved by computing the difference between the input image and the reconstructed image. DRÆM trains two sub-networks to reduce over-fitting of synthetic appearances. This method uses an encoder–decoder and an U-Net-like network to detect and locate anomalies. In order to further improve the performance of the model in accurate inpainting of abnormal images and pixel-level segmentation, we propose a multi-scale anomaly embedding inpainting and discrimination model (MÆIDM). The proposed method introduces a trainable multi-scale feature enrichment module (MFEM) in reconstructive sub-network for image inpainting and an attention discriminative sub-network for defect segmentation. In addition, the Gaussian filtering is used to smooth the anomaly score map. Extensive experiments show that our method achieves excellent performance on the anomaly detection dataset MVTec and two unpublished fabric datasets with AUC scores of 98.5% and 98.1% at the image level and pixel level, respectively. Meanwhile, our model further achieves better detection performance on the supervised DAGM surface defect detection dataset, which proves the effectiveness of the method.},
  archive      = {J_MVA},
  author       = {Sheng, Siyu and Jing, Junfeng and Jiao, Xintian and Wang, Yafei and Dong, Zhenyu},
  doi          = {10.1007/s00138-023-01425-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MÆIDM: Multi-scale anomaly embedding inpainting and discrimination for surface anomaly detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECM: Arbitrary style transfer via enhanced-channel module.
<em>MVA</em>, <em>34</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01428-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer is to fuse the style of one image with the content of another image. With the development of artificial intelligence, many style transfer methods have emerged, which focus on stylizing content features in different aspects, such as loss functions, attention mechanism, and functional instance normalization layer. However, there are still problems such as the lack of local detail information or the insufficient degree of style fusion in the generated images. To get more realistic images, we propose an Enhanced-Channel Module, which vectorizes content feature maps and style feature maps to generate content-aware channel weights. The channel weights are multiplied by the content feature maps as the map offsets to complete the style injection. Meanwhile, in order to utilize the multilayer feature maps, we train the network in a progressive rendering way using multilayer content features and deep style features. The comparison experiments with the state-of-the-art methods prove that the proposed method can generate high-quality stylization results in artistic style transfer and video style transfer tasks, and the ablation experiments prove the effectiveness of the different components.},
  archive      = {J_MVA},
  author       = {Yu, Xiaoming and Zhou, Gan},
  doi          = {10.1007/s00138-023-01428-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ECM: Arbitrary style transfer via enhanced-channel module},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic scene blind image deblurring based on local and
non-local features. <em>MVA</em>, <em>34</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01384-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring is a fundamental and challenging task in the field of computer vision. Despite image deblurring has been made considerable progress, there is still room for improvement in the visual effect and details of the images. Therefore, we present an image deblurring model based on local and non-local features for non-uniform scene deblurring in an end-to-end fashion. Correspondingly, we develop a dense dilated block (DDB) and an improved attention module (IAM) to excavate local and non-local features, respectively. DDB focuses on enhancing feature correlation and constructing complex features in high dimensions by exploiting local features. IAM is a gate mechanism, which implicates spatial context information and attention maps based on non-local channels dependencies. Compared to the previous methods, our method surpasses state-of-the-art (SOTA) methods on both synthetic datasets and real-world images.},
  archive      = {J_MVA},
  author       = {Qi, Qing},
  doi          = {10.1007/s00138-023-01384-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dynamic scene blind image deblurring based on local and non-local features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ICE-GCN: An interactional channel excitation-enhanced graph
convolutional network for skeleton-based action recognition.
<em>MVA</em>, <em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01386-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the development of depth sensors and pose estimation algorithms, skeleton-based action recognition has become prevalent in the computer vision community. Most of the existing works are based on spatio-temporal graph convolutional network frameworks, which learn and treat all spatial or temporal features equally, ignoring the interaction with channel dimension to explore different contributions of different spatio-temporal patterns along the channel direction and thus losing the ability to distinguish confusing actions with subtle differences. In this paper, an interactional channel excitation (ICE) module is proposed to explore discriminative spatio-temporal features of actions by adaptively recalibrating channel-wise pattern maps. More specifically, a channel-wise spatial excitation (CSE) is incorporated to capture the crucial body global structure patterns to excite the spatial-sensitive channels. A channel-wise temporal excitation (CTE) is designed to learn temporal inter-frame dynamics information to excite the temporal-sensitive channels. ICE enhances different backbones as a plug-and-play module. Furthermore, we systematically investigate the strategies of graph topology and argue that complementary information is necessary for sophisticated action description. Finally, together equipped with ICE, an interactional channel excited graph convolutional network with complementary topology (ICE-GCN) is proposed and evaluated on three large-scale datasets, NTU RGB+D 60, NTU RGB+D 120, and Kinetics-Skeleton. Extensive experimental results and ablation studies demonstrate that our method outperforms other SOTAs and proves the effectiveness of individual sub-modules. The code will be published at https://github.com/shuxiwang/ICE-GCN .},
  archive      = {J_MVA},
  author       = {Wang, Shuxi and Pan, Jiahui and Huang, Binyuan and Liu, Pingzhi and Li, Zina and Zhou, Chengju},
  doi          = {10.1007/s00138-023-01386-2},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ICE-GCN: An interactional channel excitation-enhanced graph convolutional network for skeleton-based action recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep 6-DoF camera relocalization in variable and dynamic
scenes by multitask learning. <em>MVA</em>, <em>34</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01388-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, direct visual localization with convolutional neural networks has attracted researchers’ attention with achieving an end-to-end process. However, on the one side, the lack of using 3D information leads to imprecise accuracy. Meanwhile, the single input image confuses the relocalization in the scenes that keep similar views at different positions. On the other side, the relocalization problem in variable or dynamic scenes is still challenging. Concentrating on these concerns, we propose two multitask relocalization networks called MMLNet and MMLNet+ for obtaining the 6-DoF camera pose in static, variable and dynamic scenes. Firstly, addressing the dataset lack of variable scenes, we construct a variable scene dataset with a semiautomatic process combining SFM and MVS algorithms with a few manual labels. Based on the process, three scenes covering an office, a bedroom and a sitting room are gathered and generated. Secondly, to enhance the perception between 2D images and 3D poses, we design a multitask network called MMLNet that regresses both camera pose and scene point cloud. Meanwhile, the Chamfer distance is joined into the original pose loss to optimize MMLNet. Moreover, MMLNet learns the pose trajectory feature by using LSTM layers to the additional pose array input, which meanwhile breaks through the limitation of single image input. Based on the MMLNet, aiming at dynamic and variable scenes, MMLNet+ outputs the auxiliary segmentation branch that distinguishes fixed, changeable or dynamic parts of the input image. Furthermore, we define the feature fusion block to implement the feature sharing among three tasks, further promoting the performance in dynamic and variable environments. Finally, experiments on static, dynamic and our constructed variable datasets demonstrate state-of-the-art relocalization performances of MMLNet and MMLNet+. Simultaneously, the positive effects of the pose learning part, reconstruction branch and segmentation task are also illustrated.},
  archive      = {J_MVA},
  author       = {Wang, Junyi and Qi, Yue},
  doi          = {10.1007/s00138-023-01388-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep 6-DoF camera relocalization in variable and dynamic scenes by multitask learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel cellular automata-based approach for generating
convolutional filters. <em>MVA</em>, <em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01389-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is a well-studied problem where the aim is to categorize given images into a predefined set of classes. Although there are different approaches for solving the problem, convolutional neural networks (CNNs) have achieved significant success in the domain. CNN uses convolutional layers to extract features from images, and these layers are usually created with a supervised training process. This training process requires a group of convolution operations and several passes over the dataset. Hence, the model possesses a heavy computational burden. In this work, a cellular automata-based unsupervised methodology is proposed to create convolutional filters. The proposed methodology accesses each data instance only twice regardless of the number of layers in the model, and it requires no backpropagation operation. Thus, the computational burden is significantly reduced compared to CNNs. The classification process can be carried out directly by using the model together with a multilayer perceptron. Also, the model can be used to enhance CNNs in terms of time and accuracy by initializing the parameters of CNN or by preprocessing the raw data. The proposed methodology creates competitive results compared to CNNs in terms of accuracy and computational complexity. Also, the results show that the performance of the CNN model can be increased by using the filters created by the proposed methodology.},
  archive      = {J_MVA},
  author       = {Yeşil, Çağrı and Korkmaz, Emin Erkan},
  doi          = {10.1007/s00138-023-01389-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel cellular automata-based approach for generating convolutional filters},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable visual transmission lines inspections using
pseudo-prototypical part network. <em>MVA</em>, <em>34</em>(3), 1–18.
(<a href="https://doi.org/10.1007/s00138-023-01390-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To guarantee the reliability of the electric energy supply, it is necessary that the transmission lines are operating without interruptions. To improve the identification of faults in the electrical power system, the unmanned aerial vehicle is used for inspection by recording photos. Based on computer vision, deep learning structures stand out for image classification have been an alternative to improve the identification of defects in transmission lines inspections. In this paper, the Pseudo-Prototypical Part Network (Ps-ProtoPNet) model is applied to perform the classification of missing insulators of high voltage transmission lines. To identify the position of the insulators chain and have the focus of the classification on the difference of insulators with failure, the YOLOv5 (n, s, m, l, and x), YOLOv6 (n, t, s, m, and l), YOLOv7 (std and x), and YOLOv8 (n, s, m, l, and x) are compared. The YOLOv8m is defined as the standard architecture for object detection since it has an mAP[0.5] of 0.9950 and mAP[0.5:0.95] of 0.9125. To classify the images, the Ps-ProtoPNet compares its various parts with the prototypes from all classes, and the image is classified based on the closest similarity to the prototypes class. The results show that the Ps-ProtoPNet achieves accuracy values sufficient to be applied in field inspections.},
  archive      = {J_MVA},
  author       = {Singh, Gurmail and Stefenon, Stefano Frizzo and Yow, Kin-Choong},
  doi          = {10.1007/s00138-023-01390-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interpretable visual transmission lines inspections using pseudo-prototypical part network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video2Flink: Real-time video partitioning in apache flink
and the cloud. <em>MVA</em>, <em>34</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01391-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video2Flink is a distributed highly scalable video processing system for bounded (i.e., stored) or unbounded (i.e., continuous) and real-time video streams with the same efficiency. It shows how complicated video processing tasks can be expressed and executed as pipelined data flows on Apache Flink, an open-source stream processing platform. Video2Flink uses Apache Kafka to facilitate the machine-to-machine (m2m) communication between the video production and the video processing system that runs on Apache Flink. Features that make the combination of Apache Kafka and Apache Flink a desirable solution to the problem of video processing are the ease of customization, portability, scalability, and fault tolerance. The application is deployed on a Flink cluster of worker machines that run on Kubernetes in the Google Cloud Platform. The experimental results support our claims of speed showing excellent speed-up results for all tested video resolutions. The highest (i.e., more than seven times) speed-up was observed with the videos of the highest resolutions and in real time.},
  archive      = {J_MVA},
  author       = {Kastrinakis, Dimitrios and Petrakis, Euripides G. M.},
  doi          = {10.1007/s00138-023-01391-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Video2Flink: Real-time video partitioning in apache flink and the cloud},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamCAR-kal: Anti-occlusion tracking algorithm for infrared
ground targets based on SiamCAR and kalman filter. <em>MVA</em>,
<em>34</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01393-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During infrared ground target tracking, to ensure accurate tracking and minimize target loss caused by background occlusion, an anti-occlusion tracking algorithm based on SiamCAR and Kalman filtering, namely SiamCAR-Kal algorithm, is proposed. First, the algorithm uses the response of the classification branch to determine the occlusion state, and then, to address the short-term total occlusion problem, it uses the Kalman filter algorithm to determine the target position based on historical information. To address the problem of low accuracy of long-term total occlusion of the Kalman filter, an extended search strategy is proposed to achieve target recapture. The experimental results show that the proposed algorithm can not only improve the tracking precision, but also effectively solve the problem of target loss caused by occlusion. It is tested on a sequence of infrared ground targets, and compared with the existing typical SiamCAR, the success rate and precision of the proposed algorithm show an improvement of 3.2% and 4.5% under one pass evaluation indexes.},
  archive      = {J_MVA},
  author       = {Fu, Guodong and Zhang, Kai and Yang, Xi and Tian, Xiaoqian and Wang, XiaoTian},
  doi          = {10.1007/s00138-023-01393-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SiamCAR-kal: Anti-occlusion tracking algorithm for infrared ground targets based on SiamCAR and kalman filter},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wide-baseline multi-camera calibration from a room filled
with people. <em>MVA</em>, <em>34</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01395-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a precise 3D reconstruction of an object or person is attempted, one typically starts from a multi-view setup with cameras spread out all around the investigation area. A triangulation of the matching joints is then performed to retrieve the 3D coordinates. However, calibrating such a setup typically requires dedicated equipment and elaborated test procedures. In this paper, we will demonstrate a calibration method based only on the detection of one or more people walking through the field of view. This, in effect, allows the calibration to happen simultaneously with the measurements being taken, which is practical when dealing with uncontrolled environments. We will also show that this calibration procedure is more accurate than a typical incremental calibration procedure using a chessboard. Conceptually, the novelty that we propose is in using semantic information (e.g. the position of the left shoulder) rather than appearance-based information to drive the calibration, as this type of information is less viewpoint dependent. Note that here we use human pose keypoints but for larger outdoor scenes, car keypoints could be used as well.},
  archive      = {J_MVA},
  author       = {Dehaeck, S. and Domken, C. and Bey-Temsamani, A. and Abedrabbo, G.},
  doi          = {10.1007/s00138-023-01395-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Wide-baseline multi-camera calibration from a room filled with people},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human skeleton behavior recognition model based on
multi-object pose estimation with spatiotemporal semantics.
<em>MVA</em>, <em>34</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01396-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object pose estimation in surveillance scenes is challenging and inaccurate due to object motion blur and pose occlusion in video data. Targeting at the temporal dependence and coherence among video frames, this paper reconstructs a multi-object pose estimation model that integrates spatiotemporal semantics for different scales and poses of video multi-objects. The model firstly, with an end-to-end detection framework, detects multiple targets in the video. Secondly, it enhances the positioning of key points of human body using the temporal cues among video frames and designs modular components to enrich the pose information, effectively refining the pose estimation. Finally, the improved human skeleton behavior recognition model based on pose estimation is employed to recognize the classroom behaviors of students oriented to video streams. Comparison with multiple classifiers through experiments reveals that the human skeleton behavior recognition model for multi-object pose estimation combined with spatiotemporal semantics exhibits an effectively improved accuracy.},
  archive      = {J_MVA},
  author       = {Liu, Jiaji and Mu, Xiaofang and Liu, Zhenyu and Li, Hao},
  doi          = {10.1007/s00138-023-01396-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Human skeleton behavior recognition model based on multi-object pose estimation with spatiotemporal semantics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic scene upgrades for trajectory prediction.
<em>MVA</em>, <em>34</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01357-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding pedestrian motion is critical for many real-world applications, e.g., autonomous driving and social robot navigation. It is a challenging problem since autonomous agents require complete understanding of its surroundings including complex spatial, social and scene dependencies. In trajectory prediction research, spatial and social interactions are widely studied while scene understanding has received less attention. In this paper, we study the effectiveness of different encoding mechanisms to understand the influence of the scene on pedestrian trajectories. We leverage a recurrent Variational Autoencoder to encode pedestrian motion history, its social interaction with other pedestrians and semantic scene information. We then evaluate the performance on various public datasets, such as ETH–UCY, Stanford Drone and Grand Central Station. Experimental results show that utilizing a fully segmented map, for explicit scene semantics, out performs other variants of scene representations (semantic and CNN embedding) for trajectory prediction tasks.},
  archive      = {J_MVA},
  author       = {Syed, Arsal and Morris, Brendan Tran},
  doi          = {10.1007/s00138-022-01357-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semantic scene upgrades for trajectory prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving visual odometry pipeline with feedback from
forward and backward motion estimates. <em>MVA</em>, <em>34</em>(2),
1–11. (<a href="https://doi.org/10.1007/s00138-023-01370-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating motion from visual cameras has become a promising art in the area of autonomous navigation and constant efforts are being made toward improving the accuracy of these estimates. In this paper, an improvement in the visual odometry algorithm is proposed that takes cues from both the forward and backward motion estimates. An error is formulated based on the consistency which measures the difference between the forward and backward motion. This error is used in a feedback mechanism to improve the triangulated 3D point estimates, thereby improving the pose estimate. Additionally, a novel means to incorporate information from multiple stereo camera setups has been devised to improve the pose estimate. The proposed scheme of joint forward–backward VO with multiple cameras and feedback mechanism (JFBVO–FM) is validated on two publicly available datasets having different environmental conditions and camera motion, that is, KITTI and EuRoC Micro Aerial Vehicle (MAV) datasets. The results are analyzed both qualitatively and quantitatively, and the proposed scheme is found to perform better as compared to the state-of-the-art methods in most of the sequences.},
  archive      = {J_MVA},
  author       = {Sardana, Raghav and Karar, Vinod and Poddar, Shashi},
  doi          = {10.1007/s00138-023-01370-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Improving visual odometry pipeline with feedback from forward and backward motion estimates},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Randomized nonlinear two-dimensional principal component
analysis network for object recognition. <em>MVA</em>, <em>34</em>(2),
1–9. (<a href="https://doi.org/10.1007/s00138-023-01371-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to capture nonlinear structures within data and more representational image features, this paper investigates a multi-stage convolutional neural network with predefined filters. The first two stages are the cascaded blocks consisted of random Fourier mapping, two-dimensional principal component analysis and activation operation. Among that, the approximate method based on Gaussian kernel is used to map the original image to random feature space. Subsequently, convolution filters are learned by two-dimensional principal component analysis. Next, the batch normalization and Gaussian linear error unit activation operation are followed. Afterward, the maximum pooling is utilized to further reduce dimensions of intermediate features. With binary hashing and encoding, the statistical histogram will be obtained and served as the higher-order feature of original image. Experiments have been carried out around the task of object recognition, and quantitative results demonstrate the proposed network has significantly advantageous both in terms of accuracy and computational time compared to the existed algorithms.},
  archive      = {J_MVA},
  author       = {Sun, Zhijian and Shao, Zhuhong and Shang, Yuanyuan and Li, Bicao and Wu, Jiasong and Bi, Hui},
  doi          = {10.1007/s00138-023-01371-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Randomized nonlinear two-dimensional principal component analysis network for object recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual context network for real-time semantic segmentation.
<em>MVA</em>, <em>34</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01373-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic segmentation is a challenging task as both segmentation accuracy and inference speed need to be considered at the same time. In this paper, a Dual Context Network (DCNet) is presented to address this challenge. It contains two independent sub-networks: Region Context Network and Pixel Context Network. Region Context Network is main network with low-resolution input and features re-weighting module to achieve sufficient receptive field. Meanwhile, Pixel Context Network with location attention module is to capture the location dependencies of each pixel for assisting the main network to recover spatial detail. A contextual feature fusion is introduced to combine output features of these two sub-networks. The experiments show that DCNet can achieve high-quality segmentation while keeping a high speed. Specifically, for Cityscapes test dataset, it can achieve 76.1% Mean IOU with the speed of 82 FPS on a single GTX 2080Ti GPU when using ResNet50 as backbone and 71.2% Mean IOU with the speed of 142 FPS when using ResNet18 as backbone.},
  archive      = {J_MVA},
  author       = {Yin, Hong and Xie, Wenbin and Zhang, Jingjing and Zhang, Yuanfa and Zhu, Weixing and Gao, Jie and Shao, Yan and Li, Yajun},
  doi          = {10.1007/s00138-023-01373-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dual context network for real-time semantic segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-based hybrid connectionist models for morphologic
reasoning. <em>MVA</em>, <em>34</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01374-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture morphology perception is essential feedback for robots in tactile-related tasks (such as robot’s electrical palpation, manipulation, or recognition of objects in complex, wet, and dark work conditions). However, it is tough to quantify morphologic information and define morphologic feature. For this reason, it is difficult to use prior tactile experience in detection, which results in large dataset requirements, time costs, and frequent model retraining for new targets. This study introduced a hybrid connectionist symbolic model (HCSM) that integrates prior symbolic human experience and the end-to-end neural network. HCSM requires smaller datasets owing to using a symbolic model based on human knowledge. Moreover, HCSM improves the transferability of detection and interpretation of recognition results. The neural network has the advantage of easy training. The HCSM combines the merits of both connectionist and symbolic models. We have implemented tactile morphologic detection of basic geometry textures (such as bulges and ridges) using the HCSM method. The trained model can be transferred to detect gaps and holes by manual adjustment of the symbolic definition, without model retraining. Similarly, other new morphology can be detected by only modifying the symbolic model. We have compared the recognition performance of the proposed model with that of the traditional classification models, such as LeNet, VGG16, ResNet, XGBoost, and DenseNet. The proposed HCSM model has achieved the best recognition accuracy. Besides, compared with classic classification models, our method is less likely to misrecognize one target as a completely different counterpart, providing a guarantee for generalization boundaries of recognition to a certain degree.},
  archive      = {J_MVA},
  author       = {He, Kai and Wang, Wenxue and Li, Gang and Yu, Peng and Tang, Fengzhen and Xi, Ning and Liu, Lianqing},
  doi          = {10.1007/s00138-023-01374-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Knowledge-based hybrid connectionist models for morphologic reasoning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Self-attention network for few-shot learning based on
nearest-neighbor algorithm. <em>MVA</em>, <em>34</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01375-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is a challenging task because it focuses on classifying new object categories given only limited labeled samples and often results in poor generalization. Most of existing methods based on metric learning are not very simple for the networks. Moreover, these methods cannot solve low-data problem and are unable to extract discriminative features. This paper presents a simple, effective and general framework for few-shot image classification. Specifically, we first exploit data augmentation technique to alleviate overfitting problem. We propose a self-attention position attention module (PAM) which is utilized to extract discriminative features for constructing a few-shot representation model. Furthermore, we design a novel nearest-neighbor learner with feature transformation to obtain the appealing accuracy in few-shot learning (FSL). Our network is comprised of backbone and attention module and trained from scratch in an end-to-end manner. The backbone module is to extract multi-level features. The self-attention PAM is used to discover non-local information and allow long-range dependency. Excellent performance on benchmark demonstrates that our work provides a unified and effective approach for few-shot image classification.},
  archive      = {J_MVA},
  author       = {Wang, Guangpeng and Wang, Yongxiong},
  doi          = {10.1007/s00138-023-01375-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Self-attention network for few-shot learning based on nearest-neighbor algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ubiquitous vision of transformers for person
re-identification. <em>MVA</em>, <em>34</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01376-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) is a computer vision-based autonomous process that re-identifies a query person from a set of gallery images captured by multiple non-overlapping cameras of a surveillance network. Since long, the convolution neural networks (CNNs) make strong baselines to solve vision problems. A limitation of CNN is the local neighborhood-dependent learning at each layer of the network, which prevents the learning of dependencies among distant regions of an image. However, these dependencies among distant parts of a person image play an imperative role in person ReID, especially in the cases of diverse inter-class and intra-class variations. In this study, we address this limitation of CNNs and provide a ubiquitous self-attention-based deep architecture for person re-identification, to learn the associations between all parts of an image, whether these are close by or far away. Secondly, in order to emphasize learning less attentive regions of the image, we expanded our work by providing an attention-based batch layoff module in our baseline model. The proposed unification of learning self-attentions among all parts of a person’s image while keeping in line with the complete context resulted in the novel transformer-based ReID architecture. We evaluated the proposed ReID model on three public ReID benchmarks Market1501, DukeMTMC-ReID and MSMT-17 and computed the cumulative matching characteristics curve and mean average precision. The proposed ReID model consistently outperformed all CNN-based vanilla ReID models for all standard performance measures of person re-identification. The implementation and trained models are made publicly available at https://git.io/JYRE3 .},
  archive      = {J_MVA},
  author       = {Perwaiz, N. and Shahzad, M. and Fraz, M. M.},
  doi          = {10.1007/s00138-023-01376-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ubiquitous vision of transformers for person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Actions as points: A simple and efficient detector for
skeleton-based temporal action detection. <em>MVA</em>, <em>34</em>(2),
1–14. (<a href="https://doi.org/10.1007/s00138-023-01377-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection, aiming to determine the fragment and category of a human action simultaneously from continuous data stream, is still a challenge issue in the field of human–robot interaction, somatosensory game and security monitoring. In this paper, we present a novel one-stage skeleton-based TAD method, Action-CenterNet(ACNet) with a simple anchor-free and fully convolutional encoder-decoder pipeline. Our approach encodes skeleton position and motion data sequence from multiple persons into multi-channel skeleton images which are subsequently preprocessed by view invariant transform and translation-scale invariant. ACNet models each action fragment as a center point along the time dimension and generates a keypoint heatmap to locate and classify action fragments. To ensure the accurate temporal coordinates, the discretization error caused by the output stride of network is also learned. Compared with two-stage methods, ACNet is end-to-end differential and flexible. ACNet is also an anchor-free method avoiding the drawbacks of anchor boxes used in anchor-based TAD methods. Experimental results on PKU-MMD dataset, NTU RGB-D dataset and HITvs dataset reveal the excellent performance of our approach.},
  archive      = {J_MVA},
  author       = {Yang, Jianhua and Wang, Ke and Li, Ruifeng},
  doi          = {10.1007/s00138-023-01377-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Actions as points: A simple and efficient detector for skeleton-based temporal action detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusing bilinear multi-channel gated vector for fine-grained
classification. <em>MVA</em>, <em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01378-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification aims to identify images belonging to multiple subcategories within the same category. Most existing methods use a single network to extract image features or learn fine-grained features by localizing and scaling key regions. Due to the limited number of components, this may miss valuable clues or cause performance degradation. This paper proposes an efficient approach to address this problem. First, we propose to learn as many global features as possible in images via a dual-baseline network. Second, considering the importance of the attention mechanism for image classification, we exploit the gated interaction of channels between global feature maps to generate attention to discover key discriminant regions of images. In the same way, the interactive channel attention and position attention of the global feature map are used to focus on the key discriminant regions of the image. In the above attention, interactive gated attention is generated by the gating vector mapped by the multi-layer perceptron MLP. Similarly, for channel attention and position attention, we perform attention based on global feature semantic information enhancement. The proposed model performs well on three datasets: CUB-200-2011, Stanford Cars, and FGVC aircraft.},
  archive      = {J_MVA},
  author       = {Zhu, Qiangxi and Kuang, Wenlan and Li, Zhixin},
  doi          = {10.1007/s00138-023-01378-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fusing bilinear multi-channel gated vector for fine-grained classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative feature learning through feature distance
loss. <em>MVA</em>, <em>34</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-023-01379-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles of convolutional neural networks have shown remarkable results in learning discriminative semantic features for image classification tasks. However, the models in the ensemble often concentrate on similar regions in images. This work proposes a novel method that forces a set of base models to learn different features for a classification task. These models are combined in an ensemble to make a collective classification. The key finding is that by forcing the models to concentrate on different features, the classification accuracy is increased. To learn different feature concepts, a so-called feature distance loss is implemented on the feature maps. The experiments on benchmark convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets (Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the proposed feature loss. The proposed method outperforms classical ensemble versions of the base models. The Class Activation Maps explicitly prove the ability to learn different feature concepts. The code is available at: https://github.com/2Obe/Feature-Distance-Loss.git .},
  archive      = {J_MVA},
  author       = {Schlagenhauf, Tobias and Lin, Yiwen and Noack, Benjamin},
  doi          = {10.1007/s00138-023-01379-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Discriminative feature learning through feature distance loss},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PM-MVS: PatchMatch multi-view stereo. <em>MVA</em>,
<em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01380-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PatchMatch Stereo is a method for generating a depth map from stereo images by repeating spatial propagation and view propagation. The concept of PatchMatch Stereo can be easily extended to Multi-View Stereo (MVS). In this paper, we present PatchMatch Multi-View Stereo (PM-MVS), which is a highly accurate 3D reconstruction method that can be used in various environments. Three techniques are introduced to PM-MVS: (i) matching score evaluation, (ii) viewpoint selection, and (iii) outlier filtering. The combination of normalized cross-correlation with bilateral weights and geometric consistency between viewpoints is used to improve the estimation accuracy of depth and normal maps at object boundaries and poor-texture regions. For each pixel, viewpoints used for stereo matching are carefully selected in order to improve robustness against disturbances such as occlusion, noise, blur, and distortion. Outliers are removed from reconstructed 3D point clouds by a weighted median filter and consistency-based filters assuming multi-view geometry. Through a set of experiments using public multi-view image datasets, we demonstrate that the proposed method exhibits efficient performance compared with conventional methods.},
  archive      = {J_MVA},
  author       = {Ito, Koichi and Ito, Takafumi and Aoki, Takafumi},
  doi          = {10.1007/s00138-023-01380-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {PM-MVS: PatchMatch multi-view stereo},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual object tracking by using ranking loss and
spatial–temporal features. <em>MVA</em>, <em>34</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-023-01381-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel two-stream deep neural network tracker for robust object tracking. In the proposed network, we use both spatial and temporal features and employ a novel loss function called ranking loss. The class confidence scores coming from the two-stream (spatial and temporal) networks are fused at the end for final decision. Using ranking loss in the proposed tracker enforces the networks to learn giving higher scores to the candidate regions that frame the target object better. As a result, the tracker returns more precise bounding boxes framing the target object, and the risk of tracking error accumulation and drifts are largely mitigated when the proposed network architecture is used with a simple yet effective model update rule. We conducted extensive experiments on six different benchmarks, including OTB-2015, VOT-2017, TC-128, DTB70, NfS and UAV123. Our proposed tracker achieves the state-of-the-art results on the most of the tested challenging tracking datasets. Especially, our results on the OTB-2015, DTB70, NfS and TC-128 datasets are very promising. The source code and trained models are available at https://github.com/Hasan4825/RankingT .},
  archive      = {J_MVA},
  author       = {Saribas, Hasan and Cevikalp, Hakan and Kahvecioglu, Sinem},
  doi          = {10.1007/s00138-023-01381-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Visual object tracking by using ranking loss and spatial–temporal features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Centroid-based graph matching networks for planar object
tracking. <em>MVA</em>, <em>34</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-023-01382-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, keypoint-based methods have received more attention on planar object tracking due to their abilities to deal with partial noises, such as occlusion and out-of-view. However, robust tracking is still a tricky problem in the case of fast movement, large transformation and motion blur. The key reason is that there are not enough matching inliers to reconstruct the homography in the presence of such perturbations. To this end, we propose a novel centroid-based graph matching networks (CGN), which consists of two components: centroid localization network (CLN) and graph matching network (GMN). In detail, the CLN reduces the search range of the tracker from the entire image to the target region by locating the centroid of the target. The CLN gives the initial guess of the position, which guarantees the proportion of inliers matching the template. Then, the keypoints in the template and the target region are modeled as two graphs connected by cross-edges, and their correspondences are established by the GMN. The GMN overcomes the impact of large transformation by exploiting the stability of the graph structure. Finally, the transformation from the template to the current frame is estimated from the matched keypoint pairs by the RANSAC algorithm. In addition, the number of labeled points in previous datasets for training matching models is too small to cope with complex transformations, so we synthesize a large-scale dataset with labels to train the GMN. Experimental results on POT-210, POIC and TMT datasets show that our proposed method outperforms the state-of-the-art baseline methods in general, with significant improvements on fast movement and motion blur.},
  archive      = {J_MVA},
  author       = {Li, Kunpeng and Liu, He and Wang, Tao},
  doi          = {10.1007/s00138-023-01382-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Centroid-based graph matching networks for planar object tracking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving knowledge distillation via pseudo-multi-teacher
network. <em>MVA</em>, <em>34</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s00138-023-01383-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing knowledge distillation methods usually directly push the student model to imitate the features or probabilities of the teacher model. However, the knowledge capacity of teachers limits students to learn undiscovered knowledge. To address this issue, we propose a pseudo-multi-teacher knowledge distillation method to augment the learning of undiscovered knowledge. Specifically, we propose a well-designed auxiliary classifier to capture semantic information in cross-layer that enables our network to obtain more abundant supervised information. Besides, we propose an ensemble module to combine the feature maps of each sub-network, which generates a more significant ensemble of features to guide the network. Furthermore, the auxiliary classifier and ensemble module are discarded after training, and thus there are no additional parameters introduced to the final model. Comprehensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_MVA},
  author       = {Li, Shunhang and Shao, Mingwen and Guo, Zihao and Zhuang, Xinkai},
  doi          = {10.1007/s00138-023-01383-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Improving knowledge distillation via pseudo-multi-teacher network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature pyramid with attention fusion for edge discontinuity
classification. <em>MVA</em>, <em>34</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s00138-023-01385-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection algorithms are beneficial to the implementation of upstream tasks. The algorithms used to treat all edges equally, but edges in edge detection can be classified into four types according to the discontinuity as reflectance, illumination, normal and depth. In order to complete and improve the unified classification detection effect of edge discontinuity, we propose a robust convolutional neural network, called FPAFNet, which is the first network to use a feature pyramid structure to uniformly detect these four types of edges. Since the gap between different edge categories is very small, the design of the network should not only consider the difference between categories, but also take account of the connections between them. The proposed method integrates contextual information through a feature pyramid with an attention fusion mechanism to find associations between categories. We improve the attention module to avoid the too-close connections between categories to distinguish them. Compared with state-of-the-art methods, our method achieves the best results on average and achieves ODS of 0.511 and 0.49 in the normal edges and reflectance edges, respectively, which greatly outperforms other methods.},
  archive      = {J_MVA},
  author       = {Sun, Mingsi and Zhao, Hongwei and Liu, Pingping and Zhou, Jianhang},
  doi          = {10.1007/s00138-023-01385-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature pyramid with attention fusion for edge discontinuity classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Self-attention network for few-shot learning
based on nearest-neighbor algorithm. <em>MVA</em>, <em>34</em>(2), 1.
(<a href="https://doi.org/10.1007/s00138-023-01387-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Wang, Guangpeng and Wang, Yongxiong},
  doi          = {10.1007/s00138-023-01387-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction to: Self-attention network for few-shot learning based on nearest-neighbor algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion-region annotation for complex videos via label
propagation across occluders. <em>MVA</em>, <em>34</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s00138-022-01348-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion cue is pivotal in moving object analysis, which is the root for motion segmentation and detection. These preprocessing tasks are building blocks for several applications such as recognition, matching and estimation. To devise a robust algorithm for motion analysis, it is imperative to have a comprehensive dataset to evaluate an algorithm’s performance. The main limitation in making these kind of datasets is the creation of ground-truth annotation of motion, as each moving object might span over multiple frames with changes in size, illumination and angle of view. Besides the optical changes, the object can undergo occlusion by static or moving occluders. The challenge increases when the video is captured by a moving camera. In this paper, we tackle the task of providing ground-truth annotation on motion regions in videos captured from a moving camera. With minimal manual annotation of an object mask, we are able to propagate the label mask in all the frames. Object label correction based on static and moving occluder is also performed by applying occluder mask tracking for a given depth ordering. A motion annotation dataset is also proposed to evaluate algorithm performance. The results show that our cascaded-naive approach provides successful results. All the resources of the annotation tool are publicly available at http://dixie.udg.edu/anntool/ .},
  archive      = {J_MVA},
  author       = {Mahmood, Muhammad Habib and Diéz, Yago and Oliver, Arnau and Salvi, Joaquim and Lladó, Xavier},
  doi          = {10.1007/s00138-022-01348-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Motion-region annotation for complex videos via label propagation across occluders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus similarity learning based on tensor nuclear norm.
<em>MVA</em>, <em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01350-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering approaches based on similarity learning have achieved good results, but they still have the following problems: (1) these approaches generally learn similar expressions on the original data, thereby disregarding the nonlinear structure of the data; (2) these methods generally do not consider the consistency and high-order relevance among multi-view data; and (3) these approaches generally use the learned similarity matrix for clustering, usually not achieving the optimal effect. To resolve the above issues, we present a new approach referred to as consensus similarity learning based on tensor nuclear norm. First, to address the first problem, we map the data of each view to the Hilbert space to discover the nonlinear structure of the data. Second, to address the second problem, we introduce the tensor nuclear norm to constrain the regularization term, and then, the consistency and high-order relevance among multi-view data can be captured. Third, to address the third problem, i.e., to obtain a better clustering effect, we learn a clustering indicator matrix in the kernel space instead of a similarity matrix for clustering by using a consensus representation term. Last, we incorporate these three steps into a unified framework and design the corresponding goal function. In addition, experimental outcomes on some datasets show that our algorithm is superior to certain representative approaches.},
  archive      = {J_MVA},
  author       = {Tang, Rong and Lu, Gui-Fu},
  doi          = {10.1007/s00138-022-01350-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Consensus similarity learning based on tensor nuclear norm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AccNet: Occluded scene text enhancing network with accretion
blocks. <em>MVA</em>, <em>34</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01351-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text with occlusions is common in the real world, and occluded text recognition is important for many machine vision applications. However, corresponding techniques are not well explored as public datasets cannot represent the situation well, and methods designed for occluded text are still scarce. In this work, we discuss different kinds of occlusions and propose an occluded scene text enhancing network to improve recognition performance. The network is based on generative adversarial networks, and we design accretion blocks to help the network generate the occluded image regions. The model is independent of the recognition networks, so it can be readily used in different frameworks and can be easily trained without the annotations of text content. We also refine the training objective to improve the framework. Experiments on several public benchmarks demonstrate that the proposed method effectively enhances occluded text images, improving recognition accuracy by over 10% on several state-of-the-art frameworks. Meanwhile, the network has no severe impact on the text images without occlusions.},
  archive      = {J_MVA},
  author       = {Gong, Yanxiang and Zhang, Zhiqiang and Duan, Guozhen and Ma, Zheng and Xie, Mei},
  doi          = {10.1007/s00138-022-01351-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {AccNet: Occluded scene text enhancing network with accretion blocks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human pose estimation based on lightweight basicblock.
<em>MVA</em>, <em>34</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s00138-022-01352-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation based on deep learning have attracted increasing attention in the past few years and have shown superior performance on various datasets. Many researchers have increased the number of network layers to improve the accuracy of the model. However, with the deepening of the number of network layers, the parameters and computation of the model are also increasing, which makes the model unable to be deployed on edge devices and mobile terminals with limited computing power, and also makes many intelligent terminals limited in volume, power consumption and storage. Inspired by the lightweight method, we propose a human pose estimation model based on the lightweight network to solve those problems, which designs the lightweight basic block module by using the deep separable convolution and the reverse bottleneck layer to accelerate the network calculation and reduce the parameters of the overall network model. Experiments on COCO dataset and MPII dataset prove that this lightweight basicblock module can effectively reduce the amount of parameters and computation of human pose estimation model.},
  archive      = {J_MVA},
  author       = {Li, Yanping and Liu, Ruyi and Wang, Xiangyang and Wang, Rui},
  doi          = {10.1007/s00138-022-01352-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Human pose estimation based on lightweight basicblock},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded attention-guided multi-granularity feature learning
for person re-identification. <em>MVA</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01353-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism has been extensively employed in the task of person re-identification, as it helps to extract much more discriminative feature representations. However, most of existing works either incorporate a single-scale attention module, or the embedded attentions work independently. Though promising results are achieved, they may fail to mine different subtle visual clues. To mitigate this issue, a novel framework called cascaded attention network (CANet) is proposed, which allows to mine diverse clues and integrate them into final multi-granularity features by a cascaded manner. Specifically, we design a novel hybrid pooling attention module (HPAM) and plug it into backbone network at different stages. To make them work collaboratively, an inter-attention regularization is applied, such that they can localize complementary salient features. Then, CANet extracts global and local features from a part-based pyramidal architecture. For better feature robustness, supervision is applied to not only the pyramidal branches, but also those intermediate attention modules. Furthermore, within each supervision branch, hybrid pooling with two different strides is executed to enhance feature representation capabilities. Extensive experiments with ablation analysis demonstrate the effectiveness of the proposed method, and state-of-the-art results are achieved on three public benchmark datasets, including Market-1501, CUHK03, and DukeMTMC-ReID.},
  archive      = {J_MVA},
  author       = {Dong, Husheng and Yang, Yuanfeng and Sun, Xun and Zhang, Liang and Fang, Ligang},
  doi          = {10.1007/s00138-022-01353-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cascaded attention-guided multi-granularity feature learning for person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamMMF: Multi-modal multi-level fusion object tracking
based on siamese networks. <em>MVA</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01354-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature-level or pixel-level fusion is a common technique for integrating different modes of information in RGB-T object tracking. A good fusion method between modalities can significantly improve the tracking performance. In this paper, a multi-modal and multi-level fusion model based on Siamese network (SiamMMF) is proposed. SiamMMF consists of two main subnetworks: a pixel-level fusion network and a feature-level fusion network. The pixel-level fusion network fuses the infrared images and the visible light images by taking the maximum values of the pixels corresponding to the different images, and the combined images are used to replace the visible light images. The infrared images and the visible light images are each input to the backbone with dual-stream structure for processing. After the extraction of deep features, the visible and infrared features from the two branches are cross-correlated to obtain a fusion result that is sent to the tracking head for tracking. Based on numerous experiments, it was found that the best tracking effect is obtained when the weighting ratio between the visible and infrared modality is set to 6:4. Nineteen pairs of RGB-T video sequences with different attributes were used to test our model and compared it with 15 trackers. For the two evaluation criteria, success rate and precision rate, our network achieved the best results.},
  archive      = {J_MVA},
  author       = {Yang, Zhen and Huang, Peng and He, Dunyun and Cai, Zhongwang and Yin, Zhijian},
  doi          = {10.1007/s00138-022-01354-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SiamMMF: Multi-modal multi-level fusion object tracking based on siamese networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method for high dynamic range 3D color modeling of objects
through a color camera. <em>MVA</em>, <em>34</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01355-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel method for enhancing the dynamic range of structured-light cameras to solve the problem of highlight that occurs when 3D modeling highly reflective objects using the structured-light method. Our method uses the differences in quantum efficiency between R, G, and B pixels in the color image sensor of a monochromatic laser to obtain structured-light images of an object under test with different luminance values. Our approach sacrifices the resolution of the image sensor to increase the dynamic range of the vision system. Additionally, to enhance our system, we leverage the backgrounds of structured-light stripe pattern images to restore the color information of measured objects, whereas the background is often removed as noise in other 3D reconstruction systems. This reduces the number of cameras required for 3D reconstruction and the matching error between point clouds and color data. We modeled both highly reflective and non-highly reflective objects and achieved satisfactory results.},
  archive      = {J_MVA},
  author       = {Zhang, Yanan and Qiao, Dayong and Xia, Changfeng and Yang, Di and Fang, Shilei},
  doi          = {10.1007/s00138-022-01355-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A method for high dynamic range 3D color modeling of objects through a color camera},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time pedestrian pose estimation, tracking and
localization for social distancing. <em>MVA</em>, <em>34</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s00138-022-01356-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The corona virus pandemic has introduced limitations which were previously not a cause for concern. Chief among them are wearing face masks in public and constraints on the physical distance between people as an effective measure to reduce the virus spread. Visual surveillance systems, which are common in urban environments and initially commissioned for security surveillance, can be re-purposed to help limit the spread of COVID-19 and prevent future pandemics. In this work, we propose a novel integration technique for real-time pose estimation and multiple human tracking in a pedestrian setting, primarily for social distancing, using CCTV camera footage. Our technique promises a sizeable increase in processing speed and improved detection in very low-resolution scenarios. Using existing surveillance systems, pedestrian pose estimation, tracking and localization for social distancing (PETL4SD) is proposed for measuring social distancing, which combines the output of multiple neural networks aided with fundamental 2D/3D vision techniques. We leverage state-of-the-art object and pose estimation algorithms, combining their strengths, for increase in speed and improvement in detections. These detections are then tracked using a bespoke version of the FASTMOT algorithm. Temporal and analogous estimation techniques are used to deal with occlusions when estimating posture. Projective geometry along with the aforementioned posture tracking is then used to localize the pedestrians. Inter-personal distances are calculated and locally inspected to detect possible violations of the social distancing rules. Furthermore, a “smart violations detector” is employed which estimates if people are together based on their current actions and eliminates false social distancing violations within groups. Finally, distances are intuitively visualized with the right perspective. All implementation is in real time and is performed on Python. Experimental results are provided to validate our proposed method quantitatively and qualitatively on public domain datasets using only a single CCTV camera feed as input. Our results show our technique to outperform the baseline in speed and accuracy in low-resolution scenarios. The code of this work will be made publicly available on GitHub at https://github.com/bilalze/PETL4SD .},
  archive      = {J_MVA},
  author       = {Abdulrahman, Bilal and Zhu, Zhigang},
  doi          = {10.1007/s00138-022-01356-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real-time pedestrian pose estimation, tracking and localization for social distancing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic cables segmentation from a substation device based
on 3D point cloud. <em>MVA</em>, <em>34</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01358-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The point cloud segmentation of a substation device attached with cables is the basis of substation identification and reconstruction. However, it is limited by a number of factors including the huge amount of point cloud data of a substation device, irregular shape, unclear feature distinction due to the auxiliary point cloud data attached to the main body of a device. Therefore, the segmentation efficiency of a substation device is very low. In order to improve the accuracy and efficiency of the point cloud segmentation, this paper proposes a method to segment the attached cables point cloud of a substation device by using the shape feature of point cloud. Firstly, according to the spatial position of the point cloud of a substation device, octree is used to conduct voxelization of the point cloud, and the point cloud resampling is operated according to point cloud density of each voxel, so as to reduce original point cloud data and improve computing efficiency. Then Mean Shift algorithm is used to locate the center axis of the point cloud, and cylinder growth method is used to initially segment cables data and locate the end of each cable. Finally, points of the end are used as seed points to carry out a region growth based on shape feature of the point cloud to realize effective segmentation of cables data. In the experiment, 303 sets of point cloud of devices are selected, including circuit breaker, voltage transformer, transformer, etc. The final result shows that the successful segmentation rate of this method reaches 95.34%, which effectively proves the feasibility of this method.},
  archive      = {J_MVA},
  author       = {Yuan, Qianjin and Chang, Jing and Luo, Yong and Ma, Tianlei and Wang, Dongshu},
  doi          = {10.1007/s00138-022-01358-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic cables segmentation from a substation device based on 3D point cloud},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fill in the blank for fashion complementary outfit product
retrieval: VISUM summer school competition. <em>MVA</em>,
<em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01359-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year, the VISion Understanding and Machine intelligence (VISUM) summer school runs a competition where participants can learn and share knowledge about Computer Vision and Machine Learning in a vibrant environment. 2021 VISUM’s focused on applying those methodologies in fashion. Recently, there has been an increase of interest within the scientific community in applying computer vision methodologies to the fashion domain. That is highly motivated by fashion being one of the world’s largest industries presenting a rapid development in e-commerce mainly since the COVID-19 pandemic. Computer Vision for Fashion enables a wide range of innovations, from personalized recommendations to outfit matching. The competition enabled students to apply the knowledge acquired in the summer school to a real-world problem. The ambition was to foster research and development in fashion outfit complementary product retrieval by leveraging vast visual and textual data with domain knowledge. For this, a new fashion outfit dataset (acquired and curated by FARFETCH) for research and benchmark purposes is introduced. Additionally, a competitive baseline with an original negative sampling process for triplet mining was implemented and served as a starting point for participants. The top 3 performing methods are described in this paper since they constitute the reference state-of-the-art for this particular problem. To our knowledge, this is the first challenge in fashion outfit complementary product retrieval. Moreover, this joint project between academia and industry brings several relevant contributions to disseminating science and technology, promoting economic and social development, and helping to connect early-career researchers to real-world industry challenges.},
  archive      = {J_MVA},
  author       = {Castro, Eduardo and Ferreira, Pedro M. and Rebelo, Ana and Rio-Torto, Isabel and Capozzi, Leonardo and Ferreira, Mafalda Falcão and Gonçalves, Tiago and Albuquerque, Tomé and Silva, Wilson and Afonso, Carolina and Gamelas Sousa, Ricardo and Cimarelli, Claudio and Daoudi, Nadia and Moreira, Gabriel and Yang, Hsiu-yu and Hrga, Ingrid and Ahmad, Javed and Keswani, Monish and Beco, Sofia},
  doi          = {10.1007/s00138-022-01359-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fill in the blank for fashion complementary outfit product retrieval: VISUM summer school competition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAU-net: U-net network based on residual multi-scale fusion
and attention skip layer for overall spine segmentation. <em>MVA</em>,
<em>34</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01360-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spine segmentation is necessary for the clinical quantitative analysis of computed tomography (CT) images and plays an important role in the early diagnosis and treatment of spine diseases. However, because of the different fields of view of sagittal CT, these images show different shapes and sizes of vertebrae, unclear vertebral boundaries, and different image scales which greatly complicate the segmentation of the spine. To solve this problem, we propose a new deep learning method for segmenting the spine. For this algorithm, we first proposed a residual feature pyramids block for capturing and fusing multi-scale information. For fusing shallow and deep features, we then propose an attention skip layer structure for suppressing the reuse of redundant information. Finally, we use a joint loss function to optimize the segmentation results and achieve the effect of clear segmentation edges. Through the combination of these three techniques, our method achieves efficient and accurate spinal segmentation. The experimental results show that our model has a good performance in spine segmentation. In particular, our achieve the Dice of 0.8973, the Hausdorff distance of 77.6277 on the VerSe 2019 dataset and the Dice of 0.8626, the Hausdorff distance of 82.6170 on the VerSe 2020 dataset.},
  archive      = {J_MVA},
  author       = {Yang, Zhaomin and Wang, Qi and Zeng, Jianchao and Qin, Pinle and Chai, Rui and Sun, Dong},
  doi          = {10.1007/s00138-022-01360-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RAU-net: U-net network based on residual multi-scale fusion and attention skip layer for overall spine segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating comprehensive scene graphs with integrated
multiple attribute detection. <em>MVA</em>, <em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01361-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graphs present the semantic highlights of the underlying image in directed graph form. Their automated generation is often restricted to detecting multiple relations for objects. The diverse attributes of the objects are neglected in the process. We propose a Multiple Attribute Detector that can capture structured attribute information of an object, i.e. attribute type, value in the form of a triplet. The module is capable of generating multiple such triplets for every detected object in the scene. It can be integrated with existing scene graph generation frameworks without altering relation detection mechanism to yield comprehensive scene graphs. We have also created new datasets for this purpose that include attribute type-value data per object.},
  archive      = {J_MVA},
  author       = {Patil, Charulata and Abhyankar, Aditya},
  doi          = {10.1007/s00138-022-01361-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Generating comprehensive scene graphs with integrated multiple attribute detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating quality grasp rectangle using Pix2Pix GAN for
intelligent robot grasping. <em>MVA</em>, <em>34</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01362-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent robot grasping is a very challenging task due to its inherent complexity and non-availability of sufficient labeled data. Since making suitable labeled data available for effective training for any deep learning-based model including deep reinforcement learning is so crucial for successful grasp learning, in this paper we propose to solve the problem of generating grasping poses/rectangles using a Pix2Pix Generative Adversarial Network (Pix2Pix GAN), which takes an image of an object as input and produces the grasping rectangle tagged with the object as output. Here, we have proposed an end-to-end grasping rectangle generating methodology and embedding it to an appropriate place of an object to be grasped. We have developed two modules to obtain an optimal grasping rectangle. With the help of the first module, the pose (position and orientation) of the generated grasping rectangle is extracted from the output of Pix2Pix GAN, and then, the extracted grasp pose is translated to the centroid of the object, since here we hypothesize that like the human way of grasping of regular shaped objects, the center of mass and centroids are the best places for stable grasping. For other irregular shaped objects, we allow the generated grasping rectangles as it is to be fed to the robot for grasp execution. The accuracy has significantly improved for generating the grasping rectangle with limited number of Cornell Grasping Dataset augmented by our proposed approach to the extent of 87.79%. Rigorous experiments with the Anukul/Baxter robot, which has 7 degrees of freedom, causing redundancy have been performed. At the grasp execution level, we propose to solve the inverse kinematics problems for such robots using Numerical Inverse-Pose solution together with Resolve-Rate control which proves to be more computationally efficient due to the common sharing of the Jacobian matrix. Experiments show that our proposed generative model-based approach gives the promising results in terms of executing successful grasps for seen as well as unseen objects ( refer to demonstrative video ).},
  archive      = {J_MVA},
  author       = {Kushwaha, Vandana and Shukla, Priya and Nandi, G. C.},
  doi          = {10.1007/s00138-022-01362-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Generating quality grasp rectangle using Pix2Pix GAN for intelligent robot grasping},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Superpixel-based foreground-preserving image stitching.
<em>MVA</em>, <em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01363-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching aims to stitch multiple images with overlapping areas into a high-resolution image with natural appearance, no ghosting and no seam, so as to obtain a wide field of vision. And the stitching is required to be as fast as possible. In this article, we discuss the loss of foreground object caused by the seam-cut method for image stitching. Moreover, we propose an improved seam-cut method based on superpixel to solve this problem. The proposed method uses the matching information of feature points and superpixel segmentation to divide the scene into foreground and background. By adjusting the penalty value of foreground superpixels in the superpixel-based seam-cut method, the foreground object can avoid the cutting of optimal seam. The experimental results demonstrate that our method will not cause the ghosting of foreground object or the loss of foreground object in the stitching result in the scene with large parallax. In this method, the foreground object will be preserved completely.},
  archive      = {J_MVA},
  author       = {Miao, Xinpeng and Qu, Tao and Chen, Xi and He, Chu},
  doi          = {10.1007/s00138-022-01363-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Superpixel-based foreground-preserving image stitching},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved deep depth estimation for environments with sparse
visual cues. <em>MVA</em>, <em>34</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01364-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep learning-based depth estimation models that learn scene structure self-supervised from monocular video base their estimation on visual cues such as vanishing points. In the established depth estimation benchmarks depicting, for example, street navigation or indoor offices, these cues can be found consistently, which enables neural networks to predict depth maps from single images. In this work, we are addressing the challenge of depth estimation from a real-world bird’s-eye perspective in an industry environment which contains, conditioned by its special geometry, a minimal amount of visual cues and, hence, requires incorporation of the temporal domain for structure from motion estimation. To enable the system to incorporate structure from motion from pixel translation when facing context-sparse, i.e., visual cue sparse, scenery, we propose a novel architecture built upon the structure from motion learner, which uses temporal pairs of jointly unrotated and stacked images for depth prediction. In order to increase the overall performance and to avoid blurred depth edges that lie in between the edges of the two input images, we integrate a geometric consistency loss into our pipeline. We assess the model’s ability to learn structure from motion by introducing a novel industry dataset whose perspective, orthogonal to the floor, contains only minimal visual cues. Through the evaluation with ground truth depth, we show that our proposed method outperforms the state of the art in difficult context-sparse environments.},
  archive      = {J_MVA},
  author       = {Joswig, Niclas and Autiosalo, Juuso and Ruotsalainen, Laura},
  doi          = {10.1007/s00138-022-01364-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Improved deep depth estimation for environments with sparse visual cues},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LDNet: Low-light image enhancement with joint lighting and
denoising. <em>MVA</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01365-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to unavoidable environmental and/or technical constraints, many photographs are often taken in low-light conditions, which result in underexposure and severe noise. Existing low-light enhancement and denoising methods can deal with both problems individually, but the forced cascading of such methods does not deal well with the combined degradation of light and noise, and is also time-consuming. To address this problem, we propose an efficient network–LDNet, to perform joint low-light enhancement and denoising tasks. LDNet contains an encoder for low-light enhancement, L-Encoder, and a decoder for denoising, D-Decoder. Specifically, we customize the lighten enhancement block (LEB) in L-Encoder to recover rich texture information and luminance information. In D-Decoder, we use image adaptive projection for denoising. Furthermore, since training an end-to-end network requires paired data support, we collect a large-scale real low-light image paired dataset (LN-data). Both the proposed network and dataset provide the basis for this challenging joint task. Extensive experimental results show that our approach achieves better results in both qualitative and quantitative evaluation, notably with a PSNR value of 27.69 and an SSIM value of 0.91 on the LN-data dataset, outperforming other optimal methods.},
  archive      = {J_MVA},
  author       = {Li, Yuhang and Liu, Tianyanshi and Fan, Jiaxin and Ding, Youdong},
  doi          = {10.1007/s00138-022-01365-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {LDNet: Low-light image enhancement with joint lighting and denoising},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Material classification of polishing and convex surface
objects based on photon accumulation point spread function (PAPSF) from
imaging model of binocular pulsed time-of-flight camera. <em>MVA</em>,
<em>34</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01366-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of materials is a research hotspot. These methods generally focus on the classification of flat materials and do not consider the influence of polishing and convex surfaces. We develop a classification algorithm of polishing and convex surface objects, and derive the photon accumulation point spread function (PAPSF) of material from the imaging model of a binocular pulsed time-of-flight (ToF) camera as the classification feature, which consists of depth distortion, the indirect reflection photon cumulant and the indirect reflection photon cumulant. We design a one-versus-all support vector machine (SVM) classifier to classify materials of polishing and convex surfaces objects. We conduct classification experiments on four plastics and four metal materials with a similar appearance. Our method in flat and raw material classification has the same classification accuracy as the latest method based on a continuous-wave- modulation ToF camera, but also our method achieved accuracies of 91.0% in flat and polishing material classification, 93.0% in different convex surface and fixed polishing material classification, 91.5% in fixed convex surface and different polishing material classification and 90.2% in polishing and convex surface material classification.},
  archive      = {J_MVA},
  author       = {Lang, Shinan and Zhang, Jizhong and Chen, Fangyi and Cai, Yiheng and Wu, Qiang},
  doi          = {10.1007/s00138-022-01366-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Material classification of polishing and convex surface objects based on photon accumulation point spread function (PAPSF) from imaging model of binocular pulsed time-of-flight camera},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning accumulative representations for sign language
recognition. <em>MVA</em>, <em>34</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01367-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accumulative representations provide a method for representing variable-length videos with constant length features. In this study, we present aligned temporal accumulative features (ATAF), a skeleton heatmap-based feature for efficient representation and modeling of isolated sign language videos. Inspired by the movement-hold model in sign linguistics, we extract keyframes, align them using temporal transformer networks (TTNs) and extract descriptors using convolutional neural networks (CNNs). In the proposed approach, the use of aligned keyframes increases the recognition power of accumulative features as linguistically significant parts of signs are represented uniquely. Since we detect keyframes using hand movement, there can be differences from signer to signer. To overcome this challenge, ATAF has been implemented with both alignment of sampled frames and keyframe alignment approaches, using both finger speed differences and hand joint heatmaps to perform end-to-end alignment during classification. Results demonstrate that the proposed method achieves state-of-the-art recognition performance on the public BosphorusSign22k (BSign22k) dataset in combination with 3D-CNNs.},
  archive      = {J_MVA},
  author       = {Kındıroglu, Ahmet Alp and Özdemir, Oğulcan and Akarun, Lale},
  doi          = {10.1007/s00138-022-01367-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Aligning accumulative representations for sign language recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visible-infrared person re-identification model based on
feature consistency and modal indistinguishability. <em>MVA</em>,
<em>34</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01368-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is used to search person images across cameras under different modalities, which can address the limitation of visible-based ReID in dark environments. Intra-class discrepancy and feature-expression discrepancy caused by modal changes are two major problems in VI-ReID. To address these problems, a VI-ReID model based on feature consistency and modal indistinguishability is proposed. Specifically, image features of the two modalities are obtained through a one-stream network. Aiming at the problem of intra-class discrepancy, the class-level central consistency loss is developed, which makes the two modal feature centroids of the same identity close to their class centroids. To reduce the discrepancy of feature expression, a modal adversarial learning strategy is designed to distinguish whether the features are consistent with the modality attributes. The aim is that the generator generates the feature which the discriminator cannot distinguish its modality, thereby shortening the modality discrepancy and improving the modal indistinguishability. The generator network is optimized by the cross-entropy loss, the triplet loss, the proposed central consistency loss and the modal adversarial loss. The discriminator network is optimized by the modal adversarial loss. Experiments on the two datasets SYSU-MM01 and RegDB demonstrate that our method has better VI-ReID performance. The code has been released in https://github.com/Nicholasxin/FCMI_VI_reid .},
  archive      = {J_MVA},
  author       = {Sun, Jia and Li, Yanfeng and Chen, Houjin and Peng, Yahui and Zhu, Jinlei},
  doi          = {10.1007/s00138-022-01368-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Visible-infrared person re-identification model based on feature consistency and modal indistinguishability},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pixel-wise confidence estimation for segmentation in
bayesian convolutional neural networks. <em>MVA</em>, <em>34</em>(1),
1–12. (<a href="https://doi.org/10.1007/s00138-022-01369-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian convolutional neural networks represent an emerging state-of-the-art computer vision framework. This particular version of CNN allows the modeling of two types of uncertainties: epistemic uncertainty and aleatoric uncertainty. These uncertainties permit to evaluate the degree of confidence in predictions yielded by a model. Nevertheless, few studies automatically integrate uncertainty in an end-to-end prediction pipeline. Our research proposes a novel way to assess the degree of confidence in yielded predictions. To that end, a model is trained using the cross-entropy loss function. Thereafter, the learning of this model is resumed with a pixel weighting dynamic calculation designed to reduce the uncertainty of well-classified pixels and penalize wrong classifications. From the epistemic uncertainty measures provided by these two trainings, a histogram is calculated. A final neural network model is used to determine an interval of confidence for the predictions. This interval defines the pixels to be considered with caution at test time, depending on the uncertainty they yield. Validation is performed and shows that the uncertainty yielded by our sample weighting provides a better confidence interval than the regular, unweighted, cross-entropy loss function. Furthermore, our expected calibration error averaged over all datasets (0.07) is lower than available methods (0.1 for the state-of-the-art, 0.18 without calibration). Furthermore, the proposed uncertainty-based thresholding provides better accuracy than baseline uncertainty thresholding, while also minimizing the number of confident errors.},
  archive      = {J_MVA},
  author       = {Martin, Rémi and Duong, Luc},
  doi          = {10.1007/s00138-022-01369-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pixel-wise confidence estimation for segmentation in bayesian convolutional neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
