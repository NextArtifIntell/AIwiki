<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---166">ML - 166</h2>
<ul>
<li><details>
<summary>
(2023a). Correction to: Efficient generator of mathematical
expressions for symbolic regression. <em>ML</em>, <em>112</em>(12),
5191. (<a href="https://doi.org/10.1007/s10994-023-06407-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Mežnar, Sebastian and Džeroski, Sašo and Todorovski, Ljupčo},
  doi          = {10.1007/s10994-023-06407-9},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5191},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Efficient generator of mathematical expressions for symbolic regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State-based episodic memory for multi-agent reinforcement
learning. <em>ML</em>, <em>112</em>(12), 5163–5190. (<a
href="https://doi.org/10.1007/s10994-023-06365-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent reinforcement learning (MARL) algorithms have made promising progress in recent years by leveraging the centralized training and decentralized execution (CTDE) paradigm. However, existing MARL algorithms still suffer from the sample inefficiency problem. In this paper, we propose a simple yet effective approach, called state-based episodic memory (SEM), to improve sample efficiency in MARL. SEM adopts episodic memory (EM) to supervise the centralized training procedure of CTDE in MARL. To the best of our knowledge, SEM is the first work to introduce EM into MARL. SEM has lower space complexity and time complexity than state and action based EM (SAEM) initially proposed for single-agent reinforcement learning when using for MARL. Experimental results on two synthetic environments and one real environment show that introducing episodic memory into MARL can improve sample efficiency, and SEM can reduce storage cost and time cost compared with SAEM.},
  archive      = {J_ML},
  author       = {Ma, Xiao and Li, Wu-Jun},
  doi          = {10.1007/s10994-023-06365-2},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5163-5190},
  shortjournal = {Mach. Learn.},
  title        = {State-based episodic memory for multi-agent reinforcement learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust generative adversarial network. <em>ML</em>,
<em>112</em>(12), 5135–5161. (<a
href="https://doi.org/10.1007/s10994-023-06367-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are one of the most popular and powerful models to learn the complex high dimensional distributions. However, they usually suffer from instability and generalization issues which may lead to poor generations. Most existing works focus on stabilizing the training for the discriminators of GANs while ignoring their generalization issue. In this work, we aim to improve the generalization capability of GANs by promoting the local robustness within the small neighborhood of the training samples. We prove that the robustness in the small neighborhood of the training sets can lead to better generalization. Particularly, we design a new robust method called Robust Generative Adversarial Network (RGAN) in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. The generator tries to map the worst input distribution (rather than a Gaussian distribution used in most GANs) to the real data distribution, while the discriminator attempts to distinguish the real and fake distributions with the worst perturbations. Intuitively, the proposed RGAN can learn a good generator and discriminator that can even perform well on the worst-case input points. Strictly, we have proved that RGAN can obtain a tighter generalization upper bound than the traditional GANs under mild assumptions, ensuring a theoretical superiority of RGAN over GANs. We conduct our proposed method on five different baselines (five popular GAN models). And a series of experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed robust frameworks outperform five baseline models substantially and consistently.},
  archive      = {J_ML},
  author       = {Zhang, Shufei and Qian, Zhuang and Huang, Kaizhu and Zhang, Rui and Xiao, Jimin and He, Yuan and Lu, Canyi},
  doi          = {10.1007/s10994-023-06367-0},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5135-5161},
  shortjournal = {Mach. Learn.},
  title        = {Robust generative adversarial network},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous multi-task gaussian cox processes.
<em>ML</em>, <em>112</em>(12), 5105–5134. (<a
href="https://doi.org/10.1007/s10994-023-06382-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel extension of multi-task Gaussian Cox processes for modeling multiple heterogeneous correlated tasks jointly, e.g., classification and regression, via multi-output Gaussian processes (MOGP). A MOGP prior over the parameters of the dedicated likelihoods for classification, regression and point process tasks can facilitate sharing of information between heterogeneous tasks, while allowing for nonparametric parameter estimation. To circumvent the non-conjugate Bayesian inference in the MOGP modulated heterogeneous multi-task framework, we employ the data augmentation technique and derive a mean-field approximation to realize closed-form iterative updates for estimating model parameters. We demonstrate the performance and inference on both 1D synthetic data as well as 2D urban data of Vancouver.},
  archive      = {J_ML},
  author       = {Zhou, Feng and Kong, Quyu and Deng, Zhijie and He, Fengxiang and Cui, Peng and Zhu, Jun},
  doi          = {10.1007/s10994-023-06382-1},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5105-5134},
  shortjournal = {Mach. Learn.},
  title        = {Heterogeneous multi-task gaussian cox processes},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to be fair? A study of label and selection bias.
<em>ML</em>, <em>112</em>(12), 5081–5104. (<a
href="https://doi.org/10.1007/s10994-023-06401-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and “We’re All Equal” on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution.},
  archive      = {J_ML},
  author       = {Favier, Marco and Calders, Toon and Pinxteren, Sam and Meyer, Jonathan},
  doi          = {10.1007/s10994-023-06401-1},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5081-5104},
  shortjournal = {Mach. Learn.},
  title        = {How to be fair? a study of label and selection bias},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UCoDe: Unified community detection with graph convolutional
networks. <em>ML</em>, <em>112</em>(12), 5057–5080. (<a
href="https://doi.org/10.1007/s10994-023-06402-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection finds homogeneous groups of nodes in a graph. Existing approaches either partition the graph into disjoint, non-overlapping, communities, or determine only overlapping communities. To date, no method supports both detections of overlapping and non-overlapping communities. We propose UCoDe, a unified method for community detection in attributed graphs that detects both overlapping and non-overlapping communities by means of a novel contrastive loss that captures node similarity on a macro-scale. Our thorough experimental assessment on real data shows that, regardless of the data distribution, our method is either the top performer or among the top performers in both overlapping and non-overlapping detection without burdensome hyper-parameter tuning.},
  archive      = {J_ML},
  author       = {Moradan, Atefeh and Draganov, Andrew and Mottin, Davide and Assent, Ira},
  doi          = {10.1007/s10994-023-06402-0},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {5057-5080},
  shortjournal = {Mach. Learn.},
  title        = {UCoDe: Unified community detection with graph convolutional networks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trimming stability selection increases variable selection
robustness. <em>ML</em>, <em>112</em>(12), 4995–5055. (<a
href="https://doi.org/10.1007/s10994-023-06384-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contamination can severely distort an estimator unless the estimation procedure is suitably robust. This is a well-known issue and has been addressed in Robust Statistics, however, the relation of contamination and distorted variable selection has been rarely considered in the literature. As for variable selection, many methods for sparse model selection have been proposed, including the Stability Selection which is a meta-algorithm based on some variable selection algorithm in order to immunize against particular data configurations. We introduce the variable selection breakdown point that quantifies the number of cases resp. cells that have to be contaminated in order to let no relevant variable be detected. We show that particular outlier configurations can completely mislead model selection. We combine the variable selection breakdown point with resampling, resulting in the Stability Selection breakdown point that quantifies the robustness of Stability Selection. We propose a trimmed Stability Selection which only aggregates the models with the best performance so that, heuristically, models computed on heavily contaminated resamples should be trimmed away. An extensive simulation study with non-robust regression and classification algorithms as well as with two robust regression algorithms reveals both the potential of our approach to boost the model selection robustness as well as the fragility of variable selection using non-robust algorithms, even for an extremely small cell-wise contamination rate.},
  archive      = {J_ML},
  author       = {Werner, Tino},
  doi          = {10.1007/s10994-023-06384-z},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4995-5055},
  shortjournal = {Mach. Learn.},
  title        = {Trimming stability selection increases variable selection robustness},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep intrinsically motivated exploration in continuous
control. <em>ML</em>, <em>112</em>(12), 4959–4993. (<a
href="https://doi.org/10.1007/s10994-023-06363-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In continuous control, exploration is often performed through undirected strategies in which parameters of the networks or selected actions are perturbed by random noise. Although the deep setting of undirected exploration has been shown to improve the performance of on-policy methods, they introduce an excessive computational complexity and are known to fail in the off-policy setting. The intrinsically motivated exploration is an effective alsetup and hyper-parameterternative to the undirected strategies, but they are usually studied for discrete action domains. In this paper, we investigate how intrinsic motivation can effectively be combined with deep reinforcement learning in the control of continuous systems to obtain a directed exploratory behavior. We adapt the existing theories on animal motivational systems into the reinforcement learning paradigm and introduce a novel and scalable directed exploration strategy. The introduced approach, motivated by the maximization of the value function’s error, can benefit from a collected set of experiences by extracting useful information and unify the intrinsic exploration motivations in the literature under a single exploration objective. An extensive set of empirical studies demonstrate that our framework extends to larger and more diverse state spaces, dramatically improves the baselines, and outperforms the undirected strategies significantly.},
  archive      = {J_ML},
  author       = {Saglam, Baturay and Kozat, Suleyman S.},
  doi          = {10.1007/s10994-023-06363-4},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4959-4993},
  shortjournal = {Mach. Learn.},
  title        = {Deep intrinsically motivated exploration in continuous control},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A geometric framework for multiclass ensemble classifiers.
<em>ML</em>, <em>112</em>(12), 4929–4958. (<a
href="https://doi.org/10.1007/s10994-023-06406-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble classifiers have been investigated by many in the artificial intelligence and machine learning community. Majority voting and weighted majority voting are two commonly used combination schemes in ensemble learning. However, understanding of them is incomplete at best, with some properties even misunderstood. In this paper, we present a group of properties of these two schemes formally under a geometric framework. Two key factors, every component base classifier’s performance and dissimilarity between each pair of component classifiers are evaluated by the same metric—the Euclidean distance. Consequently, ensembling becomes a deterministic problem and the performance of an ensemble can be calculated directly by a formula. We prove several theorems of interest and explain their implications for ensembles. In particular, we compare and contrast the effect of the number of component classifiers on these two types of ensemble schemes. Some important properties of both combination schemes are discussed. And a method to calculate the optimal weights for the weighted majority voting is presented. Empirical investigation is conducted to verify the theoretical results. We believe that the results from this paper are very useful for us to understand the fundamental properties of these two combination schemes and the principles of ensemble classifiers in general. The results are also helpful for us to investigate some issues in ensemble classifiers, such as ensemble performance prediction, diversity, ensemble pruning, and others.},
  archive      = {J_ML},
  author       = {Wu, Shengli and Li, Jinlong and Ding, Weimin},
  doi          = {10.1007/s10994-023-06406-w},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4929-4958},
  shortjournal = {Mach. Learn.},
  title        = {A geometric framework for multiclass ensemble classifiers},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiclass optimal classification trees with SVM-splits.
<em>ML</em>, <em>112</em>(12), 4905–4928. (<a
href="https://doi.org/10.1007/s10994-023-06366-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a novel mathematical optimization-based methodology to construct tree-shaped classification rules for multiclass instances. Our approach consists of building Classification Trees in which, except for the leaf nodes, the labels are temporarily left out and grouped into two classes by means of a SVM separating hyperplane. We provide a Mixed Integer Non Linear Programming formulation for the problem and report the results of an extended battery of computational experiments to assess the performance of our proposal with respect to other benchmarking classification methods.},
  archive      = {J_ML},
  author       = {Blanco, Víctor and Japón, Alberto and Puerto, Justo},
  doi          = {10.1007/s10994-023-06366-1},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4905-4928},
  shortjournal = {Mach. Learn.},
  title        = {Multiclass optimal classification trees with SVM-splits},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental permutation feature importance (iPFI): Towards
online explanations on data streams. <em>ML</em>, <em>112</em>(12),
4863–4903. (<a
href="https://doi.org/10.1007/s10994-023-06385-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence has mainly focused on static learning scenarios so far. We are interested in dynamic scenarios where data is sampled progressively, and learning is done in an incremental rather than a batch mode. We seek efficient incremental algorithms for computing feature importance (FI). Permutation feature importance (PFI) is a well-established model-agnostic measure to obtain global FI based on feature marginalization of absent features. We propose an efficient, model-agnostic algorithm called iPFI to estimate this measure incrementally and under dynamic modeling conditions including concept drift. We prove theoretical guarantees on the approximation quality in terms of expectation and variance. To validate our theoretical findings and the efficacy of our approaches in incremental scenarios dealing with streaming data rather than traditional batch settings, we conduct multiple experimental studies on benchmark data with and without concept drift.},
  archive      = {J_ML},
  author       = {Fumagalli, Fabian and Muschalik, Maximilian and Hüllermeier, Eyke and Hammer, Barbara},
  doi          = {10.1007/s10994-023-06385-y},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4863-4903},
  shortjournal = {Mach. Learn.},
  title        = {Incremental permutation feature importance (iPFI): Towards online explanations on data streams},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective negative sampling approach for contrastive
learning of sentence embedding. <em>ML</em>, <em>112</em>(12),
4837–4861. (<a
href="https://doi.org/10.1007/s10994-023-06408-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised sentence embedding learning is a fundamental task in natural language processing. Recently, unsupervised contrastive learning based on pre-trained language models has shown impressive performance in sentence embedding learning. This method aims to align positive sentence pairs while pushing apart negative sentence pairs to achieve semantic uniformity in the representation space. However, most previous literature leverages a random strategy to sample negative pairs, which suffers from the risk of selecting uninformative negative examples (e.g., easily distinguishable examples, anisotropic representations), thus greatly affecting the quality of learned representations. To address this issue, we propose nmCSE, a negative mining contrastive learning method for sentence embedding. Specifically, we introduce distance moderation and spatial uniformity as two properties of informative negative examples, and devise distance-based weighting and grid sampling as two strategies to preserve these properties, respectively. Our proposal outperforms the random strategy across seven semantic textual similarity datasets. Furthermore, our method can easily be adapted to other contrastive learning scenarios (e.g., vision), and does not introduce significant computational overhead.},
  archive      = {J_ML},
  author       = {Tan, Qitao and Song, Xiaoying and Ye, Guanghui and Wu, Chuan},
  doi          = {10.1007/s10994-023-06408-8},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4837-4861},
  shortjournal = {Mach. Learn.},
  title        = {An effective negative sampling approach for contrastive learning of sentence embedding},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning from casual conversation. <em>ML</em>,
<em>112</em>(12), 4789–4836. (<a
href="https://doi.org/10.1007/s10994-023-06383-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human social learning is an effective process that has inspired many existing machine learning approaches, such as learning from observation and learning by demonstration. In this paper, we introduce another form of social learning, learning from a casual conversation or LCC a machine learning approach in which an artificially intelligent agent learns new information through an extended natural language dialog with a human. Our system enables the agent to add or change information in its knowledge base as a result of the human’s conversational text inputs. LCC seeks to close an important gap in the state of the art that has focused on teaching computer agents how to perform specific tasks. Furthermore, LCC could also provide an efficient way to enhance the knowledge base of certain types of systems without requiring the involvement of a programmer. LCC does not require the user to enter specific information; instead, the user can converse naturally with the agent. As part of its learning process, LCC identifies the text inputs from the conversing human that contain information worth learning, and then determines whether the inputs are heretofore unknown and learns it; in agreement with what it already “knows” and ignores it; or in conflict with what it “knows” and it must resolve the conflict. LCC’s architecture consists of multiple sub-systems combined to perform the above tasks. Its learning component can add new information to the knowledge base, confirm existing information, and/or update existing information found to be related to the user input. The LCC system functionality was rigorously assessed with test statements comprising various difficulty levels. Furthermore, its acceptance by human users was evaluated by two separate groups of human test subjects—one group who interacted with the system, and a second group that evaluated the logs of the interactions of the first group. The collected results were all found to be acceptable and within the range of our expectations.},
  archive      = {J_ML},
  author       = {Mohammed Ali, Awrad E. and Gonzalez, Avelino J.},
  doi          = {10.1007/s10994-023-06383-0},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4789-4836},
  shortjournal = {Mach. Learn.},
  title        = {Machine learning from casual conversation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WEASEL 2.0: A random dilated dictionary transform for fast,
accurate and memory constrained time series classification. <em>ML</em>,
<em>112</em>(12), 4763–4788. (<a
href="https://doi.org/10.1007/s10994-023-06395-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A time series is a sequence of sequentially ordered real values in time. Time series classification (TSC) is the task of assigning a time series to one of a set of predefined classes, usually based on a model learned from examples. Dictionary-based methods for TSC rely on counting the frequency of certain patterns in time series and are important components of the currently most accurate TSC ensembles. One of the early dictionary-based methods was WEASEL, which at its time achieved SotA results while also being very fast. However, it is outperformed both in terms of speed and accuracy by other methods. Furthermore, its design leads to an unpredictably large memory footprint, making it inapplicable for many applications. In this paper, we present WEASEL 2.0, a complete overhaul of WEASEL based on two recent advancements in TSC: Dilation and ensembling of randomized hyper-parameter settings. These two techniques allow WEASEL 2.0 to work with a fixed-size memory footprint while at the same time improving accuracy. Compared to 15 other SotA methods on the UCR benchmark set, WEASEL 2.0 is significantly more accurate than other dictionary methods and not significantly worse than the currently best methods. Actually, it achieves the highest median accuracy over all data sets, and it performs best in 5 out of 12 problem classes. We thus believe that WEASEL 2.0 is a viable alternative for current TSC and also a potentially interesting input for future ensembles.},
  archive      = {J_ML},
  author       = {Schäfer, Patrick and Leser, Ulf},
  doi          = {10.1007/s10994-023-06395-w},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4763-4788},
  shortjournal = {Mach. Learn.},
  title        = {WEASEL 2.0: A random dilated dictionary transform for fast, accurate and memory constrained time series classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary-restricted metric learning. <em>ML</em>,
<em>112</em>(12), 4723–4762. (<a
href="https://doi.org/10.1007/s10994-023-06380-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning aims to learn a distance metric to properly measure the similarities between pairwise examples. Most existing learning algorithms are designed to reduce intra-class distances and meanwhile enlarge inter-class distances by critically introducing a margin between intra-class and inter-class distances. However, such learning objectives may yield boundless (distance) metric space, because their enlargements on inter-class distances are usually unconstrained. In this case, excessively enlarged inter-class distances would relatively reduce the ratio of margin to the whole distance range (i.e., the margin-range-ratio), and thus being against the initial large-margin purpose for discriminating the similarities of data pairs. To address this issue, we propose a new boundary-restricted metric (BRM), which confines the metric space by a restriction function. Such a restriction function is monotonous and gradually converges to an upper bound, which suppresses excessively large distances of data pairs and concurrently maintains the reliable discriminability. After that, the learned metric can be successfully restricted in a finite region, and thereby avoiding the reduction of margin-range-ratio. Theoretically, we prove that BRM tightens the generalization error bound of the traditional learning model without sacrificing the fitting capability or destroying the topological property of the learned metric, which implies that BRM makes a good bias-variance tradeoff for the metric learning task. Extensive experiments on toy data and real-world datasets validate the superiority of our approach over the state-of-the-art metric learning methods.},
  archive      = {J_ML},
  author       = {Chen, Shuo and Gong, Chen and Li, Xiang and Yang, Jian and Niu, Gang and Sugiyama, Masashi},
  doi          = {10.1007/s10994-023-06380-3},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4723-4762},
  shortjournal = {Mach. Learn.},
  title        = {Boundary-restricted metric learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ranking-preserved generative label enhancement. <em>ML</em>,
<em>112</em>(12), 4693–4721. (<a
href="https://doi.org/10.1007/s10994-023-06388-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is effective for addressing label ambiguity. In LDL, ground-truth label distributions are hardly available due to the high annotation cost, whereas it is relatively easy to obtain examples with logical labels. Hence, label enhancement (LE) is proposed to automatically transform logical labels into label distributions. Most existing LE methods employ discriminative approaches. However, discriminative approaches specialize in obtaining better predictive performance under supervised learning, and their capability is limited in LE that lacks supervisory information. Therefore, we propose a generative LE model, and infer label distributions by the variational Bayes capable of preserving the label ranking within the logical label vector. Our method consists of a generation process and an inference process. In the generation process, we treat label distributions as latent variables, and assume that label distributions generate logical labels and feature values of the instance itself and logical labels of the neighbors of this instance. In the inference process, we design a function, which mines the label correlation and preserves the label ranking within the logical label vector, to parameterize the variational posterior. Finally, we conduct extensive experiments to validate our proposal.},
  archive      = {J_ML},
  author       = {Lu, Yunan and Li, Weiwei and Li, Huaxiong and Jia, Xiuyi},
  doi          = {10.1007/s10994-023-06388-9},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4693-4721},
  shortjournal = {Mach. Learn.},
  title        = {Ranking-preserved generative label enhancement},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biquality learning: A framework to design algorithms dealing
with closed-set distribution shifts. <em>ML</em>, <em>112</em>(12),
4663–4692. (<a
href="https://doi.org/10.1007/s10994-023-06372-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training machine learning models from data with weak supervision and dataset shifts is still challenging. Designing algorithms when these two situations arise has not been explored much, and existing algorithms cannot always handle the most complex distributional shifts. We think the biquality data setup is a suitable framework for designing such algorithms. Biquality Learning assumes that two datasets are available at training time: a trusted dataset sampled from the distribution of interest and the untrusted dataset with dataset shifts and weaknesses of supervision (aka distribution shifts). The trusted and untrusted datasets available at training time make designing algorithms dealing with any distribution shifts possible. We propose two methods, one inspired by the label noise literature and another by the covariate shift literature for biquality learning. We experiment with two novel methods to synthetically introduce concept drift and class-conditional shifts in real-world datasets across many of them. We opened some discussions and assessed that developing biquality learning algorithms robust to distributional changes remains an interesting problem for future research.},
  archive      = {J_ML},
  author       = {Nodet, Pierre and Lemaire, Vincent and Bondu, Alexis and Cornuéjols, Antoine},
  doi          = {10.1007/s10994-023-06372-3},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4663-4692},
  shortjournal = {Mach. Learn.},
  title        = {Biquality learning: A framework to design algorithms dealing with closed-set distribution shifts},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Are LSTMs good few-shot learners? <em>ML</em>,
<em>112</em>(11), 4635–4662. (<a
href="https://doi.org/10.1007/s10994-023-06394-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning requires large amounts of data to learn new tasks well, limiting its applicability to domains where such data is available. Meta-learning overcomes this limitation by learning how to learn. Hochreiter et al. (International conference on artificial neural networks, Springer, 2001) showed that an LSTM trained with backpropagation across different tasks is capable of meta-learning. Despite promising results of this approach on small problems, and more recently, also on reinforcement learning problems, the approach has received little attention in the supervised few-shot learning setting. We revisit this approach and test it on modern few-shot learning benchmarks. We find that LSTM, surprisingly, outperform the popular meta-learning technique MAML on a simple few-shot sine wave regression benchmark, but that LSTM, expectedly, fall short on more complex few-shot image classification benchmarks. We identify two potential causes and propose a new method called Outer Product LSTM (OP-LSTM) that resolves these issues and displays substantial performance gains over the plain LSTM. Compared to popular meta-learning baselines, OP-LSTM yields competitive performance on within-domain few-shot image classification, and performs better in cross-domain settings by 0.5–1.9\% in accuracy score. While these results alone do not set a new state-of-the-art, the advances of OP-LSTM are orthogonal to other advances in the field of meta-learning, yield new insights in how LSTM work in image classification, allowing for a whole range of new research directions. For reproducibility purposes, we publish all our research code publicly.},
  archive      = {J_ML},
  author       = {Huisman, Mike and Moerland, Thomas M. and Plaat, Aske and van Rijn, Jan N.},
  doi          = {10.1007/s10994-023-06394-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4635-4662},
  shortjournal = {Mach. Learn.},
  title        = {Are LSTMs good few-shot learners?},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory test-train overlap in next-location prediction
datasets. <em>ML</em>, <em>112</em>(11), 4597–4634. (<a
href="https://doi.org/10.1007/s10994-023-06386-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-location prediction, consisting of forecasting a user’s location given their historical trajectories, has important implications in several fields, such as urban planning, geo-marketing, and disease spreading. Several predictors have been proposed in the last few years to address it, including last-generation ones based on deep learning. This paper tests the generalization capability of these predictors on public mobility datasets, stratifying the datasets by whether the trajectories in the test set also appear fully or partially in the training set. We consistently discover a severe problem of trajectory overlapping in all analyzed datasets, highlighting that predictors memorize trajectories while having limited generalization capacities. We thus propose a methodology to rerank the outputs of the next-location predictors based on spatial mobility patterns. With these techniques, we significantly improve the predictors’ generalization capability, with a relative improvement in the accuracy up to 96.15\% on the trajectories that cannot be memorized (i.e., low overlap with the training set).},
  archive      = {J_ML},
  author       = {Luca, Massimiliano and Pappalardo, Luca and Lepri, Bruno and Barlacchi, Gianni},
  doi          = {10.1007/s10994-023-06386-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4597-4634},
  shortjournal = {Mach. Learn.},
  title        = {Trajectory test-train overlap in next-location prediction datasets},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Efficient generator of mathematical expressions for
symbolic regression. <em>ML</em>, <em>112</em>(11), 4563–4596. (<a
href="https://doi.org/10.1007/s10994-023-06400-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better than a state-of-the-art system based on a similar combination of deep learning and evolutionary algorithms.},
  archive      = {J_ML},
  author       = {Mežnar, Sebastian and Džeroski, Sašo and Todorovski, Ljupčo},
  doi          = {10.1007/s10994-023-06400-2},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4563-4596},
  shortjournal = {Mach. Learn.},
  title        = {Efficient generator of mathematical expressions for symbolic regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cautious policy programming: Exploiting KL regularization
for monotonic policy improvement in reinforcement learning. <em>ML</em>,
<em>112</em>(11), 4527–4562. (<a
href="https://doi.org/10.1007/s10994-023-06368-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose cautious policy programming (CPP), a novel value-based reinforcement learning (RL) algorithm that exploits the idea of monotonic policy improvement during learning. Based on the nature of entropy-regularized RL, we derive a new entropy-regularization-aware lower bound of policy improvement that depends on the expected policy advantage function but not on state-action-space-wise maximization as in prior work. CPP leverages this lower bound as a criterion for adjusting the degree of a policy update for alleviating policy oscillation. Different from similar algorithms that are mostly theory-oriented, we also propose a novel interpolation scheme that makes CPP better scale in high dimensional control problems. We demonstrate that the proposed algorithm can trade off performance and stability in both didactic classic control problems and challenging high-dimensional Atari games.},
  archive      = {J_ML},
  author       = {Zhu, Lingwei and Matsubara, Takamitsu},
  doi          = {10.1007/s10994-023-06368-z},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4527-4562},
  shortjournal = {Mach. Learn.},
  title        = {Cautious policy programming: Exploiting KL regularization for monotonic policy improvement in reinforcement learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scale-preserving automatic concept extraction (SPACE).
<em>ML</em>, <em>112</em>(11), 4495–4525. (<a
href="https://doi.org/10.1007/s10994-023-06373-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) have become a common choice for industrial quality control, as well as other critical applications in the Industry 4.0. When these CNNs behave in ways unexpected to human users or developers, severe consequences can arise, such as economic losses or an increased risk to human life. Concept extraction techniques can be applied to increase the reliability and transparency of CNNs through generating global explanations for trained neural network models. The decisive features of image datasets in quality control often depend on the feature’s scale; for example, the size of a hole or an edge. However, existing concept extraction methods do not correctly represent scale, which leads to problems interpreting these models as we show herein. To address this issue, we introduce the Scale-Preserving Automatic Concept Extraction (SPACE) algorithm, as a state-of-the-art alternative concept extraction technique for CNNs, focused on industrial applications. SPACE is specifically designed to overcome the aforementioned problems by avoiding scale changes throughout the concept extraction process. SPACE proposes an approach based on square slices of input images, which are selected and then tiled before being clustered into concepts. Our method provides explanations of the models’ decision-making process in the form of human-understandable concepts. We evaluate SPACE on three image classification datasets in the context of industrial quality control. Through experimental results, we illustrate how SPACE outperforms other methods and provides actionable insights on the decision mechanisms of CNNs. Finally, code for the implementation of SPACE is provided.},
  archive      = {J_ML},
  author       = {Posada-Moreno, Andrés Felipe and Kreisköther, Lukas and Glander, Tassilo and Trimpe, Sebastian},
  doi          = {10.1007/s10994-023-06373-2},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4495-4525},
  shortjournal = {Mach. Learn.},
  title        = {Scale-preserving automatic concept extraction (SPACE)},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter identifiability of a deep feedforward ReLU neural
network. <em>ML</em>, <em>112</em>(11), 4431–4493. (<a
href="https://doi.org/10.1007/s10994-023-06355-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The possibility for one to recover the parameters—weights and biases—of a neural network thanks to the knowledge of its function on a subset of the input space can be, depending on the situation, a curse or a blessing. On one hand, recovering the parameters allows for better adversarial attacks and could also disclose sensitive information from the dataset used to construct the network. On the other hand, if the parameters of a network can be recovered, it guarantees the user that the features in the latent spaces can be interpreted. It also provides foundations to obtain formal guarantees on the performances of the network. It is therefore important to characterize the networks whose parameters can be identified and those whose parameters cannot. In this article, we provide a set of conditions on a deep fully-connected feedforward ReLU neural network under which the parameters of the network are uniquely identified—modulo permutation and positive rescaling—from the function it implements on a subset of the input space.},
  archive      = {J_ML},
  author       = {Bona-Pellissier, Joachim and Bachoc, François and Malgouyres, François},
  doi          = {10.1007/s10994-023-06355-4},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4431-4493},
  shortjournal = {Mach. Learn.},
  title        = {Parameter identifiability of a deep feedforward ReLU neural network},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early anomaly detection in time series: A hierarchical
approach for predicting critical health episodes. <em>ML</em>,
<em>112</em>(11), 4409–4430. (<a
href="https://doi.org/10.1007/s10994-022-06300-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of anomalous events in time series data is essential in many domains of application. In this paper we deal with critical health events, which represent a significant cause of mortality in intensive care units of hospitals. The timely prediction of these events is crucial for mitigating their consequences and improving healthcare. One of the most common approaches to tackle early anomaly detection problems is through standard classification methods. In this paper we propose a novel method that uses a layered learning architecture to address these tasks. One key contribution of our work is the idea of pre-conditional events, which denote arbitrary but computable relaxed versions of the event of interest. We leverage this idea to break the original problem into two hierarchical layers, which we hypothesize are easier to solve. The results suggest that the proposed approach leads to a better performance relative to state of the art approaches for critical health episode prediction.},
  archive      = {J_ML},
  author       = {Cerqueira, Vitor and Torgo, Luis and Soares, Carlos},
  doi          = {10.1007/s10994-022-06300-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4409-4430},
  shortjournal = {Mach. Learn.},
  title        = {Early anomaly detection in time series: A hierarchical approach for predicting critical health episodes},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature ranking for semi-supervised learning. <em>ML</em>,
<em>112</em>(11), 4379–4408. (<a
href="https://doi.org/10.1007/s10994-022-06181-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data used for analysis are becoming increasingly complex along several directions: high dimensionality, number of examples and availability of labels for the examples. This poses a variety of challenges for the existing machine learning methods, related to analyzing datasets with a large number of examples that are described in a high-dimensional space, where not all examples have labels provided. For example, when investigating the toxicity of chemical compounds, there are many compounds available that can be described with information-rich high-dimensional representations, but not all of the compounds have information on their toxicity. To address these challenges, we propose methods for semi-supervised learning (SSL) of feature rankings. The feature rankings are learned in the context of classification and regression, as well as in the context of structured output prediction (multi-label classification, MLC, hierarchical multi-label classification, HMLC and multi-target regression, MTR) tasks. This is the first work that treats the task of feature ranking uniformly across various tasks of semi-supervised structured output prediction. To the best of our knowledge, it is also the first work on SSL of feature rankings for the tasks of HMLC and MTR. More specifically, we propose two approaches—based on predictive clustering tree ensembles and the Relief family of algorithms—and evaluate their performance across 38 benchmark datasets. The extensive evaluation reveals that rankings based on Random Forest ensembles perform the best for classification tasks (incl. MLC and HMLC tasks) and are the fastest for all tasks, while ensembles based on extremely randomized trees work best for the regression tasks. Semi-supervised feature rankings outperform their supervised counterparts across the majority of datasets for all of the different tasks, showing the benefit of using unlabeled in addition to labeled data.},
  archive      = {J_ML},
  author       = {Petković, Matej and Džeroski, Sašo and Kocev, Dragi},
  doi          = {10.1007/s10994-022-06181-0},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4379-4408},
  shortjournal = {Mach. Learn.},
  title        = {Feature ranking for semi-supervised learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STUDD: A student–teacher method for unsupervised concept
drift detection. <em>ML</em>, <em>112</em>(11), 4351–4378. (<a
href="https://doi.org/10.1007/s10994-022-06188-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift detection is a crucial task in data stream evolving environments. Most of state of the art approaches designed to tackle this problem monitor the loss of predictive models. However, this approach falls short in many real-world scenarios, where the true labels are not readily available to compute the loss. In this context, there is increasing attention to approaches that perform concept drift detection in an unsupervised manner, i.e., without access to the true labels after the model is deployed. We propose a novel approach to unsupervised concept drift detection based on a student-teacher learning paradigm. Essentially, we create an auxiliary model (student) to mimic the primary model’s behaviour (teacher). At run-time, our approach is to use the teacher for predicting new instances and monitoring the mimicking loss of the student for concept drift detection. In a set of experiments using 19 data streams, we show that the proposed approach can detect concept drift and present a competitive behaviour relative to the state of the art approaches.},
  archive      = {J_ML},
  author       = {Cerqueira, Vitor and Gomes, Heitor Murilo and Bifet, Albert and Torgo, Luis},
  doi          = {10.1007/s10994-022-06188-7},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4351-4378},
  shortjournal = {Mach. Learn.},
  title        = {STUDD: A student–teacher method for unsupervised concept drift detection},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span
class="math display">Latent<em>O</em><em>u</em><em>t</em></span>: An
unsupervised deep anomaly detection approach exploiting latent space
distribution. <em>ML</em>, <em>112</em>(11), 4323–4349. (<a
href="https://doi.org/10.1007/s10994-022-06153-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection methods exploiting autoencoders (AE) have shown good performances. Unfortunately, deep non-linear architectures are able to perform high dimensionality reduction while keeping reconstruction error low, thus worsening outlier detecting performances of AEs. To alleviate the above problem, recently some authors have proposed to exploit Variational autoencoders (VAE) and bidirectional Generative Adversarial Networks (GAN), which arise as a variant of standard AEs designed for generative purposes, both enforcing the organization of the latent space guaranteeing continuity. However, these architectures share with standard AEs the problem that they generalize so well that they can also well reconstruct anomalies. In this work we argue that the approach of selecting the worst reconstructed examples as anomalies is too simplistic if a continuous latent space autoencoder-based architecture is employed. We show that outliers tend to lie in the sparsest regions of the combined latent/error space and propose the $$\mathrm{VAE}Out$$ and $${{\mathrm {Latent}}Out}$$ unsupervised anomaly detection algorithms, identifying outliers by performing density estimation in this augmented feature space. The proposed approach shows sensible improvements in terms of detection performances over the standard approach based on the reconstruction error.},
  archive      = {J_ML},
  author       = {Angiulli, Fabrizio and Fassetti, Fabio and Ferragina, Luca},
  doi          = {10.1007/s10994-022-06153-4},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4323-4349},
  shortjournal = {Mach. Learn.},
  title        = {$${{\mathrm {Latent}}Out}$$: An unsupervised deep anomaly detection approach exploiting latent space distribution},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining short text classification with diverse synthetic
exemplars and counter-exemplars. <em>ML</em>, <em>112</em>(11),
4289–4322. (<a
href="https://doi.org/10.1007/s10994-022-06150-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present xspells, a model-agnostic local approach for explaining the decisions of black box models in classification of short texts. The explanations provided consist of a set of exemplar sentences and a set of counter-exemplar sentences. The former are examples classified by the black box with the same label as the text to explain. The latter are examples classified with a different label (a form of counter-factuals). Both are close in meaning to the text to explain, and both are meaningful sentences – albeit they are synthetically generated. xspells generates neighbors of the text to explain in a latent space using Variational Autoencoders for encoding text and decoding latent instances. A decision tree is learned from randomly generated neighbors, and used to drive the selection of the exemplars and counter-exemplars. Moreover, diversity of counter-exemplars is modeled as an optimization problem, solved by a greedy algorithm with theoretical guarantee. We report experiments on three datasets showing that xspells outperforms the well-known lime method in terms of quality of explanations, fidelity, diversity, and usefulness, and that is comparable to it in terms of stability.},
  archive      = {J_ML},
  author       = {Lampridis, Orestis and State, Laura and Guidotti, Riccardo and Ruggieri, Salvatore},
  doi          = {10.1007/s10994-022-06150-7},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4289-4322},
  shortjournal = {Mach. Learn.},
  title        = {Explaining short text classification with diverse synthetic exemplars and counter-exemplars},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning biologically-interpretable latent representations
for gene expression data. <em>ML</em>, <em>112</em>(11), 4257–4287. (<a
href="https://doi.org/10.1007/s10994-022-06158-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular gene-expression datasets consist of samples with tens of thousands of measured quantities (i.e., high dimensional data). However, lower-dimensional representations that retain the useful biological information do exist. We present a novel algorithm for such dimensionality reduction called Pathway Activity Score Learning (PASL). The major novelty of PASL is that the constructed features directly correspond to known molecular pathways (genesets in general) and can be interpreted as pathway activity scores. Hence, unlike PCA and similar methods, PASL’s latent space has a fairly straightforward biological interpretation. PASL is shown to outperform in predictive performance the state-of-the-art method (PLIER) on two collections of breast cancer and leukemia gene expression datasets. PASL is also trained on a large corpus of 50000 gene expression samples to construct a universal dictionary of features across different tissues and pathologies. The dictionary validated on 35643 held-out samples for reconstruction error. It is then applied on 165 held-out datasets spanning a diverse range of diseases. The AutoML tool JADBio is employed to show that the predictive information in the PASL-created feature space is retained after the transformation. The code is available at https://github.com/mensxmachina/PASL .},
  archive      = {J_ML},
  author       = {Karagiannaki, Ioulia and Gourlia, Krystallia and Lagani, Vincenzo and Pantazis, Yannis and Tsamardinos, Ioannis},
  doi          = {10.1007/s10994-022-06158-z},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4257-4287},
  shortjournal = {Mach. Learn.},
  title        = {Learning biologically-interpretable latent representations for gene expression data},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous graph embedding with single-level aggregation
and infomax encoding. <em>ML</em>, <em>112</em>(11), 4227–4256. (<a
href="https://doi.org/10.1007/s10994-022-06160-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increasing interest in developing embedding methods for heterogeneous graph-structured data. The state-of-the-art approaches often adopt a bi-level aggregation scheme, where the first level aggregates information of neighbors belonging to the same type or group, and the second level employs the averaging or attention mechanism to aggregate the outputs of the first level. We find that bi-level aggregation may suffer from a down-weighting issue and overlook individual node information, especially when there is an imbalance in the number of different typed relations. We develop a new simple yet effective single-level aggregation scheme with infomax encoding, named HIME, for unsupervised heterogeneous graph embedding. Our single-level aggregation scheme performs relation-specific transformation to obtain homogeneous embeddings before aggregating information from multiple typed neighbors. Thus, it emphasizes each neighbor’s equal contribution and does not suffer from the down-weighting issue. Extensive experiments demonstrate that HIME consistently outperforms the state-of-the-art approaches in link prediction, node classification, and node clustering tasks.},
  archive      = {J_ML},
  author       = {Chairatanakul, Nuttapong and Liu, Xin and Hoang, Nguyen Thai and Murata, Tsuyoshi},
  doi          = {10.1007/s10994-022-06160-5},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4227-4256},
  shortjournal = {Mach. Learn.},
  title        = {Heterogeneous graph embedding with single-level aggregation and infomax encoding},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal discovery with a mixture of DAGs. <em>ML</em>,
<em>112</em>(11), 4201–4225. (<a
href="https://doi.org/10.1007/s10994-022-06159-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real causal processes may contain cycles, evolve over time or differ between populations. However, many graphical models cannot accommodate these conditions. We propose to model causation using a mixture of directed cyclic graphs (DAGs); each sample follows a joint distribution that factorizes according to a DAG, but the DAG may differ between samples due to multiple independent factors. We then introduce an algorithm called Causal Inference over Mixtures that uses longitudinal data to infer a graph summarizing the causal relations generated from a mixture of DAGs even when cycles, non-stationarity, latent variables or selection bias exist. Experiments demonstrate improved performance in inferring ancestral relations as compared to prior approaches. R code is available at https://github.com/ericstrobl/CIM.},
  archive      = {J_ML},
  author       = {Strobl, Eric V.},
  doi          = {10.1007/s10994-022-06159-y},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4201-4225},
  shortjournal = {Mach. Learn.},
  title        = {Causal discovery with a mixture of DAGs},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining classifiers by constructing familiar concepts.
<em>ML</em>, <em>112</em>(11), 4167–4200. (<a
href="https://doi.org/10.1007/s10994-022-06157-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpreting a large number of neurons in deep learning is difficult. Our proposed ‘CLAssifier-DECoder’ architecture (ClaDec) facilitates the understanding of the output of an arbitrary layer of neurons or subsets thereof. It uses a decoder that transforms the incomprehensible representation of the given neurons to a representation that is more similar to the domain a human is familiar with. In an image recognition problem, one can recognize what information (or concepts) a layer maintains by contrasting reconstructed images of ClaDec with those of a conventional auto-encoder(AE) serving as reference. An extension of ClaDec allows trading comprehensibility and fidelity. We evaluate our approach for image classification using convolutional neural networks. We show that reconstructed visualizations using encodings from a classifier capture more relevant classification information than conventional AEs. This holds although AEs contain more information on the original input. Our user study highlights that even non-experts can identify a diverse set of concepts contained in images that are relevant (or irrelevant) for the classifier. We also compare against saliency based methods that focus on pixel relevance rather than concepts. We show that ClaDec tends to highlight more relevant input areas to classification though outcomes depend on classifier architecture. Code is at https://github.com/JohnTailor/ClaDec},
  archive      = {J_ML},
  author       = {Schneider, Johannes and Vlachos, Michalis},
  doi          = {10.1007/s10994-022-06157-0},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4167-4200},
  shortjournal = {Mach. Learn.},
  title        = {Explaining classifiers by constructing familiar concepts},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree-based dynamic classifier chains. <em>ML</em>,
<em>112</em>(11), 4129–4165. (<a
href="https://doi.org/10.1007/s10994-022-06162-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifier chains are an effective technique for modeling label dependencies in multi-label classification. However, the method requires a fixed, static order of the labels. While in theory, any order is sufficient, in practice, this order has a substantial impact on the quality of the final prediction. Dynamic classifier chains denote the idea that for each instance to classify, the order in which the labels are predicted is dynamically chosen. The complexity of a naïve implementation of such an approach is prohibitive, because it would require to train a sequence of classifiers for every possible permutation of the labels. To tackle this problem efficiently, we propose a new approach based on random decision trees which can dynamically select the label ordering for each prediction. We show empirically that a dynamic selection of the next label improves over the use of a static ordering under an otherwise unchanged random decision tree model. In addition, we also demonstrate an alternative approach based on extreme gradient boosted trees, which allows for a more target-oriented training of dynamic classifier chains. Our results show that this variant outperforms random decision trees and other tree-based multi-label classification methods. More importantly, the dynamic selection strategy allows to considerably speed up training and prediction.},
  archive      = {J_ML},
  author       = {Loza Mencía, Eneldo and Kulessa, Moritz and Bohlender, Simon and Fürnkranz, Johannes},
  doi          = {10.1007/s10994-022-06162-3},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4129-4165},
  shortjournal = {Mach. Learn.},
  title        = {Tree-based dynamic classifier chains},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of proximity-based methods for detecting
temporal anomalies in business processes. <em>ML</em>, <em>112</em>(11),
4101–4128. (<a
href="https://doi.org/10.1007/s10994-022-06152-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection in process mining refers to aspects such as infrequent behavior in relation to the underlying business process models or to anomalous latencies of task execution, termed as temporal anomalies. In this work, we focus on the latter form of anomalies and we aim at investigating in depth the behavior of several proximity-based variants, which are shown to outperform simple statistical ones. We investigate multiple distance functions and approaches to establishing the outlierness of traces or individual tasks, and we explain the superiority of our proposals over existing proximity and probability distribution fitting-based techniques yielding up to 2.05X higher F1 score. We also provide guidelines as to which variant to be chosen based on the type of anomalies targeted and the dataset characteristics.},
  archive      = {J_ML},
  author       = {Mavroudopoulos, Ioannis and Gounaris, Anastasios},
  doi          = {10.1007/s10994-022-06152-5},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4101-4128},
  shortjournal = {Mach. Learn.},
  title        = {A comparison of proximity-based methods for detecting temporal anomalies in business processes},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAED: Self-attentive energy disaggregation. <em>ML</em>,
<em>112</em>(11), 4081–4100. (<a
href="https://doi.org/10.1007/s10994-021-06106-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of energy disaggregation deals with the approximation of appliance electric consumption using only the aggregate consumption measurement of a mains meter. Recent research developments have used deep neural networks and outperformed previous methods based on Hidden Markov Models. On the other hand, deep learning models are computationally heavy and require huge amounts of data. The main objective of the current paper is to incorporate the attention mechanism into neural networks in order to reduce their computational complexity. For the attention mechanism two different versions are utilized, named Additive and Dot Attention. The experiments show that they perform on par, while the Dot mechanism is slightly faster. The two versions of self-attentive neural networks are compared against two state-of-the-art energy disaggregation deep learning models. The experimental results show that the proposed architecture achieves faster or equal training and inference time and with minor performance drop depending on the device or the dataset.},
  archive      = {J_ML},
  author       = {Virtsionis-Gkalinikis, Nikolaos and Nalmpantis, Christoforos and Vrakas, Dimitris},
  doi          = {10.1007/s10994-021-06106-3},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4081-4100},
  shortjournal = {Mach. Learn.},
  title        = {SAED: Self-attentive energy disaggregation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heuristic search of optimal machine teaching curricula.
<em>ML</em>, <em>112</em>(10), 4049–4080. (<a
href="https://doi.org/10.1007/s10994-023-06347-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In curriculum learning the order of concepts is determined by the teacher but not the examples for each concept, while in machine teaching it is the examples that are chosen by the teacher to minimise the learning effort, though the concepts are taught in isolation. Curriculum teaching is the natural combination of both, where both concept order and the set of examples can be chosen to minimise the size of the whole teaching session. Yet, this simultaneous minimisation of teaching sets and concept order is computationally challenging, facing issues such as the “interposition” phenomenon: previous knowledge may be counter-productive. We build on a machine-teaching framework based on simplicity priors that can achieve short teaching sizes for large classes of languages. Given a set of concepts, we identify an inequality relating the sizes of example sets and concept descriptions. This leverages the definition of admissible heuristics for A* search to spot the optimal curricula by avoiding interposition, being able to find the shortest teaching sessions in a more efficient way than an exhaustive search and with the guarantees we do not have with a greedy algorithm. We illustrate these theoretical findings through case studies in a drawing domain, polygonal strokes on a grid described by a simple language implementing compositionality and recursion.},
  archive      = {J_ML},
  author       = {Garcia-Piqueras, Manuel and Hernández-Orallo, Jose},
  doi          = {10.1007/s10994-023-06347-4},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {4049-4080},
  shortjournal = {Mach. Learn.},
  title        = {Heuristic search of optimal machine teaching curricula},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial concept drift detection under poisoning attacks
for robust data stream mining. <em>ML</em>, <em>112</em>(10), 4013–4048.
(<a href="https://doi.org/10.1007/s10994-022-06177-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous learning from streaming data is among the most challenging topics in the contemporary machine learning. In this domain, learning algorithms must not only be able to handle massive volume of rapidly arriving data, but also adapt themselves to potential emerging changes. The phenomenon of evolving nature of data streams is known as concept drift. While there is a plethora of methods designed for detecting its occurrence, all of them assume that the drift is connected with underlying changes in the source of data. However, one must consider the possibility of a malicious injection of false data that simulates a concept drift. This adversarial setting assumes a poisoning attack that may be conducted in order to damage the underlying classification system by forcing an adaptation to false data. Existing drift detectors are not capable of differentiating between real and adversarial concept drift. In this paper, we propose a framework for robust concept drift detection in the presence of adversarial and poisoning attacks. We introduce the taxonomy for two types of adversarial concept drifts, as well as a robust trainable drift detector. It is based on the augmented restricted Boltzmann machine with improved gradient computation and energy function. We also introduce Relative Loss of Robustness—a novel measure for evaluating the performance of concept drift detectors under poisoning attacks. Extensive computational experiments, conducted on both fully and sparsely labeled data streams, prove the high robustness and efficacy of the proposed drift detection framework in adversarial scenarios.},
  archive      = {J_ML},
  author       = {Korycki, Łukasz and Krawczyk, Bartosz},
  doi          = {10.1007/s10994-022-06177-w},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {4013-4048},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial concept drift detection under poisoning attacks for robust data stream mining},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metrics and methods for robustness evaluation of neural
networks with generative models. <em>ML</em>, <em>112</em>(10),
3977–4012. (<a
href="https://doi.org/10.1007/s10994-021-05994-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that modern deep neural network classifiers are easy to fool, assuming that an adversary is able to slightly modify their inputs. Many papers have proposed adversarial attacks, defenses and methods to measure robustness to such adversarial perturbations. However, most commonly considered adversarial examples are based on perturbations in the input space of the neural network that are unlikely to arise naturally. Recently, especially in computer vision, researchers discovered “natural” perturbations, such as rotations, changes of brightness, or more high-level changes, but these perturbations have not yet been systematically used to measure the performance of classifiers. In this paper, we propose several metrics to measure robustness of classifiers to natural adversarial examples, and methods to evaluate them. These metrics, called latent space performance metrics, are based on the ability of generative models to capture probability distributions. On four image classification case studies, we evaluate the proposed metrics for several classifiers, including ones trained in conventional and robust ways. We find that the latent counterparts of adversarial robustness are associated with the accuracy of the classifier rather than its conventional adversarial robustness, but the latter is still reflected on the properties of found latent perturbations. In addition, our novel method of finding latent adversarial perturbations demonstrates that these perturbations are often perceptually small.},
  archive      = {J_ML},
  author       = {Buzhinsky, Igor and Nerinovsky, Arseny and Tripakis, Stavros},
  doi          = {10.1007/s10994-021-05994-9},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3977-4012},
  shortjournal = {Mach. Learn.},
  title        = {Metrics and methods for robustness evaluation of neural networks with generative models},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor completion with noisy side information. <em>ML</em>,
<em>112</em>(10), 3945–3976. (<a
href="https://doi.org/10.1007/s10994-023-06338-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new model for tensor completion which incorporates noisy side information available on the rows and columns of a 3-dimensional tensor. This method learns a low rank representation of the data along with regression coefficients for the observed noisy features. Given this model, we propose an efficient alternating minimization algorithm to find high-quality solutions that scales to large data sets. Through extensive computational experiments, we demonstrate that this method leads to significant gains in out-of-sample accuracy filling in missing values in both simulated and real-world data. We consider the problem of imputing drug response in three large-scale anti-cancer drug screening data sets: the Genomics of Drug Sensitivity in Cancer (GDSC), the Cancer Cell Line Encyclopedia (CCLE), and the Genentech Cell Line Screening Initiative (GCSI). On imputation tasks with 20\% to 80\% missing data, we show that the proposed method TensorGenomic matches or outperforms state-of-the-art methods including the original tensor model and a multilevel mixed effects model. With 80\% missing data, TensorGenomic improves the $$R^2$$ from 0.404 to 0.552 in the GDSC data set, 0.407 to 0.524 in the CCLE data set, and 0.331 to 0.453 in the GCSI data set compared to the tensor model which does not take into account genomic side information.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Pawlowski, Colin},
  doi          = {10.1007/s10994-023-06338-5},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3945-3976},
  shortjournal = {Mach. Learn.},
  title        = {Tensor completion with noisy side information},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning logic programs by explaining their failures.
<em>ML</em>, <em>112</em>(10), 3917–3943. (<a
href="https://doi.org/10.1007/s10994-023-06358-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists form hypotheses and experimentally test them. If a hypothesis fails (is refuted), scientists try to explain the failure to eliminate other hypotheses. The more precise the failure analysis the more hypotheses can be eliminated. Thus inspired, we introduce failure explanation techniques for inductive logic programming. Given a hypothesis represented as a logic program, we test it on examples. If a hypothesis fails, we explain the failure in terms of failing sub-programs. In case a positive example fails, we identify failing sub-programs at the granularity of literals. We introduce a failure explanation algorithm based on analysing branches of SLD-trees. We integrate a meta-interpreter based implementation of this algorithm with the test-stage of the Popper ILP system. We show that fine-grained failure analysis allows for learning fine-grained constraints on the hypothesis space. Our experimental results show that explaining failures can drastically reduce hypothesis space exploration and learning times.},
  archive      = {J_ML},
  author       = {Morel, Rolf and Cropper, Andrew},
  doi          = {10.1007/s10994-023-06358-1},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3917-3943},
  shortjournal = {Mach. Learn.},
  title        = {Learning logic programs by explaining their failures},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid ensemble method with negative correlation learning
for regression. <em>ML</em>, <em>112</em>(10), 3881–3916. (<a
href="https://doi.org/10.1007/s10994-023-06364-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid ensemble, an essential branch of ensembles, has flourished in the regression field, with studies confirming diversity’s importance. However, previous ensembles consider diversity in the sub-model training stage, with limited improvement compared to single models. In contrast, this study automatically selects and weights sub-models from a heterogeneous model pool. It solves an optimization problem using an interior-point filtering linear-search algorithm. The objective function innovatively incorporates negative correlation learning as a penalty term, with which a diverse model subset can be selected. The best sub-models from each model class are selected to build the NCL ensemble, which performance is better than the simple average and other state-of-the-art weighting methods. It is also possible to improve the NCL ensemble with a regularization term in the objective function. In practice, it is difficult to conclude the optimal sub-model for a dataset prior due to the model uncertainty. Regardless, our method would achieve comparable accuracy as the potential optimal sub-models. In conclusion, the value of this study lies in its ease of use and effectiveness, allowing the hybrid ensemble to embrace diversity and accuracy.},
  archive      = {J_ML},
  author       = {Bai, Yun and Tian, Ganglin and Kang, Yanfei and Jia, Suling},
  doi          = {10.1007/s10994-023-06364-3},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3881-3916},
  shortjournal = {Mach. Learn.},
  title        = {A hybrid ensemble method with negative correlation learning for regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LADDER: Latent boundary-guided adversarial training.
<em>ML</em>, <em>112</em>(10), 3851–3879. (<a
href="https://doi.org/10.1007/s10994-022-06203-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have recently achieved great success in many classification tasks. Unfortunately, they are vulnerable to adversarial attacks that generate adversarial examples with a small perturbation to fool DNN models, especially in model sharing scenarios. Adversarial training is proved to be the most effective strategy that injects adversarial examples into model training to improve the robustness of DNN models against adversarial attacks. However, adversarial training based on the existing adversarial examples fails to generalize well to standard, unperturbed test data. To achieve a better trade-off between standard accuracy and adversarial robustness, we propose a novel adversarial training framework called LAtent bounDary-guided aDvErsarial tRaining (LADDER) that adversarially trains DNN models on latent boundary-guided adversarial examples. As opposed to most of the existing methods that generate adversarial examples in the input space, LADDER generates a myriad of high-quality adversarial examples through adding perturbations to latent features. The perturbations are made along the normal of the decision boundary constructed by an SVM with an attention mechanism. We analyze the merits of our generated boundary-guided adversarial examples from a boundary field perspective and visualization view. Extensive experiments and detailed analysis on MNIST, SVHN, CelebA, and CIFAR-10 validate the effectiveness of LADDER in achieving a better trade-off between standard accuracy and adversarial robustness as compared with vanilla DNNs and competitive baselines.},
  archive      = {J_ML},
  author       = {Zhou, Xiaowei and Tsang, Ivor W. and Yin, Jie},
  doi          = {10.1007/s10994-022-06203-x},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3851-3879},
  shortjournal = {Mach. Learn.},
  title        = {LADDER: Latent boundary-guided adversarial training},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scenic: A language for scenario specification and data
generation. <em>ML</em>, <em>112</em>(10), 3805–3849. (<a
href="https://doi.org/10.1007/s10994-021-06120-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new probabilistic programming language for the design and analysis of cyber-physical systems, especially those based on machine learning. We consider several problems arising in the design process, including training a system to be robust to rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs, then sampling these to generate specialized training and test data. More generally, such languages can be used to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems such as autonomous cars and robots, whose environment at any point in time is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes and the behaviors of their agents over time. Scenic combines concise, readable syntax for spatiotemporal relationships with the ability to declaratively impose hard and soft constraints over the scenario. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic’s domain-specific syntax. Finally, we apply Scenic in multiple case studies for training, testing, and debugging neural networks for perception both as standalone components and within the context of a full cyber-physical system.},
  archive      = {J_ML},
  author       = {Fremont, Daniel J. and Kim, Edward and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.},
  doi          = {10.1007/s10994-021-06120-5},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3805-3849},
  shortjournal = {Mach. Learn.},
  title        = {Scenic: A language for scenario specification and data generation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Byzantine-robust distributed sparse learning for
m-estimation. <em>ML</em>, <em>112</em>(10), 3773–3804. (<a
href="https://doi.org/10.1007/s10994-021-06001-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a distributed computing environment, there is usually a small fraction of machines that are corrupted and send arbitrary erroneous information to the master machine. This phenomenon is modeled as a Byzantine failure. Byzantine-robust distributed learning has recently become an important topic in machine learning research. In this paper, we develop a Byzantine-resilient method for the distributed sparse M-estimation problem. When the loss function is non-smooth, it is computationally costly to solve the penalized non-smooth optimization problem in a direct manner. To alleviate the computational burden, we construct a pseudo-response variable and transform the original problem into an $$\ell _1$$ -penalized least-squares problem, which is much more computationally feasible. Based on this idea, we develop a communication-efficient distributed algorithm. Theoretically, we show that the proposed estimator obtains a fast convergence rate with only a constant number of iterations. Furthermore, we establish a support recovery result, which, to the best of our knowledge, is the first such result in the literature of Byzantine-robust distributed learning. We demonstrate the effectiveness of our approach in simulation.},
  archive      = {J_ML},
  author       = {Tu, Jiyuan and Liu, Weidong and Mao, Xiaojun},
  doi          = {10.1007/s10994-021-06001-x},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3773-3804},
  shortjournal = {Mach. Learn.},
  title        = {Byzantine-robust distributed sparse learning for M-estimation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A moment-matching metric for latent variable generative
models. <em>ML</em>, <em>112</em>(10), 3749–3772. (<a
href="https://doi.org/10.1007/s10994-023-06340-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to assess the quality of a fitted model in unsupervised learning problems. Latent variable models, such as variational autoencoders and Gaussian mixture models, are often trained with likelihood-based approaches. In the scope of Goodhart’s law, when a metric becomes a target it ceases to be a good metric and therefore we should not use likelihood to assess the quality of the fit of these models. The solution we propose is a new metric for model comparison or regularization that relies on moments. The key idea is to study the difference between the data moments and the model moments using a matrix norm, such as the Frobenius norm. We first show how to use this new metric for model comparison and then for regularization. We show that our proposed metric is faster to compute and has a smaller variance than the commonly used procedure of drawing samples from the fitted distribution. We conclude this article with a demonstration of both applications and we discuss our findings and future work.},
  archive      = {J_ML},
  author       = {Beaulac, Cédric},
  doi          = {10.1007/s10994-023-06340-x},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3749-3772},
  shortjournal = {Mach. Learn.},
  title        = {A moment-matching metric for latent variable generative models},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian optimization with safety constraints: Safe and
automatic parameter tuning in robotics. <em>ML</em>, <em>112</em>(10),
3713–3747. (<a
href="https://doi.org/10.1007/s10994-021-06019-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the right tuning parameters for algorithms is a pravelent problem in machine learning that can significantly affect the performance of algorithms. Data-efficient optimization algorithms, such as Bayesian optimization, have been used to automate this process. During experiments on real-world systems such as robotic platforms these methods can evaluate unsafe parameters that lead to safety-critical system failures and can destroy the system. Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been developed, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is often not desirable in practice, since they are often opposing objectives. In this paper, we present a generalized algorithm that allows for multiple safety constraints separate from the objective. Given an initial set of safe parameters, the algorithm maximizes performance but only evaluates parameters that satisfy safety for all constraints with high probability. To this end, it carefully explores the parameter space by exploiting regularity assumptions in terms of a Gaussian process prior. Moreover, we show how context variables can be used to safely transfer knowledge to new situations and tasks. We provide a theoretical analysis and demonstrate that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters in experiments on a quadrotor vehicle.},
  archive      = {J_ML},
  author       = {Berkenkamp, Felix and Krause, Andreas and Schoellig, Angela P.},
  doi          = {10.1007/s10994-021-06019-1},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3713-3747},
  shortjournal = {Mach. Learn.},
  title        = {Bayesian optimization with safety constraints: Safe and automatic parameter tuning in robotics},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global optimization of objective functions represented by
ReLU networks. <em>ML</em>, <em>112</em>(10), 3685–3712. (<a
href="https://doi.org/10.1007/s10994-021-06050-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks can learn complex, non-convex functions, and it is challenging to guarantee their correct behavior in safety-critical contexts. Many approaches exist to find failures in networks (e.g., adversarial examples), but these cannot guarantee the absence of failures. Verification algorithms address this need and provide formal guarantees about a neural network by answering “yes or no” questions. For example, they can answer whether a violation exists within certain bounds. However, individual “yes or no&quot; questions cannot answer qualitative questions such as “what is the largest error within these bounds”; the answers to these lie in the domain of optimization. Therefore, we propose strategies to extend existing verifiers to perform optimization and find: (i) the most extreme failure in a given input region and (ii) the minimum input perturbation required to cause a failure. A naive approach using a bisection search with an off-the-shelf verifier results in many expensive and overlapping calls to the verifier. Instead, we propose an approach that tightly integrates the optimization process into the verification procedure, achieving better runtime performance than the naive approach. We evaluate our approach implemented as an extension of Marabou, a state-of-the-art neural network verifier, and compare its performance with the bisection approach and MIPVerify, an optimization-based verifier. We observe complementary performance between our extension of Marabou and MIPVerify.},
  archive      = {J_ML},
  author       = {Strong, Christopher A. and Wu, Haoze and Zeljić, Aleksandar and Julian, Kyle D. and Katz, Guy and Barrett, Clark and Kochenderfer, Mykel J.},
  doi          = {10.1007/s10994-021-06050-2},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3685-3712},
  shortjournal = {Mach. Learn.},
  title        = {Global optimization of objective functions represented by ReLU networks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning domain invariant representations of heterogeneous
image data. <em>ML</em>, <em>112</em>(10), 3659–3684. (<a
href="https://doi.org/10.1007/s10994-023-06374-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning requires a huge amount of reference data, which is often difficult and expensive to obtain. Domain adaptation helps with this problem—labelled data from one dataset should help in learning on another unlabelled or scarcely labelled dataset. In remote sensing, where a variety of sensors produce images of different modalities and with different numbers of channels, it would be very beneficial to develop heterogeneous domain adaptation methods that are able to work between domains that come from different input spaces. However, this challenging problem is rarely addressed, the majority of existing heterogeneous domain adaptation work does not use raw image-data, or they rely on translation from one domain to the other, therefore ignoring domain-invariant feature extraction approaches. This article proposes novel approaches for heterogeneous image domain adaptation for both the semi-supervised and unsupervised settings. These are based on extracting domain invariant features using deep adversarial learning. For the unsupervised domain adaptation case, the impact of pseudo-labelling is also investigated. We evaluate on two heterogeneous remote sensing datasets, one being RGB, and the other multispectral, for the task of land-cover patch classification, and also on a standard computer vision benchmark of RGB-depth map object classification. The results show that the proposed domain invariant approach consistently outperforms the competing methods based on image-to-image/feature translation, in both remote sensing and in a standard computer vision problem.},
  archive      = {J_ML},
  author       = {Obrenović, Mihailo and Lampert, Thomas and Ivanović, Miloš and Gançarski, Pierre},
  doi          = {10.1007/s10994-023-06374-1},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3659-3684},
  shortjournal = {Mach. Learn.},
  title        = {Learning domain invariant representations of heterogeneous image data},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spike2CGR: An efficient method for spike sequence
classification using chaos game representation. <em>ML</em>,
<em>112</em>(10), 3633–3658. (<a
href="https://doi.org/10.1007/s10994-023-06371-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological sequence classification is an essential task in many fields, such as machine learning, biology, and bioinformatics. Due to the spread of the coronavirus disease, numerous sequence data are available to researchers. As this virus has affected many hosts (e.g. bats, humans, chickens, etc.) and transformed into different lineages/variants (e.g., alpha, beta, gamma, and omicron, etc.), the biological sequence data for this virus is accompanied by the respective information about the affected host and lineage type of the sequences. Moreover, it is well known in the biology domain that many mutations (that happen disproportionally) related to the coronavirus are present in the spike protein region. Therefore, working with only spike sequences is usually sufficient rather than using a full-length genome for sequence analysis. For a spike sequence, this paper intends to design an image representation so that sophisticated image classification algorithms can be applied to perform the tasks of coronavirus lineages and host classifications. We propose a method based on the idea of chaos game representation (CGR), called Spike2CGR, which converts spike sequences into graphical form (images), and those images are used as input to deep learning (DL) models. We also use some domain knowledge from the biology field to design a few modified versions of Spike2CGR that are biologically meaningful and outperform the SOTA in terms of predictive performance. We use different DL models to perform coronavirus lineage and host classifications and report predictive results employing various evaluation metrics. Using two real-world datasets, we show that Spike2CGR outperforms the SOTA method in terms of predictive performance on spike sequences for variant and host classification.},
  archive      = {J_ML},
  author       = {Murad, Taslim and Ali, Sarwan and Khan, Imdadullah and Patterson, Murray},
  doi          = {10.1007/s10994-023-06371-4},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3633-3658},
  shortjournal = {Mach. Learn.},
  title        = {Spike2CGR: An efficient method for spike sequence classification using chaos game representation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explanatory machine learning for sequential human teaching.
<em>ML</em>, <em>112</em>(10), 3591–3632. (<a
href="https://doi.org/10.1007/s10994-023-06351-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of comprehensibility of machine-learned theories has recently drawn increasing attention. Inductive logic programming uses logic programming to derive logic theories from small data based on abduction and induction techniques. Learned theories are represented in the form of rules as declarative descriptions of obtained knowledge. In earlier work, the authors provided the first evidence of a measurable increase in human comprehension based on machine-learned logic rules for simple classification tasks. In a later study, it was found that the presentation of machine-learned explanations to humans can produce both beneficial and harmful effects in the context of game learning. We continue our investigation of comprehensibility by examining the effects of the ordering of concept presentations on human comprehension. In this work, we examine the explanatory effects of curriculum order and the presence of machine-learned explanations for sequential problem-solving. We show that (1) there exist tasks A and B such that learning A before learning B results in better comprehension for humans in comparison to learning B before learning A and (2) there exist tasks A and B such that the presence of explanations when learning A contributes to improved human comprehension when subsequently learning B. We propose a framework for the effects of sequential teaching on comprehension based on an existing definition of comprehensibility and provide evidence for support from data collected in human trials. Our empirical study involves curricula that teach novices the merge sort algorithm. Our results show that sequential teaching of concepts with increasing complexity (a) has a beneficial effect on human comprehension and (b) leads to human re-discovery of divide-and-conquer problem-solving strategies, and (c) allows adaptations of human problem-solving strategy with better performance when machine-learned explanations are also presented.},
  archive      = {J_ML},
  author       = {Ai, Lun and Langer, Johannes and Muggleton, Stephen H. and Schmid, Ute},
  doi          = {10.1007/s10994-023-06351-8},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3591-3632},
  shortjournal = {Mach. Learn.},
  title        = {Explanatory machine learning for sequential human teaching},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster riemannian newton-type optimization by subsampling
and cubic regularization. <em>ML</em>, <em>112</em>(9), 3527–3589. (<a
href="https://doi.org/10.1007/s10994-023-06321-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is on constrained large-scale non-convex optimization where the constraint set implies a manifold structure. Solving such problems is important in a multitude of fundamental machine learning tasks. Recent advances on Riemannian optimization have enabled the convenient recovery of solutions by adapting unconstrained optimization algorithms over manifolds. However, it remains challenging to scale up and meanwhile maintain stable convergence rates and handle saddle points. We propose a new second-order Riemannian optimization algorithm, aiming at improving convergence rate and reducing computational cost. It enhances the Riemannian trust-region algorithm that explores curvature information to escape saddle points through a mixture of subsampling and cubic regularization techniques. We conduct rigorous analysis to study the convergence behavior of the proposed algorithm. We also perform extensive experiments to evaluate it based on two general machine learning tasks using multiple datasets. The proposed algorithm exhibits improved computational speed, e.g., a speed improvement from $$12\% \:\text {to} \:227\%$$ , and improved convergence behavior, e.g., an iteration number reduction from $$\mathcal{O}\left(\max\left(\epsilon_g^{-2}\epsilon_H^{-1},\epsilon_H^{-3}\right)\right) \,\text {to}\: \mathcal{O}\left(\max\left(\epsilon_g^{-2},\epsilon_H^{-3}\right)\right)$$ , compared to a large set of state-of-the-art Riemannian optimization algorithms.},
  archive      = {J_ML},
  author       = {Deng, Yian and Mu, Tingting},
  doi          = {10.1007/s10994-023-06321-0},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3527-3589},
  shortjournal = {Mach. Learn.},
  title        = {Faster riemannian newton-type optimization by subsampling and cubic regularization},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive counterfactual visual explanations with
overdetermination. <em>ML</em>, <em>112</em>(9), 3497–3525. (<a
href="https://doi.org/10.1007/s10994-023-06333-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image seeks to explain an image’s classification probability by contrasting the image with a representative contrast image, such as an auto-generated image obtained via adversarial learning. This produces a salient segmentation and a way of using image perturbations to calculate each segment’s importance. CLEAR Image then uses regression to determine a causal equation describing a classifier’s local input–output behaviour. Counterfactuals are also identified that are supported by the causal equation. Finally, CLEAR Image measures the fidelity of its explanation against the classifier. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27\% using a novel pointing game metric. CLEAR Image also identifies cases of causal overdetermination, where there are multiple segments in an image that are sufficient individually to cause the classification probability to be close to one.},
  archive      = {J_ML},
  author       = {White, Adam and Ngan, Kwun Ho and Phelan, James and Ryan, Kevin and Afgeh, Saman Sadeghi and Reyes-Aldasoro, Constantino Carlos and Garcez, Artur d’Avila},
  doi          = {10.1007/s10994-023-06333-w},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3497-3525},
  shortjournal = {Mach. Learn.},
  title        = {Contrastive counterfactual visual explanations with overdetermination},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NaCL: Noise-robust cross-domain contrastive learning for
unsupervised domain adaptation. <em>ML</em>, <em>112</em>(9), 3473–3496.
(<a href="https://doi.org/10.1007/s10994-023-06343-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Unsupervised Domain Adaptation (UDA) methods aim to enhance feature transferability possibly at the expense of feature discriminability. Recently, contrastive representation learning has been applied to UDA as a promising approach. One way is to combine the mainstream domain adaptation method with contrastive self-supervised tasks. The other way uses contrastive learning to align class-conditional distributions according to the semantic structure information of source and target domains. Nevertheless, there are some limitations in two aspects. One is that optimal solutions for the contrastive self-supervised learning and the domain discrepancy minimization may not be consistent. The other is that contrastive learning uses pseudo label information of target domain to align class-conditional distributions, where the pseudo label information contains noise such that false positive and negative pairs would deteriorate the performance of contrastive learning. To address these issues, we propose Noise-robust cross-domain Contrastive Learning (NaCL) to directly realize the domain adaptation task via simultaneously learning the instance-wise discrimination and encoding semantic structures in intra- and inter-domain to the learned representation space. More specifically, we adopt topology-based selection on the target domain to detect and remove false positive and negative pairs in contrastive loss. Theoretically, we demonstrate that not only NaCL can be considered an example of Expectation Maximization (EM), but also accurate pseudo label information is beneficial for reducing the expected error on target domain. NaCL obtains superior results on three public benchmarks. Further, NaCL can also be applied to semi-supervised domain adaptation with only minor modifications, achieving advanced diagnostic performance on COVID-19 dataset. Code is available at https://github.com/jingzhengli/NaCL},
  archive      = {J_ML},
  author       = {Li, Jingzheng and Sun, Hailong},
  doi          = {10.1007/s10994-023-06343-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3473-3496},
  shortjournal = {Mach. Learn.},
  title        = {NaCL: Noise-robust cross-domain contrastive learning for unsupervised domain adaptation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpreting machine-learning models in transformed feature
space with an application to remote-sensing classification. <em>ML</em>,
<em>112</em>(9), 3455–3471. (<a
href="https://doi.org/10.1007/s10994-023-06327-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-agnostic tools for the post-hoc interpretation of machine-learning models struggle to summarize the joint effects of strongly dependent features in high-dimensional feature spaces, which play an important role in semantic image classification, for example in remote sensing of landcover. This contribution proposes a novel approach that interprets machine-learning models through the lens of feature-space transformations. It can be used to enhance unconditional as well as conditional post-hoc diagnostic tools including partial-dependence plots, accumulated local effects (ALE) plots, permutation feature importance, or Shapley additive explanations (SHAP). While the approach can also be applied to nonlinear transformations, linear ones are particularly appealing, especially principal component analysis (PCA) and a proposed partial orthogonalization technique. Moreover, structured PCA and model diagnostics along user-defined synthetic features offer opportunities for representing domain knowledge. The new approach is implemented in the R package wiml, which can be combined with existing explainable machine-learning packages. A case study on remote-sensing landcover classification with 46 features is used to demonstrate the potential of the proposed approach for model interpretation by domain experts. It is most useful in situations where groups of feature are linearly dependent and PCA can provide meaningful multivariate data summaries.},
  archive      = {J_ML},
  author       = {Brenning, Alexander},
  doi          = {10.1007/s10994-023-06327-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3455-3471},
  shortjournal = {Mach. Learn.},
  title        = {Interpreting machine-learning models in transformed feature space with an application to remote-sensing classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble and continual federated learning for classification
tasks. <em>ML</em>, <em>112</em>(9), 3413–3453. (<a
href="https://doi.org/10.1007/s10994-023-06330-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is the state-of-the-art paradigm for training a learning model collaboratively across multiple distributed devices while ensuring data privacy. Under this framework, different algorithms have been developed in recent years and have been successfully applied to real use cases. The vast majority of work in federated learning assumes static datasets and relies on the use of deep neural networks. However, in real-world problems, it is common to have a continual data stream, which may be non-stationary, leading to phenomena such as concept drift. Besides, there are many multi-device applications where other, non-deep strategies are more suitable, due to their simplicity, explainability, or generalizability, among other reasons. In this paper we present Ensemble and Continual Federated Learning, a federated architecture based on ensemble techniques for solving continual classification tasks. We propose the global federated model to be an ensemble, consisting of several independent learners, which are locally trained. Thus, we enable a flexible aggregation of heterogeneous client models, which may differ in size, structure, or even algorithmic family. This ensemble-based approach, together with drift detection and adaptation mechanisms, also allows for continual adaptation in situations where data distribution changes over time. In order to test our proposal and illustrate how it works, we have evaluated it in different tasks related to human activity recognition using smartphones.},
  archive      = {J_ML},
  author       = {Casado, Fernando E. and Lema, Dylan and Iglesias, Roberto and Regueiro, Carlos V. and Barro, Senén},
  doi          = {10.1007/s10994-023-06330-z},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3413-3453},
  shortjournal = {Mach. Learn.},
  title        = {Ensemble and continual federated learning for classification tasks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust estimation in regression and classification methods
for large dimensional data. <em>ML</em>, <em>112</em>(9), 3361–3411. (<a
href="https://doi.org/10.1007/s10994-023-06349-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical data analysis and machine learning heavily rely on error measures for regression, classification, and forecasting. Bregman divergence ( $${\text{BD}}$$ ) is a widely used family of error measures, but it is not robust to outlying observations or high leverage points in large- and high-dimensional datasets. In this paper, we propose a new family of robust Bregman divergences called “robust- $${\text{BD}}$$ ” that are less sensitive to data outliers. We explore their suitability for sparse large-dimensional regression models with incompletely specified response variable distributions and propose a new estimate called the “penalized robust- $${\text{BD}}$$ estimate” that achieves the same oracle property as ordinary non-robust penalized least-squares and penalized-likelihood estimates. We conduct extensive numerical experiments to evaluate the performance of the proposed penalized robust- $${\text{BD}}$$ estimate and compare it with classical approaches, and show that our proposed method improves on existing approaches. Finally, we analyze a real dataset to illustrate the practicality of our proposed method. Our findings suggest that the proposed method can be a useful tool for robust statistical data analysis and machine learning in the presence of outliers and large-dimensional data.},
  archive      = {J_ML},
  author       = {Zhang, Chunming and Zhu, Lixing and Shen, Yanbo},
  doi          = {10.1007/s10994-023-06349-2},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3361-3411},
  shortjournal = {Mach. Learn.},
  title        = {Robust estimation in regression and classification methods for large dimensional data},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Considerations when learning additive explanations for
black-box models. <em>ML</em>, <em>112</em>(9), 3333–3359. (<a
href="https://doi.org/10.1007/s10994-023-06335-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model’s prediction function in different ways. We use the concepts of main and total effects to anchor additive explanations, and quantitatively evaluate additive and non-additive explanations. Even though distilled explanations are generally the most accurate additive explanations, non-additive explanations such as tree explanations that explicitly model non-additive components tend to be even more accurate. Despite this, our user study showed that machine learning practitioners were better able to leverage additive explanations for various tasks. These considerations should be taken into account when considering which explanation to trust and use to explain black-box models.},
  archive      = {J_ML},
  author       = {Tan, Sarah and Hooker, Giles and Koch, Paul and Gordo, Albert and Caruana, Rich},
  doi          = {10.1007/s10994-023-06335-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3333-3359},
  shortjournal = {Mach. Learn.},
  title        = {Considerations when learning additive explanations for black-box models},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refining neural network predictions using background
knowledge. <em>ML</em>, <em>112</em>(9), 3293–3331. (<a
href="https://doi.org/10.1007/s10994-023-06310-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has shown learning systems can use logical background knowledge to compensate for a lack of labeled training data. Many methods work by creating a loss function that encodes this knowledge. However, often the logic is discarded after training, even if it is still helpful at test time. Instead, we ensure neural network predictions satisfy the knowledge by refining the predictions with an extra computation step. We introduce differentiable refinement functions that find a corrected prediction close to the original prediction. We study how to effectively and efficiently compute these refinement functions. Using a new algorithm called iterative local refinement (ILR), we combine refinement functions to find refined predictions for logical formulas of any complexity. ILR finds refinements on complex SAT formulas in significantly fewer iterations and frequently finds solutions where gradient descent can not. Finally, ILR produces competitive results in the MNIST addition task.},
  archive      = {J_ML},
  author       = {Daniele, Alessandro and van Krieken, Emile and Serafini, Luciano and van Harmelen, Frank},
  doi          = {10.1007/s10994-023-06310-3},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3293-3331},
  shortjournal = {Mach. Learn.},
  title        = {Refining neural network predictions using background knowledge},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROAD-r: The autonomous driving dataset with logical
requirements. <em>ML</em>, <em>112</em>(9), 3261–3291. (<a
href="https://doi.org/10.1007/s10994-023-06322-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have proven to be very powerful at computer vision tasks. However, they often exhibit unexpected behaviors, acting against background knowledge about the problem at hand. This calls for models (i) able to learn from requirements expressing such background knowledge, and (ii) guaranteed to be compliant with the requirements themselves. Unfortunately, the development of such models is hampered by the lack of real-world datasets equipped with formally specified requirements. In this paper, we introduce the ROad event Awareness Dataset with logical Requirements (ROAD-R), the first publicly available dataset for autonomous driving with requirements expressed as logical constraints. Given ROAD-R, we show that current state-of-the-art models often violate its logical constraints, and that it is possible to exploit them to create models that (i) have a better performance, and (ii) are guaranteed to be compliant with the requirements themselves.},
  archive      = {J_ML},
  author       = {Giunchiglia, Eleonora and Stoian, Mihaela Cătălina and Khan, Salman and Cuzzolin, Fabio and Lukasiewicz, Thomas},
  doi          = {10.1007/s10994-023-06322-z},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3261-3291},
  shortjournal = {Mach. Learn.},
  title        = {ROAD-R: The autonomous driving dataset with logical requirements},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classifier calibration: A survey on how to assess and
improve predicted class probabilities. <em>ML</em>, <em>112</em>(9),
3211–3260. (<a
href="https://doi.org/10.1007/s10994-023-06336-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides both an introduction to and a detailed overview of the principles and practice of classifier calibration. A well-calibrated classifier correctly quantifies the level of uncertainty or confidence associated with its instance-wise predictions. This is essential for critical applications, optimal decision making, cost-sensitive classification, and for some types of context change. Calibration research has a rich history which predates the birth of machine learning as an academic field by decades. However, a recent increase in the interest on calibration has led to new methods and the extension from binary to the multiclass setting. The space of options and issues to consider is large, and navigating it requires the right set of concepts and tools. We provide both introductory material and up-to-date technical details of the main concepts and methods, including proper scoring rules and other evaluation metrics, visualisation approaches, a comprehensive account of post-hoc calibration methods for binary and multiclass classification, and several advanced topics.},
  archive      = {J_ML},
  author       = {Silva Filho, Telmo and Song, Hao and Perello-Nieto, Miquel and Santos-Rodriguez, Raul and Kull, Meelis and Flach, Peter},
  doi          = {10.1007/s10994-023-06336-7},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3211-3260},
  shortjournal = {Mach. Learn.},
  title        = {Classifier calibration: A survey on how to assess and improve predicted class probabilities},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A3T: Accuracy aware adversarial training. <em>ML</em>,
<em>112</em>(9), 3191–3210. (<a
href="https://doi.org/10.1007/s10994-023-06341-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training has been empirically shown to be more prone to overfitting than standard training. The exact underlying reasons are still not fully understood. In this paper, we identify one cause of overfitting related to current practices of generating adversarial examples from misclassified samples. We show that, following current practice, adversarial examples from misclassified samples results in harder-to-classify samples than the original ones. This leads to a complex adjustment of the decision boundary during training and hence overfitting. To mitigate this issue, we propose A3T, an accuracy aware AT method that generate adversarial example differently for misclassified and correctly classified samples. We show that our approach achieves better generalization while maintaining comparable robustness to state-of-the-art AT methods on a wide range of computer vision, natural language processing, and tabular tasks.},
  archive      = {J_ML},
  author       = {Altinisik, Enes and Messaoud, Safa and Sencar, Husrev Taha and Chawla, Sanjay},
  doi          = {10.1007/s10994-023-06341-w},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3191-3210},
  shortjournal = {Mach. Learn.},
  title        = {A3T: Accuracy aware adversarial training},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning system parameters from turing patterns.
<em>ML</em>, <em>112</em>(9), 3151–3190. (<a
href="https://doi.org/10.1007/s10994-023-06334-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Turing mechanism describes the emergence of spatial patterns due to spontaneous symmetry breaking in reaction–diffusion processes and underlies many developmental processes. Identifying Turing mechanisms in biological systems defines a challenging problem. This paper introduces an approach to the prediction of Turing parameter values from observed Turing patterns. The parameter values correspond to a parametrized system of reaction–diffusion equations that generate Turing patterns as steady state. The Gierer–Meinhardt model with four parameters is chosen as a case study. A novel invariant pattern representation based on resistance distance histograms is employed, along with Wasserstein kernels, in order to cope with the highly variable arrangement of local pattern structure that depends on the initial conditions which are assumed to be unknown. This enables us to compute physically plausible distances between patterns, to compute clusters of patterns and, above all, model parameter prediction based on training data that can be generated by numerical model evaluation with random initial data: for small training sets, classical state-of-the-art methods including operator-valued kernels outperform neural networks that are applied to raw pattern data, whereas for large training sets the latter are more accurate. A prominent property of our approach is that only a single pattern is required as input data for model parameter predicion. Excellent predictions are obtained for single parameter values and reasonably accurate results for jointly predicting all four parameter values.},
  archive      = {J_ML},
  author       = {Schnörr, David and Schnörr, Christoph},
  doi          = {10.1007/s10994-023-06334-9},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3151-3190},
  shortjournal = {Mach. Learn.},
  title        = {Learning system parameters from turing patterns},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of mutual information in variational classifiers.
<em>ML</em>, <em>112</em>(9), 3105–3150. (<a
href="https://doi.org/10.1007/s10994-023-06337-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overfitting data is a well-known phenomenon related with the generation of a model that mimics too closely (or exactly) a particular instance of data, and may therefore fail to predict future observations reliably. In practice, this behaviour is controlled by various—sometimes based on heuristics—regularization techniques, which are motivated by upper bounds to the generalization error. In this work, we study the generalization error of classifiers relying on stochastic encodings which are trained on the cross-entropy loss, which is often used in deep learning for classification problems. We derive bounds to the generalization error showing that there exists a regime where the generalization error is bounded by the mutual information between input features and the corresponding representations in the latent space, which are randomly generated according to the encoding distribution. Our bounds provide an information-theoretic understanding of generalization in the so-called class of variational classifiers, which are regularized by a Kullback–Leibler (KL) divergence term. These results give theoretical grounds for the highly popular KL term in variational inference methods that was already recognized to act effectively as a regularization penalty. We further observe connections with well studied notions such as Variational Autoencoders, Information Dropout, Information Bottleneck and Boltzmann Machines. Finally, we perform numerical experiments on MNIST, CIFAR and other datasets and show that mutual information is indeed highly representative of the behaviour of the generalization error.},
  archive      = {J_ML},
  author       = {Vera, Matias and Rey Vega, Leonardo and Piantanida, Pablo},
  doi          = {10.1007/s10994-023-06337-6},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3105-3150},
  shortjournal = {Mach. Learn.},
  title        = {The role of mutual information in variational classifiers},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new large-scale learning algorithm for generalized
additive models. <em>ML</em>, <em>112</em>(9), 3077–3104. (<a
href="https://doi.org/10.1007/s10994-023-06339-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive model plays an important role in machine learning due to its flexibility and interpretability in the prediction function. However, solving large-scale additive models is a challenging task due to several difficulties. Until now, scaling up additive models is still an open problem. To address this challenging problem, in this paper, we propose a new doubly stochastic optimization algorithm for solving the generalized additive models (DSGAM). We first propose a generalized formulation of additive models without the orthogonal hypothesis on the basis function. After that, we propose a wrapper algorithm to optimize the generalized additive models. Importantly, we introduce a doubly stochastic gradient algorithm (DSG) to solve an inner subproblem in the wrapper algorithm, which can scale well in sample size and dimensionality simultaneously. Finally, we prove the fast convergence rate of our DSGAM algorithm. The experimental results on various large-scale benchmark datasets not only confirm the fast convergence of our DSGAM algorithm, but also show a huge reduction of computational time compared with existing algorithms, while retaining the similar generalization performance.},
  archive      = {J_ML},
  author       = {Gu, Bin and Zhang, Chenkang and Huo, Zhouyuan and Huang, Heng},
  doi          = {10.1007/s10994-023-06339-4},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3077-3104},
  shortjournal = {Mach. Learn.},
  title        = {A new large-scale learning algorithm for generalized additive models},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FairSwiRL: Fair semi-supervised classification with
representation learning. <em>ML</em>, <em>112</em>(9), 3051–3076. (<a
href="https://doi.org/10.1007/s10994-023-06342-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has shown its potential in many real-world applications where only few labeled examples are available. However, when some fairness constraints need to be satisfied, semi-supervised classification models often struggle as they are required to cope with the lack of sufficient information for predicting the target variable while forgetting its relationships with any sensitive and potentially discriminatory attribute. To address this issue, we propose a fair semi-supervised representation learning architecture that leads to fair and accurate classification results even in very challenging scenarios with few labeled (but biased) instances. We show experimentally that our model can be easily adopted in very general settings, as the learned representations may be employed to train any supervised classifier. Moreover, when applied to several synthetic and real-world datasets, our method is competitive with state-of-the-art fair semi-supervised approaches.},
  archive      = {J_ML},
  author       = {Yang, Shuyi and Cerrato, Mattia and Ienco, Dino and Pensa, Ruggero G. and Esposito, Roberto},
  doi          = {10.1007/s10994-023-06342-9},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3051-3076},
  shortjournal = {Mach. Learn.},
  title        = {FairSwiRL: Fair semi-supervised classification with representation learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Also for k-means: More data does not imply better
performance. <em>ML</em>, <em>112</em>(8), 3033–3050. (<a
href="https://doi.org/10.1007/s10994-023-06361-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arguably, a desirable feature of a learner is that its performance gets better with an increasing amount of training data, at least in expectation. This issue has received renewed attention in recent years and some curious and surprising findings have been reported on. In essence, these results show that more data does actually not necessarily lead to improved performance—worse even, performance can deteriorate. Clustering, however, has not been subjected to such kind of study up to now. This paper shows that k-means clustering, a ubiquitous technique in machine learning and data mining, suffers from the same lack of so-called monotonicity and can display deterioration in expected performance with increasing training set sizes. Our main, theoretical contributions prove that 1-means clustering is monotonic, while 2-means is not even weakly monotonic, i.e., the occurrence of nonmonotonic behavior persists indefinitely, beyond any training sample size. For larger k, the question remains open.},
  archive      = {J_ML},
  author       = {Loog, Marco and Krijthe, Jesse H. and Bicego, Manuele},
  doi          = {10.1007/s10994-023-06361-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3033-3050},
  shortjournal = {Mach. Learn.},
  title        = {Also for k-means: More data does not imply better performance},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lagrangian objective function leads to improved unforeseen
attack generalization. <em>ML</em>, <em>112</em>(8), 3003–3031. (<a
href="https://doi.org/10.1007/s10994-023-06348-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent improvements in deep learning models and their practical applications have raised concerns about the robustness of these models against adversarial examples. Adversarial training (AT) has been shown effective in reaching a robust model against the attack that is used during training. However, it usually fails against other attacks, i.e., the model overfits to the training attack scheme. In this paper, we propose a new method for generating adversarial perturbations during training that mitigates the mentioned issue. More specifically, we minimize the perturbation $$\ell _p$$ norm while maximizing the classification loss in the Lagrangian form to craft adversarial examples. We argue that crafting adversarial examples based on this scheme results in enhanced attack generalization in the learned model. We compare our final model robust accuracy with the closely related state-of-the-art AT methods against attacks that were not used during training. This comparison demonstrates that our average robust accuracy against unseen attacks is 5.9\% higher in the CIFAR-10 dataset and 3.2\% higher in the ImageNet-100 dataset than corresponding state-of-the-art methods. We also demonstrate that our attack is faster than other attack schemes that are designed for unseen attack generalization and conclude that the proposed method is feasible for large datasets. Our code is available at https://github.com/rohban-lab/Lagrangian_Unseen .},
  archive      = {J_ML},
  author       = {Azizmalayeri, Mohammad and Rohban, Mohammad Hossein},
  doi          = {10.1007/s10994-023-06348-3},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3003-3031},
  shortjournal = {Mach. Learn.},
  title        = {Lagrangian objective function leads to improved unforeseen attack generalization},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified SVM algorithm based on LS-DC loss. <em>ML</em>,
<em>112</em>(8), 2975–3002. (<a
href="https://doi.org/10.1007/s10994-021-05996-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, support vector machines (SVMs) have become a popular supervised machine learning model, and plenty of distinct algorithms are designed separately based on different KKT conditions of the SVM model for classification/regression with different losses, including convex and or nonconvex loss. In this paper, we propose an algorithm that can train different SVM models in a unified scheme. First, we introduce a definition of the least squares type of difference of convex loss (LS-DC) and show that the most commonly used losses in the SVM community are LS-DC loss or can be approximated by LS-DC loss. Based on the difference of convex algorithm (DCA), we then propose a unified algorithm called UniSVM which can solve the SVM model with any convex or nonconvex LS-DC loss, wherein only a vector is computed by the specifically chosen loss. UniSVM has a dominant advantage over all existing algorithms for training robust SVM models with nonconvex losses because it has a closed-form solution per iteration, while the existing algorithms always need to solve an L1SVM/L2SVM per iteration. Furthermore, by the low-rank approximation of the kernel matrix, UniSVM can solve large-scale nonlinear problems efficiently. To verify the efficacy and feasibility of the proposed algorithm, we perform many experiments on small artificial problems and large benchmark tasks both with and without outliers for classification and regression for comparison with state-of-the-art algorithms. The experimental results demonstrate that UniSVM can achieve comparable performance in less training time. The foremost advantage of UniSVM is that its core code in Matlab is less than 10 lines; hence, it can be easily grasped by users or researchers.},
  archive      = {J_ML},
  author       = {Zhou, Shuisheng and Zhou, Wendi},
  doi          = {10.1007/s10994-021-05996-7},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2975-3002},
  shortjournal = {Mach. Learn.},
  title        = {Unified SVM algorithm based on LS-DC loss},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MapFlow: Latent transition via normalizing flow for
unsupervised domain adaptation. <em>ML</em>, <em>112</em>(8), 2953–2974.
(<a href="https://doi.org/10.1007/s10994-023-06357-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims at enhancing the generalizability of the classification model learned from the labeled source domain to an unlabeled target domain. An established approach to UDA is to constrain the classifier on an intermediate representation that is distributionally invariant across domains. However, recent theoretical and empirical research has revealed that relying only on invariance fails to guarantee a small target error, thus making equality in the distribution of representations unnecessary. In this paper, we propose to relax invariant representation learning by finding a general relationship between the source and target representations, which allows an interchange of the more discriminative domain information. To this end, we formalize the MapFlow framework, which explicitly constructs an invertible mapping between the target encoded distribution and variationally induced source representation. Empirical results on public benchmark datasets show the desirable performance of our proposed algorithm compared to state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Askari, Hossein and Latif, Yasir and Sun, Hongfu},
  doi          = {10.1007/s10994-023-06357-2},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2953-2974},
  shortjournal = {Mach. Learn.},
  title        = {MapFlow: Latent transition via normalizing flow for unsupervised domain adaptation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diametrical risk minimization: Theory and computations.
<em>ML</em>, <em>112</em>(8), 2933–2951. (<a
href="https://doi.org/10.1007/s10994-021-06036-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theoretical and empirical performance of Empirical Risk Minimization (ERM) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to ERM called Diametrical Risk Minimization (DRM), which accounts for worst-case empirical risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it can be implemented using a practical algorithm based on stochastic gradient descent. Numerical results illustrate the ability of DRM to find quality solutions with low generalization error in sharp empirical risk landscapes from benchmark neural network classification problems with corrupted labels.},
  archive      = {J_ML},
  author       = {Norton, Matthew D. and Royset, Johannes O.},
  doi          = {10.1007/s10994-021-06036-0},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2933-2951},
  shortjournal = {Mach. Learn.},
  title        = {Diametrical risk minimization: Theory and computations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating probabilistic safety guarantees for neural
network controllers. <em>ML</em>, <em>112</em>(8), 2903–2931. (<a
href="https://doi.org/10.1007/s10994-021-06065-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks serve as effective controllers in a variety of complex settings due to their ability to represent expressive policies. The complex nature of neural networks, however, makes their output difficult to verify and predict, which limits their use in safety-critical applications. While simulations provide insight into the performance of neural network controllers, they are not enough to guarantee that the controller will perform safely in all scenarios. To address this problem, recent work has focused on formal methods to verify properties of neural network outputs. For neural network controllers, we can use a dynamics model to determine the output properties that must hold for the controller to operate safely. In this work, we develop a method to use the results from neural network verification tools to provide probabilistic safety guarantees on a neural network controller. We develop an adaptive verification approach to efficiently generate an overapproximation of the neural network policy. Next, we modify the traditional formulation of Markov decision process model checking to provide guarantees on the overapproximated policy given a stochastic dynamics model. Finally, we incorporate techniques in state abstraction to reduce overapproximation error during the model checking process. We show that our method is able to generate meaningful probabilistic safety guarantees for aircraft collision avoidance neural networks that are loosely inspired by Airborne Collision Avoidance System X (ACAS X), a family of collision avoidance systems that formulates the problem as a partially observable Markov decision process (POMDP).},
  archive      = {J_ML},
  author       = {Katz, Sydney M. and Julian, Kyle D. and Strong, Christopher A. and Kochenderfer, Mykel J.},
  doi          = {10.1007/s10994-021-06065-9},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2903-2931},
  shortjournal = {Mach. Learn.},
  title        = {Generating probabilistic safety guarantees for neural network controllers},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted neural tangent kernel: A generalized and improved
network-induced kernel. <em>ML</em>, <em>112</em>(8), 2871–2901. (<a
href="https://doi.org/10.1007/s10994-023-06356-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural tangent kernel (NTK) has recently attracted intense study, as it describes the evolution of an over-parameterized neural network (NN) trained by gradient descent. However, it is now well-known that gradient descent is not always a good optimizer for NNs, which can partially explain the unsatisfactory practical performance of the NTK regression estimator. In this paper, we introduce the weighted neural tangent kernel (WNTK), a generalized and improved tool, which can capture an over-parameterized NN’s training dynamics under adjusted gradient descent direction. Theoretically, in the infinite-width limit, we prove: (1) the stability of the WNTK at initialization and during training, and (2) the equivalence between the WNTK regression estimator and the corresponding NN estimator with different learning rates on different parameters. With the proposed weight update algorithm, weight terms, or equivalently NN descent directions, can be trained through multiple-kernel optimization. Both empirical and analytical WNTKs outperform the corresponding NTKs in numerical experiments, coinciding with the fact that adjusted gradient descent could outperform original gradient descent in NNs’ training.},
  archive      = {J_ML},
  author       = {Tan, Lei and Wu, Shutong and Zhou, Wenxing and Huang, Xiaolin},
  doi          = {10.1007/s10994-023-06356-3},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2871-2901},
  shortjournal = {Mach. Learn.},
  title        = {Weighted neural tangent kernel: A generalized and improved network-induced kernel},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mirror variational transport: A particle-based algorithm for
distributional optimization on constrained domains. <em>ML</em>,
<em>112</em>(8), 2845–2869. (<a
href="https://doi.org/10.1007/s10994-023-06350-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the optimization problem of minimizing an objective functional, which admits a variational form and is defined over probability distributions on a constrained domain, which poses challenges to both theoretical analysis and algorithmic design. We propose Mirror Variational Transport (mirrorVT), which uses a set of samples, or particles, to represent the approximating distribution and deterministically updates the particles to optimize the functional. To deal with the constrained domain, in each iteration, mirrorVT maps the particles to an unconstrained dual domain, induced by a mirror map, and then approximately performs Wasserstein Gradient Descent on the manifold of distributions defined over the dual space to update each particle by a specified direction. At the end of each iteration, particles are mapped back to the original constrained domain. Through experiments on synthetic and real world data sets, we demonstrate the effectiveness of mirrorVT for the distributional optimization on the constrained domain. We also analyze its theoretical properties and characterize its convergence to the global minimum of the objective functional.},
  archive      = {J_ML},
  author       = {Nguyen, Dai Hai and Sakurai, Tetsuya},
  doi          = {10.1007/s10994-023-06350-9},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2845-2869},
  shortjournal = {Mach. Learn.},
  title        = {Mirror variational transport: A particle-based algorithm for distributional optimization on constrained domains},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable learning of matricized DNFs and its
application to boolean networks. <em>ML</em>, <em>112</em>(8),
2821–2843. (<a
href="https://doi.org/10.1007/s10994-023-06346-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean networks (BNs) are well-studied models of genomic regulation in biology where nodes are genes and their state transition is controlled by Boolean functions. We propose to learn Boolean functions as Boolean formulas in disjunctive normal form (DNFs) by an explainable neural network Mat_DNF and apply it to learning BNs. Directly expressing DNFs as a pair of binary matrices, we learn them using a single layer NN by minimizing a logically inspired non-negative cost function to zero. As a result, every parameter in the network has a clear meaning of representing a conjunction or literal in the learned DNF. Also we can prove that learning DNFs by the proposed approach is equivalent to inferring interpolants in logic between the positive and negative data. We applied our approach to learning three literature-curated BNs and confirmed its effectiveness. We also examine how generalization occurs when learning data is scarce. In doing so, we introduce two new operations that can improve accuracy, or equivalently generalizability for scarce data. The first one is to append a noise vector to the input learning vector. The second one is to continue learning even after learning error becomes zero. The first one is explainable by the second one. These two operations help us choose a learnable DNF, i.e., a root of the cost function, to achieve high generalizability.},
  archive      = {J_ML},
  author       = {Sato, Taisuke and Inoue, Katsumi},
  doi          = {10.1007/s10994-023-06346-5},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2821-2843},
  shortjournal = {Mach. Learn.},
  title        = {Differentiable learning of matricized DNFs and its application to boolean networks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized bayesian learning with metropolis-adjusted
hamiltonian monte carlo. <em>ML</em>, <em>112</em>(8), 2791–2819. (<a
href="https://doi.org/10.1007/s10994-023-06345-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning performed by a decentralized networks of agents is becoming increasingly important with the prevalence of embedded software on autonomous devices. Bayesian approaches to learning benefit from offering more information as to the uncertainty of a random quantity, and Langevin and Hamiltonian methods are effective at realizing sampling from an uncertain distribution with large parameter dimensions. Such methods have only recently appeared in the decentralized setting, and either exclusively use stochastic gradient Langevin and Hamiltonian Monte Carlo approaches that require a diminishing stepsize to asymptotically sample from the posterior and are known in practice to characterize uncertainty less faithfully than constant step-size methods with a Metropolis adjustment, or assume strong convexity properties of the potential function. We present the first approach to incorporating constant stepsize Metropolis-adjusted HMC in the decentralized sampling framework, show theoretical guarantees for consensus and probability distance to the posterior stationary distribution, and demonstrate their effectiveness numerically on standard real world problems, including decentralized learning of neural networks which is known to be highly non-convex.},
  archive      = {J_ML},
  author       = {Kungurtsev, Vyacheslav and Cobb, Adam and Javidi, Tara and Jalaian, Brian},
  doi          = {10.1007/s10994-023-06345-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2791-2819},
  shortjournal = {Mach. Learn.},
  title        = {Decentralized bayesian learning with metropolis-adjusted hamiltonian monte carlo},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on robust machine learning.
<em>ML</em>, <em>112</em>(8), 2787–2789. (<a
href="https://doi.org/10.1007/s10994-021-06113-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Senanayake, Ransalu and Fremont, Daniel J. and Kochenderfer, Mykel J. and Lomuscio, Alessio R. and Margineantu, Dragos and Ong, Cheng Soon},
  doi          = {10.1007/s10994-021-06113-4},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2787-2789},
  shortjournal = {Mach. Learn.},
  title        = {Guest editorial: Special issue on robust machine learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAC-fed: Federated adaptation for fairness and concept drift
aware stream classification. <em>ML</em>, <em>112</em>(8), 2761–2786.
(<a href="https://doi.org/10.1007/s10994-023-06360-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is an emerging collaborative learning paradigm of Machine learning involving distributed and heterogeneous clients. Enormous collections of continuously arriving heterogeneous data residing on distributed clients require federated adaptation of efficient mining algorithms to enable fair and high-quality predictions with privacy guarantees and minimal response delay. In this context, we propose a federated adaptation that mitigates discrimination embedded in the streaming data while handling concept drifts (FAC-Fed). We present a novel adaptive data augmentation method that mitigates client-side discrimination embedded in the data during optimization, resulting in an optimized and fair centralized server. Extensive experiments on a set of publicly available streaming and static datasets confirm the effectiveness of the proposed method. To the best of our knowledge, this work is the first attempt towards fairness-aware federated adaptation for stream classification, therefore, to prove the superiority of our proposed method over state-of-the-art, we compare the centralized version of our proposed method with three centralized stream classification baseline models (FABBOO, FAHT, CSMOTE). The experimental results show that our method outperforms the current methods in terms of both discrimination mitigation and predictive performance.},
  archive      = {J_ML},
  author       = {Badar, Maryam and Nejdl, Wolfgang and Fisichella, Marco},
  doi          = {10.1007/s10994-023-06360-7},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2761-2786},
  shortjournal = {Mach. Learn.},
  title        = {FAC-fed: Federated adaptation for fairness and concept drift aware stream classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust matrix estimations meet frank–wolfe algorithm.
<em>ML</em>, <em>112</em>(7), 2723–2760. (<a
href="https://doi.org/10.1007/s10994-023-06325-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimating matrix-valued model parameters with a dedicated focus on their robustness. Our setting concerns large-scale structured data so that a regularization on the matrix’s rank becomes indispensable. Though robust loss functions are expected to be effective, their practical implementations are known difficult due to the non-smooth criterion functions encountered in the optimizations. To meet the challenges, we develop a highly efficient computing scheme taking advantage of the projection-free Frank–Wolfe algorithms that require only the first-order derivative of the criterion function. Our methodological framework is broad, extensively accommodating robust loss functions in conjunction with penalty functions in the context of matrix estimation problems. We establish the non-asymptotic error bounds of the matrix estimations with the Huber loss and nuclear norm penalty in two concrete cases: matrix completion with partial and noisy observations and reduced-rank regressions. Our theory demonstrates the merits from using robust loss functions, so that matrix-valued estimators with good properties are achieved even when heavy-tailed distributions are involved. We illustrate the promising performance of our methods with extensive numerical examples and data analysis.},
  archive      = {J_ML},
  author       = {Jing, Naimin and Fang, Ethan X. and Tang, Cheng Yong},
  doi          = {10.1007/s10994-023-06325-w},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2723-2760},
  shortjournal = {Mach. Learn.},
  title        = {Robust matrix estimations meet Frank–Wolfe algorithm},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adversarial neural networks for domain
generalization: When it works and how to improve. <em>ML</em>,
<em>112</em>(7), 2685–2721. (<a
href="https://doi.org/10.1007/s10994-023-06324-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretically, domain adaptation is a well-researched problem. Further, this theory has been well-used in practice. In particular, we note the bound on target error given by Ben-David et al. (Mach Learn 79(1–2):151–175, 2010) and the well-known domain-aligning algorithm based on this work using Domain Adversarial Neural Networks (DANN) presented by Ganin and Lempitsky (in International conference on machine learning, pp 1180–1189). Recently, multiple variants of DANN have been proposed for the related problem of domain generalization, but without much discussion of the original motivating bound. In this paper, we investigate the validity of DANN in domain generalization from this perspective. We investigate conditions under which application of DANN makes sense and further consider DANN as a dynamic process during training. Our investigation suggests that the application of DANN to domain generalization may not be as straightforward as it seems. To address this, we design an algorithmic extension to DANN in the domain generalization case. Our experimentation validates both theory and algorithm.},
  archive      = {J_ML},
  author       = {Sicilia, Anthony and Zhao, Xingchen and Hwang, Seong Jae},
  doi          = {10.1007/s10994-023-06324-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2685-2721},
  shortjournal = {Mach. Learn.},
  title        = {Domain adversarial neural networks for domain generalization: When it works and how to improve},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pruning during training by network efficacy modeling.
<em>ML</em>, <em>112</em>(7), 2653–2684. (<a
href="https://doi.org/10.1007/s10994-023-06304-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are costly to train. Pruning, an approach to alleviate model complexity by zeroing out or pruning DNN elements, has shown promise in reducing training costs for DNNs with little to no efficacy at a given task. This paper presents a novel method to perform early pruning of DNN elements (e.g., neurons or convolutional filters) during the training process while minimizing losses to model performance. To achieve this, we model the efficacy of DNN elements in a Bayesian manner conditioned upon efficacy data collected during the training and prune DNN elements with low predictive efficacy after training completion. Empirical evaluations show that the proposed Bayesian early pruning improves the computational efficiency of DNN training while better preserving model performance compared to other tested pruning approaches.},
  archive      = {J_ML},
  author       = {Rajpal, Mohit and Zhang, Yehong and Low, Bryan Kian Hsiang},
  doi          = {10.1007/s10994-023-06304-1},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2653-2684},
  shortjournal = {Mach. Learn.},
  title        = {Pruning during training by network efficacy modeling},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing classifier overconfidence against adversaries
through graph algorithms. <em>ML</em>, <em>112</em>(7), 2619–2651. (<a
href="https://doi.org/10.1007/s10994-023-06307-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we show that deep learning classifiers tend to become overconfident in their answers under adversarial attacks, even when the classifier is optimized to survive such attacks. Our work draws upon stochastic geometry and graph algorithms to propose a general framework to replace the last fully connected layer and softmax output. This framework (a) can be applied to any classifier and (b) significantly reduces the classifier’s overconfidence in its output without much of an impact on its accuracy when compared to original adversarially-trained classifiers. Its relative effectiveness increases as the attacker becomes more powerful. Our use of graph algorithms in adversarial learning is new and of independent interest. Finally, we show the advantages of this last-layer softmax replacement over image tasks under common adversarial attacks.},
  archive      = {J_ML},
  author       = {Teixeira, Leonardo and Jalaian, Brian and Ribeiro, Bruno},
  doi          = {10.1007/s10994-023-06307-y},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2619-2651},
  shortjournal = {Mach. Learn.},
  title        = {Reducing classifier overconfidence against adversaries through graph algorithms},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse reinforcement learning through logic constraint
inference. <em>ML</em>, <em>112</em>(7), 2593–2618. (<a
href="https://doi.org/10.1007/s10994-023-06311-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous robots start to be integrated in human environments where explicit and implicit social norms guide the behavior of all agents. To assure safety and predictability, these artificial agents should act in accordance with the applicable social norms. However, it is not straightforward to define these rules and incorporate them in an agent’s policy. Particularly because social norms are often implicit and environment specific. In this paper, we propose a novel iterative approach to extract a set of rules from observed human trajectories. This hybrid method combines the strengths of inverse reinforcement learning and inductive logic programming. We experimentally show how our method successfully induces a compact logic program which represents the behavioral constraints applicable in a Tower of Hanoi and a traffic simulator environment. The induced program is adopted as prior knowledge by a model-free reinforcement learning agent to speed up training and prevent any social norm violation during exploration and deployment. Moreover, expressing norms as a logic program provides improved interpretability, which is an important pillar in the design of safe artificial agents, as well as transferability to similar environments.},
  archive      = {J_ML},
  author       = {Baert, Mattijs and Leroux, Sam and Simoens, Pieter},
  doi          = {10.1007/s10994-023-06311-2},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2593-2618},
  shortjournal = {Mach. Learn.},
  title        = {Inverse reinforcement learning through logic constraint inference},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SETAR-tree: A novel and accurate tree algorithm for global
time series forecasting. <em>ML</em>, <em>112</em>(7), 2555–2591. (<a
href="https://doi.org/10.1007/s10994-023-06316-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold Autoregressive (TAR) models have been widely used by statisticians for non-linear time series forecasting during the past few decades, due to their simplicity and mathematical properties. On the other hand, in the forecasting community, general-purpose tree-based regression algorithms (forests, gradient-boosting) have become popular recently due to their ease of use and accuracy. In this paper, we explore the close connections between TAR models and regression trees. These enable us to use the rich methodology from the literature on TAR models to define a hierarchical TAR model as a regression tree that trains globally across series, which we call SETAR-Tree. In contrast to the general-purpose tree-based models that do not primarily focus on forecasting, and calculate averages at the leaf nodes, we introduce a new forecasting-specific tree algorithm that trains global Pooled Regression (PR) models in the leaves allowing the models to learn cross-series information and also uses some time-series-specific splitting and stopping procedures. The depth of the tree is controlled by conducting a statistical linearity test commonly employed in TAR models, as well as measuring the error reduction percentage at each node split. Thus, the proposed tree model requires minimal external hyperparameter tuning and provides competitive results under its default configuration. We also use this tree algorithm to develop a forest where the forecasts provided by a collection of diverse SETAR-Trees are combined during the forecasting process. In our evaluation on eight publicly available datasets, the proposed tree and forest models are able to achieve significantly higher accuracy than a set of state-of-the-art tree-based algorithms and forecasting benchmarks across four evaluation metrics.},
  archive      = {J_ML},
  author       = {Godahewa, Rakshitha and Webb, Geoffrey I. and Schmidt, Daniel and Bergmeir, Christoph},
  doi          = {10.1007/s10994-023-06316-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2555-2591},
  shortjournal = {Mach. Learn.},
  title        = {SETAR-tree: A novel and accurate tree algorithm for global time series forecasting},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the discrepancy between kleinberg’s clustering axioms and
k-means clustering algorithm behavior. <em>ML</em>, <em>112</em>(7),
2501–2553. (<a
href="https://doi.org/10.1007/s10994-023-06308-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper performs an investigation of Kleinberg’s axioms (from both an intuitive and formal standpoint) as they relate to the well-known k-mean clustering method. The axioms, as well as a novel variations thereof, are analyzed in Euclidean space. A few natural properties are proposed, resulting in k-means satisfying the intuition behind Kleinberg’s axioms (or, rather, a small, and natural variation on that intuition). In particular, two variations of Kleinberg’s consistency property are proposed, called centric consistency and motion consistency. It is shown that these variations of consistency are satisfied by k-means.},
  archive      = {J_ML},
  author       = {Kłopotek, Mieczysław Alojzy and Kłopotek, Robert Albert},
  doi          = {10.1007/s10994-023-06308-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2501-2553},
  shortjournal = {Mach. Learn.},
  title        = {On the discrepancy between kleinberg’s clustering axioms and k-means clustering algorithm behavior},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse learning in hilbert scales. <em>ML</em>,
<em>112</em>(7), 2469–2499. (<a
href="https://doi.org/10.1007/s10994-022-06284-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study linear ill-posed inverse problems with noisy data in the framework of statistical learning. The corresponding linear operator equation is assumed to fit a given Hilbert scale, generated by some unbounded self-adjoint operator. Approximate reconstructions from random noisy data are obtained with general regularization schemes in such a way that these belong to the domain of the generator. The analysis has thus to distinguish two cases, the regular one, when the true solution also belongs to the domain of the generator, and the ‘oversmoothing’ one, when this is not the case. Rates of convergence for the regularized solutions will be expressed in terms of certain distance functions. For solutions with smoothness given in terms of source conditions with respect to the scale generating operator, then the error bounds can then be made explicit in terms of the sample size.},
  archive      = {J_ML},
  author       = {Rastogi, Abhishake and Mathé, Peter},
  doi          = {10.1007/s10994-022-06284-8},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2469-2499},
  shortjournal = {Mach. Learn.},
  title        = {Inverse learning in hilbert scales},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the sample complexity of actor-critic method for
reinforcement learning with function approximation. <em>ML</em>,
<em>112</em>(7), 2433–2467. (<a
href="https://doi.org/10.1007/s10994-023-06303-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning, mathematically described by Markov Decision Problems, may be approached either through dynamic programming or policy search. Actor-critic algorithms combine the merits of both approaches by alternating between steps to estimate the value function and policy gradient updates. Due to the fact that the updates exhibit correlated noise and biased gradient updates, only the asymptotic behavior of actor-critic is known by connecting its behavior to dynamical systems. This work puts forth a new variant of actor-critic that employs Monte Carlo rollouts during the policy search updates, which results in controllable bias that depends on the number of critic evaluations. As a result, we are able to provide for the first time the convergence rate of actor-critic algorithms when the policy search step employs policy gradient, agnostic to the choice of policy evaluation technique. In particular, we establish conditions under which the sample complexity is comparable to stochastic gradient method for non-convex problems or slower as a result of the critic estimation error, which is the main complexity bottleneck. These results hold in continuous state and action spaces with linear function approximation for the value function. We then specialize these conceptual results to the case where the critic is estimated by Temporal Difference, Gradient Temporal Difference, and Accelerated Gradient Temporal Difference. These learning rates are then corroborated on a navigation problem involving an obstacle and the pendulum problem which provide insight into the interplay between optimization and generalization in reinforcement learning.},
  archive      = {J_ML},
  author       = {Kumar, Harshat and Koppel, Alec and Ribeiro, Alejandro},
  doi          = {10.1007/s10994-023-06303-2},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2433-2467},
  shortjournal = {Mach. Learn.},
  title        = {On the sample complexity of actor-critic method for reinforcement learning with function approximation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised discretization by two-dimensional MDL-based
histogram. <em>ML</em>, <em>112</em>(7), 2397–2431. (<a
href="https://doi.org/10.1007/s10994-022-06294-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised discretization is a crucial step in many knowledge discovery tasks. The state-of-the-art method for one-dimensional data infers locally adaptive histograms using the minimum description length (MDL) principle, but the multi-dimensional case is far less studied: current methods consider the dimensions one at a time (if not independently), which result in discretizations based on rectangular cells of adaptive size. Unfortunately, this approach is unable to adequately characterize dependencies among dimensions and/or results in discretizations consisting of more cells (or bins) than is desirable. To address this problem, we propose an expressive model class that allows for far more flexible partitions of two-dimensional data. We extend the state of the art for the one-dimensional case to obtain a model selection problem based on the normalized maximum likelihood, a form of refined MDL. As the flexibility of our model class comes at the cost of a vast search space, we introduce a heuristic algorithm, named PALM, which partitions each dimension alternately and then merges neighboring regions, all using the MDL principle. Experiments on synthetic data show that PALM (1) accurately reveals ground truth partitions that are within the model class (i.e., the search space), given a large enough sample size; (2) approximates well a wide range of partitions outside the model class; (3) converges, in contrast to the state-of-the-art multivariate discretization method IPD. Finally, we apply our algorithm to three spatial datasets, and we demonstrate that, compared to kernel density estimation (KDE), our algorithm not only reveals more detailed density changes, but also fits unseen data better, as measured by the log-likelihood.},
  archive      = {J_ML},
  author       = {Yang, Lincen and Baratchi, Mitra and van Leeuwen, Matthijs},
  doi          = {10.1007/s10994-022-06294-6},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2397-2431},
  shortjournal = {Mach. Learn.},
  title        = {Unsupervised discretization by two-dimensional MDL-based histogram},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diverse and consistent multi-view networks for
semi-supervised regression. <em>ML</em>, <em>112</em>(7), 2359–2395. (<a
href="https://doi.org/10.1007/s10994-023-06305-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label collection is costly in many applications, which poses the need for label-efficient learning. In this work, we present Diverse and Consistent Multi-view Networks (DiCoM)—a novel semi-supervised regression technique based on a multi-view learning framework. DiCoM combines diversity with consistency—two seemingly opposing yet complementary principles of multi-view learning—based on underlying probabilistic graphical assumptions. Given multiple deep views of the same input, DiCoM encourages a negative correlation among the views’ predictions on labeled data, while simultaneously enforces their agreement on unlabeled data. DiCoM can utilize either multi-network or multi-branch architectures to make a trade-off between computational cost and modeling performance. Under realistic evaluation setups, DiCoM outperforms competing methods on tabular, time series and image data. Our ablation studies confirm the importance of having both consistency and diversity.},
  archive      = {J_ML},
  author       = {Nguyen, Cuong and Raja, Arun and Zhang, Le and Xu, Xun and Unnikrishnan, Balagopal and Ragab, Mohamed and Lu, Kangkang and Foo, Chuan-Sheng},
  doi          = {10.1007/s10994-023-06305-0},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2359-2395},
  shortjournal = {Mach. Learn.},
  title        = {Diverse and consistent multi-view networks for semi-supervised regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to increase the power of conditional randomization
tests. <em>ML</em>, <em>112</em>(7), 2317–2357. (<a
href="https://doi.org/10.1007/s10994-023-06302-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The model-X conditional randomization test is a generic framework for conditional independence testing, unlocking new possibilities to discover features that are conditionally associated with a response of interest while controlling type I error rates. An appealing advantage of this test is that it can work with any machine learning model to design powerful test statistic. In turn, the common practice in the model-X literature is to form a test statistic using machine learning models, trained to maximize predictive accuracy with the hope to attain a test with good power. However, the ideal goal here is to drive the model (during training) to maximize the power of the test, not merely the predictive accuracy. In this paper, we bridge this gap by introducing novel model-fitting schemes that are designed to explicitly improve the power of model-X tests. This is done by introducing a new cost function that aims at maximizing the test statistic used to measure violations of conditional independence. Using synthetic and real data sets, we demonstrate that the combination of our proposed loss function with various base predictive models (lasso, elastic net, and deep neural networks) consistently increases the number of correct discoveries obtained, while maintaining type I error rates under control.},
  archive      = {J_ML},
  author       = {Shaer, Shalev and Romano, Yaniv},
  doi          = {10.1007/s10994-023-06302-3},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2317-2357},
  shortjournal = {Mach. Learn.},
  title        = {Learning to increase the power of conditional randomization tests},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autoencoding slow representations for semi-supervised
data-efficient regression. <em>ML</em>, <em>112</em>(7), 2297–2315. (<a
href="https://doi.org/10.1007/s10994-022-06299-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The slowness principle is a concept inspired by the visual cortex of the brain. It postulates that the underlying generative factors of a quickly varying sensory signal change on a different, slower time scale. By applying this principle to state-of-the-art unsupervised representation learning methods one can learn a latent embedding to perform supervised downstream regression tasks more data efficient. In this paper, we compare different approaches to unsupervised slow representation learning such as $$L_p$$ norm based slowness regularization and the SlowVAE, and propose a new term based on Brownian motion used in our method, the S-VAE. We empirically evaluate these slowness regularization terms with respect to their downstream task performance and data efficiency in state estimation and behavioral cloning tasks. We find that slow representations show great performance improvements in settings where only sparse labeled training data is available. Furthermore, we present a theoretical and empirical comparison of the discussed slowness regularization terms. Finally, we discuss how the Fréchet Inception Distance (FID), commonly used to determine the generative capabilities of GANs, can predict the performance of trained models in supervised downstream tasks.},
  archive      = {J_ML},
  author       = {Struckmeier, Oliver and Tiwari, Kshitij and Kyrki, Ville},
  doi          = {10.1007/s10994-022-06299-1},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2297-2315},
  shortjournal = {Mach. Learn.},
  title        = {Autoencoding slow representations for semi-supervised data-efficient regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Model-free inverse reinforcement learning with
multi-intention, unlabeled, and overlapping demonstrations. <em>ML</em>,
<em>112</em>(7), 2263–2296. (<a
href="https://doi.org/10.1007/s10994-022-06273-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we define a novel inverse reinforcement learning (IRL) problem where the demonstrations are multi-intention, i.e., collected from multi-intention experts, unlabeled, i.e., without intention labels, and partially overlapping, i.e., shared between multiple intentions. In the presence of overlapping demonstrations, current IRL methods, developed to handle multi-intention and unlabeled demonstrations, cannot successfully learn the underlying reward functions. To solve this limitation, we propose a novel clustering-based approach to disentangle the observed demonstrations and experimentally validate its advantages. Traditional clustering-based approaches to multi-intention IRL, which are developed on the basis of model-based Reinforcement Learning (RL), formulate the problem using parametric density estimation. However, in high-dimensional environments and unknown system dynamics, i.e., model-free RL, the solution of parametric density estimation is only tractable up to the density normalization constant. To solve this, we formulate the problem as a mixture of logistic regressions to directly handle the unnormalized density. To research the challenges faced by overlapping demonstrations, we introduce the concepts of shared pair, which is a state-action pair that is shared in more than one intention, and separability, which resembles how well the multiple intentions can be separated in the joint state-action space. We provide theoretical analyses under the global optimality condition and the existence of shared pairs. Furthermore, we conduct extensive experiments on four simulated robotics tasks, extended to accept different intentions with specific levels of separability, and a synthetic driver task developed to directly control the separability. We evaluate the existing baselines on our defined problem and demonstrate, theoretically and experimentally, the advantages of our clustering-based solution, especially when the separability of the demonstrations decreases.},
  archive      = {J_ML},
  author       = {Bighashdel, Ariyan and Jancura, Pavol and Dubbelman, Gijs},
  doi          = {10.1007/s10994-022-06273-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2263-2296},
  shortjournal = {Mach. Learn.},
  title        = {Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information bottleneck and selective noise supervision for
zero-shot learning. <em>ML</em>, <em>112</em>(7), 2239–2261. (<a
href="https://doi.org/10.1007/s10994-022-06196-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen classes. Though many ZSL methods rely on a direct mapping between the visual and the semantic space, the calibration deviation and hubness problem limit the generalization capability to unseen classes. Recently emerged generative ZSL methods generate unseen image features to transform ZSL into a supervised classification problem. However, most generative models still suffer from the seen-unseen bias problem as only seen data is used for training. To address these issues, we propose a novel bidirectional embedding based generative model with a tight visual-semantic coupling constraint. We learn a unified latent space that calibrates the embedded parametric distributions of both visual and semantic spaces. Since the embedding from high-dimensional visual features comprises much non-semantic information, the alignment of visual and semantic in latent space would inevitably be deviated. Therefore, we introduce an information bottleneck constraint to ZSL for the first time to preserve essential attribute information during the mapping. Specifically, we utilize the uncertainty estimation and the wake-sleep procedure to alleviate the feature noises and improve model abstraction capability. In addition, our method can be easily extended to the transductive ZSL setting by generating labels for unseen images. We then introduce a robust self-training loss to solve this label-noise problem. Extensive experimental results show that our method outperforms the state-of-the-art methods in different ZSL settings on most benchmark datasets.},
  archive      = {J_ML},
  author       = {Zhou, Lei and Liu, Yang and Zhang, Pengcheng and Bai, Xiao and Gu, Lin and Zhou, Jun and Yao, Yazhou and Harada, Tatsuya and Zheng, Jin and Hancock, Edwin},
  doi          = {10.1007/s10994-022-06196-7},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2239-2261},
  shortjournal = {Mach. Learn.},
  title        = {Information bottleneck and selective noise supervision for zero-shot learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised change detection using guided anisotropic
diffusion. <em>ML</em>, <em>112</em>(6), 2211–2237. (<a
href="https://doi.org/10.1007/s10994-021-06008-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale datasets created from crowdsourced labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose original ideas that help us to leverage such datasets in the context of change detection. First, we propose the guided anisotropic diffusion (GAD) algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering. We then show its potential in two weakly-supervised learning strategies tailored for change detection. The first strategy is an iterative learning method that combines model optimisation and data cleansing using GAD to extract the useful information from a large scale change detection dataset generated from open vector data. The second one incorporates GAD within a novel spatial attention layer that increases the accuracy of weakly supervised networks trained to perform pixel-level predictions from image-level labels. Improvements with respect to state-of-the-art are demonstrated on 4 different public datasets.},
  archive      = {J_ML},
  author       = {Daudt, Rodrigo Caye and Le Saux, Bertrand and Boulch, Alexandre and Gousseau, Yann},
  doi          = {10.1007/s10994-021-06008-4},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2211-2237},
  shortjournal = {Mach. Learn.},
  title        = {Weakly supervised change detection using guided anisotropic diffusion},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distilling ensemble of explanations for weakly-supervised
pre-training of image segmentation models. <em>ML</em>, <em>112</em>(6),
2193–2209. (<a
href="https://doi.org/10.1007/s10994-022-06182-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While fine-tuning pre-trained networks has become a popular way to train image segmentation models, such backbone networks for image segmentation are frequently pre-trained using image classification source datasets, e.g., ImageNet. Though image classification datasets could provide the backbone networks with rich visual features and discriminative ability, they are incapable of fully pre-training the target model (i.e., backbone+segmentation modules) in an end-to-end manner. The segmentation modules are left to random initialization in the fine-tuning process due to the lack of segmentation labels in classification datasets. In our work, we propose a method that leverages Pseudo Semantic Segmentation Labels (PSSL), to enable the end-to-end pre-training for image segmentation models based on classification datasets. PSSL was inspired by the observation that the explanation results of classification models, obtained through explanation algorithms such as CAM, SmoothGrad and LIME, would be close to the pixel clusters of visual objects. Specifically, PSSL is obtained for each image by interpreting the classification results and aggregating an ensemble of explanations queried from multiple classifiers to lower the bias caused by single models. With PSSL for every image of ImageNet, the proposed method leverages a weighted segmentation learning procedure to pre-train the segmentation network en masse. Experiment results show that, with ImageNet accompanied by PSSL as the source dataset, the proposed end-to-end pre-training strategy successfully boosts the performance of various segmentation models, i.e., PSPNet-ResNet50, DeepLabV3-ResNet50, and OCRNet-HRNetW18, on a number of segmentation tasks, such as CamVid, VOC-A, VOC-C, ADE20K, and CityScapes, with significant improvements.},
  archive      = {J_ML},
  author       = {Li, Xuhong and Xiong, Haoyi and Liu, Yi and Zhou, Dingfu and Chen, Zeyu and Wang, Yaqing and Dou, Dejing},
  doi          = {10.1007/s10994-022-06182-z},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2193-2209},
  shortjournal = {Mach. Learn.},
  title        = {Distilling ensemble of explanations for weakly-supervised pre-training of image segmentation models},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving fairness generalization through a sample-robust
optimization method. <em>ML</em>, <em>112</em>(6), 2131–2192. (<a
href="https://doi.org/10.1007/s10994-022-06191-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unwanted bias is a major concern in machine learning, raising in particular significant ethical issues when machine learning models are deployed within high-stakes decision systems. A common solution to mitigate it is to integrate and optimize a statistical fairness metric along with accuracy during the training phase. However, one of the main remaining challenges is that current approaches usually generalize poorly in terms of fairness on unseen data. We address this issue by proposing a new robustness framework for statistical fairness in machine learning. The proposed approach is inspired by the domain of distributionally robust optimization and works in ensuring fairness over a variety of samplings of the training set. Our approach can be used to quantify the robustness of fairness but also to improve it when training a model. We empirically evaluate the proposed method and show that it effectively improves fairness generalization. In addition, we propose a simple yet powerful heuristic application of our framework that can be integrated into a wide range of existing fair classification techniques to enhance fairness generalization. Our extensive empirical study using two existing fair classification methods demonstrates the efficiency and scalability of the proposed heuristic approach.},
  archive      = {J_ML},
  author       = {Ferry, Julien and Aïvodji, Ulrich and Gambs, Sébastien and Huguet, Marie-José and Siala, Mohamed},
  doi          = {10.1007/s10994-022-06191-y},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2131-2192},
  shortjournal = {Mach. Learn.},
  title        = {Improving fairness generalization through a sample-robust optimization method},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial supervised contrastive learning. <em>ML</em>,
<em>112</em>(6), 2105–2130. (<a
href="https://doi.org/10.1007/s10994-022-06269-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is prevalently used in pre-training deep models, followed with fine-tuning in downstream tasks for better performance or faster training. However, pre-trained models from contrastive learning are barely robust against adversarial examples in downstream tasks since the representations learned by self-supervision may lack the robustness and also the class-wise discrimination. To tackle the above problems, we adapt the contrastive learning scheme to adversarial examples for robustness enhancement, and also extend the self-supervised contrastive approach to the supervised setting for the ability to discriminate on classes. Equipped with our new designs, we proposed adversarial supervised contrastive learning (ASCL), a novel framework for robust pre-training. Despite its simplicity, extensive experiments show that ASCL achieves significant margins in adversarial robustness over the prior arts, proceeding towards either the lightweight standard fine-tuning or adversarial fine-tuning. Moreover, ASCL also shows benefits for robustness to diverse natural corruptions, suggesting the wide applicability to all sorts of practical scenarios. Notably, ASCL demonstrate impressive results in robust transfer learning.},
  archive      = {J_ML},
  author       = {Li, Zhuorong and Yu, Daiwei and Wu, Minghui and Jin, Canghong and Yu, Hongchuan},
  doi          = {10.1007/s10994-022-06269-7},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2105-2130},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial supervised contrastive learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated imbalanced classification via layered learning.
<em>ML</em>, <em>112</em>(6), 2083–2104. (<a
href="https://doi.org/10.1007/s10994-022-06282-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address imbalanced binary classification (IBC) tasks. Applying resampling strategies to balance the class distribution of training instances is a common approach to tackle these problems. Many state-of-the-art methods find instances of interest close to the decision boundary to drive the resampling process. However, under-sampling the majority class may potentially lead to important information loss. Over-sampling also may increase the chance of overfitting by propagating the information contained in instances from the minority class. The main contribution of our work is a new method called ICLL for tackling IBC tasks which is not based on resampling training observations. Instead, ICLL follows a layered learning paradigm to model the data in two stages. In the first layer, ICLL learns to distinguish cases close to the decision boundary from cases which are clearly from the majority class, where this dichotomy is defined using a hierarchical clustering analysis. In the subsequent layer, we use instances close to the decision boundary and instances from the minority class to solve the original predictive task. A second contribution of our work is the automatic definition of the layers which comprise the layered learning strategy using a hierarchical clustering model. This is a relevant discovery as this process is usually performed manually according to domain knowledge. We carried out extensive experiments using 100 benchmark data sets. The results show that the proposed method leads to a better performance relatively to several state-of-the-art methods for IBC.},
  archive      = {J_ML},
  author       = {Cerqueira, Vitor and Torgo, Luis and Branco, Paula and Bellinger, Colin},
  doi          = {10.1007/s10994-022-06282-w},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2083-2104},
  shortjournal = {Mach. Learn.},
  title        = {Automated imbalanced classification via layered learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Can language models automate data wrangling? <em>ML</em>,
<em>112</em>(6), 2053–2082. (<a
href="https://doi.org/10.1007/s10994-022-06259-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automation of data science and other data manipulation processes depend on the integration and formatting of ‘messy’ data. Data wrangling is an umbrella term for these tedious and time-consuming tasks. Tasks such as transforming dates, units or names expressed in different formats have been challenging for machine learning because (1) users expect to solve them with short cues or few examples, and (2) the problems depend heavily on domain knowledge. Interestingly, large language models today (1) can infer from very few examples or even a short clue in natural language, and (2) can integrate vast amounts of domain knowledge. It is then an important research question to analyse whether language models are a promising approach for data wrangling, especially as their capabilities continue growing. In this paper we apply different variants of the language model Generative Pre-trained Transformer (GPT) to five batteries covering a wide range of data wrangling problems. We compare the effect of prompts and few-shot regimes on their results and how they compare with specialised data wrangling systems and other tools. Our major finding is that they appear as a powerful tool for a wide range of data wrangling tasks. We provide some guidelines about how they can be integrated into data processing pipelines, provided the users can take advantage of their flexibility and the diversity of tasks to be addressed. However, reliability is still an important issue to overcome.},
  archive      = {J_ML},
  author       = {Jaimovitch-López, Gonzalo and Ferri, Cèsar and Hernández-Orallo, José and Martínez-Plumed, Fernando and Ramírez-Quintana, María José},
  doi          = {10.1007/s10994-022-06259-9},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2053-2082},
  shortjournal = {Mach. Learn.},
  title        = {Can language models automate data wrangling?},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shift of pairwise similarities for data clustering.
<em>ML</em>, <em>112</em>(6), 2025–2051. (<a
href="https://doi.org/10.1007/s10994-022-06189-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the Min Cut cost function by a cluster dependent factor (e.g., the size or the degree of the clusters), in order to yield a more balanced partitioning. We, instead, investigate adding such regularizations to the original cost function. We first consider the case where the regularization term is the sum of the squared size of the clusters, and then generalize it to adaptive regularization of the pairwise similarities. This leads to shifting (adaptively) the pairwise similarities which might make some of them negative. We then study the connection of this method to Correlation Clustering and then propose an efficient local search optimization algorithm with fast theoretical convergence rate to solve the new clustering problem. In the following, we investigate the shift of pairwise similarities on some common clustering methods, and finally, we demonstrate the superior performance of the method by extensive experiments on different datasets.},
  archive      = {J_ML},
  author       = {Haghir Chehreghani, Morteza},
  doi          = {10.1007/s10994-022-06189-6},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2025-2051},
  shortjournal = {Mach. Learn.},
  title        = {Shift of pairwise similarities for data clustering},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal deep learning for cetacean distribution modeling
of fin whales (balaenoptera physalus) in the western mediterranean sea.
<em>ML</em>, <em>112</em>(6), 2003–2024. (<a
href="https://doi.org/10.1007/s10994-021-06029-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cetacean Distribution Modeling (CDM) is used to quantify mobile marine species distributions and densities. It is essential to better understand and protect whales and their relatives. Current CDM approaches often fail in capturing general species-environment relationships, which would be valid within a broader range of environmental conditions that characterize the surveyed regions. This paper aims at investigating the usefulness of deep learning based schemes, namely multi-task and transfer learning, in CDM. Co-training of a stochastic presence-background model on a classification task and a deterministic rule-based model on a regression task was performed. Whale presence-only records were used for the first task, and index outputs of a feeding habitat occurrence model for the second one. This new approach has been experimented through the study case of fin whales in the western Mediterranean Sea. To evaluate our approach, a new metric called True Positive rate per unit of Surface Habitat (TPSH) and an original multimodal fully-connected neural networks were developed. A Generalized Additive Model (GAM)—a standard CDM method—was also used as a reference for performance. Results show that our multi-task learning model improves both the feeding habitat model by 10.8\% and data-driven models such as GAM by 16.5\% on our TPSH metric in relative terms, revealing a higher accuracy of our approach in estimating whale presence. Such trends in results have been further supported by the use of two other independent datasets that forced models to generalize beyond their training dataset of species-environment relationships. Performance could be further improved by adopting more optimal thresholds as observed from Receiver Operating Characteristic curves, e.g. the multi-task learning model could reach absolute gains up to 10\% in the median of the True Positive Rate while maintaining its habitat spatial spreading. Globally, our work confirmed our working hypothesis that expert information on whale behaviour represent a good knowledge base for model generalization. This result can be further improved by a concurrent learning of more local species-environment relationships from in-situ presence data.},
  archive      = {J_ML},
  author       = {Cazau, D. and Nguyen Hong Duc, P. and Druon, J.-N. and Matwins, S. and Fablet, R.},
  doi          = {10.1007/s10994-021-06029-z},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2003-2024},
  shortjournal = {Mach. Learn.},
  title        = {Multimodal deep learning for cetacean distribution modeling of fin whales (Balaenoptera physalus) in the western mediterranean sea},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable clustering of segmented trajectories within a
continuous time framework: Application to maritime traffic data.
<em>ML</em>, <em>112</em>(6), 1975–2001. (<a
href="https://doi.org/10.1007/s10994-021-06004-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the surveillance of the maritime traffic, a major challenge is the automatic identification of traffic flows from a set of observed trajectories, in order to derive good management measures or to detect abnormal or illegal behaviours for example. In this paper, we propose a new modelling framework to cluster sequences of a large amount of trajectories recorded at potentially irregular frequencies. The model is specified within a continuous time framework, being robust to irregular sampling in records and accounting for possible heterogeneous movement patterns within a single trajectory. It partitions a trajectory into sub-trajectories, or movement modes, allowing a clustering of both individuals’ movement patterns and trajectories. The clustering is performed using non parametric Bayesian methods, namely the hierarchical Dirichlet process, and considers a stochastic variational inference to estimate the model’s parameters, hence providing a scalable method in an easy-to-distribute framework. Performance is assessed on both simulated data and on our motivational large trajectory dataset from the automatic identification system, used to monitor the world maritime traffic: the clusters represent significant, atomic motion-patterns, making the model informative for stakeholders.},
  archive      = {J_ML},
  author       = {Gloaguen, Pierre and Chapel, Laetitia and Friguet, Chloé and Tavenard, Romain},
  doi          = {10.1007/s10994-021-06004-8},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1975-2001},
  shortjournal = {Mach. Learn.},
  title        = {Scalable clustering of segmented trajectories within a continuous time framework: Application to maritime traffic data},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian-inspired, deep learning-based, semi-supervised
domain adaptation technique for land cover mapping. <em>ML</em>,
<em>112</em>(6), 1941–1973. (<a
href="https://doi.org/10.1007/s10994-020-05942-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover maps are a vital input variable to many types of environmental research and management. While they can be produced automatically by machine learning techniques, these techniques require substantial training data to achieve high levels of accuracy, which are not always available. One technique researchers use when labelled training data are scarce is domain adaptation (DA)—where data from an alternate region, known as the source domain, are used to train a classifier and this model is adapted to map the study region, or target domain. The scenario we address in this paper is known as semi-supervised DA, where some labelled samples are available in the target domain. In this paper we present Sourcerer, a Bayesian-inspired, deep learning-based, semi-supervised DA technique for producing land cover maps from satellite image time series (SITS) data. The technique takes a convolutional neural network trained on a source domain and then trains further on the available target domain with a novel regularizer applied to the model weights. The regularizer adjusts the degree to which the model is modified to fit the target data, limiting the degree of change when the target data are few in number and increasing it as target data quantity increases. Our experiments on Sentinel-2 time series images compare Sourcerer with two state-of-the-art semi-supervised domain adaptation techniques and four baseline models. We show that on two different source-target domain pairings Sourcerer outperforms all other methods for any quantity of labelled target data available. In fact, the results on the more difficult target domain show that the starting accuracy of Sourcerer (when no labelled target data are available), 74.2\%, is greater than the next-best state-of-the-art method trained on 20,000 labelled target instances.},
  archive      = {J_ML},
  author       = {Lucas, Benjamin and Pelletier, Charlotte and Schmidt, Daniel and Webb, Geoffrey I. and Petitjean, François},
  doi          = {10.1007/s10994-020-05942-z},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1941-1973},
  shortjournal = {Mach. Learn.},
  title        = {A bayesian-inspired, deep learning-based, semi-supervised domain adaptation technique for land cover mapping},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from self-discrepancy via multiple co-teaching for
cross-domain person re-identification. <em>ML</em>, <em>112</em>(6),
1923–1940. (<a
href="https://doi.org/10.1007/s10994-022-06184-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing clustering strategy to assign unlabeled target images with pseudo labels has become a trend for person re-identification (re-ID) algorithms in domain adaptation. A potential limitation of these clustering-based methods is that they always tend to introduce noisy labels, which will undoubtedly hamper the performance of our re-ID system. To handle this limitation, an intuitive solution is to utilize collaborative training to purify the pseudo label quality. However, there exists a challenge that the complementarity of two networks, which inevitably share a high similarity, becomes weakened gradually as training process goes on; worse still, these approaches typically ignore to consider the self-discrepancy of intra-class relations. To address this issue, in this paper, we propose a multiple co-teaching framework for domain adaptive person re-ID, opening up a promising direction about self-discrepancy problem under unsupervised condition. On top of that, a mean-teaching mechanism is leveraged to enlarge the difference and discover more complementary features in target domain. Comprehensive experiments conducted on several large-scale datasets show that our method achieves competitive performance compared with the state-of-the-arts.},
  archive      = {J_ML},
  author       = {Xiang, Suncheng and Fu, Yuzhuo and Guan, Mengyuan and Liu, Ting},
  doi          = {10.1007/s10994-022-06184-x},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1923-1940},
  shortjournal = {Mach. Learn.},
  title        = {Learning from self-discrepancy via multiple co-teaching for cross-domain person re-identification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online AutoML: An adaptive AutoML framework for online
learning. <em>ML</em>, <em>112</em>(6), 1897–1921. (<a
href="https://doi.org/10.1007/s10994-022-06262-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Machine Learning (AutoML) has been used successfully in settings where the learning task is assumed to be static. In many real-world scenarios, however, the data distribution will evolve over time, and it is yet to be shown whether AutoML techniques can effectively design online pipelines in dynamic environments. This study aims to automate pipeline design for online learning while continuously adapting to data drift. For this purpose, we design an adaptive Online Automated Machine Learning (OAML) system, searching the complete pipeline configuration space of online learners, including preprocessing algorithms and ensembling techniques. This system combines the inherent adaptation capabilities of online learners with fast automated pipeline (re)optimization. Focusing on optimization techniques that can adapt to evolving objectives, we evaluate asynchronous genetic programming and asynchronous successive halving to optimize these pipelines continually. We experiment on real and artificial data streams with varying types of concept drift to test the performance and adaptation capabilities of the proposed system. The results confirm the utility of OAML over popular online learning algorithms and underscore the benefits of continuous pipeline redesign in the presence of data drift.},
  archive      = {J_ML},
  author       = {Celik, Bilge and Singh, Prabhant and Vanschoren, Joaquin},
  doi          = {10.1007/s10994-022-06262-0},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1897-1921},
  shortjournal = {Mach. Learn.},
  title        = {Online AutoML: An adaptive AutoML framework for online learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An instance-dependent simulation framework for learning with
label noise. <em>ML</em>, <em>112</em>(6), 1871–1896. (<a
href="https://doi.org/10.1007/s10994-022-06207-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simulation framework for generating instance-dependent noisy labels via a pseudo-labeling paradigm. We show that the distribution of the synthetic noisy labels generated with our framework is closer to human labels compared to independent and class-conditional random flipping. Equipped with controllable label noise, we study the negative impact of noisy labels across a few practical settings to understand when label noise is more problematic. We also benchmark several existing algorithms for learning with noisy labels and compare their behavior on our synthetic datasets and on the datasets with independent random label noise. Additionally, with the availability of annotator information from our simulation framework, we propose a new technique, Label Quality Model (LQM), that leverages annotator features to predict and correct against noisy labels. We show that by adding LQM as a label correction step before applying existing noisy label techniques, we can further improve the models’ performance. The synthetic datasets that we generated in this work are released at https://github.com/deepmind/deepmind-research/tree/master/noisy_label .},
  archive      = {J_ML},
  author       = {Gu, Keren and Masotto, Xander and Bachani, Vandana and Lakshminarayanan, Balaji and Nikodem, Jack and Yin, Dong},
  doi          = {10.1007/s10994-022-06207-7},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1871-1896},
  shortjournal = {Mach. Learn.},
  title        = {An instance-dependent simulation framework for learning with label noise},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A viable framework for semi-supervised learning on realistic
dataset. <em>ML</em>, <em>112</em>(6), 1847–1869. (<a
href="https://doi.org/10.1007/s10994-022-06208-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised Fine-Grained Recognition is a challenging task due to the difficulty of data imbalance, high inter-class similarity and domain mismatch. Recently, this field has witnessed giant leap and many methods have gained great performance. We discover that these existing Semi-supervised Learning (SSL) methods achieve satisfactory performance owe to the exploration of unlabeled data. However, on the realistic large-scale datasets, due to the abovementioned challenges, the improvement of the quality of pseudo-labels requires further research. In this work, we propose Bilateral-Branch Self-Training Framework (BiSTF), a simple yet effective framework to improve existing semi-supervised learning methods on class-imbalanced and domain-shifted fine-grained data. By adjusting stochastic epoch update frequency, BiSTF iteratively retrains a baseline SSL model with a labeled set expanded by selectively adding pseudo-labeled samples from an unlabeled set, where the distribution of pseudo-labeled samples is the same as the labeled data. We show that BiSTF outperforms the existing state-of-the-art SSL algorithm on Semi-iNat dataset. Our code is available at https://github.com/HowieChangchn/BiSTF .},
  archive      = {J_ML},
  author       = {Chang, Hao and Xie, Guochen and Yu, Jun and Ling, Qiang and Gao, Fang and Yu, Ye},
  doi          = {10.1007/s10994-022-06208-6},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1847-1869},
  shortjournal = {Mach. Learn.},
  title        = {A viable framework for semi-supervised learning on realistic dataset},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from crowds with sparse and imbalanced annotations.
<em>ML</em>, <em>112</em>(6), 1823–1845. (<a
href="https://doi.org/10.1007/s10994-022-06185-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional supervised learning requires ground truth labels for training, whose collection however is difficult in many cases. Recently, crowdsourcing has established itself as an efficient labeling solution by resorting to non-expert crowds. To reduce the labeling error effects, one common practice is to distribute each instance to multiple workers, whereas each worker only annotates a subset of data, resulting in the sparse annotation phenomenon. In this paper, we show that when meeting with class-imbalance, i.e., even when the groundtruth labels are slightly imbalanced, the sparse annotations are prone to be skewly distributed and would bias the learning algorithm severely. To combat this issue, we propose one Distribution Aware Self-training based Crowdsourcing learning (DASC) approach, which supplements the sparse annotations by adding confident pseudo-annotations and at the same time re-balancing the annotation distribution. Specifically, we propose one distribution aware confidence measure to select the most confident pseudo-annotations, with minority/majority classes selected more/less frequently. As a universal framework, DASC is applicable to various crowdsourcing methods for consistent performance gains. We conduct extensive experiments over real-world crowdsourcing benchmarks, from slight to heavy imbalance ratio, with various annotation sparsity levels, and show that DASC substantially improves previous crowdsourcing models by $$2\%$$ - $$20\%$$ absolute test accuracy, and yields much more balanced annotations.},
  archive      = {J_ML},
  author       = {Shi, Ye and Li, Shao-Yuan and Huang, Sheng-Jun},
  doi          = {10.1007/s10994-022-06185-w},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1823-1845},
  shortjournal = {Mach. Learn.},
  title        = {Learning from crowds with sparse and imbalanced annotations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning’s shallow gains: A comparative evaluation of
algorithms for automatic music generation. <em>ML</em>, <em>112</em>(5),
1785–1822. (<a
href="https://doi.org/10.1007/s10994-023-06309-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods are recognised as state-of-the-art for many applications of machine learning. Recently, deep learning methods have emerged as a solution to the task of automatic music generation (AMG) using symbolic tokens in a target style, but their superiority over non-deep learning methods has not been demonstrated. Here, we conduct a listening study to comparatively evaluate several music generation systems along six musical dimensions: stylistic success, aesthetic pleasure, repetition or self-reference, melody, harmony, and rhythm. A range of models, both deep learning algorithms and other methods, are used to generate 30-s excerpts in the style of Classical string quartets and classical piano improvisations. Fifty participants with relatively high musical knowledge rate unlabelled samples of computer-generated and human-composed excerpts for the six musical dimensions. We use non-parametric Bayesian hypothesis testing to interpret the results, allowing the possibility of finding meaningful non-differences between systems’ performance. We find that the strongest deep learning method, a reimplemented version of Music Transformer, has equivalent performance to a non-deep learning method, MAIA Markov, demonstrating that to date, deep learning does not outperform other methods for AMG. We also find there still remains a significant gap between any algorithmic method and human-composed excerpts.},
  archive      = {J_ML},
  author       = {Yin, Zongyu and Reuben, Federico and Stepney, Susan and Collins, Tom},
  doi          = {10.1007/s10994-023-06309-w},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1785-1822},
  shortjournal = {Mach. Learn.},
  title        = {Deep learning’s shallow gains: A comparative evaluation of algorithms for automatic music generation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully convolutional open set segmentation. <em>ML</em>,
<em>112</em>(5), 1733–1784. (<a
href="https://doi.org/10.1007/s10994-021-06027-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional semantic segmentation, knowing about all existing classes is essential to yield effective results with the majority of existing approaches. However, these methods trained in a Closed Set of classes fail when new classes are found in the test phase, not being able to recognize that an unseen class has been fed. This means that they are not suitable for Open Set scenarios, which are very common in real-world computer vision and remote sensing applications. In this paper, we discuss the limitations of Closed Set segmentation and propose two fully convolutional approaches to effectively address Open Set semantic segmentation: OpenFCN and OpenPCS. OpenFCN is based on the well-known OpenMax algorithm, configuring a new application of this approach in segmentation settings. OpenPCS is a fully novel approach based on feature-space from DNN activations that serve as features for computing PCA and multi-variate gaussian likelihood in a lower dimensional space. In addition to OpenPCS and aiming to reduce the RAM memory requirements of the methodology, we also propose a slight variation of the method (OpenIPCS) that uses an iteractive version of PCA able to be trained in small batches. Experiments were conducted on the well-known ISPRS Vaihingen/Potsdam and the 2018 IEEE GRSS Data Fusion Challenge datasets. OpenFCN showed little-to-no improvement when compared to the simpler and much more time efficient SoftMax thresholding, while being some orders of magnitude slower. OpenPCS achieved promising results in almost all experiments by overcoming both OpenFCN and SoftMax thresholding. OpenPCS is also a reasonable compromise between the runtime performances of the extremely fast SoftMax thresholding and the extremely slow OpenFCN, being able to run close to real-time. Experiments also indicate that OpenPCS is effective, robust and suitable for Open Set segmentation, being able to improve the recognition of unknown class pixels without reducing the accuracy on the known class pixels. We also tested the scenario of hiding multiple known classes to simulate multimodal unknowns, resulting in an even larger gap between OpenPCS/OpenIPCS and both SoftMax thresholding and OpenFCN, implying that gaussian modeling is more robust to settings with greater openness.},
  archive      = {J_ML},
  author       = {Oliveira, Hugo and Silva, Caio and Machado, Gabriel L. S. and Nogueira, Keiller and dos Santos, Jefersson A.},
  doi          = {10.1007/s10994-021-06027-1},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1733-1784},
  shortjournal = {Mach. Learn.},
  title        = {Fully convolutional open set segmentation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAC-learning with approximate predictors. <em>ML</em>,
<em>112</em>(5), 1693–1732. (<a
href="https://doi.org/10.1007/s10994-023-06301-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate learning machines have become popular in the era of small devices, including quantised, factorised, hashed, or otherwise compressed predictors, and the quest to explain and guarantee good generalisation abilities for such methods has just begun. In this paper, we study the role of approximability in learning, both in the full precision and the approximated settings. We do this through a notion of sensitivity of predictors to the action of the approximation operator at hand. We prove upper bounds on the generalisation of such predictors, yielding the following main findings, for any PAC-learnable class and any given approximation operator: (1) We show that under mild conditions, approximable target concepts are learnable from a smaller labelled sample, provided sufficient unlabelled data; (2) We give algorithms that guarantee a good predictor whose approximation also enjoys the same generalisation guarantees; (3) We highlight natural examples of structure in the class of sensitivities, which reduce, and possibly even eliminate the otherwise abundant requirement of additional unlabelled data, and henceforth shed new light onto what makes one problem instance easier to learn than another. These results embed the scope of modern model-compression approaches into the general goal of statistical learning theory, which in return suggests appropriate algorithms through minimising uniform bounds.},
  archive      = {J_ML},
  author       = {Turner, Andrew J. and Kabán, Ata},
  doi          = {10.1007/s10994-023-06301-4},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1693-1732},
  shortjournal = {Mach. Learn.},
  title        = {PAC-learning with approximate predictors},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local2Global: A distributed approach for scaling
representation learning on graphs. <em>ML</em>, <em>112</em>(5),
1663–1692. (<a
href="https://doi.org/10.1007/s10994-022-06285-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a decentralised “local2global” approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or “patches”) and training local representations for each patch independently. In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization. A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronization during distributed training. This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner. We apply local2global on data sets of different sizes and show that our approach achieves a good trade-off between scale and accuracy on edge reconstruction and semi-supervised classification. We also consider the downstream task of anomaly detection and show how one can use local2global to highlight anomalies in cybersecurity networks.},
  archive      = {J_ML},
  author       = {Jeub, Lucas G. S. and Colavizza, Giovanni and Dong, Xiaowen and Bazzi, Marya and Cucuringu, Mihai},
  doi          = {10.1007/s10994-022-06285-7},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1663-1692},
  shortjournal = {Mach. Learn.},
  title        = {Local2Global: A distributed approach for scaling representation learning on graphs},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-model consensus of explanations and beyond for image
classification models: An empirical study. <em>ML</em>, <em>112</em>(5),
1627–1662. (<a
href="https://doi.org/10.1007/s10994-023-06312-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing explanation algorithms have found that, even if deep models make the same correct predictions on the same image, they might rely on different sets of input features for classification. However, among these features, some common features might be used by the majority of models. In this paper, we are wondering what the common features used by various models for classification are and whether the models with better performance may favor those common features. For this purpose, our work uses an explanation algorithm to attribute the importance of features (e.g., pixels or superpixels) as explanations and proposes the cross-model consensus of explanations to capture the common features. Specifically, we first prepare a set of deep models as a committee, then deduce the explanation for every model, and obtain the consensus of explanations across the entire committee through voting. With the cross-model consensus of explanations, we conduct extensive experiments using 80+ models on five datasets/tasks. We find three interesting phenomena as follows: (1) the consensus obtained from image classification models is aligned with the ground truth of semantic segmentation; (2) we measure the similarity of the explanation result of each model in the committee to the consensus (namely consensus score), and find positive correlations between the consensus score and model performance; and (3) the consensus score potentially correlates to the interpretability.},
  archive      = {J_ML},
  author       = {Li, Xuhong and Xiong, Haoyi and Huang, Siyu and Ji, Shilei and Dou, Dejing},
  doi          = {10.1007/s10994-023-06312-1},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1627-1662},
  shortjournal = {Mach. Learn.},
  title        = {Cross-model consensus of explanations and beyond for image classification models: An empirical study},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing universal adversarial perturbations for deep
neural networks. <em>ML</em>, <em>112</em>(5), 1597–1626. (<a
href="https://doi.org/10.1007/s10994-023-06306-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have shown that universal adversarial attacks can fool deep neural networks over a large set of input images with a single human-invisible perturbation. However, current methods for universal adversarial attacks are based on additive perturbation, which enables misclassification by directly adding the perturbation on the input images. In this paper, for the first time, we show that a universal adversarial attack can also be achieved through spatial transformation (non-additive). More importantly, to unify both additive and non-additive perturbations, we propose a novel unified yet flexible framework for universal adversarial attacks, called GUAP, which can initiate attacks by $$\ell _\infty$$ -norm (additive) perturbation, spatially-transformed (non-additive) perturbation, or a combination of both. Extensive experiments are conducted on two computer vision scenarios, including image classification and semantic segmentation tasks, which contain CIFAR-10, ImageNet and Cityscapes datasets with a number of different deep neural network models, including GoogLeNet, VGG16/19, ResNet101/152, DenseNet121, and FCN-8s. Empirical experiments demonstrate that GUAP can obtain higher attack success rates on these datasets compared to state-of-the-art universal adversarial attacks. In addition, we also demonstrate how universal adversarial training benefits the robustness of the model against universal attacks. We release our tool GUAP on https://github.com/TrustAI/GUAP .},
  archive      = {J_ML},
  author       = {Zhang, Yanghao and Ruan, Wenjie and Wang, Fu and Huang, Xiaowei},
  doi          = {10.1007/s10994-023-06306-z},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1597-1626},
  shortjournal = {Mach. Learn.},
  title        = {Generalizing universal adversarial perturbations for deep neural networks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning programs with magic values. <em>ML</em>,
<em>112</em>(5), 1551–1595. (<a
href="https://doi.org/10.1007/s10994-022-06274-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A magic value in a program is a constant symbol that is essential for the execution of the program but has no clear explanation for its choice. Learning programs with magic values is difficult for existing program synthesis approaches. To overcome this limitation, we introduce an inductive logic programming approach to efficiently learn programs with magic values. Our experiments on diverse domains, including program synthesis, drug design, and game playing, show that our approach can (1) outperform existing approaches in terms of predictive accuracies and learning times, (2) learn magic values from infinite domains, such as the value of pi, and (3) scale to domains with millions of constant symbols.},
  archive      = {J_ML},
  author       = {Hocquette, Céline and Cropper, Andrew},
  doi          = {10.1007/s10994-022-06274-w},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1551-1595},
  shortjournal = {Mach. Learn.},
  title        = {Learning programs with magic values},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data driven discovery of systems of ordinary differential
equations using nonconvex multitask learning. <em>ML</em>,
<em>112</em>(5), 1523–1549. (<a
href="https://doi.org/10.1007/s10994-023-06315-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In physical sciences, dynamic systems are modeled using their parameters within governing equations that often form a system of ordinary differential equations (SODE). This system consists of multiple equations, each of which relates the time derivative of a single parameter to several parameters. A parameter can appear in multiple equations, and this parameter potentially links the equations to each other. Although in certain cases the SODE can be written by domain experts, it is often unknown. With advances in sensor technology, large quantities of data can be sampled from dynamic systems, thus enabling the data-driven discovery of closed-form SODEs. State-of-the-art approaches are based on sparse single-task learning, which means that each equation from the SODE is learned independently. Omitting the coupling features of equations leads to SODEs that weakly identify the dynamic system. Furthermore, the convexity of the sparse penalty included in the learning criterion gives an SODE that is biased with respect to the true SODE. To reduce such a bias, we propose a multitask learning (MTL) based penalty which can learn the closed-form SODE with unbiasedness. The purpose of each task is to discover a single equation. But discovering an SODE is nontrivial, as dynamic systems are often nonlinear and the available data are noisy. Our proposal improves SODE identification by harnessing a nonconvex sparse matrix-structured penalty which takes into account the coupling feature as well as addresses the bias issue. Experimental results, based on noisy data simulated from known SODEs, confirm that, compared to single-task learning, MTL is more effective for recovering the closed-form SODE, and the proposed nonconvexity ensures that it can be estimated with unbiasedness. We also show the benefits of our approach on a real-world public dataset sampled from a laboratory-based ecological experiment.},
  archive      = {J_ML},
  author       = {Lejeune, Clément and Mothe, Josiane and Soubki, Adil and Teste, Olivier},
  doi          = {10.1007/s10994-023-06315-y},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1523-1549},
  shortjournal = {Mach. Learn.},
  title        = {Data driven discovery of systems of ordinary differential equations using nonconvex multitask learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning key steps to attack deep reinforcement learning
agents. <em>ML</em>, <em>112</em>(5), 1499–1522. (<a
href="https://doi.org/10.1007/s10994-023-06318-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning agents are vulnerable to adversarial attacks. In particular, recent studies have shown that attacking a few key steps can effectively decrease the agent’s cumulative reward. However, all existing attacking methods define those key steps with human-designed heuristics, and it is not clear how more effective key steps can be identified. This paper introduces a novel reinforcement learning framework that learns key steps through interacting with the agent. The proposed framework does not require any human heuristics nor knowledge, and can be flexibly coupled with any white-box or black-box adversarial attack scenarios. Experiments on benchmark Atari games across different scenarios demonstrate that the proposed framework is superior to existing methods for identifying effective key steps. The results highlight the weakness of RL agents even under budgeted attacks.},
  archive      = {J_ML},
  author       = {Yu, Chien-Min and Chen, Ming-Hsin and Lin, Hsuan-Tien},
  doi          = {10.1007/s10994-023-06318-9},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1499-1522},
  shortjournal = {Mach. Learn.},
  title        = {Learning key steps to attack deep reinforcement learning agents},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math display"><em>α</em></span> ILP: Thinking
visual scenes as differentiable logic programs. <em>ML</em>,
<em>112</em>(5), 1465–1497. (<a
href="https://doi.org/10.1007/s10994-023-06320-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural learning has shown remarkable performance at learning representations for visual object categorization. However, deep neural networks such as CNNs do not explicitly encode objects and relations among them. This limits their success on tasks that require a deep logical understanding of visual scenes, such as Kandinsky patterns and Bongard problems. To overcome these limitations, we introduce $$\alpha {\textit{ILP}}$$ , a novel differentiable inductive logic programming framework that learns to represent scenes as logic programs—intuitively, logical atoms correspond to objects, attributes, and relations, and clauses encode high-level scene information. $$\alpha$$ ILP has an end-to-end reasoning architecture from visual inputs. Using it, $$\alpha$$ ILP performs differentiable inductive logic programming on complex visual scenes, i.e., the logical rules are learned by gradient descent. Our extensive experiments on Kandinsky patterns and CLEVR-Hans benchmarks demonstrate the accuracy and efficiency of $$\alpha {\textit{ILP}}$$ in learning complex visual-logical concepts.},
  archive      = {J_ML},
  author       = {Shindo, Hikaru and Pfanschilling, Viktor and Dhami, Devendra Singh and Kersting, Kristian},
  doi          = {10.1007/s10994-023-06320-1},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1465-1497},
  shortjournal = {Mach. Learn.},
  title        = {$$\alpha$$ ILP: Thinking visual scenes as differentiable logic programs},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated proximal algorithm for regularized nonconvex
and nonsmooth bi-level optimization. <em>ML</em>, <em>112</em>(5),
1433–1463. (<a
href="https://doi.org/10.1007/s10994-023-06329-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important machine learning applications involve regularized nonconvex bi-level optimization. However, the existing gradient-based bi-level optimization algorithms cannot handle nonconvex or nonsmooth regularizers, and they suffer from a high computation complexity in nonconvex bi-level optimization. In this work, we study a proximal gradient-type algorithm that adopts the approximate implicit differentiation (AID) scheme for nonconvex bi-level optimization with possibly nonconvex and nonsmooth regularizers. In particular, the algorithm applies the Nesterov’s momentum to accelerate the computation of the implicit gradient involved in AID. We provide a comprehensive analysis of the global convergence properties of this algorithm through identifying its intrinsic potential function. In particular, we formally establish the convergence of the model parameters to a critical point of the bi-level problem, and obtain an improved computation complexity $$\widetilde{\mathcal {O}}(\kappa ^{3.5}\epsilon ^{-2})$$ over the state-of-the-art result. Moreover, we analyze the asymptotic convergence rates of this algorithm under a class of local nonconvex geometries characterized by a Łojasiewicz-type gradient inequality. Experiment on hyper-parameter optimization demonstrates the effectiveness of our algorithm.},
  archive      = {J_ML},
  author       = {Chen, Ziyi and Kailkhura, Bhavya and Zhou, Yi},
  doi          = {10.1007/s10994-023-06329-6},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1433-1463},
  shortjournal = {Mach. Learn.},
  title        = {An accelerated proximal algorithm for regularized nonconvex and nonsmooth bi-level optimization},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlated product of experts for sparse gaussian process
regression. <em>ML</em>, <em>112</em>(5), 1411–1432. (<a
href="https://doi.org/10.1007/s10994-022-06297-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes (GPs) are an important tool in machine learning and statistics. However, off-the-shelf GP inference procedures are limited to datasets with several thousand data points because of their cubic computational complexity. For this reason, many sparse GPs techniques have been developed over the past years. In this paper, we focus on GP regression tasks and propose a new approach based on aggregating predictions from several local and correlated experts. Thereby, the degree of correlation between the experts can vary between independent up to fully correlated experts. The individual predictions of the experts are aggregated taking into account their correlation resulting in consistent uncertainty estimates. Our method recovers independent Product of Experts, sparse GP and full GP in the limiting cases. The presented framework can deal with a general kernel function and multiple variables, and has a time and space complexity which is linear in the number of experts and data samples, which makes our approach highly scalable. We demonstrate superior performance, in a time vs. accuracy sense, of our proposed method against state-of-the-art GP approximations for synthetic as well as several real-world datasets with deterministic and stochastic optimization.},
  archive      = {J_ML},
  author       = {Schürch, Manuel and Azzimonti, Dario and Benavoli, Alessio and Zaffalon, Marco},
  doi          = {10.1007/s10994-022-06297-3},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1411-1432},
  shortjournal = {Mach. Learn.},
  title        = {Correlated product of experts for sparse gaussian process regression},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesizing explainable counterfactual policies for
algorithmic recourse with program synthesis. <em>ML</em>,
<em>112</em>(4), 1389–1409. (<a
href="https://doi.org/10.1007/s10994-022-06293-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to provide counterfactual interventions—sequences of actions we would have had to take for a desirable outcome to happen—is essential to explain how to change an unfavourable decision by a black-box machine learning model (e.g., being denied a loan request). Existing solutions have mainly focused on generating feasible interventions without providing explanations of their rationale. Moreover, they need to solve a separate optimization problem for each user. In this paper, we take a different approach and learn a program that outputs a sequence of explainable counterfactual actions given a user description and a causal graph. We leverage program synthesis techniques, reinforcement learning coupled with Monte Carlo Tree Search for efficient exploration, and rule learning to extract explanations for each recommended action. An experimental evaluation on synthetic and real-world datasets shows how our approach, FARE (eFficient counterfActual REcourse), generates effective interventions by making orders of magnitude fewer queries to the black-box classifier with respect to existing solutions, with the additional benefit of complementing them with interpretable explanations.},
  archive      = {J_ML},
  author       = {De Toni, Giovanni and Lepri, Bruno and Passerini, Andrea},
  doi          = {10.1007/s10994-022-06293-7},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1389-1409},
  shortjournal = {Mach. Learn.},
  title        = {Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imbalanced regression using regressor-classifier ensembles.
<em>ML</em>, <em>112</em>(4), 1365–1387. (<a
href="https://doi.org/10.1007/s10994-022-06199-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension to the federated ensemble regression using classification algorithm, an ensemble learning algorithm for regression problems which leverages the distribution of the samples in a learning set to achieve improved performance. We evaluated the extension using four classifiers and four regressors, two discretizers, and 119 responses from a wide variety of datasets in different domains. Additionally, we compared our algorithm to two resampling methods aimed at addressing imbalanced datasets. Our results show that the proposed extension is highly unlikely to perform worse than the base case, and on average outperforms the two resampling methods with significant differences in performance.},
  archive      = {J_ML},
  author       = {Orhobor, Oghenejokpeme I. and Grinberg, Nastasiya F. and Soldatova, Larisa N. and King, Ross D.},
  doi          = {10.1007/s10994-022-06199-4},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1365-1387},
  shortjournal = {Mach. Learn.},
  title        = {Imbalanced regression using regressor-classifier ensembles},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate models of radiative transfer codes for atmospheric
trace gas retrievals from satellite observations. <em>ML</em>,
<em>112</em>(4), 1337–1363. (<a
href="https://doi.org/10.1007/s10994-022-06155-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inversion of radiative transfer models (RTMs) is key to interpreting satellite observations of air quality and greenhouse gases, but is computationally expensive. Surrogate models that emulate the full forward physical RTM can speed up the simulation, reducing computational and timing costs and allowing the use of more advanced physics for trace gas retrievals. In this study, we present the development of surrogate models for two RTMs: the RemoTeC algorithm using the LINTRAN RTM and the SCIATRAN RTM. We estimate the intrinsic dimensionality of the input and output spaces and embed them in lower dimensional subspaces to facilitate the learning task. Two methods are tested for dimensionality reduction, autoencoders and principle component analysis (PCA), with PCA consistently outperforming autoencoders. Different sampling methods are employed for generating the training datasets: sampling focused on expected atmospheric parameters and latin hypercube sampling. The results show that models trained on the smaller (n = 1000) uniformly sampled dataset can perform as well as those trained on the larger (n = 50000), more focused dataset. Surrogate models for both datasets are able to accurately emulate Sentinel 5P spectra within a millisecond or less, as compared to the minutes or hours needed to simulate the full physical model. The SCIATRAN-trained forward surrogate models are able to generalize the emulation to a broader set of parameters and can be used for less constrained applications, while achieving a normalized RMSE of 7.3\%. On the other hand, models trained on the LINTRAN dataset can completely replace the RTM simulation in more focused expected ranges of atmospheric parameters, as they achieve a normalized RMSE of 0.3\%.},
  archive      = {J_ML},
  author       = {Brence, Jure and Tanevski, Jovan and Adams, Jennifer and Malina, Edward and Džeroski, Sašo},
  doi          = {10.1007/s10994-022-06155-2},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1337-1363},
  shortjournal = {Mach. Learn.},
  title        = {Surrogate models of radiative transfer codes for atmospheric trace gas retrievals from satellite observations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural predictor-based automated graph classifier framework.
<em>ML</em>, <em>112</em>(4), 1315–1335. (<a
href="https://doi.org/10.1007/s10994-022-06287-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Architecture Search (Graph-NAS) methods have shown great potential in finding better graph neural network designs compared to handcrafted designs. However, existing Graph-NAS frameworks are based on complex algorithms and fail to maintain low costs for high scalability with high performance. They require full training of thousands of graph neural networks to inform the search process, resulting in a prohibitive computational cost, which is not necessarily affordable for the users interested. Due to the computation cost, many researchers have limited the search space exploration ability, which may lead to a local optimum solution. In this paper, we propose a performance predictor-based graph neural architecture search (PGNAS) framework. The proposed approach consists of three conceptually much simpler and basic phases, and can broadly explore a search space with a much cheaper computation cost. We train n sampled architectures from a search space to generate n (architecture, validation accuracy) pairs used to train a performance distributions learner where the features are represented by the architecture description and the validation accuracy denotes the target. Next, we use this performance distribution learner to predict the validation accuracies of architectures in the search space. Finally, we train the top-K predicted architectures and choose the architecture with the best validation result. Although our approach seems simple, it is efficient and scalable; experiment results show that PGNAS outperforms existing both handcrafted and Graph-NAS models on four benchmark datasets.},
  archive      = {J_ML},
  author       = {Oloulade, Babatounde Moctard and Gao, Jianliang and Chen, Jiamin and Al-Sabri, Raeed and Lyu, Tengfei},
  doi          = {10.1007/s10994-022-06287-5},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1315-1335},
  shortjournal = {Mach. Learn.},
  title        = {Neural predictor-based automated graph classifier framework},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning approach using natural language processing
and time-series forecasting towards enhanced food safety. <em>ML</em>,
<em>112</em>(4), 1287–1313. (<a
href="https://doi.org/10.1007/s10994-022-06151-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various application domains/sectors, data collected from the respective industries are complemented with open data providing added value to the overall analysis and decision making process. Open data refer to weather data, transportation information, stock/investment products prices, or even health-related data. One of the application domains that could harvest the added-value of analytics (including open-data) refers to the food industry and more specifically the decisions related to food recalls. The collected data can be analyzed in real-time through Artificial Intelligence techniques and obtain insights about potential unsafe goods and products. These insights are exploited to drive decision making, such as which goods are more probable to be harmful in the near future and subsequently optimize the food supply chain. The latter reflects the overall food recall process monitoring and is enhanced through a data-driven forecasting approach. This provides actionable insights regarding the enhancement of the food safety across the food supply chain given that goods and products can become unsafe for plenty of reasons, such as mislabeling allergens, contamination etc. To address this challenge, this paper introduces a deep learning approach leveraging Natural Language Processing and Time-series Forecasting techniques, to monitor and analyze the risk associated with each food product category and the corresponding potential recalls. Furthermore, we propose a technique that exploits reinforcement learning to utilize historical recall announcements of food products for predicting their future recalls, thus providing insights to food companies regarding upcoming trends in food recalls that can lead to timely recalls. We also evaluate and demonstrate the effectiveness and added-value of the proposed approaches through a real-world scenario that yields promising results. While several techniques/models have been analyzed and applied to address the challenge of food recall predictions, the usage of analogous/surrogate data has also been studied and evaluated towards more accurate outcomes.},
  archive      = {J_ML},
  author       = {Makridis, Georgios and Mavrepis, Philip and Kyriazis, Dimosthenis},
  doi          = {10.1007/s10994-022-06151-6},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1287-1313},
  shortjournal = {Mach. Learn.},
  title        = {A deep learning approach using natural language processing and time-series forecasting towards enhanced food safety},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithm selection on a meta level. <em>ML</em>,
<em>112</em>(4), 1253–1286. (<a
href="https://doi.org/10.1007/s10994-022-06161-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of selecting an algorithm that appears most suitable for a specific instance of an algorithmic problem class, such as the Boolean satisfiability problem, is called instance-specific algorithm selection. Over the past decade, the problem has received considerable attention, resulting in a number of different methods for algorithm selection. Although most of these methods are based on machine learning, surprisingly little work has been done on meta learning, that is, on taking advantage of the complementarity of existing algorithm selection methods in order to combine them into a single superior algorithm selector. In this paper, we introduce the problem of meta algorithm selection, which essentially asks for the best way to combine a given set of algorithm selectors. We present a general methodological framework for meta algorithm selection as well as several concrete learning methods as instantiations of this framework, essentially combining ideas of meta learning and ensemble learning. In an extensive experimental evaluation, we demonstrate that ensembles of algorithm selectors can significantly outperform single algorithm selectors and have the potential to form the new state of the art in algorithm selection.},
  archive      = {J_ML},
  author       = {Tornede, Alexander and Gehring, Lukas and Tornede, Tanja and Wever, Marcel and Hüllermeier, Eyke},
  doi          = {10.1007/s10994-022-06161-4},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1253-1286},
  shortjournal = {Mach. Learn.},
  title        = {Algorithm selection on a meta level},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale principle of relevant information for
hyperspectral image classification. <em>ML</em>, <em>112</em>(4),
1227–1252. (<a
href="https://doi.org/10.1007/s10994-021-06011-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel architecture, termed multiscale principle of relevant information (MPRI), to learn discriminative spectral-spatial features for hyperspectral image classification. MPRI inherits the merits of the principle of relevant information (PRI) to effectively extract multiscale information embedded in the given data, and also takes advantage of the multilayer structure to learn representations in a coarse-to-fine manner. Specifically, MPRI performs spectral-spatial pixel characterization (using PRI) and feature dimensionality reduction (using regularized linear discriminant analysis) iteratively and successively. Extensive experiments on three benchmark data sets demonstrate that MPRI outperforms existing state-of-the-art methods (including deep learning based ones) qualitatively and quantitatively, especially in the scenario of limited training samples. Code of MPRI is available at http://bit.ly/MPRI_HSI.},
  archive      = {J_ML},
  author       = {Wei, Yantao and Yu, Shujian and Giraldo, Luis Sanchez and Príncipe, José C.},
  doi          = {10.1007/s10994-021-06011-9},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1227-1252},
  shortjournal = {Mach. Learn.},
  title        = {Multiscale principle of relevant information for hyperspectral image classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bimodal variational autoencoder for audiovisual speech
recognition. <em>ML</em>, <em>112</em>(4), 1201–1226. (<a
href="https://doi.org/10.1007/s10994-021-06112-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion is the idea of combining information in a joint representation of multiple modalities. The goal of multimodal fusion is to improve the accuracy of results from classification or regression tasks. This paper proposes a Bimodal Variational Autoencoder (BiVAE) model for audiovisual features fusion. Reliance on audiovisual signals in a speech recognition task increases the recognition accuracy, especially when an audio signal is corrupted. The BiVAE model is trained and validated on the CUAVE dataset. Three classifiers have evaluated the fused audiovisual features: Long-short Term Memory, Deep Neural Network, and Support Vector Machine. The experiment involves the evaluation of the fused features in the case of whether two modalities are available or there is only one modality available (i.e., cross-modality). The experimental results display the superiority of the proposed model (BiVAE) of audiovisual features fusion over the state-of-the-art models by an average accuracy difference $$\simeq $$ 3.28\% and 13.28\% for clean and noisy, respectively. Additionally, BiVAE outperforms the state-of-the-art models in the case of cross-modality by an accuracy difference $$\simeq $$ 2.79\% when the only audio signal is available and 1.88\% when the only video signal is available. Furthermore, SVM satisfies the best recognition accuracy compared with other classifiers.},
  archive      = {J_ML},
  author       = {Sayed, Hadeer M. and ElDeeb, Hesham E. and Taie, Shereen A.},
  doi          = {10.1007/s10994-021-06112-5},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1201-1226},
  shortjournal = {Mach. Learn.},
  title        = {Bimodal variational autoencoder for audiovisual speech recognition},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-aware process discovery for malware detection: An
empirical study. <em>ML</em>, <em>112</em>(4), 1171–1199. (<a
href="https://doi.org/10.1007/s10994-022-06154-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices are undeniably becoming essential in our lives and our daily activities. The adoption of mobile applications increases the human computing experience and the capability to access and exchange data. However, mobile devices are also the target of several malware attacks, usually obtained by evolving existing malicious code. This allows researchers and practitioners to recognize malware applications based on their similarities with existing infected applications. This study uses a multi-perspective declarative language to model the behavior of infected and trusted applications by discovering it from their system call traces. The obtained models are used to classify malware applications and evaluate if they belong to a known malware family. The approach has been evaluated on a dataset obtained by capturing system call traces from more than 160K trusted and infected applications, the latter gathered from 27 known malware families. The empirical study shows the good performance of the approach in the identification of the infected applications and their membership to a specific malware family. In addition, the approach exhibits a high level of robustness to code transformations and major evasion techniques.},
  archive      = {J_ML},
  author       = {Bernardi, Mario Luca and Cimitile, Marta and Maggi, Fabrizio Maria},
  doi          = {10.1007/s10994-022-06154-3},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1171-1199},
  shortjournal = {Mach. Learn.},
  title        = {Data-aware process discovery for malware detection: An empirical study},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Naive automated machine learning. <em>ML</em>,
<em>112</em>(4), 1131–1170. (<a
href="https://doi.org/10.1007/s10994-022-06200-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential task of automated machine learning ( $$\text {AutoML}$$ ) is the problem of automatically finding the pipeline with the best generalization performance on a given dataset. This problem has been addressed with sophisticated $$\text {black-box}$$ optimization techniques such as Bayesian optimization, grammar-based genetic algorithms, and tree search algorithms. Most of the current approaches are motivated by the assumption that optimizing the components of a pipeline in isolation may yield sub-optimal results. We present $$\text {Naive AutoML}$$ , an approach that precisely realizes such an in-isolation optimization of the different components of a pre-defined pipeline scheme. The returned pipeline is obtained by just taking the best algorithm of each slot. The isolated optimization leads to substantially reduced search spaces, and, surprisingly, this approach yields comparable and sometimes even better performance than current state-of-the-art optimizers.},
  archive      = {J_ML},
  author       = {Mohr, Felix and Wever, Marcel},
  doi          = {10.1007/s10994-022-06200-0},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1131-1170},
  shortjournal = {Mach. Learn.},
  title        = {Naive automated machine learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Satellite derived bathymetry using deep learning.
<em>ML</em>, <em>112</em>(4), 1107–1130. (<a
href="https://doi.org/10.1007/s10994-021-05977-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coastal development and urban planning are facing different issues including natural disasters and extreme storm events. The ability to track and forecast the evolution of the physical characteristics of coastal areas over time is an important factor in coastal development, risk mitigation and overall coastal zone management. Traditional bathymetry measurements are obtained using echo-sounding techniques which are considered expensive and not always possible due to various complexities. Remote sensing tools such as satellite imagery can be used to estimate bathymetry using incident wave signatures and inversion models such as physical models of waves. In this work, we present two novel approaches to bathymetry estimation using deep learning and we compare the two proposed methods in terms of accuracy, computational costs, and applicability to real data. We show that deep learning is capable of accurately estimating ocean depth in a variety of simulated cases which offers a new approach for bathymetry estimation and a novel application for deep learning.},
  archive      = {J_ML},
  author       = {Al Najar, Mahmoud and Thoumyre, Grégoire and Bergsma, Erwin W. J. and Almar, Rafael and Benshila, Rachid and Wilson, Dennis G.},
  doi          = {10.1007/s10994-021-05977-w},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1107-1130},
  shortjournal = {Mach. Learn.},
  title        = {Satellite derived bathymetry using deep learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large scale multi-output multi-class classification using
gaussian processes. <em>ML</em>, <em>112</em>(4), 1077–1106. (<a
href="https://doi.org/10.1007/s10994-022-06289-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-output Gaussian processes (MOGPs) can help to improve predictive performance for some output variables, by leveraging the correlation with other output variables. In this paper, our main motivation is to use multiple-output Gaussian processes to exploit correlations between outputs where each output is a multi-class classification problem. MOGPs have been mostly used for multi-output regression. There are some existing works that use MOGPs for other types of outputs, e.g., multi-output binary classification. However, MOGPs for multi-class classification has been less studied. The reason is twofold: 1) when using a softmax function, it is not clear how to scale it beyond the case of a few outputs; 2) most common type of data in multi-class classification problems consists of image data, and MOGPs are not specifically designed to image data. We thus propose a new MOGPs model called Multi-output Gaussian Processes with Augment &amp; Reduce (MOGPs-AR) that can deal with large scale classification and downsized image input data. Large scale classification is achieved by subsampling both training data sets and classes in each output whereas downsized image input data is handled by incorporating a convolutional kernel into the new model. We show empirically that our proposed model outperforms single-output Gaussian processes in terms of different performance metrics and multi-output Gaussian processes in terms of scalability, both in synthetic and in real classification problems. We include an example with the Ommiglot dataset where we showcase the properties of our model.},
  archive      = {J_ML},
  author       = {Ma, Chunchao and Álvarez, Mauricio A.},
  doi          = {10.1007/s10994-022-06289-3},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1077-1106},
  shortjournal = {Mach. Learn.},
  title        = {Large scale multi-output multi-class classification using gaussian processes},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond confusion matrix: Learning from multiple annotators
with awareness of instance features. <em>ML</em>, <em>112</em>(3),
1053–1075. (<a
href="https://doi.org/10.1007/s10994-022-06211-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from multiple annotators aims to induce a high-quality classifier from training instances, where each of them is associated with a set of observed labels provided by multiple annotators under the impact of their varying abilities and own biases. When modeling the probability transition process from latent true labels to observed labels, most existing methods adopt class-level confusion matrices of annotators which assume that observed labels do not depend on the instance features and are just determined by the true labels. However, in practice the labeling process of annotators is impacted not only by the correlation between classes but also by the content of instances. Thus using only class-level confusion matrices to characterize the probability transition process may limit the performance that the classifier can achieve. In this work, we propose the noise transition matrix, that incorporates the impact of instance features on annotators’ performance based on confusion matrices. Furthermore, we propose a simple and effective learning framework, which consists of a classifier module and a noise transition matrix module in a unified neural network architecture. Experimental results on synthetic and real datasets demonstrate the noise transition matrix is better than the confusion matrix for modeling multiple annotators and the superiority of our method in comparison with state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Li, Jingzheng and Sun, Hailong and Li, Jiyi},
  doi          = {10.1007/s10994-022-06211-x},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1053-1075},
  shortjournal = {Mach. Learn.},
  title        = {Beyond confusion matrix: Learning from multiple annotators with awareness of instance features},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Troubleshooting image segmentation models with
human-in-the-loop. <em>ML</em>, <em>112</em>(3), 1033–1051. (<a
href="https://doi.org/10.1007/s10994-021-06110-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation lays the foundation for many high-stakes vision applications such as autonomous driving and medical image analysis. It is, therefore, of great importance to not only improve the accuracy of segmentation models on well-established benchmarks, but also enhance their robustness in the real world so as to avoid sparse but fatal failures. In this paper, instead of chasing state-of-the-art performance on existing benchmarks, we turn our attention to a new challenging problem: how to efficiently expose failures of “top-performing” segmentation models in the real world and how to leverage such counterexamples to rectify the models. To achieve this with minimal human labelling effort, we first automatically sample a small set of images that are likely to falsify the target model from a large corpus of web images via the maximum discrepancy competition principle. We then propose a weakly labelling strategy to further reduce the number of false positives, before time-consuming pixel-level labelling by humans. Finally, we fine-tune the model to harness the identified failures, and repeat the whole process, resulting in an efficient and progressive framework for troubleshooting segmentation models. We demonstrate the feasibility of our framework using the semantic segmentation task in PASCAL VOC, and find that the fine-tuned model exhibits significantly improved generalization when applied to real-world images with greater content diversity. The code is available at https://github.com/VITA-Group/Troubleshooting_Image_Segmentation.},
  archive      = {J_ML},
  author       = {Wang, Haotao and Chen, Tianlong and Wang, Zhangyang and Ma, Kede},
  doi          = {10.1007/s10994-021-06110-7},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1033-1051},
  shortjournal = {Mach. Learn.},
  title        = {Troubleshooting image segmentation models with human-in-the-loop},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAFS: A domain aware few shot generative model for event
detection. <em>ML</em>, <em>112</em>(3), 1011–1031. (<a
href="https://doi.org/10.1007/s10994-022-06198-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more, large-scale pre-trained models show apparent advantages in solving the event detection (ED), i.e., a task to solve the problem of event classification by identifying trigger words. However, this kind of model depends heavily on labeled training data. Unfortunately, there is not enough such data for some particular areas, such as finance, due to the high cost of the data annotation process. Besides, the manually labeled training data has many problems like uneven sampling distribution, poor diversity, and massive long-tail data. Recently, some researchers have used the generative model to label data. However, training the generative models needs rich domain knowledge, which cannot be obtained from a Few-Shot resource. Therefore, we propose a Domain-Aware Few-Shot (DAFS) generative model that can generate domain based training data through a relatively small amount of labeled data. First, DAFS utilizes self-supervised information from various categories of sentences to calculate words’ transition probability under different domain and retain key triggers in each sentence. Then, we apply our joint algorithm to generate labeled training data that considers both diversity and effectiveness. Experimental results demonstrate that the training data generated by DAFS significantly improves the performance of ED in actual financial data. Especially when there are no more than 20 training data, DAFS can still ensure the generative quality to a certain extent. It also obtains new state-of-the-art results on ACE2005 multilingual corpora.},
  archive      = {J_ML},
  author       = {Xia, Nan and Yu, Hang and Wang, Yin and Xuan, Junyu and Luo, Xiangfeng},
  doi          = {10.1007/s10994-022-06198-5},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1011-1031},
  shortjournal = {Mach. Learn.},
  title        = {DAFS: A domain aware few shot generative model for event detection},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial vulnerability bounds for gaussian process
classification. <em>ML</em>, <em>112</em>(3), 971–1009. (<a
href="https://doi.org/10.1007/s10994-022-06224-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting ML classifiers from adversarial examples is crucial. We propose that the main threat is an attacker perturbing a confidently classified input to produce a confident misclassification. We consider in this paper the $$L_0$$ attack in which a small number of inputs can be perturbed by the attacker at test-time. To quantify the risk of this form of attack we have devised a formal guarantee in the form of an adversarial bound (AB) for a binary, Gaussian process classifier using the EQ kernel. This bound holds for the entire input domain, bounding the potential of any future adversarial attack to cause a confident misclassification. We explore how to extend to other kernels and investigate how to maximise the bound by altering the classifier (for example by using sparse approximations). We test the bound using a variety of datasets and show that it produces relevant and practical bounds for many of them.},
  archive      = {J_ML},
  author       = {Smith, Michael Thomas and Grosse, Kathrin and Backes, Michael and Álvarez, Mauricio A.},
  doi          = {10.1007/s10994-022-06224-6},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {971-1009},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial vulnerability bounds for gaussian process classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Responsible model deployment via model-agnostic uncertainty
learning. <em>ML</em>, <em>112</em>(3), 939–970. (<a
href="https://doi.org/10.1007/s10994-022-06248-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliably predicting potential failure risks of machine learning (ML) systems when deployed with production data is a crucial aspect of trustworthy AI. This paper introduces the Risk Advisor, a novel post-hoc meta-learner for estimating failure risks and predictive uncertainties of any already-trained black-box classification model. In addition to providing a risk score, the Risk Advisor decomposes the uncertainty estimates into aleatoric and epistemic uncertainty components, thus giving informative insights into the sources of uncertainty inducing the failures. Consequently, Risk Advisor can distinguish between failures caused by data variability, data shifts and model limitations and provide useful guidance on appropriate risk mitigation actions (e.g., collecting more data to counter data shift). Extensive experiments on various families of black-box classification models and on real-world and synthetic datasets covering common ML failure scenarios show that the Risk Advisor reliably predicts deployment-time failure risks in all the scenarios, and outperforms strong baselines.},
  archive      = {J_ML},
  author       = {Lahoti, Preethi and Gummadi, Krishna and Weikum, Gerhard},
  doi          = {10.1007/s10994-022-06248-y},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {939-970},
  shortjournal = {Mach. Learn.},
  title        = {Responsible model deployment via model-agnostic uncertainty learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference over radiative transfer models using variational
and expectation maximization methods. <em>ML</em>, <em>112</em>(3),
921–937. (<a href="https://doi.org/10.1007/s10994-021-05999-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earth observation from satellites offers the possibility to monitor our planet with unprecedented accuracy. Radiative transfer models (RTMs) encode the energy transfer through the atmosphere, and are used to model and understand the Earth system, as well as to estimate the parameters that describe the status of the Earth from satellite observations by inverse modeling. However, performing inference over such simulators is a challenging problem. RTMs are nonlinear, non-differentiable and computationally costly codes, which adds a high level of difficulty in inference. In this paper, we introduce two computational techniques to infer not only point estimates of biophysical parameters but also their joint distribution. One of them is based on a variational autoencoder approach and the second one is based on a Monte Carlo Expectation Maximization (MCEM) scheme. We compare and discuss benefits and drawbacks of each approach. We also provide numerical comparisons in synthetic simulations and the real PROSAIL model, a popular RTM that combines land vegetation leaf and canopy modeling. We analyze the performance of the two approaches for modeling and inferring the distribution of three key biophysical parameters for quantifying the terrestrial biosphere.},
  archive      = {J_ML},
  author       = {Svendsen, Daniel Heestermans and Hernández-Lobato, Daniel and Martino, Luca and Laparra, Valero and Moreno-Martínez, Álvaro and Camps-Valls, Gustau},
  doi          = {10.1007/s10994-021-05999-4},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {921-937},
  shortjournal = {Mach. Learn.},
  title        = {Inference over radiative transfer models using variational and expectation maximization methods},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Positive-unlabeled classification under class-prior shift: A
prior-invariant approach based on density ratio estimation. <em>ML</em>,
<em>112</em>(3), 889–919. (<a
href="https://doi.org/10.1007/s10994-022-06190-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from positive and unlabeled (PU) data is an important problem in various applications. Most of the recent approaches for PU classification assume that the class-prior (the ratio of positive samples) in the training unlabeled dataset is identical to that of the test data, which does not hold in many practical cases. In addition, we usually do not know the class-priors of the training and test data, thus we have no clue on how to train a classifier without them. To address these problems, we propose a novel PU classification method based on density ratio estimation. A notable advantage of our proposed method is that it does not require the class-priors in the training phase; class-prior shift is incorporated only in the test phase. We theoretically justify our proposed method and experimentally demonstrate its effectiveness.},
  archive      = {J_ML},
  author       = {Nakajima, Shota and Sugiyama, Masashi},
  doi          = {10.1007/s10994-022-06190-z},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {889-919},
  shortjournal = {Mach. Learn.},
  title        = {Positive-unlabeled classification under class-prior shift: A prior-invariant approach based on density ratio estimation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-constrained reinforcement learning with a
distributional safety critic. <em>ML</em>, <em>112</em>(3), 859–887. (<a
href="https://doi.org/10.1007/s10994-022-06187-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety is critical to broadening the real-world use of reinforcement learning. Modeling the safety aspects using a safety-cost signal separate from the reward and bounding the expected safety-cost is becoming standard practice, since it avoids the problem of finding a good balance between safety and performance. However, it can be risky to set constraints only on the expectation neglecting the tail of the distribution, which might have prohibitively large values. In this paper, we propose a method called Worst-Case Soft Actor Critic for safe RL that approximates the distribution of accumulated safety-costs to achieve risk control. More specifically, a certain level of conditional Value-at-Risk from the distribution is regarded as a safety constraint, which guides the change of adaptive safety weights to achieve a trade-off between reward and safety. As a result, we can compute policies whose worst-case performance satisfies the constraints. We investigate two ways to estimate the safety-cost distribution, namely a Gaussian approximation and a quantile regression algorithm. On the one hand, the Gaussian approximation is simple and easy to implement, but may underestimate the safety cost, on the other hand, the quantile regression leads to a more conservative behavior. The empirical analysis shows that the quantile regression method achieves excellent results in complex safety-constrained environments, showing good risk control.},
  archive      = {J_ML},
  author       = {Yang, Qisong and Simão, Thiago D. and Tindemans, Simon H. and Spaan, Matthijs T. J.},
  doi          = {10.1007/s10994-022-06187-8},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {859-887},
  shortjournal = {Mach. Learn.},
  title        = {Safety-constrained reinforcement learning with a distributional safety critic},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explicit explore, exploit, or escape ( <span
class="math display"><em>E</em><sup>4</sup></span> ): Near-optimal
safety-constrained reinforcement learning in polynomial time.
<em>ML</em>, <em>112</em>(3), 817–858. (<a
href="https://doi.org/10.1007/s10994-022-06201-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning (RL), an agent must explore an initially unknown environment in order to learn a desired behaviour. When RL agents are deployed in real world environments, safety is of primary concern. Constrained Markov decision processes (CMDPs) can provide long-term safety constraints; however, the agent may violate the constraints in an effort to explore its environment. This paper proposes a model-based RL algorithm called Explicit Explore, Exploit, or Escape ( $$E^{4}$$ ), which extends the Explicit Explore or Exploit ( $$E^{3}$$ ) algorithm to a robust CMDP setting. $$E^4$$ explicitly separates exploitation, exploration, and escape CMDPs, allowing targeted policies for policy improvement across known states, discovery of unknown states, as well as safe return to known states. $$E^4$$ robustly optimises these policies on the worst-case CMDP from a set of CMDP models consistent with the empirical observations of the deployment environment. Theoretical results show that $$E^4$$ finds a near-optimal constraint-satisfying policy in polynomial time whilst satisfying safety constraints throughout the learning process. We then discuss $$E^4$$ as a practical algorithmic framework, including robust-constrained offline optimisation algorithms, the design of uncertainty sets for the transition dynamics of unknown states, and how to further leverage empirical observations and prior knowledge to relax some of the worst-case assumptions underlying the theory.},
  archive      = {J_ML},
  author       = {Bossens, David M. and Bishop, Nicholas},
  doi          = {10.1007/s10994-022-06201-z},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {817-858},
  shortjournal = {Mach. Learn.},
  title        = {Explicit explore, exploit, or escape ( $$E^4$$ ): Near-optimal safety-constrained reinforcement learning in polynomial time},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine truth serum: A surprisingly popular approach to
improving ensemble methods. <em>ML</em>, <em>112</em>(3), 789–815. (<a
href="https://doi.org/10.1007/s10994-022-06183-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wisdom of the crowd (Surowiecki, 2005a) disclosed a striking fact that the majority voting answer from a crowd is usually more accurate than a few individual experts. The same story is observed in machine learning - ensemble methods (Dietterich, 2000) leverage this idea to exploit multiple machine learning algorithms in various settings e.g., supervised learning and semi-supervised learning to achieve better performance by aggregating the predictions of different algorithms than that obtained from any constituent algorithm alone. Nonetheless, the existing aggregating rule would fail when the majority answer of all the constituent algorithms is more likely to be wrong. In this paper, we extend the idea proposed in Bayesian Truth Serum (Prelec, 2004) that “a surprisingly more popular answer is more likely to be the true answer instead of the majority one” to supervised classification further improved by ensemble final predictions method and semi-supervised classification (e.g., MixMatch (Berthelot et al., 2019)) enhanced by ensemble data augmentations method. The challenge for us is to define or detect when an answer should be considered as being “surprising”. We present two machine learning aided methods which can reveal the truth when the minority instead of majority has the true answer on both settings of supervised and semi-supervised classification problems. We name our proposed method the Machine Truth Serum. Our experiments on a set of classification tasks (image, text, etc.) show that the classification performance can be further improved by applying Machine Truth Serum in the ensemble final predictions step (supervised) and in the ensemble data augmentations step (semi-supervised).},
  archive      = {J_ML},
  author       = {Luo, Tianyi and Liu, Yang},
  doi          = {10.1007/s10994-022-06183-y},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {789-815},
  shortjournal = {Mach. Learn.},
  title        = {Machine truth serum: A surprisingly popular approach to improving ensemble methods},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting upper atmospheric scalars advection using deep
learning: An <span class="math display"><em>O</em><sub>3</sub></span>
experiment. <em>ML</em>, <em>112</em>(3), 765–788. (<a
href="https://doi.org/10.1007/s10994-020-05944-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather forecast based on extrapolation methods is gathering a lot of attention due to the advance of artificial intelligence. Recent works on deep neural networks (CNN, RNN, LSTM, etc.) are enabling the development of spatiotemporal prediction models based on the analysis of historical time-series, images, and satellite data. In this paper, we focus on the use of deep learning for the forecast of stratospheric Ozone ( $$O_3$$ ), especially in the cases of exchanges between the polar vortex and mid-latitudes known as Ozone Secondary Events (OSE). Secondary effects of the Antarctic Ozone Hole are regularly observed above populated zones on South America, south of Africa, and New Zealand, resulting in abrupt reductions in the total ozone column of more than 10\% and a consequent increase in UV radiation in densely populated areas. We study different OSE events from the literature, comparing real data with predictions from our model. We obtained interesting results and insights that may lead to accurate and fast prediction models to forecast stratospheric Ozone and the occurrence of OSE.},
  archive      = {J_ML},
  author       = {Steffenel, Luiz Angelo and Anabor, Vagner and Kirsch Pinheiro, Damaris and Guzman, Lissette and Dornelles Bittencourt, Gabriela and Bencherif, Hassan},
  doi          = {10.1007/s10994-020-05944-x},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {765-788},
  shortjournal = {Mach. Learn.},
  title        = {Forecasting upper atmospheric scalars advection using deep learning: An $$O_3$$ experiment},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial learning for counterfactual fairness.
<em>ML</em>, <em>112</em>(3), 741–763. (<a
href="https://doi.org/10.1007/s10994-022-06206-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fairness has become an important topic in the machine learning research community. In particular, counterfactual fairness aims at building prediction models which ensure fairness at the most individual level. Rather than globally considering equity over the entire population, the idea is to imagine what any individual would look like with a variation of a given attribute of interest, such as a different gender or race for instance. Existing approaches rely on Variational Auto-encoding of individuals, using Maximum Mean Discrepancy (MMD) penalization to limit the statistical dependence of inferred representations with their corresponding sensitive attributes. This enables the simulation of counterfactual samples used for training the target fair model, the goal being to produce similar outcomes for every alternate version of any individual. In this work, we propose to rely on an adversarial neural learning approach, that enables more powerful inference than with MMD penalties, and is particularly better fitted for the continuous setting, where values of sensitive attributes cannot be exhaustively enumerated. Experiments show significant improvements in term of counterfactual fairness for both the discrete and the continuous settings.},
  archive      = {J_ML},
  author       = {Grari, Vincent and Lamprier, Sylvain and Detyniecki, Marcin},
  doi          = {10.1007/s10994-022-06206-8},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {741-763},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial learning for counterfactual fairness},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedding to reference t-SNE space addresses batch effects
in single-cell classification. <em>ML</em>, <em>112</em>(2), 721–740.
(<a href="https://doi.org/10.1007/s10994-021-06043-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction techniques, such as t-SNE, can construct informative visualizations of high-dimensional data. When jointly visualising multiple data sets, a straightforward application of these methods often fails; instead of revealing underlying classes, the resulting visualizations expose dataset-specific clusters. To circumvent these batch effects, we propose an embedding procedure that uses a t-SNE visualization constructed on a reference data set as a scaffold for embedding new data points. Each data instance from a new, unseen, secondary data is embedded independently and does not change the reference embedding. This prevents any interactions between instances in the secondary data and implicitly mitigates batch effects. We demonstrate the utility of this approach by analyzing six recently published single-cell gene expression data sets with up to tens of thousands of cells and thousands of genes. The batch effects in our studies are particularly strong as the data comes from different institutions using different experimental protocols. The visualizations constructed by our proposed approach are clear of batch effects, and the cells from secondary data sets correctly co-cluster with cells of the same type from the primary data. We also show the predictive power of our simple, visual classification approach in t-SNE space matches the accuracy of specialized machine learning techniques that consider the entire compendium of features that profile single cells.},
  archive      = {J_ML},
  author       = {Poličar, Pavlin G. and Stražar, Martin and Zupan, Blaž},
  doi          = {10.1007/s10994-021-06043-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {721-740},
  shortjournal = {Mach. Learn.},
  title        = {Embedding to reference t-SNE space addresses batch effects in single-cell classification},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relational data embeddings for feature enrichment with
background information. <em>ML</em>, <em>112</em>(2), 687–720. (<a
href="https://doi.org/10.1007/s10994-022-06277-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many machine-learning tasks, augmenting the data table at hand with features built from external sources is key to improving performance. For instance, estimating housing prices benefits from background information on the location, such as the population density or the average income. However, this information must often be assembled across many tables, requiring time and expertise from the data scientist. Instead, we propose to replace human-crafted features by vectorial representations of entities (e.g. cities) that capture the corresponding information. We represent the relational data on the entities as a graph and adapt graph-embedding methods to create feature vectors for each entity. We show that two technical ingredients are crucial: modeling well the different relationships between entities, and capturing numerical attributes. We adapt knowledge graph embedding methods that were primarily designed for graph completion. Yet, they model only discrete entities, while creating good feature vectors from relational data also requires capturing numerical attributes. For this, we introduce KEN: Knowledge Embedding with Numbers. We thoroughly evaluate approaches to enrich features with background information on 7 prediction tasks. We show that a good embedding model coupled with KEN can perform better than manually handcrafted features, while requiring much less human effort. It is also competitive with combinatorial feature engineering methods, but much more scalable. Our approach can be applied to huge databases, creating general-purpose feature vectors reusable in various downstream tasks.},
  archive      = {J_ML},
  author       = {Cvetkov-Iliev, Alexis and Allauzen, Alexandre and Varoquaux, Gaël},
  doi          = {10.1007/s10994-022-06277-7},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {687-720},
  shortjournal = {Mach. Learn.},
  title        = {Relational data embeddings for feature enrichment with background information},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchically structured task-agnostic continual learning.
<em>ML</em>, <em>112</em>(2), 655–686. (<a
href="https://doi.org/10.1007/s10994-022-06283-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One notable weakness of current machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We derive this principle from a Bayesian perspective and show its connections to previous approaches to continual learning. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Equipped with a diverse and specialized set of parameters, each path can be regarded as a distinct sub-network that learns to solve tasks. To improve expert allocation, we introduce diversity objectives, which we evaluate in additional ablation studies. Importantly, our approach can operate in a task-agnostic way, i.e., it does not require task-specific knowledge, as is the case with many existing continual learning algorithms. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method on continual reinforcement learning and variants of the MNIST, CIFAR-10, and CIFAR-100 datasets.},
  archive      = {J_ML},
  author       = {Hihn, Heinke and Braun, Daniel A.},
  doi          = {10.1007/s10994-022-06283-9},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {655-686},
  shortjournal = {Mach. Learn.},
  title        = {Hierarchically structured task-agnostic continual learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust federated learning under statistical heterogeneity
via hessian-weighted aggregation. <em>ML</em>, <em>112</em>(2), 633–654.
(<a href="https://doi.org/10.1007/s10994-022-06292-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning, client models are often trained on local training sets that vary in size and distribution. Such statistical heterogeneity in training data leads to performance variations across local models. Even within a model, some parameter estimates can be more reliable than others. Most existing FL approaches (such as FedAvg), however, do not explicitly address such variations in client parameter estimates and treat all local parameters with equal importance in the model aggregation. This disregard of varying evidential credence among client models often leads to slow convergence and a sensitive global model. We address this gap by proposing an aggregation mechanism based upon the Hessian matrix. Further, by making use of the first-order information of the loss function, we can use the Hessian as a scaling matrix in a manner akin to that employed in Quasi-Newton methods. This treatment captures the impact of data quality variations across local models. Experiments show that our method is superior to the baselines of Federated Average (FedAvg), FedProx, Federated Curvature (FedCurv) and Federated Newton Learn (FedNL) for image classification on MNIST, Fashion-MNIST, and CIFAR-10 datasets when the client models are trained using statistically heterogeneous data.},
  archive      = {J_ML},
  author       = {Ahmad, Adnan and Luo, Wei and Robles-Kelly, Antonio},
  doi          = {10.1007/s10994-022-06292-8},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {633-654},
  shortjournal = {Mach. Learn.},
  title        = {Robust federated learning under statistical heterogeneity via hessian-weighted aggregation},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Circular-symmetric correlation layer. <em>ML</em>,
<em>112</em>(2), 611–631. (<a
href="https://doi.org/10.1007/s10994-022-06288-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the vast success of standard planar convolutional neural networks, they are not the most efficient choice for analyzing signals that lie on an arbitrarily curved manifold, such as a cylinder. The problem arises when one performs a planar projection of these signals and inevitably causes them to be distorted or broken where there is valuable information. We propose a Circular-symmetric Correlation Layer (CCL) based on the formalism of roto-translation equivariant correlation on the continuous group $$S^1 \times \mathbb {R}$$ , and implement it efficiently using the well-known Fast Fourier Transform (FFT) algorithm. We showcase the performance analysis of a general network equipped with CCL on various recognition and classification tasks and datasets.},
  archive      = {J_ML},
  author       = {Azari, Bahar and Erdoğmuş, Deniz},
  doi          = {10.1007/s10994-022-06288-4},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {611-631},
  shortjournal = {Mach. Learn.},
  title        = {Circular-symmetric correlation layer},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient learning of large sets of locally optimal
classification rules. <em>ML</em>, <em>112</em>(2), 571–610. (<a
href="https://doi.org/10.1007/s10994-022-06290-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional rule learning algorithms aim at finding a set of simple rules, where each rule covers as many examples as possible. In this paper, we argue that the rules found in this way may not be the optimal explanations for each of the examples they cover. Instead, we propose an efficient algorithm that aims at finding the best rule covering each training example in a greedy optimization consisting of one specialization and one generalization loop. These locally optimal rules are collected and then filtered for a final rule set, which is much larger than the sets learned by conventional rule learning algorithms. A new example is classified by selecting the best among the rules that cover this example. In our experiments on small to very large datasets, the approach’s average classification accuracy is higher than that of state-of-the-art rule learning algorithms. Moreover, the algorithm is highly efficient and can inherently be processed in parallel without affecting the learned rule set and so the classification accuracy. We thus believe that it closes an important gap for large-scale classification rule induction.},
  archive      = {J_ML},
  author       = {Huynh, Van Quoc Phuong and Fürnkranz, Johannes and Beck, Florian},
  doi          = {10.1007/s10994-022-06290-w},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {571-610},
  shortjournal = {Mach. Learn.},
  title        = {Efficient learning of large sets of locally optimal classification rules},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FFNSL: Feed-forward neural-symbolic learner. <em>ML</em>,
<em>112</em>(2), 515–569. (<a
href="https://doi.org/10.1007/s10994-022-06278-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic-based machine learning aims to learn general, interpretable knowledge in a data-efficient manner. However, labelled data must be specified in a structured logical form. To address this limitation, we propose a neural-symbolic learning framework, called Feed-Forward Neural-Symbolic Learner (FFNSL), that integrates a logic-based machine learning system capable of learning from noisy examples, with neural networks, in order to learn interpretable knowledge from labelled unstructured data. We demonstrate the generality of FFNSL on four neural-symbolic classification problems, where different pre-trained neural network models and logic-based machine learning systems are integrated to learn interpretable knowledge from sequences of images. We evaluate the robustness of our framework by using images subject to distributional shifts, for which the pre-trained neural networks may predict incorrectly and with high confidence. We analyse the impact that these shifts have on the accuracy of the learned knowledge and run-time performance, comparing FFNSL to tree-based and pure neural approaches. Our experimental results show that FFNSL outperforms the baselines by learning more accurate and interpretable knowledge with fewer examples.},
  archive      = {J_ML},
  author       = {Cunnington, Daniel and Law, Mark and Lobo, Jorge and Russo, Alessandra},
  doi          = {10.1007/s10994-022-06278-6},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {515-569},
  shortjournal = {Mach. Learn.},
  title        = {FFNSL: Feed-forward neural-symbolic learner},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning multi-agent coordination through
connectivity-driven communication. <em>ML</em>, <em>112</em>(2),
483–514. (<a href="https://doi.org/10.1007/s10994-022-06286-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In artificial multi-agent systems, the ability to learn collaborative policies is predicated upon the agents’ communication skills: they must be able to encode the information received from the environment and learn how to share it with other agents as required by the task at hand. We present a deep reinforcement learning approach, Connectivity Driven Communication (CDC), that facilitates the emergence of multi-agent collaborative behaviour only through experience. The agents are modelled as nodes of a weighted graph whose state-dependent edges encode pair-wise messages that can be exchanged. We introduce a graph-dependent attention mechanisms that controls how the agents’ incoming messages are weighted. This mechanism takes into full account the current state of the system as represented by the graph, and builds upon a diffusion process that captures how the information flows on the graph. The graph topology is not assumed to be known a priori, but depends dynamically on the agents’ observations, and is learnt concurrently with the attention mechanism and policy in an end-to-end fashion. Our empirical results show that CDC is able to learn effective collaborative policies and can over-perform competing learning algorithms on cooperative navigation tasks.},
  archive      = {J_ML},
  author       = {Pesce, Emanuele and Montana, Giovanni},
  doi          = {10.1007/s10994-022-06286-6},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {483-514},
  shortjournal = {Mach. Learn.},
  title        = {Learning multi-agent coordination through connectivity-driven communication},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WINTENDED: WINdowed TENsor decomposition for densification
event detection in time-evolving networks. <em>ML</em>, <em>112</em>(2),
459–481. (<a href="https://doi.org/10.1007/s10994-021-05979-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Densification events in time-evolving networks refer to instants in which the network density, that is, the number of edges, is substantially larger than in the remaining. These events can occur at a global level, involving the majority of the nodes in the network, or at a local level involving only a subset of nodes.While global densification events affect the overall structure of the network, the same does not hold in local densification events, which may remain undetectable by the existing detection methods. In order to address this issue, we propose WINdowed TENsor decomposition for Densification Event Detection (WINTENDED) for the detection and characterization of both global and local densification events. Our method combines a sliding window decomposition with statistical tools to capture the local dynamics of the network and automatically find the irregular behaviours. According to our experimental evaluation, WINTENDED is able to spot global densification events at least as accurately as its competitors, while also being able to find local densification events, on the contrary to its competitors.},
  archive      = {J_ML},
  author       = {Fernandes, Sofia and Fanaee-T, Hadi and Gama, João and Tišljarić, Leo and Šmuc, Tomislav},
  doi          = {10.1007/s10994-021-05979-8},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {459-481},
  shortjournal = {Mach. Learn.},
  title        = {WINTENDED: WINdowed TENsor decomposition for densification event detection in time-evolving networks},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained regret minimization for multi-criterion
multi-armed bandits. <em>ML</em>, <em>112</em>(2), 431–458. (<a
href="https://doi.org/10.1007/s10994-022-06291-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic multi-armed bandit setting and study the problem of constrained regret minimization over a given time horizon. Each arm is associated with an unknown, possibly multi-dimensional distribution, and the merit of an arm is determined by several, possibly conflicting attributes. The aim is to optimize a ‘primary’ attribute subject to user-provided constraints on other ‘secondary’ attributes. We assume that the attributes can be estimated using samples from the arms’ distributions, and that the estimators enjoy suitable concentration properties. We propose an algorithm called Con-LCB that guarantees a logarithmic regret, i.e., the average number of plays of all non-optimal arms is at most logarithmic in the horizon. The algorithm also outputs a boolean flag that correctly identifies, with high probability, whether the given instance is feasible/infeasible with respect to the constraints. We also show that Con-LCB is optimal within a universal constant, i.e., that more sophisticated algorithms cannot do much better universally. Finally, we establish a fundamental trade-off between regret minimization and feasibility identification. Our framework finds natural applications, for instance, in financial portfolio optimization, where risk constrained maximization of expected return is meaningful.},
  archive      = {J_ML},
  author       = {Kagrecha, Anmol and Nair, Jayakrishnan and Jagannathan, Krishna},
  doi          = {10.1007/s10994-022-06291-9},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {431-458},
  shortjournal = {Mach. Learn.},
  title        = {Constrained regret minimization for multi-criterion multi-armed bandits},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Model-free inverse reinforcement learning
with multi-intention, unlabeled, and overlapping demonstrations.
<em>ML</em>, <em>112</em>(2), 429–430. (<a
href="https://doi.org/10.1007/s10994-022-06298-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Bighashdel, Ariyan and Jancura, Pavol and Dubbelman, Gijs},
  doi          = {10.1007/s10994-022-06298-2},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {429-430},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parameter-less algorithm for tensor co-clustering.
<em>ML</em>, <em>112</em>(2), 385–427. (<a
href="https://doi.org/10.1007/s10994-021-06002-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of the data produced by human activities and modern cyber-physical systems involve complex relations among their features. Such relations can be often represented by means of tensors, which can be viewed as generalization of matrices and, as such, can be analyzed by using higher-order extensions of existing machine learning methods, such as clustering and co-clustering. Tensor co-clustering, in particular, has been proven useful in many applications, due to its ability of coping with n-modal data and sparsity. However, setting up a co-clustering algorithm properly requires the specification of the desired number of clusters for each mode as input parameters. This choice is already difficult in relatively easy settings, like flat clustering on data matrices, but on tensors it could be even more frustrating. To face this issue, we propose a new tensor co-clustering algorithm that does not require the number of desired co-clusters as input, as it optimizes an objective function based on a measure of association across discrete random variables (called Goodman and Kruskal’s $$\tau$$ ) that is not affected by their cardinality. We introduce different optimization schemes and show their theoretical and empirical convergence properties. Additionally, we show the effectiveness of our algorithm on both synthetic and real-world datasets, also in comparison with state-of-the-art co-clustering methods based on tensor factorization and latent block models.},
  archive      = {J_ML},
  author       = {Battaglia, Elena and Pensa, Ruggero G.},
  doi          = {10.1007/s10994-021-06002-w},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {385-427},
  shortjournal = {Mach. Learn.},
  title        = {A parameter-less algorithm for tensor co-clustering},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A family of pairwise multi-marginal optimal transports that
define a generalized metric. <em>ML</em>, <em>112</em>(1), 353–384. (<a
href="https://doi.org/10.1007/s10994-022-06280-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Optimal transport (OT) problem is rapidly finding its way into machine learning. Favoring its use are its metric properties. Many problems admit solutions with guarantees only for objects embedded in metric spaces, and the use of non-metrics can complicate solving them. Multi-marginal OT (MMOT) generalizes OT to simultaneously transporting multiple distributions. It captures important relations that are missed if the transport only involves two distributions. Research on MMOT, however, has been focused on its existence, uniqueness, practical algorithms, and the choice of cost functions. There is a lack of discussion on the metric properties of MMOT, which limits its theoretical and practical use. Here, we prove new generalized metric properties for a family of pairwise MMOTs. We first explain the difficulty of proving this via two negative results. Afterward, we prove the MMOTs’ metric properties. Finally, we show that the generalized triangle inequality of this family of MMOTs cannot be improved. We illustrate the superiority of our MMOTs over other generalized metrics, and over non-metrics in both synthetic and real tasks.},
  archive      = {J_ML},
  author       = {Mi, Liang and Sheikholeslami, Azadeh and Bento, José},
  doi          = {10.1007/s10994-022-06280-y},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {353-384},
  shortjournal = {Mach. Learn.},
  title        = {A family of pairwise multi-marginal optimal transports that define a generalized metric},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconciling privacy and utility: An unscented kalman
filter-based framework for differentially private machine learning.
<em>ML</em>, <em>112</em>(1), 311–351. (<a
href="https://doi.org/10.1007/s10994-022-06279-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is extensively used in fields involving sensitive data, data holders are seeking to protect the privacy of data and build the ML models with high-quality utility. Differential privacy provides a feasible solution for them. However, there is a mutual constraint on the privacy and utility of the models under this solution. Therefore, how to improve (or even maximize) the utility of the model while preserving privacy becomes an urgent problem. To resolve this problem, we apply unscented Kalman filter (UKF) to various implementations of differential privacy (DP)-enabled ML (DPML). We propose a UKF-based DP-enabled ML (UKF-DPML) framework that achieves higher model utility with the given privacy budget $$\epsilon$$ . An evaluation module is included in the framework to ensure a fair estimation of DPML models. We validate the effectiveness of this framework through mathematical reasoning, followed by empirical evaluation of various implementations of UKF-DPML and DPML respectively. In the evaluation, we measure the ability of withstanding real-world privacy attacks and providing accurate classification, thus assessing the privacy and utility of the model. We conduct a range of privacy budgets and implementations on three datasets, each of which provides the same mathematical privacy guarantees. By measuring the resistance of UKF-DPML and DPML models to membership and attribute inference attacks and their classification accuracy, we obtain that applying UKF to the aggregates perturbed by DP noises results in higher utility with the same privacy budget and the effect of improved utility is related to the stage where UKF is applied.},
  archive      = {J_ML},
  author       = {Tang, Kunsheng and Li, Ping and Song, Yide and Luo, Tian},
  doi          = {10.1007/s10994-022-06279-5},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {311-351},
  shortjournal = {Mach. Learn.},
  title        = {Reconciling privacy and utility: An unscented kalman filter-based framework for differentially private machine learning},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic customer segmentation via hierarchical
fragmentation-coagulation processes. <em>ML</em>, <em>112</em>(1),
281–310. (<a href="https://doi.org/10.1007/s10994-022-06276-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding customer behavior is necessary to develop efficient marketing strategies or launch tailored programs with social value for the public. Customer segmentation is a critical task for understanding diverse and dynamic customer behavior. However, as the popularity of different products varies, building dynamic customer behavior models for products with few customers may overfit the data. In this paper, we propose a new Bayesian nonparametric model for dynamic customer segmentation—Hierarchical Fragmentation-Coagulation Processes (HFCP), which allows sharing behavior patterns across multiple products. We conduct comprehensive empirical evaluations using two real-world purchase datasets. Our results show that HFCP can: (i) determine the number of groups required to model diverse customer behavior automatically; (ii) capture the changes such as split and merge of customer groups over time; (iii) discover behavior patterns shared among products and identify products with similar or different purchase behavior impacted by promotion, brand choice and change of seasons; and (iv) overcome overfitting problems and outperform previous customer segmentation models on estimating behavior for unseen customers. Hence, HFCP is a flexible and accurate segmentation model that can be used by stakeholders to understand dynamic customer behavior and compare the purchase behavior for different products.},
  archive      = {J_ML},
  author       = {Luo, Ling and Li, Bin and Fan, Xuhui and Wang, Yang and Koprinska, Irena and Chen, Fang},
  doi          = {10.1007/s10994-022-06276-8},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {281-310},
  shortjournal = {Mach. Learn.},
  title        = {Dynamic customer segmentation via hierarchical fragmentation-coagulation processes},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing p-laplacian: Spectral hypergraph theory and a
partitioning algorithm. <em>ML</em>, <em>112</em>(1), 241–280. (<a
href="https://doi.org/10.1007/s10994-022-06264-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For hypergraph clustering, various methods have been proposed to define hypergraph p-Laplacians in the literature. This work proposes a general framework for an abstract class of hypergraph p-Laplacians from a differential-geometric view. This class includes previously proposed hypergraph p-Laplacians and also includes previously unstudied novel generalizations. For this abstract class, we extend current spectral theory by providing an extension of nodal domain theory for the eigenvectors of our hypergraph p-Laplacian. We use this nodal domain theory to provide bounds on the eigenvalues via a higher-order Cheeger inequality. Following our extension of spectral theory, we propose a novel hypergraph partitioning algorithm for our generalized p-Laplacian. Our empirical study shows that our algorithm outperforms spectral methods based on existing p-Laplacians.},
  archive      = {J_ML},
  author       = {Saito, Shota and Herbster, Mark},
  doi          = {10.1007/s10994-022-06264-y},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {241-280},
  shortjournal = {Mach. Learn.},
  title        = {Generalizing p-laplacian: Spectral hypergraph theory and a partitioning algorithm},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-armed bandits with censored consumption of resources.
<em>ML</em>, <em>112</em>(1), 217–240. (<a
href="https://doi.org/10.1007/s10994-022-06271-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a resource-aware variant of the classical multi-armed bandit problem: In each round, the learner selects an arm and determines a resource limit. It then observes a corresponding (random) reward, provided the (random) amount of consumed resources remains below the limit. Otherwise, the observation is censored, i.e., no reward is obtained. For this problem setting, we introduce a measure of regret, which incorporates both the actual amount of consumed resources of each learning round and the optimality of realizable rewards as well as the risk of exceeding the allocated resource limit. Thus, to minimize regret, the learner needs to set a resource limit and choose an arm in such a way that the chance to realize a high reward within the predefined resource limit is high, while the resource limit itself should be kept as low as possible. We propose a UCB-inspired online learning algorithm, which we analyze theoretically in terms of its regret upper bound. In a simulation study, we show that our learning algorithm outperforms straightforward extensions of standard multi-armed bandit algorithms.},
  archive      = {J_ML},
  author       = {Bengs, Viktor and Hüllermeier, Eyke},
  doi          = {10.1007/s10994-022-06271-z},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {217-240},
  shortjournal = {Mach. Learn.},
  title        = {Multi-armed bandits with censored consumption of resources},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limits of multi-relational graphs. <em>ML</em>,
<em>112</em>(1), 177–216. (<a
href="https://doi.org/10.1007/s10994-022-06281-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphons are limits of large graphs. Motivated by a theoretical problem from statistical relational learning, we develop a generalization of basic results from graphon theory into the “multi-relational” setting. We show that their multi-relational counterparts, which we call multi-relational graphons, are analogically limits of large multi-relational graphs. We extend the cut-distance topology for graphons to multi-relational graphons and prove its compactness and the density of multi-relational graphs in this topology. In turn, compactness enables to prove the large deviation principle for Multi-Relational Graphs (LDP) which enables to prove the most typical random graphs constrained by marginal statistics converge asymptotically to constrained multi-relational graphons with maximum entropy. We show the equivalence between a restricted version of Markov Logic Network and Multi-Relational Graphons with maximum entropy.},
  archive      = {J_ML},
  author       = {Alvarado, Juan and Wang, Yuyi and Ramon, Jan},
  doi          = {10.1007/s10994-022-06281-x},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {177-216},
  shortjournal = {Mach. Learn.},
  title        = {Limits of multi-relational graphs},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Root-finding approaches for computing conformal prediction
set. <em>ML</em>, <em>112</em>(1), 151–176. (<a
href="https://doi.org/10.1007/s10994-022-06233-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction constructs a confidence set for an unobserved response of a feature vector based on previous identically distributed and exchangeable observations of responses and features. It has a coverage guarantee at any nominal level without additional assumptions on their distribution. Its computation deplorably requires a refitting procedure for all replacement candidates of the target response. In regression settings, this corresponds to an infinite number of model fits. Apart from relatively simple estimators that can be written as pieces of linear function of the response, efficiently computing such sets is difficult, and is still considered as an open problem. We exploit the fact that, often, conformal prediction sets are intervals whose boundaries can be efficiently approximated by classical root-finding algorithms. We investigate how this approach can overcome many limitations of formerly used strategies; we discuss its complexity and drawbacks.},
  archive      = {J_ML},
  author       = {Ndiaye, Eugene and Takeuchi, Ichiro},
  doi          = {10.1007/s10994-022-06233-5},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {151-176},
  shortjournal = {Mach. Learn.},
  title        = {Root-finding approaches for computing conformal prediction set},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning of network bottlenecks via minimax paths.
<em>ML</em>, <em>112</em>(1), 131–150. (<a
href="https://doi.org/10.1007/s10994-022-06270-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study bottleneck identification in networks via extracting minimax paths. Many real-world networks have stochastic weights for which full knowledge is not available in advance. Therefore, we model this task as a combinatorial semi-bandit problem to which we apply a combinatorial version of Thompson Sampling and establish an upper bound on the corresponding Bayesian regret. Due to the computational intractability of the problem, we then devise an alternative problem formulation which approximates the original objective. Finally, we experimentally evaluate the performance of Thompson Sampling with the approximate formulation on real-world directed and undirected networks.},
  archive      = {J_ML},
  author       = {Åkerblom, Niklas and Hoseini, Fazeleh Sadat and Haghir Chehreghani, Morteza},
  doi          = {10.1007/s10994-022-06270-0},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {131-150},
  shortjournal = {Mach. Learn.},
  title        = {Online learning of network bottlenecks via minimax paths},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAP inference algorithms without approximation for
collective graphical models on path graphs via discrete difference of
convex algorithm. <em>ML</em>, <em>112</em>(1), 99–129. (<a
href="https://doi.org/10.1007/s10994-022-06275-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective graphical model (CGM) is a probabilistic model that provides a framework for analyzing aggregated count data. Maximum a posteriori (MAP) inference of unobserved variables under given observations is one of the essential operations in CGM. Because the MAP inference problem is known to be NP-hard in general, the current mainstream approach is to solve an alternative problem obtained by approximating the objective function and applying continuous relaxation. However, this approach has two significant drawbacks. First, the quality of the solution deteriorates when the values in the count data are negligible due to the inaccuracy of Stirling’s approximation. Second, the application of continuous relaxation causes the violation of integrality constraints. This paper proposes novel algorithms for MAP inference in CGMs on path graphs to overcome these problems. Our method is based on the discrete difference of convex algorithm (DCA); DCA is a general framework to minimize the sum of a convex function and a concave function by repeatedly minimizing surrogate functions. Utilizing the particular structure of path graphs, we efficiently solve the surrogate function minimization by minimum convex cost flow algorithms. Furthermore, our approach also leads to a new method of solving another important task; MAP inference of the sample size in CGM on path graphs. Our method is naturally applicable to this task, allowing us to design very efficient algorithms. Experimental results on synthetic and real-world datasets show the effectiveness of the proposed algorithms.},
  archive      = {J_ML},
  author       = {Akagi, Yasunori and Marumo, Naoki and Kim, Hideaki and Kurashima, Takeshi and Toda, Hiroyuki},
  doi          = {10.1007/s10994-022-06275-9},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {99-129},
  shortjournal = {Mach. Learn.},
  title        = {MAP inference algorithms without approximation for collective graphical models on path graphs via discrete difference of convex algorithm},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partially hidden markov chain multivariate linear
autoregressive model: Inference and forecasting—application to machine
health prognostics. <em>ML</em>, <em>112</em>(1), 45–97. (<a
href="https://doi.org/10.1007/s10994-022-06209-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series subject to regime shifts have attracted much interest in domains such as econometry, finance or meteorology. For discrete-valued regimes, models such as the popular Hidden Markov Chain (HMC) describe time series whose state process is unknown at all time-steps. Sometimes, time series are annotated. Thus, another category of models handles the case with regimes observed at all time-steps. We present a novel model which addresses the intermediate case: (i) state processes associated to such time series are modelled by Partially Hidden Markov Chains (PHMCs); (ii) a multivariate linear autoregressive (MLAR) model drives the dynamics of the time series, within each regime. We describe a variant of the expectation maximization (EM) algorithm devoted to PHMC-MLAR model learning. We propose a hidden state inference procedure and a forecasting function adapted to the semi-supervised framework. We first assess inference and prediction performances, and analyze EM convergence times for PHMC-MLAR, using simulated data. We show the benefits of using partially observed states as well as a fully labelled scheme with unreliable labels, to decrease EM convergence times. We highlight the robustness of PHMC-MLAR to labelling errors in inference and prediction tasks. Finally, using turbofan engine data from a NASA repository, we show that PHMC-MLAR outperforms or largely outperforms other models: PHMC and MSAR (Markov Switching AutoRegressive model) for the feature prediction task, PHMC and five out of six recent state-of-the-art methods for the prediction of machine useful remaining life.},
  archive      = {J_ML},
  author       = {Dama, Fatoumata and Sinoquet, Christine},
  doi          = {10.1007/s10994-022-06209-5},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {45-97},
  shortjournal = {Mach. Learn.},
  title        = {Partially hidden markov chain multivariate linear autoregressive model: Inference and forecasting—application to machine health prognostics},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLISEMAP: Supervised dimensionality reduction through local
explanations. <em>ML</em>, <em>112</em>(1), 1–43. (<a
href="https://doi.org/10.1007/s10994-022-06261-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for explaining black box learning models often focus on building local explanations of the models’ behaviour for particular data items. It is possible to create global explanations for all data items, but these explanations generally have low fidelity for complex black box models. We propose a new supervised manifold visualisation method, slisemap, that simultaneously finds local explanations for all data items and builds a (typically) two-dimensional global visualisation of the black box model such that data items with similar local explanations are projected nearby. We provide a mathematical derivation of our problem and an open source implementation implemented using the GPU-optimised PyTorch library. We compare slisemap to multiple popular dimensionality reduction methods and find that slisemap is able to utilise labelled data to create embeddings with consistent local white box models. We also compare slisemap to other model-agnostic local explanation methods and show that slisemap provides comparable explanations and that the visualisations can give a broader understanding of black box regression and classification models.},
  archive      = {J_ML},
  author       = {Björklund, Anton and Mäkelä, Jarmo and Puolamäki, Kai},
  doi          = {10.1007/s10994-022-06261-1},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {1-43},
  shortjournal = {Mach. Learn.},
  title        = {SLISEMAP: Supervised dimensionality reduction through local explanations},
  volume       = {112},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
