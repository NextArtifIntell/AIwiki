<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv---174">IJCV - 174</h2>
<ul>
<li><details>
<summary>
(2023). GLENet: Boosting 3D object detectors with generative label
uncertainty estimation. <em>IJCV</em>, <em>131</em>(12), 3332–3352. (<a
href="https://doi.org/10.1007/s11263-023-01869-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent ambiguity in ground-truth annotations of 3D bounding boxes, caused by occlusions, signal missing, or manual annotation errors, can confuse deep 3D object detectors during training, thus deteriorating detection accuracy. However, existing methods overlook such issues to some extent and treat the labels ass deterministic. In this paper, we formulate the label uncertainty problem as the diversity of potentially plausible bounding boxes of objects. Then, we propose GLENet, a generative framework adapted from conditional variational autoencoders, to model the one-to-many relationship between a typical 3D object and its potential ground-truth bounding boxes with latent variables. The label uncertainty generated by GLENet is a plug-and-play module and can be conveniently integrated into existing deep 3D detectors to build probabilistic detectors and supervise the learning of the localization uncertainty. Besides, we propose an uncertainty-aware quality estimator architecture in probabilistic detectors to guide the training of the IoU-branch with predicted localization uncertainty. We incorporate the proposed methods into various popular base 3D detectors and demonstrate significant and consistent performance gains on both KITTI and Waymo benchmark datasets. Especially, the proposed GLENet-VR outperforms all published LiDAR-based approaches by a large margin and achieves the top rank among single-modal methods on the challenging KITTI test set. The source code and pre-trained models are publicly available at https://github.com/Eaphan/GLENet .},
  archive      = {J_IJCV},
  author       = {Zhang, Yifan and Zhang, Qijian and Zhu, Zhiyu and Hou, Junhui and Yuan, Yixuan},
  doi          = {10.1007/s11263-023-01869-9},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3332-3352},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {GLENet: Boosting 3D object detectors with generative label uncertainty estimation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial coreset selection for efficient robust training.
<em>IJCV</em>, <em>131</em>(12), 3307–3331. (<a
href="https://doi.org/10.1007/s11263-023-01860-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection objective to reduce the training set size effectively. Once built, we run adversarial training over this subset of the training data. Unlike existing methods, our approach can be adapted to a wide variety of training objectives, including TRADES, $$\ell _p$$ -PGD, and Perceptual Adversarial Training. We conduct extensive experiments to demonstrate that our approach speeds up adversarial training by 2–3 times while experiencing a slight degradation in the clean and robust accuracy.},
  archive      = {J_IJCV},
  author       = {Dolatabadi, Hadi M. and Erfani, Sarah M. and Leckie, Christopher},
  doi          = {10.1007/s11263-023-01860-4},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3307-3331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adversarial coreset selection for efficient robust training},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic contrastive bootstrapping for single-positive
multi-label recognition. <em>IJCV</em>, <em>131</em>(12), 3289–3306. (<a
href="https://doi.org/10.1007/s11263-023-01849-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning multi-label image recognition with incomplete annotation is gaining popularity due to its superior performance and significant labor savings when compared to training with fully labeled datasets. Existing literature mainly focuses on label completion and co-occurrence learning while facing difficulties with the most common single-positive label manner. To tackle this problem, we present a semantic contrastive bootstrapping (Scob) approach to gradually recover the cross-object relationships by introducing class activation as semantic guidance. With this learning guidance, we then propose a recurrent semantic masked transformer to extract iconic object-level representations and delve into the contrastive learning problems on multi-label classification tasks. We further propose a bootstrapping framework in an Expectation-Maximization fashion that iteratively optimizes the network parameters and refines semantic guidance to alleviate possible disturbance caused by wrong semantic guidance. Extensive experimental results demonstrate that the proposed joint learning framework surpasses the state-of-the-art models by a large margin on four public multi-label image recognition benchmarks. Codes can be found at https://github.com/iCVTEAM/Scob .},
  archive      = {J_IJCV},
  author       = {Chen, Cheng and Zhao, Yifan and Li, Jia},
  doi          = {10.1007/s11263-023-01849-z},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3289-3306},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic contrastive bootstrapping for single-positive multi-label recognition},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic context removal: A general training strategy for
robust models on video action predictive tasks. <em>IJCV</em>,
<em>131</em>(12), 3272–3288. (<a
href="https://doi.org/10.1007/s11263-023-01850-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting future actions is an essential feature of intelligent systems and embodied AI. However, compared to the traditional recognition tasks, the uncertainty of the future and the reasoning ability requirement make prediction tasks very challenging and far beyond solved. In this field, previous methods usually care more about the model architecture design but little attention has been put on how to train models with a proper learning policy. To this end, in this work, we propose a simple but effective training strategy, Dynamic Context Removal (DCR), which dynamically schedules the visibility of context in different training stages. It follows the human-like curriculum learning process, i.e., gradually removing the event context to increase the prediction difficulty till satisfying the final prediction target. Besides, we explore how to train robust models that give consistent predictions at different levels of observable context. Our learning scheme is plug-and-play and easy to integrate widely-used reasoning models including Transformer and LSTM, with advantages in both effectiveness and efficiency. We study two action prediction problems, i.e., Video Action Anticipation and Early Action Recognition. In extensive experiments, our method achieves state-of-the-art results on several widely-used benchmarks.},
  archive      = {J_IJCV},
  author       = {Xu, Xinyu and Li, Yong-Lu and Lu, Cewu},
  doi          = {10.1007/s11263-023-01850-6},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3272-3288},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dynamic context removal: A general training strategy for robust models on video action predictive tasks},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward practical weakly supervised semantic segmentation via
point-level supervision. <em>IJCV</em>, <em>131</em>(12), 3252–3271. (<a
href="https://doi.org/10.1007/s11263-023-01862-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) aims to reduce the cost of collecting dense pixel-level annotations for segmentation models by adopting weak labels to train. Although WSSS methods have achieved great success, recent approaches mainly concern the image-level label-based WSSS, which is limited to object-centric datasets instead of more challenging practical datasets that contain many co-occurrent classes. In comparison, point-level labels could provide some spatial information to address the class co-occurrent confusion problem. Meanwhile, it only requires an additional click when recognizing the targets, which is of negligible annotation overhead. Thus, we choose to study utilizing point labels for the general-purpose WSSS. The main difficulty of utilizing point-level labels is bridging the gap between the sparse point-level labels and the dense pixel-level predictions. To alleviate this problem, we propose a superpixel augmented pseudo-mask generation strategy and a class-aware contrastive learning approach, which manages to recover reliable dense constraints and apply them both to the segmentation models’ final prediction and the intermediate features. Diagnostic experiments on the challenging Pascal VOC, Cityscapes, and the ADE20k datasets demonstrate that our approach can efficiently and effectively compensate for the sparse point-level labels and achieve cutting-edge performance on the point-based segmentation problems.},
  archive      = {J_IJCV},
  author       = {Fan, Junsong and Zhang, Zhaoxiang},
  doi          = {10.1007/s11263-023-01862-2},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3252-3271},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Toward practical weakly supervised semantic segmentation via point-level supervision},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STP-SOM: Scale-transfer learning for pansharpening via
estimating spectral observation model. <em>IJCV</em>, <em>131</em>(12),
3226–3251. (<a
href="https://doi.org/10.1007/s11263-023-01840-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening strives to improve the spatial resolution of multi-spectral images while maintaining spectral fidelity. However, existing methods usually cannot guarantee a balance between spatial and spectral quality, and degrade when handling the desirable scale of full resolution. To address these challenges, this prospective study proposes a scale-transfer learning framework via estimating spectral observation model. Specifically, we design a cross-spectral transfer network to learn an expected spectral observation model by the cycle adversarial between spectral degradation and interpolation, which describes the accurate nonlinear mapping process from multi-spectral to panchromatic images. Having the favorable spectral observation model established, a scale-transfer pansharpening paradigm with co-learning of full and reduced resolutions can be constructed. First, we develop a spectral observation model-based spatial fidelity term at the reduced-resolution scale, which can alleviate the imbalance problem of spectral and spatial information widespread in current supervised paradigms. Second, we explore the reprojection regularization from full to reduced resolution based on the spectral observation model, which facilitates the ability of the pansharpening model to be extended to the scale of full resolution. Extensive experiments demonstrate the advantage of our method over the current state-of-the-arts in terms of information balance and scale transformation. We further apply our method to produce the high-resolution normalized difference vegetation index and achieve vegetation enhancement with competitive performance. Moreover, our method is lightweight and faster than other comparative methods.},
  archive      = {J_IJCV},
  author       = {Zhang, Hao and Ma, Jiayi},
  doi          = {10.1007/s11263-023-01840-8},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3226-3251},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {STP-SOM: Scale-transfer learning for pansharpening via estimating spectral observation model},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pyramid attention network for image restoration.
<em>IJCV</em>, <em>131</em>(12), 3207–3225. (<a
href="https://doi.org/10.1007/s11263-023-01843-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity refers to the image prior widely used in image restoration algorithms that small but similar patterns tend to occur at different locations and scales. However, recent advanced deep convolutional neural network-based methods for image restoration do not take full advantage of self-similarities by relying on self-attention neural modules that only process information at the same scale. To solve this problem, we present a novel Pyramid Attention module for image restoration, which captures long-range feature correspondences from a multi-scale feature pyramid. Inspired by the fact that corruptions, such as noise or compression artifacts, drop drastically at coarser image scales, our attention module is designed to be able to borrow clean signals from their “clean” correspondences at the coarser levels. The proposed pyramid attention module is a generic building block that can be flexibly integrated into various neural architectures. Its effectiveness is validated through extensive experiments on multiple image restoration tasks: image denoising, demosaicing, compression artifact reduction, and super resolution. Without any bells and whistles, our PANet (pyramid attention module with simple network backbones) can produce state-of-the-art results with superior accuracy and visual quality. Our code is available at https://github.com/SHI-Labs/Pyramid-Attention-Networks},
  archive      = {J_IJCV},
  author       = {Mei, Yiqun and Fan, Yuchen and Zhang, Yulun and Yu, Jiahui and Zhou, Yuqian and Liu, Ding and Fu, Yun and Huang, Thomas S. and Shi, Humphrey},
  doi          = {10.1007/s11263-023-01843-5},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3207-3225},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pyramid attention network for image restoration},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient annotation and learning for 3D hand pose
estimation: A survey. <em>IJCV</em>, <em>131</em>(12), 3193–3206. (<a
href="https://doi.org/10.1007/s11263-023-01856-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this survey, we present a systematic review of 3D hand pose estimation from the perspective of efficient annotation and learning. 3D hand pose estimation has been an important research area owing to its potential to enable various applications, such as video understanding, AR/VR, and robotics. However, the performance of models is tied to the quality and quantity of annotated 3D hand poses. Under the status quo, acquiring such annotated 3D hand poses is challenging, e.g., due to the difficulty of 3D annotation and the presence of occlusion. To reveal this problem, we review the pros and cons of existing annotation methods classified as manual, synthetic-model-based, hand-sensor-based, and computational approaches. Additionally, we examine methods for learning 3D hand poses when annotated data are scarce, including self-supervised pretraining, semi-supervised learning, and domain adaptation. Based on the study of efficient annotation and learning, we further discuss limitations and possible future directions in this field.},
  archive      = {J_IJCV},
  author       = {Ohkawa, Takehiko and Furuta, Ryosuke and Sato, Yoichi},
  doi          = {10.1007/s11263-023-01856-0},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3193-3206},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Efficient annotation and learning for 3D hand pose estimation: A survey},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PIDray: A large-scale x-ray benchmark for real-world
prohibited item detection. <em>IJCV</em>, <em>131</em>(12), 3170–3192.
(<a href="https://doi.org/10.1007/s11263-023-01855-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic security inspection relying on computer vision technology is a challenging task in real-world scenarios due to many factors, such as intra-class variance, class imbalance, and occlusion. Most previous methods rarely touch the cases where the prohibited items are deliberately hidden in messy objects because of the scarcity of large-scale datasets, hindering their applications. To address this issue and facilitate related research, we present a large-scale dataset, named PIDray, which covers various cases in real-world scenarios for prohibited item detection, especially for deliberately hidden items. In specific, PIDray collects 124, 486 X-ray images for 12 categories of prohibited items, and each image is manually annotated with careful inspection, which characterizes it, to our best knowledge, with the largest volume and varieties of annotated images with prohibited items to date. Meanwhile, we propose a general divide-and-conquer pipeline to develop baseline algorithms on PIDray. Specifically, we adopt the tree-like structure to suppress the influence of the long-tailed issue in the PIDray dataset, where the first course-grained node is tasked with the binary classification to alleviate the influence of head category, while the subsequent fine-grained node is dedicated to the specific tasks of the tail categories. Based on this simple yet effective scheme, we offer strong task-specific baselines across object detection, instance segmentation, and multi-label classification tasks and verify the generalization ability on common datasets (e.g., COCO and PASCAL VOC). Extensive experiments on PIDray demonstrate that the proposed method performs favorably against current state-of-the-art methods, especially for deliberately hidden items. Our benchmark and codes are available at https://github.com/lutao2021/PIDray .},
  archive      = {J_IJCV},
  author       = {Zhang, Libo and Jiang, Lutao and Ji, Ruyi and Fan, Heng},
  doi          = {10.1007/s11263-023-01855-1},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3170-3192},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PIDray: A large-scale X-ray benchmark for real-world prohibited item detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end alternating optimization for real-world blind
super resolution. <em>IJCV</em>, <em>131</em>(12), 3152–3169. (<a
href="https://doi.org/10.1007/s11263-023-01833-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind super-resolution (SR) usually involves two sub-problems: (1) estimating the degradation of the given low-resolution (LR) image; (2) super-resolving the LR image to its high-resolution (HR) counterpart. Both problems are ill-posed due to the information loss in the degrading process. Most previous methods try to solve the two problems independently, but often fall into a dilemma: a good super-resolved HR result requires an accurate degradation estimation, which however, is difficult to be obtained without the help of original HR information. To address this issue, instead of considering these two problems independently, we adopt an alternating optimization algorithm, which can estimate the degradation and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely Restorer and Estimator. Restorer restores the SR image based on the estimated degradation, and Estimator estimates the degradation with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, both Restorer and Estimator could get benefited from the intermediate results of each other, and make each sub-problem easier. Moreover, Restorer and Estimator are optimized in an end-to-end manner, thus they could get more tolerant of the estimation deviations of each other and cooperate better to achieve more robust and accurate final results. Extensive experiments on both synthetic datasets and real-world images show that the proposed method can largely outperform state-of-the-art methods and produce more visually favorable results.},
  archive      = {J_IJCV},
  author       = {Luo, Zhengxiong and Huang, Yan and Li, Shang and Wang, Liang and Tan, Tieniu},
  doi          = {10.1007/s11263-023-01833-7},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3152-3169},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {End-to-end alternating optimization for real-world blind super resolution},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Super vision transformer. <em>IJCV</em>, <em>131</em>(12),
3136–3151. (<a
href="https://doi.org/10.1007/s11263-023-01861-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We attempt to reduce the computational costs in vision transformers (ViTs), which increase quadratically in the token number. We present a novel training paradigm that trains only one ViT model at a time, but is capable of providing improved image recognition performance with various computational costs. Here, the trained ViT model, termed super vision transformer (SuperViT), is empowered with the versatile ability to solve incoming patches of multiple sizes as well as preserve informative tokens with multiple keeping rates (the ratio of keeping tokens) to achieve good hardware efficiency for inference, given that the available hardware resources often change from time to time. Experimental results on ImageNet demonstrate that our SuperViT can considerably reduce the computational costs of ViT models with even performance increase. For example, we reduce 2 $$\times $$ FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2\% and 0.7\% for 1.5 $$\times $$ reduction. Also, our SuperViT significantly outperforms existing studies on efficient vision transformers. For example, when consuming the same amount of FLOPs, our SuperViT surpasses the recent state-of-the-art EViT by 1.1\% when using DeiT-S as their backbones. The project of this work is made publicly available at https://github.com/lmbxmu/SuperViT .},
  archive      = {J_IJCV},
  author       = {Lin, Mingbao and Chen, Mengzhao and Zhang, Yuxin and Shen, Chunhua and Ji, Rongrong and Cao, Liujuan},
  doi          = {10.1007/s11263-023-01861-3},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3136-3151},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Super vision transformer},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized gradient flow based saliency for pruning deep
convolutional neural networks. <em>IJCV</em>, <em>131</em>(12),
3121–3135. (<a
href="https://doi.org/10.1007/s11263-023-01854-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model filter pruning has shown efficiency in compressing deep convolutional neural networks by removing unimportant filters without sacrificing the performance. However, most existing criteria are empirical, and overlook the relationship between channel saliencies and the non-linear activation functions within the networks. To address these problems, we propose a novel channel pruning method coined gradient flow based saliency (GFBS). Instead of relying on the magnitudes of the entire feature maps, GFBS evaluates the channel saliencies from the gradient flow perspective and only requires the information in normalization and activation layers. Concretely, we first integrate the effects of normalization and ReLU activation layers into convolutional layers based on Taylor expansion. Then, through backpropagation, the derived channel saliency of each layer is indicated by of the first-order Taylor polynomial of the scaling parameter and the signed shifting parameter in the normalization layers. To validate the efficiency and generalization ability of GFBS, we conduct extensive experiments on various tasks, including image classification (CIFAR, ImageNet), image denoising, object detection, and 3D object classification. GFBS could feasibly cooperate with the baseline networks and compress them with only negligible performance drop. Moreover, we extended our method to pruning scratch networks and GFBS is capable to identify subnetworks with comparable performance with the baseline model at an early training stage. Our code has been released at https://github.com/CUHK-AIM-Group/GFBS .},
  archive      = {J_IJCV},
  author       = {Liu, Xinyu and Li, Baopu and Chen, Zhen and Yuan, Yixuan},
  doi          = {10.1007/s11263-023-01854-2},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3121-3135},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized gradient flow based saliency for pruning deep convolutional neural networks},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Learning enriched hop-aware correlation for
robust 3D human pose estimation. <em>IJCV</em>, <em>131</em>(11), 3119.
(<a href="https://doi.org/10.1007/s11263-023-01786-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhang, Shengping and Wang, Chenyang and Nie, Liqiang and Yao, Hongxun and Huang, Qingming and Tian, Qi},
  doi          = {10.1007/s11263-023-01786-x},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3119},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: Learning enriched hop-aware correlation for robust 3D human pose estimation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction: Towards automated ethogramming:
Cognitively-inspired event segmentation for streaming wildlife video
monitoring. <em>IJCV</em>, <em>131</em>(11), 3118. (<a
href="https://doi.org/10.1007/s11263-023-01847-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Mounir, Ramy and Shahabaz, Ahmed and Gula, Roman and Theuerkauf, Jörn and Sarkar, Sudeep},
  doi          = {10.1007/s11263-023-01847-1},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3118},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: towards automated ethogramming: cognitively-inspired event segmentation for streaming wildlife video monitoring},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performing melanoma diagnosis by an effective multi-view
convolutional network architecture. <em>IJCV</em>, <em>131</em>(11),
3094–3117. (<a
href="https://doi.org/10.1007/s11263-023-01848-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the proved effectiveness of deep learning models in solving complex problems, the melanoma diagnosis remains as a challenging task mainly due to the high levels of inter and intra-class variability present in images of moles. Aimed at filling this gap, in this work we propose a novel architecture for melanoma diagnosis which is inspired on multi-view learning and data augmentation. In order to make the model more transformation-invariant, the proposed architecture creates different independent and specific views of an image. The transformations applied on the original images are learned by genetic algorithms which find the best set of transformations. Also, the final predictions are yielded by aggregating information that comes from independent views. To evaluate the suitability of the proposal, an extensive experimental study on fifteen public melanoma-image datasets was conducted. The impact of the parameters of the genetic algorithms on the proposed architecture were analyzed, and it was also demonstrated that the proposed architecture attained better results when using transformations derived from the genetic algorithm than using random transformations. Finally, the results showed the suitability of the proposed model, where four state-of-the-art data augmentation techniques were significantly outperformed.},
  archive      = {J_IJCV},
  author       = {Pérez, Eduardo and Reyes, Óscar},
  doi          = {10.1007/s11263-023-01848-0},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3094-3117},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Performing melanoma diagnosis by an effective multi-view convolutional network architecture},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical curriculum learning for no-reference image
quality assessment. <em>IJCV</em>, <em>131</em>(11), 3074–3093. (<a
href="https://doi.org/10.1007/s11263-023-01851-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite remarkable success has been achieved by convolutional neural networks (CNNs) in no-reference image quality assessment (NR-IQA), there still exist many challenges in improving the performance of IQA for authentically distorted images. An important factor is that the insufficient annotated data limits the training of high-capacity CNNs to accommodate diverse distortions, complicated semantic structures and high-variance quality scores of these images. To address this problem, this paper proposes a hierarchical curriculum learning (HCL) framework for NR-IQA. The main idea of the proposed framework is to leverage the external data to learn the prior knowledge about IQA widely and progressively. Specifically, as a closely-related task with NR-IQA, image restoration is used as the first curriculum to learn the image quality related knowledge (i.e., semantic and distortion information) on massive distorted-reference image pairs. Then multiple lightweight subnetworks are designed to learn human scoring rules on multiple available synthetic IQA datasets independently, and a cross-dataset quality assessment correlation (CQAC) module is proposed to fully explore the similarities and diversities of different scoring rules. Finally, the whole model is fine-tuned on the target authentic IQA dataset to fuse the learned knowledge and adapt to the target data distribution. Experimental results show that our model achieves state-of-the-art performance on multiple standard authentic IQA datasets. Moreover, the generalization of our model is fully validated by the cross-dataset evaluation and the gMAD competition. In addition, extensive analyses prove that the proposed HCL framework is effective in improving the performance of our model.},
  archive      = {J_IJCV},
  author       = {Wang, Juan and Chen, Zewen and Yuan, Chunfeng and Li, Bing and Ma, Wentao and Hu, Weiming},
  doi          = {10.1007/s11263-023-01851-5},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3074-3093},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical curriculum learning for no-reference image quality assessment},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universal prototype transport for zero-shot action
recognition and localization. <em>IJCV</em>, <em>131</em>(11),
3060–3073. (<a
href="https://doi.org/10.1007/s11263-023-01846-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of recognizing action categories in videos when no training examples are available. The current state-of-the-art enables such a zero-shot recognition by learning universal mappings from videos to a semantic space, either trained on large-scale seen actions or on objects. While effective, we find that universal action and object mappings are biased to specific regions in the semantic space. These biases lead to a fundamental problem: many unseen action categories are simply never inferred during testing. For example on UCF-101, a quarter of the unseen actions are out of reach with a state-of-the-art universal action model. To that end, this paper introduces universal prototype transport for zero-shot action recognition. The main idea is to re-position the semantic prototypes of unseen actions by matching them to the distribution of all test videos. For universal action models, we propose to match distributions through a hyperspherical optimal transport from unseen action prototypes to the set of all projected test videos. The resulting transport couplings in turn determine the target prototype for each unseen action. Rather than directly using the target prototype as final result, we re-position unseen action prototypes along the geodesic spanned by the original and target prototypes as a form of semantic regularization. For universal object models, we outline a variant that defines target prototypes based on an optimal transport between unseen action prototypes and object prototypes. Empirically, we show that universal prototype transport diminishes the biased selection of unseen action prototypes and boosts both universal action and object models for zero-shot classification and spatio-temporal localization.},
  archive      = {J_IJCV},
  author       = {Mettes, Pascal},
  doi          = {10.1007/s11263-023-01846-2},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3060-3073},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Universal prototype transport for zero-shot action recognition and localization},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GenKL: An iterative framework for resolving label ambiguity
and label non-conformity in web images via a new generalized KL
divergence. <em>IJCV</em>, <em>131</em>(11), 3035–3059. (<a
href="https://doi.org/10.1007/s11263-023-01815-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web image datasets curated online inherently contain ambiguous in-distribution instances and out-of-distribution instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For “entropy” to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are “almost” uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $$(\alpha , \beta )$$ -generalized KL divergence, $${\mathcal {D}}_{\text {KL}}^{\alpha , \beta }(p\Vert q)$$ , which can be used to identify significantly more NC instances. Theoretical properties of $${\mathcal {D}}_{\text {KL}}^{\alpha , \beta }(p\Vert q)$$ are proven, and we also show empirically that a simple use of $${\mathcal {D}}_{\text {KL}}^{\alpha , \beta }(p\Vert q)$$ outperforms all baselines on the NC instance identification task. Building upon $$(\alpha ,\beta )$$ -generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances. When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $$81.34\%$$ , $$85.73\%$$ and $$78.99\%$$ / $$92.54\%$$ (top-1/top-5), respectively.},
  archive      = {J_IJCV},
  author       = {Huang, Xia and Chong, Kai Fong Ernest},
  doi          = {10.1007/s11263-023-01815-9},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3035-3059},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {GenKL: An iterative framework for resolving label ambiguity and label non-conformity in web images via a new generalized KL divergence},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Camouflaged object segmentation with omni perception.
<em>IJCV</em>, <em>131</em>(11), 3019–3034. (<a
href="https://doi.org/10.1007/s11263-023-01838-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object segmentation (COS) is a very challenging task due to the deceitful appearances of the candidate objects to the noisy backgrounds. Most existing state-of-the-art methods mimic the first-positioning-then-focus mechanism of predators, but still fail in positioning camouflaged objects in cluttered scenes or delineating their boundaries. The key reason is that their methods do not have a comprehensive understanding of the scene when they spot and focus on the objects, so that they are easily attracted by local surroundings. An ideal COS model should be able to process local and global information at the same time, i.e., to have omni perception of the scene through the whole process of camouflaged object segmentation. To this end, we propose to learn the omni perception for the first-positioning-then-focus COS scheme. Specifically, we propose an omni perception network (OPNet) with two novel modules, i.e., the pyramid positioning module (PPM) and dual focus module (DFM). They are proposed to integrate local features and global representations for accurate positioning of the camouflaged objects and focus on their boundaries, respectively. Extensive experiments demonstrate that our method, which runs at 54 fps, significantly outperforms 15 cutting-edge models on 4 challenging datasets under 4 standard metrics. The code will be made publicly available.},
  archive      = {J_IJCV},
  author       = {Mei, Haiyang and Xu, Ke and Zhou, Yunduo and Wang, Yang and Piao, Haiyin and Wei, Xiaopeng and Yang, Xin},
  doi          = {10.1007/s11263-023-01838-2},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3019-3034},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Camouflaged object segmentation with omni perception},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HiEve: A large-scale benchmark for human-centric video
analysis in complex events. <em>IJCV</em>, <em>131</em>(11), 2994–3018.
(<a href="https://doi.org/10.1007/s11263-023-01842-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the development of modern smart cities, human-centric video analysis has been encountering the challenge of analyzing diverse and complex events in real scenes. A complex event relates to dense crowds, anomalous individuals, or collective behaviors. However, limited by the scale and coverage of existing video datasets, few human analysis approaches have reported their performances on such complex events. To this end, we present a new large-scale dataset with comprehensive annotations, named human-in-events or human-centric video analysis in complex events (HiEve), for the understanding of human motions, poses, and actions in a variety of realistic events, especially in crowd and complex events. It contains a record number of poses (&gt;  1 M), the largest number of action instances (&gt; 56k) under complex events, as well as one of the largest numbers of trajectories lasting for longer time (with an average trajectory length of &gt; 480 frames). Based on its diverse annotation, we present two simple baselines for action recognition and pose estimation, respectively. They leverage cross-label information during training to enhance the feature learning in corresponding visual tasks. Experiments show that they could boost the performance of existing action recognition and pose estimation pipelines. More importantly, they prove the widely ranged annotations in HiEve can improve various video tasks. Furthermore, we conduct extensive experiments to benchmark recent video analysis approaches together with our baseline methods, demonstrating HiEve is a challenging dataset for human-centric video analysis. We expect that the dataset will advance the development of cutting-edge techniques in human-centric analysis and the understanding of complex events. The dataset is available at http://humaninevents.org .},
  archive      = {J_IJCV},
  author       = {Lin, Weiyao and Liu, Huabin and Liu, Shizhan and Li, Yuxi and Xiong, Hongkai and Qi, Guojun and Sebe, Nicu},
  doi          = {10.1007/s11263-023-01842-6},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2994-3018},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HiEve: A large-scale benchmark for human-centric video analysis in complex events},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic feature interaction framework for multi-task
visual perception. <em>IJCV</em>, <em>131</em>(11), 2977–2993. (<a
href="https://doi.org/10.1007/s11263-023-01835-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task visual perception has a wide range of applications in scene understanding such as autonomous driving. In this work, we devise an efficient unified framework to solve multiple common perception tasks, including instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation. Simply sharing the same visual feature representations for these tasks impairs the performance of tasks, while independent task-specific feature extractors lead to parameter redundancy and latency. Thus, we design two feature-merge branches to learn feature basis, which can be useful to, and thus shared by, multiple perception tasks. Then, each task takes the corresponding feature basis as the input of the prediction task head to fulfill a specific task. In particular, one feature merge branch is designed for instance-level recognition the other for dense predictions. To enhance inter-branch communication, the instance branch passes pixel-wise spatial information of each instance to the dense branch using efficient dynamic convolution weighting. Moreover, a simple but effective dynamic routing mechanism is proposed to isolate task-specific features and leverage common properties among tasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to parameter-efficient predictions for multi-task perception. In addition, as tasks benefit from co-training with each other, our solution achieves on par results on partially labeled settings on nuScenes and outperforms previous works for 3D detection and depth estimation on the Cityscapes dataset with full supervision.},
  archive      = {J_IJCV},
  author       = {Xi, Yuling and Chen, Hao and Wang, Ning and Wang, Peng and Zhang, Yanning and Shen, Chunhua and Liu, Yifan},
  doi          = {10.1007/s11263-023-01835-5},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2977-2993},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A dynamic feature interaction framework for multi-task visual perception},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-image person re-identification via
modal-consistent metric learning. <em>IJCV</em>, <em>131</em>(11),
2959–2976. (<a
href="https://doi.org/10.1007/s11263-023-01841-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-image person re-identification (AIPR) is a cross-modal retrieval task that searches person images who meet a list of attributes. Due to large modal gaps between attributes and images, current AIPR methods generally depend on cross-modal feature alignment, but they do not pay enough attention to similarity metric jitters among varying modal configurations (i.e., attribute probe vs. image gallery, image probe vs. attribute gallery, image probe vs. image gallery, and attribute probe vs. attribute gallery). In this paper, we propose a modal-consistent metric learning (MCML) method that stably measures comprehensive similarities between attributes and images. Our MCML is with favorable properties that differ in two significant ways from previous methods. First, MCML provides a complete multi-modal triplet (CMMT) loss function that pulls the distance between the farthest positive pair as close as possible while pushing the distance between the nearest negative pair as far as possible, independent of their modalities. Second, MCML develops a modal-consistent matching regularization (MCMR) to reduce the diversity of matching matrices and guide consistent matching behaviors on varying modal configurations. Therefore, our MCML integrates the CMMT loss function and MCMR, requiring no complex cross-modal feature alignments. Theoretically, we offer the generalization bound to establish the stability of our MCML model by applying on-average stability. Experimentally, extensive results on PETA and Market-1501 datasets show that the proposed MCML is superior to the state-of-the-art approaches.},
  archive      = {J_IJCV},
  author       = {Zhu, Jianqing and Liu, Liu and Zhan, Yibing and Zhu, Xiaobin and Zeng, Huanqiang and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01841-7},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2959-2976},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Attribute-image person re-identification via modal-consistent metric learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep unfolding for snapshot compressive imaging.
<em>IJCV</em>, <em>131</em>(11), 2933–2958. (<a
href="https://doi.org/10.1007/s11263-023-01844-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) systems aim to capture high-dimensional ( $$\ge 3$$ D) images in a single shot using 2D detectors. SCI devices consist of two main parts: a hardware encoder and a software decoder. The hardware encoder typically consists of an (optical) imaging system designed to capture compressed measurements. The software decoder, on the other hand, refers to a reconstruction algorithm that retrieves the desired high-dimensional signal from those measurements. In this paper, leveraging the idea of deep unrolling, we propose an SCI recovery algorithm, namely GAP-net, which unfolds the generalized alternating projection (GAP) algorithm. At each stage, GAP-net passes its current estimate of the desired signal through a trained convolutional neural network (CNN). The CNN operates as a denoiser projecting the estimate back to the desired signal space. For the GAP-net that employs trained auto-encoder-based denoisers, we prove a probabilistic global convergence result. Finally, we investigate the performance of GAP-net in solving video SCI and spectral SCI problems. In both cases, GAP-net demonstrates competitive performance on both synthetic and real data. In addition to its high accuracy and speed, we show that GAP-net is flexible with respect to signal modulation implying that a trained GAP-net decoder can be applied in different systems. Our code is available at https://github.com/mengziyi64/GAP-net .},
  archive      = {J_IJCV},
  author       = {Meng, Ziyi and Yuan, Xin and Jalali, Shirin},
  doi          = {10.1007/s11263-023-01844-4},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2933-2958},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep unfolding for snapshot compressive imaging},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep corner. <em>IJCV</em>, <em>131</em>(11), 2908–2932. (<a
href="https://doi.org/10.1007/s11263-023-01837-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown promising results on joint learning of local feature detectors and descriptors. To address the lack of ground-truth keypoint supervision, previous methods mainly inject appropriate knowledge about keypoint attributes into the network to facilitate model learning. In this paper, inspired by traditional corner detectors, we develop an end-to-end deep network, named Deep Corner, which adds a local similarity-based keypoint measure into a plain convolutional network. Deep Corner enables finding reliable keypoints and thus benefits the learning of the distinctive descriptors. Moreover, to improve keypoint localization, we first study previous multi-level keypoint detection strategies and then develop a multi-level U-Net architecture, where the similarity of features at multiple levels can be exploited effectively. Finally, to improve the invariance of descriptors, we propose a feature self-transformation operation, which transforms the learned features adaptively according to the specific local information. The experimental results on several tasks and comprehensive ablation studies demonstrate the effectiveness of our method and the involved components.},
  archive      = {J_IJCV},
  author       = {Zhao, Shanshan and Gong, Mingming and Zhao, Haimei and Zhang, Jing and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01837-3},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2908-2932},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep corner},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving domain adaptation through class aware frequency
transformation. <em>IJCV</em>, <em>131</em>(11), 2888–2907. (<a
href="https://doi.org/10.1007/s11263-023-01810-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore the usage of the frequency transformation for reducing the domain shift between the source and target domain (e.g., synthetic image and real image respectively) towards solving the domain adaptation task. Most of the unsupervised domain adaptation (UDA) algorithms focus on reducing the global domain shift between labelled source and unlabelled target domains by matching the marginal distributions under a small domain gap assumption. UDA performance degrades for the cases where the domain gap between source and target distribution is large. In order to bring the source and the target domains closer, we propose a novel approach based on traditional image processing technique Class Aware Frequency Transformation (CAFT) that utilizes pseudo label based class consistent low-frequency swapping for improving the overall performance of the existing UDA algorithms. The proposed approach, when compared with the state-of-the-art deep learning based methods, is computationally more efficient and can easily be plugged into any existing UDA algorithm to improve its performance. Additionally, we introduce a novel approach based on absolute difference of top-2 class prediction probabilities for filtering target pseudo labels into clean and noisy sets. Samples with clean pseudo labels can be used to improve the performance of unsupervised learning algorithms. We name the overall framework as CAFT++. We evaluate the same on the top of different UDA algorithms across many public domain adaptation datasets. Our extensive experiments indicate that CAFT++ is able to achieve significant performance gains across all the popular benchmarks.},
  archive      = {J_IJCV},
  author       = {Kumar, Vikash and Patil, Himanshu and Lal, Rohit and Chakraborty, Anirban},
  doi          = {10.1007/s11263-023-01810-0},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2888-2907},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Improving domain adaptation through class aware frequency transformation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep physics-guided unrolling generalization for compressed
sensing. <em>IJCV</em>, <em>131</em>(11), 2864–2887. (<a
href="https://doi.org/10.1007/s11263-023-01814-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By absorbing the merits of both the model- and data-driven methods, deep physics-engaged learning scheme achieves high-accuracy and interpretable image reconstruction. It has attracted growing attention and become the mainstream for inverse imaging tasks. Focusing on the image compressed sensing (CS) problem, we find the intrinsic defect of this emerging paradigm, widely implemented by deep algorithm-unrolled networks, in which more plain iterations involving real physics will bring enormous computation cost and long inference time, hindering their practical application. A novel deep Physics-guided unRolled recovery Learning (RL) framework is proposed by generalizing the traditional iterative recovery model from image domain (ID) to the high-dimensional feature domain (FD). A compact multiscale unrolling architecture is then developed to enhance the network capacity and keep real-time inference speeds. Taking two different perspectives of optimization and range-nullspace decomposition, instead of building an algorithm-specific unrolled network, we provide two implementations: PRL-PGD and PRL-RND. Experiments exhibit the significant performance and efficiency leading of PRL networks over other state-of-the-art methods with a large potential for further improvement and real application to other inverse imaging problems or optimization models.},
  archive      = {J_IJCV},
  author       = {Chen, Bin and Song, Jiechong and Xie, Jingfen and Zhang, Jian},
  doi          = {10.1007/s11263-023-01814-w},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2864-2887},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep physics-guided unrolling generalization for compressed sensing},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal transport view of class-imbalanced visual
recognition. <em>IJCV</em>, <em>131</em>(11), 2845–2863. (<a
href="https://doi.org/10.1007/s11263-023-01831-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models have achieved impressive success in class-imbalanced visual recognition. In the view of optimal transport, the current evaluation protocol for class-imbalanced visual recognition can be interpreted as follows: during training, the neural network learns an optimal transport mapping with an uneven source label distribution, and during evaluation, this mapping is needed to transfer the instances to the uniform target label distribution. The label distribution’s inconsistency leads to poor cross-entropy loss performance. In this paper, we first prove that the cross-entropy loss in the classification network is a smooth approximation to the optimal transport, enhancing the interpretability of the classifier. Motivated by this conclusion, we introduce a simple and effective method named Post Optimal Transport (Post OT). Post OT can match the arbitrary target label distribution, which may also be class-imbalanced by post-processing the predictions of a model. In addition, we propose Adaptive Bias Loss (ABL) based on optimal transport theory to shift the label distribution in the class-imbalanced training, which does not depend on the category frequencies in the training set and also can avoid the overfitting of tail classes. Extensive experiments verify that our methods achieve state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, ImageNet-LT, Places365-LT, and iNaturalist 2018.},
  archive      = {J_IJCV},
  author       = {Jin, Lianbao and Lang, Dayu and Lei, Na},
  doi          = {10.1007/s11263-023-01831-9},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2845-2863},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An optimal transport view of class-imbalanced visual recognition},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic generation of 3D scene animation based on dynamic
knowledge graphs and contextual encoding. <em>IJCV</em>,
<em>131</em>(11), 2816–2844. (<a
href="https://doi.org/10.1007/s11263-023-01839-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although novel 3D animation techniques could be boosted by a large variety of deep learning methods, flexible automatic 3D applications (involving animated figures such as humans and low-life animals) are still rarely studied in 3D computer vision. This is due to lacking of arbitrary 3D data acquisition environment, especially those involving human populated scenes. Given a single image, the 3D animation aided by contextual inference is still plagued by limited reconstruction clues without prior knowledge pertinent to the identified figures/objects and/or their possible relationship w.r.t. the environment. To alleviate such difficulty in time-varying 3D animation, we devise a dynamic scene creation framework via a dynamic knowledge graph (DKG). The DKG encodes both temporal and spatial contextual clues to enable and facilitate human interactions with the affordance environment. Furthermore, we construct the DKG-driven variational auto-encoder (DVAE) upon animation kinematics knowledge conveyed by meta-motion sequences, which are disentangled from videos of prior scenes. It is then possible to utilize the DKG to induce the animations in certain scenes, thus, we could automatically and physically generate plausible 3D animations that afford vivid interactions among humans, low-and life animals in the environment. The extensive experimental results and comprehensive evaluations confirm our DKGs’ representation and modeling power towards new animation production in 3D graphics and vision applications.},
  archive      = {J_IJCV},
  author       = {Song, Wenfeng and Zhang, Xinyu and Guo, Yuting and Li, Shuai and Hao, Aimin and Qin, Hong},
  doi          = {10.1007/s11263-023-01839-1},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2816-2844},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Automatic generation of 3D scene animation based on dynamic knowledge graphs and contextual encoding},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCP–NAS: Discrepant child–parent neural architecture search
for 1-bit CNNs. <em>IJCV</em>, <em>131</em>(11), 2793–2815. (<a
href="https://doi.org/10.1007/s11263-023-01836-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) proves to be among the effective approaches for many tasks by generating an application-adaptive neural architecture, which is still challenged by high computational cost and memory consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binary weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to reduce the computation and memory cost of NAS by taking advantage of the strengths of each in a unified framework, while searching the 1-bit CNNs is more challenging due to the more complicated processes involved. In this paper, we introduce Discrepant Child–Parent Neural Architecture Search (DCP–NAS) to efficiently search 1-bit CNNs, based on a new framework of searching the 1-bit model (Child) under the supervision of a real-valued model (Parent). Particularly, we first utilize a Parent model to calculate a tangent direction, based on which the tangent propagation method is introduced to search the optimized 1-bit Child. We further observe a coupling relationship between the weights and architecture parameters existing in such differentiable frameworks. To address the issue, we propose a decoupled optimization method to search an optimized architecture. Extensive experiments demonstrate that our DCP–NAS achieves much better results than prior arts on both CIFAR-10 and ImageNet datasets. In particular, the backbones achieved by our DCP–NAS achieve strong generalization performance on person re-identification and object detection.},
  archive      = {J_IJCV},
  author       = {Li, Yanjing and Xu, Sheng and Cao, Xianbin and Zhuo, Li’an and Zhang, Baochang and Wang, Tian and Guo, Guodong},
  doi          = {10.1007/s11263-023-01836-4},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2793-2815},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DCP–NAS: Discrepant Child–Parent neural architecture search for 1-bit CNNs},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Poincaré kernels for hyperbolic representations.
<em>IJCV</em>, <em>131</em>(11), 2770–2792. (<a
href="https://doi.org/10.1007/s11263-023-01834-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding data in hyperbolic spaces has proven beneficial for many advanced machine learning applications. However, working in hyperbolic spaces is not without difficulties as a result of its curved geometry (e.g., computing the Fréchet mean of a set of points requires an iterative algorithm). In Euclidean spaces, one can resort to kernel machines that not only enjoy rich theoretical properties but that can also lead to superior representational power (e.g., infinite-width neural networks). In this paper, we introduce valid kernel functions for hyperbolic representations. This brings in two major advantages, 1. kernelization will pave the way to seamlessly benefit the representational power from kernel machines in conjunction with hyperbolic embeddings, and 2. the rich structure of the Hilbert spaces associated with kernel machines enables us to simplify various operations involving hyperbolic data. That said, identifying valid kernel functions on curved spaces is not straightforward and is indeed considered an open problem in the learning community. Our work addresses this gap and develops several positive definite kernels in hyperbolic spaces (modeled by a Poincaré ball), the proposed kernels include the rich universal ones (e.g., Poincaré RBF kernel), or realize the multiple kernel learning scheme (e.g., Poincaré radial kernel). We comprehensively study the proposed kernels on a variety of challenging tasks including few-shot learning, zero-shot learning, person re-identification, deep metric learning, knowledge distillation and self-supervised learning. The consistent performance gain over different tasks shows the benefits of the kernelization for hyperbolic representations.},
  archive      = {J_IJCV},
  author       = {Fang, Pengfei and Harandi, Mehrtash and Lan, Zhenzhong and Petersson, Lars},
  doi          = {10.1007/s11263-023-01834-6},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2770-2792},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Poincaré kernels for hyperbolic representations},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAFT-MSF: Self-supervised monocular scene flow using
recurrent optimizer. <em>IJCV</em>, <em>131</em>(11), 2757–2769. (<a
href="https://doi.org/10.1007/s11263-023-01828-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular approach to estimate scene flow is to utilize point cloud data from various Lidar scans. However, there is little attention to learning 3D motion from camera images. Learning scene flow from a monocular camera remains a challenging task due to its ill-posedness as well as lack of annotated data. Self-supervised methods demonstrate learning scene flow estimation from unlabeled data, yet their accuracy lags behind (semi-)supervised methods. In this paper, we introduce a self-supervised monocular scene flow method that substantially improves the accuracy over the previous approaches. Based on RAFT, a state-of-the-art optical flow model, we design a new decoder to iteratively update 3D motion fields and disparity maps simultaneously. Furthermore, we propose an enhanced upsampling layer and a disparity initialization technique, which overall further improves accuracy up to 7.2\%. Our method achieves state-of-the-art accuracy among all self-supervised monocular scene flow methods, improving accuracy by 34.2\%. Our fine-tuned model outperforms the best previous semi-supervised method with 228 times faster runtime. Code will be publicly available to ensure reproducibility.},
  archive      = {J_IJCV},
  author       = {Bayramli, Bayram and Hur, Junhwa and Lu, Hongtao},
  doi          = {10.1007/s11263-023-01828-4},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2757-2769},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RAFT-MSF: Self-supervised monocular scene flow using recurrent optimizer},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer-based context condensation for boosting feature
pyramids in object detection. <em>IJCV</em>, <em>131</em>(10),
2738–2756. (<a
href="https://doi.org/10.1007/s11263-023-01830-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current object detectors typically have a feature pyramid (FP) module for multi-level feature fusion (MFF) which aims to mitigate the gap between features from different levels and form a comprehensive object representation to achieve better detection performance. However, they usually require heavy cross-level connections or iterative refinement to obtain better MFF result, making them complicated in structure and inefficient in computation. To address these issues, we propose a novel and efficient context modeling mechanism that can help existing FPs deliver better MFF results while reducing the computational costs effectively. In particular, we introduce a novel insight that comprehensive contexts can be decomposed and condensed into two types of representations for higher efficiency. The two representations include a locally concentrated representation and a globally summarized representation, where the former focuses on extracting context cues from nearby areas while the latter extracts general contextual representations of the whole image scene as global context cues. By collecting the condensed contexts, we employ a Transformer decoder to investigate the relations between them and each local feature from the FP and then refine the MFF results accordingly. As a result, we obtain a simple and light-weight Transformer-based Context Condensation (TCC) module, which can boost various FPs and lower their computational costs simultaneously. Extensive experimental results on the challenging MS COCO dataset show that TCC is compatible to four representative FPs and consistently improves their detection accuracy by up to 7.8\% in terms of average precision and reduce their complexities by up to around 20\% in terms of GFLOPs, helping them achieve state-of-the-art performance more efficiently. Code will be released at https://github.com/zhechen/TCC .},
  archive      = {J_IJCV},
  author       = {Chen, Zhe and Zhang, Jing and Xu, Yufei and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01830-w},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2738-2756},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Transformer-based context condensation for boosting feature pyramids in object detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visually-guided audio spatialization in video with
geometry-aware multi-task learning. <em>IJCV</em>, <em>131</em>(10),
2723–2737. (<a
href="https://doi.org/10.1007/s11263-023-01816-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binaural audio provides human listeners with an immersive spatial sound experience, but most existing videos lack binaural audio recordings. We propose an audio spatialization method that draws on visual information in videos to convert their monaural (single-channel) audio to binaural audio. Whereas existing approaches leverage visual features extracted directly from video frames, our approach explicitly disentangles the geometric cues present in the visual stream to guide the learning process. In particular, we develop a multi-task framework that learns geometry-aware features for binaural audio generation by accounting for the underlying room impulse response, the visual stream’s coherence with the sound source(s) positions, and the consistency in geometry of the sounding objects over time. Furthermore, we introduce two new large video datasets: one with realistic binaural audio simulated for real-world scanned environments, and the other with pseudo-binaural audio obtained from ambisonic sounds in YouTube $$360^{\circ }$$ videos. On three datasets, we demonstrate the efficacy of our method, which achieves state-of-the-art results.},
  archive      = {J_IJCV},
  author       = {Garg, Rishabh and Gao, Ruohan and Grauman, Kristen},
  doi          = {10.1007/s11263-023-01816-8},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2723-2737},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visually-guided audio spatialization in video with geometry-aware multi-task learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional temporal variational AutoEncoder for action
video prediction. <em>IJCV</em>, <em>131</em>(10), 2699–2722. (<a
href="https://doi.org/10.1007/s11263-023-01832-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To synthesize a realistic action sequence based on a single human image, it is crucial to model both motion patterns and diversity in the action video. This paper proposes an Action Conditional Temporal Variational AutoEncoder (ACT-VAE) to improve motion prediction accuracy and capture movement diversity. ACT-VAE predicts pose sequences for an action clip from a single input image. It is implemented as a deep generative model that maintains temporal coherence according to the action category with a novel temporal modeling on latent space. Further, ACT-VAE is a general action sequence prediction framework. When connected with a plug-and-play Pose-to-Image network, ACT-VAE can synthesize image sequences. Extensive experiments bear out our approach can predict accurate pose and synthesize realistic image sequences, surpassing state-of-the-art approaches. Compared to existing methods, ACT-VAE improves model accuracy and preserves diversity.},
  archive      = {J_IJCV},
  author       = {Xu, Xiaogang and Wang, Yi and Wang, Liwei and Yu, Bei and Jia, Jiaya},
  doi          = {10.1007/s11263-023-01832-8},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2699-2722},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Conditional temporal variational AutoEncoder for action video prediction},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anti-bandit for neural architecture search. <em>IJCV</em>,
<em>131</em>(10), 2682–2698. (<a
href="https://doi.org/10.1007/s11263-023-01826-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) is a highly challenging task that requires consideration of search space, search efficiency, and adversarial robustness of the network. In this paper, to accelerate the training speed, we reformulate NAS as a multi-armed bandit problem and present Anti-Bandit NAS (ABanditNAS) method, which exploits Upper Confidence Bounds (UCB) to abandon arms for search efficiency and Lower Confidence Bounds (LCB) for fair competition between arms. Based on the presented ABanditNAS, the adversarially robust optimization and architecture search can be solved in a unified framework. Specifically, our proposed framework defends against adversarial attacks based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters, and convolutions. The theoretical analysis on the rationality of the two confidence bounds in ABanditNAS are provided and extensive experiments on three benchmarks are conducted. The results demonstrate that the presented ABanditNAS achieves competitive accuracy at a reduced search cost compared to prior methods.},
  archive      = {J_IJCV},
  author       = {Wang, Runqi and Yang, Linlin and Chen, Hanlin and Wang, Wei and Doermann, David and Zhang, Baochang},
  doi          = {10.1007/s11263-023-01826-6},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2682-2698},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Anti-bandit for neural architecture search},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). InstaBoost++: Visual coherence principles for unified 2D/3D
instance level data augmentation. <em>IJCV</em>, <em>131</em>(10),
2665–2681. (<a
href="https://doi.org/10.1007/s11263-023-01807-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-level perception tasks like object detection, instance segmentation, and 3D detection require many training samples to achieve satisfactory performance. The meticulous labels for these tasks are usually expensive to obtain and data augmentation is a natural choice to tackle such a problem. However, instance-level augmentation is less studied in previous research. In this paper, we present an effective, efficient and unified crop-paste mechanism to augment the training set utilizing existing instance-level annotations. Our design is derived from visual coherence and mines three inherent principles that widely exist in real-world data: (i) background coherence in local neighbor area, (ii) appearance coherence for instance placement, and (iii) instance coherence within the same category. Such methodologies are unified for various tasks including object detection, instance segmentation, and 3D detection. Extensive experiments demonstrate that our proposed approaches can successfully boost the performance of diverse frameworks on various datasets across multiple tasks, without modifying the network structure. Remarkable improvements are obtained: 5.1 mAP for object detection and 3.2 mAP for instance segmentation on COCO dataset, and 6.9 mAP for 3D detection on ScanNetV2 dataset. Our method can be easily integrated into different frameworks without affecting the training and inference efficiency.},
  archive      = {J_IJCV},
  author       = {Sun, Jianhua and Fang, Hao-Shu and Li, Yuxuan and Wang, Runzhong and Gou, Minghao and Lu, Cewu},
  doi          = {10.1007/s11263-023-01807-9},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2665-2681},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {InstaBoost++: Visual coherence principles for unified 2D/3D instance level data augmentation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pyramid NeRF: Frequency guided fast radiance field
optimization. <em>IJCV</em>, <em>131</em>(10), 2649–2664. (<a
href="https://doi.org/10.1007/s11263-023-01829-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis using implicit neural functions such as Neural Radiance Field (NeRF) has achieved significant progress recently. However, it is very computationally expensive to train a NeRF due to the disordered frequency optimization. In this paper, we propose the Pyramid NeRF, which guides the NeRF training in a ‘low-frequency first, high-frequency second’ style using the image pyramids and could improve the training and inference speed at $$15\times $$ and $$805\times $$ , respectively. The high training efficiency is guaranteed by (i) organized frequency-guided optimization could improve the convergency speed and efficiently reduce the training iterations and (ii) progressive subdivision, which replaces a single large multi-layer perceptron (MLP) with thousands of tiny MLPs, could significantly decrease the execution time of running MLPs. Experiments on various synthetic and real scenes verify the high efficiency of the Pyramid NeRF. Meanwhile, the structure and perceptual similarities could be better recovered.},
  archive      = {J_IJCV},
  author       = {Zhu, Junyu and Zhu, Hao and Zhang, Qi and Zhu, Fang and Ma, Zhan and Cao, Xun},
  doi          = {10.1007/s11263-023-01829-3},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2649-2664},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pyramid NeRF: Frequency guided fast radiance field optimization},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Don’t be so dense: Sparse-to-sparse GAN training without
sacrificing performance. <em>IJCV</em>, <em>131</em>(10), 2635–2648. (<a
href="https://doi.org/10.1007/s11263-023-01824-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper does not describe a novel method. Instead, it studies an incremental, yet must-know baseline given the recent progress in sparse neural network training and Generative Adversarial Networks (GANs). GANs have received an upsurging interest since being proposed due to the high quality of the generated data. While achieving increasingly impressive results, the resource demands associated with the large model size hinders the usage of GANs in resource-limited scenarios. For inference, the existing model compression techniques can reduce the model complexity with comparable performance. However, the training efficiency of GANs has less been explored due to the fragile training process of GANs. In this paper, we, for the first time, explore the possibility of directly training sparse GAN from scratch without involving any dense or pre-training steps. Even more unconventionally, our proposed method enables directly training sparse unbalanced GANs with an extremely sparse generator from scratch. Instead of training full GANs, we start with sparse GANs and dynamically explore the parameter space spanned over the generator throughout training. Such a sparse-to-sparse training procedure enhances the capacity of the highly sparse generator progressively while sticking to a fixed small parameter budget with appealing training and inference efficiency gains. Extensive experiments with modern GAN architectures validate the effectiveness of our method. Our sparsified GANs, trained from scratch in one single run, are able to outperform the ones learned by expensive iterative pruning and re-training. Perhaps most importantly, we find instead of inheriting parameters from expensive pre-trained GANs, directly training sparse GANs from scratch can be a much more efficient solution. For example, only training with a 80\% sparse generator and a 70\% sparse discriminator, our method can achieve even better performance than the dense BigGAN.},
  archive      = {J_IJCV},
  author       = {Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  doi          = {10.1007/s11263-023-01824-8},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2635-2648},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Don’t be so dense: Sparse-to-sparse GAN training without sacrificing performance},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DOVE: Learning deformable 3D objects by watching videos.
<em>IJCV</em>, <em>131</em>(10), 2623–2634. (<a
href="https://doi.org/10.1007/s11263-023-01819-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning deformable 3D objects from 2D images is often an ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects “in the wild”. A more natural way of establishing correspondences is by watching videos of objects moving around. In this paper, we present DOVE, a method that learns textured 3D models of deformable object categories from monocular videos available online, without keypoint, viewpoint or template shape supervision. By resolving symmetry-induced pose ambiguities and leveraging temporal correspondences in videos, the model automatically learns to factor out 3D shape, articulated pose and texture from each individual RGB frame, and is ready for single-image inference at test time. In the experiments, we show that existing methods fail to learn sensible 3D shapes without additional keypoint or template supervision, whereas our method produces temporally consistent 3D models, which can be animated and rendered from arbitrary viewpoints. Project page: https://dove3d.github.io/ .},
  archive      = {J_IJCV},
  author       = {Wu, Shangzhe and Jakab, Tomas and Rupprecht, Christian and Vedaldi, Andrea},
  doi          = {10.1007/s11263-023-01819-5},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2623-2634},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DOVE: Learning deformable 3D objects by watching videos},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-spectrum out-of-distribution detection. <em>IJCV</em>,
<em>131</em>(10), 2607–2622. (<a
href="https://doi.org/10.1007/s11263-023-01811-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing out-of-distribution (OOD) detection literature clearly defines semantic shift as a sign of OOD but does not have a consensus over covariate shift. Samples experiencing covariate shift but not semantic shift from the in-distribution (ID) are either excluded from the test set or treated as OOD, which contradicts the primary goal in machine learning—being able to generalize beyond the training distribution. In this paper, we take into account both shift types and introduce full-spectrum OOD (F-OOD) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and design three benchmarks. These new benchmarks have a more fine-grained categorization of distributions (i.elet@tokeneonedot, training ID, covariate-shifted ID, near-OOD, and far-OOD) for the purpose of more comprehensively evaluating the pros and cons of algorithms. To address the F-OOD detection problem, we propose SEM, a simple feature-based semantics score function. SEM is mainly composed of two probability measures: one is based on high-level features containing both semantic and non-semantic information, while the other is based on low-level feature statistics only capturing non-semantic image styles. With a simple combination, the non-semantic part is canceled out, which leaves only semantic information in SEM that can better handle F-OOD detection. Extensive experiments on the three new benchmarks show that SEM significantly outperforms current state-of-the-art methods. Our code and benchmarks are released in https://github.com/Jingkang50/OpenOOD .},
  archive      = {J_IJCV},
  author       = {Yang, Jingkang and Zhou, Kaiyang and Liu, Ziwei},
  doi          = {10.1007/s11263-023-01811-z},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2607-2622},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Full-spectrum out-of-distribution detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A2B: Anchor to barycentric coordinate for robust
correspondence. <em>IJCV</em>, <em>131</em>(10), 2582–2606. (<a
href="https://doi.org/10.1007/s11263-023-01827-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a long-standing problem of repeated patterns in correspondence problems, where mismatches frequently occur because of inherent ambiguity. The unique position information associated with repeated patterns makes coordinate representations a useful supplement to appearance representations for improving feature correspondences. However, the issue of appropriate coordinate representation has remained unresolved. In this study, we demonstrate that geometric-invariant coordinate representations, such as barycentric coordinates, can significantly reduce mismatches between features. The first step is to establish a theoretical foundation for geometrically invariant coordinates. We present a seed matching and filtering network (SMFNet) that combines feature matching and consistency filtering with a coarse-to-fine matching strategy in order to acquire reliable sparse correspondences. We then introduce Degree, a novel anchor-to-barycentric (A2B) coordinate encoding approach, which generates multiple affine-invariant correspondence coordinates from paired images. Degree can be used as a plug-in with standard descriptors, feature matchers, and consistency filters to improve the matching quality. Extensive experiments in synthesized indoor and outdoor datasets demonstrate that Degree alleviates the problem of repeated patterns and helps achieve state-of-the-art performance. Furthermore, Degree also reports competitive performance in the third Image Matching Challenge at CVPR 2021. This approach offers a new perspective to alleviate the problem of repeated patterns and emphasizes the importance of choosing coordinate representations for feature correspondences.},
  archive      = {J_IJCV},
  author       = {Zhao, Weiyue and Lu, Hao and Cao, Zhiguo and Li, Xin},
  doi          = {10.1007/s11263-023-01827-5},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2582-2606},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A2B: Anchor to barycentric coordinate for robust correspondence},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of methods for automated quality control based on
images. <em>IJCV</em>, <em>131</em>(10), 2553–2581. (<a
href="https://doi.org/10.1007/s11263-023-01822-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of quality control based on images is important in industrial production. Nevertheless, this problem has not been addressed in computer vision for a long time. In recent years, this has changed: driven by publicly available datasets, a variety of methods have been proposed for detecting anomalies and defects in workpieces. In this survey, we present more than 40 methods that promise the best results for this task. In a comprehensive benchmark, we show that more datasets and metrics are needed to move the field forward. Further, we highlight strengths and weaknesses, discuss research gaps and future research areas.},
  archive      = {J_IJCV},
  author       = {Diers, Jan and Pigorsch, Christian},
  doi          = {10.1007/s11263-023-01822-w},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2553-2581},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A survey of methods for automated quality control based on images},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When multi-focus image fusion networks meet traditional
edge-preservation technology. <em>IJCV</em>, <em>131</em>(10),
2529–2552. (<a
href="https://doi.org/10.1007/s11263-023-01806-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating the decision map with accurate boundaries is the key to fusing multi-focus images. In this paper, we introduce edge-preservation (EP) techniques into neural networks to improve the quality of decision maps, supported by an interesting phenomenon we found: the maps generated by traditional EP techniques are similar to the feature maps in the trained network with excellent performance. Based on the manifold theory in the field of edge-preservation, we propose a novel edge-aware layer derived from isometric domain transformation and a recursive filter, which effectively eliminates burrs and pseudo-edges in the decision map by highlighting the edge discrepancy between the focused and defocused regions. This edge-aware layer is incorporated to a Siamese-style encoder and a decoder to form a complete segmentation architecture, termed Y-Net, which can contrastively learn and capture the feature differences of the sourced images with a relatively small number of training data (i.e., 10,000 image pairs). In addition, a new strategy based on randomization is devised to generate masks and simulate multi-focus images with natural images, which alleviates the absence of ground-truth and the lack of training sets in multi-focus image fusion (MFIF) task. The experimental results on four publicly available datasets demonstrate that Y-Net with the edge-aware layers is superior to other state-of-the-art fusion networks in terms of qualitative and quantitative comparison.},
  archive      = {J_IJCV},
  author       = {Wang, Zeyu and Li, Xiongfei and Zhao, Libo and Duan, Haoran and Wang, Shidong and Liu, Hao and Zhang, Xiaoli},
  doi          = {10.1007/s11263-023-01806-w},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2529-2552},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {When multi-focus image fusion networks meet traditional edge-preservation technology},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What limits the performance of local self-attention?
<em>IJCV</em>, <em>131</em>(10), 2516–2528. (<a
href="https://doi.org/10.1007/s11263-023-01813-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although self-attention is powerful in modeling long-range dependencies, the performance of local self-attention (LSA) is just similar to depth-wise convolution, which puzzles researchers on whether to use LSA or its counterparts, which one is better, and what limits the performance of LSA. To clarify these, we comprehensively investigate LSA and its counterparts from channel setting and spatial processing. We find that the devil lies in attention generation and application, where relative position embedding and neighboring filter application are key factors. Based on these findings, we propose enhanced local self-attention (ELSA) with Hadamard attention and the ghost head. Hadamard attention introduces the Hadamard product to efficiently generate attention in the neighboring area, while maintaining the high-order mapping. The ghost head combines attention maps with static matrices to increase channel capacity. Experiments demonstrate the effectiveness of ELSA. Without architecture/hyperparameter modification, drop-in replacing LSA with ELSA boosts Swin Transformer by up to $$+$$ 1.4 on top-1 accuracy. ELSA also consistently benefits VOLO from D1 to D5, where ELSA-VOLO-D5 achieves 87.2 on the ImageNet-1K without extra training images. In addition, we evaluate ELSA in downstream tasks. ELSA significantly improves the baseline by up to $$+$$ 1.9 box Ap/ $$+$$ 1.3 mask Ap on the COCO, and by up to $$+$$ 1.9 mIoU on the ADE20K.},
  archive      = {J_IJCV},
  author       = {Zhou, Jingkai and Wang, Pichao and Tang, Jiasheng and Wang, Fan and Liu, Qiong and Li, Hao and Jin, Rong},
  doi          = {10.1007/s11263-023-01813-x},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2516-2528},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {What limits the performance of local self-attention?},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Importance first: Generating scene graph of human interest.
<em>IJCV</em>, <em>131</em>(10), 2489–2515. (<a
href="https://doi.org/10.1007/s11263-023-01817-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph aims to faithfully reveal humans’ perception of image content. When humans look at a scene, they usually focus on their interested parts in a special priority. This innate habit indicates a hierarchical preference about human perception. Therefore, we argue to generate the Scene Graph of Interest which should be hierarchically constructed, so that the important primary content is firstly presented while the secondary one is presented on demand. To achieve this goal, we propose the Tree–Guided Importance Ranking (TGIR) model. We represent the scene with a hierarchical structure by firstly detecting objects in the scene and organizing them into a Hierarchical Entity Tree (HET) according to their spatial scale, considering that larger objects are more likely to be noticed instantly. After that, the scene graph is generated guided by structural information of HET which is modeled by the elaborately designed Hierarchical Contextual Propagation (HCP) module. To further highlight the key relationship in the scene graph, all relationships are re-ranked through additionally estimating their importance by the Relationship Ranking Module (RRM). To train RRM, the most direct way is to collect the key relationship annotation, which is the so-called Direct Supervision scheme. As collecting annotation may be cumbersome, we further utilize two intuitive and effective cues, visual saliency and spatial scale, and treat them as Approximate Supervision, according to the findings that these cues are positively correlated with relationship importance. With these readily available cues, the RRM is still able to estimate the importance even without key relationship annotation. Experiments indicate that our method not only achieves state-of-the-art performances on scene graph generation, but also is expert in mining image-specific relationships which play a great role in serving subsequent tasks such as image captioning and cross-modal retrieval.},
  archive      = {J_IJCV},
  author       = {Wang, Wenbin and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  doi          = {10.1007/s11263-023-01817-7},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2489-2515},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Importance first: Generating scene graph of human interest},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to remove shadows from a single image.
<em>IJCV</em>, <em>131</em>(9), 2471–2488. (<a
href="https://doi.org/10.1007/s11263-023-01823-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent learning-based shadow removal methods have achieved remarkable performance. However, they basically require massive paired shadow and shadow-free images for model training, which limits their generalization capability since these data are often cumbersome to obtain and lack of diversity. To address the problem, we present Self-ShadowGAN, a novel adversarial framework that is able to learn to remove shadows in an image by training solely on the image itself, using the shadow mask as the only supervision. Our approach is built upon the concept of histogram matching, by constraining the deshadowed regions produced by a shadow relighting network share similar histograms to the original shadow-free regions via a histogram-based discriminator. In order to speed up the single image training, we define the shadow relighting network to be lightweight multi-layer perceptions (MLPs) that estimate spatially-varying shadow relighting coefficients, where the parameters of the MLPs are predicted from a low-resolution input by a fast convolutional network and then upsampled back to the original full-resolution. Experimental results show that our method performs favorably against the state-of-the-art shadow removal methods, and is effective to process previously challenging shadow images.},
  archive      = {J_IJCV},
  author       = {Jiang, Hao and Zhang, Qing and Nie, Yongwei and Zhu, Lei and Zheng, Wei-Shi},
  doi          = {10.1007/s11263-023-01823-9},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2471-2488},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to remove shadows from a single image},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perspective-1-ellipsoid: Formulation, analysis and solutions
of the camera pose estimation problem from one ellipse-ellipsoid
correspondence. <em>IJCV</em>, <em>131</em>(9), 2446–2470. (<a
href="https://doi.org/10.1007/s11263-023-01794-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, camera pose estimation from correspondences between 3D geometric entities and their projections into the image has been a widely investigated problem. Although most state-of-the-art methods exploit low-level primitives such as points or lines, the emergence of very effective CNN-based object detectors in the recent years has paved the way to the use of higher-level features carrying semantically meaningful information. Pioneering works in that direction have shown that modelling 3D objects by ellipsoids and 2D detections by ellipses offers a convenient manner to link 2D and 3D data. However, the mathematical formalism most often used in the related litterature does not enable to easily distinguish ellipsoids and ellipses from other quadrics and conics, leading to a loss of specificity potentially detrimental in some developments. Moreover, the linearization process of the projection equation creates an over-representation of the camera parameters, also possibly causing an efficiency loss. In this paper, we therefore introduce an ellipsoid-specific theoretical framework and demonstrate its beneficial properties in the context of pose estimation. More precisely, we first show that the proposed formalism enables to reduce the pose estimation problem to a position or orientation-only estimation problem in which the remaining unknowns can be derived in closed-form. Then, we demonstrate that it can be further reduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analytical derivations of the pose as a function of that unique scalar unknown. We illustrate our theoretical considerations by visual examples and include a discussion on the practical aspects. Finally, we release this paper along with the corresponding source code in order to contribute towards more efficient resolutions of ellipsoid-related pose estimation problems. The source code is available here: https://gitlab.inria.fr/vgaudill/p1e .},
  archive      = {J_IJCV},
  author       = {Gaudillière, Vincent and Simon, Gilles and Berger, Marie-Odile},
  doi          = {10.1007/s11263-023-01794-x},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2446-2470},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Perspective-1-ellipsoid: Formulation, analysis and solutions of the camera pose estimation problem from one ellipse-ellipsoid correspondence},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning geometric transformation for point cloud
completion. <em>IJCV</em>, <em>131</em>(9), 2425–2445. (<a
href="https://doi.org/10.1007/s11263-023-01820-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to estimate the missing shape from a partial point cloud. Existing encoder-decoder based generative models usually reconstruct the complete point cloud from the learned distribution of the shape prior, which may lead to distortion of geometric details (such as sharp structures and structures without smooth surfaces) due to the information loss of the latent space embedding. To address this problem, we formulate point cloud completion as a geometric transformation problem and propose a simple yet effective geometric transformation network (GTNet). It exploits the repetitive geometric structures in common 3D objects to recover the complete shapes, which contains three sub-networks: geometric patch network, structure transformation network, and detail refinement network. Specifically, the geometric patch network iteratively discovers repetitive geometric structures that are related or similar to the missing parts. Then, the structure transformation network uses the discovered geometric structures to complete the corresponding missing parts by learning their spatial transformations such as symmetry, rotation, translation, and uniform scaling. Finally, the detail refinement network performs global optimization to eliminate unnatural structures. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods on the Shape-Net55-34, MVP, PCN, and KITTI datasets. Models and code will be available at https://github.com/ivislabhit/GTNet .},
  archive      = {J_IJCV},
  author       = {Zhang, Shengping and Liu, Xianzhu and Xie, Haozhe and Nie, Liqiang and Zhou, Huiyu and Tao, Dacheng and Li, Xuelong},
  doi          = {10.1007/s11263-023-01820-y},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2425-2445},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning geometric transformation for point cloud completion},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skeletonizing caenorhabditis elegans based on u-net
architectures trained with a multi-worm low-resolution synthetic
dataset. <em>IJCV</em>, <em>131</em>(9), 2408–2424. (<a
href="https://doi.org/10.1007/s11263-023-01818-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeletonization algorithms are used as basic methods to solve tracking problems, pose estimation, or predict animal group behavior. Traditional skeletonization techniques, based on image processing algorithms, are very sensitive to the shapes of the connected components in the initial segmented image, especially when these are low-resolution images. Currently, neural networks are an alternative providing more robust results in the presence of image-based noise. However, training a deep neural network requires a very large and balanced dataset, which is sometimes too expensive or impossible to obtain. This work proposes a new training method based on a custom-generated dataset with a synthetic image simulator. This training method was applied to different U-Net neural networks architectures to solve the problem of skeletonization using low-resolution images of multiple Caenorhabditis elegans contained in Petri dishes measuring 55 mm in diameter. These U-Net models had only been trained and validated with a synthetic image; however, they were successfully tested with a dataset of real images. All the U-Net models presented a good generalization of the real dataset, endorsing the proposed learning method, and also gave good skeletonization results in the presence of image-based noise. The best U-Net model presented a significant improvement of 3.32\% with respect to previous work using traditional image processing techniques.},
  archive      = {J_IJCV},
  author       = {Layana Castro, Pablo E. and García Garví, Antonio and Navarro Moya, Francisco and Sánchez-Salmerón, Antonio-José},
  doi          = {10.1007/s11263-023-01818-6},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2408-2424},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Skeletonizing caenorhabditis elegans based on U-net architectures trained with a multi-worm low-resolution synthetic dataset},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Making the invisible visible: Toward high-quality terahertz
tomographic imaging via physics-guided restoration. <em>IJCV</em>,
<em>131</em>(9), 2388–2407. (<a
href="https://doi.org/10.1007/s11263-023-01812-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terahertz (THz) tomographic imaging has recently attracted significant attention thanks to its non-invasive, non-destructive, non-ionizing, material-classification, and ultra-fast nature for object exploration and inspection. However, its strong water absorption nature and low noise tolerance lead to undesired blurs and distortions of reconstructed THz images. The diffraction-limited THz signals highly constrain the performances of existing restoration methods. To address the problem, we propose a novel multi-view Subspace-Attention-guided Restoration Network (SARNet) that fuses multi-view and multi-spectral features of THz images for effective image restoration and 3D tomographic reconstruction. To this end, SARNet uses multi-scale branches to extract intra-view spatio-spectral amplitude and phase features and fuse them via shared subspace projection and self-attention guidance. We then perform inter-view fusion to further improve the restoration of individual views by leveraging the redundancies between neighboring views. Here, we experimentally construct a THz time-domain spectroscopy (THz-TDS) system covering a broad frequency range from 0.1 to 4 THz for building up a temporal/spectral/spatial/material THz database of hidden 3D objects. Complementary to a quantitative evaluation, we demonstrate the effectiveness of our SARNet model on 3D THz tomographic reconstruction applications.},
  archive      = {J_IJCV},
  author       = {Su, Weng-Tai and Hung, Yi-Chun and Yu, Po-Jen and Yang, Shang-Hua and Lin, Chia-Wen},
  doi          = {10.1007/s11263-023-01812-y},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2388-2407},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Making the invisible visible: Toward high-quality terahertz tomographic imaging via physics-guided restoration},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised domain generalization with stochastic
StyleMatch. <em>IJCV</em>, <em>131</em>(9), 2377–2387. (<a
href="https://doi.org/10.1007/s11263-023-01821-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. To this end, we study semi-supervised domain generalization (SSDG), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. We design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (DG) and semi-supervised learning (SSL). We find that the DG methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. We propose StyleMatch, a simple approach that extends FixMatch with a couple of new ingredients tailored for SSDG: (1) stochastic modeling for reducing overfitting in scarce labels, and (2) multi-view consistency learning for enhancing domain generalization. Despite the concise designs, StyleMatch achieves significant improvements in SSDG. We hope our approach and the comprehensive benchmarks can pave the way for future research on generalizable and data-efficient learning systems. The source code is released at https://github.com/KaiyangZhou/ssdg-benchmark .},
  archive      = {J_IJCV},
  author       = {Zhou, Kaiyang and Loy, Chen Change and Liu, Ziwei},
  doi          = {10.1007/s11263-023-01821-x},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2377-2387},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semi-supervised domain generalization with stochastic StyleMatch},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards fine-grained optimal 3D face dense registration: An
iterative dividing and diffusing method. <em>IJCV</em>, <em>131</em>(9),
2356–2376. (<a
href="https://doi.org/10.1007/s11263-023-01825-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense vertex-to-vertex correspondence (i.e. registration) between 3D faces is a fundamental and challenging issue for 3D &amp;2D face analysis. While the sparse landmarks are definite with anatomically ground-truth correspondence, the dense vertex correspondences on most facial regions are unknown. In this view, the current methods commonly result in reasonable but diverse solutions, which deviate from the optimum to the dense registration problem. In this paper, we revisit dense registration by a dimension-degraded problem, i.e. proportional segmentation of a line, and employ an iterative dividing and diffusing method to reach an optimum solution that is robust to different initializations. We formulate a local registration problem for dividing and a linear least-square problem for diffusing, with constraints on fixed features on a 3D facial surface. We further propose a multi-resolution algorithm to accelerate the computational process. The proposed method is linked to a novel local scaling metric, where we illustrate the physical significance as smooth adaptions for local cells of 3D facial shapes. Extensive experiments on public datasets demonstrate the effectiveness of the proposed method in various aspects. Generally, the proposed method leads to not only significantly better representations of 3D facial data, but also coherent local deformations with elegant grid architecture for fine-grained registrations.},
  archive      = {J_IJCV},
  author       = {Fan, Zhenfeng and Peng, Silong and Xia, Shihong},
  doi          = {10.1007/s11263-023-01825-7},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2356-2376},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards fine-grained optimal 3D face dense registration: An iterative dividing and diffusing method},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-aware visual decomposition for image coding.
<em>IJCV</em>, <em>131</em>(9), 2333–2355. (<a
href="https://doi.org/10.1007/s11263-023-01809-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel image coding framework with semantic-aware visual decomposition towards extremely low bitrate compression. In particular, an input image is analyzed into a semantic map as structural representation and semantic-wise texture representation and further compressed into bitstreams at the encoder side. On the decoder side, the received bitstreams of dual-layer representations are decoded and reconstructed for target image synthesis with generative models. Moreover, the attention mechanism is introduced into the model architecture for texture representation modeling and a coherency regularization is proposed to further optimize the texture representation space by aligning the representation space with the source pixel space for higher synthesis quality. Besides, we also propose a cross-channel entropy module and control the quantization scale to facilitate rate-distortion optimization. Upon compressing the decomposed components into the bitstream, the simple yet effective representation philosophy benefits image compression in many aspects. First, in terms of compression performance, compact representations, and high visual synthesis quality can bring remarkable advantages. Second, the proposed framework yields a physically explainable bitstream composed of the structural segment and semantic-wise texture segments. Third and most importantly, subsequent vision tasks (e.g., content manipulation) can receive fundamental support from the semantic-aware visual decomposition and synthesis mechanism. Extensive experimental results demonstrate the superiority of the proposed framework towards efficient visual representation learning, high efficiency image compression ( $$&lt;0.1$$ bpp), and intelligent visual applications (e.g., manipulation and analysis).},
  archive      = {J_IJCV},
  author       = {Chang, Jianhui and Zhang, Jian and Li, Jiguo and Wang, Shiqi and Mao, Qi and Jia, Chuanmin and Ma, Siwei and Gao, Wen},
  doi          = {10.1007/s11263-023-01809-7},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2333-2355},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-aware visual decomposition for image coding},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On making SIFT features affine covariant. <em>IJCV</em>,
<em>131</em>(9), 2316–2332. (<a
href="https://doi.org/10.1007/s11263-023-01802-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An approach is proposed for recovering affine correspondences (ACs) from orientation- and scale-covariant, e.g., SIFT, features exploiting pre-estimated epipolar geometry. The method calculates the affine parameters consistent with the epipolar geometry from the point coordinates and the scales and rotations which the feature detector obtains. The proposed closed-form solver returns a single solution and is extremely fast, i.e., 0.5 $$\upmu $$ seconds on average. Possible applications include estimating the homography from a single upgraded correspondence and, also, estimating the surface normal for each correspondence found in a pre-calibrated image pair (e.g., stereo rig). As the second contribution, we propose a minimal solver that estimates the relative pose of a vehicle-mounted camera from a single SIFT correspondence with the corresponding surface normal obtained from, e.g., upgraded ACs. The proposed algorithms are tested both on synthetic data and on a number of publicly available real-world datasets. Using the upgraded features and the proposed solvers leads to a significant speed-up in the homography, multi-homography and relative pose estimation problems with better or comparable accuracy to the state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Barath, Daniel},
  doi          = {10.1007/s11263-023-01802-0},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2316-2332},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On making SIFT features affine covariant},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blur invariants for image recognition. <em>IJCV</em>,
<em>131</em>(9), 2298–2315. (<a
href="https://doi.org/10.1007/s11263-023-01798-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blur is an image degradation that makes object recognition challenging. Restoration approaches solve this problem via image deblurring, deep learning methods rely on the augmentation of training sets. Invariants with respect to blur offer an alternative way of describing and recognising blurred images without any deblurring and data augmentation. In this paper, we present an original theory of blur invariants. Unlike all previous attempts, the new theory requires no prior knowledge of the blur type. The invariants are constructed in the Fourier domain by means of orthogonal projection operators and moment expansion is used for efficient and stable computation. Applying a general substitution rule, combined invariants to blur and spatial transformations are easy to construct and use. Experimental comparison to Convolutional Neural Networks shows the advantages of the proposed theory.},
  archive      = {J_IJCV},
  author       = {Flusser, Jan and Lébl, Matěj and Šroubek, Filip and Pedone, Matteo and Kostková, Jitka},
  doi          = {10.1007/s11263-023-01798-7},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2298-2315},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Blur invariants for image recognition},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards automated ethogramming: Cognitively-inspired event
segmentation for streaming wildlife video monitoring. <em>IJCV</em>,
<em>131</em>(9), 2267–2297. (<a
href="https://doi.org/10.1007/s11263-023-01781-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in visual perceptual tasks have been mainly driven by the amount, and types, of annotations of large-scale datasets. Researchers have focused on fully-supervised settings to train models using offline epoch-based schemes. Despite the evident advancements, limitations and cost of manually annotated datasets have hindered further development for event perceptual tasks, such as detection and localization of objects and events in videos. The problem is more apparent in zoological applications due to the scarcity of annotations and length of videos-most videos are at most ten minutes long. Inspired by cognitive theories, we present a self-supervised perceptual prediction framework to tackle the problem of temporal event segmentation by building a stable representation of event-related objects. The approach is simple but effective. We rely on LSTM predictions of high-level features computed by a standard deep learning backbone. For spatial segmentation, the stable representation of the object is used by an attention mechanism to filter the input features before the prediction step. The self-learned attention maps effectively localize the object as a side effect of perceptual prediction. We demonstrate our approach on long videos from continuous wildlife video monitoring, spanning multiple days at 25 FPS. We aim to facilitate automated ethogramming by detecting and localizing events without the need for labels. Our approach is trained in an online manner on streaming input and requires only a single pass through the video, with no separate training set. Given the lack of long and realistic (includes real-world challenges) datasets, we introduce a new wildlife video dataset–nest monitoring of the Kagu (a flightless bird from New Caledonia)–to benchmark our approach. Our dataset features a video from 10 days (over 23 million frames) of continuous monitoring of the Kagu in its natural habitat. We annotate every frame with bounding boxes and event labels. Additionally, each frame is annotated with time-of-day and illumination conditions. We will make the dataset, which is the first of its kind, and the code available to the research community. We find that the approach significantly outperforms other self-supervised, traditional (e.g., Optical Flow, Background Subtraction) and NN-based (e.g., PA-DPC, DINO, iBOT), baselines and performs on par with supervised boundary detection approaches (i.e., PC). At a recall rate of 80\%, our best performing model detects one false positive activity every 50 min of training. On average, we at least double the performance of self-supervised approaches for spatial segmentation. Additionally, we show that our approach is robust to various environmental conditions (e.g., moving shadows). We also benchmark the framework on other datasets (i.e., Kinetics-GEBD, TAPOS) from different domains to demonstrate its generalizability. The data and code are available on our project page: https://aix.eng.usf.edu/research_automated_ethogramming.html},
  archive      = {J_IJCV},
  author       = {Mounir, Ramy and Shahabaz, Ahmed and Gula, Roman and Theuerkauf, Jörn and Sarkar, Sudeep},
  doi          = {10.1007/s11263-023-01781-2},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2267-2297},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards automated ethogramming: Cognitively-inspired event segmentation for streaming wildlife video monitoring},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A family of approaches for full 3D reconstruction of objects
with complex surface reflectance. <em>IJCV</em>, <em>131</em>(9),
2243–2266. (<a
href="https://doi.org/10.1007/s11263-023-01795-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction of general scenes remains an open challenge with current techniques often reliant on assumptions on the scene’s surface reflectance, which restrict the range of objects that can be modelled. Helmholtz Stereopsis offers an appealing framework to make the modelling process agnostic to surface reflectance. However, previous formulations have been almost exclusively limited to 2.5D modelling. To address this gap, this paper introduces a family of reconstruction approaches that exploit Helmholtz reciprocity to produce complete 3D models of objects with arbitrary unknown reflectance. This includes an approach based on the fusion of (orthographic or perspective) view-dependent reconstructions, a volumetric approach optimising surface location within a voxel grid, and a mesh-based formulation optimising vertices positions of a given mesh topology. The contributed approaches are evaluated on synthetic and real datasets, including novel full 3D datasets publicly released with this paper, with experimental comparison against a wide range of competing methods. Results demonstrate the benefits of the different approaches and their abilities to achieve high quality full 3D reconstructions of complex objects.},
  archive      = {J_IJCV},
  author       = {Addari, Gianmarco and Guillemaut, Jean-Yves},
  doi          = {10.1007/s11263-023-01795-w},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2243-2266},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A family of approaches for full 3D reconstruction of objects with complex surface reflectance},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view consistent generative adversarial networks for
compositional 3D-aware image synthesis. <em>IJCV</em>, <em>131</em>(8),
2219–2242. (<a
href="https://doi.org/10.1007/s11263-023-01805-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies compositional 3D-aware image synthesis for both single-object and multi-object scenes. We observe that two challenges remain in this field: existing approaches (1) lack geometry constraints and thus compromise the multi-view consistency of the single object, and (2) can not scale to multi-object scenes with complex backgrounds. To address these challenges coherently, we propose multi-view consistent generative adversarial networks (MVCGAN) for compositional 3D-aware image synthesis. First, we build the geometry constraints on the single object by leveraging the underlying 3D information. Specifically, we enforce the photometric consistency between pairs of views, encouraging the model to learn the inherent 3D shape. Second, we adapt MVCGAN to multi-object scenarios. In particular, we formulate the multi-object scene generation as a “decompose and compose” process. During training, we adopt the top-down strategy to decompose training images into objects and backgrounds. When rendering, we deploy a reverse bottom-up manner by composing the generated objects and background into the holistic scene. Extensive experiments on both single-object and multi-object datasets show that the proposed method achieves competitive performance for 3D-aware image synthesis.},
  archive      = {J_IJCV},
  author       = {Zhang, Xuanmeng and Zheng, Zhedong and Gao, Daiheng and Zhang, Bang and Yang, Yi and Chua, Tat-Seng},
  doi          = {10.1007/s11263-023-01805-x},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2219-2242},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-view consistent generative adversarial networks for compositional 3D-aware image synthesis},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance segmentation in the dark. <em>IJCV</em>,
<em>131</em>(8), 2198–2218. (<a
href="https://doi.org/10.1007/s11263-023-01808-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing instance segmentation techniques are primarily tailored for high-visibility inputs, but their performance significantly deteriorates in extremely low-light environments. In this work, we take a deep look at instance segmentation in the dark and introduce several techniques that substantially boost the low-light inference accuracy. The proposed method is motivated by the observation that noise in low-light images introduces high-frequency disturbances to the feature maps of neural networks, thereby significantly degrading performance. To suppress this “feature noise”, we propose a novel learning method that relies on an adaptive weighted downsampling layer, a smooth-oriented convolutional block, and disturbance suppression learning. These components effectively reduce feature noise during downsampling and convolution operations, enabling the model to learn disturbance-invariant features. Furthermore, we discover that high-bit-depth RAW images can better preserve richer scene information in low-light conditions compared to typical camera sRGB outputs, thus supporting the use of RAW-input algorithms. Our analysis indicates that high bit-depth can be critical for low-light instance segmentation. To mitigate the scarcity of annotated RAW datasets, we leverage a low-light RAW synthetic pipeline to generate realistic low-light data. In addition, to facilitate further research in this direction, we capture a real-world low-light instance segmentation dataset comprising over two thousand paired low/normal-light images with instance-level pixel-wise annotations. Remarkably, without any image preprocessing, we achieve satisfactory performance on instance segmentation in very low light (4\% AP higher than state-of-the-art competitors), meanwhile opening new opportunities for future research. Our code and dataset are publicly available to the community ( https://github.com/Linwei-Chen/LIS ).},
  archive      = {J_IJCV},
  author       = {Chen, Linwei and Fu, Ying and Wei, Kaixuan and Zheng, Dezhi and Heide, Felix},
  doi          = {10.1007/s11263-023-01808-8},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2198-2218},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Instance segmentation in the dark},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking portrait matting with privacy preserving.
<em>IJCV</em>, <em>131</em>(8), 2172–2197. (<a
href="https://doi.org/10.1007/s11263-023-01797-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and directs the network to reacquire the face context at both data and feature level. Extensive experiments on P3M-10k and public benchmarks demonstrate the superiority of P3M-Net over state-of-the-art methods and the effectiveness of P3M-CP in improving the cross-domain generalization ability, implying a great significance of P3M for future research and real-world applications. The dataset, code and models are available here ( https://github.com/ViTAE-Transformer/P3M-Net ).},
  archive      = {J_IJCV},
  author       = {Ma, Sihan and Li, Jizhizi and Zhang, Jing and Zhang, He and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01797-8},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2172-2197},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking portrait matting with privacy preserving},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight and progressively-scalable networks for semantic
segmentation. <em>IJCV</em>, <em>131</em>(8), 2153–2171. (<a
href="https://doi.org/10.1007/s11263-023-01801-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale learning frameworks have been regarded as a capable class of models to boost semantic segmentation. The problem nevertheless is not trivial especially for the real-world deployments, which often demand high efficiency in inference latency. In this paper, we thoroughly analyze the design of convolutional blocks (the type of convolutions and the number of channels in convolutions), and the ways of interactions across multiple scales, all from lightweight standpoint for semantic segmentation. With such in-depth comparisons, we conclude three principles, and accordingly devise Lightweight and Progressively-Scalable Networks (LPS-Net) that novelly expands the network complexity in a greedy manner. Technically, LPS-Net first capitalizes on the principles to build a tiny network. Then, LPS-Net progressively scales the tiny network to larger ones by expanding a single dimension (the number of convolutional blocks, the number of channels, or the input resolution) at one time to meet the best speed/accuracy tradeoff. Extensive experiments conducted on three datasets consistently demonstrate the superiority of LPS-Net over several efficient semantic segmentation methods. More remarkably, our LPS-Net achieves 73.4\% mIoU on Cityscapes test set, with the speed of 413.5FPS on an NVIDIA GTX 1080Ti, leading to a performance improvement by 1.5\% and a 65\% speed-up against the state-of-the-art STDC. Code is available at https://github.com/YihengZhang-CV/LPS-Net .},
  archive      = {J_IJCV},
  author       = {Zhang, Yiheng and Yao, Ting and Qiu, Zhaofan and Mei, Tao},
  doi          = {10.1007/s11263-023-01801-1},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2153-2171},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight and progressively-scalable networks for semantic segmentation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal 3D object detection in autonomous driving: A
survey. <em>IJCV</em>, <em>131</em>(8), 2122–2152. (<a
href="https://doi.org/10.1007/s11263-023-01784-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed the rapid development of autonomous driving systems. However, it remains a daunting task to achieve full autonomy, especially when it comes to understanding the ever-changing, complex driving scenes. To alleviate the difficulty of perception, self-driving vehicles are usually equipped with a suite of sensors (e.g., cameras, LiDARs), hoping to capture the scenes with overlapping perspectives to minimize blind spots. Fusing these data streams and exploiting their complementary properties is thus rapidly becoming the current trend. Nonetheless, combining data that are captured by different sensors with drastically different ranging/ima-ging mechanisms is not a trivial task; instead, many factors need to be considered and optimized. If not careful, data from one sensor may act as noises to data from another sensor, with even poorer results by fusing them. Thus far, there has been no in-depth guidelines to designing the multi-modal fusion based 3D perception algorithms. To fill in the void and motivate further investigation, this survey conducts a thorough study of tens of recent deep learning based multi-modal 3D detection networks (with a special emphasis on LiDAR-camera fusion), focusing on their fusion stage (i.e., when to fuse), fusion inputs (i.e., what to fuse), and fusion granularity (i.e., how to fuse). These important design choices play a critical role in determining the performance of the fusion algorithm. In this survey, we first introduce the background of popular sensors used for self-driving, their data properties, and the corresponding object detection algorithms. Next, we discuss existing datasets that can be used for evaluating multi-modal 3D object detection algorithms. Then we present a review of multi-modal fusion based 3D detection networks, taking a close look at their fusion stage, fusion input and fusion granularity, and how these design choices evolve with time and technology. After the review, we discuss open challenges as well as possible solutions. We hope that this survey can help researchers to get familiar with the field and embark on investigations in the area of multi-modal 3D object detection.},
  archive      = {J_IJCV},
  author       = {Wang, Yingjie and Mao, Qiuyu and Zhu, Hanqi and Deng, Jiajun and Zhang, Yu and Ji, Jianmin and Li, Houqiang and Zhang, Yanyong},
  doi          = {10.1007/s11263-023-01784-z},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2122-2152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal 3D object detection in autonomous driving: A survey},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPL-net: Spatial-semantic patch learning network for facial
attribute recognition with limited labeled data. <em>IJCV</em>,
<em>131</em>(8), 2097–2121. (<a
href="https://doi.org/10.1007/s11263-023-01787-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based facial attribute recognition (FAR) methods rely heavily on large-scale labeled training data. Unfortunately, in many real-world applications, only limited labeled data are available, resulting in the performance deterioration of these methods. To address this issue, we propose a novel spatial-semantic patch learning network (SPL-Net), consisting of a multi-branch shared subnetwork (MSS), three auxiliary task subnetworks (ATS), and an FAR subnetwork, for attribute classification with limited labeled data. Considering the diversity of facial attributes, MSS includes a task-shared branch and four region branches, each of which contains cascaded dual cross attention modules to extract region-specific features. SPL-Net involves a two-stage learning procedure. In the first stage, MSS and ATS are jointly trained to perform three auxiliary tasks (i.e., a patch rotation task (PRT), a patch segmentation task (PST), and a patch classification task (PCT)), which exploit the spatial-semantic relationship on large-scale unlabeled facial data from various perspectives. Specifically, PRT encodes the spatial information of facial images based on self-supervised learning. PST and PCT respectively capture the pixel-level and image-level semantic information of facial images by leveraging a facial parsing model. Thus, a well-pretrained MSS is obtained. In the second stage, based on the pre-trained MSS, an FAR model is easily fine-tuned to predict facial attributes by requiring only a small amount of labeled data. Experimental results on challenging facial attribute datasets (including CelebA, LFWA, and MAAD) show the superiority of SPL-Net over several state-of-the-art methods in the case of limited labeled data.},
  archive      = {J_IJCV},
  author       = {Yan, Yan and Shu, Ying and Chen, Si and Xue, Jing-Hao and Shen, Chunhua and Wang, Hanzi},
  doi          = {10.1007/s11263-023-01787-w},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2097-2121},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SPL-net: Spatial-semantic patch learning network for facial attribute recognition with limited labeled data},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving semi-supervised and domain-adaptive semantic
segmentation with self-supervised depth estimation. <em>IJCV</em>,
<em>131</em>(8), 2070–2096. (<a
href="https://doi.org/10.1007/s11263-023-01799-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep networks for semantic segmentation requires large amounts of labeled training data, which presents a major challenge in practice, as labeling segmentation masks is a highly labor-intensive process. To address this issue, we present a framework for semi-supervised and domain-adaptive semantic segmentation, which is enhanced by self-supervised monocular depth estimation (SDE) trained only on unlabeled image sequences. In particular, we utilize SDE as an auxiliary task comprehensively across the entire learning framework: First, we automatically select the most useful samples to be annotated for semantic segmentation based on the correlation of sample diversity and difficulty between SDE and semantic segmentation. Second, we implement a strong data augmentation by mixing images and labels using the geometry of the scene. Third, we transfer knowledge from features learned during SDE to semantic segmentation by means of transfer and multi-task learning. And fourth, we exploit additional labeled synthetic data with Cross-Domain DepthMix and Matching Geometry Sampling to align synthetic and real data. We validate the proposed model on the Cityscapes dataset, where all four contributions demonstrate significant performance gains, and achieve state-of-the-art results for semi-supervised semantic segmentation as well as for semi-supervised domain adaptation. In particular, with only 1/30 of the Cityscapes labels, our method achieves 92\% of the fully-supervised baseline performance and even 97\% when exploiting additional data from GTA. The source code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth .},
  archive      = {J_IJCV},
  author       = {Hoyer, Lukas and Dai, Dengxin and Wang, Qin and Chen, Yuhua and Van Gool, Luc},
  doi          = {10.1007/s11263-023-01799-6},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2070-2096},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Improving semi-supervised and domain-adaptive semantic segmentation with self-supervised depth estimation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and accurate 3D registration from line intersection
constraints. <em>IJCV</em>, <em>131</em>(8), 2044–2069. (<a
href="https://doi.org/10.1007/s11263-023-01774-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Registration is a fundamental part of several robotics and automation tasks. While classical methods predominantly exploit constraints from points or plane correspondences, we have a different take using line intersections. In other words, we focus on exploiting geometric constraints arising from the intersection of two (different) 3D line segments in two scans. In particular, we derive nine minimal solvers from various geometric constraints arising from line intersections along with other constraints: plane correspondences, point correspondences, and line matches. We follow a two-step method for 3D registration: a coarse estimation with outlier rejection followed by refinement. In the first step, we use a hybrid RANSAC loop that utilizes all the minimal solvers. This RANSAC outputs a rough estimate for the 3D registration and the outlier/inlier classification for the 3D features. As for the refinement, we offer a non-linear technique using all the inliers obtained from the RANSAC and the coarse estimate. This method is of alternate minimization type, in which we alternate between estimating the rotation and the translation at each step. Thorough experiments with simulated data and two real-world datasets show that using these features and the combined solvers improves accuracy and is faster than the baselines.},
  archive      = {J_IJCV},
  author       = {Mateus, André and Ranade, Siddhant and Ramalingam, Srikumar and Miraldo, Pedro},
  doi          = {10.1007/s11263-023-01774-1},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2044-2069},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast and accurate 3D registration from line intersection constraints},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-view view synthesis with self-rectified
pseudo-stereo. <em>IJCV</em>, <em>131</em>(8), 2032–2043. (<a
href="https://doi.org/10.1007/s11263-023-01803-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing novel views from a single view image is a highly ill-posed problem. We discover an effective solution to reduce the learning ambiguity by expanding the single-view view synthesis problem to a multi-view setting. Specifically, we leverage the reliable and explicit stereo prior to generate a pseudo-stereo viewpoint, which serves as an auxiliary input to construct the 3D space. In this way, the challenging novel view synthesis process is decoupled into two simpler problems of stereo synthesis and 3D reconstruction. In order to synthesize a structurally correct and detail-preserved stereo image, we propose a self-rectified stereo synthesis to amend erroneous regions in an identify-rectify manner. Hard-to-train and incorrect warping samples are first discovered by two strategies, (1) pruning the network to reveal low-confident predictions; and (2) bidirectionally matching between stereo images to allow the discovery of improper mapping. These regions are then inpainted to form the final pseudo-stereo. With the aid of this extra input, a preferable 3D reconstruction can be easily obtained, and our method can work with arbitrary 3D representations. Extensive experiments show that our method outperforms state-of-the-art single-view view synthesis methods and stereo synthesis methods.},
  archive      = {J_IJCV},
  author       = {Zhou, Yang and Wu, Hanjie and Liu, Wenxi and Xiong, Zheng and Qin, Jing and He, Shengfeng},
  doi          = {10.1007/s11263-023-01803-z},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2032-2043},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Single-view view synthesis with self-rectified pseudo-stereo},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View birdification in the crowd: Ground-plane localization
from perceived movements. <em>IJCV</em>, <em>131</em>(8), 2015–2031. (<a
href="https://doi.org/10.1007/s11263-023-01788-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce view birdification, the problem of recovering ground-plane movements of people in a crowd from an ego-centric video captured from an observer (e.g., a person or a vehicle) also moving in the crowd. Recovered ground-plane movements would provide a sound basis for situational understanding and benefit downstream applications in computer vision and robotics. In this paper, we formulate view birdification as a geometric trajectory reconstruction problem and derive a cascaded optimization method from a Bayesian perspective. The method first estimates the observer’s movement and then localizes surrounding pedestrians for each frame while taking into account the local interactions between them. We introduce three datasets by leveraging synthetic and real trajectories of people in crowds and evaluate the effectiveness of our method. The results demonstrate the accuracy of our method and set the ground for further studies of view birdification as an important but challenging visual understanding problem.},
  archive      = {J_IJCV},
  author       = {Nishimura, Mai and Nobuhara, Shohei and Nishino, Ko},
  doi          = {10.1007/s11263-023-01788-9},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2015-2031},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {View birdification in the crowd: Ground-plane localization from perceived movements},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoEncoder-driven multimodal collaborative learning for
medical image synthesis. <em>IJCV</em>, <em>131</em>(8), 1995–2014. (<a
href="https://doi.org/10.1007/s11263-023-01791-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal medical images have been widely applied in various clinical diagnoses and treatments. Due to the practical restrictions, certain modalities may be hard to acquire, resulting in incomplete data. Existing methods attempt to generate the missing data with multiple available modalities. However, the modality differences in tissue contrast and lesion appearance become an obstacle to making a precise estimation. To address this issue, we propose an autoencoder-driven multimodal collaborative learning framework for medical image synthesis. The proposed approach takes an autoencoder to comprehensively supervise the synthesis network using the self-representation of target modality, which provides target-modality-specific prior to guide multimodal image fusion. Furthermore, we endow the autoencoder with adversarial learning capabilities by converting its encoder into a pixel-sensitive discriminator capable of both reconstruction and discrimination. To this end, the generative model is completely supervised by the autoencoder. Considering the efficiency of multimodal generation, we also introduce a modality mask vector as the target modality label to guide the synthesis direction, empowering our method to estimate any missing modality with a single model. Extensive experiments on multiple medical image datasets demonstrate the significant generalization capability as well as the superior synthetic quality of the proposed method, compared with other competing methods. The source code will be available: https://github.com/bcaosudo/AE-GAN .},
  archive      = {J_IJCV},
  author       = {Cao, Bing and Bi, Zhiwei and Hu, Qinghua and Zhang, Han and Wang, Nannan and Gao, Xinbo and Shen, Dinggang},
  doi          = {10.1007/s11263-023-01791-0},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1995-2014},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoEncoder-driven multimodal collaborative learning for medical image synthesis},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised secondary landmark detection via 3D
representation learning. <em>IJCV</em>, <em>131</em>(8), 1980–1994. (<a
href="https://doi.org/10.1007/s11263-023-01804-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological developments have lead to great advances in the computerized tracking of joints and other landmarks in moving animals, including humans. Such tracking promises important advances in biology and biomedicine. Modern tracking models depend critically on labor-intensive annotated datasets of primary landmarks by non-expert humans. However, such annotation approaches can be costly and impractical for secondary landmarks, that is, ones that reflect fine-grained geometry of animals, and that are often specific to customized behavioral tasks. Due to visual and geometric ambiguity, non-experts are often not qualified for secondary landmark annotation, which can require anatomical and zoological knowledge. These barriers significantly impede downstream behavioral studies because the learned tracking models exhibit limited generalizability. We hypothesize that there exists a shared representation between the primary and secondary landmarks because the range of motion of the secondary landmarks can be approximately spanned by that of the primary landmarks. We present a method to learn this spatial relationship of the primary and secondary landmarks in three dimensional space, which can, in turn, self-supervise the secondary landmark detector. This 3D representation learning is generic, and can therefore be applied to various multiview settings across diverse organisms, including macaques, flies, and humans.},
  archive      = {J_IJCV},
  author       = {Bala, Praneet and Zimmermann, Jan and Park, Hyun Soo and Hayden, Benjamin Y.},
  doi          = {10.1007/s11263-023-01804-y},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1980-1994},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised secondary landmark detection via 3D representation learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BARC: Breed-augmented regression using classification for 3D
dog reconstruction from images. <em>IJCV</em>, <em>131</em>(8),
1964–1979. (<a
href="https://doi.org/10.1007/s11263-023-01780-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/ .},
  archive      = {J_IJCV},
  author       = {Rueegg, Nadine and Zuffi, Silvia and Schindler, Konrad and Black, Michael J.},
  doi          = {10.1007/s11263-023-01780-3},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1964-1979},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BARC: Breed-augmented regression using classification for 3D dog reconstruction from images},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D object detection for autonomous driving: A comprehensive
survey. <em>IJCV</em>, <em>131</em>(8), 1909–1963. (<a
href="https://doi.org/10.1007/s11263-023-01790-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving, in recent years, has been receiving increasing attention for its potential to relieve drivers’ burdens and improve the safety of driving. In modern autonomous driving pipelines, the perception system is an indispensable component, aiming to accurately estimate the status of surrounding environments and provide reliable observations for prediction and planning. 3D object detection, which aims to predict the locations, sizes, and categories of the 3D objects near an autonomous vehicle, is an important part of a perception system. This paper reviews the advances in 3D object detection for autonomous driving. First, we introduce the background of 3D object detection and discuss the challenges in this task. Second, we conduct a comprehensive survey of the progress in 3D object detection from the aspects of models and sensory inputs, including LiDAR-based, camera-based, and multi-modal detection approaches. We also provide an in-depth analysis of the potentials and challenges in each category of methods. Additionally, we systematically investigate the applications of 3D object detection in driving systems. Finally, we conduct a performance analysis of the 3D object detection approaches, and we further summarize the research trends over the years and prospect the future directions of this area.},
  archive      = {J_IJCV},
  author       = {Mao, Jiageng and Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  doi          = {10.1007/s11263-023-01790-1},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1909-1963},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {3D object detection for autonomous driving: A comprehensive survey},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Credible dual-expert learning for weakly supervised semantic
segmentation. <em>IJCV</em>, <em>131</em>(8), 1892–1908. (<a
href="https://doi.org/10.1007/s11263-023-01796-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Great progress has been witnessed for weakly supervised semantic segmentation, which aims to segment objects without dense pixel annotations. Most approaches concentrate on generating high quality pseudo labels, which are then fed into a standard segmentation model as supervision. However, such a solution has one major limitation: noise of pseudo labels is inevitable, which is unsolvable for the standard segmentation model. In this paper, we propose a credible dual-expert learning (CDL) framework to mitigate the noise of pseudo labels. Specifically, we first observe that the model predictions with different optimization loss functions will have different credible regions; thus, it is possible to make self-corrections with multiple predictions. Based on this observation, we design a dual-expert structure to mine credible predictions, which are then processed by our noise correction module to update pseudo labels in an online way. Meanwhile, to handle the case that the dual-expert produces incredible predictions for the same region, we design a relationship transfer module to provide feature relationships, enabling our noise correction module to transfer predictions from the credible regions to such incredible regions. Considering the above designs, we propose a base CDL network and an extended CDL network to satisfy different requirements. Extensive experiments show that directly replacing our model with a conventional fully supervised segmentation model, the performances of various weakly supervised semantic segmentation pipelines were boosted, achieving new state-of-the-art performances on both PASCAL VOC 2012 and MS COCO with a clear margin. Code will be available at: https://github.com/zbf1991/CDL .},
  archive      = {J_IJCV},
  author       = {Zhang, Bingfeng and Xiao, Jimin and Wei, Yunchao and Zhao, Yao},
  doi          = {10.1007/s11263-023-01796-9},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1892-1908},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Credible dual-expert learning for weakly supervised semantic segmentation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric and statistical models for analysis of two-object
complexes. <em>IJCV</em>, <em>131</em>(8), 1877–1891. (<a
href="https://doi.org/10.1007/s11263-023-01800-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated shape features involving nearby objects often contain important anatomic information. However, it is difficult to capture shape information within and between objects for a joint analysis of multi-object complexes. This paper proposes (1) capturing between-object shape based on an explicit mathematical model called a linking structure, (2) capturing shape features that are invariant to rigid transformation using local affine frames and (3) capturing Correlation of Within- and Between-Object (CoWBO) shape features using a statistical method called NEUJIVE. The resulting correlated shape features give comprehensive understanding of multi-object complexes from various perspectives. First, these features explicitly account for the positional and geometric relations between objects that can be anatomically important. Second, the local affine frames give rise to rich interior geometric features that are invariant to global alignment. Third, the joint analysis of within- and between-object shape yields robust and useful features. To demonstrate the proposed methods, we classify individuals with autism and controls using the extracted shape features of two functionally related brain structures, the hippocampus and the caudate. We found that the CoWBO features give the best classification performance among various choices of shape features. Moreover, the group difference is statistically significant in the feature space formed by the proposed methods.},
  archive      = {J_IJCV},
  author       = {Liu, Zhiyuan and Damon, James and Marron, J. S. and Pizer, Stephen},
  doi          = {10.1007/s11263-023-01800-2},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {1877-1891},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Geometric and statistical models for analysis of two-object complexes},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Learning to adapt to light. <em>IJCV</em>,
<em>131</em>(7), 1875. (<a
href="https://doi.org/10.1007/s11263-023-01764-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Kai-Fu and Cheng, Cheng and Zhao, Shi-Xuan and Yan, Hong-Mei and Zhang, Xian-Shi and Li, Yong-Jie},
  doi          = {10.1007/s11263-023-01764-3},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1875},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: Learning to adapt to light},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-target knowledge distillation via student
self-reflection. <em>IJCV</em>, <em>131</em>(7), 1857–1874. (<a
href="https://doi.org/10.1007/s11263-023-01792-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a simple yet effective technique for deep model compression, which aims to transfer the knowledge learned by a large teacher model to a small student model. To mimic how the teacher teaches the student, existing knowledge distillation methods mainly adapt an unidirectional knowledge transfer, where the knowledge extracted from different intermedicate layers of the teacher model is used to guide the student model. However, it turns out that the students can learn more effectively through multi-stage learning with a self-reflection in the real-world education scenario, which is nevertheless ignored by current knowledge distillation methods. Inspired by this, we devise a new knowledge distillation framework entitled multi-target knowledge distillation via student self-reflection or MTKD-SSR, which can not only enhance the teacher’s ability in unfolding the knowledge to be distilled, but also improve the student’s capacity of digesting the knowledge. Specifically, the proposed framework consists of three target knowledge distillation mechanisms: a stage-wise channel distillation (SCD), a stage-wise response distillation (SRD), and a cross-stage review distillation (CRD), where SCD and SRD transfer feature-based knowledge (i.e., channel features) and response-based knowledge (i.e., logits) at different stages, respectively; and CRD encourages the student model to conduct self-reflective learning after each stage by a self-distillation of the response-based knowledge. Experimental results on five popular visual recognition datasets, CIFAR-100, Market-1501, CUB200-2011, ImageNet, and Pascal VOC, demonstrate that the proposed framework significantly outperforms recent state-of-the-art knowledge distillation methods.},
  archive      = {J_IJCV},
  author       = {Gou, Jianping and Xiong, Xiangshuo and Yu, Baosheng and Du, Lan and Zhan, Yibing and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01792-z},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1857-1874},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-target knowledge distillation via student self-reflection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory based temporal fusion network for video deblurring.
<em>IJCV</em>, <em>131</em>(7), 1840–1856. (<a
href="https://doi.org/10.1007/s11263-023-01793-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video deblurring is one of the most challenging vision tasks because of the complex spatial-temporal relationship and a number of uncertainty factors involved in video acquisition. As different moving objects in the video exhibit different motion trajectories, it is difficult to accurately capture their spatial-temporal relationships. In this paper, we proposed a memory-based temporal fusion network (TFN) to capture local spatial-temporal relationships across the input sequence for video deblurring. Our temporal fusion network consists of a memory network and a temporal fusion block. The memory network stores the extracted spatial-temporal relationships and guides the temporal fusion blocks to extract local spatial-temporal relationships more accurately. In addition, in order to enable our model to more effectively fuse the multiscale features of the previous frame, we propose a multiscale and multi-hop reconstruction memory network (RMN) based on the attention mechanism and memory network. We constructed a feature extractor that integrates residual dense blocks with three downsample layers to extract hierarchical spatial features. Finally, we feed these aggregated local features into a reconstruction module to restore sharp video frames. Experimental results on public datasets show that our temporal fusion network has achieved a significant performance improvement in terms of PSNR metrics (over 1dB) over existing state-of-the-art video deblurring methods.},
  archive      = {J_IJCV},
  author       = {Wang, Chaohua and Dong, Weisheng and Li, Xin and Wu, Fangfang and Wu, Jinjian and Shi, Guangming},
  doi          = {10.1007/s11263-023-01793-y},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1840-1856},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Memory based temporal fusion network for video deblurring},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust-region adaptive frequency for online continual
learning. <em>IJCV</em>, <em>131</em>(7), 1825–1839. (<a
href="https://doi.org/10.1007/s11263-023-01775-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paradigm of online continual learning, one neural network is exposed to a sequence of tasks, where the data arrive in an online fashion and previously seen data are not accessible. Such online fashion causes insufficient learning and severe forgetting on past tasks issues, preventing a good stability-plasticity trade-off, where ideally the network is expected to have high plasticity to adapt to new tasks well and have the stability to prevent forgetting on old tasks simultaneously. To solve these issues, we propose a trust-region adaptive frequency approach, which alternates between standard-process and intra-process updates. Specifically, the standard-process replays data stored in a coreset and interleaves the data with current data, and the intra-process updates the network parameters based on the coreset. Furthermore, to improve the unsatisfactory performance stemming from online fashion, the frequency of the intra-process is adjusted based on a trust region, which is measured by the confidence score of current data. During the intra-process, we distill the dark knowledge to retain useful learned knowledge. Moreover, to store more representative data in the coreset, a confidence-based coreset selection is presented in an online manner. The experimental results on standard benchmarks show that the proposed method significantly outperforms state-of-art continual learning algorithms.},
  archive      = {J_IJCV},
  author       = {Kong, Yajing and Liu, Liu and Qiao, Maoying and Wang, Zhen and Tao, Dacheng},
  doi          = {10.1007/s11263-023-01775-0},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1825-1839},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Trust-region adaptive frequency for online continual learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advanced binary neural network for single image super
resolution. <em>IJCV</em>, <em>131</em>(7), 1808–1824. (<a
href="https://doi.org/10.1007/s11263-023-01789-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary neural network (BNN) is an effective approach to accelerate the model inference and has been initially applied in the field of single image super resolution (SISR). However, the optimization of efficiency and accuracy remains a major challenge for achieving further improvements. While existing BNN-based SR methods solve the SISR problems by proposing a residual block-oriented quantization mechanism, the quantization process in the up-sampling stage and the representation tendency of binary super resolution networks are ignored. In this paper, we propose an Advanced Binary Super Resolution (ABSR) method to optimize the binary generator in terms of quantization mechanism and up-sampling strategy. Specifically, we first design an excitation-selection mechanism for binary inference, which could distinctively implement self-adjustment of activation and significantly reduce inference errors. Furthermore, we construct a binary up-sampling strategy that achieves performance almost equal to that of real-valued up-sampling modules, and fully frees up the inference speed of the binary network. Extensive experiments show that the ABSR not only reaches state-of-the-art BNN-based SR performance in terms of objective metrics and visual quality, but also reduces computational consumption drastically.},
  archive      = {J_IJCV},
  author       = {Xin, Jingwei and Wang, Nannan and Jiang, Xinrui and Li, Jie and Gao, Xinbo},
  doi          = {10.1007/s11263-023-01789-8},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1808-1824},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Advanced binary neural network for single image super resolution},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural architecture search for dense prediction tasks in
computer vision. <em>IJCV</em>, <em>131</em>(7), 1784–1807. (<a
href="https://doi.org/10.1007/s11263-023-01785-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning in recent years has lead to a rising demand for neural network architecture engineering. As a consequence, neural architecture search (NAS), which aims at automatically designing neural network architectures in a data-driven manner rather than manually, has evolved as a popular field of research. With the advent of weight sharing strategies across architectures, NAS has become applicable to a much wider range of problems. In particular, there are now many publications for dense prediction tasks in computer vision that require pixel-level predictions, such as semantic segmentation or object detection. These tasks come with novel challenges, such as higher memory footprints due to high-resolution data, learning multi-scale representations, longer training times, and more complex and larger neural architectures. In this manuscript, we provide an overview of NAS for dense prediction tasks by elaborating on these novel challenges and surveying ways to address them to ease future research and application of existing methods to novel problems.},
  archive      = {J_IJCV},
  author       = {Mohan, Rohit and Elsken, Thomas and Zela, Arber and Metzen, Jan Hendrik and Staffler, Benedikt and Brox, Thomas and Valada, Abhinav and Hutter, Frank},
  doi          = {10.1007/s11263-023-01785-y},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1784-1807},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Neural architecture search for dense prediction tasks in computer vision},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning accurate performance predictors for ultrafast
automated model compression. <em>IJCV</em>, <em>131</em>(7), 1761–1783.
(<a href="https://doi.org/10.1007/s11263-023-01783-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an ultrafast automated model compression framework called SeerNet for flexible network deployment. Conventional non-differen-tiable methods discretely search the desirable compression policy based on the accuracy from exhaustively trained lightweight models, and existing differentiable methods optimize an extremely large supernet to obtain the required compressed model for deployment. They both cause heavy computational cost due to the complex compression policy search and evaluation process. On the contrary, we obtain the optimal efficient networks by directly optimizing the compression policy with an accurate performance predictor, where the ultrafast automated model compression for various computational cost constraint is achieved without complex compression policy search and evaluation. Specifically, we first train the performance predictor based on the accuracy from uncertain compression policies actively selected by efficient evolutionary search, so that informative supervision is provided to learn the accurate performance predictor with acceptable cost. Then we leverage the gradient that maximizes the predicted performance under the barrier complexity constraint for ultrafast acquisition of the desirable compression policy, where adaptive update stepsizes with momentum are employed to enhance optimality of the acquired pruning and quantization strategy. Compared with the state-of-the-art automated model compression methods, experimental results on image classification and object detection show that our method achieves competitive accuracy-complexity trade-offs with significant reduction of the search cost. Code is available at https://github.com/ZiweiWangTHU/SeerNet .},
  archive      = {J_IJCV},
  author       = {Wang, Ziwei and Lu, Jiwen and Xiao, Han and Liu, Shengyu and Zhou, Jie},
  doi          = {10.1007/s11263-023-01783-0},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1761-1783},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning accurate performance predictors for ultrafast automated model compression},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disassembling convolutional segmentation network.
<em>IJCV</em>, <em>131</em>(7), 1741–1760. (<a
href="https://doi.org/10.1007/s11263-023-01776-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the convolutional segmentation network has achieved remarkable performance in the computer vision area. However, training a practicable segmentation network is time- and resource-consuming. In this paper, focusing on the semantic image segmentation task, we attempt to disassemble a convolutional segmentation network into category-aware convolution kernels and achieve customizable tasks without additional training by utilizing those kernels. The core of disassembling convolutional segmentation networks is how to identify the relevant convolution kernels for a specific category. According to the encoder-decoder network architecture, the disassembling framework, named Disassembler, is devised to be composed of the forward channel-wise activation attribution and backward gradient attribution. In the forward channel-wise activation attribution process, for each image, the activation values of each feature map in the high-confidence mask area are summed into category-aware probability vectors. In the backward gradient attribution process, the positive gradients w.r.t. each feature map in the high-confidence mask area are summed into a relative coefficient vector for each category. With the cooperation of two vectors, the Disassembler can effectively disassemble category-aware convolution kernels. Extensive experiments demonstrate that the proposed Disassembler can accomplish the category-customizable task without additional training. The disassembled category-aware sub-network achieves comparable performance without any finetuning and will outperform existing state-of-the-art methods with one epoch of finetuning.},
  archive      = {J_IJCV},
  author       = {Hu, Kaiwen and Gao, Jing and Mao, Fangyuan and Song, Xinhui and Cheng, Lechao and Feng, Zunlei and Song, Mingli},
  doi          = {10.1007/s11263-023-01776-z},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1741-1760},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Disassembling convolutional segmentation network},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From open set to closed set: Supervised spatial
divide-and-conquer for object counting. <em>IJCV</em>, <em>131</em>(7),
1722–1740. (<a
href="https://doi.org/10.1007/s11263-023-01782-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual counting, a task that aims to estimate the number of objects from an image/video, is an open-set problem by nature as the number of population can vary in $$[0,+\infty )$$ in theory. However, collected data are limited in reality, which means that only a closed set is observed. Existing methods typically model this task through regression, while they are prone to suffer from unseen scenes with counts out of the scope of the closed set. In fact, counting has an interesting and exclusive property—spatially decomposable. A dense region can always be divided until sub-region counts are within the previously observed closed set. We therefore introduce the idea of spatial divide-and-conquer (S-DC) that transforms open-set counting into a closed set problem. This idea is implemented by a novel Supervised Spatial Divide-and-Conquer Network (SS-DCNet). It can learn from a closed set but generalize to open-set scenarios via S-DC. We provide mathematical analyses and a controlled experiment on synthetic data, demonstrating why closed-set modeling works well. Experiments show that SS-DCNet achieves state-of-the-art performance in crowd counting, vehicle counting and plant counting. SS-DCNet also demonstrates superior transferablity under the cross-dataset setting. Code and models are available at: https://git.io/SS-DCNet .},
  archive      = {J_IJCV},
  author       = {Xiong, Haipeng and Lu, Hao and Liu, Chengxin and Liu, Liang and Shen, Chunhua and Cao, Zhiguo},
  doi          = {10.1007/s11263-023-01782-1},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1722-1740},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From open set to closed set: Supervised spatial divide-and-conquer for object counting},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-calibration networks for weakly-supervised video
representation learning. <em>IJCV</em>, <em>131</em>(7), 1704–1721. (<a
href="https://doi.org/10.1007/s11263-023-01779-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leverage of large volumes of web videos paired with the query (short phrase for searching the video) or surrounding text (long textual description, e.g., video title) offers an economic and extensible alternative to supervised video representation learning. Nevertheless, modeling such weakly visual-textual connection is not trivial due to query polysemy (i.e., many possible meanings for a query) and text isomorphism (i.e., same syntactic structure of different text). In this paper, we introduce a new design of mutual calibration between query and text to achieve more reliable visual-textual supervision for video representation learning. Specifically, we present Bi-Calibration Networks (BCN) that novelly couples two calibrations to learn the correction from text to query and vice versa. Technically, BCN executes clustering on all the titles of the videos searched by an identical query and takes the centroid of each cluster as a text prototype. All the queries constitute the query set. The representation learning of BCN is then formulated as video classification over text prototypes and queries, with text-to-query and query-to-text calibrations. A selection scheme is also devised to balance the two calibrations. Two large-scale web video datasets paired with query and title, named YOVO-3M and YOVO-10M, are newly collected for weakly-supervised video feature learning. The video features of BCN with ResNet backbone learnt on YOVO-3M (3M YouTube videos) obtain superior results under linear protocol on action recognition. More remarkably, BCN trained on the larger set of YOVO-10M (10M YouTube videos) with further fine-tuning leads to 1.3\% gain in top-1 accuracy on Kinetics-400 dataset over the state-of-the-art TAda2D method with ImageNet pre-training. Source code and datasets are available at https://github.com/FuchenUSTC/BCN .},
  archive      = {J_IJCV},
  author       = {Long, Fuchen and Yao, Ting and Qiu, Zhaofan and Tian, Xinmei and Luo, Jiebo and Mei, Tao},
  doi          = {10.1007/s11263-023-01779-w},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1704-1721},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bi-calibration networks for weakly-supervised video representation learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial learning domain-invariant conditional features
for robust face anti-spoofing. <em>IJCV</em>, <em>131</em>(7),
1680–1703. (<a
href="https://doi.org/10.1007/s11263-023-01778-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing has been widely exploited in recent years to ensure security in face recognition systems; however, this technology suffers from poor generalization performance on unseen samples. Most previous methods align the marginal distributions from multiple source domains to learn domain-invariant features to mitigate domain shift. However, the category information of samples from different domains is ignored during these marginal distribution alignments; this can potentially lead to features of one category from one domain being misaligned to those of different categories from other domains, although the marginal distributions across domains are well aligned from the whole point of view. In this paper, we propose a simple but effective conditional domain adversarial framework whose main goal is to align the conditional distributions across domains to learn domain-invariant conditional features. Specifically, we first construct a parallel domain structure and its corresponding regularization to reduce negative influences from the finite samples and diversity of spoof face images on the conditional distribution alignments. Then, based on the parallel domain structure, a feature extractor and a global domain classifier, which play a conditional domain adversarial game, are leveraged to make the features of the same category across different domains indistinguishable. Moreover, intra-domain and cross-domain discrimination regularization are further exploited in conjunction with conditional domain adversarial training to minimize the classification error of class predictors. Extensive qualitative and quantitative experiments demonstrate that the proposed method learns well-generalized features from fewer source domains and achieves state-of-the-art performance on six public datasets.},
  archive      = {J_IJCV},
  author       = {Jiang, Fangling and Li, Qi and Liu, Pengcheng and Zhou, Xiang-Dong and Sun, Zhenan},
  doi          = {10.1007/s11263-023-01778-x},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1680-1703},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adversarial learning domain-invariant conditional features for robust face anti-spoofing},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive deep PnP algorithm for video snapshot compressive
imaging. <em>IJCV</em>, <em>131</em>(7), 1662–1679. (<a
href="https://doi.org/10.1007/s11263-023-01777-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Snapshot compressive imaging (SCI) is a promising technique to capture high-speed videos, which transforms the imaging speed from the detector to mask modulating and only needs a single measurement to capture multiple frames. The algorithm to reconstruct high-speed frames from the measurement plays a vital role in SCI. In this paper, we consider the promising reconstruction algorithm framework, namely plug-and-play (PnP), which is flexible to the encoding process comparing with other deep learning networks. One drawback of existing PnP algorithms is that they use a pre-trained denoising network as a plugged prior while the training data of the network might be different from the task in real applications. Towards this end, in this work, we propose the online PnP algorithm which can adaptively update the network’s parameters within the PnP iteration; this makes the denoising network more applicable to the desired data in the SCI reconstruction. Furthermore, for color video imaging, RGB frames need to be recovered from Bayer pattern or named demosaicing in the camera pipeline. To address this challenge, we design a two-stage reconstruction framework to optimize these two coupled ill-posed problems and introduce a deep demosaicing prior specifically for video demosaicing in SCI. Extensive results on both simulation and real datasets verify the superiority of our adaptive deep PnP algorithm. Code is available at https://github.com/xyvirtualgroup/AdaptivePnP_SCI .},
  archive      = {J_IJCV},
  author       = {Wu, Zongliang and Yang, Chengshuai and Su, Xiongfei and Yuan, Xin},
  doi          = {10.1007/s11263-023-01777-y},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1662-1679},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive deep PnP algorithm for video snapshot compressive imaging},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient person search: An anchor-free approach.
<em>IJCV</em>, <em>131</em>(7), 1642–1661. (<a
href="https://doi.org/10.1007/s11263-023-01772-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search aims to simultaneously localize and identify a query person from uncropped images. To achieve this goal, state-of-the-art models typically add a re-id branch upon two-stage detectors like Faster R-CNN. Owing to the ROI-Align operation, this pipeline yields promising accuracy as re-id features are explicitly aligned with the corresponding object regions, but in the meantime, it introduces high computational overhead due to dense object anchors. In this work, we present an anchor-free approach to efficiently tackling this challenging task, by introducing the following dedicated designs. First, we select an anchor-free detector (i.e., FCOS) as the prototype of our framework. Due to the lack of dense object anchors, it exhibits significantly higher efficiency compared with existing person search models. Second, when directly accommodating this anchor-free detector for person search, there exist several misalignment issues in different levels (i.e., scale, region, and task). To address these issues, we propose an aligned feature aggregation module to generate more discriminative and robust feature embeddings. Accordingly, we name our framework as Feature-Aligned Person Search Network (AlignPS). Third, by investigating the advantages of both anchor-based and anchor-free models, we further augment AlignPS with an ROI-Align head, which significantly improves the robustness of re-id features while still keeping our model highly efficient. Our framework not only achieves state-of-the-art or competitive performance on two challenging person search benchmarks, but can be also extended to other challenging searching tasks such as animal and object search. All the source codes, data, and trained models are available at: https://github.com/daodaofr/alignps .},
  archive      = {J_IJCV},
  author       = {Yan, Yichao and Li, Jinpeng and Qin, Jie and Zheng, Peng and Liao, Shengcai and Yang, Xiaokang},
  doi          = {10.1007/s11263-023-01772-3},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1642-1661},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Efficient person search: An anchor-free approach},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangling geometric deformation spaces in generative
latent shape models. <em>IJCV</em>, <em>131</em>(7), 1611–1641. (<a
href="https://doi.org/10.1007/s11263-023-01750-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complete representation of 3D objects requires characterizing the space of deformations in an interpretable manner, from articulations of a single instance to changes in shape across categories. In this work, we improve on a prior generative model of geometric disentanglement for 3D shapes, wherein the space of object geometry is factorized into rigid orientation, non-rigid pose, and intrinsic shape. The resulting model can be trained from raw 3D shapes, without correspondences, labels, or even rigid alignment, using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space. Our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space. The geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object. Furthermore, it enables tasks like pose transfer and pose-aware retrieval without requiring supervision. We evaluate our model on its generative modelling, representation learning, and disentanglement performance, showing improved rotation invariance and intrinsic-extrinsic factorization quality over the prior model.},
  archive      = {J_IJCV},
  author       = {Aumentado-Armstrong, Tristan and Tsogkas, Stavros and Dickinson, Sven and Jepson, Allan},
  doi          = {10.1007/s11263-023-01750-9},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {1611-1641},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Disentangling geometric deformation spaces in generative latent shape models},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RELAX: Representation learning explainability.
<em>IJCV</em>, <em>131</em>(6), 1584–1610. (<a
href="https://doi.org/10.1007/s11263-023-01773-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant improvements that self-supervised representation learning has led to when learning from unlabeled data, no methods have been developed that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations that significantly outperform the gradient-based baselines. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Moreover, we conduct a user study to assess how well the proposed approach aligns with human intuition and show that the proposed method outperforms the baselines in both the quantitative and human evaluation studies. Finally, we illustrate the usability of RELAX in several use cases and highlight that incorporating uncertainty can be essential for providing faithful explanations, taking a crucial step towards explaining representations.},
  archive      = {J_IJCV},
  author       = {Wickstrøm, Kristoffer K. and Trosten, Daniel J. and Løkse, Sigurd and Boubekki, Ahcène and Mikalsen, Karl øyvind and Kampffmeyer, Michael C. and Jenssen, Robert},
  doi          = {10.1007/s11263-023-01773-2},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1584-1610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RELAX: Representation learning explainability},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning enriched hop-aware correlation for robust 3D human
pose estimation. <em>IJCV</em>, <em>131</em>(6), 1566–1583. (<a
href="https://doi.org/10.1007/s11263-023-01770-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution networks (GCNs) based methods for 3D human pose estimation usually aggregate immediate features of single-hop nodes, which are unaware of the correlation of multi-hop nodes and therefore neglect long-range dependency for predicting complex poses. In addition, they typically operate either on single-scale or sequential down-sampled multi-scale graph representations, resulting in the loss of contextual information or spatial details. To address these problems, this paper proposes a parallel hop-aware graph attention network (PHGANet) for 3D human pose estimation, which learns enriched hop-aware correlation of the skeleton joints while maintaining the spatially-precise representations of the human graph. Specifically, we propose a hop-aware skeletal graph attention (HSGAT) module to capture the semantic correlation of multi-hop nodes, which first calculates skeleton-based 1-hop attention and then disseminates it to arbitrary hops via graph connectivity. To alleviate the redundant noise introduced by the interactions with distant nodes, HSGAT uses an attenuation strategy to separate attention from distinct hops and assign them learnable attenuation weights according to their distances adaptively. Upon HSGAT, we further build PHGANet with multiple parallel branches of stacked HSGAT modules to learn the enriched hop-aware correlation of human skeletal structures at different scales. In addition, a joint centrality encoding scheme is proposed to introduce node importance as a bias in the learned graph representation, which makes the core joints (e.g., neck and pelvis) more influential during node aggregation. Experimental results indicate that PHGANet performs favorably against state-of-the-art methods on the Human3.6M and MPI-INF-3DHP benchmarks. Models and code are available at https://github.com/ChenyangWang95/PHGANet/.},
  archive      = {J_IJCV},
  author       = {Zhang, Shengping and Wang, Chenyang and Nie, Liqiang and Yao, Hongxun and Huang, Qingming and Tian, Qi},
  doi          = {10.1007/s11263-023-01770-5},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1566-1583},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning enriched hop-aware correlation for robust 3D human pose estimation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extreme low-resolution action recognition with confident
spatial-temporal attention transfer. <em>IJCV</em>, <em>131</em>(6),
1550–1565. (<a
href="https://doi.org/10.1007/s11263-023-01771-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition on extreme low-resolution videos, e.g., a resolution of $$12 \times 16$$ pixels, plays a vital role in far-view surveillance and privacy-preserving multimedia analysis. As low-resolution videos often only contain limited information, it is difficult for us to perform action recognition in them. Given the fact that one same action may be represented by videos in both high resolution (HR) and extreme low resolution (eLR), it is worth studying to utilize the relevant HR data to improve the eLR action recognition. In this work, we propose a novel Confident Spatial-Temporal Attention Transfer (CSTAT) for eLR action recognition. CSTAT acquires information from HR data by reducing the attention differences with a transfer-learning strategy. Besides, the confidence of the supervisory signal is also taken into consideration for a more reliable transferring process. Experimental results demonstrate that, the proposed method can effectively improve the accuracy of eLR action recognition and achieve state-of-the-art performances on $$12\times 16$$ HMDB51, $$12\times 16$$ Kinects-400, and $$12\times 16$$ Something-Something v2.},
  archive      = {J_IJCV},
  author       = {Bai, Yucai and Zou, Qin and Chen, Xieyuanli and Li, Lingxi and Ding, Zhengming and Chen, Long},
  doi          = {10.1007/s11263-023-01771-4},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1550-1565},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Extreme low-resolution action recognition with confident spatial-temporal attention transfer},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view tracking, re-ID, and social network analysis of a
flock of visually similar birds in an outdoor aviary. <em>IJCV</em>,
<em>131</em>(6), 1532–1549. (<a
href="https://doi.org/10.1007/s11263-023-01768-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to capture detailed interactions among individuals in a social group is foundational to our study of animal behavior and neuroscience. Recent advances in deep learning and computer vision are driving rapid progress in methods that can record the actions and interactions of multiple individuals simultaneously. Many social species, such as birds, however, live deeply embedded in a three-dimensional world. This world introduces additional perceptual challenges such as occlusions, orientation-dependent appearance, large variation in apparent size, and poor sensor coverage for 3D reconstruction, that are not encountered by applications studying animals that move and interact only on 2D planes. Here we introduce a system for studying the behavioral dynamics of a group of songbirds as they move throughout a 3D aviary. We study the complexities that arise when tracking a group of closely interacting animals in three dimensions and introduce a novel dataset for evaluating multi-view trackers. Finally, we analyze captured ethogram data and demonstrate that social context affects the distribution of sequential interactions between birds in the aviary.},
  archive      = {J_IJCV},
  author       = {Xiao, Shiting and Wang, Yufu and Perkes, Ammon and Pfrommer, Bernd and Schmidt, Marc and Daniilidis, Kostas and Badger, Marc},
  doi          = {10.1007/s11263-023-01768-z},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1532-1549},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-view tracking, re-ID, and social network analysis of a flock of visually similar birds in an outdoor aviary},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Through hawks’ eyes: Synthetically reconstructing the visual
field of a bird in flight. <em>IJCV</em>, <em>131</em>(6), 1497–1531.
(<a href="https://doi.org/10.1007/s11263-022-01733-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Birds of prey rely on vision to execute flight manoeuvres that are key to their survival, such as intercepting fast-moving targets or navigating through clutter. A better understanding of the role played by vision during these manoeuvres is not only relevant within the field of animal behaviour, but could also have applications for autonomous drones. In this paper, we present a novel method that uses computer vision tools to analyse the role of active vision in bird flight, and demonstrate its use to answer behavioural questions. Combining motion capture data from Harris’ hawks with a hybrid 3D model of the environment, we render RGB images, semantic maps, depth information and optic flow outputs that characterise the visual experience of the bird in flight. In contrast with previous approaches, our method allows us to consider different camera models and alternative gaze strategies for the purposes of hypothesis testing, allows us to consider visual input over the complete visual field of the bird, and is not limited by the technical specifications and performance of a head-mounted camera light enough to attach to a bird’s head in flight. We present pilot data from three sample flights: a pursuit flight, in which a hawk intercepts a moving target, and two obstacle avoidance flights. With this approach, we provide a reproducible method that facilitates the collection of large volumes of data across many individuals, opening up new avenues for data-driven models of animal behaviour.},
  archive      = {J_IJCV},
  author       = {Miñano, Sofía and Golodetz, Stuart and Cavallari, Tommaso and Taylor, Graham K.},
  doi          = {10.1007/s11263-022-01733-2},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1497-1531},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Through hawks’ eyes: Synthetically reconstructing the visual field of a bird in flight},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep memory-augmented proximal unrolling network for
compressive sensing. <em>IJCV</em>, <em>131</em>(6), 1477–1496. (<a
href="https://doi.org/10.1007/s11263-023-01765-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping a truncated optimization method into a deep neural network, deep proximal unrolling network has attracted attention in compressive sensing due to its good interpretability and high performance. Each stage in such networks corresponds to one iteration in optimization. By understanding the network from the perspective of the human brain’s memory processing, we find there exist two categories of memory transmission: intra-stage and inter-stage. For intra-stage, existing methods increase the number of parameters to maximize the information flow. For inter-stage, there are also two methods. One is to transfer the information between adjacent stages, which can be regarded as short-term memory that is usually lost seriously. The other is a mechanism to ensure that the previous stages affect the current stage, which has not been explicitly studied. In this paper, a novel deep proximal unrolling network with persistent memory is proposed, dubbed deep Memory-Augmented Proximal Unrolling Network (MAPUN). We design a memory-augmented proximal mapping module that ensures maximum information flow for intra- and inter-stage. Specifically, we present a self-modulated block that can adaptively develop feature modulation for intra-stage and introduce two types of memory augmentation mechanisms for inter-stage, namely High-throughput Short-term Memory (HSM) and Cross-stage Long-term Memory (CLM). HSM is exploited to allow the network to transmit multi-channel short-term memory, which greatly reduces information loss between adjacent stages. CLM is utilized to develop the dependency of deep information across cascading stages, which greatly enhances network representation capability. Extensive experiments show that our MAPUN outperforms existing state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Song, Jiechong and Chen, Bin and Zhang, Jian},
  doi          = {10.1007/s11263-023-01765-2},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1477-1496},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep memory-augmented proximal unrolling network for compressive sensing},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refractive pose refinement. <em>IJCV</em>, <em>131</em>(6),
1448–1476. (<a
href="https://doi.org/10.1007/s11263-023-01763-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate absolute and relative pose estimation under refraction, which are essential problems for refractive structure from motion. To cope with refraction effects, we first formulate geometric constraints for establishing iterative algorithms to optimize absolute and relative pose. By classifying two scenarios according to the geometric relationship between the camera and refractive interface, we derive the corresponding solutions to solve the optimization problems efficiently. In the scenario where the geometry between the camera and refractive interface is fixed (e.g., underwater imaging), we also show that the refractive epipolar constraint for relative pose can be established as a summation of the classical essential matrix and two correction terms caused by refraction by using the virtual camera transformation. Thanks to its succinct form, the resulting refractive epipolar constraint can be efficiently optimized. We evaluate our proposed algorithms on synthetic data showing superior accuracy and computational efficiency compared to state-of-the-art (SOTA) methods. We further demonstrate the application of the proposed algorithms in refractive structure from motion on real data. Our datasets (Hu et al., RefractiveSfM, https://github.com/diku-dk/RefractiveSfM , 2022) and code (Hu et al., DIKU Refractive Scenes Dataset 2022, Data, 2022) are publicly available.},
  archive      = {J_IJCV},
  author       = {Hu, Xiao and Lauze, François and Pedersen, Kim Steenstrup},
  doi          = {10.1007/s11263-023-01763-4},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1448-1476},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Refractive pose refinement},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A minimal solution for image-based sphere estimation.
<em>IJCV</em>, <em>131</em>(6), 1428–1447. (<a
href="https://doi.org/10.1007/s11263-023-01766-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel minimal solver for sphere fitting via its 2D central projection, i.e., a special ellipse. The input of the presented algorithm consists of contour points detected in a camera image. General ellipse fitting problems require five contour points. However, taking advantage of the isotropic spherical target, three points are enough to define the tangent cone parameters of the sphere. This yields the sought ellipse parameters. Similarly, the sphere center can be estimated from the cone if the radius is known. These proposed geometric methods are rapid, numerically stable, and easy to implement. Experimental results—on synthetic, photorealistic, and real images—showcase the superiority of the proposed solutions to the state-of-the-art methods. A real-world LiDAR-camera calibration application justifies the utility of the sphere-based approach resulting in an error below a few centimeters.},
  archive      = {J_IJCV},
  author       = {Tóth, Tekla and Hajder, Levente},
  doi          = {10.1007/s11263-023-01766-1},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1428-1447},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A minimal solution for image-based sphere estimation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised visual tracking of marine animals using
autonomous underwater vehicles. <em>IJCV</em>, <em>131</em>(6),
1406–1427. (<a
href="https://doi.org/10.1007/s11263-023-01762-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-situ visual observations of marine organisms is crucial to developing behavioural understandings and their relations to their surrounding ecosystem. Typically, these observations are collected via divers, tags, and remotely-operated or human-piloted vehicles. Recently, however, autonomous underwater vehicles equipped with cameras and embedded computers with GPU capabilities are being developed for a variety of applications, and in particular, can be used to supplement these existing data collection mechanisms where human operation or tags are more difficult. Existing approaches have focused on using fully-supervised tracking methods, but labelled data for many underwater species are severely lacking. Semi-supervised trackers may offer alternative tracking solutions because they require less data than fully-supervised counterparts. However, because there are not existing realistic underwater tracking datasets, the performance of semi-supervised tracking algorithms in the marine domain is not well understood. To better evaluate their performance and utility, in this paper we provide (1) a novel dataset specific to marine animals located at http://warp.whoi.edu/vmat/ , (2) an evaluation of state-of-the-art semi-supervised algorithms in the context of underwater animal tracking, and (3) an evaluation of real-world performance through demonstrations using a semi-supervised algorithm on-board an autonomous underwater vehicle to track marine animals in the wild.},
  archive      = {J_IJCV},
  author       = {Cai, Levi and McGuire, Nathan E. and Hanlon, Roger and Mooney, T. Aran and Girdhar, Yogesh},
  doi          = {10.1007/s11263-023-01762-5},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1406-1427},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semi-supervised visual tracking of marine animals using autonomous underwater vehicles},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved 3D markerless mouse pose estimation using temporal
semi-supervision. <em>IJCV</em>, <em>131</em>(6), 1389–1405. (<a
href="https://doi.org/10.1007/s11263-023-01756-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional markerless pose estimation from multi-view video is emerging as an exciting method for quantifying the behavior of freely moving animals. Nevertheless, scientifically precise 3D animal pose estimation remains challenging, primarily due to a lack of large training and benchmark datasets and the immaturity of algorithms tailored to the demands of animal experiments and body plans. Existing techniques employ fully supervised convolutional neural networks (CNNs) trained to predict body keypoints in individual video frames, but this demands a large collection of labeled training samples to achieve desirable 3D tracking performance. Here, we introduce a semi-supervised learning strategy that incorporates unlabeled video frames via a simple temporal constraint applied during training. In freely moving mice, our new approach improves the current state-of-the-art performance of multi-view volumetric 3D pose estimation and further enhances the temporal stability and skeletal consistency of 3D tracking.},
  archive      = {J_IJCV},
  author       = {Li, Tianqing and Severson, Kyle S. and Wang, Fan and Dunn, Timothy W.},
  doi          = {10.1007/s11263-023-01756-3},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1389-1405},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Improved 3D markerless mouse pose estimation using temporal semi-supervision},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-driven detection of invertebrate species in deep-sea
video. <em>IJCV</em>, <em>131</em>(6), 1367–1388. (<a
href="https://doi.org/10.1007/s11263-023-01755-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each year, underwater remotely operated vehicles (ROVs) collect thousands of hours of video of unexplored ocean habitats revealing a plethora of information regarding biodiversity on Earth. However, fully utilizing this information remains a challenge as proper annotations and analysis require trained scientists’ time, which is both limited and costly. To this end, we present a Dataset for Underwater Substrate and Invertebrate Analysis (DUSIA), a benchmark suite and growing large-scale dataset to train, validate, and test methods for temporally localizing four underwater substrates as well as temporally and spatially localizing 59 underwater invertebrate species. DUSIA currently includes over ten hours of footage across 25 videos captured in 1080p at 30 fps by an ROV following pre-planned transects across the ocean floor near the Channel Islands of California. Each video includes annotations indicating the start and end times of substrates across the video in addition to counts of species of interest. Some frames are annotated with precise bounding box locations for invertebrate species of interest, as seen in Fig. 1. To our knowledge, DUSIA is the first dataset of its kind for deep sea exploration, with video from a moving camera, that includes substrate annotations and invertebrate species that are present at significant depths where sunlight does not penetrate. Additionally, we present the novel context-driven object detector (CDD) where we use explicit substrate classification to influence an object detection network to simultaneously predict a substrate and species class influenced by that substrate. We also present a method for improving training on partially annotated bounding box frames. Finally, we offer a baseline method for automating the counting of invertebrate species of interest.},
  archive      = {J_IJCV},
  author       = {McEver, R. Austin and Zhang, Bowen and Levenson, Connor and Iftekhar, A S M and Manjunath, B. S.},
  doi          = {10.1007/s11263-023-01755-4},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1367-1388},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context-driven detection of invertebrate species in deep-sea video},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMG: A micro-gesture dataset towards spontaneous body
gestures for emotional stress state analysis. <em>IJCV</em>,
<em>131</em>(6), 1346–1366. (<a
href="https://doi.org/10.1007/s11263-023-01761-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore using body gestures for hidden emotional state analysis. As an important non-verbal communicative fashion, human body gestures are capable of conveying emotional information during social communication. In previous works, efforts have been made mainly on facial expressions, speech, or expressive body gestures to interpret classical expressive emotions. Differently, we focus on a specific group of body gestures, called micro-gestures (MGs), used in the psychology research field to interpret inner human feelings. MGs are subtle and spontaneous body movements that are proven, together with micro-expressions, to be more reliable than normal facial expressions for conveying hidden emotional information. In this work, a comprehensive study of MGs is presented from the computer vision aspect, including a novel spontaneous micro-gesture (SMG) dataset with two emotional stress states and a comprehensive statistical analysis indicating the correlations between MGs and emotional states. Novel frameworks are further presented together with various state-of-the-art methods as benchmarks for automatic classification, online recognition of MGs, and emotional stress state recognition. The dataset and methods presented could inspire a new way of utilizing body gestures for human emotion understanding and bring a new direction to the emotion AI community. The source code and dataset are made available: https://github.com/mikecheninoulu/SMG .},
  archive      = {J_IJCV},
  author       = {Chen, Haoyu and Shi, Henglin and Liu, Xin and Li, Xiaobai and Zhao, Guoying},
  doi          = {10.1007/s11263-023-01761-6},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1346-1366},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SMG: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantics-guided intra-category knowledge transfer for
generalized zero-shot learning. <em>IJCV</em>, <em>131</em>(6),
1331–1345. (<a
href="https://doi.org/10.1007/s11263-023-01767-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) requires one to associate visual and semantic information observed from data of seen classes, so that test data of unseen classes can be recognized based on the described semantic representation. Aiming at synthesizing visual data from the given semantic inputs, hallucination-based ZSL approaches might suffer from mode collapse and biased problems due to the lack of ability in modeling the desirable visual features for unseen categories. In this paper, we present a generative model of Cross-Modal Consistency GAN (CMC-GAN), which performs semantics-guided intra-category knowledge transfer across image categories, so that data hallucination for unseen classes can be achieved with proper semantics and sufficient visual diversity. In our experiments, we perform standard and generalized ZSL on four benchmark datasets, confirming the effectiveness of our approach over that of state-of-the-art ZSL methods.},
  archive      = {J_IJCV},
  author       = {Yang, Fu-En and Lee, Yuan-Hao and Lin, Chia-Ching and Wang, Yu-Chiang Frank},
  doi          = {10.1007/s11263-023-01767-0},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1331-1345},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantics-guided intra-category knowledge transfer for generalized zero-shot learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PhysFormer++: Facial video-based physiological measurement
with SlowFast temporal difference transformer. <em>IJCV</em>,
<em>131</em>(6), 1307–1330. (<a
href="https://doi.org/10.1007/s11263-023-01758-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video without any contact, has great potential in many applications (e.g., remote healthcare and affective computing). Recent deep learning approaches focus on mining subtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper, we propose two end-to-end video transformer based architectures, namely PhysFormer and PhysFormer++, to adaptively aggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in PhysFormer, the temporal difference transformers first enhance the quasi-periodic rPPG features with temporal difference guided global attention, and then refine the local spatio-temporal representation against interference. To better exploit the temporal contextual and periodic rPPG clues, we also extend the PhysFormer to the two-pathway SlowFast based PhysFormer++ with temporal difference periodic and cross-attention transformers. Furthermore, we propose the label distribution learning and a curriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer and PhysFormer++ and alleviate overfitting. Comprehensive experiments are performed on four benchmark datasets to show our superior performance on both intra- and cross-dataset testings. Unlike most transformer networks needed pretraining from large-scale datasets, the proposed PhysFormer family can be easily trained from scratch on rPPG datasets, which makes it promising as a novel transformer baseline for the rPPG community.},
  archive      = {J_IJCV},
  author       = {Yu, Zitong and Shen, Yuming and Shi, Jingang and Zhao, Hengshuang and Cui, Yawen and Zhang, Jiehua and Torr, Philip and Zhao, Guoying},
  doi          = {10.1007/s11263-023-01758-1},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1307-1330},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PhysFormer++: Facial video-based physiological measurement with SlowFast temporal difference transformer},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction: Spatio-temporal outdoor lighting aggregation on
image sequences using transformer networks. <em>IJCV</em>,
<em>131</em>(5), 1302–1306. (<a
href="https://doi.org/10.1007/s11263-022-01747-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Lee, Haebom and Homeyer, Christian and Herzog, Robert and Rexilius, Jan and Rother, Carsten},
  doi          = {10.1007/s11263-022-01747-w},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1302-1306},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Spatio-temporal outdoor lighting aggregation on image sequences using transformer networks},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction: Spatial monitoring and insect behavioural
analysis using computer vision for precision pollination. <em>IJCV</em>,
<em>131</em>(5), 1300–1301. (<a
href="https://doi.org/10.1007/s11263-022-01741-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Ratnayake, Malika Nisal and Amarathunga, Don Chathurika and Zaman, Asaduz and Dyer, Adrian G. and Dorin, Alan},
  doi          = {10.1007/s11263-022-01741-2},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1300-1301},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Spatial monitoring and insect behavioural analysis using computer vision for precision pollination},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Editor’s note: Special issue on computer vision and
cultural heritage preservation. <em>IJCV</em>, <em>131</em>(5), 1299.
(<a href="https://doi.org/10.1007/s11263-023-01769-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-023-01769-y},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1299},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision and cultural heritage preservation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deblurring low-light images with events. <em>IJCV</em>,
<em>131</em>(5), 1284–1298. (<a
href="https://doi.org/10.1007/s11263-023-01754-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern image-based deblurring methods usually show degenerate performance in low-light conditions since the images often contain most of the poorly visible dark regions and a few saturated bright regions, making the amount of effective features that can be extracted for deblurring limited. In contrast, event cameras can trigger events with a very high dynamic range and low latency, which hardly suffer from saturation and naturally encode dense temporal information about motion. However, in low-light conditions existing event-based deblurring methods would become less robust since the events triggered in dark regions are often severely contaminated by noise, leading to inaccurate reconstruction of the corresponding intensity values. Besides, since they directly adopt the event-based double integral model to perform pixel-wise reconstruction, they can only handle low-resolution grayscale active pixel sensor images provided by the DAVIS camera, which cannot meet the requirement of daily photography. In this paper, to apply events to deblurring low-light images robustly, we propose a unified two-stage framework along with a motion-aware neural network tailored to it, reconstructing the sharp image under the guidance of high-fidelity motion clues extracted from events. Besides, we build an RGB-DAVIS hybrid camera system to demonstrate that our method has the ability to deblur high-resolution RGB images due to the natural advantages of our two-stage framework. Experimental results show our method achieves state-of-the-art performance on both synthetic and real-world images.},
  archive      = {J_IJCV},
  author       = {Zhou, Chu and Teng, Minggui and Han, Jin and Liang, Jinxiu and Xu, Chao and Cao, Gang and Shi, Boxin},
  doi          = {10.1007/s11263-023-01754-5},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1284-1298},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deblurring low-light images with events},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual restoration of ancient wooden ships through
non-rigid 3D shape assembly with ruled-surface FFD. <em>IJCV</em>,
<em>131</em>(5), 1269–1283. (<a
href="https://doi.org/10.1007/s11263-023-01759-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, 3D data has been widely used in archaeology and in the field of conservation and restoration of cultural properties. Virtual restoration, which reconstructs the original state in virtual space, is one of the promising applications utilizing 3D scanning data. Though many studies of virtual restoration have been conducted, it is still challenging to restore the cultural properties that consist of multiple deformable components because it is not feasible to identify the original shape uniquely. As a solution to such a problem, we proposed a non-rigid 3D shape assembly method for virtually restoring wooden ships that are composed of multiple timbers. The deformed timber can be well represented by ruled surface. We proposed a free-form deformation method with a ruled surface and an assembly method to align the deformable components mutually. The method employs a bottom-up approach that does not require reference data for target objects. The proposed framework narrows down the searching space for optimization using the physical constraints of wooden materials, and it can also obtain optimal solutions. We also showed an experimental result, where we virtually restored King Khufu’s first solar boat. The boat was originally constructed by assembling several timbers. The boat was reconstructed as a real object and is currently exhibited at a museum. However, unfortunately, the entire shape of the boat is slightly distorted. We applied the proposed method using archaeological knowledge and then showed the virtual restoration results using the acquired 3D data of the boat’s components.},
  archive      = {J_IJCV},
  author       = {Nemoto, Takashi and Kobayashi, Tetsuya and Kagesawa, Masataka and Oishi, Takeshi and Kurokochi, Hiromasa and Yoshimura, Sakuji and Zidan, Eissa and Taha, Mamdouh},
  doi          = {10.1007/s11263-023-01759-0},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1269-1283},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Virtual restoration of ancient wooden ships through non-rigid 3D shape assembly with ruled-surface FFD},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifting 2D human pose to 3D with domain adapted 3D body
concept. <em>IJCV</em>, <em>131</em>(5), 1250–1268. (<a
href="https://doi.org/10.1007/s11263-023-01749-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifting the 2D human pose to the 3D pose is an important yet challenging task. Existing 3D human pose estimation suffers from (1) the inherent ambiguity between the 2D and 3D data, and (2) the lack of well-labeled 2D–3D pose pairs in the wild. Human beings are able to imagine the 3D human pose from a 2D image or a set of 2D body key-points with the least ambiguity, which should be attributed to the prior knowledge of the human body that we have acquired in our mind. Inspired by this, we propose a new framework that leverages the labeled 3D human poses to learn a 3D concept of the human body to reduce ambiguity. To have consensus on the body concept from the 2D pose, our key insight is to treat the 2D human pose and the 3D human pose as two different domains. By adapting the two domains, the body knowledge learned from 3D poses is applied to 2D poses and guides the 2D pose encoder to generate informative 3D “imagination” as an embedding in pose lifting. Benefiting from the domain adaptation perspective, the proposed framework unifies the supervised and semi-supervised 3D pose estimation in a principled framework. Extensive experiments demonstrate that the proposed approach can achieve state-of-the-art performance on standard benchmarks. More importantly, it is validated that the explicitly learned 3D body concept effectively alleviates the 2D–3D ambiguity, improves the generalization, and enables the network to leverage the abundant unlabeled 2D data.},
  archive      = {J_IJCV},
  author       = {Nie, Qiang and Liu, Ziwei and Liu, Yunhui},
  doi          = {10.1007/s11263-023-01749-2},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1250-1268},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lifting 2D human pose to 3D with domain adapted 3D body concept},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DDPNAS: Efficient neural architecture search via dynamic
distribution pruning. <em>IJCV</em>, <em>131</em>(5), 1234–1249. (<a
href="https://doi.org/10.1007/s11263-023-01753-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has demonstrated state-of-the-art performance on various computer vision tasks. Despite the superior performance achieved, the efficiency and generality of existing methods are highly valued due to their high computational complexity and low generality. In this paper, we propose an efficient and unified NAS framework termed DDPNAS via dynamic distribution pruning, facilitating a theoretical bound on accuracy and efficiency. In particular, we first sample architectures from a joint categorical distribution. Then the search space is dynamically pruned and its distribution is updated every few epochs. With the proposed efficient network generation method, we directly obtain the optimal neural architectures on given constraints, which is practical for on-device models across diverse search spaces and constraints. The architectures searched by our method achieve remarkable top-1 accuracies, 97.56 and 77.2 on CIFAR-10 and ImageNet (mobile settings), respectively, with the fastest search process, i.e., only 1.8 GPU hours on a Tesla V100. Codes for searching and network generation are available at: https://openi.pcl.ac.cn/PCL_AutoML/XNAS.},
  archive      = {J_IJCV},
  author       = {Zheng, Xiawu and Yang, Chenyi and Zhang, Shaokun and Wang, Yan and Zhang, Baochang and Wu, Yongjian and Wu, Yunsheng and Shao, Ling and Ji, Rongrong},
  doi          = {10.1007/s11263-023-01753-6},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1234-1249},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DDPNAS: Efficient neural architecture search via dynamic distribution pruning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta attention-generation network for cross-granularity
few-shot learning. <em>IJCV</em>, <em>131</em>(5), 1211–1233. (<a
href="https://doi.org/10.1007/s11263-023-01760-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained classification with few labeled samples has urgent needs in practice since fine-grained samples are more difficult and expensive to collect and annotate. Standard few-shot learning (FSL) focuses on generalising across seen and unseen classes, where the classes are at the same level of granularity. Therefore, when applying existing FSL methods to tackle this problem, large amounts of labeled samples for some fine-grained classes are required. Since samples of coarse-grained classes are much cheaper and easier to obtain, it is desired to learn knowledge from coarse-grained categories that can be transferred to fine-grained classes with a few samples. In this paper, we propose a novel learning problem called cross-granularity few-shot learning (CG-FSL), where sufficient samples of coarse-grained classes are available for training, but in the test stage, the goal is to classify the fine-grained subclasses. This learning paradigm follows the laws of cognitive neurology. We first give an analysis of CG-FSL through the Structural Causal Model (SCM) and figure out that the standard FSL model learned at the coarse-grained level is actually a confounder. We thus perform backdoor adjustment to decouple the interferences and consequently derive a causal CG-FSL model called Meta Attention-Generation Network (MAGN), which is trained in a bilevel optimization manner. We construct benchmarks from several fine-grained image datasets for the CG-FSL problem and empirically show that our model significantly outperforms standard FSL methods and baseline CG-FSL methods.},
  archive      = {J_IJCV},
  author       = {Qiang, Wenwen and Li, Jiangmeng and Su, Bing and Fu, Jianlong and Xiong, Hui and Wen, Ji-Rong},
  doi          = {10.1007/s11263-023-01760-7},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1211-1233},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Meta attention-generation network for cross-granularity few-shot learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface to the special issue on pattern recognition (DAGM
GCPR 2021). <em>IJCV</em>, <em>131</em>(5), 1210. (<a
href="https://doi.org/10.1007/s11263-023-01757-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Bauckhage, Christian and Förstner, Wolfgang and Gall, Juergen and Möller, Michael and Schwing, Alexander},
  doi          = {10.1007/s11263-023-01757-2},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1210},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Preface to the special issue on pattern recognition (DAGM GCPR 2021)},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic shape analysis of surfaces with second-order sobolev
metrics: A comprehensive numerical framework. <em>IJCV</em>,
<em>131</em>(5), 1183–1209. (<a
href="https://doi.org/10.1007/s11263-022-01743-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a set of numerical methods for Riemannian shape analysis of 3D surfaces within the setting of invariant (elastic) second-order Sobolev metrics. More specifically, we address the computation of geodesics and geodesic distances between parametrized or unparametrized immersed surfaces represented as 3D meshes. Building on this, we develop tools for the statistical shape analysis of sets of surfaces, including methods for estimating Karcher means and performing tangent PCA on shape populations, and for computing parallel transport along paths of surfaces. Our proposed approach fundamentally relies on a relaxed variational formulation for the geodesic matching problem via the use of varifold fidelity terms, which enable us to enforce reparametrization independence when computing geodesics between unparametrized surfaces, while also yielding versatile algorithms that allow us to compare surfaces with varying sampling or mesh structures. Importantly, we demonstrate how our relaxed variational framework can be extended to tackle partially observed data. The different benefits of our numerical pipeline are illustrated over various examples, synthetic and real.},
  archive      = {J_IJCV},
  author       = {Hartman, Emmanuel and Sukurdeep, Yashil and Klassen, Eric and Charon, Nicolas and Bauer, Martin},
  doi          = {10.1007/s11263-022-01743-0},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1183-1209},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Elastic shape analysis of surfaces with second-order sobolev metrics: A comprehensive numerical framework},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Editor’s note: Special issue on 3D computer vision.
<em>IJCV</em>, <em>131</em>(5), 1182. (<a
href="https://doi.org/10.1007/s11263-023-01751-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-023-01751-8},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1182},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on 3D computer vision},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic curriculum learning for great ape detection in the
wild. <em>IJCV</em>, <em>131</em>(5), 1163–1181. (<a
href="https://doi.org/10.1007/s11263-023-01748-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel end-to-end curriculum learning approach for sparsely labelled animal datasets leveraging large volumes of unlabelled data to improve supervised species detectors. We exemplify the method in detail on the task of finding great apes in camera trap footage taken in challenging real-world jungle environments. In contrast to previous semi-supervised methods, our approach adjusts learning parameters dynamically over time and gradually improves detection quality by steering training towards virtuous self-reinforcement. To achieve this, we propose integrating pseudo-labelling with curriculum learning policies and show how learning collapse can be avoided. We discuss theoretical arguments, ablations, and significant performance improvements against various state-of-the-art systems when evaluating on the Extended PanAfrican Dataset holding approx. 1.8M frames. We also demonstrate our method can outperform supervised baselines with significant margins on sparse label versions of other animal datasets such as Bees and Snapshot Serengeti. We note that performance advantages are strongest for smaller labelled ratios common in ecological applications. Finally, we show that our approach achieves competitive benchmarks for generic object detection in MS-COCO and PASCAL-VOC indicating wider applicability of the dynamic learning concepts introduced. We publish all relevant source code, network weights, and data access details for full reproducibility.},
  archive      = {J_IJCV},
  author       = {Yang, Xinyu and Burghardt, Tilo and Mirmehdi, Majid},
  doi          = {10.1007/s11263-023-01748-3},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1163-1181},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dynamic curriculum learning for great ape detection in the wild},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ViTAEv2: Vision transformer advanced by exploring inductive
bias for image recognition and beyond. <em>IJCV</em>, <em>131</em>(5),
1141–1162. (<a
href="https://doi.org/10.1007/s11263-022-01739-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we leverage the two IBs and propose the ViTAE transformer, which utilizes a reduction cell for multi-scale feature and a normal cell for locality. The two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline and representative models. Besides, we scale up our ViTAE model to 644 M parameters and obtain the state-of-the-art classification performance, i.e., 88.5\% Top-1 classification accuracy on ImageNet validation set and the best 91.2\% Top-1 classification accuracy on ImageNet Real validation set, without using extra private data. It demonstrates that the introduced inductive bias still helps when the model size becomes large. The source code and pretrained models are publicly available atcode.},
  archive      = {J_IJCV},
  author       = {Zhang, Qiming and Xu, Yufei and Zhang, Jing and Tao, Dacheng},
  doi          = {10.1007/s11263-022-01739-w},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1141-1162},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ViTAEv2: Vision transformer advanced by exploring inductive bias for image recognition and beyond},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expression-preserving face frontalization improves visually
assisted speech processing. <em>IJCV</em>, <em>131</em>(5), 1122–1140.
(<a href="https://doi.org/10.1007/s11263-022-01742-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face frontalization consists of synthesizing a frontal view from a profile one. This paper proposes a frontalization method that preserves non-rigid facial deformations, i.e. facial expressions. It is shown that expression-preserving frontalization boosts the performance of visually assisted speech processing. The method alternates between the estimation of (i) the rigid transformation (scale, rotation, and translation) and (ii) the non-rigid deformation between an arbitrarily-viewed face and a face model. The method has two important merits: it can deal with non-Gaussian errors in the data and it incorporates a dynamical face deformation model. For that purpose, we use the Student’s t-distribution in combination with a Bayesian filter in order to account for both rigid head motions and time-varying facial deformations, e.g. caused by speech production. The zero-mean normalized cross-correlation score is used to evaluate the ability of the method to preserve facial expressions. The method is thoroughly evaluated and compared with several state of the art methods, either based on traditional geometric models or on deep learning. Moreover, we show that the method, when incorporated into speech processing pipelines, improves word recognition rates and speech intelligibility scores by a considerable margin.},
  archive      = {J_IJCV},
  author       = {Kang, Zhiqi and Sadeghi, Mostafa and Horaud, Radu and Alameda-Pineda, Xavier},
  doi          = {10.1007/s11263-022-01742-1},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1122-1140},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Expression-preserving face frontalization improves visually assisted speech processing},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relating view directions of complementary-view mobile
cameras via the human shadow. <em>IJCV</em>, <em>131</em>(5), 1106–1121.
(<a href="https://doi.org/10.1007/s11263-022-01744-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential of video surveillance can be further explored by using mobile cameras. Drone-mounted cameras at a high altitude can provide top views of a scene from a global perspective while cameras worn by people on the ground can provide first-person views of the same scene with more local details. To relate these two views for collaborative analysis, we propose to localize the field of view of the first-person-view cameras in the global top view. This is a very challenging problem due to their large view differences and indeterminate camera motions. In this work, we explore the use of sunlight direction as a bridge to relate the two views. Specifically, we design a shadow-direction-aware network to simultaneously locate the shadow vanishing point in the first-person view as well as the shadow direction in the top view. Then we apply multi-view geometry to estimate the yaw and pitch angles of the first-person-view camera in the top view. We build a new synthetic dataset consisting of top-view and first-person-view image pairs for performance evaluation. Quantitative results on this synthetic dataset show the superiority of our method compared with the existing methods, which achieve the view angle estimation errors of 1.61 $$^{\circ }$$ (pitch angle) and 15.13 $$^{\circ }$$ (yaw angle), respectively. The qualitative results on real images also show the effectiveness of the proposed method.},
  archive      = {J_IJCV},
  author       = {Han, Ruize and Gan, Yiyang and Wang, Likai and Li, Nan and Feng, Wei and Wang, Song},
  doi          = {10.1007/s11263-022-01744-z},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1106-1121},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Relating view directions of complementary-view mobile cameras via the human shadow},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deeply explain CNN via hierarchical decomposition.
<em>IJCV</em>, <em>131</em>(5), 1091–1105. (<a
href="https://doi.org/10.1007/s11263-022-01746-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, some attribution methods for explaining CNNs attempt to study how the intermediate features affect network prediction. However, they usually ignore the feature hierarchies among the intermediate features. This paper introduces a hierarchical decomposition framework to explain CNN’s decision-making process in a top-down manner. Specifically, we propose a gradient-based activation propagation (gAP) module that can decompose any intermediate CNN decision to its lower layers and find the supporting features. Then we utilize the gAP module to iteratively decompose the network decision to the supporting evidence from different CNN layers. The proposed framework can generate a deep hierarchy of strongly associated supporting evidence for the network decision, which provides insight into the decision-making process. Moreover, gAP is effort-free for understanding CNN-based models without network architecture modification and extra training processes. Experiments show the effectiveness of the proposed method. The data and source code will be publicly available at https://mmcheng.net/hdecomp/ .},
  archive      = {J_IJCV},
  author       = {Cheng, Ming-Ming and Jiang, Peng-Tao and Han, Ling-Hao and Wang, Liang and Torr, Philip},
  doi          = {10.1007/s11263-022-01746-x},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1091-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deeply explain CNN via hierarchical decomposition},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentimental visual captioning using multimodal transformer.
<em>IJCV</em>, <em>131</em>(4), 1073–1090. (<a
href="https://doi.org/10.1007/s11263-023-01752-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new task called sentimental visual captioning that generates captions with the inherent sentiment reflected by the input image or video. Compared with the stylized visual captioning task that requires a predefined style independent of the image or video, our new task automatically analyzes the inherent sentiment tendency from the visual content. With this in mind, we propose a multimodal Transformer model namely Senti-Transformer for sentimental visual captioning, which integrates both content and sentiment information from multiple modalities and incorporates prior sentimental knowledge to generate sentimental sentence. Specifically, we extract prior knowledge from sentimental corpus to obtain sentimental textual information and design a multi-head Transformer encoder to encode multimodal features. Then we decompose the attention layer in the middle of Transformer decoder to focus on important features of each modality, and the attended features are integrated through an intra- and inter-modality fusion mechanism for generating sentimental sentences. To effectively train the proposed model using the external sentimental corpus as well as the paired images or videos and factual sentences in existing captioning datasets, we propose a two-stage training strategy that first learns to incorporate sentimental elements into the sentences via a regularization term and then learns to generate fluent and relevant sentences with the inherent sentimental styles via reinforcement learning with a sentimental reward. Extensive experiments on both image and video datasets demonstrate the effectiveness and superiority of our Senti-Transformer on sentimental visual captioning. Source code is available at https://github.com/ezeli/InSentiCap_ext .},
  archive      = {J_IJCV},
  author       = {Wu, Xinxiao and Li, Tong},
  doi          = {10.1007/s11263-023-01752-7},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1073-1090},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Sentimental visual captioning using multimodal transformer},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Spatio-temporal outdoor lighting aggregation on image
sequences using transformer networks. <em>IJCV</em>, <em>131</em>(4),
1060–1072. (<a
href="https://doi.org/10.1007/s11263-022-01725-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we focus on outdoor lighting estimation by aggregating individual noisy estimates from images, exploiting the rich image information from wide-angle cameras and/or temporal image sequences. Photographs inherently encode information about the lighting of the scene in the form of shading and shadows. Recovering the lighting is an inverse rendering problem and as that ill-posed. Recent research based on deep neural networks has shown promising results for estimating light from a single image, but with shortcomings in robustness. We tackle this problem by combining lighting estimates from several image views sampled in the angular and temporal domains of an image sequence. For this task, we introduce a transformer architecture that is trained in an end-2-end fashion without any statistical post-processing as required by previous work. Thereby, we propose a positional encoding that takes into account camera alignment and ego-motion estimation to globally register the individual estimates when computing attention between visual words. We show that our method leads to improved lighting estimation while requiring fewer hyperparameters compared to the state of the art.},
  archive      = {J_IJCV},
  author       = {Lee, Haebom and Homeyer, Christian and Herzog, Robert and Rexilius, Jan and Rother, Carsten},
  doi          = {10.1007/s11263-022-01725-2},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1060-1072},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatio-temporal outdoor lighting aggregation on image sequences using transformer networks},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient recurrent adversarial framework for
unsupervised real-time video enhancement. <em>IJCV</em>,
<em>131</em>(4), 1042–1059. (<a
href="https://doi.org/10.1007/s11263-022-01735-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video enhancement is a challenging problem, more than that of stills, mainly due to high computational cost, larger data volumes and the difficulty of achieving consistency in the spatio-temporal domain. In practice, these challenges are often coupled with the lack of example pairs, which inhibits the application of supervised learning strategies. To address these challenges, we propose an efficient adversarial video enhancement framework that learns directly from unpaired video examples. In particular, our framework introduces new recurrent cells that consist of interleaved local and global modules for implicit integration of spatial and temporal information. The proposed design allows our recurrent cells to efficiently propagate spatio-temporal information across frames and reduces the need for high complexity networks. Our setting enables learning from unpaired videos in a cyclic adversarial manner, where the proposed recurrent units are employed in all architectures. Efficient training is accomplished by introducing one single discriminator that learns the joint distribution of source and target domain simultaneously. The enhancement results demonstrate clear superiority of the proposed video enhancer over the state-of-the-art methods, in all terms of visual quality, quantitative metrics, and inference speed. Notably, our video enhancer is capable of enhancing over 35 frames per second of FullHD video (1080x1920).},
  archive      = {J_IJCV},
  author       = {Fuoli, Dario and Huang, Zhiwu and Paudel, Danda Pani and Van Gool, Luc and Timofte, Radu},
  doi          = {10.1007/s11263-022-01735-0},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1042-1059},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An efficient recurrent adversarial framework for unsupervised real-time video enhancement},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning to adapt to light. <em>IJCV</em>, <em>131</em>(4),
1022–1041. (<a
href="https://doi.org/10.1007/s11263-022-01745-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light adaptation or brightness correction is a key step in improving the contrast and visual appeal of an image. There are multiple light-related tasks (for example, low-light enhancement and exposure correction) and previous studies have mainly investigated these tasks individually. It is interesting to consider whether the common light adaptation sub-problem in these light-related tasks can be executed by a unified model, especially considering that our visual system adapts to external light in such way. In this study, we propose a biologically inspired method to handle light-related image enhancement tasks with a unified network (called LA-Net). First, we proposed a new goal-oriented task decomposition perspective to solve general image enhancement problems, and specifically decouple light adaptation from multiple light-related tasks with frequency-based decomposition. Then, a unified module is built inspired by biological visual adaptation to achieve light adaptation in the low-frequency pathway. Combined with the proper noise suppression and detail enhancement along the high-frequency pathway, the proposed network performs unified light adaptation across various scenes. Extensive experiments on three tasks—low-light enhancement, exposure correction, and tone mapping—demonstrate that the proposed method obtains reasonable performance simultaneously for all of these three tasks compared with recent methods designed for these individual tasks. Our code is made publicly available at https://github.com/kaifuyang/LA-Net .},
  archive      = {J_IJCV},
  author       = {Yang, Kai-Fu and Cheng, Cheng and Zhao, Shi-Xuan and Yan, Hong-Mei and Zhang, Xian-Shi and Li, Yong-Jie},
  doi          = {10.1007/s11263-022-01745-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1022-1041},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to adapt to light},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Free-view face relighting using a hybrid parametric neural
model on a SMALL-OLAT dataset. <em>IJCV</em>, <em>131</em>(4),
1002–1021. (<a
href="https://doi.org/10.1007/s11263-022-01730-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of neural relighting techniques has by far outpaced the rate of their corresponding training data (e.g., OLAT) generation. For example, high-quality relighting from a single portrait image still requires supervision from comprehensive datasets covering broad diversities in gender, race, complexion, and facial geometry. We present a hybrid parametric neural relighting (PN-Relighting) framework for single portrait relighting, using a much smaller OLAT dataset or SMOLAT. At the core of PN-Relighting, we employ parametric 3D faces coupled with appearance inference and implicit material modelling to enrich SMOLAT for handling in-the-wild images. Specifically, we tailor an appearance inference module to generate detailed geometry and albedo on top of the parametric face and develop a neural rendering module to first construct an implicit material representation from SMOLAT and then conduct self-supervised training on in-the-wild image datasets. Comprehensive experiments show that PN-Relighting produces comparable high-quality relighting to TotalRelighting (Pandey et al., 2021), but with a smaller dataset. It further improves shape estimation and naturally supports free-viewpoint rendering and partial skin material editing. PN-Relighting also serves as a data augmenter to produce rich OLAT datasets beyond the original capture.},
  archive      = {J_IJCV},
  author       = {Wang, Youjia and He, Kai and Zhou, Taotao and Yao, Kaixin and Li, Nianyi and Xu, Lan and Yu, Jingyi},
  doi          = {10.1007/s11263-022-01730-5},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1002-1021},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Free-view face relighting using a hybrid parametric neural model on a SMALL-OLAT dataset},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised and long-tailed object detection with
CascadeMatch. <em>IJCV</em>, <em>131</em>(4), 987–1001. (<a
href="https://doi.org/10.1007/s11263-022-01738-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on long-tailed object detection in the semi-supervised learning setting, which poses realistic challenges, but has rarely been studied in the literature. We propose a novel pseudo-labeling-based detector called CascadeMatch. Our detector features a cascade network architecture, which has multi-stage detection heads with progressive confidence thresholds. To avoid manually tuning the thresholds, we design a new adaptive pseudo-label mining mechanism to automatically identify suitable values from data . To mitigate confirmation bias, where a model is negatively reinforced by incorrect pseudo-labels produced by itself, each detection head is trained by the ensemble pseudo-labels of all detection heads. Experiments on two long-tailed datasets, i.e., LVIS and COCO-LT, demonstrate that CascadeMatch surpasses existing state-of-the-art semi-supervised approaches—across a wide range of detection architectures—in handling long-tailed object detection. For instance, CascadeMatch outperforms Unbiased Teacher by 1.9 $$\hbox {AP}^{{\text {Fix}}}$$ on LVIS when using a ResNet50-based Cascade R-CNN structure, and by 1.7 $$\hbox {AP}^{{\text {Fix}}}$$ when using Sparse R-CNN with a Transformer encoder. We also show that CascadeMatch can even handle the challenging sparsely annotated object detection problem. Code: https://github.com/yuhangzang/CascadeMatch .},
  archive      = {J_IJCV},
  author       = {Zang, Yuhang and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
  doi          = {10.1007/s11263-022-01738-x},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {987-1001},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semi-supervised and long-tailed object detection with CascadeMatch},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth descent synchronization in <span
class="math display"> SO (<em>D</em>)</span>. <em>IJCV</em>,
<em>131</em>(4), 968–986. (<a
href="https://doi.org/10.1007/s11263-022-01686-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give robust recovery results for synchronization on the rotation group, $${{\,\mathrm{\text {SO}}\,}}(D)$$ . In particular, we consider an adversarial corruption setting, where a limited percentage of the observations are arbitrarily corrupted. We develop a novel algorithm that exploits Tukey depth in the tangent space of $${{\,\mathrm{\text {SO}}\,}}(D)$$ . This algorithm, called Depth Descent Synchronization, exactly recovers the underlying rotations up to an outlier percentage of $$1/(D(D-1)+2)$$ , which corresponds to 1/4 for $${{\,\mathrm{\text {SO}}\,}}(2)$$ and 1/8 for $${{\,\mathrm{\text {SO}}\,}}(3)$$ . In the case of $${{\,\mathrm{\text {SO}}\,}}(2)$$ , we demonstrate that a variant of this algorithm converges linearly to the ground truth rotations. We implement this algorithm for the case of $${{\,\mathrm{\text {SO}}\,}}(3)$$ and demonstrate that it performs competitively on baseline synthetic data.},
  archive      = {J_IJCV},
  author       = {Maunu, Tyler and Lerman, Gilad},
  doi          = {10.1007/s11263-022-01686-6},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {968-986},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Depth descent synchronization in $${{\,\mathrm{\text {SO}}\,}}(D)$$},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RM3D: Robust data-efficient 3D scene parsing via traditional
and learnt 3D descriptors-based semantic region merging. <em>IJCV</em>,
<em>131</em>(4), 938–967. (<a
href="https://doi.org/10.1007/s11263-022-01740-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing state-of-the-art 3D point clouds understanding methods merely perform well in a fully supervised manner. To the best of our knowledge, there exists no unified framework which simultaneously solves the downstream high-level understanding tasks including both segmentation and detection, especially when labels are extremely limited. This work presents a general and simple framework to tackle point clouds understanding when labels are limited. The first contribution is that we have done extensive methodology comparisons of traditional and learnt 3D descriptors for the task of weakly supervised 3D scene understanding, and validated that our adapted traditional PFH-based 3D descriptors show excellent generalization ability across different domains. The second contribution is that we proposed a learning-based region merging strategy based on the affinity provided by both the traditional/learnt 3D descriptors and learnt semantics. The merging process takes both low-level geometric and high-level semantic feature correlations into consideration. Experimental results demonstrate that our framework has the best performance among the three most important weakly supervised point clouds understanding tasks including semantic segmentation, instance segmentation, and object detection even when very limited number of points are labeled. Our method, termed Region Merging 3D (RM3D), has superior performance on ScanNet data-efficient learning online benchmarks and other four large-scale 3D understanding benchmarks under various experimental settings, outperforming current arts by a margin for various 3D understanding tasks without complicated learning strategies such as active learning.},
  archive      = {J_IJCV},
  author       = {Liu, Kangcheng},
  doi          = {10.1007/s11263-022-01740-3},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {938-967},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RM3D: Robust data-efficient 3D scene parsing via traditional and learnt 3D descriptors-based semantic region merging},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SHARP: Shape-aware reconstruction of people in loose
clothing. <em>IJCV</em>, <em>131</em>(4), 918–937. (<a
href="https://doi.org/10.1007/s11263-022-01736-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have enabled 3D human body reconstruction from a monocular image, which has broad applications in multiple domains. In this paper, we propose SHARP (SHape Aware Reconstruction of People in loose clothing), a novel end-to-end trainable network that accurately recovers the 3D geometry and appearance of humans in loose clothing from a monocular image. SHARP uses a sparse and efficient fusion strategy to combine parametric body prior with a non-parametric 2D representation of clothed humans. The parametric body prior enforces geometrical consistency on the body shape and pose, while the non-parametric representation models loose clothing and handles self-occlusions as well. We also leverage the sparseness of the non-parametric representation for faster training of our network while using losses on 2D maps. Another key contribution is 3DHumans, our new life-like dataset of 3D human body scans with rich geometrical and textural details. We evaluate SHARP on 3DHumans and other publicly available datasets, and show superior qualitative and quantitative performance than existing state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Jinka, Sai Sagar and Srivastava, Astitva and Pokhariya, Chandradeep and Sharma, Avinash and Narayanan, P. J.},
  doi          = {10.1007/s11263-022-01736-z},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {918-937},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SHARP: Shape-aware reconstruction of people in loose clothing},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WATB: Wild animal tracking benchmark. <em>IJCV</em>,
<em>131</em>(4), 899–917. (<a
href="https://doi.org/10.1007/s11263-022-01732-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of computer vision technology, many advanced computer vision methods have been successfully applied to animal detection, tracking, recognition and behavior analysis, which is of great help to ecological protection, biodiversity conservation and environmental protection. As existing datasets applied to target tracking contain various kinds of common objects, but rarely focus on wild animals, this paper proposes the first benchmark, named Wild Animal Tracking Benchmark (WATB), to encourage further progress of research and applications of visual object tracking. WATB contains more than 203,000 frames and 206 video sequences, and covers different kinds of animals from land, sea and sky. The average length of the videos is over 980 frames. Each video is manually labelled with thirteen challenge attributes including illumination variation, rotation, deformation, and so on. In the dataset, all frames are annotated with axis-aligned bounding boxes. To reveal the performance of these existing tracking algorithms and provide baseline results for future research on wild animal tracking, we benchmark a total of 38 state-of-the-art trackers and rank them according to tracking accuracy. Evaluation results demonstrate that the trackers based on deep networks perform much better than other trackers like correlation filters. Another finding on the basis of the evaluation results is that wild animals tracking is still a big challenge in computer vision community. The benchmark WATB and evaluation results are released on the project website https://w-1995.github.io/ .},
  archive      = {J_IJCV},
  author       = {Wang, Fasheng and Cao, Ping and Li, Fu and Wang, Xing and He, Bing and Sun, Fuming},
  doi          = {10.1007/s11263-022-01732-3},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {899-917},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WATB: Wild animal tracking benchmark},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theory and closed-form solutions for three- and n-layer flat
refractive geometry. <em>IJCV</em>, <em>131</em>(4), 877–898. (<a
href="https://doi.org/10.1007/s11263-022-01729-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop in this paper a new formulation for the calibration and pose estimation of three-layer flat-refractive geometry. We then extend it into the general n-layer case. We show that contrary to state-of-the-art theory, our new forms yield stable solutions under the presence of high levels of noise while improving the accuracy estimates by two orders of magnitude or more. Our closed-form expressions facilitate a direct and linear solution of the pose parameters and need no a priori knowledge. In developing our forms, we provide new insights into the nature of such systems, specifically showing that the effect of refraction on the system is captured by a single surface replacing all layers. By characterizing this surface form, we solve the n-layer problem using a fixed set of parameters, irrespective of the number of layers. Finally, we identify a new configuration in which the camera obtains central-perspective properties. Such configuration contrasts the axial nature of this system and can simplify subsequent processing. Our analyses demonstrate that reaching the levels of accuracies we record requires only simple means, e.g., a single image of a planar target. Hence, further to advancing the theory of flat-refractive geometry, we also provide a viable framework for its application.},
  archive      = {J_IJCV},
  author       = {Elnashef, Bashar and Filin, Sagi},
  doi          = {10.1007/s11263-022-01729-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {877-898},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Theory and closed-form solutions for three- and n-layer flat refractive geometry},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delving into calibrated depth for accurate RGB-d salient
object detection. <em>IJCV</em>, <em>131</em>(4), 855–876. (<a
href="https://doi.org/10.1007/s11263-022-01734-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed growing interests in RGB-D Salient Object Detection (SOD), benefiting from the ample spatial layout cues embedded in depth maps to help SOD models distinguish salient objects from complex backgrounds or similar surroundings. Despite these progresses, this emerging line of research has been considerably hindered by the noise and ambiguity that prevail in raw depth images, as well as the coarse object boundaries in saliency predictions. To address the aforementioned issues, we propose a Depth Calibration and Boundary-aware Fusion (DCBF) framework that contains two novel components: (1) a learning strategy to calibrate the latent bias in the original depth maps towards boosting the SOD performance; (2) a boundary-aware multimodal fusion module to fuse the complementary cues from RGB and depth channels, as well as to improve object boundary qualities. In addition, we introduce a new saliency dataset, HiBo-UA, which contains 1515 high-resolution RGB-D images with finely-annotated pixel-level labels. To our best knowledge, this is the first RGB-D-based high-resolution saliency dataset with significantly higher image resolution (nearly 7 $$\times $$ ) than the widely used DUT-D dataset. The proposed high-resolution dataset with richer object boundary details is capable of accurately assessing the performance of various saliency models, in order to retain fine-grained object boundaries. It also facilitates the growing need of our research community in accessing higher-resolution data. Extensive empirical experiments demonstrate the superior performance of our approach against 31 state-of-the-art methods. It is worth noting that our calibrated depth alone can work in a plug-and-play manner; empirically it is shown to bring noticeable improvements when applied to existing state-of-the-art RGB-D SOD models.},
  archive      = {J_IJCV},
  author       = {Li, Jingjing and Ji, Wei and Zhang, Miao and Piao, Yongri and Lu, Huchuan and Cheng, Li},
  doi          = {10.1007/s11263-022-01734-1},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {855-876},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Delving into calibrated depth for accurate RGB-D salient object detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). U-turn: Crafting adversarial queries with opposite-direction
features. <em>IJCV</em>, <em>131</em>(4), 835–854. (<a
href="https://doi.org/10.1007/s11263-022-01737-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to craft adversarial queries for image retrieval, which uses image features for similarity measurement. Many commonly used methods are developed in the context of image classification. However, these methods, which attack prediction probabilities, only exert an indirect influence on the image features and are thus found less effective when being applied to the retrieval problem. In designing an attack method specifically for image retrieval, we introduce opposite-direction feature attack (ODFA), a white-box attack approach that directly attacks query image features to generate adversarial queries. As the name implies, the main idea underpinning ODFA is to impel the original image feature to the opposite direction, similar to a U-turn. This simple idea is experimentally evaluated on five retrieval datasets. We show that the adversarial queries generated by ODFA cause true matches no longer to be seen at the top ranks, and the attack success rate is consistently higher than classifier attack methods. In addition, our method of creating adversarial queries can be extended for multi-scale query inputs and is generalizable to other retrieval models without foreknowing their weights, i.e., the black-box setting.},
  archive      = {J_IJCV},
  author       = {Zheng, Zhedong and Zheng, Liang and Yang, Yi and Wu, Fei},
  doi          = {10.1007/s11263-022-01737-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {835-854},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {U-turn: Crafting adversarial queries with opposite-direction features},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overcoming the domain gap in neural action representations.
<em>IJCV</em>, <em>131</em>(3), 813–833. (<a
href="https://doi.org/10.1007/s11263-022-01713-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relating behavior to brain activity in animals is a fundamental goal in neuroscience, with practical applications in building robust brain-machine interfaces. However, the domain gap between individuals is a major issue that prevents the training of general models that work on unlabeled subjects. Since 3D pose data can now be reliably extracted from multi-view video sequences without manual intervention, we propose to use it to guide the encoding of neural action representations together with a set of neural and behavioral augmentations exploiting the properties of microscopy imaging. To test our method, we collect a large dataset that features flies and their neural activity. To reduce the domain gap, during training, we mix features of neural and behavioral data across flies that seem to be performing similar actions. To show our method can generalize further neural modalities and other downstream tasks, we test our method on a human neural Electrocorticography dataset, and another RGB video data of human activities from different viewpoints. We believe our work will enable more robust neural decoding algorithms to be used in future brain-machine interfaces.},
  archive      = {J_IJCV},
  author       = {Günel, Semih and Aymanns, Florian and Honari, Sina and Ramdya, Pavan and Fua, Pascal},
  doi          = {10.1007/s11263-022-01713-6},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {813-833},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Overcoming the domain gap in neural action representations},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Project to adapt: Domain adaptation for depth completion
from noisy and sparse sensor data. <em>IJCV</em>, <em>131</em>(3),
796–812. (<a href="https://doi.org/10.1007/s11263-022-01726-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to predict a dense depth map from a sparse depth input. The acquisition of dense ground-truth annotations for depth completion settings can be difficult and, at the same time, a significant domain gap between real LiDAR measurements and synthetic data has prevented from successful training of models in virtual settings. We propose a domain adaptation approach for sparse-to-dense depth completion that is trained from synthetic data, without annotations in the real domain or additional sensors. Our approach simulates the real sensor noise in an RGB + LiDAR set-up, and consists of three modules: simulating the real LiDAR input in the synthetic domain via projections, filtering the real noisy LiDAR for supervision and adapting the synthetic RGB image using a CycleGAN approach. We extensively evaluate these modules in the KITTI depth completion benchmark.},
  archive      = {J_IJCV},
  author       = {Lopez-Rodriguez, Adrian and Busam, Benjamin and Mikolajczyk, Krystian},
  doi          = {10.1007/s11263-022-01726-1},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {796-812},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Project to adapt: Domain adaptation for depth completion from noisy and sparse sensor data},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A closer look at few-shot 3D point cloud classification.
<em>IJCV</em>, <em>131</em>(3), 772–795. (<a
href="https://doi.org/10.1007/s11263-022-01731-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on few-shot learning (FSL) has been fast-growing in the 2D image domain due to the less requirement for labeled training data and greater generalization for novel classes. However, its application in 3D point cloud data is relatively under-explored. Not only need to distinguish unseen classes as in the 2D domain, 3D FSL is more challenging in terms of irregular structures, subtle inter-class differences, and high intra-class variances when trained on a low number of data. Moreover, different architectures and learning algorithms make it difficult to study the effectiveness of existing 2D FSL algorithms when migrating to the 3D domain. In this work, for the first time, we perform systematic and extensive investigations of directly applying recent 2D FSL works to 3D point cloud related backbone networks and thus suggest a strong learning baseline for few-shot 3D point cloud classification. Furthermore, we propose a new network, Point-cloud Correlation Interaction (PCIA), with three novel plug-and-play components called Salient-Part Fusion (SPF) module, Self-Channel Interaction Plus (SCI+) module, and Cross-Instance Fusion Plus (CIF+) module to obtain more representative embeddings and improve the feature distinction. These modules can be inserted into most FSL algorithms with minor changes and significantly improve the performance. Experimental results on three benchmark datasets, ModelNet40-FS, ShapeNet70-FS, and ScanObjectNN-FS, demonstrate that our method achieves state-of-the-art performance for the 3D FSL task.},
  archive      = {J_IJCV},
  author       = {Ye, Chuangguan and Zhu, Hongyuan and Zhang, Bo and Chen, Tao},
  doi          = {10.1007/s11263-022-01731-4},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {772-795},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A closer look at few-shot 3D point cloud classification},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DESC: Domain adaptation for depth estimation via semantic
consistency. <em>IJCV</em>, <em>131</em>(3), 752–771. (<a
href="https://doi.org/10.1007/s11263-022-01718-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate real depth annotations are difficult to acquire, needing the use of special devices such as a LiDAR sensor. Self-supervised methods try to overcome this problem by processing video or stereo sequences, which may not always be available. Instead, in this paper, we propose a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset. We bridge the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. We enforce consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduce priors in the form of instance heights. Our approach is evaluated on standard domain adaptation benchmarks for monocular depth estimation and show consistent improvement upon the state-of-the-art. Code available at https://github.com/alopezgit/DESC .},
  archive      = {J_IJCV},
  author       = {Lopez-Rodriguez, Adrian and Mikolajczyk, Krystian},
  doi          = {10.1007/s11263-022-01718-1},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {752-771},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DESC: Domain adaptation for depth estimation via semantic consistency},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shuffled linear regression with outliers in both covariates
and responses. <em>IJCV</em>, <em>131</em>(3), 732–751. (<a
href="https://doi.org/10.1007/s11263-022-01709-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a shuffled linear regression problem. As a variant of ordinary linear regression, it requires estimating not only the regression variable, but also permutational correspondences between the covariates and responses. While existing formulations require the underlying ground-truth correspondences to be an ideal bijection such that all pieces of data should match, such a requirement barely holds in real-world applications due to either missing data or outliers. In this work, we generalize the formulation of shuffled linear regression to a broader range of conditions where only a part of the data should correspond. To this end, the effective recovery condition and NP-hardness of the proposed formulation are also studied. Moreover, we present a simple yet effective algorithm for deriving the solution. Its global convergence property and convergence rate are also analyzed in detail. Distinct tasks validate the effectiveness of our proposed formulation and the solution method.},
  archive      = {J_IJCV},
  author       = {Li, Feiran and Fujiwara, Kent and Okura, Fumio and Matsushita, Yasuyuki},
  doi          = {10.1007/s11263-022-01709-2},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {732-751},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Shuffled linear regression with outliers in both covariates and responses},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video region annotation with sparse bounding boxes.
<em>IJCV</em>, <em>131</em>(3), 717–731. (<a
href="https://doi.org/10.1007/s11263-022-01719-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analysis has been moving towards more detailed interpretation (e.g., segmentation) with encouraging progress. These tasks, however, increasingly rely on densely annotated training data both in space and time. Since such annotation is labor-intensive, few densely annotated video data with detailed region boundaries exist. This work aims to resolve this dilemma by learning to automatically generate region boundaries for all frames of a video from sparsely annotated bounding boxes of target regions. We achieve this with a Volumetric Graph Convolutional Network (VGCN), which learns to iteratively find keypoints on the region boundaries using the spatio-temporal volume of surrounding appearance and motion. We show that the global optimization of VGCN leads to more accurate annotation that generalizes better. Experimental results using three latest datasets (two real and one synthetic), including ablation studies, demonstrate the effectiveness and superiority of our method.},
  archive      = {J_IJCV},
  author       = {Xu, Yuzheng and Wu, Yang and binti Zuraimi, Nur Sabrina and Nobuhara, Shohei and Nishino, Ko},
  doi          = {10.1007/s11263-022-01719-0},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {717-731},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Video region annotation with sparse bounding boxes},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SegMix: Co-occurrence driven mixup for semantic segmentation
and adversarial robustness. <em>IJCV</em>, <em>131</em>(3), 701–716. (<a
href="https://doi.org/10.1007/s11263-022-01720-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. In this work, this is accomplished for the task of dense image labelling by blending images based on (i) categorical clustering or (ii) the co-occurrence likelihood of categories. We then train a source separation network which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation and salient object detection network while simultaneously increasing robustness to adversarial attacks.},
  archive      = {J_IJCV},
  author       = {Islam, Md Amirul and Kowal, Matthew and Derpanis, Konstantinos G. and Bruce, Neil D. B.},
  doi          = {10.1007/s11263-022-01720-7},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {701-716},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SegMix: Co-occurrence driven mixup for semantic segmentation and adversarial robustness},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-adversarial faster-RCNN with paradigm teacher for
unrestricted object detection. <em>IJCV</em>, <em>131</em>(3), 680–700.
(<a href="https://doi.org/10.1007/s11263-022-01728-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the cross-domain object detection task has been raised by reducing the domain disparity and learning domain invariant features. Inspired by the image-level discrepancy dominated in object detection, we introduce a Multi-Adversarial Faster-RCNN (MAF). Our proposed MAF has two distinct contributions: (1) The Hierarchical Domain Feature Alignment (HDFA) module is introduced to minimize the image-level domain disparity, where Scale Reduction Module (SRM) reduces the feature map size without information loss and increases the training efficiency. (2) Aggregated Proposal Feature Alignment (APFA) module integrates the proposal feature and the detection results to enhance the semantic alignment, in which a weighted GRL (WGRL) layer highlights the hard-confused features rather than the easily-confused features. However, MAF only considers the domain disparity and neglects domain adaptability. As a result, the label-agnostic and inaccurate target distribution leads to the source error collapse, which is harmful to domain adaptation. Therefore, we further propose a Paradigm Teacher (PT) with knowledge distillation and formulated an extensive Paradigm Teacher MAF (PT-MAF), which has two new contributions: (1) The Paradigm Teacher (PT) overcomes source error collapse to improve the adaptability of the model. (2) The Dual-Discriminator HDFA (D $$^{2}$$ -HDFA) improves the marginal distribution and achieves better alignment compared to HDFA. Extensive experiments on numerous benchmark datasets, including the Cityscapes, Foggy Cityscapes, Pascal VOC, Clipart, Watercolor, etc. demonstrate the superiority of our approach over SOTA methods.},
  archive      = {J_IJCV},
  author       = {He, Zhenwei and Zhang, Lei and Gao, Xinbo and Zhang, David},
  doi          = {10.1007/s11263-022-01728-z},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {680-700},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-adversarial faster-RCNN with paradigm teacher for unrestricted object detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic modelling for interactive action assessment.
<em>IJCV</em>, <em>131</em>(3), 659–679. (<a
href="https://doi.org/10.1007/s11263-022-01695-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action assessment, the task of visually assessing the quality of performing an action, has attracted much attention in recent years, with promising applications in areas such as medical treatment and sporting events. However, most existing methods of action assessment mainly target the actions performed by a single person; in particular, they neglect the asymmetric relations among agents (e.g., between persons and objects), limiting their performance in many nonindividual actions. In this work, we formulate a framework for modelling asymmetric interactions among agents for action assessment, considering the subordinations among agents in many interactive actions. Specifically, we propose an asymmetric interaction learner consisting of an automatic assigner and an asymmetric interaction network search module. The automatic assigner is designed to automatically group agents within an action into a primary agent (e.g., human) and secondary agents (e.g., objects); the asymmetric interaction network search module adaptively learns the asymmetric interactions between these agents. We conduct experiments on the JIGSAWS dataset containing surgical actions and additionally collect two new datasets, TASD-2 and PaSk, for action assessment on interactive sporting actions. The experimental results on these three datasets demonstrate the effectiveness of our framework in achieving state-of-the-art performance. The extensive experiments on the AQA-7 dataset also indicate the robustness of our model in conventional action assessment settings.},
  archive      = {J_IJCV},
  author       = {Gao, Jibin and Pan, Jia-Hui and Zhang, Shao-Jie and Zheng, Wei-Shi},
  doi          = {10.1007/s11263-022-01695-5},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {659-679},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Automatic modelling for interactive action assessment},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bipartite graph reasoning GANs for person pose and facial
image synthesis. <em>IJCV</em>, <em>131</em>(3), 644–658. (<a
href="https://doi.org/10.1007/s11263-022-01722-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel bipartite graph reasoning Generative Adversarial Network (BiGraphGAN) for two challenging tasks: person pose and facial image synthesis. The proposed graph generator consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed bipartite graph reasoning (BGR) block aims to reason the long-range cross relations between the source and target pose in a bipartite graph, which mitigates some of the challenges caused by pose deformation. Moreover, we propose a new interaction-and-aggregation (IA) block to effectively update and enhance the feature representation capability of both a person’s shape and appearance in an interactive way. To further capture the change in pose of each part more precisely, we propose a novel part-aware bipartite graph reasoning (PBGR) block to decompose the task of reasoning the global structure transformation with a bipartite graph into learning different local transformations for different semantic body/face parts. Experiments on two challenging generation tasks with three public datasets demonstrate the effectiveness of the proposed methods in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN .},
  archive      = {J_IJCV},
  author       = {Tang, Hao and Shao, Ling and Torr, Philip H. S. and Sebe, Nicu},
  doi          = {10.1007/s11263-022-01722-5},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {644-658},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bipartite graph reasoning GANs for person pose and facial image synthesis},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting consistency regularization for semi-supervised
learning. <em>IJCV</em>, <em>131</em>(3), 626–643. (<a
href="https://doi.org/10.1007/s11263-022-01723-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art across a variety of standard semi-supervised learning benchmarks as well as imbalanced semi-supervised learning benchmarks. Particularly, we outperform previous work by a significant margin in low data regimes and at large imbalance ratios. Extensive experiments are conducted to analyze the method, and the code will be published.},
  archive      = {J_IJCV},
  author       = {Fan, Yue and Kukleva, Anna and Dai, Dengxin and Schiele, Bernt},
  doi          = {10.1007/s11263-022-01723-4},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {626-643},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting consistency regularization for semi-supervised learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active perception for visual-language navigation.
<em>IJCV</em>, <em>131</em>(3), 607–625. (<a
href="https://doi.org/10.1007/s11263-022-01721-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to take navigation actions at every step. In contrast, when humans face such a challenge, we can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make a more confident navigation decision. This work draws inspiration from human navigation behavior and endows an agent with an active perception ability for more intelligent navigation. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides (i) when and where to explore, (ii) what information is worth gathering during exploration, and (iii) how to adjust the navigation decision after the exploration. In this way, the agent is able to turn its past experiences as well as new explored knowledge to contexts for more confident navigation decision making. In addition, an external memory is used to explicitly store the visited visual environments and thus allows the agent to adopt a late action-taking strategy to avoid duplicate exploration and navigation movements. Our experimental results on two standard benchmark datasets show promising exploration strategies emerged from training, which leads to significant boost in navigation performance.},
  archive      = {J_IJCV},
  author       = {Wang, Hanqing and Wang, Wenguan and Liang, Wei and Hoi, Steven C. H. and Shen, Jianbing and Gool, Luc Van},
  doi          = {10.1007/s11263-022-01721-6},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {607-625},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Active perception for visual-language navigation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Spatial monitoring and insect behavioural analysis using
computer vision for precision pollination. <em>IJCV</em>,
<em>131</em>(3), 591–606. (<a
href="https://doi.org/10.1007/s11263-022-01715-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insects are the most important global pollinator of crops and play a key role in maintaining the sustainability of natural ecosystems. Insect pollination monitoring and management are therefore essential for improving crop production and food security. Computer vision facilitated pollinator monitoring can intensify data collection over what is feasible using manual approaches. The new data it generates may provide a detailed understanding of insect distributions and facilitate fine-grained analysis sufficient to predict their pollination efficacy and underpin precision pollination. Current computer vision facilitated insect tracking in complex outdoor environments is restricted in spatial coverage and often constrained to a single insect species. This limits its relevance to agriculture. Therefore, in this article we introduce a novel system to facilitate markerless data capture for insect counting, insect motion tracking, behaviour analysis and pollination prediction across large agricultural areas. Our system is comprised of edge computing multi-point video recording, offline automated multi-species insect counting, tracking and behavioural analysis. We implement and test our system on a commercial berry farm to demonstrate its capabilities. Our system successfully tracked four insect varieties, at nine monitoring stations within polytunnels, obtaining an F-score above 0.8 for each variety. The system enabled calculation of key metrics to assess the relative pollination impact of each insect variety. With this technological advancement, detailed, ongoing data collection for precision pollination becomes achievable. This is important to inform growers and apiarists managing crop pollination, as it allows data-driven decisions to be made to improve food production and food security.},
  archive      = {J_IJCV},
  author       = {Ratnayake, Malika Nisal and Amarathunga, Don Chathurika and Zaman, Asaduz and Dyer, Adrian G. and Dorin, Alan},
  doi          = {10.1007/s11263-022-01715-4},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {591-606},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatial monitoring and insect behavioural analysis using computer vision for precision pollination},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Going deeper than tracking: A survey of computer-vision
based recognition of animal pain and emotions. <em>IJCV</em>,
<em>131</em>(2), 572–590. (<a
href="https://doi.org/10.1007/s11263-022-01716-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in animal motion tracking and pose recognition have been a game changer in the study of animal behavior. Recently, an increasing number of works go ‘deeper’ than tracking, and address automated recognition of animals’ internal states such as emotions and pain with the aim of improving animal welfare, making this a timely moment for a systematization of the field. This paper provides a comprehensive survey of computer vision-based research on recognition of pain and emotional states in animals, addressing both facial and bodily behavior analysis. We summarize the efforts that have been presented so far within this topic—classifying them across different dimensions, highlight challenges and research gaps, and provide best practice recommendations for advancing the field, and some future directions for research.},
  archive      = {J_IJCV},
  author       = {Broomé, Sofia and Feighelstein, Marcelo and Zamansky, Anna and Carreira Lencioni, Gabriel and Haubro Andersen, Pia and Pessanha, Francisca and Mahmoud, Marwa and Kjellström, Hedvig and Salah, Albert Ali},
  doi          = {10.1007/s11263-022-01716-3},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {572-590},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Going deeper than tracking: A survey of computer-vision based recognition of animal pain and emotions},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific bias filtering for single labeled domain
generalization. <em>IJCV</em>, <em>131</em>(2), 552–571. (<a
href="https://doi.org/10.1007/s11263-022-01712-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional Domain Generalization (CDG) utilizes multiple labeled source datasets to train a generalizable model for unseen target domains. However, due to expensive annotation costs, the requirements of labeling all the source data are hard to be met in real-world applications. In this paper, we investigate a Single Labeled Domain Generalization (SLDG) task with only one source domain being labeled, which is more practical and challenging than the CDG task. A major obstacle in the SLDG task is the discriminability-generalization bias: the discriminative information in the labeled source dataset may contain domain-specific bias, constraining the generalization of the trained model. To tackle this challenging task, we propose a novel framework called Domain-Specific Bias Filtering (DSBF), which initializes a discriminative model with the labeled source data and then filters out its domain-specific bias with the unlabeled source data for generalization improvement. We divide the filtering process into (1) feature extractor debiasing via k-means clustering-based semantic feature re-extraction and (2) classifier rectification through attention-guided semantic feature projection. DSBF unifies the exploration of the labeled and the unlabeled source data to enhance the discriminability and generalization of the trained model, resulting in a highly generalizable model. We further provide theoretical analysis to verify the proposed domain-specific bias filtering process. Extensive experiments on multiple datasets show the superior performance of DSBF in tackling both the challenging SLDG task and the CDG task.},
  archive      = {J_IJCV},
  author       = {Yuan, Junkun and Ma, Xu and Chen, Defang and Kuang, Kun and Wu, Fei and Lin, Lanfen},
  doi          = {10.1007/s11263-022-01712-7},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {552-571},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Domain-specific bias filtering for single labeled domain generalization},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PV-RCNN++: Point-voxel feature set abstraction with local
vector representation for 3D object detection. <em>IJCV</em>,
<em>131</em>(2), 531–551. (<a
href="https://doi.org/10.1007/s11263-022-01710-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection on point clouds. First, we propose a novel 3D detector, PV-RCNN, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e., the voxel-to-keypoint scene encoding and the keypoint-to-grid RoI feature abstraction. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is about $$3\times $$ faster than PV-RCNN, while also achieving better performance. The experiments demonstrate that our proposed PV-RCNN++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 FPS inference speed on the detection range of $$150m \times 150m$$ .},
  archive      = {J_IJCV},
  author       = {Shi, Shaoshuai and Jiang, Li and Deng, Jiajun and Wang, Zhe and Guo, Chaoxu and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  doi          = {10.1007/s11263-022-01710-9},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {531-551},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PV-RCNN++: Point-voxel feature set abstraction with local vector representation for 3D object detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Animal pose tracking: 3D multimodal dataset and token-based
pose optimization. <em>IJCV</em>, <em>131</em>(2), 514–530. (<a
href="https://doi.org/10.1007/s11263-022-01714-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tracking of the 3D pose of animals from video recordings is critical for many behavioral studies, yet there is a dearth of publicly available datasets that the computer vision community could use for model development. We here introduce the Rodent3D dataset that records animals exploring their environment and/or interacting with each other with multiple cameras and modalities (RGB, depth, thermal infrared). Rodent3D consists of 200 min of multimodal video recordings from up to three thermal and three RGB-D synchronized cameras (approximately 4 million frames). For the task of optimizing estimates of pose sequences provided by existing pose estimation methods, we provide a baseline model called OptiPose. While deep-learned attention mechanisms have been used for pose estimation in the past, with OptiPose, we propose a different way by representing 3D poses as tokens for which deep-learned context models pay attention to both spatial and temporal keypoint patterns. Our experiments show how OptiPose is highly robust to noise and occlusion and can be used to optimize pose sequences provided by state-of-the-art models for animal pose estimation.},
  archive      = {J_IJCV},
  author       = {Patel, Mahir and Gu, Yiwen and Carstensen, Lucas C. and Hasselmo, Michael E. and Betke, Margrit},
  doi          = {10.1007/s11263-022-01714-5},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {514-530},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Animal pose tracking: 3D multimodal dataset and token-based pose optimization},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AnimalTrack: A benchmark for multi-animal tracking in the
wild. <em>IJCV</em>, <em>131</em>(2), 496–513. (<a
href="https://doi.org/10.1007/s11263-022-01711-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-animal tracking (MAT), a multi-object tracking (MOT) problem, is crucial for animal motion and behavior analysis and has many crucial applications such as biology, ecology and animal conservation. Despite its importance, MAT is largely under-explored compared to other MOT problems such as multi-human tracking due to the scarcity of dedicated benchmarks. To address this problem, we introduce AnimalTrack, a dedicated benchmark for multi-animal tracking in the wild. Specifically, AnimalTrack consists of 58 sequences from a diverse selection of 10 common animal categories. On average, each sequence comprises of 33 target objects for tracking. In order to ensure high quality, every frame in AnimalTrack is manually labeled with careful inspection and refinement. To our best knowledge, AnimalTrack is the first benchmark dedicated to multi-animal tracking. In addition, to understand how existing MOT algorithms perform on AnimalTrack and provide baselines for future comparison, we extensively evaluate 14 state-of-the-art representative trackers. The evaluation results demonstrate that, not surprisingly, most of these trackers become degenerated due to the differences between pedestrians and animals in various aspects (e.g., pose, motion, and appearance), and more efforts are desired to improve multi-animal tracking. We hope that AnimalTrack together with evaluation and analysis will foster further progress on multi-animal tracking. The dataset and evaluation as well as our analysis will be made available upon the acceptance.},
  archive      = {J_IJCV},
  author       = {Zhang, Libo and Gao, Junyuan and Xiao, Zhen and Fan, Heng},
  doi          = {10.1007/s11263-022-01711-8},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {496-513},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AnimalTrack: A benchmark for multi-animal tracking in the wild},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent graph neural networks for video instance
segmentation. <em>IJCV</em>, <em>131</em>(2), 471–495. (<a
href="https://doi.org/10.1007/s11263-022-01703-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video instance segmentation is one of the core problems in computer vision. Formulating a purely learning-based method, which models the generic track management required to solve the video instance segmentation task, is a highly challenging problem. In this work, we propose a novel learning framework where the entire video instance segmentation problem is modeled jointly. To this end, we design a graph neural network that in each frame jointly processes all detections and a memory of previously seen tracks. Past information is considered and processed via a recurrent connection. We demonstrate the effectiveness of the proposed approach in comprehensive experiments. Our approach operates online at over 25 FPS and obtains 16.3 AP on the challenging OVIS benchmark, setting a new state-of-the-art. We further conduct detailed ablative experiments that validate the different aspects of our approach. Code is available at https://github.com/emibr948/RGNNVIS-PlusPlus .},
  archive      = {J_IJCV},
  author       = {Brissman, Emil and Johnander, Joakim and Danelljan, Martin and Felsberg, Michael},
  doi          = {10.1007/s11263-022-01703-8},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {471-495},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Recurrent graph neural networks for video instance segmentation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-guided multi-patch network with self-supervision for
non-uniform motion deblurring. <em>IJCV</em>, <em>131</em>(2), 453–470.
(<a href="https://doi.org/10.1007/s11263-022-01708-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary deep learning multi-scale deblurring models suffer from many issues: (I) They perform poorly on non-uniformly blurred images/videos; (II) Simply increasing the model depth with finer-scale levels cannot improve deblurring; (III) Individual RGB frames contain a limited motion information for deblurring; (IV) Previous models have a limited robustness to spatial transformations and noise. Below, we propose several mechanisms based on the multi-patch network to address the above issues: (I) We present a novel self-supervised event-guided deep hierarchical Multi-patch Network (MPN) to deal with blurry images and videos via fine-to-coarse hierarchical localized representations; (II) We propose a novel stacked pipeline, StackMPN, to improve the deblurring performance under the increased network depth; (III) We propose an event-guided architecture to exploit motion cues contained in videos to tackle complex blur in videos; (IV) We propose a novel self-supervised step to expose the model to random transformations (rotations, scale changes), and make it robust to Gaussian noises. Our MPN achieves the state of the art on the GoPro and VideoDeblur datasets with a 40 $$\times $$ faster runtime compared to current multi-scale methods. With 30 ms to process an image at 1280 $$\times $$ 720 resolution, it is the first real-time deep motion deblurring model for 720p images at 30 fps. For StackMPN, we obtain significant improvements over 1.2 dB on the GoPro dataset by increasing the network depth. Utilizing the event information and self-supervision further boost results to 33.83 dB.},
  archive      = {J_IJCV},
  author       = {Zhang, Hongguang and Zhang, Limeng and Dai, Yuchao and Li, Hongdong and Koniusz, Piotr},
  doi          = {10.1007/s11263-022-01708-3},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {453-470},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Event-guided multi-patch network with self-supervision for non-uniform motion deblurring},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient model for a camera behind a parallel refractive
slab. <em>IJCV</em>, <em>131</em>(2), 431–452. (<a
href="https://doi.org/10.1007/s11263-022-01691-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new and efficient solution for the forward world to image projection of a pinhole camera with distortions placed behind a planar refractive slab. Firstly, we introduce a novel way to compute the projection by reducing the problem to finding a real quantity called a slab shift. We characterize a physically meaningful slab shift as the unique solution of a fixed point equation on the one hand and as a specific uniquely defined root of a quartic polynomial in the unknown slab shift on the other. In the latter case we obtain a closed-form formula, which provides the unique physically meaningful projection. Secondly, we develop an approximation of the projection through the slab that reaches single-precision floating point accuracy for practically relevant problem instances with considerably lower computational costs compared to the exact solution. We demonstrate the accuracy and the efficiency of our method with realistic synthetic experiments. We demonstrate with real experiments that our method enables efficient camera calibration behind the windshield for automotive industry applications.},
  archive      = {J_IJCV},
  author       = {Lasaruk, Aless and Pajdla, Tomas},
  doi          = {10.1007/s11263-022-01691-9},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {431-452},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An efficient model for a camera behind a parallel refractive slab},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robots understanding contextual information in
human-centered environments using weakly supervised mask data
distillation. <em>IJCV</em>, <em>131</em>(2), 407–430. (<a
href="https://doi.org/10.1007/s11263-022-01706-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information contained within human environments, such as text on signs, symbols and objects provide important information for robots to use for exploration and navigation. To identify and segment contextual information from images obtained in these environments data-driven methods such as Convolutional Neural Networks (CNNs) can be used. However, these methods require significant amounts of human labeled data which is time-consuming to obtain. In this paper, we present the novel Weakly Supervised Mask Data Distillation (WeSuperMaDD) architecture for autonomously generating pseudo segmentation labels (PSLs) using CNNs not specifically trained for the task of text segmentation, e.g., CNNs alternatively trained for: object classification or image captioning. WeSuperMaDD is uniquely able to generate PSLs using learned image features from datasets that are sparse and with limited diversity, which are common in robot navigation tasks in human-centred environments (i.e., malls, stores). Our proposed architecture uses a new mask refinement system which automatically searches for the PSL with the fewest foreground pixels that satisfies cost constraints. This removes the need for handcrafted heuristic rules. Extensive experiments were conducted to validate the performance of WeSuperMaDD in generating PSLs for datasets containing text of various scales, fonts, orientations, curvatures, and perspectives in several indoor/outdoor environments. A detailed comparison study conducted with existing approaches found a significant improvement in PSL quality. Furthermore, an instance segmentation CNN trained using the WeSuperMaDD architecture achieved measurable improvements in accuracy when compared to an instance segmentation CNN trained with Naïve PSLs. We also found our method to have comparable performance to existing text detection methods.},
  archive      = {J_IJCV},
  author       = {Dworakowski, Daniel and Fung, Angus and Nejat, Goldie},
  doi          = {10.1007/s11263-022-01706-5},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {407-430},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robots understanding contextual information in human-centered environments using weakly supervised mask data distillation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on advances in computer
vision and applications (ACCV 2020). <em>IJCV</em>, <em>131</em>(2),
406. (<a href="https://doi.org/10.1007/s11263-022-01717-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
  doi          = {10.1007/s11263-022-01717-2},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {406},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on advances in computer vision and applications (ACCV 2020)},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on computer vision from 2D to
3D. <em>IJCV</em>, <em>131</em>(2), 405. (<a
href="https://doi.org/10.1007/s11263-022-01724-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Smith, William},
  doi          = {10.1007/s11263-022-01724-3},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {405},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on computer vision from 2D to 3D},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning with complex-valued neural networks and
dependable learning. <em>IJCV</em>, <em>131</em>(1), 385–404. (<a
href="https://doi.org/10.1007/s11263-022-01700-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a flexible, general framework for few-shot learning where both inter-class differences and intra-class relationships are fully considered to improve recognition performance significantly. We introduce complex-valued convolutional neural networks (CNNs) to describe the subtle difference among inter-class samples and Dependable Learning to capture the intra-class relationship. Conventional CNNs use only real-valued CNNs and fail to extract more detailed information. Complex-valued CNNs, on the other hand, can provide amplitude and phase information to enhance the feature representation ability based on the proposed complex metric module (CMM). Building upon the recent episodic training mechanism, CMMs can improve the representation capacity by extracting robust complex-valued features to facilitate the modeling of subtle relationships among few-shot samples. Furthermore, we use Dependable Learning as a new learning paradigm, to promote a robust model against perturbation based on a new bilinear optimization to enhance the feature extraction capacity for very few available intra-class samples. Experiments on two benchmark datasets show that the proposed methods significantly improve the performance over other approaches and achieve state-of-the-art results.},
  archive      = {J_IJCV},
  author       = {Wang, Runqi and Liu, Zhen and Zhang, Baochang and Guo, Guodong and Doermann, David},
  doi          = {10.1007/s11263-022-01700-x},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {385-404},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Few-shot learning with complex-valued neural networks and dependable learning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LIP: Local importance-based pooling. <em>IJCV</em>,
<em>131</em>(1), 363–384. (<a
href="https://doi.org/10.1007/s11263-022-01707-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for visual recognition tasks, these layers might lose discriminative details due to improper pooling strategies. In this paper, we present a unified framework (LAN) over the common downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a view of local aggregation based on importance. In this LAN framework, we analyze the issues of these widely-used pooling layers and figure out the criteria of designing an effective downsampling layer. Based on this analysis, we propose a simple, general, and effective pooling operation based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP is able to enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. To further modulate different pooling windows for more effective pooling, we present the improved version of LIP, termed LIP++, by introducing an explicit margin term and efficient logit modules. Our LIP++ can yield consistent accuracy improvement over the original LIP yet with a smaller computational cost. Extensive experiments show that our presented LIP method consistently yields notable gains with different CNN architectures on the image classification task. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent performance improvement over the vanilla ResNets on both bounding box detection and instance segmentation. Finally, we also verify the effectiveness of LIP on the tasks of pose estimation and semantic segmentation, demonstrating its generalization to the dense prediction task.},
  archive      = {J_IJCV},
  author       = {Gao, Ziteng and Wang, Limin and Wu, Gangshan},
  doi          = {10.1007/s11263-022-01707-4},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {363-384},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LIP: Local importance-based pooling},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cutting-plane method for sublabel-accurate relaxation of
problems with product label spaces. <em>IJCV</em>, <em>131</em>(1),
346–362. (<a href="https://doi.org/10.1007/s11263-022-01704-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems in imaging and low-level vision can be formulated as nonconvex variational problems. A promising class of approaches to tackle such problems are convex relaxation methods, which consider a lifting of the energy functional to a higher-dimensional space. However, they come with increased memory requirements due to the lifting. The present paper is an extended version of the earlier conference paper by Ye et al. (in: DAGM German conference on pattern recognition (GCPR), 2021) which combined two recent approaches to make lifting more scalable: product-space relaxation and sublabel-accurate discretization. Furthermore, it is shown that a simple cutting-plane method can be used to solve the resulting semi-infinite optimization problem. This journal version extends the previous conference work with additional experiments, a more detailed outline of the complete algorithm and a user-friendly introduction to functional lifting methods.},
  archive      = {J_IJCV},
  author       = {Ye, Zhenzhang and Haefner, Bjoern and Quéau, Yvain and Möllenhoff, Thomas and Cremers, Daniel},
  doi          = {10.1007/s11263-022-01704-7},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {346-362},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A cutting-plane method for sublabel-accurate relaxation of problems with product label spaces},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimal solvers for relative pose estimation of multi-camera
systems using affine correspondences. <em>IJCV</em>, <em>131</em>(1),
324–345. (<a href="https://doi.org/10.1007/s11263-022-01690-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose three novel solvers for estimating the relative pose of a multi-camera system from affine correspondences (ACs). A new constraint is derived interpreting the relationship of ACs and the generalized camera model. Using the constraint, we demonstrate efficient solvers for two types of motions. Considering that the cameras undergo planar motion, we propose a minimal solution using a single AC and a solver with two ACs to overcome the degenerate case. Also, we propose a minimal solution using two ACs (a minimal number of one AC and one point correspondence) with known vertical direction, e.g., from an IMU. Since the proposed methods require significantly fewer correspondences than state-of-the-art algorithms, they can be efficiently used within RANSAC for outlier removal and initial motion estimation. The solvers are tested both on synthetic data and on three real-world scenes. It is shown that the accuracy of the estimated poses is superior to the state-of-the-art techniques. Source code is released at https://github.com/jizhaox/relative_pose_gcam_affine .},
  archive      = {J_IJCV},
  author       = {Guan, Banglei and Zhao, Ji and Barath, Daniel and Fraundorfer, Friedrich},
  doi          = {10.1007/s11263-022-01690-w},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {324-345},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Minimal solvers for relative pose estimation of multi-camera systems using affine correspondences},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AOE-net: Entities interactions modeling with adaptive
attention mechanism for temporal action proposals generation.
<em>IJCV</em>, <em>131</em>(1), 302–323. (<a
href="https://doi.org/10.1007/s11263-022-01702-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet $$-$$ 1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous state-of-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Our source code is publicly available at https://github.com/UARK-AICV/AOE-Net .},
  archive      = {J_IJCV},
  author       = {Vo, Khoa and Truong, Sang and Yamazaki, Kashu and Raj, Bhiksha and Tran, Minh-Triet and Le, Ngan},
  doi          = {10.1007/s11263-022-01702-9},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {302-323},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AOE-net: Entities interactions modeling with adaptive attention mechanism for temporal action proposals generation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-world video deblurring: A benchmark dataset and an
efficient recurrent neural network. <em>IJCV</em>, <em>131</em>(1),
284–301. (<a href="https://doi.org/10.1007/s11263-022-01705-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world video deblurring in real time still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue that needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https://github.com/zzh-tech/ESTRNN .},
  archive      = {J_IJCV},
  author       = {Zhong, Zhihang and Gao, Ye and Zheng, Yinqiang and Zheng, Bo and Sato, Imari},
  doi          = {10.1007/s11263-022-01705-6},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {284-301},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Real-world video deblurring: A benchmark dataset and an efficient recurrent neural network},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual object tracking in first person vision.
<em>IJCV</em>, <em>131</em>(1), 259–283. (<a
href="https://doi.org/10.1007/s11263-022-01694-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The understanding of human-object interactions is fundamental in First Person Vision (FPV). Visual tracking algorithms which follow the objects manipulated by the camera wearer can provide useful information to effectively model such interactions. In the last years, the computer vision community has significantly improved the performance of tracking algorithms for a large variety of target objects and scenarios. Despite a few previous attempts to exploit trackers in the FPV domain, a methodical analysis of the performance of state-of-the-art trackers is still missing. This research gap raises the question of whether current solutions can be used “off-the-shelf” or more domain-specific investigations should be carried out. This paper aims to provide answers to such questions. We present the first systematic investigation of single object tracking in FPV. Our study extensively analyses the performance of 42 algorithms including generic object trackers and baseline FPV-specific trackers. The analysis is carried out by focusing on different aspects of the FPV setting, introducing new performance measures, and in relation to FPV-specific tasks. The study is made possible through the introduction of TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV poses new challenges to current visual trackers. We highlight the factors causing such behavior and point out possible research directions. Despite their difficulties, we prove that trackers bring benefits to FPV downstream tasks requiring short-term object tracking. We expect that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated.},
  archive      = {J_IJCV},
  author       = {Dunnhofer, Matteo and Furnari, Antonino and Farinella, Giovanni Maria and Micheloni, Christian},
  doi          = {10.1007/s11263-022-01694-6},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {259-283},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visual object tracking in first person vision},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OpenMonkeyChallenge: Dataset and benchmark challenges for
pose estimation of non-human primates. <em>IJCV</em>, <em>131</em>(1),
243–258. (<a href="https://doi.org/10.1007/s11263-022-01698-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to automatically estimate the pose of non-human primates as they move through the world is important for several subfields in biology and biomedicine. Inspired by the recent success of computer vision models enabled by benchmark challenges (e.g., object detection), we propose a new benchmark challenge called OpenMonkeyChallenge that facilitates collective community efforts through an annual competition to build generalizable non-human primate pose estimation models. To host the benchmark challenge, we provide a new public dataset consisting of 111,529 annotated (17 body landmarks) photographs of non-human primates in naturalistic contexts obtained from various sources including the Internet, three National Primate Research Centers, and the Minnesota Zoo. Such annotated datasets will be used for the training and testing datasets to develop generalizable models with standardized evaluation metrics. We demonstrate the effectiveness of our dataset quantitatively by comparing it with existing datasets based on seven state-of-the-art pose estimation models.},
  archive      = {J_IJCV},
  author       = {Yao, Yuan and Bala, Praneet and Mohan, Abhiraj and Bliss-Moreau, Eliza and Coleman, Kristine and Freeman, Sienna M. and Machado, Christopher J. and Raper, Jessica and Zimmermann, Jan and Hayden, Benjamin Y. and Park, Hyun Soo},
  doi          = {10.1007/s11263-022-01698-2},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {243-258},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OpenMonkeyChallenge: Dataset and benchmark challenges for pose estimation of non-human primates},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-augmented deep unfolding network for guided image
super-resolution. <em>IJCV</em>, <em>131</em>(1), 215–242. (<a
href="https://doi.org/10.1007/s11263-022-01699-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided image super-resolution (GISR) aims to obtain a high-resolution (HR) target image by enhancing the spatial resolution of a low-resolution (LR) target image under the guidance of a HR image. However, previous model-based methods mainly take the entire image as a whole, and assume the prior distribution between the HR target image and the HR guidance image, simply ignoring many non-local common characteristics between them. To alleviate this issue, we firstly propose a maximum a posteriori (MAP) estimation model for GISR with two types of priors on the HR target image, i.e., local implicit prior and global implicit prior. The local implicit prior aims to model the complex relationship between the HR target image and the HR guidance image from a local perspective, and the global implicit prior considers the non-local auto-regression property between the two images from a global perspective. Secondly, we design a novel alternating optimization algorithm to solve this model for GISR. The algorithm is in a concise framework that facilitates to be replicated into commonly used deep network structures. Thirdly, to reduce the information loss across iterative stages, the persistent memory mechanism is introduced to augment the information representation by exploiting the Long short-term memory unit (LSTM) in the image and feature spaces. In this way, a deep network with certain interpretation and high representation ability is built. Extensive experimental results validate the superiority of our method on a variety of GISR tasks, including Pan-sharpening, depth image super-resolution, and MR image super-resolution. Code will be released at https://github.com/manman1995/pansharpening .},
  archive      = {J_IJCV},
  author       = {Zhou, Man and Yan, Keyu and Pan, Jinshan and Ren, Wenqi and Xie, Qi and Cao, Xiangyong},
  doi          = {10.1007/s11263-022-01699-1},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {215-242},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Memory-augmented deep unfolding network for guided image super-resolution},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vis-MVSNet: Visibility-aware multi-view stereo network.
<em>IJCV</em>, <em>131</em>(1), 199–214. (<a
href="https://doi.org/10.1007/s11263-022-01697-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet significantly improves depth accuracy in reconstruction scenes with severe occlusion. Extensive experiments are performed on DTU, BlendedMVS, Tanks and Temples and ETH3D datasets to justify the effectiveness of the proposed framework.},
  archive      = {J_IJCV},
  author       = {Zhang, Jingyang and Li, Shiwei and Luo, Zixin and Fang, Tian and Yao, Yao},
  doi          = {10.1007/s11263-022-01697-3},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {199-214},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vis-MVSNet: Visibility-aware multi-view stereo network},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance based image classification: A solution to
generative classification’s conundrum? <em>IJCV</em>, <em>131</em>(1),
177–198. (<a href="https://doi.org/10.1007/s11263-022-01675-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most classifiers rely on discriminative boundaries that separate instances of each class from everything else. We argue that discriminative boundaries are counter-intuitive as they define semantics by what-they-are-not; and should be replaced by generative classifiers which define semantics by what-they-are. Unfortunately, generative classifiers are significantly less accurate. This may be caused by the tendency of generative models to focus on easy to model semantic generative factors and ignore non-semantic factors that are important but difficult to model. We propose a new generative model in which semantic factors are accommodated by shell theory’s Wen-Yan et al. (IEEE Trans Pattern Anal Mach Intell, 2021) hierarchical generative process and non-semantic factors by an instance specific noise term. We use the model to develop a classification scheme which suppresses the impact of noise while preserving semantic cues. The result is a surprisingly accurate generative classifier, that takes the form of a modified nearest-neighbor algorithm; we term it distance classification. Unlike discriminative classifiers, a distance classifier: defines semantics by what-they-are; is amenable to incremental updates; and scales well with the number of classes.},
  archive      = {J_IJCV},
  author       = {Lin, Wen-Yan and Liu, Siying and Dai, Bing Tian and Li, Hongdong},
  doi          = {10.1007/s11263-022-01675-9},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {177-198},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Distance based image classification: A solution to generative classification’s conundrum?},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimally distorted structured adversarial attacks.
<em>IJCV</em>, <em>131</em>(1), 160–176. (<a
href="https://doi.org/10.1007/s11263-022-01701-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White box adversarial perturbations are generated via iterative optimization algorithms most often by minimizing an adversarial loss on a $$\ell _p$$ neighborhood of the original image, the so-called distortion set. Constraining the adversarial search with different norms results in disparately structured adversarial examples. Here we explore several distortion sets with structure-enhancing algorithms. These new structures for adversarial examples might provide challenges for provable and empirical robust mechanisms. Because adversarial robustness is still an empirical field, defense mechanisms should also reasonably be evaluated against differently structured attacks. Besides, these structured adversarial perturbations may allow for larger distortions size than their $$\ell _p$$ counterpart while remaining imperceptible or perceptible as natural distortions of the image. We will demonstrate in this work that the proposed structured adversarial examples can significantly bring down the classification accuracy of adversarially trained classifiers while showing a low $$\ell _2$$ distortion rate. For instance, on ImagNet dataset the structured attacks drop the accuracy of the adversarial model to near zero with only 50\% of $$\ell _2$$ distortion generated using white-box attacks like PGD. As a byproduct, our findings on structured adversarial examples can be used for adversarial regularization of models to make models more robust or improve their generalization performance on datasets that are structurally different.},
  archive      = {J_IJCV},
  author       = {Kazemi, Ehsan and Kerdreux, Thomas and Wang, Liqiang},
  doi          = {10.1007/s11263-022-01701-w},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {160-176},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Minimally distorted structured adversarial attacks},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Invertible rescaling network and its extensions.
<em>IJCV</em>, <em>131</em>(1), 134–159. (<a
href="https://doi.org/10.1007/s11263-022-01688-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image rescaling is a commonly used bidirectional operation, which first downscales high-resolution images to fit various display screens or to be storage- and bandwidth-friendly, and afterward upscales the corresponding low-resolution images to recover the original resolution or the details in the zoom-in images. However, the non-injective downscaling mapping discards high-frequency contents, leading to the ill-posed problem for the inverse restoration task. This can be abstracted as a general image degradation–restoration problem with information loss. In this work, we propose a novel invertible framework to handle this general problem, which models the bidirectional degradation and restoration from a new perspective, i.e. invertible bijective transformation. The invertibility enables the framework to model the information loss of pre-degradation in the form of distribution, which could mitigate the ill-posed problem during post-restoration. To be specific, we develop invertible models to generate valid degraded images and meanwhile transform the distribution of lost contents to the fixed distribution of a latent variable during the forward degradation. Then restoration is made tractable by applying the inverse transformation on the generated degraded image together with a randomly-drawn latent variable. We start from image rescaling and instantiate the model as Invertible Rescaling Network, which can be easily extended to the similar decolorization–colorization task. We further propose to combine the invertible framework with existing degradation methods such as image compression for wider applications. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of upscaling and colorizing reconstruction from downscaled and decolorized images, and rate-distortion of image compression. Code is available at https://github.com/pkuxmq/Invertible-Image-Rescaling .},
  archive      = {J_IJCV},
  author       = {Xiao, Mingqing and Zheng, Shuxin and Liu, Chang and Lin, Zhouchen and Liu, Tie-Yan},
  doi          = {10.1007/s11263-022-01688-4},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {134-159},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Invertible rescaling network and its extensions},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Common pole–polar properties of central catadioptric sphere
and line images used for camera calibration. <em>IJCV</em>,
<em>131</em>(1), 121–133. (<a
href="https://doi.org/10.1007/s11263-022-01696-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Central catadioptric cameras with a single effective viewpoint contain both mirrors and pinhole cameras that increase the imaging field of view. In this study, the common pole–polar properties of central catadioptric sphere or line images are investigated and used for camera calibration. From these properties, the pole and polar with respect to the image of absolute conic and the modified image of absolute conic, respectively, can be recovered according to the generalized eigenvalue decomposition. Moreover, these techniques are valid for paracatadioptric sensors with the degenerate conic dual to the circular points being considered. At least three images of spheres or lines are required to completely calibrate any central catadioptric camera. The intrinsic parameters of the camera, the shape of reflective mirror, and the distortion parameters can be linearly estimated using the algebraic and geometric constraints of the sphere or line images obtained by the central catadioptric camera. The obtained experimental results demonstrate the effectiveness and feasibility of the proposed calibration algorithm.},
  archive      = {J_IJCV},
  author       = {Yang, Fengli and Zhao, Yue and Wang, Xuechun},
  doi          = {10.1007/s11263-022-01696-4},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {121-133},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Common Pole–Polar properties of central catadioptric sphere and line images used for camera calibration},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A CNN based approach for the point-light photometric stereo
problem. <em>IJCV</em>, <em>131</em>(1), 101–120. (<a
href="https://doi.org/10.1007/s11263-022-01689-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing the 3D shape of an object using several images under different light sources is a very challenging task, especially when realistic assumptions such as light propagation and attenuation, perspective viewing geometry and specular light reflection are considered. Many of works tackling Photometric Stereo (PS) problems often relax most of the aforementioned assumptions. Especially they ignore specular reflection and global illumination effects. In this work, we propose a CNN-based approach capable of handling these realistic assumptions by leveraging recent improvements of deep neural networks for far-field Photometric Stereo and adapt them to the point light setup. We achieve this by employing an iterative procedure of point-light PS for shape estimation which has two main steps. Firstly we train a per-pixel CNN to predict surface normals from reflectance samples. Secondly, we compute the depth by integrating the normal field in order to iteratively estimate light directions and attenuation which is used to compensate the input images to compute reflectance samples for the next iteration. Our approach sigificantly outperforms the state-of-the-art on the DiLiGenT real world dataset. Furthermore, in order to measure the performance of our approach for near-field point-light source PS data, we introduce LUCES the first real-world ’dataset for near-fieLd point light soUrCe photomEtric Stereo’ of 14 objects of different materials were the effects of point light sources and perspective viewing are a lot more significant. Our approach also outperforms the competition on this dataset as well. Data and test code are available at the project page.},
  archive      = {J_IJCV},
  author       = {Logothetis, Fotios and Mecca, Roberto and Budvytis, Ignas and Cipolla, Roberto},
  doi          = {10.1007/s11263-022-01689-3},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {101-120},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A CNN based approach for the point-light photometric stereo problem},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to collocate visual-linguistic neural modules for
image captioning. <em>IJCV</em>, <em>131</em>(1), 82–100. (<a
href="https://doi.org/10.1007/s11263-022-01692-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans tend to decompose a sentence into different parts like sth do sth at someplace and then fill each part with certain content. Inspired by this, we follow the principle of modular design to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the widely used neural module networks in VQA, where the language (i.e., question) is fully observable, the task of collocating visual-linguistic modules is more challenging. This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: (1) distinguishable module design—four modules in the encoder including one linguistic module for function words and three visual modules for different content words (i.e., noun, adjective, and verb) and another linguistic one in the decoder for commonsense reasoning, (2) a self-attention based module controller for robustifying the visual reasoning, (3) a part-of-speech based syntax loss imposed on the module controller for further regularizing the training of our CVLNM. Extensive experiments on the MS-COCO dataset show that our CVLNM is more effective, e.g., achieving a new state-of-the-art 129.5 CIDEr-D, and more robust, e.g., being less likely to overfit to dataset bias and suffering less when fewer training samples are available. Codes are available at https://github.com/GCYZSL/CVLMN .},
  archive      = {J_IJCV},
  author       = {Yang, Xu and Zhang, Hanwang and Gao, Chongyang and Cai, Jianfei},
  doi          = {10.1007/s11263-022-01692-8},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {82-100},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to collocate visual-linguistic neural modules for image captioning},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delving deeper into anti-aliasing in ConvNets.
<em>IJCV</em>, <em>131</em>(1), 67–81. (<a
href="https://doi.org/10.1007/s11263-022-01672-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling (Zhang in: ICML, 2020). However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks, including image classification, semantic segmentation, instance segmentation, video instance segmentation, and image-to-image translation. Both qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition. Code is available at https://maureenzou.github.io/ddac/ .},
  archive      = {J_IJCV},
  author       = {Zou, Xueyan and Xiao, Fanyi and Yu, Zhiding and Li, Yuheng and Lee, Yong Jae},
  doi          = {10.1007/s11263-022-01672-y},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {67-81},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Delving deeper into anti-aliasing in ConvNets},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-light image enhancement via breaking down the darkness.
<em>IJCV</em>, <em>131</em>(1), 48–66. (<a
href="https://doi.org/10.1007/s11263-022-01667-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in low-light environments often suffer from complex degradation. Simply adjusting light would inevitably result in burst of hidden noise and color distortion. To seek results with satisfied lighting, cleanliness, and realism from degraded inputs, this paper presents a novel framework inspired by the divide-and-rule principle, greatly alleviating the degradation entanglement. Assuming that an image can be decomposed into texture (with possible noise) and color components, one can specifically execute noise removal and color correction along with light adjustment. For this purpose, we propose to convert an image from the RGB colorspace into a luminance-chrominance one. An adjustable noise suppression network is designed to eliminate noise in the brightened luminance, having the illumination map estimated to indicate noise amplification levels. The enhanced luminance further serves as guidance for the chrominance mapper to generate realistic colors. Extensive experiments are conducted to reveal the effectiveness of our design, and demonstrate its superiority over state-of-the-art alternatives both quantitatively and qualitatively on several benchmark datasets. Our code has been made publicly available at https://github.com/mingcv/Bread .},
  archive      = {J_IJCV},
  author       = {Guo, Xiaojie and Hu, Qiming},
  doi          = {10.1007/s11263-022-01667-9},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {48-66},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Low-light image enhancement via breaking down the darkness},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distribution-sensitive information retention for accurate
binary neural network. <em>IJCV</em>, <em>131</em>(1), 26–47. (<a
href="https://doi.org/10.1007/s11263-022-01687-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model binarization is an effective method of compressing neural networks and accelerating their inference process, which enables state-of-the-art models to run on resource-limited devices. Recently, advanced binarization methods have been greatly improved by minimizing the quantization error directly in the forward process. However, a significant performance gap still exists between the 1-bit model and the 32-bit one. The empirical study shows that binarization causes a great loss of information in the forward and backward propagation which harms the performance of binary neural networks (BNNs). We present a novel distribution-sensitive information retention network (DIR-Net) that retains the information in the forward and backward propagation by improving internal propagation and introducing external representations. The DIR-Net mainly relies on three technical contributions: (1) Information Maximized Binarization (IMB): minimizing the information loss and the binarization error of weights/activations simultaneously by weight balance and standardization; (2) Distribution-sensitive Two-stage Estimator (DTE): retaining the information of gradients by distribution-sensitive soft approximation by jointly considering the updating capability and accurate gradient; (3) Representation-align Binarization-aware Distillation (RBD): retaining the representation information by distilling the representations between full-precision and binarized networks. The DIR-Net investigates both forward and backward processes of BNNs from the unified information perspective, thereby providing new insight into the mechanism of network binarization. The three techniques in our DIR-Net are versatile and effective and can be applied in various structures to improve BNNs. Comprehensive experiments on the image classification and objective detection tasks show that our DIR-Net consistently outperforms the state-of-the-art binarization approaches under mainstream and compact architectures, such as ResNet, VGG, EfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on real-world resource-limited devices which achieves $$11.1\times $$ storage saving and $$5.4\times $$ speedup.},
  archive      = {J_IJCV},
  author       = {Qin, Haotong and Zhang, Xiangguo and Gong, Ruihao and Ding, Yifu and Xu, Yi and Liu, Xianglong},
  doi          = {10.1007/s11263-022-01687-5},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {26-47},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Distribution-sensitive information retention for accurate binary neural network},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Population-based evolutionary gaming for unsupervised person
re-identification. <em>IJCV</em>, <em>131</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11263-022-01693-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification has achieved great success through the self-improvement of individual neural networks. However, limited by the lack of diversity of discriminant information, a single network has difficulty learning sufficient discrimination ability by itself under unsupervised conditions. To address this limit, we develop a population-based evolutionary gaming (PEG) framework in which a population of diverse neural networks are trained concurrently through selection, reproduction, mutation, and population mutual learning iteratively. Specifically, the selection of networks to preserve is modeled as a cooperative game and solved by the best-response dynamics, then the reproduction and mutation are implemented by cloning and fluctuating hyper-parameters of networks to learn more diversity, and population mutual learning improves the discrimination of networks by knowledge distillation from each other within the population. In addition, we propose a cross-reference scatter (CRS) to approximately evaluate re-ID models without labeled samples and adopt it as the criterion of network selection in PEG. CRS measures a model’s performance by indirectly estimating the accuracy of its predicted pseudo-labels according to the cohesion and separation of the feature space. Extensive experiments demonstrate that (1) CRS approximately measures the performance of models without labeled samples; (2) and PEG produces new state-of-the-art accuracy for person re-identification, indicating the great potential of population-based network cooperative training for unsupervised learning. The code is released on github.com/YunpengZhai/PEG.},
  archive      = {J_IJCV},
  author       = {Zhai, Yunpeng and Peng, Peixi and Jia, Mengxi and Li, Shiyong and Chen, Weiqiang and Gao, Xuesong and Tian, Yonghong},
  doi          = {10.1007/s11263-022-01693-7},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Population-based evolutionary gaming for unsupervised person re-identification},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
