<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---105">COAP - 105</h2>
<ul>
<li><details>
<summary>
(2023). COAP 2022 best paper prize. <em>COAP</em>, <em>86</em>(3),
1373–1375. (<a
href="https://doi.org/10.1007/s10589-023-00538-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  doi          = {10.1007/s10589-023-00538-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1373-1375},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2022 best paper prize},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The deepest event cuts in risk-averse optimization with
application to radiation therapy design. <em>COAP</em>, <em>86</em>(3),
1347–1372. (<a
href="https://doi.org/10.1007/s10589-023-00531-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our study is motivated by radiation therapy design for cancer treatment. We consider large-scale problems with stochastic order constraints. We establish a general result about the form of the deepest cuts associated with events of positive probability which are used in the numerical approximation of the functional constraints. An efficient method using the deepest cuts is proposed for the numerical solution of problems with second-order dominance constraints and increasing convex order constraints. We the propose a new methodology for the radiation-therapy design for cancer treatment. We introduce a risk-averse optimization problem with two types of stochastic order relations and with coherent measures of risk and consider the effect of the risk models in three versions of the problem formulation. Additionally, we propose a method that creates flexible (floating) benchmark distributions when benchmark distributions are not given apriori or when the provided distributions lead to infeasibility. We devise a numerical method using floating benchmarks for solving the proposed risk-averse optimization models for radiation therapy design. The models and methods are verified by using clinical data confirming the viability of the proposed methodology and its efficiency.},
  archive      = {J_COAP},
  author       = {Vitt, Constantine A. and Dentcheva, Darinka and Ruszczyński, Andrzej and Sandberg, Nolan},
  doi          = {10.1007/s10589-023-00531-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1347-1372},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The deepest event cuts in risk-averse optimization with application to radiation therapy design},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generic linear convergence through metric subregularity in a
variable-metric extension of the proximal point algorithm.
<em>COAP</em>, <em>86</em>(3), 1327–1346. (<a
href="https://doi.org/10.1007/s10589-023-00494-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proximal point algorithm finds a zero of a maximal monotone mapping by iterations in which the mapping is made strongly monotone by the addition of a proximal term. Here it is articulated with the norm behind the proximal term possibly shifting from one iteration to the next, but under conditions that eventually make the metric settle down. Despite the varying geometry, the sequence generated by the algorithm is shown to converge to a particular solution. Although this is not the first variable-metric extension of proximal point algorithm, it is the first to retain the flexibility needed for applications to augmented Lagrangian methodology and progressive decoupling. Moreover, in a generic sense, the convergence it generates is Q-linear at a rate that depends in a simple way on the modulus of metric subregularity of the mapping at that solution. This is a tighter rate than previously identified and reveals for the first time the definitive role of metric subregularity in how the proximal point algorithm performs, even in fixed-metric mode.},
  archive      = {J_COAP},
  author       = {Rockafellar, R. Tyrrell},
  doi          = {10.1007/s10589-023-00494-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1327-1346},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Generic linear convergence through metric subregularity in a variable-metric extension of the proximal point algorithm},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Error estimates for runge–kutta schemes of optimal control
problems with index 1 DAEs. <em>COAP</em>, <em>86</em>(3), 1299–1325.
(<a href="https://doi.org/10.1007/s10589-023-00484-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we derive error estimates for Runge–Kutta schemes of optimal control problems subject to index one differential–algebraic equations (DAEs). Usually, Runge–Kutta methods applied to DAEs approximate the differential and algebraic state in an analogous manner. These schemes can be considered as discretizations of the index reduced system where the algebraic equation is solved for the algebraic variable to get an explicit ordinary differential equation. However, in optimal control this approach yields discrete necessary conditions that are not consistent with the continuous necessary conditions which are essential for deriving error estimates. Therefore, we suggest to treat the algebraic variable like a control, obtaining a new type of Runge–Kutta scheme. For this method we derive consistent necessary conditions and compare the discrete and continuous systems to get error estimates up to order three for the states and control as well as the multipliers.},
  archive      = {J_COAP},
  author       = {Martens, Björn},
  doi          = {10.1007/s10589-023-00484-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1299-1325},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Error estimates for Runge–Kutta schemes of optimal control problems with index 1 DAEs},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimality conditions for tucker low-rank tensor
optimization. <em>COAP</em>, <em>86</em>(3), 1275–1298. (<a
href="https://doi.org/10.1007/s10589-023-00465-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization problems with tensor variables are widely used in statistics, machine learning, pattern recognition, signal processing, computer vision, etc. Among these applications, the low-rankness of tensors is an intrinsic property that can help unearth potential but important structure or feature in the corresponding high-dimensional multi-way datasets, leading to the study on low-rank tensor optimization (LRTO for short). For the general framework of LRTO, little has been addressed in optimization theory. This motivates us to study the optimality conditions, with special emphasis on the Tucker low-rank constrained problems and the Tucker low-rank decomposition-based reformulations. It is noteworthy that all the involved optimization problems are nonconvex, and even discontinuous, due to the complexity of the tensor Tucker rank function or the multi-linear decomposition with the orthogonality or even group sparsity constraints imposed on factor matrices. By employing the tools in variational analysis, especially the normal cones to low-rank matrices and the properties of matrix manifolds, we propose necessary and/or sufficient optimality conditions for Tucker low-rank tensor optimization problems, which will enrich the context of the nonconvex and nonsmooth optimization.},
  archive      = {J_COAP},
  author       = {Luo, Ziyan and Qi, Liqun},
  doi          = {10.1007/s10589-023-00465-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1275-1298},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimality conditions for tucker low-rank tensor optimization},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization over the pareto front of nonconvex
multi-objective optimal control problems. <em>COAP</em>, <em>86</em>(3),
1247–1274. (<a
href="https://doi.org/10.1007/s10589-023-00535-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous optimization of multiple objective functions results in a set of trade-off, or Pareto, solutions. Choosing a, in some sense, best solution in this set is in general a challenging task: In the case of three or more objectives the Pareto front is usually difficult to view, if not impossible, and even in the case of just two objectives constructing the whole Pareto front so as to visually inspect it might be very costly. Therefore, optimization over the Pareto (or efficient) set has been an active area of research. Although there is a wealth of literature involving finite dimensional optimization problems in this area, there is a lack of problem formulation and numerical methods for optimal control problems, except for the convex case. In this paper, we formulate the problem of optimizing over the Pareto front of nonconvex constrained and time-delayed optimal control problems as a bi-level optimization problem. Motivated by existing solution differentiability results, we propose an algorithm incorporating (i) the Chebyshev scalarization, (ii) a concept of the essential interval of weights, and (iii) the simple but effective bisection method, for optimal control problems with two objectives. We illustrate the working of the algorithm on two example problems involving an electric circuit and treatment of tuberculosis and discuss future lines of research for new computational methods.},
  archive      = {J_COAP},
  author       = {Kaya, C. Yalçın and Maurer, Helmut},
  doi          = {10.1007/s10589-023-00535-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1247-1274},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimization over the pareto front of nonconvex multi-objective optimal control problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extension of switch point algorithm to boundary-value
problems. <em>COAP</em>, <em>86</em>(3), 1229–1246. (<a
href="https://doi.org/10.1007/s10589-023-00530-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an earlier paper ( https://doi.org/10.1137/21M1393315 ), the switch point algorithm was developed for solving optimal control problems whose solutions are either singular or bang-bang or both singular and bang-bang, and which possess a finite number of jump discontinuities in an optimal control at the points in time where the solution structure changes. The class of control problems that were considered had a given initial condition, but no terminal constraint. The theory is now extended to include problems with both initial and terminal constraints, a structure that often arises in boundary-value problems. Substantial changes to the theory are needed to handle this more general setting. Nonetheless, the derivative of the cost with respect to a switch point is again the jump in the Hamiltonian at the switch point.},
  archive      = {J_COAP},
  author       = {Hager, William W.},
  doi          = {10.1007/s10589-023-00530-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1229-1246},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Extension of switch point algorithm to boundary-value problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perturbation analysis of the euclidean distance matrix
optimization problem and its numerical implications. <em>COAP</em>,
<em>86</em>(3), 1193–1227. (<a
href="https://doi.org/10.1007/s10589-023-00505-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Euclidean distance matrices have lately received increasing attention in applications such as multidimensional scaling and molecular conformation from nuclear magnetic resonance data in computational chemistry. In this paper, we focus on the perturbation analysis of the Euclidean distance matrix optimization problem (EDMOP). Under Robinson’s constraint qualification, we establish a number of equivalent characterizations of strong regularity and strong stability at a locally optimal solution of EDMOP. Those results extend the corresponding characterizations in Semidefinite Programming and are tailored to the special structure in EDMOP. As an application, we demonstrate a numerical implication of the established results on an alternating direction method of multipliers (ADMM) to a stress minimization problem, which is an important instance of EDMOP. The implication is that the ADMM method converges to a strongly stable solution under reasonable assumptions.},
  archive      = {J_COAP},
  author       = {Guo, Shaoyan and Qi, Hou-Duo and Zhang, Liwei},
  doi          = {10.1007/s10589-023-00505-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1193-1227},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Perturbation analysis of the euclidean distance matrix optimization problem and its numerical implications},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the SCD semismooth* newton method for generalized
equations with application to a class of static contact problems with
coulomb friction. <em>COAP</em>, <em>86</em>(3), 1159–1191. (<a
href="https://doi.org/10.1007/s10589-022-00429-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, a variant of the semismooth $$^{*}$$ Newton method is developed for the numerical solution of generalized equations, in which the multi-valued part is a so-called SCD (subspace containing derivative) mapping. Under a rather mild regularity requirement, the method exhibits (locally) superlinear convergence behavior. From the main conceptual algorithm, two implementable variants are derived whose efficiency is tested via a generalized equation modeling a discretized static contact problem with Coulomb friction.},
  archive      = {J_COAP},
  author       = {Gfrerer, Helmut and Mandlmayr, Michael and Outrata, Jiří V. and Valdman, Jan},
  doi          = {10.1007/s10589-022-00429-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1159-1191},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the SCD semismooth* newton method for generalized equations with application to a class of static contact problems with coulomb friction},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radius theorems for subregularity in infinite dimensions.
<em>COAP</em>, <em>86</em>(3), 1117–1158. (<a
href="https://doi.org/10.1007/s10589-022-00431-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper continues our previous work (Dontchev et al. in Set-Valued Var Anal 28:451–473, 2020) on the radius of subregularity that was initiated by Asen Dontchev. We extend the results of (Dontchev et al. in Set-Valued Var Anal 28:451–473, 2020) to general Banach/Asplund spaces and to other classes of perturbations, and sharpen the coderivative tools used in the analysis of the robustness of well-posedness of mathematical problems and related regularity properties of mappings involved in the statements. We also expand the selection of classes of perturbations, for which the formula for the radius of strong subregularity is valid.},
  archive      = {J_COAP},
  author       = {Gfrerer, Helmut and Kruger, Alexander Y.},
  doi          = {10.1007/s10589-022-00431-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1117-1158},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Radius theorems for subregularity in infinite dimensions},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relaxed dissipativity assumptions and a simplified algorithm
for multiobjective MPC. <em>COAP</em>, <em>86</em>(3), 1081–1116. (<a
href="https://doi.org/10.1007/s10589-022-00398-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonlinear model predictive control (MPC) with multiple competing cost functions. In each step of the scheme, a multiobjective optimal control problem with a nonlinear system and terminal conditions is solved. We propose an algorithm and give performance guarantees for the resulting MPC closed loop system. Thereby, we significantly simplify the assumptions made in the literature so far by assuming strict dissipativity and the existence of a compatible terminal cost for one of the competing objective functions only. We give conditions which ensure asymptotic stability of the closed loop and, what is more, obtain performance estimates for all cost criteria. Numerical simulations on various instances illustrate our findings. The proposed algorithm requires the selection of an efficient solution in each iteration, thus we examine several selection rules and their impact on the results. and we also examine numerically how different selection rules impact the results},
  archive      = {J_COAP},
  author       = {Eichfelder, Gabriele and Grüne, Lars and Krügel, Lisa and Schießl, Jonas},
  doi          = {10.1007/s10589-022-00398-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1081-1116},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Relaxed dissipativity assumptions and a simplified algorithm for multiobjective MPC},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the solution stability of parabolic optimal control
problems. <em>COAP</em>, <em>86</em>(3), 1035–1079. (<a
href="https://doi.org/10.1007/s10589-023-00473-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper investigates stability properties of solutions of optimal control problems constrained by semilinear parabolic partial differential equations. Hölder or Lipschitz dependence of the optimal solution on perturbations are obtained for problems in which the equation and the objective functional are affine with respect to the control. The perturbations may appear in both the equation and in the objective functional and may nonlinearly depend on the state and control variables. The main results are based on an extension of recently introduced assumptions on the joint growth of the first and second variation of the objective functional. The stability of the optimal solution is obtained as a consequence of a more general result obtained in the paper–the metric subregularity of the mapping associated with the system of first order necessary optimality conditions. This property also enables error estimates for approximation methods. A Lipschitz estimate for the dependence of the optimal control on the Tikhonov regularization parameter is obtained as a by-product.},
  archive      = {J_COAP},
  author       = {Corella, Alberto Domínguez and Jork, Nicolai and Veliov, Vladimir M.},
  doi          = {10.1007/s10589-023-00473-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1035-1079},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the solution stability of parabolic optimal control problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study of progressive hedging for stochastic integer
programming. <em>COAP</em>, <em>86</em>(3), 989–1034. (<a
href="https://doi.org/10.1007/s10589-023-00532-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by recent literature demonstrating the surprising effectiveness of the heuristic application of progressive hedging (PH) to stochastic mixed-integer programming (SMIP) problems, we provide theoretical support for the inclusion of integer variables, bridging the gap between theory and practice. We provide greater insight into the following observed phenomena of PH as applied to SMIP where optimal or at least feasible convergence is observed. We provide an analysis of a modified PH algorithm from a different viewpoint, drawing on the interleaving of (split) proximal-point methods (including PH), Gauss–Seidel methods, and the utilisation of variational analysis tools. Through this analysis, we show that under mild conditions, convergence to a feasible solution should be expected. In terms of convergence analysis, we provide two main contributions. First, we contribute insight into the convergence of proximal-point-like methods in the presence of integer variables via the introduction of the notion of persistent local minima. Secondly, we contribute an enhanced Gauss–Seidel convergence analysis that accommodates the variation of the objective function under mild assumptions. We provide a practical implementation of a modified PH and demonstrate its convergent behaviour with computational experiments in line with the provided analysis.},
  archive      = {J_COAP},
  author       = {Christiansen, Jeffrey and Dandurand, Brian and Eberhard, Andrew and Oliveira, Fabricio},
  doi          = {10.1007/s10589-023-00532-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {989-1034},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A study of progressive hedging for stochastic integer programming},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust and continuous metric subregularity for linear
inequality systems. <em>COAP</em>, <em>86</em>(3), 967–988. (<a
href="https://doi.org/10.1007/s10589-022-00437-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces two new variational properties, robust and continuous metric subregularity, for finite linear inequality systems under data perturbations. The motivation of this study goes back to the seminal work by Dontchev, Lewis, and Rockafellar (2003) on the radius of metric regularity. In contrast to the metric regularity, the unstable continuity behavoir of the (always finite) metric subregularity modulus leads us to consider the aforementioned properties. After characterizing both of them, the radius of robust metric subregularity is computed and some insights on the radius of continuous metric subregularity are provided.},
  archive      = {J_COAP},
  author       = {Camacho, J. and Cánovas, M. J. and López, M. A. and Parra, J.},
  doi          = {10.1007/s10589-022-00437-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {967-988},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Robust and continuous metric subregularity for linear inequality systems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated minimax algorithm for convex-concave saddle
point problems with nonsmooth coupling function. <em>COAP</em>,
<em>86</em>(3), 925–966. (<a
href="https://doi.org/10.1007/s10589-022-00378-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we aim to solve a convex-concave saddle point problem, where the convex-concave coupling function is smooth in one variable and nonsmooth in the other and not assumed to be linear in either. The problem is augmented by a nonsmooth regulariser in the smooth component. We propose and investigate a novel algorithm under the name of OGAProx, consisting of an optimistic gradient ascent step in the smooth variable coupled with a proximal step of the regulariser, and which is alternated with a proximal step in the nonsmooth component of the coupling function. We consider the situations convex-concave, convex-strongly concave and strongly convex-strongly concave related to the saddle point problem under investigation. Regarding iterates we obtain (weak) convergence, a convergence rate of order $$\mathcal {O}(\frac{1}{K})$$ and linear convergence like $$\mathcal {O}(\theta ^{K})$$ with $$\theta &lt; 1$$ , respectively. In terms of function values we obtain ergodic convergence rates of order $$\mathcal {O}(\frac{1}{K})$$ , $$\mathcal {O}(\frac{1}{K^{2}})$$ and $$\mathcal {O}(\theta ^{K})$$ with $$\theta &lt; 1$$ , respectively. We validate our theoretical considerations on a nonsmooth-linear saddle point problem, the training of multi kernel support vector machines and a classification problem incorporating minimax group fairness.},
  archive      = {J_COAP},
  author       = {Boţ, Radu Ioan and Csetnek, Ernö Robert and Sedlmayer, Michael},
  doi          = {10.1007/s10589-022-00378-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {925-966},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated minimax algorithm for convex-concave saddle point problems with nonsmooth coupling function},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A filippov approximation theorem for strengthened one-sided
lipschitz differential inclusions. <em>COAP</em>, <em>86</em>(3),
885–923. (<a href="https://doi.org/10.1007/s10589-023-00517-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider differential inclusions with strengthened one-sided Lipschitz (SOSL) right-hand sides. The class of SOSL multivalued maps is wider than the class of Lipschitz ones and a subclass of the class of one-sided Lipschitz maps. We prove a Filippov approximation theorem for the solutions of such differential inclusions with perturbations in the right-hand side, both of the set of the velocities (outer perturbations) and of the state (inner perturbations). The obtained estimate of the distance between the approximate and exact solution extends the known Filippov estimate for Lipschitz maps to SOSL ones and improves the order of approximation with respect to the inner perturbation known for one-sided Lipschitz (OSL) right-hand sides from $$\frac{1}{2}$$ to 1.},
  archive      = {J_COAP},
  author       = {Baier, Robert and Farkhi, Elza},
  doi          = {10.1007/s10589-023-00517-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {885-923},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A filippov approximation theorem for strengthened one-sided lipschitz differential inclusions},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear singularly perturbed systems without slow-fast split.
<em>COAP</em>, <em>86</em>(3), 871–884. (<a
href="https://doi.org/10.1007/s10589-022-00412-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine linear, time invariant, singularly perturbed differential equations, where a split into slow and fast variables is not prescribed. Simple linear algebra considerations give rise to a useful order reduction type framework. A comparison with the classical order reduction method is provided, and the relation to efficient computations is pointed out.},
  archive      = {J_COAP},
  author       = {Artstein, Zvi},
  doi          = {10.1007/s10589-022-00412-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {871-884},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Linear singularly perturbed systems without slow-fast split},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed forward-backward methods for ring networks.
<em>COAP</em>, <em>86</em>(3), 845–870. (<a
href="https://doi.org/10.1007/s10589-022-00400-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose and analyse forward-backward-type algorithms for finding a zero of the sum of finitely many monotone operators, which are not based on reduction to a two operator inclusion in the product space. Each iteration of the studied algorithms requires one resolvent evaluation per set-valued operator, one forward evaluation per cocoercive operator, and two forward evaluations per monotone operator. Unlike existing methods, the structure of the proposed algorithms are suitable for distributed, decentralised implementation in ring networks without needing global summation to enforce consensus between nodes.},
  archive      = {J_COAP},
  author       = {Aragón-Artacho, Francisco J. and Malitsky, Yura and Tam, Matthew K. and Torregrosa-Belén, David},
  doi          = {10.1007/s10589-022-00400-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {845-870},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributed forward-backward methods for ring networks},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). First order inertial optimization algorithms with threshold
effects associated with dry friction. <em>COAP</em>, <em>86</em>(3),
801–843. (<a href="https://doi.org/10.1007/s10589-023-00509-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Hilbert space setting, we consider new first order optimization algorithms which are obtained by temporal discretization of a damped inertial autonomous dynamic involving dry friction. The function f to be minimized is assumed to be differentiable (not necessarily convex). The dry friction potential function $$ \varphi $$ , which has a sharp minimum at the origin, enters the algorithm via its proximal mapping, which acts as a soft thresholding operator on the sum of the velocity and the gradient terms. After a finite number of steps, the structure of the algorithm changes, losing its inertial character to become the steepest descent method. The geometric damping driven by the Hessian of f makes it possible to control and attenuate the oscillations. The algorithm generates convergent sequences when f is convex, and in the nonconvex case when f satisfies the Kurdyka–Lojasiewicz property. The convergence results are robust with respect to numerical errors, and perturbations. The study is then extended to the case of a nonsmooth convex function f, in which case the algorithm involves the proximal operators of f and $$\varphi $$ separately. Applications are given to the Lasso problem and nonsmooth d.c. programming.},
  archive      = {J_COAP},
  author       = {Adly, Samir and Attouch, Hedy and Le, Manh Hung},
  doi          = {10.1007/s10589-023-00509-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {801-843},
  shortjournal = {Comput. Optim. Appl.},
  title        = {First order inertial optimization algorithms with threshold effects associated with dry friction},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface to asen l. Dontchev memorial special issue.
<em>COAP</em>, <em>86</em>(3), 795–800. (<a
href="https://doi.org/10.1007/s10589-023-00537-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Hager, William W. and Rockafellar, R. Tyrrell and Veliov, Vladimir M.},
  doi          = {10.1007/s10589-023-00537-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {795-800},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface to asen l. dontchev memorial special issue},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A space–time variational method for optimal control
problems: Well-posedness, stability and numerical solution.
<em>COAP</em>, <em>86</em>(2), 767–794. (<a
href="https://doi.org/10.1007/s10589-023-00507-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an optimal control problem constrained by a parabolic partial differential equation with Robin boundary conditions. We use a space–time variational formulation in Lebesgue–Bochner spaces yielding a boundedly invertible solution operator. The abstract formulation of the optimal control problem yields the Lagrange function and Karush–Kuhn–Tucker conditions in a natural manner. This results in space–time variational formulations of the adjoint and gradient equation in Lebesgue–Bochner spaces, which are proven to be boundedly invertible. Necessary and sufficient optimality conditions are formulated and the optimality system is shown to be boundedly invertible. Next, we introduce a conforming uniformly stable simultaneous space–time (tensorproduct) discretization of the optimality system in these Lebesgue–Bochner spaces. Using finite elements of appropriate orders in space and time for trial and test spaces, this setting is known to be equivalent to a Crank–Nicolson time-stepping scheme for parabolic problems. Comparisons with existing methods are detailed. We show numerical comparisons with time-stepping methods. The space–time method shows good stability properties and requires fewer degrees of freedom in time to reach the same accuracy.},
  archive      = {J_COAP},
  author       = {Beranek, Nina and Reinhold, Martin Alexander and Urban, Karsten},
  doi          = {10.1007/s10589-023-00507-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {767-794},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A space–time variational method for optimal control problems: Well-posedness, stability and numerical solution},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse optimization via vector k-norm and DC programming
with an application to feature selection for support vector machines.
<em>COAP</em>, <em>86</em>(2), 745–766. (<a
href="https://doi.org/10.1007/s10589-023-00506-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse optimization is about finding minimizers of functions characterized by a number of nonzero components as small as possible, such paradigm being of great practical relevance in Machine Learning, particularly in classification approaches based on support vector machines. By exploiting some properties of the k-norm of a vector, namely, of the sum of its k largest absolute-value components, we formulate a sparse optimization problem as a mixed-integer nonlinear program, whose continuous relaxation is equivalent to the unconstrained minimization of a difference-of-convex function. The approach is applied to Feature Selection in the support vector machine framework, and tested on a set of benchmark instances. Numerical comparisons against both the standard $$\ell _1$$ -based support vector machine and a simple version of the Slope method are presented, that demonstrate the effectiveness of our approach in achieving high sparsity level of the solutions without impairing test-correctness.},
  archive      = {J_COAP},
  author       = {Gaudioso, Manlio and Giallombardo, Giovanni and Miglionico, Giovanna},
  doi          = {10.1007/s10589-023-00506-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {745-766},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Sparse optimization via vector k-norm and DC programming with an application to feature selection for support vector machines},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A trust-region LP-newton method for constrained nonsmooth
equations under hölder metric subregularity. <em>COAP</em>,
<em>86</em>(2), 711–743. (<a
href="https://doi.org/10.1007/s10589-023-00498-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe and analyze a globally convergent algorithm to find a possible nonisolated zero of a piecewise smooth mapping over a polyhedral set. Such formulation includes Karush–Kuhn–Tucker systems, variational inequalities problems, and generalized Nash equilibrium problems. Our algorithm is based on a modification of the fast locally convergent Linear Programming (LP)-Newton method with a trust-region strategy for globalization that makes use of the natural merit function. The transition between global and local convergence occurs naturally under mild assumption. Our local convergence analysis of the method is performed under a Hölder metric subregularity condition of the mapping defining the possibly nonsmooth equation and the Hölder continuity of the derivative of the selection mapping. We present numerical results that show the feasibility of the approach.},
  archive      = {J_COAP},
  author       = {Becher, Letícia and Fernández, Damián and Ramos, Alberto},
  doi          = {10.1007/s10589-023-00498-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {711-743},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A trust-region LP-newton method for constrained nonsmooth equations under hölder metric subregularity},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-based stochastic inexact algorithm for a class of
stochastic nonsmooth convex composite problems. <em>COAP</em>,
<em>86</em>(2), 669–710. (<a
href="https://doi.org/10.1007/s10589-023-00504-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a dual-based stochastic inexact algorithm is developed to solve a class of stochastic nonsmooth convex problems with underlying structure. This algorithm can be regarded as an integration of a deterministic augmented Lagrangian method and some stochastic approximation techniques. By utilizing the sparsity of the second order information, each subproblem is efficiently solved by a superlinearly convergent semismooth Newton method. We derive some almost surely convergence properties and convergence rate of objective values. Furthermore, we present some results related to convergence rate of distance between iteration points and solution set under error bound conditions. Numerical results demonstrate favorable comparison of the proposed algorithm with some existing methods.},
  archive      = {J_COAP},
  author       = {Lin, Gui-Hua and Yang, Zhen-Ping and Yin, Hai-An and Zhang, Jin},
  doi          = {10.1007/s10589-023-00504-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {669-710},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A dual-based stochastic inexact algorithm for a class of stochastic nonsmooth convex composite problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-norm regularized problems: Equivalent surrogates,
proximal MM method and statistical error bound. <em>COAP</em>,
<em>86</em>(2), 627–667. (<a
href="https://doi.org/10.1007/s10589-023-00496-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the zero-norm regularized problem, we verify that the penalty problem of its equivalent MPEC reformulation is a global exact penalty, which implies a family of equivalent surrogates. For a subfamily of these surrogates, the critical point set is demonstrated to coincide with the d-directional stationary point set and when a critical point has no too small nonzero component, it is a strongly local optimal solution of the surrogate problem and the zero-norm regularized problem. We also develop a proximal majorization-minimization (MM) method for solving the DC (difference of convex functions) surrogates, and provide its global and linear convergence analysis. For the limit of the generated sequence, the statistical error bound is established under a mild condition, which implies its good quality from a statistical respective. Numerical comparisons with ADMM for solving the DC surrogate and APG for solving its partially smoothed form indicate that our proximal MM method armed with an inexact dual PPA plus the semismooth Newton method (PMMSN for short) is remarkably superior to ADMM and APG in terms of the quality of solutions and the CPU time.},
  archive      = {J_COAP},
  author       = {Zhang, Dongdong and Pan, Shaohua and Bi, Shujun and Sun, Defeng},
  doi          = {10.1007/s10589-023-00496-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {627-667},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Zero-norm regularized problems: Equivalent surrogates, proximal MM method and statistical error bound},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCORE: Approximating curvature information under
self-concordant regularization. <em>COAP</em>, <em>86</em>(2), 599–626.
(<a href="https://doi.org/10.1007/s10589-023-00502-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization problems that include regularization functions in their objectives are regularly solved in many applications. When one seeks second-order methods for such problems, it may be desirable to exploit specific properties of some of these regularization functions when accounting for curvature information in the solution steps to speed up convergence. In this paper, we propose the SCORE (self-concordant regularization) framework for unconstrained minimization problems which incorporates second-order information in the Newton-decrement framework for convex optimization. We propose the generalized Gauss–Newton with Self-Concordant Regularization (GGN-SCORE) algorithm that updates the minimization variables each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing computational overhead. GGN-SCORE demonstrates how to speed up convergence while also improving model generalization for problems that involve regularized minimization under the proposed SCORE framework. Numerical experiments show the efficiency of our method and its fast convergence, which compare favorably against baseline first-order and quasi-Newton methods. Additional experiments involving non-convex (overparameterized) neural network training problems show that the proposed method is promising for non-convex optimization.},
  archive      = {J_COAP},
  author       = {Adeoye, Adeyemi D. and Bemporad, Alberto},
  doi          = {10.1007/s10589-023-00502-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {599-626},
  shortjournal = {Comput. Optim. Appl.},
  title        = {SCORE: Approximating curvature information under self-concordant regularization},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complexity analysis of interior-point methods for
second-order stationary points of nonlinear semidefinite optimization
problems. <em>COAP</em>, <em>86</em>(2), 555–598. (<a
href="https://doi.org/10.1007/s10589-023-00501-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a primal-dual interior-point method (IPM) with convergence to second-order stationary points (SOSPs) of nonlinear semidefinite optimization problems, abbreviated as NSDPs. As far as we know, the current algorithms for NSDPs only ensure convergence to first-order stationary points such as Karush–Kuhn–Tucker points, but without a worst-case iteration complexity. The proposed method generates a sequence approximating SOSPs while minimizing a primal-dual merit function for NSDPs by using scaled gradient directions and directions of negative curvature. Under some assumptions, the generated sequence accumulates at an SOSP with a worst-case iteration complexity. This result is also obtained for a primal IPM with a slight modification. Finally, our numerical experiments show the benefits of using directions of negative curvature in the proposed method.},
  archive      = {J_COAP},
  author       = {Arahata, Shun and Okuno, Takayuki and Takeda, Akiko},
  doi          = {10.1007/s10589-023-00501-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {555-598},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Complexity analysis of interior-point methods for second-order stationary points of nonlinear semidefinite optimization problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly majorized algorithm for sparsity-inducing
optimization problems with regularizer-compatible constraints.
<em>COAP</em>, <em>86</em>(2), 521–553. (<a
href="https://doi.org/10.1007/s10589-023-00503-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of sparsity-inducing optimization problems whose constraint set is regularizer-compatible, in the sense that, the constraint set becomes easy-to-project-onto after a coordinate transformation induced by the sparsity-inducing regularizer. Our model is general enough to cover, as special cases, the ordered LASSO model in Tibshirani and Suo (Technometrics 58:415–423, 2016) and its variants with some commonly used nonconvex sparsity-inducing regularizers. The presence of both the sparsity-inducing regularizer and the constraint set poses challenges on the design of efficient algorithms. In this paper, by exploiting absolute-value symmetry and other properties in the sparsity-inducing regularizer, we propose a new algorithm, called the doubly majorized algorithm (DMA), for this class of problems. The DMA makes use of projections onto the constraint set after the coordinate transformation in each iteration, and hence can be performed efficiently. Without invoking any commonly used constraint qualification conditions such as those based on horizon subdifferentials, we show that any accumulation point of the sequence generated by DMA is a so-called $$\psi _\textrm{opt}$$ -stationary point, a new notion of stationarity we define as inspired by the notion of L-stationarity in Beck and Eldar (SIAM J Optim 23:1480–1509, 2013) and Beck and Hallak (Math Oper Res 41:196–223, 2016) . We also show that any global minimizer of our model has to be a $$\psi _\textrm{opt}$$ -stationary point, again without imposing any constraint qualification conditions. Finally, we illustrate numerically the performance of DMA on solving variants of ordered LASSO with nonconvex regularizers.},
  archive      = {J_COAP},
  author       = {Liu, Tianxiang and Pong, Ting Kei and Takeda, Akiko},
  doi          = {10.1007/s10589-023-00503-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {521-553},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Doubly majorized algorithm for sparsity-inducing optimization problems with regularizer-compatible constraints},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A branch-and-prune algorithm for discrete nash equilibrium
problems. <em>COAP</em>, <em>86</em>(2), 491–519. (<a
href="https://doi.org/10.1007/s10589-023-00500-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a branch-and-prune procedure for discrete Nash equilibrium problems with a convex description of each player’s strategy set. The derived pruning criterion does not require player convexity, but only strict convexity of some player’s objective function in a single variable. If satisfied, it prunes choices for this variable by stating activity of certain constraints. This results in a synchronous branching and pruning method. An algorithmic implementation and numerical tests are presented for randomly generated instances with convex polyhedral strategy sets and convex quadratic as well as non-convex quadratic objective functions.},
  archive      = {J_COAP},
  author       = {Schwarze, Stefan and Stein, Oliver},
  doi          = {10.1007/s10589-023-00500-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {491-519},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A branch-and-prune algorithm for discrete nash equilibrium problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral conjugate gradient methods for vector optimization
problems. <em>COAP</em>, <em>86</em>(2), 457–489. (<a
href="https://doi.org/10.1007/s10589-023-00508-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an extension of the spectral conjugate gradient (SCG) methods for solving unconstrained vector optimization problems, with respect to the partial order induced by a pointed, closed and convex cone with a nonempty interior. We first study the direct extension version of the SCG methods and its global convergence without imposing an explicit restriction on parameters. It shows that the methods may lose their good scalar properties, like yielding descent directions, in the vector setting. By using a truncation technique, we then propose a modified self-adjusting SCG algorithm which is more suitable for various parameters. Global convergence of the new scheme covers the vector extensions of three different spectral parameters and the corresponding Perry, Andrei, and Dai–Kou conjugate parameters (SP, N, and JC schemes, respectively) without regular restarts and any convex assumption. Under inexact line searches, we prove that the sequences generated by the proposed methods find points that satisfy the first-order necessary condition for Pareto-optimality. Finally, numerical experiments illustrating the practical behavior of the methods are presented.},
  archive      = {J_COAP},
  author       = {He, Qing-Rui and Chen, Chun-Rong and Li, Sheng-Jie},
  doi          = {10.1007/s10589-023-00508-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {457-489},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Spectral conjugate gradient methods for vector optimization problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated proximal gradient method for multiobjective
optimization. <em>COAP</em>, <em>86</em>(2), 421–455. (<a
href="https://doi.org/10.1007/s10589-023-00497-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an accelerated proximal gradient method for multiobjective optimization, in which each objective function is the sum of a continuously differentiable, convex function and a closed, proper, convex function. Extending first-order methods for multiobjective problems without scalarization has been widely studied, but providing accelerated methods with accurate proofs of convergence rates remains an open problem. Our proposed method is a multiobjective generalization of the accelerated proximal gradient method, also known as the Fast Iterative Shrinkage-Thresholding Algorithm, for scalar optimization. The key to this successful extension is solving a subproblem with terms exclusive to the multiobjective case. This approach allows us to demonstrate the global convergence rate of the proposed method ( $$O(1 / k^2)$$ ), using a merit function to measure the complexity. Furthermore, we present an efficient way to solve the subproblem via its dual representation, and we confirm the validity of the proposed method through some numerical experiments.},
  archive      = {J_COAP},
  author       = {Tanabe, Hiroki and Fukuda, Ellen H. and Yamashita, Nobuo},
  doi          = {10.1007/s10589-023-00497-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {421-455},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated proximal gradient method for multiobjective optimization},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GLISp-r: A preference-based optimization algorithm with
convergence guarantees. <em>COAP</em>, <em>86</em>(1), 383–420. (<a
href="https://doi.org/10.1007/s10589-023-00491-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preference-based optimization algorithms are iterative procedures that seek the optimal calibration of a decision vector based only on comparisons between couples of different tunings. At each iteration, a human decision-maker expresses a preference between two calibrations (samples), highlighting which one, if any, is better than the other. The optimization procedure must use the observed preferences to find the tuning of the decision vector that is most preferred by the decision-maker, while also minimizing the number of comparisons. In this work, we formulate the preference-based optimization problem from a utility theory perspective. Then, we propose GLISp-r, an extension of a recent preference-based optimization procedure called GLISp. The latter uses a Radial Basis Function surrogate to describe the tastes of the decision-maker. Iteratively, GLISp proposes new samples to compare with the best calibration available by trading off exploitation of the surrogate model and exploration of the decision space. In GLISp-r, we propose a different criterion to use when looking for new candidate samples that is inspired by MSRS, a popular procedure in the black-box optimization framework. Compared to GLISp, GLISp-r is less likely to get stuck on local optima of the preference-based optimization problem. We motivate this claim theoretically, with a proof of global convergence, and empirically, by comparing the performances of GLISp and GLISp-r on several benchmark optimization problems.},
  archive      = {J_COAP},
  author       = {Previtali, Davide and Mazzoleni, Mirko and Ferramosca, Antonio and Previdi, Fabio},
  doi          = {10.1007/s10589-023-00491-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {383-420},
  shortjournal = {Comput. Optim. Appl.},
  title        = {GLISp-r: A preference-based optimization algorithm with convergence guarantees},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A smoothing newton method based on the modulus equation for
a class of weakly nonlinear complementarity problems. <em>COAP</em>,
<em>86</em>(1), 345–381. (<a
href="https://doi.org/10.1007/s10589-023-00482-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By equivalently transforming a class of weakly nonlinear complementarity problems into a modulus equation, and introducing a smoothing approximation of the absolute value function, a smoothing Newton method is established for solving the weakly nonlinear complementarity problem. Under some mild assumptions, the proposed method is shown to possess global convergence and locally quadratical convergence. Especially, the global convergence results do not need a priori existence of an accumulation point with some suitable conditions. Numerical results are given to show the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Huang, Baohua and Li, Wen},
  doi          = {10.1007/s10589-023-00482-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {345-381},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A smoothing newton method based on the modulus equation for a class of weakly nonlinear complementarity problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence of an asynchronous block-coordinate
forward-backward algorithm for convex composite optimization.
<em>COAP</em>, <em>86</em>(1), 303–344. (<a
href="https://doi.org/10.1007/s10589-023-00489-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the convergence properties of a randomized block-coordinate descent algorithm for the minimization of a composite convex objective function, where the block-coordinates are updated asynchronously and randomly according to an arbitrary probability distribution. We prove that the iterates generated by the algorithm form a stochastic quasi-Fejér sequence and thus converge almost surely to a minimizer of the objective function. Moreover, we prove a general sublinear rate of convergence in expectation for the function values and a linear rate of convergence in expectation under an error bound condition of Tseng type. Under the same condition strong convergence of the iterates is provided as well as their linear convergence rate.},
  archive      = {J_COAP},
  author       = {Traoré, Cheik and Salzo, Saverio and Villa, Silvia},
  doi          = {10.1007/s10589-023-00489-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {303-344},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of an asynchronous block-coordinate forward-backward algorithm for convex composite optimization},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Average curvature FISTA for nonconvex smooth composite
optimization problems. <em>COAP</em>, <em>86</em>(1), 275–302. (<a
href="https://doi.org/10.1007/s10589-023-00490-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A previous authors’ paper introduces an accelerated composite gradient (ACG) variant, namely AC-ACG, for solving nonconvex smooth composite optimization (N-SCO) problems. In contrast to other ACG variants, AC-ACG estimates the local upper curvature of the N-SCO problem by using the average of the observed upper-Lipschitz curvatures obtained during the previous iterations, and uses this estimation and two composite resolvent evaluations to compute the next iterate. This paper presents an alternative FISTA-type ACG variant, namely AC-FISTA, which has the following additional features: (i) it performs an average of one composite resolvent evaluation per iteration; and (ii) it estimates the local upper curvature by using the average of the previously observed upper (instead of upper-Lipschitz) curvatures. These two properties acting together yield a practical AC-FISTA variant which substantially outperforms earlier ACG variants, including the AC-ACG variants discussed in the aforementioned authors’ paper.},
  archive      = {J_COAP},
  author       = {Liang, Jiaming and Monteiro, Renato D. C.},
  doi          = {10.1007/s10589-023-00490-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {275-302},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Average curvature FISTA for nonconvex smooth composite optimization problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient global algorithm for indefinite separable
quadratic knapsack problems with box constraints. <em>COAP</em>,
<em>86</em>(1), 241–273. (<a
href="https://doi.org/10.1007/s10589-023-00488-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The indefinite separable quadratic knapsack problem (ISQKP) with box constraints is known to be NP-hard. In this paper, we propose a new branch-and-bound algorithm based on a convex envelope relaxation that can be efficiently solved by exploiting its special dual structure. Benefiting from a new branching strategy, the complexity of the proposed algorithm is quadratic in terms of the number of variables when the number of negative eigenvalues in the objective function of ISQKP is fixed. We then improve the proposed algorithm for the case that ISQKP has symmetric structures. The improvement is achieved by constructing tight convex relaxations based on the aggregate functions. Numerical experiments on large-size instances show that the proposed algorithm is much faster than Gurobi and CPLEX. It turns out that the proposed algorithm can solve the instances of size up to three million in less than twenty seconds on average and its improved version is still very efficient for problems with symmetric structures.},
  archive      = {J_COAP},
  author       = {Li, Shaoze and Deng, Zhibin and Lu, Cheng and Wu, Junhao and Dai, Jinyu and Wang, Qiao},
  doi          = {10.1007/s10589-023-00488-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {241-273},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient global algorithm for indefinite separable quadratic knapsack problems with box constraints},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective algorithms for separable nonconvex quadratic
programming with one quadratic and box constraints. <em>COAP</em>,
<em>86</em>(1), 199–240. (<a
href="https://doi.org/10.1007/s10589-023-00485-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider in this paper a separable and nonconvex quadratic program (QP) with a quadratic constraint and a box constraint that arises from application in optimal portfolio deleveraging (OPD) in finance and is known to be NP-hard. We first propose an improved Lagrangian breakpoint search algorithm based on the secant approach (called ILBSSA) for this nonconvex QP, and show that it converges to either a suboptimal solution or a global solution of the problem. We then develop a successive convex optimization (SCO) algorithm to improve the quality of suboptimal solutions derived from ILBSSA, and show that it converges to a KKT point of the problem. Second, we develop a new global algorithm (called ILBSSA-SCO-BB), which integrates the ILBSSA and SCO methods, convex relaxation and branch-and-bound framework, to find a globally optimal solution to the underlying QP within a pre-specified $$\epsilon $$ -tolerance. We establish the convergence of the ILBSSA-SCO-BB algorithm and its complexity. Preliminary numerical results are reported to demonstrate the effectiveness of the ILBSSA-SCO-BB algorithm in finding a globally optimal solution to large-scale OPD instances.},
  archive      = {J_COAP},
  author       = {Luo, Hezhi and Zhang, Xianye and Wu, Huixian and Xu, Weiqiang},
  doi          = {10.1007/s10589-023-00485-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {199-240},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Effective algorithms for separable nonconvex quadratic programming with one quadratic and box constraints},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On solving difference of convex functions programs with
linear complementarity constraints. <em>COAP</em>, <em>86</em>(1),
163–197. (<a href="https://doi.org/10.1007/s10589-023-00487-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a large class of Mathematical Programs with Linear Complementarity Constraints which minimizes a continuously differentiable DC function (Difference of Convex functions) on a set defined by linear constraints and linear complementarity constraints, named Difference of Convex functions programs with Linear Complementarity Constraints. Using exact penalty techniques, we reformulate it, via four penalty functions, as standard Difference of Convex functions programs. The difference of convex functions algorithm (DCA), an efficient approach in nonconvex programming framework, is then developed to solve the resulting problems. Two particular cases are considered: quadratic problems with linear complementarity constraints and asymmetric eigenvalue complementarity problems. Numerical experiments are performed on several benchmark data, and the results show the effectiveness and the superiority of the proposed approaches comparing with some standard methods.},
  archive      = {J_COAP},
  author       = {Le Thi, Hoai An and Nguyen, Thi Minh Tam and Dinh, Tao Pham},
  doi          = {10.1007/s10589-023-00487-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {163-197},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On solving difference of convex functions programs with linear complementarity constraints},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On proximal augmented lagrangian based decomposition methods
for dual block-angular convex composite programming problems.
<em>COAP</em>, <em>86</em>(1), 117–161. (<a
href="https://doi.org/10.1007/s10589-023-00493-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design inexact proximal augmented Lagrangian based decomposition methods for convex composite programming problems with dual block-angular structures. Our methods are particularly well suited for convex quadratic programming problems arising from stochastic programming models. The algorithmic framework is based on the application of the abstract inexact proximal ADMM framework developed in [Chen, Sun, Toh, Math. Prog. 161:237–270] to the dual of the target problem, as well as the application of the recently developed symmetric Gauss-Seidel decomposition theorem for solving a proximal multi-block convex composite quadratic programming problem. The key issues in our algorithmic design are firstly in designing appropriate proximal terms to decompose the computation of the dual variable blocks of the target problem to make the subproblems in each iteration easier to solve, and secondly to develop novel numerical schemes to solve the decomposed subproblems efficiently. Our inexact augmented Lagrangian based decomposition methods have guaranteed convergence. We present an application of the proposed algorithms to the doubly nonnegative relaxations of uncapacitated facility location problems, as well as to two-stage stochastic optimization problems. We conduct numerous numerical experiments to evaluate the performance of our method against state-of-the-art solvers such as Gurobi and MOSEK. Moreover, our proposed algorithms also compare favourably to the well-known progressive hedging algorithm of Rockafellar and Wets.},
  archive      = {J_COAP},
  author       = {Ding, Kuang-Yu and Lam, Xin-Yee and Toh, Kim-Chuan},
  doi          = {10.1007/s10589-023-00493-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {117-161},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On proximal augmented lagrangian based decomposition methods for dual block-angular convex composite programming problems},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating stochastic sequential quadratic programming for
equality constrained optimization using predictive variance reduction.
<em>COAP</em>, <em>86</em>(1), 79–116. (<a
href="https://doi.org/10.1007/s10589-023-00483-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a stochastic method for solving equality constrained optimization problems that utilizes predictive variance reduction. Specifically, we develop a method based on the sequential quadratic programming paradigm that employs variance reduction in the gradient approximations. Under reasonable assumptions, we prove that a measure of first-order stationarity evaluated at the iterates generated by our proposed algorithm converges to zero in expectation from arbitrary starting points, for both constant and adaptive step size strategies. Finally, we demonstrate the practical performance of our proposed algorithm on constrained binary classification problems that arise in machine learning.},
  archive      = {J_COAP},
  author       = {Berahas, Albert S. and Shi, Jiahao and Yi, Zihong and Zhou, Baoyu},
  doi          = {10.1007/s10589-023-00483-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {79-116},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerating stochastic sequential quadratic programming for equality constrained optimization using predictive variance reduction},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recycling basic columns of the splitting preconditioner in
interior point methods. <em>COAP</em>, <em>86</em>(1), 49–78. (<a
href="https://doi.org/10.1007/s10589-023-00492-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretical results and numerical experiments show that the linear systems originating from the last iterations of interior point methods (IPM) are very ill-conditioned. For this reason, preconditioners are necessary to approach this problem. In addition to that, in large-scale problems, the use of iterative methods and implicit preconditioners is essential because we only compute matrix–vector multiplications. Preconditioners with a lower computational cost than the splitting preconditioner only have good performance in the initial iterations of the IPM, so this preconditioner has become very important in the last iterations. The study of improvements thereof is justified. This paper studies the variation of the diagonal matrix D entries that appear in the linear systems to be solved to try to reuse or recycle some linearly independent columns of the splitting preconditioner base previously computed in a given IPM iteration to build another basis in the next one. It is justified by the fact that a subset of linearly independent columns remains linearly independent, and from that available subset, one may complete the number of columns necessary to form the new base. The numerical results show that the column recycling proposal improves the speed and robustness of the original approach for a test set, especially for large-scale problems.},
  archive      = {J_COAP},
  author       = {Castro, Cecilia Orellana and Heredia, Manolo Rodriguez and Oliveira, Aurelio R. L.},
  doi          = {10.1007/s10589-023-00492-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {49-78},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Recycling basic columns of the splitting preconditioner in interior point methods},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A structured modified newton approach for solving systems of
nonlinear equations arising in interior-point methods for quadratic
programming. <em>COAP</em>, <em>86</em>(1), 1–48. (<a
href="https://doi.org/10.1007/s10589-023-00486-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus in this work is on interior-point methods for inequality-constrained quadratic programs, and particularly on the system of nonlinear equations to be solved for each value of the barrier parameter. Newton iterations give high quality solutions, but we are interested in modified Newton systems that are computationally less expensive at the expense of lower quality solutions. We propose a structured modified Newton approach where each modified Jacobian is composed of a previous Jacobian, plus one low-rank update matrix per succeeding iteration. Each update matrix is, for a given rank, chosen such that the distance to the Jacobian at the current iterate is minimized, in both 2-norm and Frobenius norm. The approach is structured in the sense that it preserves the nonzero pattern of the Jacobian. The choice of update matrix is supported by results in an ideal theoretical setting. We also produce numerical results with a basic interior-point implementation to investigate the practical performance within and beyond the theoretical framework. In order to improve performance beyond the theoretical framework, we also motivate and construct two heuristics to be added to the method.},
  archive      = {J_COAP},
  author       = {Ek, David and Forsgren, Anders},
  doi          = {10.1007/s10589-023-00486-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-48},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A structured modified newton approach for solving systems of nonlinear equations arising in interior-point methods for quadratic programming},
  volume       = {86},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A communication-efficient and privacy-aware distributed
algorithm for sparse PCA. <em>COAP</em>, <em>85</em>(3), 1033–1072. (<a
href="https://doi.org/10.1007/s10589-023-00481-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse principal component analysis (PCA) improves interpretability of the classic PCA by introducing sparsity into the dimension-reduction process. Optimization models for sparse PCA, however, are generally non-convex, non-smooth and more difficult to solve, especially on large-scale datasets requiring distributed computation over a wide network. In this paper, we develop a distributed and centralized algorithm called DSSAL1 for sparse PCA that aims to achieve low communication overheads by adapting a newly proposed subspace-splitting strategy to accelerate convergence. Theoretically, convergence to stationary points is established for DSSAL1. Extensive numerical results show that DSSAL1 requires far fewer rounds of communication than state-of-the-art peer methods. In addition, we make the case that since messages exchanged in DSSAL1 are well-masked, the possibility of private-data leakage in DSSAL1 is much lower than in some other distributed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Lei and Liu, Xin and Zhang, Yin},
  doi          = {10.1007/s10589-023-00481-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1033-1072},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A communication-efficient and privacy-aware distributed algorithm for sparse PCA},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DC semidefinite programming and cone constrained DC
optimization II: Local search methods. <em>COAP</em>, <em>85</em>(3),
993–1031. (<a href="https://doi.org/10.1007/s10589-023-00479-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The second part of our study is devoted to a detailed convergence analysis of two extensions of the well-known DCA method for solving DC (Difference of Convex functions) optimization problems to the case of general cone constrained DC optimization problems. We study the global convergence of the DCA for cone constrained problems and present a comprehensive analysis of a version of the DCA utilizing exact penalty functions. In particular, we study the exactness property of the penalized convex subproblems and provide two types of sufficient conditions for the convergence of the exact penalty method to a feasible and critical point of a cone constrained DC optimization problem from an infeasible starting point. In the numerical section of this work, the exact penalty DCA is applied to the problem of computing compressed modes for variational problems and the sphere packing problem on Grassmannian.},
  archive      = {J_COAP},
  author       = {Dolgopolik, M. V.},
  doi          = {10.1007/s10589-023-00479-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {993-1031},
  shortjournal = {Comput. Optim. Appl.},
  title        = {DC semidefinite programming and cone constrained DC optimization II: Local search methods},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence rate estimates for penalty methods revisited.
<em>COAP</em>, <em>85</em>(3), 973–992. (<a
href="https://doi.org/10.1007/s10589-023-00476-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the classical quadratic penalty, it is known that the distance from the solution of the penalty subproblem to the solution of the original problem is at worst inversely proportional to the value of the penalty parameter under the linear independence constraint qualification, strict complementarity, and the second-order sufficient optimality conditions. Moreover, using solutions of the penalty subproblem, one can obtain certain useful Lagrange multipliers estimates whose distance to the optimal ones is also at least inversely proportional to the value of the parameter. We show that the same properties hold more generally, namely, under the (weaker) strict Mangasarian–Fromovitz constraint qualification and second-order sufficiency (and without strict complementarity). Moreover, under the linear independence constraint qualification and strong second-order sufficiency (also without strict complementarity), we demonstrate local uniqueness and Lipschitz continuity of stationary points of penalty subproblems. In addition, those results follow from the analysis of general power penalty functions, of which quadratic penalty is a special case.},
  archive      = {J_COAP},
  author       = {Izmailov, A. F. and Solodov, M. V.},
  doi          = {10.1007/s10589-023-00476-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {973-992},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence rate estimates for penalty methods revisited},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inexact penalty decomposition methods for optimization
problems with geometric constraints. <em>COAP</em>, <em>85</em>(3),
937–971. (<a href="https://doi.org/10.1007/s10589-023-00475-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a theoretical and numerical investigation of a penalty decomposition scheme for the solution of optimization problems with geometric constraints. In particular, we consider some situations where parts of the constraints are nonconvex and complicated, like cardinality constraints, disjunctive programs, or matrix problems involving rank constraints. By a variable duplication and decomposition strategy, the method presented here explicitly handles these difficult constraints, thus generating iterates which are feasible with respect to them, while the remaining (standard and supposingly simple) constraints are tackled by sequential penalization. Inexact optimization steps are proven sufficient for the resulting algorithm to work, so that it is employable even with difficult objective functions. The current work is therefore a significant generalization of existing papers on penalty decomposition methods. On the other hand, it is related to some recent publications which use an augmented Lagrangian idea to solve optimization problems with geometric constraints. Compared to these methods, the decomposition idea is shown to be numerically superior since it allows much more freedom in the choice of the subproblem solver, and since the number of certain (possibly expensive) projection steps is significantly less. Extensive numerical results on several highly complicated classes of optimization problems in vector and matrix spaces indicate that the current method is indeed very efficient to solve these problems.},
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Lapucci, Matteo},
  doi          = {10.1007/s10589-023-00475-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {937-971},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact penalty decomposition methods for optimization problems with geometric constraints},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Riemannian optimization on unit sphere with p-norm and its
applications. <em>COAP</em>, <em>85</em>(3), 897–935. (<a
href="https://doi.org/10.1007/s10589-023-00477-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study deals with Riemannian optimization on the unit sphere in terms of p-norm with general $$p&gt; 1$$ . As a Riemannian submanifold of the Euclidean space, the geometry of the sphere with p-norm is investigated, and several geometric tools used for Riemannian optimization, such as retractions and vector transports, are proposed and analyzed. Applications to Riemannian optimization on the sphere with nonnegative constraints and $$\textit{L}_{\textit{p}}$$ -regularization-related optimization are also discussed. As practical examples, the former includes nonnegative principal component analysis, and the latter is closely related to the Lasso regression and box-constrained problems. Numerical experiments verify that Riemannian optimization on the sphere with p-norm has substantial potential for such applications, and the proposed framework provides a theoretical basis for such optimization.},
  archive      = {J_COAP},
  author       = {Sato, Hiroyuki},
  doi          = {10.1007/s10589-023-00477-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {897-935},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian optimization on unit sphere with p-norm and its applications},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional gradient method for vector optimization.
<em>COAP</em>, <em>85</em>(3), 857–896. (<a
href="https://doi.org/10.1007/s10589-023-00478-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a conditional gradient method for solving constrained vector optimization problems with respect to a partial order induced by a closed, convex and pointed cone with nonempty interior. When the partial order under consideration is the one induced by the non-negative orthant, we regain the method for multiobjective optimization recently proposed by Assunção et al. (Comput Optim Appl 78(3):741–768, 2021). In our method, the construction of the auxiliary subproblem is based on the well-known oriented distance function. Three different types of step size strategies (Armijo, adaptative and nonmonotone) are considered. Without convexity assumption related to the objective function, we obtain the stationarity of accumulation points of the sequences produced by the proposed method equipped with the Armijo or the nonmonotone step size rule. To obtain the convergence result of the method with the adaptative step size strategy, we introduce a useful cone convexity condition which allows us to circumvent the intricate question of the Lipschitz continuity of Jocabian for the objective function. This condition helps us to generalize the classical descent lemma to the vector optimization case. Under convexity assumption for the objective function, it is proved that all accumulation points of any generated sequences obtained by our method are weakly efficient solutions. Numerical experiments illustrating the practical behavior of the methods are presented.},
  archive      = {J_COAP},
  author       = {Chen, Wang and Yang, Xinmin and Zhao, Yong},
  doi          = {10.1007/s10589-023-00478-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {857-896},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Conditional gradient method for vector optimization},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence of derivative-free nonmonotone direct search
methods for unconstrained and box-constrained mixed-integer
optimization. <em>COAP</em>, <em>85</em>(3), 821–856. (<a
href="https://doi.org/10.1007/s10589-023-00469-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a class of nonmonotone Direct Search Methods that converge to stationary points of unconstrained and boxed constrained mixed-integer optimization problems. A new concept is introduced: the quasi-descent direction. A point x is stationary on a set of search directions if there exists no feasible qdd on that set. The method does not require the computation of derivatives nor the explicit manipulation of asymptotically dense matrices. Preliminary numerical experiments carried out on small to medium problems are encouraging.},
  archive      = {J_COAP},
  author       = {García Palomares, Ubaldo M.},
  doi          = {10.1007/s10589-023-00469-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {821-856},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of derivative-free nonmonotone direct search methods for unconstrained and box-constrained mixed-integer optimization},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Increasing reliability of price signals in long term energy
management problems. <em>COAP</em>, <em>85</em>(3), 787–820. (<a
href="https://doi.org/10.1007/s10589-023-00480-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining reliable price indicators in the long-term is fundamental for optimal management problems in the energy sector. In hydro-dominated systems, the random components of rain and snow that arrive to the reservoirs have a significant impact on the interaction of the low-cost technology of hydro-generation with more expensive ones. The sample employed to discretize uncertainty changes certain Lagrange multipliers in the corresponding optimization problem that represent a marginal cost for the power system and, therefore, changes the price signals. The effect of sampling in yielding different price indicators can be observed even when running twice the same code on the same computer. Although such values are statistically correct, the variability on the dual output puts at stake economic analyses based on marginal prices. To address this issue, we propose a dual regularization that yields sample-insensitive indicators for a two-stage stochastic model. It is shown that the approach provides the minimal-norm multiplier of the energy management problem in the limit, when certain parameter is driven to zero. The new method is implemented in a rolling horizon mode for a real-life case, representing the Northern European energy system over a period of one year with hourly discretization. When compared to SDDP, an established method in the area, the approach yields a significant reduction in the variance of the optimal Lagrange multipliers used to compute the prices, with respect to different samples.},
  archive      = {J_COAP},
  author       = {Erbs, Guillaume and Lage, Clara and Sagastizábal, Claudia and Solodov, Mikhail},
  doi          = {10.1007/s10589-023-00480-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {787-820},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Increasing reliability of price signals in long term energy management problems},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Greedy PSB methods with explicit superlinear convergence.
<em>COAP</em>, <em>85</em>(3), 753–786. (<a
href="https://doi.org/10.1007/s10589-023-00495-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Rodomanov and Nesterov proposed a class of greedy quasi-Newton methods and established the first explicit local superlinear convergence result for Quasi-Newton type methods. In this paper, we study a variant of Powell-Symmetric-Broyden (PSB) updates based on the greedy strategy. Firstly, we give explicit condition-number-free superlinear convergence rates of proposed greedy PSB methods. Secondly, we prove the global convergence of greedy PSB methods by applying the trust-region framework. One advantage of this result is that the initial Hessian approximation can be chosen arbitrarily. Thirdly, we analyze the behaviour of the randomized PSB method, that selects the direction randomly from any spherical symmetry distribution. Finally, preliminary numerical experiments illustrate the efficiency of proposed PSB methods compared with the standard SR1 method and PSB method. Our results are given under the assumption that the objective function is a strongly convex function, and its gradient and Hessian are Lipschitz continuous.},
  archive      = {J_COAP},
  author       = {Ji, Zhen-Yuan and Dai, Yu-Hong},
  doi          = {10.1007/s10589-023-00495-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {753-786},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Greedy PSB methods with explicit superlinear convergence},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust weber problem with uncertain demand.
<em>COAP</em>, <em>85</em>(3), 705–752. (<a
href="https://doi.org/10.1007/s10589-023-00470-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weber problem is an important model in facility location field, and it can be modeled as a stochastic problem when the future demand of customers is uncertain. By minimizing the maximal expectation of the objective on an ambiguity set, the distributionally robust optimization (DRO) can utilize the valuable information from historical data and thus it has become an attractive formulation for stochastic problems. In this paper, an extended moment-based DRO formulation and a polynomial-time algorithm are contributed to solving the Weber problem with uncertain demand. Specifically, by constructing a new ambiguity set, which is proved to contain the true distribution with high probability, an extended DRO formulation allowing positive semidefinite covariance matrix is first built for general stochastic problems. To obtain a more robust solution, the Weber problem is then reformulated into an equivalent stochastic variational inequality (SVI). Following the extended DRO formulation, a distributionally robust Weber problem (DRWP) is further developed with minimizing the expectation of a residual function of the SVI. The DRWP can be transformed to a semidefinite program (SDP) with an undesired exponential number of constraints. Through the adoption of a plane decomposition technique, a simple algorithm is proposed to solve the resulted SDP and obtain the optimal solution to DRWP in polynomial time. Some preliminary numerical results demonstrate the effectiveness of the proposed DRWP and algorithm.},
  archive      = {J_COAP},
  author       = {Gu, Yan and Jiang, Jianlin and Zhang, Shun},
  doi          = {10.1007/s10589-023-00470-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {705-752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributionally robust weber problem with uncertain demand},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quadratic regularization methods with finite-difference
gradient approximations. <em>COAP</em>, <em>85</em>(3), 683–703. (<a
href="https://doi.org/10.1007/s10589-022-00373-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two quadratic regularization methods with finite-difference gradient approximations for smooth unconstrained optimization problems. One method is based on forward finite-difference gradients, while the other is based on central finite-difference gradients. In both methods, the accuracy of the gradient approximations and the regularization parameter in the quadratic models are jointly adjusted using a nonmonotone acceptance condition for the trial points. When the objective function is bounded from below and has Lipschitz continuous gradient, it is shown that the method based on forward finite-difference gradients needs at most $${\mathcal{O}}\left( n\epsilon ^{-2}\right) $$ function evaluations to generate a $$\epsilon $$ -approximate stationary point, where n is the problem dimension. Under the additional assumption that the Hessian of the objective is Lipschitz continuous, an evaluation complexity bound of the same order is proved for the method based on central finite-difference gradients. Numerical results are also presented. They confirm the theoretical findings and illustrate the relative efficiency of the proposed methods.},
  archive      = {J_COAP},
  author       = {Grapiglia, Geovani Nunes},
  doi          = {10.1007/s10589-022-00373-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {683-703},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quadratic regularization methods with finite-difference gradient approximations},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface to the 5th brazil–china symposium on applied and
computational mathematics. <em>COAP</em>, <em>85</em>(3), 681–682. (<a
href="https://doi.org/10.1007/s10589-023-00499-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Yuan, Jinyun},
  doi          = {10.1007/s10589-023-00499-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {681-682},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface to the 5th Brazil–China symposium on applied and computational mathematics},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stochastic variance-reduced accelerated primal-dual method
for finite-sum saddle-point problems. <em>COAP</em>, <em>85</em>(2),
653–679. (<a href="https://doi.org/10.1007/s10589-023-00472-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a variance-reduced primal-dual algorithm with Bregman distance functions for solving convex-concave saddle-point problems with finite-sum structure and nonbilinear coupling function. This type of problem typically arises in machine learning and game theory. Based on some standard assumptions, the algorithm is proved to converge with oracle complexities of $${\mathcal {O}}(\frac{\sqrt{n}}{\epsilon })$$ and $${\mathcal {O}}(\frac{n}{\sqrt{\epsilon }}+\frac{1}{\epsilon ^{1.5}})$$ using constant and non-constant parameters, respectively where n is the number of function components. Compared with existing methods, our framework yields a significant improvement over the number of required primal-dual gradient samples to achieve $$\epsilon $$ -accuracy of the primal-dual gap. We also present numerical experiments to showcase the superior performance of our method compared with state-of-the-art methods.},
  archive      = {J_COAP},
  author       = {Yazdandoost Hamedani, Erfan and Jalilzadeh, Afrooz},
  doi          = {10.1007/s10589-023-00472-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {653-679},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stochastic variance-reduced accelerated primal-dual method for finite-sum saddle-point problems},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partially symmetric tensor structure preserving rank-r
approximation via BFGS algorithm. <em>COAP</em>, <em>85</em>(2),
621–652. (<a href="https://doi.org/10.1007/s10589-023-00471-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that many tensor data have symmetric or partially symmetric structure and structural tensors have structure preserving Candecomp/Parafac (CP) decompositions. However, the well-known alternating least squares (ALS) method cannot realize structure preserving CP decompositions of tensors. Hence, in this paper, we consider numerical problems of structure preserving rank-R approximation and structure preserving CP decomposition of partially symmetric tensors. For the problem of structure preserving rank-R approximation, we derive the gradient formula of the objective function, obtain BFGS iterative formulas, propose a BFGS algorithm for positive partially symmetric rank-R approximation, and discuss the convergence of the algorithm. For the problem of structure preserving CP decomposition, we give a necessary condition for partially symmetric tensors with even orders to have positive partially symmetric CP decompositions, and design a general partially symmetric rank-R approximation algorithm. Finally, some numerical examples are given. Through numerical examples, we find that if a tensor has a positive partially symmetric CP decomposition then its partially symmetric rank CP decomposition must be a positive CP decomposition. In addition, we compare the BFGS algorithm proposed in this paper with the standard CP-ALS method. Numerical examples show that the BFGS algorithm has better stability and faster computing speed than CP-ALS algorithm.},
  archive      = {J_COAP},
  author       = {Chen, Ciwen and Ni, Guyan and Yang, Bo},
  doi          = {10.1007/s10589-023-00471-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {621-652},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Partially symmetric tensor structure preserving rank-R approximation via BFGS algorithm},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly iteratively reweighted algorithm for constrained
compressed sensing models. <em>COAP</em>, <em>85</em>(2), 583–619. (<a
href="https://doi.org/10.1007/s10589-023-00468-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithmic framework for constrained compressed sensing models that admit nonconvex sparsity-inducing regularizers including the log-penalty function as objectives, and nonconvex loss functions such as the Cauchy loss function and the Tukey biweight loss function in the constraint. Our framework employs iteratively reweighted $$\ell _1$$ and $$\ell _2$$ schemes to construct subproblems that can be efficiently solved by well-developed solvers for basis pursuit denoising such as SPGL1 by van den Berg and Friedlander (SIAM J Sci Comput 31:890-912, 2008). We propose a new termination criterion for the subproblem solvers that allows them to return an infeasible solution, with a suitably constructed feasible point satisfying a descent condition. The feasible point construction step is the key for establishing the well-definedness of our proposed algorithm, and we also prove that any accumulation point of this sequence of feasible points is a stationary point of the constrained compressed sensing model, under suitable assumptions. Finally, we compare numerically our algorithm (with subproblems solved by SPGL1 or the alternating direction method of multipliers) against the SCP $$_\textrm{ls}$$ in Yu et al. (SIAM J Optim 31: 2024-2054, 2021) on solving constrained compressed sensing models with the log-penalty function as the objective and the Cauchy loss function in the constraint, for badly scaled measurement matrices. Our computational results show that our approaches return solutions with better recovery errors, and are always faster.},
  archive      = {J_COAP},
  author       = {Sun, Shuqin and Pong, Ting Kei},
  doi          = {10.1007/s10589-023-00468-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {583-619},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Doubly iteratively reweighted algorithm for constrained compressed sensing models},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semismooth newton based dual proximal point algorithm for
maximum eigenvalue problem. <em>COAP</em>, <em>85</em>(2), 547–582. (<a
href="https://doi.org/10.1007/s10589-023-00467-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum eigenvalue problem is to minimize the maximum eigenvalue function over an affine subspace in a symmetric matrix space, which has many applications in structural engineering, such as combinatorial optimization, control theory and structural design. Based on classical analysis of proximal point (Ppa) algorithm and semismooth analysis of nonseparable spectral operator, we propose an efficient semismooth Newton based dual proximal point (Ssndppa) algorithm to solve the maximum eigenvalue problem, in which an inexact semismooth Newton (Ssn) algorithm is applied to solve inner subproblem of the dual proximal point (d-Ppa) algorithm. Global convergence and locally asymptotically superlinear convergence of the d-Ppa algorithm are established under very mild conditions, and fast superlinear or even quadratic convergence of the Ssn algorithm is obtained when the primal constraint nondegeneracy condition holds for the inner subproblem. Computational costs of the Ssn algorithm for solving the inner subproblem can be reduced by fully exploiting low-rank or high-rank property of a matrix. Numerical experiments on max-cut problems and randomly generated maximum eigenvalue optimization problems demonstrate that the Ssndppa algorithm substantially outperforms the Sdpnal+ solver and several state-of-the-art first-order algorithms.},
  archive      = {J_COAP},
  author       = {Liu, Yong-Jin and Yu, Jing},
  doi          = {10.1007/s10589-023-00467-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {547-582},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A semismooth newton based dual proximal point algorithm for maximum eigenvalue problem},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated inexact dampened augmented lagrangian method
for linearly-constrained nonconvex composite optimization problems.
<em>COAP</em>, <em>85</em>(2), 509–545. (<a
href="https://doi.org/10.1007/s10589-023-00464-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes and analyzes an accelerated inexact dampened augmented Lagrangian (AIDAL) method for solving linearly-constrained nonconvex composite optimization problems. Each iteration of the AIDAL method consists of: (i) inexactly solving a dampened proximal augmented Lagrangian (AL) subproblem by calling an accelerated composite gradient (ACG) subroutine; (ii) applying a dampened and under-relaxed Lagrange multiplier update; and (iii) using a novel test to check whether the penalty parameter of the AL function should be increased. Under several mild assumptions involving the dampening factor and the under-relaxation constant, it is shown that the AIDAL method generates an approximate stationary point of the constrained problem in $$\mathcal{O}(\varepsilon ^{-5/2}\log \varepsilon ^{-1})$$ iterations of the ACG subroutine, for a given tolerance $$\varepsilon &gt;0$$ . Numerical experiments are also given to show the computational efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Kong, Weiwei and Monteiro, Renato D. C.},
  doi          = {10.1007/s10589-023-00464-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {509-545},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated inexact dampened augmented lagrangian method for linearly-constrained nonconvex composite optimization problems},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal control of the stationary kirchhoff equation.
<em>COAP</em>, <em>85</em>(2), 479–508. (<a
href="https://doi.org/10.1007/s10589-023-00463-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an optimal control problem for the steady-state Kirchhoff equation, a prototype for nonlocal partial differential equations, different from fractional powers of closed operators. Existence and uniqueness of solutions of the state equation, existence of global optimal solutions, differentiability of the control-to-state map and first-order necessary optimality conditions are established. The aforementioned results require the controls to be functions in $$H^1$$ and subject to pointwise lower and upper bounds. In order to obtain the Newton differentiability of the optimality conditions, we employ a Moreau–Yosida-type penalty approach to treat the control constraint and study its convergence. The first-order optimality conditions of the regularized problems are shown to be Newton differentiable, and a generalized Newton method is detailed. A discretization of the optimal control problem by piecewise linear finite elements is proposed and numerical results are presented.},
  archive      = {J_COAP},
  author       = {Hashemi, Masoumeh and Herzog, Roland and Surowiec, Thomas M.},
  doi          = {10.1007/s10589-023-00463-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {479-508},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimal control of the stationary kirchhoff equation},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A relaxation-based probabilistic approach for
PDE-constrained optimization under uncertainty with pointwise state
constraints. <em>COAP</em>, <em>85</em>(2), 441–478. (<a
href="https://doi.org/10.1007/s10589-023-00461-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of convex risk-neutral PDE-constrained optimization problems subject to pointwise control and state constraints. Due to the many challenges associated with almost sure constraints on pointwise evaluations of the state, we suggest a relaxation via a smooth functional bound with similar properties to well-known probability constraints. First, we introduce and analyze the relaxed problem, discuss its asymptotic properties, and derive formulae for the gradient the adjoint calculus. We then build on the theoretical results by extending a recently published online convex optimization algorithm (OSA) to the infinite-dimensional setting. Similar to the regret-based analysis of time-varying stochastic optimization problems, we enhance the method further by allowing for periodic restarts at pre-defined epochs. Not only does this allow for larger step sizes, it also proves to be an essential factor in obtaining high-quality solutions in practice. The behavior of the algorithm is demonstrated in a numerical example involving a linear advection–diffusion equation with random inputs. In order to judge the quality of the solution, the results are compared to those arising from a sample average approximation (SAA). This is done first by comparing the resulting cumulative distributions of the objectives at the optimal solution as a function of step numbers and epoch lengths. In addition, we conduct statistical tests to further analyze the behavior of the online algorithm and the quality of its solutions. For a sufficiently large number of steps, the solutions from OSA and SAA lead to random integrands for the objective and penalty functions that appear to be drawn from similar distributions.},
  archive      = {J_COAP},
  author       = {Kouri, Drew P. and Staudigl, Mathias and Surowiec, Thomas M.},
  doi          = {10.1007/s10589-023-00461-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {441-478},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A relaxation-based probabilistic approach for PDE-constrained optimization under uncertainty with pointwise state constraints},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally optimal univariate spline approximations.
<em>COAP</em>, <em>85</em>(2), 409–439. (<a
href="https://doi.org/10.1007/s10589-023-00462-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the problem of computing optimal spline approximations for univariate least-squares splines from a combinatorial optimization perspective. In contrast to most approaches from the literature we aim at globally optimal coefficients as well as a globally optimal placement of a fixed number of knots for a discrete variant of this problem. To achieve this, two different possibilities are developed. The first approach that we present is the formulation of the problem as a mixed-integer quadratically constrained problem, which can be solved using commercial optimization solvers. The second method that we propose is a branch-and-bound algorithm tailored specifically to the combinatorial formulation. We compare our algorithmic approaches empirically on both, real and synthetic curve fitting data sets from the literature. The numerical experiments show that our approach to tackle the least-squares spline approximation problem with free knots is able to compute solutions to problems of realistic sizes within reasonable computing times.},
  archive      = {J_COAP},
  author       = {Mohr, Robert and Coblenz, Maximilian and Kirst, Peter},
  doi          = {10.1007/s10589-023-00462-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {409-439},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Globally optimal univariate spline approximations},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Results for the close-enough traveling salesman problem with
a branch-and-bound algorithm. <em>COAP</em>, <em>85</em>(2), 369–407.
(<a href="https://doi.org/10.1007/s10589-023-00474-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Close-Enough Traveling Salesman Problem is a generalization of the Traveling Salesman Problem that requires a salesman to just get close enough to each customer instead of visiting the exact location of each customer. In this paper, we propose improvements to an existing branch-and-bound (B &amp;B) algorithm for this problem that finds and proves optimality of solutions by examining partial sequences. The proposed improvements include a new search strategy, a simplified branching vertex selection scheme, a method to avoid unnecessary computation, a method to improve the quality of feasible solutions, and a method to reduce the space requirement of the algorithm. Numerical experiments show that the improved B &amp;B algorithm proves optimality faster on some instances, finds good feasible solutions faster than the best known existing algorithm on instances that cannot be solved to optimality, and uses less space during the solving process.},
  archive      = {J_COAP},
  author       = {Zhang, Wenda and Sauppe, Jason J. and Jacobson, Sheldon H.},
  doi          = {10.1007/s10589-023-00474-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {369-407},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Results for the close-enough traveling salesman problem with a branch-and-bound algorithm},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Branch-and-model: A derivative-free global optimization
algorithm. <em>COAP</em>, <em>85</em>(2), 337–367. (<a
href="https://doi.org/10.1007/s10589-023-00466-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel derivative-free global optimization algorithm Branch-and-Model (BAM). The BAM algorithm partitions the search domain dynamically, builds surrogate models around carefully selected evaluated points, and uses these models to exploit local function trends and speed up convergence. For model construction, BAM employs Automated Learning of Algebraic Models (ALAMO). The ALAMO algorithm generates algebraic models of the black-box function using various base functions and selection criteria. BAM’s potentially optimal identification scheme saves computational effort and prevents delays in searching for optimal solutions. The BAM algorithm is guaranteed to converge to the globally optimal function value under mild assumptions. Extensive computational experiments over 500 publicly open-source test problems and one industrially-relevant application show that BAM outperforms state-of-the-art DFO algorithms regardless of problem convexity and smoothness, especially for higher-dimensional problems.},
  archive      = {J_COAP},
  author       = {Ma, Kaiwen and Rios, Luis Miguel and Bhosekar, Atharv and Sahinidis, Nikolaos V. and Rajagopalan, Sreekanth},
  doi          = {10.1007/s10589-023-00466-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {337-367},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Branch-and-model: A derivative-free global optimization algorithm},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-time-level model for mission and flight planning of an
inhomogeneous fleet of unmanned aerial vehicles. <em>COAP</em>,
<em>85</em>(1), 293–335. (<a
href="https://doi.org/10.1007/s10589-023-00450-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the mission and flight planning problem for an inhomogeneous fleet of unmanned aerial vehicles (UAVs). Therein, the mission planning problem of assigning targets to a fleet of UAVs and the flight planning problem of finding optimal flight trajectories between a given set of waypoints are combined into one model and solved simultaneously. Thus, trajectories of an inhomogeneous fleet of UAVs have to be specified such that the sum of waypoint-related scores is maximized, considering technical and environmental constraints. Several aspects of an existing basic model are expanded to achieve a more detailed solution. A two-level time grid approach is presented to smooth the computed trajectories. The three-dimensional mission area can contain convex-shaped restricted airspaces and convex subareas where wind affects the flight trajectories. Furthermore, the flight dynamics are related to the mass change, due to fuel consumption, and the operating range of every UAV is altitude-dependent. A class of benchmark instances for collision avoidance is adapted and expanded to fit our model and we prove an upper bound on its objective value. Finally, the presented features and results are tested and discussed on several test instances using GUROBI as a state-of-the-art numerical solver.},
  archive      = {J_COAP},
  author       = {Schmidt, Johannes and Fügenschuh, Armin},
  doi          = {10.1007/s10589-023-00450-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {293-335},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A two-time-level model for mission and flight planning of an inhomogeneous fleet of unmanned aerial vehicles},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An alternative extrapolation scheme of PDHGM for saddle
point problem with nonlinear function. <em>COAP</em>, <em>85</em>(1),
263–291. (<a href="https://doi.org/10.1007/s10589-023-00453-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Primal-dual hybrid gradient (PDHG) method is a canonical and popular prototype for solving saddle point problem (SPP). However, the nonlinear coupling term in SPP excludes the application of PDHG on far-reaching real-world problems. In this paper, following the seminal work by Valkonen (Inverse Problems 30, 2014), we devise a variant iterative scheme for solving SPP with nonlinear function by exerting an alternative extrapolation procedure. The novel iterative scheme falls exactly into the proximal point algorithmic framework without any residuals, which indicates that the associated inclusion problem is “nearer” to the KKT mapping induced by SPP. Under the metrically regular assumption on KKT mapping, we simplify the local convergence of the proposed method on contractive perspective. Numerical simulations on a PDE-constrained nonlinear inverse problem demonstrate the compelling performance of the proposed method.},
  archive      = {J_COAP},
  author       = {Gao, Ying and Zhang, Wenxing},
  doi          = {10.1007/s10589-023-00453-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {263-291},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An alternative extrapolation scheme of PDHGM for saddle point problem with nonlinear function},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A simultaneous diagonalization based SOCP relaxation for
portfolio optimization with an orthogonality constraint. <em>COAP</em>,
<em>85</em>(1), 247–261. (<a
href="https://doi.org/10.1007/s10589-023-00452-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The portfolio rebalancing with transaction costs plays an important role in both theoretical analyses and commercial applications. This paper studies a standard portfolio problem that is subject to an additional orthogonality constraint guaranteeing that buying and selling a same security do not occur at the same time point. Incorporating the orthogonality constraint into the portfolio problem leads to a quadratic programming problem with linear complementarity constraints. We derive an enhanced simultaneous diagonalization based second order cone programming (ESDSOCP) relaxation by taking advantage of the feature that the objective and constraint matrices are commutative. The ESDSOCP relaxation has lower computational complexity than the semi-definite programming (SDP) relaxation, and it is proved to be as tight as the SDP relaxation. It is worth noting that the original simultaneous diagonalization based second order cone programming relaxation (SDSOCP) is only guaranteed to be as tight as the SDP relaxation on condition that the objective matrix is positive definite. Note that the objective matrix in this paper is positive semidefinite (while not positive definite), thus the ESDSOCP relaxation outperforms the original SDSOCP relaxation. We further design a branch and bound algorithm based on the ESDSOCP relaxation to find the global optimal solution and computational results illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Xu, Zhijun and Zhou, Jing},
  doi          = {10.1007/s10589-023-00452-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {247-261},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A simultaneous diagonalization based SOCP relaxation for portfolio optimization with an orthogonality constraint},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple approximative algorithms for free-support wasserstein
barycenters. <em>COAP</em>, <em>85</em>(1), 213–246. (<a
href="https://doi.org/10.1007/s10589-023-00458-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing Wasserstein barycenters of discrete measures has recently attracted considerable attention due to its wide variety of applications in data science. In general, this problem is NP-hard, calling for practical approximative algorithms. In this paper, we analyze a well-known simple framework for approximating Wasserstein- $${\varvec{p}}$$ barycenters, where we mainly consider the most common case $${\varvec{p}}={\varvec{2}}$$ and $${\varvec{p}}={\varvec{1}}$$ , which is not as well discussed. The framework produces sparse support solutions and shows good numerical results in the free-support setting. Depending on the desired level of accuracy, this requires only $${\varvec{N}}-{\varvec{1}}$$ or $${\varvec{N(N}}-{\varvec{1)/2 }}$$ standard two-marginal optimal transport (OT) computations between the $${\varvec{N}}$$ input measures, respectively, which is fast, memory-efficient and easy to implement using any OT solver as a black box. What is more, these methods yield a relative error of at most $${\varvec{N}}$$ and $${\varvec{2}}$$ , respectively, for both $${\varvec{p}}={\varvec{ 1, 2}}$$ . We show that these bounds are practically sharp. In light of the hardness of the problem, it is not surprising that such guarantees cannot be close to optimality in general. Nevertheless, these error bounds usually turn out to be drastically lower for a given particular problem in practice and can be evaluated with almost no computational overhead, in particular without knowledge of the optimal solution. In our numerical experiments, this guaranteed errors of at most a few percent.},
  archive      = {J_COAP},
  author       = {Lindheim, Johannes von},
  doi          = {10.1007/s10589-023-00458-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {213-246},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Simple approximative algorithms for free-support wasserstein barycenters},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integer set reduction for stochastic mixed-integer
programming. <em>COAP</em>, <em>85</em>(1), 181–211. (<a
href="https://doi.org/10.1007/s10589-023-00457-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage stochastic mixed-integer programs (SMIPs) with general integer variables in the second-stage are generally difficult to solve. This paper develops the theory of integer set reduction for characterizing a subset of the convex hull of feasible integer points of the second-stage subproblem which can be used for solving the SMIP with pure integer recourse. The basic idea is to use the smallest possible subset of the subproblem feasible integer set to generate a valid inequality like Fenchel decomposition cuts with a goal of reducing computation time. An algorithm for obtaining such a subset based on the solution of the subproblem linear programming relaxation is devised and incorporated into a decomposition method for SMIP. To demonstrate the performance of the new integer set reduction methodology, a computational study based on randomly generated knapsack test instances was performed. The results of the study show that integer set reduction aids in speeding up cut generation, leading to better bounds in solving SMIPs with pure integer recourse than using a direct solver.},
  archive      = {J_COAP},
  author       = {Venkatachalam, Saravanan and Ntaimo, Lewis},
  doi          = {10.1007/s10589-023-00457-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {181-211},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Integer set reduction for stochastic mixed-integer programming},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A split levenberg-marquardt method for large-scale sparse
problems. <em>COAP</em>, <em>85</em>(1), 147–179. (<a
href="https://doi.org/10.1007/s10589-023-00460-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider large-scale nonlinear least squares problems with sparse residuals, each of them depending on a small number of variables. A decoupling procedure which results in a splitting of the original problems into a sequence of independent problems of smaller sizes is proposed and analysed. The smaller size problems are modified in a way that offsets the error made by disregarding dependencies that allow us to split the original problem. The resulting method is a modification of the Levenberg-Marquardt method with smaller computational costs. Global convergence is proved as well as local linear convergence under suitable assumptions on sparsity. The method is tested on the network localization simulated problems with up to one million variables and its efficiency is demonstrated.},
  archive      = {J_COAP},
  author       = {Krejić, Nataša and Malaspina, Greta and Swaenen, Lense},
  doi          = {10.1007/s10589-023-00460-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {147-179},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A split levenberg-marquardt method for large-scale sparse problems},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient implementable inexact entropic proximal point
algorithm for a class of linear programming problems. <em>COAP</em>,
<em>85</em>(1), 107–146. (<a
href="https://doi.org/10.1007/s10589-023-00459-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a class of specially structured linear programming (LP) problems, which has favorable modeling capability for important application problems in different areas such as optimal transport, discrete tomography, and economics. To solve these generally large-scale LP problems efficiently, we design an implementable inexact entropic proximal point algorithm (iEPPA) combined with an easy-to-implement dual block coordinate descent method as a subsolver. Unlike existing entropy-type proximal point algorithms, our iEPPA employs a more practically checkable stopping condition for solving the associated subproblems while achieving provable convergence. Moreover, when solving the capacity constrained multi-marginal optimal transport (CMOT) problem (a special case of our LP problem), our iEPPA is able to bypass the underlying numerical instability issues that often appear in the popular entropic regularization approach, since our algorithm does not require the proximal parameter to be very small in order to obtain an accurate approximate solution. Numerous numerical experiments show that our iEPPA is efficient and robust for solving some large-scale CMOT problems on synthetic data. The preliminary experiments on the discrete tomography problem also highlight the potential modeling capability of our model.},
  archive      = {J_COAP},
  author       = {Chu, Hong T. M. and Liang, Ling and Toh, Kim-Chuan and Yang, Lei},
  doi          = {10.1007/s10589-023-00459-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {107-146},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient implementable inexact entropic proximal point algorithm for a class of linear programming problems},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A harmonic framework for stepsize selection in gradient
methods. <em>COAP</em>, <em>85</em>(1), 75–106. (<a
href="https://doi.org/10.1007/s10589-023-00455-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the use of inverse harmonic Rayleigh quotients with target for the stepsize selection in gradient methods for nonlinear unconstrained optimization problems. This not only provides an elegant and flexible framework to parametrize and reinterpret existing stepsize schemes, but it also gives inspiration for new flexible and tunable families of steplengths. In particular, we analyze and extend the adaptive Barzilai–Borwein method to a new family of stepsizes. While this family exploits negative values for the target, we also consider positive targets. We present a convergence analysis for quadratic problems extending results by Dai and Liao (IMA J Numer Anal 22(1):1–10, 2002), and carry out experiments outlining the potential of the approaches.},
  archive      = {J_COAP},
  author       = {Ferrandi, Giulia and Hochstenbach, Michiel E. and Krejić, Nataša},
  doi          = {10.1007/s10589-023-00455-6},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {75-106},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A harmonic framework for stepsize selection in gradient methods},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A limited memory quasi-newton approach for multi-objective
optimization. <em>COAP</em>, <em>85</em>(1), 33–73. (<a
href="https://doi.org/10.1007/s10589-023-00454-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we deal with the class of unconstrained multi-objective optimization problems. In this setting we introduce, for the first time in the literature, a Limited Memory Quasi-Newton type method, which is well suited especially in large scale scenarios. The proposed algorithm approximates, through a suitable positive definite matrix, the convex combination of the Hessian matrices of the objectives; the update formula for the approximation matrix can be seen as an extension of the one used in the popular L-BFGS method for scalar optimization. Equipped with a Wolfe type line search, the considered method is proved to be well defined even in the nonconvex case. Furthermore, for twice continuously differentiable strongly convex problems, we state global and R-linear convergence to Pareto optimality of the sequence of generated points. The performance of the new algorithm is empirically assessed by a thorough computational comparison with state-of-the-art Newton and Quasi-Newton approaches from the multi-objective optimization literature. The results of the experiments highlight that the proposed approach is generally efficient and effective, outperforming the competitors in most settings. Moreover, the use of the limited memory method results to be beneficial within a global optimization framework for Pareto front approximation.},
  archive      = {J_COAP},
  author       = {Lapucci, Matteo and Mansueto, Pierluigi},
  doi          = {10.1007/s10589-023-00454-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {33-73},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A limited memory quasi-newton approach for multi-objective optimization},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An inexact riemannian proximal gradient method.
<em>COAP</em>, <em>85</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10589-023-00451-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of minimizing the summation of a differentiable function and a nonsmooth function on a Riemannian manifold. In recent years, proximal gradient method and its variants have been generalized to the Riemannian setting for solving such problems. Different approaches to generalize the proximal mapping to the Riemannian setting lead different versions of Riemannian proximal gradient methods. However, their convergence analyses all rely on solving their Riemannian proximal mapping exactly, which is either too expensive or impracticable. In this paper, we study the convergence of an inexact Riemannian proximal gradient method. It is proven that if the proximal mapping is solved sufficiently accurately, then the global convergence and local convergence rate based on the Riemannian Kurdyka–Łojasiewicz property can be guaranteed. Moreover, practical conditions on the accuracy for solving the Riemannian proximal mapping are provided. As a byproduct, the proximal gradient method on the Stiefel manifold proposed in Chen et al. [SIAM J Optim 30(1):210–239, 2020] can be viewed as the inexact Riemannian proximal gradient method provided the proximal mapping is solved to certain accuracy. Finally, numerical experiments on sparse principal component analysis are conducted to test the proposed practical conditions.},
  archive      = {J_COAP},
  author       = {Huang, Wen and Wei, Ke},
  doi          = {10.1007/s10589-023-00451-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact riemannian proximal gradient method},
  volume       = {85},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sequential adaptive regularisation using cubics algorithm
for solving nonlinear equality constrained optimization. <em>COAP</em>,
<em>84</em>(3), 1005–1033. (<a
href="https://doi.org/10.1007/s10589-022-00449-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive regularisation algorithm using cubics (ARC) is initially proposed for unconstrained optimization. ARC has excellent convergence properties and complexity. In this paper, we extend ARC to solve nonlinear equality constrained optimization and propose a sequential adaptive regularisation using cubics algorithm inspired by sequential quadratic programming (SQP) methods. In each iteration of our method, the trial step is computed via composite-step approach, i.e., it is decomposed into the sum of normal step and tangential step. By means of reduced-Hessian approach, a new ARC subproblem for nonlinear equality constrained optimization is constructed to compute the tangential step, which can supply sufficient decrease required in the proposed algorithm. Once the trial step is obtained, the ratio of the penalty function reduction to the model function reduction is calculated to determine whether the trial point is accepted. The global convergence of the algorithm is investigated under some mild assumptions. Preliminary numerical experiments are reported to show the performance of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Pei, Yonggang and Song, Shaofang and Zhu, Detong},
  doi          = {10.1007/s10589-022-00449-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1005-1033},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A sequential adaptive regularisation using cubics algorithm for solving nonlinear equality constrained optimization},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifted stationary points of sparse optimization with
complementarity constraints. <em>COAP</em>, <em>84</em>(3), 973–1003.
(<a href="https://doi.org/10.1007/s10589-022-00444-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to compute lifted stationary points of a sparse optimization problem ( $$P_{0}$$ ) with complementarity constraints. We define a continuous relaxation problem ( $$R_{\nu }$$ ) that has the same global minimizers and optimal value with problem ( $$P_{0}$$ ). Problem ( $$R_{\nu }$$ ) is a mathematical program with complementarity constraints (MPCC) and a difference-of-convex objective function. We define MPCC lifted-stationarity of ( $$R_{\nu }$$ ) and show that it is weaker than directional stationarity, but stronger than Clarke stationarity for local optimality. Moreover, we propose an approximation method to solve ( $$R_{\nu }$$ ) and an augmented Lagrangian method to solve its subproblem ( $$R_{\nu ,\sigma }$$ ), which relaxes the equality constraint in ( $$R_{\nu }$$ ) with a tolerance $$\sigma $$ . We prove the convergence of our algorithm to an MPCC lifted-stationary point of problem ( $$R_{\nu }$$ ) and use a sparse optimization problem with vertical linear complementarity constraints to demonstrate the efficiency of our algorithm on finding sparse solutions in practice.},
  archive      = {J_COAP},
  author       = {Liu, Shisen and Chen, Xiaojun},
  doi          = {10.1007/s10589-022-00444-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {973-1003},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Lifted stationary points of sparse optimization with complementarity constraints},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the asymptotic rate of convergence of stochastic newton
algorithms and their weighted averaged versions. <em>COAP</em>,
<em>84</em>(3), 921–972. (<a
href="https://doi.org/10.1007/s10589-022-00442-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most machine learning methods can be regarded as the minimization of an unavailable risk function. To optimize the latter, with samples provided in a streaming fashion, we define general (weighted averaged) stochastic Newton algorithms, for which a theoretical analysis of their asymptotic efficiency is conducted. The corresponding implementations are shown not to require the inversion of a Hessian estimate at each iteration under a quite flexible framework that covers the case of linear, logistic or softmax regressions to name a few. Numerical experiments on simulated and real data give the empirical evidence of the pertinence of the proposed methods, which outperform popular competitors particularly in case of bad initializations.},
  archive      = {J_COAP},
  author       = {Boyer, Claire and Godichon-Baggioni, Antoine},
  doi          = {10.1007/s10589-022-00442-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {921-972},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the asymptotic rate of convergence of stochastic newton algorithms and their weighted averaged versions},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A matrix nonconvex relaxation approach to unconstrained
binary polynomial programs. <em>COAP</em>, <em>84</em>(3), 875–919. (<a
href="https://doi.org/10.1007/s10589-022-00443-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with a class of unconstrained binary polynomial programs (UBPPs), which covers the classical binary quadratic program and has a host of applications in many science and engineering fields. We start with the global exact penalty of its DC constrained SDP reformulation, and propose a continuous relaxation approach by seeking a finite number of approximate stationary points for the factorized form of the global exact penalty with increasing penalty parameters. A globally convergent majorization-minimization method with extrapolation is developed to capture such stationary points. Under a mild condition, we show that the rank-one projection of the output for the relaxation approach is an approximate feasible solution of the UBPP and quantify the lower bound of its minus objective value from the optimal value. Numerical comparisons with the SDP relaxation method armed with a special random rounding technique and the DC relaxation approaches armed with the solvers for linear and quadratic SDPs confirm the efficiency of the proposed relaxation approach, which can solve the instance of 20,000 variables in 15 min and yield a lower bound for the optimal value and the known best value with a relative error at most 1.824 and 2.870\%, respectively.},
  archive      = {J_COAP},
  author       = {Qian, Yitian and Pan, Shaohua and Bi, Shujun},
  doi          = {10.1007/s10589-022-00443-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {875-919},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A matrix nonconvex relaxation approach to unconstrained binary polynomial programs},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Majorization-minimization-based levenberg–marquardt method
for constrained nonlinear least squares. <em>COAP</em>, <em>84</em>(3),
833–874. (<a href="https://doi.org/10.1007/s10589-022-00447-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new Levenberg–Marquardt (LM) method for solving nonlinear least squares problems with convex constraints is described. Various versions of the LM method have been proposed, their main differences being in the choice of a damping parameter. In this paper, we propose a new rule for updating the parameter so as to achieve both global and local convergence even under the presence of a convex constraint set. The key to our results is a new perspective of the LM method from majorization-minimization methods. Specifically, we show that if the damping parameter is set in a specific way, the objective function of the standard subproblem in LM methods becomes an upper bound on the original objective function under certain standard assumptions. Our method solves a sequence of the subproblems approximately using an (accelerated) projected gradient method. It finds an $$\varepsilon$$ -stationary point after $$O(\varepsilon ^{-2})$$ computation and achieves local quadratic convergence for zero-residual problems under a local error bound condition. Numerical results on compressed sensing and matrix factorization show that our method converges faster in many cases than existing methods.},
  archive      = {J_COAP},
  author       = {Marumo, Naoki and Okuno, Takayuki and Takeda, Akiko},
  doi          = {10.1007/s10589-022-00447-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {833-874},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Majorization-minimization-based Levenberg–Marquardt method for constrained nonlinear least squares},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational aspects of column generation for nonlinear and
conic optimization: Classical and linearized schemes. <em>COAP</em>,
<em>84</em>(3), 789–831. (<a
href="https://doi.org/10.1007/s10589-022-00445-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving large scale nonlinear optimization problems requires either significant computing resources or the development of specialized algorithms. For Linear Programming (LP) problems, decomposition methods can take advantage of problem structure, gradually constructing the full problem by generating variables or constraints. We first present a direct adaptation of the Column Generation (CG) methodology for nonlinear optimization problems, such that when optimizing over a structured set $${\mathcal {X}}$$ plus a moderate number of complicating constraints, we solve a succession of (1) restricted master problems on a smaller set $${\mathcal {S}}\subset {\mathcal {X}}$$ and (2) pricing problems that are Lagrangean relaxations wrt the complicating constraints. The former provides feasible solutions and feeds dual information to the latter. In turn, the pricing problem identifies a variable of interest that is then taken into account into an updated subset $${\mathcal {S}}&#39;\subset {\mathcal {X}}$$ . Our approach is valid whenever the master problem has zero Lagrangean duality gap wrt to the complicating constraints, and not only when $${\mathcal {S}}$$ is the convex hull of the generated variables as in CG for LPs, but also with a variety of subsets such as the conic hull, the linear span, and a special variable aggregation set. We discuss how the structure of $${\mathcal {S}}$$ and its update mechanism influence the number of iterations required to reach near-optimality and the difficulty of solving the restricted master problems, and present linearized schemes that alleviate the computational burden of solving the pricing problem. We test our methods on synthetic portfolio optimization instances with up to 5 million variables including nonlinear objective functions and second order cone constraints. We show that some CGs with linearized pricing are 2–3 times faster than solving the complete problem directly and are able to provide solutions within 1\% of optimality in 6 h for the larger instances, whereas solving the complete problem runs out of memory.},
  archive      = {J_COAP},
  author       = {Chicoisne, Renaud},
  doi          = {10.1007/s10589-022-00445-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {789-831},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Computational aspects of column generation for nonlinear and conic optimization: Classical and linearized schemes},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). T-product factorization based method for matrix and tensor
completion problems. <em>COAP</em>, <em>84</em>(3), 761–788. (<a
href="https://doi.org/10.1007/s10589-022-00439-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low rank matrix and tensor completion problems are to recover the incomplete two and higher order data of low rank structures. The essential problem in the matrix and tensor completion problems is how to improve the efficiency. For a matrix completion problem, we establish a relationship between matrix rank and tensor tubal rank, and reformulate matrix completion problem as a third order tensor completion problem. For the reformulated tensor completion problem, we adopt a two-stage strategy based on tensor factorization algorithm. In this way, a matrix completion problem of big size can be solved via some matrix computations of smaller sizes. For a third order tensor completion problem, to fully exploit the low rank structures, we introduce the double tubal rank which combines the tubal rank of two tensors, original tensor and the reshaped tensor of the mode-3 unfolding matrix of original tensor. Based on this, we propose a reweighted tensor factorization algorithm for third order tensor completion. Extensive numerical experiments demonstrate that the proposed methods outperform state-of-the-art methods in terms of both accuracy and running time.},
  archive      = {J_COAP},
  author       = {Yu, Quan and Zhang, Xinzhen},
  doi          = {10.1007/s10589-022-00439-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {761-788},
  shortjournal = {Comput. Optim. Appl.},
  title        = {T-product factorization based method for matrix and tensor completion problems},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A smoothing proximal gradient algorithm with extrapolation
for the relaxation of <span
class="math display"><em>ℓ</em><sub>0</sub></span> regularization
problem. <em>COAP</em>, <em>84</em>(3), 737–760. (<a
href="https://doi.org/10.1007/s10589-022-00446-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the exact continuous relaxation model of $${\ell_{0}}$$ regularization problem, which was given by Bian and Chen (SIAM J Numer Anal 58:858–883, 2020) and propose a smoothing proximal gradient algorithm with extrapolation (SPGE) for this kind of problems. Under a general choice of extrapolation parameter, it is proved that all the accumulation points have a common support set, and the ability of the SPGE algorithm to identify the zero entries of the accumulation point within finite iterations is available. We show that any accumulation point of the sequence generated by the SPGE algorithm is a lifted stationary point of the relaxation model. Moreover, a convergence rate concerning proximal residual is established. Finally, we conduct three numerical experiments to illustrate the efficiency of the SPGE algorithm compared with the smoothing proximal gradient (SPG) algorithm proposed by Bian and Chen (2020).},
  archive      = {J_COAP},
  author       = {Zhang, Jie and Yang, Xinmin and Li, Gaoxi and Zhang, Ke},
  doi          = {10.1007/s10589-022-00446-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {737-760},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A smoothing proximal gradient algorithm with extrapolation for the relaxation of $${\ell_{0}}$$ regularization problem},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conic formulation of QPCCs applied to truly sparse QPs.
<em>COAP</em>, <em>84</em>(3), 703–735. (<a
href="https://doi.org/10.1007/s10589-022-00440-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study (nonconvex) quadratic optimization problems with complementarity constraints, establishing an exact completely positive reformulation under—apparently new—mild conditions involving only the constraints, not the objective. Moreover, we also give the conditions for strong conic duality between the obtained completely positive problem and its dual. Our approach is based on purely continuous models which avoid any branching or use of large constants in implementation. An application to pursuing interpretable sparse solutions of quadratic optimization problems is shown to satisfy our settings, and therefore we link quadratic problems with an exact sparsity term $$\Vert {{\mathsf {x}}}\Vert _0$$ to copositive optimization. The covered problem class includes sparse least-squares regression under linear constraints, for instance. Numerical comparisons between our method and other approximations are reported from the perspective of the objective function value.},
  archive      = {J_COAP},
  author       = {Bomze, Immanuel M. and Peng, Bo},
  doi          = {10.1007/s10589-022-00440-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {703-735},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Conic formulation of QPCCs applied to truly sparse QPs},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secant penalized BFGS: A noise robust quasi-newton method
via penalizing the secant condition. <em>COAP</em>, <em>84</em>(3),
651–702. (<a href="https://doi.org/10.1007/s10589-022-00448-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new variant of the BFGS method designed to perform well when gradient measurements are corrupted by noise. We show that treating the secant condition with a penalty method approach motivated by regularized least squares estimation generates a parametric family with the original BFGS update at one extreme and not updating the inverse Hessian approximation at the other extreme. Furthermore, we find the curvature condition is relaxed as the family moves towards not updating the inverse Hessian approximation, and disappears entirely at the extreme where the inverse Hessian approximation is not updated. These developments allow us to develop a method we refer to as Secant Penalized BFGS (SP-BFGS) that allows one to relax the secant condition based on the amount of noise in the gradient measurements. SP-BFGS provides a means of incrementally updating the new inverse Hessian approximation with a controlled amount of bias towards the previous inverse Hessian approximation, which allows one to replace the overwriting nature of the original BFGS update with an averaging nature that resists the destructive effects of noise and can cope with negative curvature measurements. We discuss the theoretical properties of SP-BFGS, including convergence when minimizing strongly convex functions in the presence of uniformly bounded noise. Finally, we present extensive numerical experiments using over 30 problems from the CUTEst test problem set that demonstrate the superior performance of SP-BFGS compared to BFGS in the presence of both noisy function and gradient evaluations.},
  archive      = {J_COAP},
  author       = {Irwin, Brian and Haber, Eldad},
  doi          = {10.1007/s10589-022-00448-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {651-702},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Secant penalized BFGS: A noise robust quasi-newton method via penalizing the secant condition},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-level distributed algorithm for nonconvex constrained
optimization. <em>COAP</em>, <em>84</em>(2), 609–649. (<a
href="https://doi.org/10.1007/s10589-022-00433-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop distributed algorithms for nonconvex optimization problems with complicated constraints associated with a network. The network can be a physical one, such as an electric power network, where the constraints are nonlinear power flow equations, or an abstract one that represents constraint couplings between decision variables of different agents. Despite the recent development of distributed algorithms for nonconvex programs, highly complicated constraints still pose a significant challenge in theory and practice. We first identify some difficulties with the existing algorithms based on the alternating direction method of multipliers (ADMM) for dealing with such problems. We then propose a reformulation that enables us to design a two-level algorithm, which embeds a specially structured three-block ADMM at the inner level in an augmented Lagrangian method framework. Furthermore, we prove the global and local convergence as well as iteration complexity of this new scheme for general nonconvex constrained programs, and show that our analysis can be extended to handle more complicated multi-block inner-level problems. Finally, we demonstrate with computation that the new scheme provides convergent and parallelizable algorithms for various nonconvex applications, and is able to complement the performance of the state-of-the-art distributed algorithms in practice by achieving either faster convergence in optimality gap or in feasibility or both.},
  archive      = {J_COAP},
  author       = {Sun, Kaizhao and Sun, X. Andy},
  doi          = {10.1007/s10589-022-00433-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {609-649},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A two-level distributed algorithm for nonconvex constrained optimization},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OFFO minimization algorithms for second-order optimality and
their complexity. <em>COAP</em>, <em>84</em>(2), 573–607. (<a
href="https://doi.org/10.1007/s10589-022-00435-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Adagrad-inspired class of algorithms for smooth unconstrained optimization is presented in which the objective function is never evaluated and yet the gradient norms decrease at least as fast as $$\mathcal{O}(1/\sqrt{k+1})$$ while second-order optimality measures converge to zero at least as fast as $$\mathcal{O}(1/(k+1)^{1/3})$$ . This latter rate of convergence is shown to be essentially sharp and is identical to that known for more standard algorithms (like trust-region or adaptive-regularization methods) using both function and derivatives’ evaluations. A related “divergent stepsize” method is also described, whose essentially sharp rate of convergence is slighly inferior. It is finally discussed how to obtain weaker second-order optimality guarantees at a (much) reduced computational cost.},
  archive      = {J_COAP},
  author       = {Gratton, S. and Toint, Ph. L.},
  doi          = {10.1007/s10589-022-00435-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {573-607},
  shortjournal = {Comput. Optim. Appl.},
  title        = {OFFO minimization algorithms for second-order optimality and their complexity},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed stochastic gradient tracking methods with
momentum acceleration for non-convex optimization. <em>COAP</em>,
<em>84</em>(2), 531–572. (<a
href="https://doi.org/10.1007/s10589-022-00432-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a distributed non-convex optimization problem of minimizing the sum of all local cost functions over a network of agents. This problem often appears in large-scale distributed machine learning, known as non-convex empirical risk minimization. In this paper, we propose two accelerated algorithms, named DSGT-HB and DSGT-NAG, which combine the distributed stochastic gradient tracking (DSGT) method with momentum accelerated techniques. Under appropriate assumptions, we prove that both algorithms sublinearly converge to a neighborhood of a first-order stationary point of the distributed non-convex optimization. Moreover, we derive the conditions under which DSGT-HB and DSGT-NAG achieve a network-independent linear speedup. Numerical experiments for a distributed non-convex logistic regression problem on real data sets and a deep neural network on the MNIST database show the superiorities of DSGT-HB and DSGT-NAG compared with DSGT.},
  archive      = {J_COAP},
  author       = {Gao, Juan and Liu, Xin-Wei and Dai, Yu-Hong and Huang, Yakui and Gu, Junhua},
  doi          = {10.1007/s10589-022-00432-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {531-572},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributed stochastic gradient tracking methods with momentum acceleration for non-convex optimization},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On global convergence of alternating least squares for
tensor approximation. <em>COAP</em>, <em>84</em>(2), 509–529. (<a
href="https://doi.org/10.1007/s10589-022-00428-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternating least squares is a classic, easily implemented, yet widely used method for tensor canonical polyadic approximation. Its subsequential and global convergence is ensured if the partial Hessians of the blocks during the whole sequence are uniformly positive definite. This paper shows that this positive definiteness assumption can be weakened in two ways. Firstly, if the smallest positive eigenvalues of the partial Hessians are uniformly positive, and the solutions of the subproblems are properly chosen, then global convergence holds. This allows the partial Hessians to be only positive semidefinite. Next, if at a limit point, the partial Hessians are positive definite, then global convergence also holds. We also discuss the connection of such an assumption to the uniqueness of exact CP decomposition.},
  archive      = {J_COAP},
  author       = {Yang, Yuning},
  doi          = {10.1007/s10589-022-00428-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {509-529},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On global convergence of alternating least squares for tensor approximation},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A global exact penalty for rank-constrained optimization
problem and applications. <em>COAP</em>, <em>84</em>(2), 477–508. (<a
href="https://doi.org/10.1007/s10589-022-00427-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a rank-constrained optimization problem where the objective function is continuously differentiable on a closed convex set. After replacing the rank constraint by an equality of the truncated difference of L1 and L2 norm, and adding the equality constraint into the objective to get a penalty problem, we prove that the penalty problem is exact in the sense that the set of its global (local) optimal solutions coincides with that of the original problem when the penalty parameter is over a certain threshold. This establishes the theoretical guarantee for the truncated difference of L1 and L2 norm regularization optimization including the work of Ma et al. (SIAM J Imaging Sci 10(3):1346–1380, 2017). Besides, for the penalty problem, we propose an extrapolation proximal difference of convex algorithm (epDCA) and prove the sequence generated by epDCA converges to a stationary point of the penalty problem. Further, an adaptive penalty method based on epDCA is constructed for the original rank-constrained problem. The efficiency of the algorithms is verified via numerical experiments for the nearest low-rank correlation matrix problem and the matrix completion problem.},
  archive      = {J_COAP},
  author       = {Yang, Zhikai and Han, Le},
  doi          = {10.1007/s10589-022-00427-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {477-508},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A global exact penalty for rank-constrained optimization problem and applications},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient differentiable quadratic programming layers: An
ADMM approach. <em>COAP</em>, <em>84</em>(2), 449–476. (<a
href="https://doi.org/10.1007/s10589-022-00422-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in neural-network architecture allow for seamless integration of convex optimization problems as differentiable layers in an end-to-end trainable neural network. Integrating medium and large scale quadratic programs into a deep neural network architecture, however, is challenging as solving quadratic programs exactly by interior-point methods has worst-case cubic complexity in the number of variables. In this paper, we present an alternative network layer architecture based on the alternating direction method of multipliers (ADMM) that is capable of scaling to moderate sized problems with 100–1000 decision variables and thousands of training examples. Backward differentiation is performed by implicit differentiation of a customized fixed-point iteration. Simulated results demonstrate the computational advantage of the ADMM layer, which for medium scale problems is approximately an order of magnitude faster than the state-of-the-art layers. Furthermore, our novel backward-pass routine is computationally efficient in comparison to the standard approach based on unrolled differentiation or implicit differentiation of the KKT optimality conditions. We conclude with examples from portfolio optimization in the integrated prediction and optimization paradigm.},
  archive      = {J_COAP},
  author       = {Butler, Andrew and Kwon, Roy H.},
  doi          = {10.1007/s10589-022-00422-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {449-476},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient differentiable quadratic programming layers: An ADMM approach},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loss functions for finite sets. <em>COAP</em>,
<em>84</em>(2), 421–447. (<a
href="https://doi.org/10.1007/s10589-022-00420-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies loss functions for finite sets. For a given finite set S, we give sum-of-square type loss functions of minimum degree. When S is the vertex set of a standard simplex, we show such loss functions have no spurious minimizers (i.e., every local minimizer is a global one). Up to transformations, we give similar loss functions without spurious minimizers for general finite sets. When S is approximately given by a sample set T, we show how to get loss functions by solving a quadratic optimization problem. Numerical experiments and applications are given to show the efficiency of these loss functions.},
  archive      = {J_COAP},
  author       = {Nie, Jiawang and Zhong, Suhan},
  doi          = {10.1007/s10589-022-00420-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {421-447},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Loss functions for finite sets},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A subgradient method with non-monotone line search.
<em>COAP</em>, <em>84</em>(2), 397–420. (<a
href="https://doi.org/10.1007/s10589-022-00438-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a subgradient method with non-monotone line search for the minimization of convex functions with simple convex constraints. Different from the standard subgradient method with prefixed step sizes, the new method selects the step sizes in an adaptive way. Under mild conditions asymptotic convergence results and iteration-complexity bounds are obtained. Preliminary numerical results illustrate the relative efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Ferreira, O. P. and Grapiglia, G. N. and Santos, E. M. and Souza, J. C. O.},
  doi          = {10.1007/s10589-022-00438-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {397-420},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A subgradient method with non-monotone line search},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inexact gradient projection method with relative error
tolerance. <em>COAP</em>, <em>84</em>(2), 363–395. (<a
href="https://doi.org/10.1007/s10589-022-00425-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A gradient projection method with feasible inexact projections is proposed in the present paper. The inexact projection is performed using a general relative error tolerance. Asymptotic convergence analysis under quasiconvexity assumption and iteration-complexity bounds under convexity assumption of the method employing constant and Armijo step sizes are presented. Numerical results are reported illustrating the potential advantages of considering inexact projections instead of exact ones in some medium scale instances of a least squares problem over the spectrohedron.},
  archive      = {J_COAP},
  author       = {Aguiar, A. A. and Ferreira, O. P. and Prudente, L. F.},
  doi          = {10.1007/s10589-022-00425-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {363-395},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact gradient projection method with relative error tolerance},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An abstract convergence framework with application to
inertial inexact forward–backward methods. <em>COAP</em>,
<em>84</em>(2), 319–362. (<a
href="https://doi.org/10.1007/s10589-022-00441-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a novel abstract descent scheme suited for the minimization of proper and lower semicontinuous functions. The proposed abstract scheme generalizes a set of properties that are crucial for the convergence of several first-order methods designed for nonsmooth nonconvex optimization problems. Such properties guarantee the convergence of the full sequence of iterates to a stationary point, if the objective function satisfies the Kurdyka–Łojasiewicz property. The abstract framework allows for the design of new algorithms. We propose two inertial-type algorithms with implementable inexactness criteria for the main iteration update step. The first algorithm, i $$^2$$ Piano, exploits large steps by adjusting a local Lipschitz constant. The second algorithm, iPila, overcomes the main drawback of line-search based methods by enforcing a descent only on a merit function instead of the objective function. Both algorithms have the potential to escape local minimizers (or stationary points) by leveraging the inertial feature. Moreover, they are proved to enjoy the full convergence guarantees of the abstract descent scheme, which is the best we can expect in such a general nonsmooth nonconvex optimization setup using first-order methods. The efficiency of the proposed algorithms is demonstrated on two exemplary image deblurring problems, where we can appreciate the benefits of performing a linesearch along the descent direction inside an inertial scheme.},
  archive      = {J_COAP},
  author       = {Bonettini, Silvia and Ochs, Peter and Prato, Marco and Rebegoldi, Simone},
  doi          = {10.1007/s10589-022-00441-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {319-362},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An abstract convergence framework with application to inertial inexact forward–backward methods},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On FISTA with a relative error rule. <em>COAP</em>,
<em>84</em>(2), 295–318. (<a
href="https://doi.org/10.1007/s10589-022-00421-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast iterative shrinkage/thresholding algorithm (FISTA) is one of the most popular first-order iterations for minimizing the sum of two convex functions. FISTA is known to improve the complexity of the classical proximal gradient method (PGM) from $$O(k^{-1})$$ to the optimal complexity $$O(k^{-2})$$ in terms of the sequence of the functional values. When the evaluation of the proximal operator is hard, inexact versions of FISTA might be used to solve the problem. In this paper, we proposed an inexact version of FISTA by solving the proximal subproblem inexactly using a relative error criterion instead of exogenous and diminishing error rules. The introduced relative error rule in the FISTA iteration is related to the progress of the algorithm at each step and does not increase the computational burden per iteration. Moreover, the proposed algorithm recovers the same optimal convergence rate as FISTA. Some numerical experiments are also reported to illustrate the numerical behavior of the relative inexact method when compared with FISTA under an absolute error criterion.},
  archive      = {J_COAP},
  author       = {Bello-Cruz, Yunier and Gonçalves, Max L. N. and Krislock, Nathan},
  doi          = {10.1007/s10589-022-00421-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {295-318},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On FISTA with a relative error rule},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From inexact optimization to learning via gradient
concentration. <em>COAP</em>, <em>84</em>(1), 265–294. (<a
href="https://doi.org/10.1007/s10589-022-00408-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization in machine learning typically deals with the minimization of empirical objectives defined by training data. The ultimate goal of learning, however, is to minimize the error on future data (test error), for which the training data provides only partial information. In this view, the optimization problems that are practically feasible are based on inexact quantities that are stochastic in nature. In this paper, we show how probabilistic results, specifically gradient concentration, can be combined with results from inexact optimization to derive sharp test error guarantees. By considering unconstrained objectives, we highlight the implicit regularization properties of optimization for learning.},
  archive      = {J_COAP},
  author       = {Stankewitz, Bernhard and Mücke, Nicole and Rosasco, Lorenzo},
  doi          = {10.1007/s10589-022-00408-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {265-294},
  shortjournal = {Comput. Optim. Appl.},
  title        = {From inexact optimization to learning via gradient concentration},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avoiding bad steps in frank-wolfe variants. <em>COAP</em>,
<em>84</em>(1), 225–264. (<a
href="https://doi.org/10.1007/s10589-022-00434-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of Frank-Wolfe (FW) variants is often complicated by the presence of different kinds of “good” and “bad” steps. In this article, we aim to simplify the convergence analysis of specific variants by getting rid of such a distinction between steps, and to improve existing rates by ensuring a non-trivial bound at each iteration. In order to do this, we define the Short Step Chain (SSC) procedure, which skips gradient computations in consecutive short steps until proper conditions are satisfied. This algorithmic tool allows us to give a unified analysis and converge rates in the general smooth non convex setting, as well as a linear convergence rate under a Kurdyka-Łojasiewicz (KL) property. While the KL setting has been widely studied for proximal gradient type methods, to our knowledge, it has never been analyzed before for the Frank-Wolfe variants considered in the paper. An angle condition, ensuring that the directions selected by the methods have the steepest slope possible up to a constant, is used to carry out our analysis. We prove that such a condition is satisfied, when considering minimization problems over a polytope, by the away step Frank-Wolfe (AFW), the pairwise Frank-Wolfe (PFW), and the Frank-Wolfe method with in face directions (FDFW).},
  archive      = {J_COAP},
  author       = {Rinaldi, Francesco and Zeffiro, Damiano},
  doi          = {10.1007/s10589-022-00434-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {225-264},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Avoiding bad steps in frank-wolfe variants},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved penalty algorithm using model order reduction
for MIPDECO problems with partial observations. <em>COAP</em>,
<em>84</em>(1), 191–223. (<a
href="https://doi.org/10.1007/s10589-022-00386-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses optimal control problems governed by a linear time-dependent partial differential equation (PDE) as well as integer constraints on the control. Moreover, partial observations are assumed in the objective function. The resulting problem poses several numerical challenges due to the mixture of combinatorial aspects, induced by integer variables, and large scale linear algebra issues, arising from the PDE discretization. Since classical solution approaches such as the branch-and-bound framework are typically overwhelmed by such large-scale problems, this work extends an improved penalty algorithm proposed by the authors, to the time-dependent setting. The main contribution is a novel combination of an interior point method, preconditioning, and model order reduction yielding a tailored local optimization solver at the heart of the overall solution procedure. A thorough numerical investigation is carried out both for the heat equation as well as a convection-diffusion problem demonstrating the versatility of the approach.},
  archive      = {J_COAP},
  author       = {Garmatter, Dominik and Porcelli, Margherita and Rinaldi, Francesco and Stoll, Martin},
  doi          = {10.1007/s10589-022-00386-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {191-223},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An improved penalty algorithm using model order reduction for MIPDECO problems with partial observations},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid limited memory gradient projection methods for
box-constrained optimization problems. <em>COAP</em>, <em>84</em>(1),
151–189. (<a href="https://doi.org/10.1007/s10589-022-00409-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient projection methods represent effective tools for solving large-scale constrained optimization problems thanks to their simple implementation and low computational cost per iteration. Despite these good properties, a slow convergence rate can affect gradient projection schemes, especially when high accurate solutions are needed. A strategy to mitigate this drawback consists in properly selecting the values for the steplength along the negative gradient. In this paper, we consider the class of gradient projection methods with line search along the projected arc for box-constrained minimization problems and we analyse different strategies to define the steplength. It is well known in the literature that steplength selection rules able to approximate, at each iteration, the eigenvalues of the inverse of a suitable submatrix of the Hessian of the objective function can improve the performance of gradient projection methods. In this perspective, we propose an automatic hybrid steplength selection technique that employs a proper alternation of standard Barzilai–Borwein rules, when the final active set is not well approximated, and a generalized limited memory strategy based on the Ritz-like values of the Hessian matrix restricted to the inactive constraints, when the final active set is reached. Numerical experiments on quadratic and non-quadratic test problems show the effectiveness of the proposed steplength scheme.},
  archive      = {J_COAP},
  author       = {Crisci, Serena and Porta, Federica and Ruggiero, Valeria and Zanni, Luca},
  doi          = {10.1007/s10589-022-00409-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {151-189},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Hybrid limited memory gradient projection methods for box-constrained optimization problems},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained and unconstrained deep image prior optimization
models with automatic regularization. <em>COAP</em>, <em>84</em>(1),
125–149. (<a href="https://doi.org/10.1007/s10589-022-00392-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Image Prior (DIP) is currently among the most efficient unsupervised deep learning based methods for ill-posed inverse problems in imaging. This novel framework relies on the implicit regularization provided by representing images as the output of generative Convolutional Neural Network (CNN) architectures. So far, DIP has been shown to be an effective approach when combined with classical and novel regularizers. Unfortunately, to obtain appropriate solutions, all the models proposed up to now require an accurate estimate of the regularization parameter. To overcome this difficulty, we consider a locally adapted regularized unconstrained model whose local regularization parameters are automatically estimated for additively separable regularizers. Moreover, we propose a novel constrained formulation in analogy to Morozov’s discrepancy principle which enables the application of a broader range of regularizers. Both the unconstrained and the constrained models are solved via the proximal gradient descent-ascent method. Numerical results demonstrate the robustness with respect to image content, noise levels and hyperparameters of the proposed models on both denoising and deblurring of simulated as well as real natural and medical images.},
  archive      = {J_COAP},
  author       = {Cascarano, Pasquale and Franchini, Giorgia and Kobler, Erich and Porta, Federica and Sebastiani, Andrea},
  doi          = {10.1007/s10589-022-00392-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {125-149},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Constrained and unconstrained deep image prior optimization models with automatic regularization},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nested primal–dual FISTA-like scheme for composite convex
optimization problems. <em>COAP</em>, <em>84</em>(1), 85–123. (<a
href="https://doi.org/10.1007/s10589-022-00410-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nested primal–dual algorithm with extrapolation on the primal variable suited for minimizing the sum of two convex functions, one of which is continuously differentiable. The proposed algorithm can be interpreted as an inexact inertial forward–backward algorithm equipped with a prefixed number of inner primal–dual iterations for the proximal evaluation and a “warm–start” strategy for starting the inner loop, and generalizes several nested primal–dual algorithms already available in the literature. By appropriately choosing the inertial parameters, we prove the convergence of the iterates to a saddle point of the problem, and provide an O(1/n) convergence rate on the primal–dual gap evaluated at the corresponding ergodic sequences. Numerical experiments on some image restoration problems show that the combination of the “warm–start” strategy with an appropriate choice of the inertial parameters is strictly required in order to guarantee the convergence to the real minimum point of the objective function.},
  archive      = {J_COAP},
  author       = {Bonettini, S. and Prato, M. and Rebegoldi, S.},
  doi          = {10.1007/s10589-022-00410-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {85-123},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A nested primal–dual FISTA-like scheme for composite convex optimization problems},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stochastic first-order trust-region method with inexact
restoration for finite-sum minimization. <em>COAP</em>, <em>84</em>(1),
53–84. (<a href="https://doi.org/10.1007/s10589-022-00430-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a stochastic first-order trust-region method with inexact function and gradient evaluations for solving finite-sum minimization problems. Using a suitable reformulation of the given problem, our method combines the inexact restoration approach for constrained optimization with the trust-region procedure and random models. Differently from other recent stochastic trust-region schemes, our proposed algorithm improves feasibility and optimality in a modular way. We provide the expected number of iterations for reaching a near-stationary point by imposing some probability accuracy requirements on random functions and gradients which are, in general, less stringent than the corresponding ones in literature. We validate the proposed algorithm on some nonconvex optimization problems arising in binary classification and regression, showing that it performs well in terms of cost and accuracy, and allows to reduce the burdensome tuning of the hyper-parameters involved.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Krejić, Nataša and Morini, Benedetta and Rebegoldi, Simone},
  doi          = {10.1007/s10589-022-00430-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {53-84},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stochastic first-order trust-region method with inexact restoration for finite-sum minimization},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A random time-dependent noncooperative equilibrium problem.
<em>COAP</em>, <em>84</em>(1), 27–52. (<a
href="https://doi.org/10.1007/s10589-022-00368-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the random time-dependent oligopolistic market equilibrium problem. For such a problem the firms’ point of view has been analyzed in Barbagallo and Guarino Lo Bianco (Optim. Lett. 14: 2479–2493, 2020) while here the policymaker’s point of view is studied. The random dynamic optimal control equilibrium conditions are expressed by means of an inverse stochastic time-dependent variational inequality which is proved to be equivalent to a stochastic time-dependent variational inequality. Some existence and well-posedness results for optimal regulatory taxes are obtained. Moreover a numerical scheme to compute the solution to the stochastic time-dependent variational inequality is presented. Finally an example is discussed.},
  archive      = {J_COAP},
  author       = {Barbagallo, Annamaria and Guarino Lo Bianco, Serena},
  doi          = {10.1007/s10589-022-00368-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {27-52},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A random time-dependent noncooperative equilibrium problem},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cartoon-texture evolution for two-region image segmentation.
<em>COAP</em>, <em>84</em>(1), 5–26. (<a
href="https://doi.org/10.1007/s10589-022-00387-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-region image segmentation is the process of dividing an image into two regions of interest, i.e., the foreground and the background. To this aim, Chan et al. (SIAM J Appl Math 66(5):1632–1648, 2006) designed a model well suited for smooth images. One drawback of this model is that it may produce a bad segmentation when the image contains oscillatory components. Based on a cartoon-texture decomposition of the image to be segmented, we propose a new model that is able to produce an accurate segmentation of images also containing noise or oscillatory information like texture. The novel model leads to a non-smooth constrained optimization problem which we solve by means of the ADMM method. The convergence of the numerical scheme is also proved. Several experiments on smooth, noisy, and textural images show the effectiveness of the proposed model.},
  archive      = {J_COAP},
  author       = {Antonelli, Laura and De Simone, Valentina and Viola, Marco},
  doi          = {10.1007/s10589-022-00387-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {5-26},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cartoon-texture evolution for two-region image segmentation},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue for SIMAI 2020–2021: Large-scale optimization
and applications. <em>COAP</em>, <em>84</em>(1), 1–4. (<a
href="https://doi.org/10.1007/s10589-022-00436-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Ruggiero, Valeria and Toraldo, Gerardo},
  doi          = {10.1007/s10589-022-00436-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-4},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Special issue for SIMAI 2020–2021: Large-scale optimization and applications},
  volume       = {84},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
