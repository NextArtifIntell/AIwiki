<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDAR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdar---29">IJDAR - 29</h2>
<ul>
<li><details>
<summary>
(2023). Review of chart image detection and classification.
<em>IJDAR</em>, <em>26</em>(4), 453–474. (<a
href="https://doi.org/10.1007/s10032-022-00424-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a complete review of different approaches across all components of the chart image detection and classification up to date. A set of 89 scientific papers is collected, analyzed, and enlisted into four categories: chart-type classification, chart text processing, chart data extraction, and chart description generation. Detailed information about problem formulation and a research field is provided, and an overview of used methods in each category. Each paper&#39;s contribution is noted, including the essential information for authors in this research field. In the end, a comparison is made between the reported results. The state-of-the-art methods in each category are described, and a research direction is given. We have also analyzed the open challenges that still exist and require the author&#39;s attention.},
  archive      = {J_IJDAR},
  author       = {Bajić, Filip and Job, Josip},
  doi          = {10.1007/s10032-022-00424-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {453-474},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Review of chart image detection and classification},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A brief review of state-of-the-art object detectors on
benchmark document images datasets. <em>IJDAR</em>, <em>26</em>(4),
433–451. (<a href="https://doi.org/10.1007/s10032-023-00431-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document image analysis (DIA) has become a challenging brand in computer vision, which is the foundation of document understanding applications. Page object detection is one of the crucial tasks in DIA, locating instances of semantic objects and graphical objects on a page of documents. Based on the development of deep learning algorithms, the performance of detectors is greatly improved. Therefore, this paper reviews the development of page object detection and publicly available document image datasets in recent years. The research also empirically evaluates the state-of-the-art object detection methods on page object detection problems. In particular, we experiment with fourteen methods on two benchmark datasets: IIIT-AR-13k and UIT-DODV-Ext. Finally, we discuss the architecture and performance of these methods to demonstrate their effectiveness and point out a set of development trends. The analysis provides a comprehensive evaluation, which follows state-of-the-art algorithms and future research.},
  archive      = {J_IJDAR},
  author       = {Nguyen, Trong Thuan and Le, Hai and Nguyen, Truong and Vo, Nguyen D. and Nguyen, Khang},
  doi          = {10.1007/s10032-023-00431-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {433-451},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A brief review of state-of-the-art object detectors on benchmark document images datasets},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end pipeline for historical censuses processing.
<em>IJDAR</em>, <em>26</em>(4), 419–432. (<a
href="https://doi.org/10.1007/s10032-023-00428-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censuses are structured documents of great value for social and demographic history, which became widespread from the nineteenth century on. However, the plurality of formats and the natural variability of historical data make their extraction arduous and often lead to ungeneric recognition algorithms. We propose an end-to-end processing pipeline, based on optimization, in an attempt to reduce the number of free parameters. The layout analysis is based on semantic segmentation using neural networks for a generic recognition of the explicit column structure. The implicit row structure is deduced directly from the position of the text segments. The handwritten text detection is complemented by an intelligent framing method which significantly improves the quality of the HTR. In the end, we propose to combine several post-correction approaches, neural networks, and language models, to further improve the performance. Ultimately, our flexible methods make it possible to accurately detect more than 98% of the columns and 88% of the rows, despite the lack of graphical separator and the diversity of formats. Thanks to various reframing and post-correction strategies, HTR results reach the excellent performance of 3.44% character error rate on these noisy nineteenth century data. In total, more than 18,831 pages were extracted in 72 censuses over a century. This large historical dataset, as well as training data, is made open-access and released along with this article.},
  archive      = {J_IJDAR},
  author       = {Petitpierre, Rémi and Kramer, Marion and Rappo, Lucas},
  doi          = {10.1007/s10032-023-00428-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {419-432},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {An end-to-end pipeline for historical censuses processing},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HWNet v3: A joint embedding framework for recognition and
retrieval of handwritten text. <em>IJDAR</em>, <em>26</em>(4), 401–417.
(<a href="https://doi.org/10.1007/s10032-022-00423-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an efficient label embedding framework for word images enables effective word spotting of handwritten documents. In this work, we propose different schemes of label embedding for word images using deep neural architectures and their representations. We refer to our first scheme as the two-stage label embedding technique which projects both word images and their corresponding textual strings into a common subspace. We further introduce an end-to-end label embedding scheme using deep neural architecture which simplifies the embedding process and reports state-of-the-art performance for the task of word spotting and recognition. We also validate the role of synthetic data as a complementary modality to further enhance the embedding process. On the challenging IAM handwritten dataset, we report an mAP of 0.9753 for query-by-string-based word spotting, while under lexicon-based word recognition, our proposed method reports 1.67 and 3.62 character and word error rates, respectively. We also present the detailed ablation study on various variants of our end-to-end embedding architecture and perform analysis under varying embedding sizes. We further validate the embedding scheme on degraded printed document datasets from both Latin and Indic scripts.},
  archive      = {J_IJDAR},
  author       = {Krishnan, Praveen and Dutta, Kartik and Jawahar, C. V.},
  doi          = {10.1007/s10032-022-00423-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {401-417},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {HWNet v3: A joint embedding framework for recognition and retrieval of handwritten text},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory-based recognition of in-air handwritten assamese
words using a hybrid classifier network. <em>IJDAR</em>, <em>26</em>(4),
375–400. (<a href="https://doi.org/10.1007/s10032-022-00426-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-air handwriting is a rapidly emerging human–machine interactive paradigm that helps users to write and communicate naturally and intuitively in free space. In this paper, we develop a hybrid one-dimensional convolutional recurrent attention framework model for in-air handwritten Assamese word recognition (IAHAWR) which associates an encoder and a decoder framework for efficiently recognizing air-written words. The encoder is an assimilation of 1D convolutional neural network and bidirectional gated recurrent unit neural network for input trajectory feature sequence learning, while the decoder is an attention-based gated recurrent unit for predicting the target words. In contrast to conventional pen-based handwriting, in-air handwriting is intricate in the sense that the handwriting is finished in a single continuous stroke giving rise to many irrelevant motions called ligatures in between adjacent character strokes. So, we have imbibed a salient stroke extraction and a critical point detection scheme into the proposed system, which helps in removal of insignificant ligatures thus enhancing the recognition performance. Further, air-writing trajectories contain intermittent jitters and suffer wide variations in writing patterns due to unrestricted writing in free space. So, we incorporate a multistage word normalization methodology which generalizes the air-written patterns and aids in efficient recognition. We have assessed the performance of our proposed system on an air-written Assamese word dataset as well as some air-written Latin words. Experimental evaluation connotes that our proposed IAHAWR system can effectively procure characteristic information from air-writing sequences and provides comparable recognition accuracy and computational performance with that of other state-of-the-art recognition frameworks.},
  archive      = {J_IJDAR},
  author       = {Choudhury, Ananya and Sarma, Kandarpa Kumar},
  doi          = {10.1007/s10032-022-00426-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {12},
  number       = {4},
  pages        = {375-400},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Trajectory-based recognition of in-air handwritten assamese words using a hybrid classifier network},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LSTM-based siamese neural network for urdu news story
segmentation. <em>IJDAR</em>, <em>26</em>(3), 363–373. (<a
href="https://doi.org/10.1007/s10032-023-00441-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News story segmentation is a challenging task mainly due to the dynamic range of topics, smooth story transitions, and varied duration of each story. This paper presents a technique to segment stories from Urdu news bulletins. The technique relies on a Long Short-Term Memory-based Siamese neural network that is trained on positive (belonging to the same story) and negative (belonging to different stories) pairs of sentences. The model, once trained, identifies the transition between stories by detecting the dissimilarity between the adjacent sentences of a given text. For algorithmic development and experimental study, we employ two datasets, a dataset of Urdu news as well as transcriptions of news bulletins from multiple news channels. Experiments report promising results in identifying story boundaries validating the ideas put forward in this study.},
  archive      = {J_IJDAR},
  author       = {Bhatti, Muhammad Nauman Ahmed and Siddiqi, Imran and Moetesum, Momina},
  doi          = {10.1007/s10032-023-00441-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {363-373},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {LSTM-based siamese neural network for urdu news story segmentation},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end optical music recognition for pianoform sheet
music. <em>IJDAR</em>, <em>26</em>(3), 347–362. (<a
href="https://doi.org/10.1007/s10032-023-00432-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end solutions have brought about significant advances in the field of Optical Music Recognition. These approaches directly provide the symbolic representation of a given image of a musical score. Despite this, several documents, such as pianoform musical scores, cannot yet benefit from these solutions since their structural complexity does not allow their effective transcription. This paper presents a neural method whose objective is to transcribe these musical scores in an end-to-end fashion. We also introduce the GrandStaff dataset, which contains 53,882 single-system piano scores in common western modern notation. The sources are encoded in both a standard digital music representation and its adaptation for current transcription technologies. The method proposed in this paper is trained and evaluated using this dataset. The results show that the approach presented is, for the first time, able to effectively transcribe pianoform notation in an end-to-end manner.},
  archive      = {J_IJDAR},
  author       = {Ríos-Vila, Antonio and Rizo, David and Iñesta, José M. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s10032-023-00432-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {347-362},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {End-to-end optical music recognition for pianoform sheet music},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Line extraction in handwritten documents via instance
segmentation. <em>IJDAR</em>, <em>26</em>(3), 335–346. (<a
href="https://doi.org/10.1007/s10032-023-00438-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extraction of text lines from handwritten document images is important for downstream text recognition tasks. It is challenging since handwritten documents do not follow strict rules. Significant variations in line, word, and character spacing and line skews are acceptable as long as the text remains legible. Traditional rule-based methods that work well for printed documents do not carry over to the handwritten domain. In this work, lines are treated as objects to leverage the power of deep learning-based object detection and segmentation frameworks. A key benefit of learnable models is that lines can be implicitly defined through annotations of training images which allows unwanted textual content to be ignored when required. A deep instance segmentation model trained in end-to-end fashion without any dataset-specific pre- or post-processing achieves 0.858 pixel IU and 0.899 line IU scores averaged over 9 different datasets comprising a wide variety of handwritten scripts, layouts, page backgrounds, line orientations, and interline spacings. It achieves state-of-the-art results on DIVA-HisDB, VML-AHTE, and READ-BAD datasets and almost state-of-the-art results on Digital Peter, ICDAR2015-HTR, ICDAR2017, and Bozen datasets. We also introduce a new, annotated dataset for Urdu script. Our model trained only on Urdu generalizes to multiple other scripts, indicating that it learns a script-invariant representation of text lines. All code, pre-trained models, and the new Urdu dataset can be accessed at https://github.com/AdeelaIslam/HLExt-via-IS .},
  archive      = {J_IJDAR},
  author       = {Islam, Adeela and Anjum, Tayaba and Khan, Nazar},
  doi          = {10.1007/s10032-023-00438-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {335-346},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Line extraction in handwritten documents via instance segmentation},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accurate approach to real-time machine-readable zone
detection with mobile devices. <em>IJDAR</em>, <em>26</em>(3), 321–334.
(<a href="https://doi.org/10.1007/s10032-023-00435-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider a problem of machine-readable zone (MRZ) detection in document images on mobile devices. MRZ recognition is actively used for fast and reliable automatic personal data extraction from passports, IDs and visas. However, due to the low computing power and limited battery life of most mobile devices, the requirements for the complexity of the used models increase significantly. We present a state-of-the-art MRZ detection approach based on YOLO-MRZ—extremely fast, compact and accurate deep learning model. We consider the MRZ as a graphical object and use the object detection approach to find it. Proposed YOLO-MRZ is 83 times faster than Tiny YOLO v3, weights only 1 MB and well suited for embedded systems and mobile devices: It achieved 62 FPS on the Apple iPhone SE (2020). We address the small-scale MRZ detection problem with two-stage approach in which the YOLO-MRZ model is run twice: If the detected MRZ bounding box is too small or does not meet geometric criteria, we construct the ROI image based on it and run the same detector in the ROI. To assess the quality, we have tested it on 4 public datasets: SyntheticMRZ, MIDV-500, MIDV-2019 and MIDV-2020. Our approach outperforms all other solutions by a wide margin.},
  archive      = {J_IJDAR},
  author       = {Gayer, Alexander and Ershova, Daria and Arlazarov, Vladimir V.},
  doi          = {10.1007/s10032-023-00435-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {321-334},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {An accurate approach to real-time machine-readable zone detection with mobile devices},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IAMonSense: Multi-level handwriting classification using
spatiotemporal information. <em>IJDAR</em>, <em>26</em>(3), 303–319. (<a
href="https://doi.org/10.1007/s10032-023-00433-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online handwriting classification has become an open research problem as it serves as a preliminary step for handwriting recognition systems and applications in several other fields. This paper aims to extend the current trends and knowledge with multiple contributions in handwriting classification using spatiotemporal information. Firstly, it enriches the annotations of several publicly available online handwriting datasets, SenseThePen, IAM-onDB, and IAMonDo, for online handwriting classification and recognition tasks. The said datasets are updated with three distinguished levels of annotations, i.e., stroke, sequence, and line levels. The enriched annotations of these datasets extend their functionality for online handwriting classification at different levels for further research analysis. In addition to enrichment, it also unifies the annotation levels across the datasets, which enables the research community to benchmark proposed methods for comparative analysis using multiple datasets. All the datasets with enriched annotations are made publicly available for the research community as part of the IAMonSense dataset. Moreover, this paper presents a comprehensive benchmark of these datasets using multiple deep neural networks such as traditional convolutional neural networks (CNNs), graph convolutional networks(GCNs), attention-based neural networks, and transformers. These benchmarks can be used later on for further development in online handwriting classification.},
  archive      = {J_IJDAR},
  author       = {Mustafid, Ahmad and Younas, Junaid and Lukowicz, Paul and Ahmed, Sheraz},
  doi          = {10.1007/s10032-023-00433-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {303-319},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {IAMonSense: Multi-level handwriting classification using spatiotemporal information},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online handwriting trajectory reconstruction from kinematic
sensors using temporal convolutional network. <em>IJDAR</em>,
<em>26</em>(3), 289–302. (<a
href="https://doi.org/10.1007/s10032-023-00430-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting with digital pens is a common way to facilitate human–computer interaction through the use of online handwriting (OH) trajectory reconstruction. In this work, we focus on a digital pen equipped with sensors from which one wants to reconstruct the OH trajectory. Such a pen allows to write on any surface and to get the digital trace, which can help learning to write, by writing on paper, and can be useful for many other applications such as collaborative meetings, etc. In this paper, we introduce a novel processing pipeline that maps the sensor signals of the pen to the corresponding OH trajectory. Notably, in order to tackle the difference of sampling rates between the pen and the tablet (which provides ground truth information), our preprocessing pipeline relies on Dynamic Time Warping to align the signals. We introduce a dedicated neural network architecture, inspired by a Temporal Convolutional Network, to reconstruct the online trajectory from the pen sensor signals. Finally, we also present a new benchmark dataset on which our method is evaluated both qualitatively and quantitatively, showing a notable improvement over its most notable competitor.},
  archive      = {J_IJDAR},
  author       = {Swaileh, Wassim and Imbert, Florent and Soullard, Yann and Tavenard, Romain and Anquetil, Eric},
  doi          = {10.1007/s10032-023-00430-1},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {289-302},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Online handwriting trajectory reconstruction from kinematic sensors using temporal convolutional network},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Printed ottoman text recognition using synthetic data and
data augmentation. <em>IJDAR</em>, <em>26</em>(3), 273–287. (<a
href="https://doi.org/10.1007/s10032-023-00436-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Ottoman script, which was in use for over five centuries, is an Arabic alphabet-based writing system. It became obsolete after the change of alphabet in Turkey. There are plenty of Ottoman documents, overwhelmingly printed in Naskh style. This work presents a DL-based character recognition system for the printed Ottoman script. We first generate a synthetic text image dataset from a text corpus and then augment it using some image processing methods. We develop a hybrid convolutional neural network-bidirectional long short-term memory recognizer and train it with the original and the augmented datasets. Finally, we apply a transfer learning procedure for adapting the system to real image data. The proposed system obtains 0.11 CER on synthetic data and 0.16 CER on real data comprising of line images from a printed historical Ottoman book.},
  archive      = {J_IJDAR},
  author       = {Bilgin Tasdemir, Esma F.},
  doi          = {10.1007/s10032-023-00436-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {273-287},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Printed ottoman text recognition using synthetic data and data augmentation},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale genealogical information extraction from
handwritten quebec parish records. <em>IJDAR</em>, <em>26</em>(3),
255–272. (<a href="https://doi.org/10.1007/s10032-023-00427-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a complete workflow designed for extracting information from Quebec handwritten parish registers. The acts in these documents contain individual and family information highly valuable for genetic, demographic and social studies of the Quebec population. From an image of parish records, our workflow is able to identify the acts and extract personal information. The workflow is divided into successive steps: page classification, text line detection, handwritten text recognition, named entity recognition and act detection and classification. For all these steps, different machine learning models are compared. Once the information is extracted, validation rules designed by experts are then applied to standardize the extracted information and ensure its consistency with the type of act (birth, marriage and death). This validation step is able to reject records that are considered invalid or merged. The full workflow has been used to process over two million pages of Quebec parish registers from the 19–20th centuries. On a sample comprising 65% of registers, 3.2 million acts were recognized. Verification of the birth and death acts from this sample shows that 74% of them are considered complete and valid. These records will be integrated into the BALSAC database and linked together to recreate family and genealogical relations at large scale.},
  archive      = {J_IJDAR},
  author       = {Tarride, Solène and Maarand, Martin and Boillet, Mélodie and McGrath, James and Capel, Eugénie and Vézina, Hélène and Kermorvant, Christopher},
  doi          = {10.1007/s10032-023-00427-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {255-272},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Large-scale genealogical information extraction from handwritten quebec parish records},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Historical document image analysis using controlled data for
pre-training. <em>IJDAR</em>, <em>26</em>(3), 241–254. (<a
href="https://doi.org/10.1007/s10032-023-00437-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using neural networks for semantic labeling has become a dominant technique for layout analysis of historical document images. However, to train or fine-tune appropriate models, large labeled datasets are needed. This paper addresses the case when only limited labeled data are available and promotes a novel approach using so-called controlled data to pre-train the networks. Two different strategies are proposed: The first addresses the real labeling task by using artificial data; the second uses real data to pre-train the networks with a pretext task. To assess these strategies, a large set of experiments has been carried out on a text line detection and classification task using different variants of U-Net. The observations, obtained from two different datasets, show that globally the approach reduces the training time while offering similar or better performance. Furthermore, the effect is bigger on lightweight network architectures.},
  archive      = {J_IJDAR},
  author       = {Rahal, Najoua and Vögtlin, Lars and Ingold, Rolf},
  doi          = {10.1007/s10032-023-00437-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {241-254},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Historical document image analysis using controlled data for pre-training},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of incunable glyphs and out-of-distribution
detection with joint energy-based models. <em>IJDAR</em>,
<em>26</em>(3), 223–240. (<a
href="https://doi.org/10.1007/s10032-023-00442-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical character recognition (OCR) has proved a powerful tool for the digital analysis of printed historical documents. However, its ability to localize and identify individual glyphs is challenged by the tremendous variety in historical type design, the physicality of the printing process, and the state of conservation. We propose to mitigate these problems by a downstream fine-tuning step that corrects for pathological and undesirable extraction results. We implement this idea by using a joint energy-based model which classifies individual glyphs and simultaneously prunes potential out-of-distribution (OOD) samples like rubrications, initials, or ligatures. During model training, we introduce specific margins in the energy spectrum that aid this separation and explore the glyph distribution’s typical set to stabilize the optimization procedure. We observe strong classification at 0.972 AUPRC across 42 lower- and uppercase glyph types on a challenging digital reproduction of Johannes Balbus’ Catholicon, matching the performance of purely discriminative methods. At the same time, we achieve OOD detection rates of 0.989 AUPRC and 0.946 AUPRC for OOD ‘clutter’ and ‘ligatures’ which substantially improves upon recently proposed OOD detection techniques. The proposed approach can be easily integrated into the postprocessing phase of current OCR to aid reproduction and shape analysis research.},
  archive      = {J_IJDAR},
  author       = {Kordon, Florian and Weichselbaumer, Nikolaus and Herz, Randall and Mossman, Stephen and Potten, Edward and Seuret, Mathias and Mayr, Martin and Christlein, Vincent},
  doi          = {10.1007/s10032-023-00442-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {223-240},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Classification of incunable glyphs and out-of-distribution detection with joint energy-based models},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheme for palimpsests reconstruction using synthesized
dataset. <em>IJDAR</em>, <em>26</em>(3), 211–222. (<a
href="https://doi.org/10.1007/s10032-023-00439-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Palimpsest Manuscripts Reconstruction Generative Adversarial Network, a novel framework for restoring the original forms of input palimpsests; it removes the over-text and refills the missing gaps in the text and background in an end-to-end manner. The structure and attributes of the under-text are encoded using reference patches. The generator network combines the encoded reference with an input palimpsest patch and restores the original form. To train our model, we synthesize palimpsests that mimic the attributes of the original ones. We compare the performance of our model with the state-of-art models using five different evaluation metrics, such as PSNR and SSIM. We show that our approach not only achieves state-of-the-art performance in terms of PSNR/SSIM metrics but also significantly improves the visual quality of the restored images.},
  archive      = {J_IJDAR},
  author       = {Madi, Boraq and Alaasam, Reem and Shammas, Raed and El-Sana, Jihad},
  doi          = {10.1007/s10032-023-00439-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {211-222},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Scheme for palimpsests reconstruction using synthesized dataset},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing the potential of active learning for document
image classification. <em>IJDAR</em>, <em>26</em>(3), 187–209. (<a
href="https://doi.org/10.1007/s10032-023-00429-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been extensively researched in the field of document analysis and has shown excellent performance across a wide range of document-related tasks. As a result, a great deal of emphasis is now being placed on its practical deployment and integration into modern industrial document processing pipelines. It is well known, however, that deep learning models are data-hungry and often require huge volumes of annotated data in order to achieve competitive performances. And since data annotation is a costly and labor-intensive process, it remains one of the major hurdles to their practical deployment. This study investigates the possibility of using active learning to reduce the costs of data annotation in the context of document image classification, which is one of the core components of modern document processing pipelines. The results of this study demonstrate that by utilizing active learning (AL), deep document classification models can achieve competitive performances to the models trained on fully annotated datasets and, in some cases, even surpass them by annotating only 15–40% of the total training dataset. Furthermore, this study demonstrates that modern AL strategies significantly outperform random querying, and in many cases achieve comparable performance to the models trained on fully annotated datasets even in the presence of practical deployment issues such as data imbalance, and annotation noise, and thus, offer tremendous benefits in real-world deployment of deep document classification models. The code to reproduce our experiments is publicly available at https://github.com/saifullah3396/doc_al .},
  archive      = {J_IJDAR},
  author       = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10032-023-00429-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {187-209},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Analyzing the potential of active learning for document image classification},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inv3D: A high-resolution 3D invoice dataset for
template-guided single-image document unwarping. <em>IJDAR</em>,
<em>26</em>(3), 175–186. (<a
href="https://doi.org/10.1007/s10032-023-00434-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous business workflows involve printed forms, such as invoices or receipts, which are often manually digitalized to persistently search or store the data. As hardware scanners are costly and inflexible, smartphones are increasingly used for digitalization. Here, processing algorithms need to deal with prevailing environmental factors, such as shadows or crumples. Current state-of-the-art approaches learn supervised image dewarping models based on pairs of raw images and rectification meshes. The available results show promising predictive accuracies for dewarping, but generated errors still lead to sub-optimal information retrieval. In this paper, we explore the potential of improving dewarping models using additional, structured information in the form of invoice templates. We provide two core contributions: (1) a novel dataset, referred to as Inv3D, comprising synthetic and real-world high-resolution invoice images with structural templates, rectification meshes, and a multiplicity of per-pixel supervision signals and (2) a novel image dewarping algorithm, which extends the state-of-the-art approach GeoTr to leverage structural templates using attention. Our extensive evaluation includes an implementation of DewarpNet and shows that exploiting structured templates can improve the performance for image dewarping. We report superior performance for the proposed algorithm on our new benchmark for all metrics, including an improved local distortion of 26.1 %. We made our new dataset and all code publicly available at https://felixhertlein.github.io/inv3d .},
  archive      = {J_IJDAR},
  author       = {Hertlein, Felix and Naumann, Alexander and Philipp, Patrick},
  doi          = {10.1007/s10032-023-00434-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {175-186},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Inv3D: A high-resolution 3D invoice dataset for template-guided single-image document unwarping},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial for special issue on “advanced topics in document
analysis and recognition.” <em>IJDAR</em>, <em>26</em>(3), 171–173. (<a
href="https://doi.org/10.1007/s10032-023-00448-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Kise, Koichi and Zanibbi, Richard and Jain, Rajiv and Fink, Gernot A.},
  doi          = {10.1007/s10032-023-00448-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {171-173},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Editorial for special issue on “advanced topics in document analysis and recognition”},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive dewarping of severely warped camera-captured
document images based on document map generation. <em>IJDAR</em>,
<em>26</em>(2), 149–169. (<a
href="https://doi.org/10.1007/s10032-022-00425-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated dewarping of camera-captured handwritten documents is a challenging research problem in Computer Vision and Pattern Recognition. Most available systems assume the shape of the camera-captured image boundaries to be anywhere between trapezoidal and octahedral, with linear distortion in areas between the boundaries for dewarping. The majority of the state-of-the-art applications successfully dewarp the simple-to-medium range geometrical distortions with partial selection of control points by a user. The proposed work implements a fully automated technique for control point detection from simple-to-complex geometrical distortions in camera-captured document images. The input image is subject to preprocessing, corner point detection, document map generation, and rendering of the de-warped document image. The proposed algorithm has been tested on five different camera-captured document datasets (one internal and four external publicly available) consisting of 958 images. Both quantitative and qualitative evaluations have been performed to test the efficacy of the proposed system. On the quantitative front, an Intersection Over Union (IoU) score of 0.92, 0.88, and 0.80 for document map generation for low-, medium-, and high-complexity datasets, respectively. Additionally, accuracies of the recognized texts, obtained from a market leading OCR engine, are utilized for quantitative comparative analysis on document images before and after the proposed enhancement. Finally, the qualitative analysis visually establishes the system’s reliability by demonstrating improved readability even for severely distorted image samples.},
  archive      = {J_IJDAR},
  author       = {Nachappa, C. H. and Rani, N. Shobha and Pati, Peeta Basa and Gokulnath, M.},
  doi          = {10.1007/s10032-022-00425-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {149-169},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Adaptive dewarping of severely warped camera-captured document images based on document map generation},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refocus attention span networks for handwriting line
recognition. <em>IJDAR</em>, <em>26</em>(2), 131–147. (<a
href="https://doi.org/10.1007/s10032-022-00422-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks have achieved outstanding recognition performance for handwriting identification despite the enormous variety observed across diverse handwriting structures and poor-quality scanned documents. We initially proposed a BiLSTM baseline model with a sequential architecture well-suited for modeling text lines due to its ability to learn probability distributions over character or word sequences. However, employing such recurrent paradigms prevents parallelization and suffers from vanishing gradients for long sequences during training. To alleviate these limitations, we propose four significant contributions to this work. First, we devised an end-to-end model composed of a split-attention CNN-backbone that serves as a feature extraction method and a self-attention Transformer encoder–decoder that serves as a transcriber method to recognize handwriting manuscripts. The multi-head self-attention layers in an encoder–decoder transformer-based enhance the model’s ability to tackle handwriting recognition and learn the linguistic dependencies of character sequences. Second, we conduct various studies on transfer learning (TL) from large datasets to a small database, determining which model layers require fine-tuning. Third, we attained an efficient paradigm by combining different strategies of TL with data augmentation (DA). Finally, since the robustness of the proposed model is lexicon-free and can recognize sentences not presented in the training phase, the model is only trained on a few labeled examples with no extra cost of generating and training on synthetic datasets. We recorded comparable and outperformed Character and Word Error Rates CER/WER on four benchmark datasets to the most recent (SOTA) models.},
  archive      = {J_IJDAR},
  author       = {Hamdan, Mohammed and Chaudhary, Himanshu and Bali, Ahmed and Cheriet, Mohamed},
  doi          = {10.1007/s10032-022-00422-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {131-147},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Refocus attention span networks for handwriting line recognition},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tables to LaTeX: Structure and content extraction from
scientific tables. <em>IJDAR</em>, <em>26</em>(2), 121–130. (<a
href="https://doi.org/10.1007/s10032-022-00420-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific documents contain tables that list important information in a concise fashion. Structure and content extraction from tables embedded within PDF research documents is a very challenging task due to the existence of visual features like spanning cells and content features like mathematical symbols and equations. Most existing table structure identification methods tend to ignore these academic writing features. In this paper, we adapt the transformer-based language modeling paradigm for scientific table structure and content extraction. Specifically, the proposed model converts a tabular image to its corresponding LaTeX source code. Overall, we outperform the current state-of-the-art baselines and achieve an exact match accuracy of 70.35 and 49.69% on table structure and content extraction, respectively. Further analysis demonstrates that the proposed models efficiently identify the number of rows and columns, the alphanumeric characters, the LaTeX tokens, and symbols.},
  archive      = {J_IJDAR},
  author       = {Kayal, Pratik and Anand, Mrinal and Desai, Harsh and Singh, Mayank},
  doi          = {10.1007/s10032-022-00420-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {121-130},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Tables to LaTeX: Structure and content extraction from scientific tables},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retrieval-based language model adaptation for handwritten
chinese text recognition. <em>IJDAR</em>, <em>26</em>(2), 109–119. (<a
href="https://doi.org/10.1007/s10032-022-00419-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In handwritten text recognition, compared to human, computers are far short of linguistic context knowledge, especially domain-matched knowledge. In this paper, we present a novel retrieval-based method to obtain an adaptive language model for offline recognition of unconstrained handwritten Chinese texts. The content of handwritten texts to be recognized is varied and usually unknown a priori. Therefore we adopt a two-pass recognition strategy. In the first pass, we utilize a common language model to obtain initial recognition results, which are used to retrieve the related contents from Internet. In the content retrieval, we evaluate different types of semantic representation from BERT output and the traditional TF–IDF representation. Then, we dynamically generate an adaptive language model from these related contents, which will consequently be combined with the common language model and applied in the second-pass recognition. We evaluate the proposed method on two benchmark unconstrained handwriting datasets, namely CASIA-HWDB and ICDAR-2013. Experimental results show that the proposed retrieval-based language model adaptation yields improvements in recognition performance, despite the reduced Internet contents hereby employed.},
  archive      = {J_IJDAR},
  author       = {Hu, Shuying and Wang, Qiufeng and Huang, Kaizhu and Wen, Min and Coenen, Frans},
  doi          = {10.1007/s10032-022-00419-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {109-119},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Retrieval-based language model adaptation for handwritten chinese text recognition},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WriterINet: A multi-path deep CNN for offline
text-independent writer identification. <em>IJDAR</em>, <em>26</em>(2),
89–107. (<a href="https://doi.org/10.1007/s10032-022-00418-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting-based identification is a fundamental pattern recognition problem that has attracted considerable interest in recent years. Writer identification is a major challenge, considering how diverse the written content is and how much handwriting differs between writers. This paper presents WriterINet, a CNN-based approach to learning and characterizing each writer’s writing style. The proposed WriterINet approach takes handwritten documents as input, decomposing each of them into word images and connected component sub-images. Each segmented image is then fed into our feature learning step to generate discriminative and deep features. To this end, we developed a powerful deep CNN feature architecture consisting of two CNN streams derived from fine-tuning the ResNet-50 and DenseNet-201 models. Deep learned features are computed from all segmented images representing the writer’s document and classified using a proposed 1D artificial neural network for predicting writer identification by averaging the similarity scores. Experimental results on IAM, ICDAR2013, CVL, IFN $$/$$ ENIT, ICDAR2011, Firemaker and CERUG show that our WriterINet approach achieves the highest or competitive performance over the state-of-the-art.},
  archive      = {J_IJDAR},
  author       = {Chahi, A. and El merabet, Y. and Ruichek, Y. and Touahni, R.},
  doi          = {10.1007/s10032-022-00418-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {89-107},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {WriterINet: A multi-path deep CNN for offline text-independent writer identification},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cover-based multiple book genre recognition using an
improved multimodal network. <em>IJDAR</em>, <em>26</em>(1), 65–88. (<a
href="https://doi.org/10.1007/s10032-022-00413-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the idiom not to prejudge something by its outward appearance, we consider deep learning to learn whether we can judge a book by its cover or, more precisely, by its text and design. The classification was accomplished using three strategies, i.e., text only, image only, and both text and image. State-of-the-art CNNs (convolutional neural networks) models were used to classify books through cover images. The Gram and SE layers (squeeze and excitation) were used as an attention unit in them to learn the optimal features and identify characteristics from the cover image. The Gram layer enabled more accurate multi-genre classification than the SE layer. The text-based classification was done using word-based, character-based, and feature engineering-based models. We designed EXplicit interActive Network (EXAN) composed of context-relevant layers and multi-level attention layers to learn features from books title. We designed an improved multimodal fusion architecture for multimodal classification that uses an attention mechanism between modalities. The disparity in modalities convergence speed is addressed by pre-training each sub-network independently prior to end-to-end training of the model. Two book cover datasets were used in this study. Results demonstrated that text-based classifiers are superior to image-based classifiers. The proposed multimodal network outperformed all models for this task with the highest accuracy of 69.09% and 38.12% for Latin and Arabic book cover datasets. Similarly, the proposed EXAN surpassed the extant text classification models by scoring the highest prediction rates of 65.20% and 33.8% for Latin and Arabic book cover datasets.},
  archive      = {J_IJDAR},
  author       = {Rasheed, Assad and Umar, Arif Iqbal and Shirazi, Syed Hamad and Khan, Zakir and Shahzad, Muhammad},
  doi          = {10.1007/s10032-022-00413-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {65-88},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Cover-based multiple book genre recognition using an improved multimodal network},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pho(SC)-CTC—a hybrid approach towards zero-shot word image
recognition. <em>IJDAR</em>, <em>26</em>(1), 51–63. (<a
href="https://doi.org/10.1007/s10032-022-00407-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating words in a historical document image archive for word image recognition purpose demands time and skilled human resource (like historians, paleographers). In a real-life scenario, obtaining sample images for all possible words is also not feasible. However, zero-shot learning methods could aptly be used to recognize unseen/out-of-lexicon words in such historical document images. Based on previous state-of-the-art method for zero-shot word recognition “Pho(SC)Net”, we propose a hybrid model based on the CTC framework (Pho(SC)-CTC) that takes advantage of the rich features learned by Pho(SC)Net followed by a “connectionist temporal classification” (CTC) framework to perform the final classification. Encouraging results were obtained on two publicly available historical document datasets and one synthetic handwritten dataset, which justifies the efficacy of Pho(SC)-CTC and Pho(SC)Net.},
  archive      = {J_IJDAR},
  author       = {Bhatt, Ravi and Rai, Anuj and Chanda, Sukalpa and Krishnan, Narayanan C.},
  doi          = {10.1007/s10032-022-00407-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {51-63},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Pho(SC)-CTC—a hybrid approach towards zero-shot word image recognition},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence-aware multimodal page classification of brazilian
legal documents. <em>IJDAR</em>, <em>26</em>(1), 33–49. (<a
href="https://doi.org/10.1007/s10032-022-00406-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Brazilian Supreme Court receives tens of thousands of cases each semester. Court employees spend thousands of hours to execute the initial analysis and classification of those cases—which takes effort away from posterior, more complex stages of the case management workflow. In this paper, we explore multimodal classification of documents from Brazil’s Supreme Court. We train and evaluate our methods on a novel multimodal dataset of 6510 lawsuits (339,478 pages) with manual annotation assigning each page to one of six classes. Each lawsuit is an ordered sequence of pages, which are stored both as an image and as a corresponding text extracted through optical character recognition. We first train two unimodal classifiers: A ResNet pre-trained on ImageNet is fine-tuned on the images, and a convolutional network with filters of multiple kernel sizes is trained from scratch on document texts. We use them as extractors of visual and textual features, which are then combined through our proposed fusion module. Our fusion module can handle missing textual or visual input by using learned embeddings for missing data. Moreover, we experiment with bidirectional long short-term memory (biLSTM) networks and linear-chain conditional random fields to model the sequential nature of the pages. The multimodal approaches outperform both textual and visual classifiers, especially when leveraging the sequential nature of the pages.},
  archive      = {J_IJDAR},
  author       = {Luz de Araujo, Pedro H. and de Almeida, Ana Paula G. S. and Ataides Braz, Fabricio and Correia da Silva, Nilton and de Barros Vidal, Flavio and de Campos, Teofilo E.},
  doi          = {10.1007/s10032-022-00406-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {33-49},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Sequence-aware multimodal page classification of brazilian legal documents},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open writer identification from offline handwritten
signatures by jointing the one-class symbolic data analysis classifier
and feature-dissimilarities. <em>IJDAR</em>, <em>26</em>(1), 15–31. (<a
href="https://doi.org/10.1007/s10032-022-00403-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Usually, a large number of reference signatures are required for building the writing style model from offline handwritten signatures (OHSs). Moreover, the existing writer identification systems from OHSs are generally closed systems that require a retraining process when a new writer is added. This paper proposes an open writer identification system from OHSs, based on a new scheme of the one-class symbolic data analysis (OC-SDA) classifier, using few reference signatures. For generating more data, intra-class feature-dissimilarities, generated from curvelet transform, are introduced for building the symbolic representation model (SRM) associated with each writer. Feature-dissimilarities allow capturing more efficiently the intra-personnel variability produced naturally by a writer and, thus, increase the inter-personnel variability. Instead of using the mean and the standard deviation for building the OC-SDA model, intra-class feature-dissimilarities generated for each writer are modeled through a new weighted membership function, inspired from the real probability distribution of training intra-class feature-dissimilarities. The comparative analysis against the state-of-the-art works shows that the proposed OC-SDA classifier outperforms the existing classifiers on three public signature datasets GPDS-300, CEDAR-55 and MCYT-75, using only five reference signatures, achieving 98.31%, 98.06% and 99.89%, respectively, even when a combination of multiple classifiers is performed or even using learned features. Moreover, the evaluation of the proposed writer identification system in front of skilled forgeries shows its ability to detect also possible forged signatures in addition to the genuine ones.},
  archive      = {J_IJDAR},
  author       = {Djoudjai, Mohamed Anis and Chibani, Youcef},
  doi          = {10.1007/s10032-022-00403-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {15-31},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Open writer identification from offline handwritten signatures by jointing the one-class symbolic data analysis classifier and feature-dissimilarities},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). YOLO-table: Disclosure document table detection with
involution. <em>IJDAR</em>, <em>26</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10032-022-00400-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As financial document automation becomes more general, table detection is receiving more and more attention as an important part of document automation. Disclosure documents contain both bordered and borderless tables of varying lengths, and there is currently no model that performs well on these types of documents. To solve this problem, we propose a table detection model based on YOLO-table. We introduce involution into the backbone of the network to improve the network’s ability to learn table spatial layout features and design a simple Feature Pyramid Network to improve model effectiveness. In addition, this paper proposes a table-based augment method. We experiment on a disclosure document dataset, and the results show that the F1-measure of the YOLO-table reaches 97.3%. Compared with YOLOv3, our method improves the accuracy by 2.8% and the speed by 1.25 times. It also evaluates the ICDAR2013 and ICDAR2019 Table Competition datasets and achieves state-of-the-art performance.},
  archive      = {J_IJDAR},
  author       = {Zhang, Daqian and Mao, Ruibin and Guo, Runting and Jiang, Yang and Zhu, Jing},
  doi          = {10.1007/s10032-022-00400-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {YOLO-table: Disclosure document table detection with involution},
  volume       = {26},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
