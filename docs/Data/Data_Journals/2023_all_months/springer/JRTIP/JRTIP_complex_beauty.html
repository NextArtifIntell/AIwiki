<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---123">JRTIP - 123</h2>
<ul>
<li><details>
<summary>
(2023). How to compute the convex hull of a binary shape? A
real-time algorithm to compute the convex hull of a binary shape.
<em>JRTIP</em>, <em>20</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01359-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an algorithm to compute the convex hull of a binary shape. Efficient algorithms to compute the convex hull of a set of points had been proposed long time ago. For a binary shape, the common practice is to rely on one of them: to compute the convex hull of binary shape, all pixels of the shape are first listed, and then the convex hull is computed on this list of points. The computed convex hull is finally rasterized to provide the final result [as, for example, in the famous scikit-image library (scikit-image: Image processing in python. https://scikit-image.org )]. To compute the convex hull of an arbitrary set of points, the points of the list that lie on the outline of the convex hull must be selected (to simplify, we call these points “extrema”). To find them, for an arbitrary set of points, it is necessary to browse all the points but not in the particular case of a binary shape. In this specific situation, the extrema necessarily belong to the inner boundary of the shape. It is a waste of time to browse all the pixels as it is possible to discard most of them when we search for these extrema. Based on this analysis, we propose a new method to compute the convex hull dedicated to binary shapes. This method browses as few pixels as possible to select a small subset of boundary pixels. Then it deduces the convex hull only from this subset. As the size of the subset is very small, the convex hull is computed in real time. We compare it with the commonly used methods and common functions from libraries to prove that our approach is faster. This comparison shows that, for a very small shape, the difference is acceptable, but when the area of the shape grows, this difference becomes significant. This leads us to conclude that substituting current functions to compute convex hull of binary shapes with our algorithm in frequently used libraries would lead to a great improvement.},
  archive      = {J_JRTIP},
  author       = {Fabrizio, Jonathan},
  doi          = {10.1007/s11554-023-01359-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {How to compute the convex hull of a binary shape? a real-time algorithm to compute the convex hull of a binary shape},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time quality improvement method based on an adaptive
dynamic screened poisson equation for surveillance video in sand-dust
weather. <em>JRTIP</em>, <em>20</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01361-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The videos captured by the surveillance equipment in sand-dust weather will have poor visibility, color distortion, and low contrast, and the performance of surveillance systems is seriously interfered. To solve this problem, a real-time quality improvement method based on an adaptive dynamic screened Poisson equation for surveillance video in sand-dust weather is proposed in the paper. In the proposed method, the surveillance video collected in sand-dust weather is first divided into several segments using a keyframe extraction method, and then each segment of the surveillance video is processed according to the frame difference strategy to obtain the enhanced surveillance video. There are three steps in processing surveillance video segments, one is to extract the background frame using a multi-frame averaging method, the second is to enhance the background frame using an automatically improving frame quality method based on screened Poisson equation, and the third is to use the enhanced background image and the frame difference information to obtain the enhancement result of the frame image to be processed. Through qualitative and quantitative comprehensive experiments on sand-dust images and videos, the experimental results are compared with the existing related methods, the results of processing sand-dust images using our improving frame quality method have the best visual effect and the highest total scores in quantitative analysis. The results of the frame difference strategy show an average 11.36 $$\times$$ speed up as compared with the framewise quality improvement method and realize the goal of real-time processing surveillance video.},
  archive      = {J_JRTIP},
  author       = {Ni, Dongdong and Jia, Zhenhong and Yang, Jie and Kasabov, Nikola},
  doi          = {10.1007/s11554-023-01361-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time quality improvement method based on an adaptive dynamic screened poisson equation for surveillance video in sand-dust weather},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time online unsupervised domain adaptation for
real-world person re-identification. <em>JRTIP</em>, <em>20</em>(6),
1–12. (<a href="https://doi.org/10.1007/s11554-023-01362-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap toward practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation ( $$\hbox {R}^2$$ OUDA) for Person Re-identification. The $$\hbox {R}^2$$ OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new $$\hbox {R}^2$$ OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean Teaching ( $$\hbox {R}^2$$ MMT), a novel multi-camera system for real-world person re-identification. Taking a popular person re-identification dataset, $$\hbox {R}^2$$ MMT was used to construct over 100 data subsets and train more than 3000 models, exploring the breadth of the $$\hbox {R}^2$$ OUDA setting to understand the training time and accuracy trade-offs and limitations for real-world applications. $$\hbox {R}^2$$ MMT, a real-world system able to respect the strict constraints of the proposed $$\hbox {R}^2$$ OUDA setting, achieves accuracies within $$0.1\%$$ of comparable OUDA methods that cannot be applied directly to real-world applications.},
  archive      = {J_JRTIP},
  author       = {Neff, Christopher and Danesh Pazho, Armin and Tabkhi, Hamed},
  doi          = {10.1007/s11554-023-01362-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time online unsupervised domain adaptation for real-world person re-identification},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast detection of bag-breakups in pulsating and steady
airflow using video analysis and deep learning. <em>JRTIP</em>,
<em>20</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01363-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection methods based on deep learning have made great progress in recent years and have been used successfully in many different applications. However, since they have been evaluated predominantly on datasets of natural images, it is still unclear how accurate and effective they can be if used in special domain applications, for example in scientific, industrial, etc. images, where the properties of the images are very different from those taken in natural scenes. In this study, we illustrate the challenges one needs to face in such a setting on a concrete practical application, involving the detection of a particular fluid phenomenon—bag-breakup—in images of droplet scattering, which differ significantly from natural images. Using two technologically mature and state-of-the-art object detection methods, RetinaNet and YOLOv7, we discuss what strategies need to be considered in this problem setting, and perform both quantitative and qualitative evaluations to study their effects. Additionally, we also propose a new method to further improve accuracy of detection by utilizing information from several consecutive frames. We hope that the practical insights gained in this study can be of use to other researchers and practitioners when targeting applications where the images differ greatly from natural images.},
  archive      = {J_JRTIP},
  author       = {Morita, Daiki and Raytchev, Bisser and Elhanashi, Abdussalam and Kawaguchi, Mikimasa and Ogata, Yoichi and Higaki, Toru and Kaneda, Kazufumi and Nakashima, Akira and Saponara, Sergio},
  doi          = {10.1007/s11554-023-01363-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast detection of bag-breakups in pulsating and steady airflow using video analysis and deep learning},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight container code recognition based on multi-reuse
feature fusion and multi-branch structure merger. <em>JRTIP</em>,
<em>20</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01364-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of global shipping, intelligent container code recognition has become a popular research topic. To address the issues of poor recognition accuracy and real-time performance in the complex port environment, this paper proposes a real-time container code recognition method based on YOLOV5. The proposed method consists of two stages: container code localization and container code recognition. First, for container code localization, the backbone component consists of MobileOne blocks to enhance feature extraction capability and accelerate inference speed. Second, for container code recognition, a lightweight MobileNetV3 is used as the backbone. Third, a feature extraction module, called Multiple Reuse Feature Pyramid Networks (MRFPN), is designed to extract sufficient semantic information. Experimental results show that the algorithm achieves accuracy rates of 96.5% and 93.5% in the container code localization and container code recognition stages, respectively, while significantly reducing the number of parameters, computations, and model size, and improving inference speed. Additionally, the proposed method performs better in terms of real-time performance and can meet the requirements of practical applications.},
  archive      = {J_JRTIP},
  author       = {Yang, Dapeng and Wang, Guanghui and Liu, Mingtang and Yue, Shuang and Zhang, Hao and Chen, Xiaokang and Zhang, Mengxiao},
  doi          = {10.1007/s11554-023-01364-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight container code recognition based on multi-reuse feature fusion and multi-branch structure merger},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on coal gangue classification recognition method
based on the combination of CNN and SVM. <em>JRTIP</em>, <em>20</em>(6),
1–10. (<a href="https://doi.org/10.1007/s11554-023-01365-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the traditional machine learning methods rely on manual experience and deep learning classification model depth and complex structure, resulting in poor gangue classification performance, this paper proposes a coal gangue recognition method (CNN-SVM) based on the combination of convolutional neural network (CNN) and support vector machine (SVM). Firstly, we use a generative adversarial network (DCGAN) to generate new coal gangue samples and expand the gangue dataset by traditional image enhancement techniques to increase the data samples and improve the generalization of the model; then we construct an efficient and simple CNN as a coal gangue feature extractor and verify the effect of convolutional kernel size on the accuracy of the model, and determine the 5 $$\times$$ 5 size of the convolutional kernel to extract more accurate and comprehensive coal gangue; Finally, it is combined with SVM using grid optimization to improve the accuracy of coal gangue recognition. The experimental results show that the recognition accuracy of the constructed model reaches 97.5 $$\%$$ , which has obvious advantages compared with traditional classification models and classical classification models, and the recognition speed is faster compared with the mainstream classification models, which provides a new idea for coal gangue recognition.},
  archive      = {J_JRTIP},
  author       = {Ruxin, Gao and Yabo, Du and Tengfei, Wang},
  doi          = {10.1007/s11554-023-01365-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on coal gangue classification recognition method based on the combination of CNN and SVM},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pse: Mixed quantization framework of neural networks for
efficient deployment. <em>JRTIP</em>, <em>20</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01366-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantizing is a promising approach to facilitate deploying deep neural networks on resource-limited devices. However, existing methods are challenged by obtaining computation acceleration and parameter compression while maintaining excellent performance. To achieve this goal, we propose PSE, a mixed quantization framework which combines product quantization (PQ), scalar quantization (SQ), and error correction. Specifically, we first employ PQ to obtain the floating-point codebook and index matrix of the weight matrix. Then, we use SQ to quantize the codebook into integers and reconstruct an integer weight matrix. Finally, we propose an error correction algorithm to update the quantized codebook and minimize the quantization error. We extensively evaluate our proposed method on various backbones, including VGG-16, ResNet-18/50, MobileNetV2, ShuffleNetV2, EfficientNet-B3/B7, and DenseNet-201 on CIFAR-10 and ILSVRC-2012 benchmarks. The experiments demonstrate that PSE reduces computation complexity and model size with acceptable accuracy loss. For example, ResNet-18 achieves 1.8 $$\times$$ acceleration ratio and 30.4 $$\times$$ compression ratio with less than 1.54% accuracy loss on CIFAR-10.},
  archive      = {J_JRTIP},
  author       = {Yang, Yingqing and Tian, Guanzhong and Liu, Mingyuan and Chen, Yihao and Chen, Jun and Liu, Yong and Pan, Yu and Ma, Longhua},
  doi          = {10.1007/s11554-023-01366-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pse: Mixed quantization framework of neural networks for efficient deployment},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time power line segmentation detection based on
multi-attention with strong semantic feature extractor. <em>JRTIP</em>,
<em>20</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01367-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power line is an important part of the transmission line, is the only carrier of power transmission, so the detection of power lines is to ensure the stable operation of the power system is an important means. Therefore, to improve the efficiency of power line detection, this paper proposes a real-time power line segmentation method based on multi-attention mechanism and strong semantic feature extraction. The method is improved based on the DeepLab V3+ codec model. In the encoder part, the Convolutional Block Attention Module (CBAM) is firstly introduced in MobileNetV2 network, which strengthens the ability of contextual information interaction; the ASPPAttention fast feature fusion structure is proposed, which achieves the fast extraction of multi-dimensional effective information by designing the depth-separated convolution of different perceptual fields and strengthens the pixel-level feature encoding ability through the coordinate attention (CA) mechanism; in the decoder part, this paper proposes a real-time power line segmentation method based on multiple attention mechanisms and strong semantic feature extraction. In the decoder part, this paper proposes a real-time power line segmentation method based on multi-attention mechanism and strong semantic feature extraction. In the decoder part, a lightweight inverted convolutional decoder structure is proposed, which improves the feature extraction capability of the model by introducing an inverted bottleneck convolution structure in two quadruple downsampling layers with fewer parameters, and avoids heterogeneous splitting through the introduction of the CA attention mechanism; during the training process, the model convergence is accelerated through the migration of the VOC’s training weights, and the model convergence is avoided through the introduction of the Dice Loss, the effect of the number of samples on the model to accelerate the model convergence speed. The loss avoids the effect of sample number on model generalisation. The experimental results show that the mean intersection over union (mIoU) of this paper can reach 48.5%, the accuracy can reach 97.5%, and the detection speed of the model can reach 40.8 frames per second (fps), which is better than HRNet, PSPNet, DeepLab V3+ and other network models in the balance of speed and accuracy.},
  archive      = {J_JRTIP},
  author       = {Zhao, Qian and Ji, Tangyu and Liang, Shuang and Yu, WenTao and Yan, Chao},
  doi          = {10.1007/s11554-023-01367-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time power line segmentation detection based on multi-attention with strong semantic feature extractor},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-cost system for real-time verification of personal
protective equipment in industrial facilities using edge computing
devices. <em>JRTIP</em>, <em>20</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11554-023-01368-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensure worker safety in the industry is crucial. Despite efforts to improve safety, statistics show a plateau in the reduction of these accidents in recent years. To decrease the number of accidents, compliance with established industrial safety standards and regulations by competent authorities must be ensured, including the use of Personal Protective Equipment (PPE). PPE usage is of paramount importance, as it is essential to prevent accidents from occurring. This work aims to improve worker safety by verifying PPE usage. Technology plays a key role here. A cost-effective solution is proposed to monitor PPE usage in real time. Most existing safety control systems are costly and require considerable maintenance. A low-cost computer vision system is proposed to supervise safety in industrial facilities. This system uses object detection and tracking technology in low-cost embedded devices and can generate alarms in real time if PPE is not used. Unlike other works, temporal information is used to generate the alarms. Safety managers receive this information to take necessary actions. Emphasis has been placed on cost, scalability, and ease of use to facilitate system implementation in industrial plants. The result is an effective system that improves worker safety by verifying established safety measures at a reduced cost. The methodology used improves the Average Precision of PPE detection by 6%. In addition, unlike other studies, the problem of application deployment is addressed, which has an impact on its cost.},
  archive      = {J_JRTIP},
  author       = {Lema, Darío G. and Usamentiaga, Rubén and García, Daniel F.},
  doi          = {10.1007/s11554-023-01368-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-cost system for real-time verification of personal protective equipment in industrial facilities using edge computing devices},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CityUPlaces: A new dataset for efficient vision-based
recognition. <em>JRTIP</em>, <em>20</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01369-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new dataset named CityUPlaces, comprising 17,771 images from various campus buildings, which contains 9 major categories and further derives 18 minor categories based on the internal and external scenes of these identities. Each category is not balanced ranging from 344 to 1539 with diverse variations in angle, attractions, views, illumination, etc. Compared to existing large-scale datasets, the proposed dataset shows its strengths in two aspects: (1) it contains a moderate number of both indoor and outdoor images under different conditions for each identity, which enables diverse real-time recognition tasks by featuring hierarchical categorization with reasonable dataset size; (2) the issue of label noise is significantly alleviated for each identity in the dedicated annotation and filtering stages to facilitate the subsequent tasks. This provides great flexibility to perform these vision-based tasks with different learning objectives in a real-time mode. Moreover, we propose a novel lightweight classification framework that outperforms state-of-the-art baselines on the dataset with the relatively low computational complexity of fewer training parameters and floating-point operations per second, by taking advantage of the involved coarse-to-fined learning strategy in a self-transfer manner. This laterally confirms the applicability of the new dataset. We also conduct experiments on the MIT Indoors and Paris datasets, where the proposed method still achieves superior performance that validates its efficacy. The dataset and code will be publicly available in the future.},
  archive      = {J_JRTIP},
  author       = {Wu, Haowei and Wu, Gengshen and Hu, Jinming and Xu, Shuaixin and Zhang, Songhao and Liu, Yi},
  doi          = {10.1007/s11554-023-01369-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CityUPlaces: A new dataset for efficient vision-based recognition},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A global re-detection method based on siamese network in
long-term visual tracking. <em>JRTIP</em>, <em>20</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01370-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-term visual tracking, object often disappears due to occlusion or out-of-view, and the tracking algorithm needs to re-detect the object when it reappears. Therefore, the re-detection method is very important for long-term visual tracking, and the performance of traditional re-detection methods needs to be further improved. In addition, most of the traditional re-detection methods use the global sliding window or gradually expanding the search region to complete the tracking task. The computational efficiency of these methods is very low, resulting in slow running speeds. Aiming at these problems, inspired by the GlobalTrack, we propose a global re-detection method based on the Siamese network. Firstly, a feature fusion refinement module is proposed to obtain more comprehensive features. Secondly, a multi-scale residual block is proposed to emphasize the important information in the features. Finally, the features are processed by a two-stage region proposal network to obtain high-quality proposal boxes. The global re-detection method proposed in this paper is combined with four existing short-term visual tracking algorithms and constructed as four long-term visual tracking algorithms. The constructed long-term visual tracking algorithms are experimented on the long-term tracking dataset, and the experimental results show that the constructed long-term visual tracking algorithms have good long-term tracking performance and the speed can meet the real-time requirements.},
  archive      = {J_JRTIP},
  author       = {Hou, Zhiqiang and Han, Ruoxue and Ma, Jingyuan and Ma, Sugang and Yu, Wangsheng and Fan, Jiulun},
  doi          = {10.1007/s11554-023-01370-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A global re-detection method based on siamese network in long-term visual tracking},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local feature driven fuzzy local information c-means
clustering with kernel metric for blurred and noisy image segmentation.
<em>JRTIP</em>, <em>20</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s11554-023-01371-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel fuzzy weighted local information C-means clustering is a widely used robust segmentation algorithm for noisy images. However, it cannot effectively solve the segmentation problem of blurred and noisy images. A local feature driven fuzzy local information C-means clustering with kernel metric for blurred and noisy image segmentation is proposed in this paper. Firstly, a local ternary pattern is used to extract the feature information of the blurred and noisy images; Secondly, the image feature information is embedded into the objective function of fuzzy local information clustering and an optimization model for blurred and noisy image segmentation is established. Thirdly, Lagrange multiplier method is used to solve this optimization problem, and a dual-level alternating iterative clustering algorithm for blurred and noisy image segmentation is obtained. Experimental results demonstrate that the proposed algorithm has better segmentation performance for blurred and noisy images than the latest robust fuzzy clustering-related algorithms, and its PSNR and ACC values increase by about 0.09 ~ 1.07 and 0.08 ~ 0.13, respectively.},
  archive      = {J_JRTIP},
  author       = {Wu, Chengmao and Qi, Xiao},
  doi          = {10.1007/s11554-023-01371-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Local feature driven fuzzy local information C-means clustering with kernel metric for blurred and noisy image segmentation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating xilinx FPGA and intelligent techniques for
improved precision in 3D brain tumor segmentation in medical imaging.
<em>JRTIP</em>, <em>20</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01372-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image processing plays an indispensable role in diagnosing and treating brain tumors. The objective of this study was to introduce an innovative hardware architecture designed to enhance the accuracy of segmenting brain tumors in 3D MRI images. The proposed approach combines intelligent algorithms, specifically particle swarm optimization and Darwin particle swarm optimization, to achieve high precision in tumor segmentation. We implemented this method on a global framework for Xilinx Virtex6 FPGA. The performance and robustness of the model were examined using two distinct datasets: BRATS 2021 and BRATS 2013. Experimental findings underscored the model&#39;s outstanding efficacy. The results indicated that our hardware architecture delivered robust and efficient performance in segmenting brain tumors. The introduced hardware architecture offers significant potential in aiding clinicians to diagnose patients and determine lesion extents. By employing advanced algorithms and hardware designs, this method promises to boost the speed and precision of brain tumor diagnosis and treatment. The study thus contributes an invaluable tool for healthcare professionals in the realm of brain tumor detection and management.},
  archive      = {J_JRTIP},
  author       = {Gtifa, Wafa and Sakly, Anis},
  doi          = {10.1007/s11554-023-01372-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Integrating xilinx FPGA and intelligent techniques for improved precision in 3D brain tumor segmentation in medical imaging},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frappe: Fast fiducial detection on low cost hardware.
<em>JRTIP</em>, <em>20</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01373-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Square fiducial markers are widely used in robotics to easily obtain pose and other information about the world from camera images. Processing the images to extract the markers is usually performed centrally with standard libraries but the code is typically aimed at PC-level hardware. Platforms with constrained processing power have difficulty handling multiple camera streams at real-time refresh rates. We introduce the Frappe (Fiducial Recognition Accelerated with Parallel Processing Elements) algorithm for detecting and decoding the popular ArUco tags. Designed to be implemented on the low cost hardware of the Raspberry Pi Zero, we show tag detection and decoding on images of 640 × 480 resolution exceeding 60 Hz, five times faster than the standard ArUco library, while maintaining similar detection performance and using much less energy. Using Frappe, we demonstrate improved real-world performance on a visual navigation task with our DOTS robot.},
  archive      = {J_JRTIP},
  author       = {Jones, Simon and Hauert, Sabine},
  doi          = {10.1007/s11554-023-01373-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Frappe: Fast fiducial detection on low cost hardware},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel algorithm for human action recognition in compressed
domain using attention-guided approach. <em>JRTIP</em>, <em>20</em>(6),
1–12. (<a href="https://doi.org/10.1007/s11554-023-01374-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herein, a novel methodology is proposed for real-time human activity detection and recognition in a compressed domain of videos using motion vectors and attention-guided bidirectional LSTM, and it is termed as MVABLSTM. The videos in MPEG-4 and H.264 compression formats are considered for the present study. Any video source without any prior setup could be considered by adopting the proposed method to various video codecs and camera settings. Existing algorithms for human action recognition in a compressed domain video have some limitations in this regard, such as (i) requirement of keyframes at a fixed interval, (ii) usage of P-frames only, and (iii) normally support single codec only. These limitations are overcome in the proposed method using arbitrary keyframe intervals, using both P- and B-frames, and supporting MPEG-4 as well as H.264 codecs. The experimentation is carried out using the benchmark datasets, namely UCF101, HMDB51, and THUMOS14, and the recognition accuracy in a compressed domain is found to be comparable to that observed in raw video data but with reduced computational time. The proposed MVABLSTM method has outperformed other recent methods in the literature in terms of a lesser (65%) number of parameters and (92%) GFLOPS, while significantly improving accuracy by 0.8%, 5.95%, and 16.65% for UCF101, HMDB51, and THUMOS14, respectively, and speed by 8% in MPEG-4 domain. The performance analysis of the proposed method has been done using MVABLSTM variants in different codecs in comparison with the state-of-the-art network models.},
  archive      = {J_JRTIP},
  author       = {Praveenkumar, S. M. and Patil, Prakashgoud and Hiremath, P. S.},
  doi          = {10.1007/s11554-023-01374-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel algorithm for human action recognition in compressed domain using attention-guided approach},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient memory reuse methodology for CNN-based real-time
image processing in mobile-embedded systems. <em>JRTIP</em>,
<em>20</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01375-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time image processing applications such as intelligent security and traffic management requires pattern recognition tasks, such face recognition, and license plate detection, to execute in mobile-embedded systems. These mobile-embedded applications employ the deep neural network (DNN), especially convolutional neural network (CNN), to complete the image classification. However, deploying CNN models on embedded platforms is challenging as memory-costly CNNs are in conflict with the highly limited memory budget. To address this challenge, a variety of CNN memory reduction methodologies have been proposed. Among these methodologies, CNN memory reuse has no influence on accuracy and throughput of CNN and is easy to realize, which is most suitable for embedded application. However, the existing memory reuse algorithms cannot achieve stable optimal solution. To solve the problem, we first improve an existing memory reuse algorithm. Compared with its original version, the improved algorithm provides 7–25% less memory consumption of intermediate results. We further propose a novel CNN memory reuse algorithm. In the new algorithm, we significantly make use of CNN structure to reuse memory and obtain optimal solution at most cases. Compared with two existing memory reuse algorithms, the new algorithm can reduce the memory footprint by an average of 20.3% and 9.4%.},
  archive      = {J_JRTIP},
  author       = {Zhao, Kairong and Chang, Yinghui and Wu, Weikang and Luo, Hongyin and Li, Zirun and He, Shan and Guo, Donghui},
  doi          = {10.1007/s11554-023-01375-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient memory reuse methodology for CNN-based real-time image processing in mobile-embedded systems},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of optimized versatile video coding
software decoders on embedded platforms. <em>JRTIP</em>, <em>20</em>(6),
1–13. (<a href="https://doi.org/10.1007/s11554-023-01376-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the global demand for high-resolution videos and the emergence of new multimedia applications have created the need for a new video coding standard. Therefore, in July 2020, the versatile video coding (VVC) standard was released, providing up to 50% bit-rate savings for the same video quality compared to its predecessor high-efficiency video coding (HEVC). However, these bit-rate savings come at the cost of high computational complexity, particularly for live applications and on resource-constrained embedded devices. This paper evaluates two optimized VVC software decoders, named OpenVVC and Versatile Video deCoder (VVdeC), designed for low resources platforms. These decoders exploit optimization techniques such as data-level parallelism using single instruction multiple data (SIMD) instructions and functional-level parallelism using frame, tile, and slice-based parallelisms. Furthermore, a comparison of decoding runtime, energy, and memory consumption between the two decoders is presented while targeting two different resource-constraint embedded devices. The results showed that both decoders achieve real-time decoding of full high-definition (FHD) resolution on the first platform using 8 cores and high-definition (HD) real-time decoding for the second platform using only 4 cores with comparable results in terms of the average energy consumed: around 26 J and 15 J for the 8 cores and 4 cores platforms, respectively. Furthermore, OpenVVC showed better results regarding memory usage with a lower average maximum memory consumed during runtime than VVdeC.},
  archive      = {J_JRTIP},
  author       = {Saha, Anup and Hamidouche, Wassim and Chavarrías, Miguel and Pescador, Fernando and Farhat, Ibrahim},
  doi          = {10.1007/s11554-023-01376-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance analysis of optimized versatile video coding software decoders on embedded platforms},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time fall detection model based on BlazePose and
improved ST-GCN. <em>JRTIP</em>, <em>20</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01377-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing timely rescue when a fall occurs can greatly reduce fall mortality for older people. With the growing number of single-resided elders, real-time smart fall incident detection has become a new research hotspot. Accuracy, computational complexity and real-time response are key issues to be solved in this topic. A fall detection model that combines an improved Spatial–Temporal Graph Convolutional Network (ST-GCN) with the BlazePose algorithm is proposed in this paper. The computational speed is improved by removing four redundant layers from the ST-GCN network. Meanwhile, an attention mechanism focussing on the key joints involved in the falling action and their correlation is applied in the model, which introduces an Effective SE Block (ESE Block) to the residual structure of ST-GCN. It is achieved by fusing the original features with channel attention weights obtained by global average pooling and fully connected operations for the joint features. The BlazePose algorithm of the mediapipe framework is used to recognise human targets and locate the spatial coordinates of specific joints. Then the spatiotemporal graph features of the human body are extracted by the improved ST-GCN from the temporal and spatial displacements of 30 consecutive frames. Furthermore, fall behaviour is judged by the action type defined by the spatiotemporal graph. The accuracy of the proposed model for public datasets, such as Le2i Fall, Multicam Fall and UR Fall, is 99.29%, 99.22% and 98.64% respectively, which are higher than the Alphapose + ST-GCN model by 9.04%, 20% and 25.2%. Such accuracy is even better than the existing best algorithms by 0.89%, 0.92% and 1.04%. When running on the i5-10200H CPU and the Jetson Nano edge computing device, the Alphapose + ST-GCN model achieves frame rates of 11.42fps and 1.5fps, whilst the frame rates of this paper are up to 24.5fps and 9.37fps. The experimental results fully show that based on BlazePose with the improved ST-GCN makes the fall detection model higher accuracy, faster speed, real-time performance and high compatibility with the Jetson Nano edge computing device.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yu and Gan, Junsi and Zhao, Zewei and Chen, Junliang and Chen, Xiaofeng and Diao, Yinliang and Tu, Shuqin},
  doi          = {10.1007/s11554-023-01377-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time fall detection model based on BlazePose and improved ST-GCN},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Lite-YOLOv3: A real-time object detector based on
multi-scale slice depthwise convolution and lightweight attention
mechanism. <em>JRTIP</em>, <em>20</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01379-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detector performance gains often occur with deeper networks and heavier computational overhead, yet scenarios with constrained calculation and storage demand real-time performance while consuming fewer resources. Existing methods tend to be caught in a tough decision between parameters, computation, speed and accuracy. We propose a lightweight real-time object detector Lite-YOLOv3 from the optimization of YOLOv3. Firstly, sparse pruning of the trained model significantly decreases the parameters and calculations while boosting the speed. Secondly, a channel-wise convolution attention (CWA) mechanism is proposed to enhance the feature extraction capability of the backbone with essentially no extra computational burden. Furthermore, a multi-scale slice depthwise convolution with efficient channel attention (MSD-ECA) is proposed to enhance the receptive field and cross-scale information representation. Finally, SIoU is chosen as the localization loss function to improve the training speed and regression accuracy. For 512 $$\times$$ 512 input, Lite-YOLOv3 achieves 74.1 $$\%$$ mAP at 113 FPS on the PASCAL VOC07+12 dataset and 52.4 $$\%$$ mAP on the MS COCO2017 dataset. The experimental results show that compared with YOLOv3, Lite-YOLOv3 is slightly inferior in accuracy, the parameters and calculations are only 24.8 $$\%$$ and 30.7 $$\%$$ , respectively, the inference speed is 1.7 times faster, which sufficiently proves the effectiveness of the proposed method and is also comparable with other models.},
  archive      = {J_JRTIP},
  author       = {Zhou, Yipeng and Qian, Huaming and Ding, Peng},
  doi          = {10.1007/s11554-023-01379-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lite-YOLOv3: A real-time object detector based on multi-scale slice depthwise convolution and lightweight attention mechanism},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-performance fractional anisotropic diffusion filter for
portable applications. <em>JRTIP</em>, <em>20</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01339-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anisotropic diffusion is one of the most effective methods used in image processing. It can be used to eliminate the small textures of an image while preserving its significant edges. In this paper, a new anisotropic diffusion filter is proposed based on a fractional calculus kernel rather than integer kernel to improve the overall performance of the filter. Integer and fractional anisotropic filters are implemented using the Genesys-2 FPGA kit to utilize the efficiency of parallelism in FPGAs. Integer and fractional anisotropic filters are tested against the achievable PSNR value vs the number of iterations. The proposed fractional anisotropic filter has a better PSNR value using a smaller number of iterations, reducing the power and area compared to integer anisotropic filter. The proposed filter can be used in image smoothing, edge detection, image segmentation, image denoising, and cartooning. In addition, the proposed filter reduces the power consumption by 58.2% compared to integer-order filters, which makes the proposed filter suitable for battery-based applications.},
  archive      = {J_JRTIP},
  author       = {AbdAlRahman, Alaa and Al-Atabany, Walid I. and Soltan, Ahmed and Radwan, Ahmed G.},
  doi          = {10.1007/s11554-023-01339-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High-performance fractional anisotropic diffusion filter for portable applications},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complementary spatial transformer network for real-time 3D
object recognition. <em>JRTIP</em>, <em>20</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01340-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny Deep Learning Models offer many advantages in various applications. From the perspective of statistical machine learning theory the contributions of this paper is to complement the research advances and results obtained so far in real-time 3D object recognition. We propose a Tiny Deep Learning Model named Complementary Spatial Transformer Network (CSTN) for Real-Time 3D object recognition. It turns out that CSTN’s working, and analysis are much simplified in a target space setting. We make algorithmic enhancements to perform CSTN computations faster and keep the learning part of CSTN in minimal size. Finally, we provide the experimental verifications of the results obtained in publicly available point cloud data sets ModelNet40 and ShapeNetCore with our model performing 1.65–2 times better in DPS (Detections/s) rate on GPU hardware for 3D object recognition, when compared to state-of-the-art networks. Complementary Spatial Transformer Network architecture requires only 10–35% of trainable parameters, when compared to state-of-the-art networks, making the network easier to deploy in edge AI devices.},
  archive      = {J_JRTIP},
  author       = {Krishna Kumar, K. P. and Paul, Varghese},
  doi          = {10.1007/s11554-023-01340-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Complementary spatial transformer network for real-time 3D object recognition},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based object tracking in walking droplet and
granular intruder experiments. <em>JRTIP</em>, <em>20</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01341-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep-learning based tracking objects of interest in walking droplet and granular intruder experiments. In a typical walking droplet experiment, a liquid droplet, known as walker, propels itself laterally on the free surface of a vibrating bath of the same liquid. This lateral motion stems from the interaction between the droplet and the wave it generates upon successive bounces off the vibrating liquid surface. A walker can exhibit a highly irregular trajectory over the course of its motion, including rapid acceleration and complex interactions with the other walkers present in the bath. In analogy with the hydrodynamic experiments, the granular matter experiments consist of a vibrating bath of very small solid particles and a larger solid called intruder. Like the fluid droplets, the intruder interacts with and travels the domain due to the waves of the bath but tends to move much slower and much less smoothly than the droplets. When multiple intruders are introduced, they also exhibit complex interactions with each other. We leverage the state-of-the-art object detection model YOLO (You Only Look Once) and the Hungarian Algorithm to accurately extract the trajectory of a walker or intruder in real-time. Our proposed methodology is capable of tracking individual walker(s) or intruder(s) in digital images acquired from a broad spectrum of experimental settings and does not suffer from any identity-switch issues. Thus, the deep learning approach developed in this work could be used to automatize the efficient, fast and accurate extraction of observables of interests in walking droplet, granular intruder experiments and similar particle tracking experiments. Such extraction capabilities are critically enabling for downstream tasks such as building data-driven dynamical models for the coarse-grained dynamics and interactions of the objects of interest.},
  archive      = {J_JRTIP},
  author       = {Kara, Erdi and Zhang, George and Williams, Joseph J. and Ferrandez-Quinto, Gonzalo and Rhoden, Leviticus J. and Kim, Maximilian and Kutz, J. Nathan and Rahman, Aminur},
  doi          = {10.1007/s11554-023-01341-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning based object tracking in walking droplet and granular intruder experiments},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: A new approach to snow avalanche rescue
using UAV pictures based on convolutional neural networks.
<em>JRTIP</em>, <em>20</em>(5), 1–2. (<a
href="https://doi.org/10.1007/s11554-023-01342-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Zhang, Suyu and Gavrilovskaya, Nadezhda and Al Said, Nidal and Afandi, Waleed Saeed},
  doi          = {10.1007/s11554-023-01342-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-2},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: A new approach to snow avalanche rescue using UAV pictures based on convolutional neural networks},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An RGB-d SLAM algorithm based on adaptive semantic
segmentation in dynamic environment. <em>JRTIP</em>, <em>20</em>(5),
1–14. (<a href="https://doi.org/10.1007/s11554-023-01343-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the existing visual SLAM (simultaneous localization and mapping) algorithms are applied to dynamic environments, the pose error estimated by the system often increases sharply, or even the algorithm fails due to the interference of dynamic objects. To adapt to dynamic scenes, a dynamic object processing part needs to be added to the system. However, some existing processing methods lead to reduced real-time performance, which is not conducive to the real-time localization and navigation of mobile robots. To solve the above problems, an RGB-D SLAM system is proposed in this paper for indoor dynamic environments. The system designs an adaptive semantic segmentation tracking algorithm to meet the requirements of localization accuracy and real-time performance in dynamic scenes. First, a lightweight semantic segmentation network is used to provide a priori information about the object. According to this prior information and the motion state of the object in the previous scene, each feature point is assigned a motion level and is classified as a static point, movable static point, or dynamic point. Then, whether the current frame needs semantic segmentation is adaptively determined according to the motion level information of the feature points. Some appropriate feature points (static points) are selected for initial pose estimation, and then, secondary optimization of the pose is performed according to the results of weighted static constraints. In order to verify the effectiveness of the proposed algorithm, experiments are carried out on the TUM RGB-D dynamic scene dataset and compared with ORB-SLAM2 and other SLAM algorithms for dynamic environments. The results show that the proposed algorithm performs well on most datasets, and the positioning accuracy in indoor dynamic environments can be improved by 90.57% compared with the ORB-SLAM2 algorithm. In addition, a 3D semantic map of static backgrounds in dynamic scenes has been established, using dense point cloud maps to visualize 3D scene information, and incorporating semantic information to label objects in the scene, to guide advanced tasks such as robot navigation and enhance the usability of the system.},
  archive      = {J_JRTIP},
  author       = {Wei, Song and Li, Zhang},
  doi          = {10.1007/s11554-023-01343-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An RGB-D SLAM algorithm based on adaptive semantic segmentation in dynamic environment},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster RCNN based robust vehicle detection algorithm for
identifying and classifying vehicles. <em>JRTIP</em>, <em>20</em>(5),
1–10. (<a href="https://doi.org/10.1007/s11554-023-01344-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have shown tremendous success in the detection of objects and vehicles in recent years. However, when using CNNs to identify real-time vehicle detection in a moving context remains difficult. Many obscured and truncated cars, as well as huge vehicle scale fluctuations in traffic photos, provide these issues. To improve the performance of detection findings, we used multiscale feature maps from CNN or input pictures with numerous resolutions to adapt the base network to match different scales. This research presents an enhanced framework depending on Faster R-CNN for rapid vehicle recognition which presents better accuracy and fast processing time. Research results on our custom dataset indicate that our recommended methodology performed better in terms of detection efficiency and processing time, especially in comparison to the earlier age of Faster R-CNN models.},
  archive      = {J_JRTIP},
  author       = {Alam, Md Khorshed and Ahmed, Asif and Salih, Rania and Al Asmari, Abdullah Faiz Saeed and Khan, Mohammad Arsalan and Mustafa, Noman and Mursaleen, Mohammad and Islam, Saiful},
  doi          = {10.1007/s11554-023-01344-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Faster RCNN based robust vehicle detection algorithm for identifying and classifying vehicles},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Journal of real-time image processing: Fifth issue of volume
20. <em>JRTIP</em>, <em>20</em>(5), 1. (<a
href="https://doi.org/10.1007/s11554-023-01345-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-023-01345-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Fifth issue of volume 20},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High efficiency lossless image recompression algorithm with
asymmetric numeral systems for real-time mobile application.
<em>JRTIP</em>, <em>20</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01346-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JPEG images are widely used by individual users, data centers, cloud storages, and network file systems. The huge transmission and storage bandwidth of existing JPEG images is becoming a big challenge due to its low compression efficiency. Some methods such as Google’s Brunsli can further compress these JPEG images adequately and restore them back to JPEG format losslessly when needed, but its decoding speed is not fast enough especially for real-time mobile applications. To further reduce its decoding complexity, we proposed three optimizations based on Brunsli, which include using asymmetric numeral systems coding to replace the existing arithmetic coding, designing the joint encoding of multiple symbols and cache-friendly optimizing the data structures. Experimental results demonstrate that compared with Brunsli, the proposed method achieves average 1.86–2.25 times decoding faster on mobile platform with comparable compression efficiency which is really valuable for power sensitive real-time mobile applications.},
  archive      = {J_JRTIP},
  author       = {Sheng, Qinghua and Zhu, Haonan and Sheng, Haixiang and Huang, Xiaofang and Jiang, Jie and Lai, Changcai},
  doi          = {10.1007/s11554-023-01346-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High efficiency lossless image recompression algorithm with asymmetric numeral systems for real-time mobile application},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time detection and location of reserved anchor hole in
coal mine roadway support steel belt. <em>JRTIP</em>, <em>20</em>(5),
1–14. (<a href="https://doi.org/10.1007/s11554-023-01347-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the current coal mine roadway using supporting steel belt steel belt auxiliary anchor support, the location of the supporting steel belt anchor holes is mainly done manually; if the location is not accurate, there will be large safety hazards and other problems. An intelligent real-time detection and location method of anchor holes in coal mine roadway support steel belt based on deep learning model and depth camera is proposed. First, to reduce the influence of water mist and dust on the camera and improve the image quality of the camera, the image is pre-processed using contrast limited adaptive histogram equalization. Second, the YOLOv5s model is improved by adding SPD-Conv and coordinate attention mechanisms to improve the detection capability of the model. Third, a real-time depth map restoration method that fully preserves object edge features is proposed to avoid errors caused by areas with depth values of 0 in the depth map when locating anchor holes in combination with a depth camera. Finally, the improved YOLOv5s model proposed in this paper combined with the repaired depth map was used to achieve the detection and location of anchor holes in a laboratory simulated tunnel with a location error of less than 5 mm and an average FPS of 28.9. In summary, the real-time target detection and location method based on a deep learning model and a depth camera is feasible in an unstructured environment in coal mines.},
  archive      = {J_JRTIP},
  author       = {Wang, Hongwei and Zhang, Fujing and Wang, Haoran and Li, Zhenglong and Wang, Yuheng},
  doi          = {10.1007/s11554-023-01347-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection and location of reserved anchor hole in coal mine roadway support steel belt},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fisher pruning for developing real-time UAV trackers.
<em>JRTIP</em>, <em>20</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01348-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-based tracking has shown large potential in various domains such as transportation, logistics, public safety, and more. However, deploying deep learning (DL)-based tracking algorithms on UAVs is challenging because of limitations in computing resources, battery capacity, and maximum load. Discriminative correlation filter (DCF)-based trackers have become a popular choice in the UAV tracking community owing to their ability to provide superior efficiency while consuming fewer resources. However, the limited representation learning ability of DCF-based trackers leads to lower precision in complex scenarios compared to DL-based methods. Filter pruning is a prevalent practice for deploying deep neural networks on edge devices with constrained resources, and it may be an effective way to solve problems encountered when deploying deep learning trackers on UAVs. However, the application of filter pruning to UAV tracking is underexplored, and a straightforward and useful pruning standard is desirable. This paper proposes using Fisher pruning to reduce the SiamFC++ model for UAV tracking, resulting in the F-SiamFC++ tracker. The proposed tracker achieves a remarkable balance between precision and efficiency, as demonstrated through exhaustive experiments on four popular UAV benchmarks: UAVDT, DTB70, UAV123@10fps, and Vistrone2018, showing state-of-the-art performance.},
  archive      = {J_JRTIP},
  author       = {Zhong, Pengzhi and Wu, Wanying and Dai, Xiaowei and Zhao, Qijun and Li, Shuiwang},
  doi          = {10.1007/s11554-023-01348-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fisher pruning for developing real-time UAV trackers},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time recognition gait framework for personal
authentication via image-based neural network: Accelerated by feature
reduction in time and frequency domains. <em>JRTIP</em>, <em>20</em>(5),
1–17. (<a href="https://doi.org/10.1007/s11554-023-01349-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, personal authentication based on attitude estimation—gait recognition authentication has become a popular research topic because of its long-range, non-invasive, non-contact, high-precision, and other advantages. However, at present, most relevant research prefers to use the acquired original data directly for iteration and learning. As a result, it takes too long to learn relevant models in the use scenarios with complicated data and heavy human traffic, such as airports and railway stations, where real-time identification cannot be completed while maintaining accuracy, and thus a scheme to improve the learning and recognition speed is needed. Therefore, in this paper, we proposed an innovative real-time MediaPipe-based gait analysis framework and a new Composite Filter Feature Selection (CFFS) method via key nodes, angles, and lengths calculating. Then, based on the proposed method, we extract the aimed features as a new dataset and verified it by 1D-CNN neural network. Furthermore, we also applied Hilbert–Huang transform to investigate these extracted gait features in the frequency domain, improving the performance of our proposed framework to achieve real time under higher recognition accuracy. The experimental results show that the innovative gait recognition framework and data processing technology can reduce the gait feature data, speed up the process of gait recognition, and still maintain the original recognition accuracy. It can also be applied to various large, enclosed spaces with the huge human flow, which has played a role in improving the safety factor, saving labor costs, and accelerating economic consumption.},
  archive      = {J_JRTIP},
  author       = {Huang, Xuan and Dong, Ran and Wu, Bo and Sato, Kiminori and Ikuno, Soichiro and Wang, Zijun and Nishimura, Shoji},
  doi          = {10.1007/s11554-023-01349-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time recognition gait framework for personal authentication via image-based neural network: Accelerated by feature reduction in time and frequency domains},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time institution video data analysis using fog
computing and adaptive background subtraction. <em>JRTIP</em>,
<em>20</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01350-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for video surveillance systems has led to a surge in research towards developing smart video surveillance systems that can combat the growing levels of insecurity. However, the massive amount of video data generated by these systems has overwhelmed the storage and processing capabilities of analytic applications. This paper proposes a fog computing-based smart video surveillance system that provides less latency, network bandwidth, and response time by localizing data to the edges of the network. The proposed system incorporates two key preprocessing steps, namely adaptive key frame extraction and adaptive contour-based background subtraction, to increase the quality of detecting abnormal motions from surveillance video streams. The adaptive key frame extraction mechanism extracts key frames from the available frames in the video sequence using a sliding window technique. The high-level semantic information of video frames is learned using the visual geometry group-16 Transfer Learning (VGG-16 TL) technique to better represent the video content. The adaptive contour-based background subtraction mechanism separates target foreground pixels from the background scenes, paving the way for easy detection of abnormal motions in the video frames. The proposed system leverages fog computing to process and store data, reducing latency and improving overall performance. The fog layer performs adaptive background subtraction, contour detection, and object analysis, ensuring timely processing of video frames and minimizing the need for transmitting large amounts of data to the cloud. The proposed system is evaluated using a real-time video dataset in terms of accuracy, compression ratio, precision, recall, processing time, latency, and network bandwidth, demonstrating the efficacy of the proposed system for abnormal motion detection and data transmission. Overall, the proposed fog computing-based smart video surveillance system provides an effective solution for detecting abnormal motions in real-time institution video data with reduced latency, network bandwidth, and response time, demonstrating the potential of fog computing for video surveillance applications.},
  archive      = {J_JRTIP},
  author       = {Amshavalli, R. S. and Kalaivani, J.},
  doi          = {10.1007/s11554-023-01350-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time institution video data analysis using fog computing and adaptive background subtraction},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Part-based tracking for object pose estimation.
<em>JRTIP</em>, <em>20</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11554-023-01351-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation is crucial in human–computer interaction systems. The traditional point-based detection approaches rely on the robustness of feature points, the tracking methods utilize the similarity between frames to improve the speed, while the recent studies based on neural networks concentrate on solving specific invariance problems. Different from these methods, PTPE (Part-based Tracking for Pose Estimation) proposed in this paper focuses on how to balance the speed and accuracy under different conditions. In this method, the point matching is transformed into the part matching inside an object to enhance the reliability of the features. Additionally, a fast interframe tracking method is combined with learning models and structural information to enhance robustness. During tracking, multiple strategies are adopted for the different parts according to the matching effects evaluated by the learning models, so as to develop the locality and avoid the time consumption caused by undifferentiated full frame detection or learning. In addition, the constraints between parts are applied for parts detection optimization. Experiments show that PTPE is efficient both in accuracy and speed, especially in complex environments, when compared with classical algorithms that focus only on detection, interframe tracking, self-supervised models, and graph matching.},
  archive      = {J_JRTIP},
  author       = {Ye, Shuang and Ye, Jianhong and Lei, Qing},
  doi          = {10.1007/s11554-023-01351-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Part-based tracking for object pose estimation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable and custom-precision floating-point hardware
convolution core for using in AI edge processors. <em>JRTIP</em>,
<em>20</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01352-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI algorithms such as CNNs devices have necessitated the design of lightweight, low-power, and fast hardware in edge processors. In this paper, a floating-point convolution core is proposed for edge processors to implement CNN. At first, the conventional CNN networks were analyzed in terms of the abundance of filter size. Considering the performance and execution time, the focus of the research is on size of 3 × 3. Next, using the proposed 10-input adder instead of nine 2-input adders and modifying the multipliers, an optimum 32-bit 3 × 3 convolution core has been designed. After studying different bit widths in mantissa and the exponent of floating-point numbers in CNNs, 13-bit is considered as the minimum bit width without accuracy losing. The 3 × 3 core with the new bit width is implemented. Since filter sizes 1 × 1 and 5 × 5 are also available in conventional networks, the new scalable architecture is designed to support all three sizes. Finally, the YOLOv4-tiny object detection and GoogLeNet are used as two benchmarks to evaluate the final 3 × 3 scalable core. The results have shown that despite using floating-point calculations, the FPS is equal to 45.9, which is equal to the previous works that were done in fixed-point, while the accuracy of the proposed work is 84% which is similar to the 32-bit floating point.},
  archive      = {J_JRTIP},
  author       = {Shafiei, Mahdi and Daryanavard, Hassan and Hatam, Ahmad},
  doi          = {10.1007/s11554-023-01352-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Scalable and custom-precision floating-point hardware convolution core for using in AI edge processors},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated and real-time social distancing, mask
detection, and facial temperature video measurement system for pandemic
monitoring. <em>JRTIP</em>, <em>20</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01353-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new Edge-AI algorithm for real-time and multi-feature (social distancing, mask detection, and facial temperature) measurement to minimize the spread of COVID-19 among individuals. COVID-19 has extenuated the need for an intelligent surveillance video system that can monitor the status of social distancing, mask detection, and measure the temperature of faces simultaneously using deep learning (DL) models. In this research, we utilized the fusion of three different YOLOv4-tiny object detectors for each task of the integrated system. This DL model is used for object detection and targeted for real-time applications. The proposed models have been trained for different data sets, which include people detection, mask detection, and facial detection for measuring the temperature, and evaluated on these existing data sets. Thermal and visible cameras have been used for the proposed approach. The thermal camera is used for social distancing and facial temperature measurement, while a visible camera is used for mask detection. The proposed method has been executed on NVIDIA platforms to assess algorithmic performance. For evaluation of the trained models, accuracy, recall, and precision have been measured. We obtained promising results for real-time detection for human recognition. Different couples of thermal and visible cameras and different NVIDIA edge platforms have been adopted to explore solutions with different trade-offs between cost and performance. The multi-feature algorithm is designed to monitor the individuals continuously in the targeted environments, thus reducing the impact of COVID-19 spread.},
  archive      = {J_JRTIP},
  author       = {Elhanashi, Abdussalam and Saponara, Sergio and Dini, Pierpaolo and Zheng, Qinghe and Morita, Daiki and Raytchev, Bisser},
  doi          = {10.1007/s11554-023-01353-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An integrated and real-time social distancing, mask detection, and facial temperature video measurement system for pandemic monitoring},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ATT-YOLOv5-ghost: Water surface object detection in complex
scenes. <em>JRTIP</em>, <em>20</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01354-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Unmanned Surface Vehicles (USVs) have been widely used in water surface monitoring and management. The main problems of the USVs-based water surface object detection method are that the features will be lost when downsampling complex water surface environment images, resulting in low detection accuracy. Moreover, the number of parameters and calculation amount of these models are too much, which will seriously affect the speed of training and detection. Therefore, this paper proposed the ATT-YOLOv5-Ghost algorithm. First, we added an Efficient Channel Attention (ECA) module to each CSP1 unit of the backbone CSPDarknet, which solves the problem of accuracy drop caused by multi-scale feature loss during downsampling. Second, we proposed a method combining ECA and Ghost modules. In the process of feature fusion, the problems such as the increase of parameters, slow detection speed and repeated gradient calculation caused by too complex algorithm were solved. The ATT-YOLOv5-Ghost algorithm improves the detection accuracy by 4.6% compared with the baseline. The FPS can reach 64.9, and the computational amount is reduced by 8.9%. The algorithm complexity was significantly reduced.},
  archive      = {J_JRTIP},
  author       = {Deng, Liwei and Liu, Zhen and Wang, Jiandong and Yang, Baisong},
  doi          = {10.1007/s11554-023-01354-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ATT-YOLOv5-ghost: Water surface object detection in complex scenes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and intelligent measurement of concrete aggregate
volume based on monocular vision mapping. <em>JRTIP</em>,
<em>20</em>(5), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01355-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to prevent the abnormal appearance of gravel aggregate material level in the mixing plant, improve the safety of the concrete mixing plant system as well as the efficient and high-quality production of the concrete mixing plant, this paper proposes a monocular vision-based fast and intelligent measurement method for the volume of concrete aggregate. This method combines peak and valley positioning information derived from target detection model to find the height of aggregate level and diameter of the surface circle formed by the valley. The relationship between this localization data and the height, angle and volume data are then analyzed separately, and finally, the volume of the aggregate is calculated. The experimental results show that the average accuracy in the peak state of the fast and intelligent measurement method of concrete aggregate volume based on monocular vision is 93.06%, the average accuracy of the volume in the valley state is 92.20%, and the real-time monitoring speed can reach between 111 and 115f/s. The effects can meet the real-time measurement of aggregate volume in the high-level storage bin.},
  archive      = {J_JRTIP},
  author       = {Liu, Yingjie and Yue, Shuang and Li, Bin and Wang, Guanghui and Liu, Mingtang and Zhang, Jinhao and Shangguan, Linjian},
  doi          = {10.1007/s11554-023-01355-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast and intelligent measurement of concrete aggregate volume based on monocular vision mapping},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and implementation of hardware-efficient architecture
for saturation-based image dehazing algorithm. <em>JRTIP</em>,
<em>20</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01356-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For real-time single-image dehazing, this paper suggests a straightforward and efficient saturation-based transmission map estimation method. For the suggested image dehazing algorithm, the design of a hardware-efficient very large scale integration (VLSI) architecture is also provided. By removing the computationally demanding sorting operations, the algorithm computes the dark channel, increases the robustness of atmospheric light estimation using a hardware-friendly local atmospheric light estimation module based on the pixel saturation values, and reduces the effects of halo artifacts using an edge-preserving filter to estimate the saturation-based transmission map. Compared to previous sophisticated dehazing approaches, this study exhibits competitive performance in the quality of the dehazed images. The best of the existing dehazing architecture as well as the proposed architecture are described in Verilog hardware description language (HDL), functionally verified using Vivado 2019.1 simulator, and synthesized using Cadence genus compiler. The results of the implementation show that the suggested design is hardware-efficient and offers higher throughput. The suggested dehazing architecture achieves better results in terms of area and delay than the most recent methods and is appropriate for applications with hardware restrictions.},
  archive      = {J_JRTIP},
  author       = {George, Anuja and Jayakumar, E. P.},
  doi          = {10.1007/s11554-023-01356-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Design and implementation of hardware-efficient architecture for saturation-based image dehazing algorithm},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A face recognition application for alzheimer’s patients
using ESP32-CAM and raspberry pi. <em>JRTIP</em>, <em>20</em>(5), 1–16.
(<a href="https://doi.org/10.1007/s11554-023-01357-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a real-time face recognition application to aid people living with Alzheimer’s in identifying the people around them. This is achieved by developing a portable system consisting of glasses with an ESP32-CAM and a single-board microcomputer (the Raspberry Pi). The proposed system operates automatically and does not require physical interaction with the user. It utilizes wireless technologies to capture real-time video frames of human faces and transmit them (via Wi-Fi) to the Raspberry Pi, which detects and recognizes the captured human face and sends voice-activated feedback to the user’s ears over Bluetooth to pronounce their name. Several incompatibility challenges are encountered and appropriately handled during the system’s development, integration, and testing processes. A fully functional prototype is developed and tested successfully. When compared to the state-of-the-art, the obtained results have demonstrated superior performance in terms of a training accuracy of 99.46% and a face recognition accuracy of 99.48%. The entire processing time from capturing the human face to generating the voice message is found to be about one second (730 ms on a laptop and 1109 ms on a Raspberry Pi). The developed technology is anticipated to improve the patient’s quality of life and reduce their dependence on others.},
  archive      = {J_JRTIP},
  author       = {Kadhim, Thair A. and Hariri, Walid and Smaoui Zghal, Nadia and Ben Aissa, Dalenda},
  doi          = {10.1007/s11554-023-01357-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A face recognition application for alzheimer’s patients using ESP32-CAM and raspberry pi},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). MSSD: Multi-scale object detector based on spatial pyramid
depthwise convolution and efficient channel attention mechanism.
<em>JRTIP</em>, <em>20</em>(5), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01358-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has made widespread development and remarkable progress in various fields, but, in complex application scenarios, often encounters the situation that the target features are inconspicuous and the scale range is large, making it incapable of achieving the desirable results, especially for small targets. This paper proposes a multi-scale object detector MSSD based on spatial pyramid depthwise convolution (SPDC) and efficient channel attention mechanism (ECAM) from the optimization of SSD. Firstly, use ResNet50 to replace VGG as backbone to obtain more representative features. Secondly, a plug-and-play spatial pyramid depthwise convolution module SPDC is proposed to enhance perceptual field and multi-scale feature extraction capabilities. Furthermore, we design a straightforward efficient channel attention mechanism (ECAM) to scale the weights of features on channels to derive more robust features. Finally, the feature pyramid network (FPN) with ECAM (ECAM-FPN) module is introduced in the prediction feature layer for deep feature fusion to obtain multi-scale features rich in semantic and detail information. For 300 $$\times$$ 300 input, MSSD achieves 82.5 $$\%$$ mAP on PASCAL VOC07+12 dataset at 56 FPS and 48.2 $$\%$$ mAP on MS COCO2017 dataset, which are 8.2 $$\%$$ and 7.0 $$\%$$ higher than SSD(300), respectively. Detection of small targets is improved by 0.8 $$\%$$ on COCO and by 6.5 $$\%$$ when scaled to 512 $$\times$$ 512. The proposed method has significant gains in cross-scale target detection while satisfying real time and is comparable with other methods.},
  archive      = {J_JRTIP},
  author       = {Zhou, Yipeng and Qian, Huaming and Ding, Peng},
  doi          = {10.1007/s11554-023-01358-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MSSD: Multi-scale object detector based on spatial pyramid depthwise convolution and efficient channel attention mechanism},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Yolo-inspection: Defect detection method for power
transmission lines based on enhanced YOLOv5s. <em>JRTIP</em>,
<em>20</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01360-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of defective components in transmission lines and timely feedback to inspectors for timely maintenance can ensure the stable operation of the power system. A defect detection system based on “edge-cloud-end” collaboration is introduced to solve the problems of high bandwidth consumption and response delay in the cloud server-based approach. The system transfers the operation of image detection to the edge device, which reduces the data transmission and improves the response speed of the system. To balance the detection speed and accuracy of the algorithm, the YOLO-inspection algorithm applied on edge devices is proposed. The algorithm uses GhostNetV2 to reconstruct the C3 module in the YOLOv5 model, which reduces the computational complexity and captures the correlation between distant pixels so that it is more targeted to the critical region of the defective target. Meanwhile, based on the feature fusion network, a dynamic adaptive weight assignment module and cross-scale connectivity are designed to effectively reduce information loss and help the network learn fine-grained features. The improved algorithm is deployed on the NVIDIA Jetson Xavier NX platform, and the model is optimally accelerated using TensorRT. Experimental results show that the method proposed in this paper can accurately identify defective samples, and the YOLO-inspection algorithm has superior generalization ability under the harsh conditions of low light and snowfall weather conditions. On the edge computing platform, the mean average precision (mAP) can reach 94.3 $$\%$$ , and the inference speed can reach 63 frames per second (FPS). It can be proved that the method has good detection performance.},
  archive      = {J_JRTIP},
  author       = {Lu, Lihui and Chen, Zhencong and Wang, Rifan and Liu, Li and Chi, Haoqing},
  doi          = {10.1007/s11554-023-01360-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Yolo-inspection: Defect detection method for power transmission lines based on enhanced YOLOv5s},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speeding-up and compression convolutional neural networks by
low-rank decomposition without fine-tuning. <em>JRTIP</em>,
<em>20</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01274-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of convolutional neural network (CNN), the accuracy of CNN has been significantly improved, which also brings great challenges to the deployment of mobile terminals or embedded devices with limited resources. Recently, significant achievements have been made in compressing CNN through low-rank decomposition. Unlike existing methods that use the same decomposition form and decomposition strategy with fine-tuning based on singular value decomposition (SVD), our method uses different decomposition forms for different layers, and proposes decomposition strategies without fine-tuning. We present a simple and effective scheme to compress the entire CNN, which is called cosine similarity SVD without fine-tuning. For the AlexNet, our cosine similarity algorithm of rank selection takes 84% of the time to find the rank compared with the bayesian optimization (BayesOpt) algorithm. After we tested various CNNs (AlexNet, VGG-16, VGG-19, and ResNet-50) on different data sets, experimental results show that the weight parameter drop can exceed 50% when the accuracy loss is less than 1% without fine-tuning. The floating point operations (FLOPs) drop is about 20%, and the accuracy loss is less than 1% without fine-tuning.},
  archive      = {J_JRTIP},
  author       = {Zhang, Meng and Liu, Fei and Weng, Dongpeng},
  doi          = {10.1007/s11554-023-01274-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Speeding-up and compression convolutional neural networks by low-rank decomposition without fine-tuning},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced data fusion of ultrasonic and stereo vision in
real-time obstacle detection. <em>JRTIP</em>, <em>20</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01314-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, the accuracy and speed of obstacle detection in data fusion of ultrasonic and stereo vision have been improved. The smoothness assumption has been used in such a way that the responses are significantly improved without increasing calculation. In addition, with the development of the proposed method to run on the graphics card, the cross-checking process has been done without the need to change the reference image and without more calculation of the cost function. The results of this study show that the proposed method improved the quality of the responses compared to the previous study, and the obstacle detection rate in intelligent vehicles has increased to 41 pairs of frames per second. This processing rate is 477.40 times faster than the usual local stereo method and 33.77% faster than the previous study on the data fusion of ultrasonic and stereo vision.},
  archive      = {J_JRTIP},
  author       = {Gholami, Farshad and Khanmirza, Esmaeel and Riahi, Mohammad},
  doi          = {10.1007/s11554-023-01314-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced data fusion of ultrasonic and stereo vision in real-time obstacle detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based lightweight radar target detection
method. <em>JRTIP</em>, <em>20</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01316-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For target detection tasks in complicated backgrounds, a deep learning-based radar target detection method is suggested to address the problems of a high false alarm rate and the difficulties of achieving high-performance detection by conventional methods. Considering the issues of large parameter count and memory occupation of the deep learning-based target detection models, a lightweight target detection method based on improved YOLOv4-tiny is proposed. The technique applies depthwise separable convolution (DSC) and bottleneck architecture (BA) to the YOLOv4-tiny network. Moreover, it introduces the convolutional block attention module (CBAM) in the improved feature fusion network. It allows the network to be lightweight while ensuring detection accuracy. We choose a certain number of pulses from the pulse-compressed radar data for clutter suppression and Doppler processing to obtain range–Doppler (R–D) images. Experiments are run on the R–D two-dimensional echo images, and the results demonstrate that the proposed method can quickly and accurately detect dim radar targets against complicated backgrounds. Compared with other algorithms, our approach is more balanced regarding detection accuracy, model size, and detection speed.},
  archive      = {J_JRTIP},
  author       = {Liang, Siyuan and Chen, Rongrong and Duan, Guodong and Du, Jianbo},
  doi          = {10.1007/s11554-023-01316-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning-based lightweight radar target detection method},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A new approach to snow avalanche rescue using UAV pictures
based on convolutional neural networks. <em>JRTIP</em>, <em>20</em>(4),
1–17. (<a href="https://doi.org/10.1007/s11554-023-01317-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Zhang, Suyu and Gavrilovskaya, Nadezhda and Al Said, Nidal and Afandi, Waleed Saeed},
  doi          = {10.1007/s11554-023-01317-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new approach to snow avalanche rescue using UAV pictures based on convolutional neural networks},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Language meets YOLOv8 for metric monocular SLAM.
<em>JRTIP</em>, <em>20</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01318-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach that combines spoken language and visual object detection to produce a depth image to perform metric monocular SLAM in real time and without requiring a depth or stereo camera. We propose a methodology where a compact matrix representation of the language and objects, along with a partitioning algorithm, is used to resolve the association between the objects mentioned in the spoken description and the objects visually detected in the image. The spoken language is processed online using Whisper, a popular automatic speech recognition system, while the YOLOv8 network is used for object detection. Camera pose estimation and mapping of the scene are performed using ORB-SLAM. The full system runs in real time, allowing a user to explore the scene with a handheld camera, observe the objects detected by YOLOv8, and provide depth information of these objects with respect to the camera via a spoken description. We have performed experiments in indoor and outdoor scenarios, comparing the resulting camera trajectory and map obtained with our approach against that obtained when using RGB-D images. Our results are comparable to those obtained with the latter without losing real-time performance.},
  archive      = {J_JRTIP},
  author       = {Martinez-Carranza, Jose and Hernández-Farías, Delia Irazú and Rojas-Perez, L. Oyuki and Cabrera-Ponce, Aldrich A.},
  doi          = {10.1007/s11554-023-01318-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Language meets YOLOv8 for metric monocular SLAM},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time textile fabric flaw inspection system using
grouped sparse dictionary. <em>JRTIP</em>, <em>20</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01319-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fabric surface flaw inspection is essential for textile quality control, and it is demanding to replace human inspectors with the automatic machine vision-based flaw inspection system. To alleviate the time-consuming problem of sparse coding in detecting phase, this work presents a real-time fabric flaw inspection method by using grouped sparse dictionary. Firstly, the overcomplete sparse dictionary is learned from normal fabric images; secondly, the learned sparse dictionary is grouped into several sub-dictionaries by evaluating reconstruction error. Finally, the grouped dictionary is used to represent image and identify flaw regions as they cannot be represented well, leading to large reconstruction error. In addition, a non-maximum suppression algorithm is also proposed to reduce false inspection further. Experiments on various fabric flaws and real-time implementation on the proposed vision-based hardware system are conducted to evaluate the performance of proposed method. In comparison with other dictionary learning methods, the experimental results demonstrate that the proposed method can reduce the running time significantly and achieve a decent performance, which is capable of meeting the real-time inspection requirement without compromising inspection accuracy.},
  archive      = {J_JRTIP},
  author       = {Wang, Xiaohu and Yan, Benchao and Pan, Ruru and Zhou, Jian},
  doi          = {10.1007/s11554-023-01319-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time textile fabric flaw inspection system using grouped sparse dictionary},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time enhancement using multi-linear adaptive gamma
correction (MLAGC) for better night driving. <em>JRTIP</em>,
<em>20</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s11554-023-01320-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a real-time video-stream enhancement scheme is proposed along with a hardware prototype development. The proposed technique employs a multi-linear adaptive gamma correction method to locally enhance the dark video frames in presence of high intensity optical sources. The proposed algorithm employs three linear functions corresponding to low, medium and high intensity regions for proper enhancement of different intensity regions. The slope and range of the linear functions are automatically derived from intensity distribution and hence the overall multi-linear function becomes highly nonlinear and adaptive to varying constraints. Consequently, the input–output mapping function of the algorithm works as a spreading function in the low intensity range for contrast improvement and preserves the information of the high intensity regions to avoid information loss due to over-enhancement. The experimental results show that the proposed algorithm exhibits better performance than various existing algorithms in terms of subjective and objective measures.},
  archive      = {J_JRTIP},
  author       = {Pattanayak, Abanikanta and Acharya, Aditya and Dash, Judhisthir},
  doi          = {10.1007/s11554-023-01320-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time enhancement using multi-linear adaptive gamma correction (MLAGC) for better night driving},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient model for real-time wildfire detection in
complex scenarios based on multi-head attention mechanism.
<em>JRTIP</em>, <em>20</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01321-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfire is a common natural disaster that destroys the ecological environment and endangers the lives and property of people. The average precision and speed of wildfire detection algorithms are critical in complex wildfire detection scenarios. However, the complex wildfire scenarios and multi-scale flaming objects decrease the detection accuracy of the model. How to design an accurate and fast wildfire detection model is a challenging study. To improve the average detection precision of the model while reducing its computational cost as much as possible, we propose an efficient model for real-time wildfire detection in complex scenarios called FireDetn. First, we use four different detection heads to help FireDetn detect different size flame objects. Then, we integrate Transformer Encoder blocks with multi-head attention in FireDetn to enhance its ability to capture global feature information and contextual information, thus improving its average precision in complex scenarios. Finally, we fuse the spatial pyramid pooling fast structure, which benefits in detecting multi-scale flame objects at a lower computational cost. Our experimental results show that the FireDetn can detect about 112 frames per second, meeting the requirement of real-time detection. We also compare FireDetn with other algorithms, demonstrating that FireDetn obtains satisfactory results in average precision and speed.},
  archive      = {J_JRTIP},
  author       = {Wang, Xiaotian and Pan, Zhongjie and Gao, Hang and He, Ningxin and Gao, Tiegang},
  doi          = {10.1007/s11554-023-01321-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient model for real-time wildfire detection in complex scenarios based on multi-head attention mechanism},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An investigation of camera movements and capture techniques
on optical flow for real-time rendering and presentation.
<em>JRTIP</em>, <em>20</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01322-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New and interesting uses for portable devices include the creation and viewing of 3D models and 360-degree photos of real landscapes. To provide a 3D model and a 360-degree view of a scenario, these apps search for real-time rendering and presentation. This study examines the impact of real-time image processing on movements in camera view and the application of optical flow algorithms. The optical flows are affected by how the objects involved motion. The way the image is recorded and the camera movements affect the relative motion. As a consequence, camera motions are responsive to optical flow algorithms. To record an image, the camera may pan around, move in a straight path, or move randomly. We have captured datasets of videos produced in different contexts to better replicate real-world scenarios. Each dataset was captured in a variety of illumination situations, camera movements, and indoor and outdoor recording sites. Here, to determine the most effective optical flow algorithms for use in near real-time applications, such as augmented reality and virtual reality, we compare results based on quality and processing delay for each video frame. We conducted a comparison study to better understand how random camera motion affects real-time video processing. These methods can be used to handle a variety of real-world issues, such as object tracking, video segmentation, structure from motion, gesture tracking, and so on.},
  archive      = {J_JRTIP},
  author       = {Modi, Nishant and Ramakrishna, M.},
  doi          = {10.1007/s11554-023-01322-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An investigation of camera movements and capture techniques on optical flow for real-time rendering and presentation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight real-time lane detection algorithm based on
ghost convolution and self batch normalization. <em>JRTIP</em>,
<em>20</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01323-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lane detection algorithm based on lane shape prediction with transformers (LSTR) is designed to address the problems of a large number of feature extraction network parameters, low utilization of original feature information, and easy loss of detail and edge information in the current lane detection algorithm. First, to reduce the number of parameters in the lane detection network and achieve a lightweight design, the ordinary convolution is replaced by ghost convolution (Ghost-Conv) with good performance; second, to enhance the utilization of the original feature information in the network and improve the lane detection accuracy, a self batch normalization (Self-BN) module is proposed to retain more original feature information by changing the normalization to achieve the improvement of the lane detection accuracy, and finally, to improve the accuracy of the network for lane detection, an efficient channel attention (ECA) mechanism is introduced to enhance the extraction effect of lane detail information and edge information. Experiments are conducted on the open source dataset TuSimple, and the results show that the proposed algorithm reduces the number of parameters and computation by nearly half, improves the detection speed by 33 FPS, increases the detection accuracy by 0.96%, reaches 97.11%, and reduces the false positive rate and the false negative rate by 0.55% and 0.71%, respectively, meeting the real-time requirements of autonomous driving, compared to the original network. Compared to other lane detection networks, it also has great advantages.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Ji, Wei and Zhang, Shanghui and Song, Yijing and He, Lintao and Xue, Hang},
  doi          = {10.1007/s11554-023-01323-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight real-time lane detection algorithm based on ghost convolution and self batch normalization},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hardware acceleration of YOLOv7-tiny using high-level
synthesis tools. <em>JRTIP</em>, <em>20</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01324-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs have emerged as a promising platform for implementing neural networks due to their reconfigurability, parallelism, and low power consumption. Nonetheless, designing and optimizing FPGA-based neural network accelerators is a complex and time-consuming task with register transfer level (RTL) languages. High-level synthesis (HLS) tools provide a higher level of abstraction for FPGA design, enabling designers to concentrate on top-level design aspects, such as algorithms, rather than low-level hardware implementation details. One of the state-of-the-art object detection networks is you look only once (YOLO) network series which is constructed using different neural network technologies using cross-stage connections and feature extraction techniques like pyramid networks. In this paper, we propose a method for the implementation of YOLOv7-tiny network on FPGAs using HLS tools. We present a comprehensive analysis of the performance and resource utilization of FPGA-based neural network accelerators. Our methods show excellent results for real-time application requirements such as latency. Specifically, our work reduces the usage of digital signal processing (DSP) units by 90% and it saves up to 60% of flip-flops compared to state-of-the-art designs, while achieving competitive usage of block RAM and look-up tables. Additionally, the achieved design latency of 15 ms is extremely suitable for real-time applications. Also we will propose a method for BRAM utilization method and off-chip memory access.},
  archive      = {J_JRTIP},
  author       = {Hosseiny, Adib and Jahanirad, Hadi},
  doi          = {10.1007/s11554-023-01324-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware acceleration of YOLOv7-tiny using high-level synthesis tools},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel algorithm for wrapped phase denoising.
<em>JRTIP</em>, <em>20</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01325-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a parallel implementation of a fixed-point algorithm for wrapped phase denoising. The model based on total variation efficiently estimates discontinuous phase maps and further incorporates the Pythagorean trigonometric identity between the real and imaginary parts of the phase map, enhancing the quality of the restored phase. In this work, two parallel C/C++ implementations of the sequential version of algorithms were developed. The implementations include execution in a multi-core CPU and a GPU using OpenMP and CUDA, respectively. We show performance comparisons of the parallel implementations with advanced methods using synthetic and experimental data. Results show that our parallel implementations achieve speedups over the serial implementation of $$12\times$$ for multi-core CPU and $$110\times$$ for GPU.},
  archive      = {J_JRTIP},
  author       = {Jesús May-Cen, Iván de and Hernandez-Lopez, Francisco J. and Legarda-Sáenz, Ricardo and Brito-Loeza, Carlos},
  doi          = {10.1007/s11554-023-01325-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel algorithm for wrapped phase denoising},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and efficient face detector based on large kernel
attention for CPU device. <em>JRTIP</em>, <em>20</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01326-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient face detectors with low computational cost and fast speed are still a pressing problem despite the significant progress made in uncontrolled face detection. To address this issue, we propose two lightweight face detectors for CPU device, named speed-priority face detector (SPFD) and accuracy-priority face detector (APFD). In the case of SPFD, we propose a simplified version of FaceBoxes by reducing convolutional layers and channel number for reducing model complexity and computational cost, as well as replacing max pooling layers with group convolutional layers to learn local information and features’ integration. Additionally, large kernel attention modules based on prior knowledge of face size and network architecture are applied to increase the expression ability of features and capture the key information from long distances. Finally, an iterative retrained method is designed to further enhance the accuracy without increasing any cost during model testing. Regarding APFD, a new anchor generation strategy is utilized to find out more faces based on SPFD. Extensive experiments conducted on WIDER FACE validation dataset indicate that our detectors exceed FaceBoxes comprehensively in terms of accuracy and speed. Specifically, SPFD outperforms FaceBoxes by 4.1%, 8.6%, and 9.6% on WIDEFACE validation dataset while achieving the fastest detection speed on CPU device for VGA-resolution images. The average speed can reach 45FPS on CPU devices, while the size of its parameters is only 0.66 times of FaceBoxes. Moreover, the APFD outperforms many famous and lightweight face detectors and attains superior accuracy (easy: 91.4%, medium:88.1%, and hard: 64.7%) with 20FPS; it achieves the best trade-off between accuracy and speed for face detection.},
  archive      = {J_JRTIP},
  author       = {Qi, Shuaihui and Song, Xiaofeng and Li, Zhiyuan and Xie, Tao},
  doi          = {10.1007/s11554-023-01326-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast and efficient face detector based on large kernel attention for CPU device},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reference picture selection scheme for robust HEVC video
transmission using compressed domain saliency map. <em>JRTIP</em>,
<em>20</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01327-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high level of compression achieved by high efficiency video coding (HEVC) helps reduce network traffic loads and mitigate data rate requirements. However, HEVC is vulnerable to error-prone channels where transmission errors can result in severe degradation of video quality. In this paper, a saliency-aware encoding scheme is proposed to improve the error robustness of HEVC streaming, by reducing temporal error propagation in case of packet loss. The proposed scheme firstly introduces a saliency detection model in compressed domain, based on two HEVC features derived from the depth splitting of the coding unit and the residual. Incorporated with the saliency map, an improved reference frame selection strategy is then introduced to reduce the inter-prediction mismatch that occurs at the decoder after packet loss. Specifically, the reference frames are dynamically selected based on the saliency-weighted Lagrangian optimisation, which not only reduces the number of prediction units (PUs) that depend on a single reference in saliency regions but also chooses the optimal coding mode for non-saliency regions. Finally, the most salient PUs are required to select the reference block in which most of the pixels are coded with intra-mode, for providing more robust reference to saliency regions. The simulation results show that the proposed reference picture selection scheme outperforms other reference methods with higher error robustness and a smaller loss in coding efficiency. Compared to the HEVC reference software, the proposed scheme is able to improve the quality of recovered video after packet loss, achieving average PSNR gains of up to 1.92 dB.},
  archive      = {J_JRTIP},
  author       = {Xu, Jiajun and Wang, Bing and Peng, Qiang},
  doi          = {10.1007/s11554-023-01327-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reference picture selection scheme for robust HEVC video transmission using compressed domain saliency map},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-complexity CNN-based CU partitioning for intra frames.
<em>JRTIP</em>, <em>20</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01328-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The High-Efficiency Video Coding (HEVC) standard has high compression efficiency. This efficiency is achieved at the expense of increasing the computational complexity. The HEVC encoder has the hierarchical search for optimal Coding Unit (CU) partitioning. It is based on rate-distortion optimization. Various solutions are proposed to reduce the encoding time. But, the machine learning-based methods have more effective in reducing the encoding time. Yet, deep learning tools have a relatively high computational load. So, in this paper a new low complexity convolutional neural network has been designed. It is called Convolutional Neural Network-based CTU Partitioner (CNNCP). It reduces the computational complexity of the HEVC encoding. The CNNCP takes the CTU luminance component and the quantization parameter (QP) as inputs, and provides the CU depth matrix in output at once. The CNNCP does not follow the hierarchical approach. Thus, it has a fixed computation structure that facilitates the use of parallel processing tools. The CNNCP has a simple structure with a least number of parameters, and thus, it has the least computational complexity. It has been trained and tested with a large database for all QP values. The results show that it reduced the encoding time by more than 90%, and makes it suitable for real-time applications.},
  archive      = {J_JRTIP},
  author       = {Rahimi, Yaser and Rezaei, Mehdi and Jafari, Pouria},
  doi          = {10.1007/s11554-023-01328-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-complexity CNN-based CU partitioning for intra frames},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-YOLOv4: Lightweight YOLOv4 based on modified RFB-s and
depthwise separable convolution for multi-target detection in complex
scenes. <em>JRTIP</em>, <em>20</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01329-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and diversity of complex scenes make multi-target detection a great challenge. Traditional target detectors based on deep learning suffer from extreme computational complexity, excessive memory consumption and poor real-time performance. We propose a lightweight multi-target detection model named L-YOLOv4. First, we choose the lightweight backbone network MobileNeXt instead of CSPDarknet53 to build up the efficiency of feature extraction and reduce the model complexity. Then we introduce an modified Receive Field Block smaller, which uses asymmetric cavity convolution and SE module to enhance the network feature extraction ability and increase the receptive field. Finally, we propose a DSC-ECA module consisting of a depthwise separable convolution (DSC) and an efficient channel attention (ECA) to replace standard convolutions. It diminishes the model parameters significantly and compensates for the accuracy loss. Experimental results show that L-YOLOv4 achieves 74.85 $$\%$$ on the PASCAL VOC07+12 dataset and 25.28 $$\%$$ on the MS COCO dataset. Under the premise of ensuring detection accuracy, L-YOLOv4 reduces 77.23 $$\%$$ of the parameters and the computational cost is only 16 $$\%$$ of that of YOLOv4. Its detection speed is 45.2 frames per second (FPS) on 3060Ti, which enables lightweight and real-time detection of the model.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Qian, Huaming and Bao, Jiabing and Zhou, Yipeng and Yan, Shuya},
  doi          = {10.1007/s11554-023-01329-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {L-YOLOv4: Lightweight YOLOv4 based on modified RFB-s and depthwise separable convolution for multi-target detection in complex scenes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). U-DPnet: An ultralight convolutional neural network for the
detection of apples in orchards. <em>JRTIP</em>, <em>20</em>(4), 1–11.
(<a href="https://doi.org/10.1007/s11554-023-01330-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate detection of apples is critical for the successful implementation of harvesting robots in orchards. However, due to limited memory resources on robotic platforms, it is imperative to develop lightweight detection algorithms that can operate in real-time. To address this challenge, we propose an ultralight convolutional neural network, U-DPnet, based on depth-separable convolution. Our approach incorporates the cross-stage deep separable module (CDM) and the multi-cascade deep separable module (MDM) in the backbone for nonlinear unit addition and attention mechanisms, which reduce the volume of the network while improving the feature representation capability. A simplified bi-directional feature pyramid network (BiFPN) is constructed in the neck for multi-scale feature fusion, and Adaptive feature propagation (AFP) is designed between the neck and the backbone for smooth feature transitions across different scales. To further reduce the network volume, we develop a uniform channel downsampling and network weight-sharing strategy. Multiple loss functions and label assignment strategies are used to optimize the training process. The performance of U-DPnet is verified on a homemade Apple dataset. Experimental results demonstrate that U-DPnet achieves detection accuracy and speed comparable to that of the 7 SOTA models. Moreover, U-DPnet exhibits an absolute advantage in model volume and computations (only 1.067M Params and 0.563G FLOPs, 39.79% and 36.36% less than yolov5-n).},
  archive      = {J_JRTIP},
  author       = {Wan, Hao and Zeng, Xilei and Fan, Zeming and Zhang, Shanshan and Zhang, Ke},
  doi          = {10.1007/s11554-023-01330-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {U-DPnet: An ultralight convolutional neural network for the detection of apples in orchards},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient lightweight CNN model for real-time fire smoke
detection. <em>JRTIP</em>, <em>20</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01331-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early fire and smoke detection with computer vision have attracted much attention in recent years, and a lot of fire detectors based on deep neural network have been proposed to improve the detection accuracy. However, most current fire detectors still suffer from low detection accuracy caused by the multi-scale variation of the fire and smoke, or the high false accept rate due to the fire-like or smoke-like objects within the background. In this paper, to address the above challenges, we propose an effective real-time fire detection network (AERNet) with two key functional modules, which achieves a good tradeoff between the detection accuracy and speed. First, we employ a lightweight backbone network Squeeze and Excitation-GhostNet (SE-GhostNet) to extract features, which can make it easier to distinguish the fire and smoke from the background and reduce the model parameters greatly. Second, a Multi-Scale Detection module is constructed to selectively emphasize the contribution of different features by channel and space. Finally, we adopt the decoupled head to predict the classes and locations of fire or smoke respectively. In the experiment, we propose a more challenging dataset “Smoke and Fire-dataset” (“SF-dataset”) to evaluate the proposed algorithm, which includes 18,217 images. And the results show that the proposed method outperforms most SOTA methods in detection accuracy, model size, and detection speed.},
  archive      = {J_JRTIP},
  author       = {Sun, Bangyong and Wang, Yu and Wu, Siyuan},
  doi          = {10.1007/s11554-023-01331-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient lightweight CNN model for real-time fire smoke detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary algorithm for optimized CNN architecture search
applied to real-time boat detection in aerial images. <em>JRTIP</em>,
<em>20</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01332-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When processing the detection of boats in aerial images by neural networks, we have always been concerned about the execution time of these networks in the equipment on board the Unmanned Aerial Vehicle (UAV). Throughout its mission, the UAV will capture images that must be processed in real time. For this purpose, a network optimized for execution time is essential. This article proposes an enhanced Network Architecture Search (NAS) method for searching for time-optimized detection networks, for a given dataset, using an evolutionary algorithm. The search uses mutations as a mechanism of evolution that affect the structure of the network and the hyper-parameters of its layers. Its original fitness function allows the choice of architectures that are not very greedy in terms of operations, specifically favouring small networks whose advantages are to be fast and quick to train, thus accelerating the search algorithm. Using this method, we were able to obtain detection networks with an improved mean Average Precision (mAP) compared to the initial network (parent) but with much fewer FLoating-point OPerations (Flops): 68% of operations reduction. This induces considerable gain in terms of execution time with 50 Frames Processed per Second (FPS) in an embedded environment on a drone.},
  archive      = {J_JRTIP},
  author       = {Zerrouk, Ilham and Moumen, Younes and Khiati, Wassim and El Habchi, Ali and Berrich, Jamal and Bouchentouf, Toumi},
  doi          = {10.1007/s11554-023-01332-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Evolutionary algorithm for optimized CNN architecture search applied to real-time boat detection in aerial images},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time and efficient surface defect detection method
based on YOLOv4. <em>JRTIP</em>, <em>20</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01333-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a lightweight and fast detection framework called Mixed YOLOv4-LITE series based on You Only Look Once (YOLOv4) for industrial defect detection. To reduce the size of the model and achieve a better balance between accuracy and speed, MobileNet series (MobileNetv1, MobileNetv2, MobileNetv3) and depthwise separable convolutions are employed in the modified network architecture to replace the backbone network CSPdarknet53 and traditional convolution in the neck and head of YOLOv4, respectively. Additionally, we combine the Mosic data enhancement method to enrich the dataset. To accelerate the convergence of the network, transfer learning is used in the training stage, in which pseudo-convergence is precluded as much as possible by adjusting the learning rate of the cosine annealing scheduler. Finally, we evaluate the proposed methods on both public defect datasets, NEU-DET and PCB-DET, with different types and scales. On NEU-DET, Mixed YOLOv4-LITEv1 achieved an improvement of 214% in detection speed while maintaining accuracy, detecting at a rate of 88 FPS on a single GPU. Meanwhile, Mixed YOLOv4-LITEv1 realizes an outstanding maximum improvement of 200% in detection speed while only losing a mean average precision (mAP) value of 1.77% on PCB-DET. Furthermore, the sizes of our proposed series models are only about one-fifth of the original YOLOv4 model. The extensive test results indicate that our work can provide an efficient scheme with low deployment cost for surface defect detection at different scales in multiple scenarios, meeting the needs of practical industrial applications.},
  archive      = {J_JRTIP},
  author       = {Liu, Jiansheng and Cui, Guolong and Xiao, Chengdi},
  doi          = {10.1007/s11554-023-01333-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and efficient surface defect detection method based on YOLOv4},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient image color enhancement using a new tint
intensification algorithm. <em>JRTIP</em>, <em>20</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01334-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital photos are deemed an important source of information. The global request for lucid images has upsurged in the last decade. Accordingly, increasing the quality of images can be made through different features. Among such, color is a crucial feature as it gives an image a pleasing look and holds key significant information. As known, digital images are obtained with degradations, and deficient colors are an effect that can be observed in different types of images obtained by various modern imaging systems. Improving the colors while preserving the other important image features and details is needed for various real-world uses. Hence, an expeditious tint intensification (TI) algorithm that can boost the colors is introduced in this research, in that it begins by converting the image to the HSV domain and preserving the hue channel while processing the other two channels of saturation and value using different concepts. The image is then converted to the RGB domain and processed again using several methods to produce the desired output. Different experiments on real-world images have been made, comparisons with various algorithms are also attained and the results have been assessed using three dedicated image evaluation metrics. Promising results have been obtained, in that the TI algorithm is proven to deliver visually pleasing results, in that the colors appear vivid, the contrast is adequate, and the brightness is preserved with no visible filtering flaws. This is imperative because a few computations have been used to produce high-quality results in a fast and efficient way. The outcomes of this study are significant as they can be utilized in different important research areas.},
  archive      = {J_JRTIP},
  author       = {Al-Ameen, Zohair},
  doi          = {10.1007/s11554-023-01334-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient image color enhancement using a new tint intensification algorithm},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time deployment of BI-RADS breast cancer classifier
using deep-learning and FPGA techniques. <em>JRTIP</em>, <em>20</em>(4),
1–13. (<a href="https://doi.org/10.1007/s11554-023-01335-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is commonly recognized as the second most frequent malignancy in women worldwide. Breast cancer therapy includes surgical surgery, radiation therapy, and medication which can be exceedingly successful, with 90% or higher survival rates, especially when the condition is discovered early. This work is one such approach for early detection of breast cancer relying on the BI-RADS score. In this regard, a computer-aided-diagnosis system based on a bespoke Digital Mammogram Diagnostic Convolutional Neural Network (DMD-CNN) model that can aid in the categorization of mammogram breast lesions is proposed. Furthermore, a PYNQ-based acceleration through the Artix 7 FPGA is employed for deployment of DMD-CNN model’s hardware acceleration platform which is the first of its kind for breast cancer, yielding a performance accuracy of 98.2%, the proposed model exceeded the state-of-the-art approach. The comparative analysis performed in the study has shown that the proposed method has resulted in a 4% increase in accuracy and a good recognition rate of 96% when compared to the existing model. A k-fold cross-validation (k = 5, 7, 9 the reported accuracy score values are 96.2%, 97.5% and 98.1%, respectively) approach was used to test and assess the integrated system. Extensive testing using mammography datasets was carried out to determine the increased performance of the suggested approach. Experiments reveal that when compared to the DMD-CNN model acceleration to GPU, the suggested solution not only optimizes resource utilization but also decreases power consumption to 3.12 W. Hardware acceleration through FPGA resulted in processing and analyzing nearly 91 images in a second where a single image will be processed using CPU.},
  archive      = {J_JRTIP},
  author       = {Maria, H. Heartlin and Kayalvizhi, R. and Malarvizhi, S. and Venkatraman, Revathi and Patil, Shantanu and Kumar, A. Senthil},
  doi          = {10.1007/s11554-023-01335-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time deployment of BI-RADS breast cancer classifier using deep-learning and FPGA techniques},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of a real-time eye movement-based computer
interface for communication with improved accuracy for disabled people
under natural head movements. <em>JRTIP</em>, <em>20</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01336-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the scarcity of effective communication systems has been an essential issue for disabled people [physically disabled, locomotor disability, and amyotrophic lateral sclerosis (ALS)] who cannot speak, walk, or move their hands. The lives of disabled people depend on others for survival, so they need assistive technology to live independently. This research paper aims to develop an efficient real-time eye-gaze communication system using a low-cost webcam for disabled persons. This proposed work developed a Video-Oculography (VOG) based system under natural head movements using a 5-point user-specific calibration (algorithmic calibration) approach for eye-tracking and cursor movement. During calibration, some parameters are calculated and used to control the computer with the eyes. Additionally, we designed a graphical user interface (GUI) to examine the performance and fulfill the basic daily needs of disabled individuals. The proposed method enables disabled persons to operate a computer by moving and blinking their eyes, similar to a typical computer user. The overall cost of the developed system is low (Cost &lt; $50, varies based on camera usage) compared to the cost of various existing systems. The proposed system is tested with disabled and non-disabled individuals and has achieved an average blinking accuracy of 97.66%. The designed system has attained an average typing speed of 15 and 20 characters per minute for disabled and non-disabled participants, respectively. On average, the system has achieved a visual angle accuracy of 2.2 degrees for disabled participants and 0.8 degrees for non-disabled participants. The experiment’s outcomes demonstrate that the developed system is robust and accurate.},
  archive      = {J_JRTIP},
  author       = {Chhimpa, Govind Ram and Kumar, Ajay and Garhwal, Sunita and Dhiraj},
  doi          = {10.1007/s11554-023-01336-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Development of a real-time eye movement-based computer interface for communication with improved accuracy for disabled people under natural head movements},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPGA acceleration of secret sharing for 3D data cubes.
<em>JRTIP</em>, <em>20</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01337-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secret sharing can protect the security of important secret information in the network environment. However, the computational complexity and processing delay increase drastically when the secret information contains large amounts of data, such as three-dimensional (3D) data cubes. To improve the efficiency of secret sharing on 3D data cubes, this paper proposes a hardware architecture to accelerate the generation of shares and reconstruction of the secret. The proposed hardware architecture parallelizes the secret sharing process and optimizes on this basis to save large amounts of circuit resources. Simulation results show that this architecture performs secret sharing more than ten times faster than the software implementation. This study enables the secret sharing of 3D data cubes, which protect large amounts of data information throughout the process and allows complete reconstruction of 3D data cubes. To be able to handle secrets without decoding them, we then extend the hardware architecture to the four basic processes of multi-party computation, demonstrating the feasibility of the structure and providing preliminary research tools for the effective implementation of multi-party computation.},
  archive      = {J_JRTIP},
  author       = {Wu, Zi-Ming and Liu, Tao and Yan, Bin and Pan, Jeng-Shyang and Yang, Hong-Mei},
  doi          = {10.1007/s11554-023-01337-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA acceleration of secret sharing for 3D data cubes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart camera with image encryption: A secure solution for
real-time monitoring in industry 4.0. <em>JRTIP</em>, <em>20</em>(4),
1–20. (<a href="https://doi.org/10.1007/s11554-023-01338-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 standards have been widely adopted by most industries, encompassing remote monitoring for workplace safety, logistics, asset monitoring, quality management, and product monitoring. Real-time data, including images, must be transmitted for effective remote monitoring. However, due to cybersecurity concerns, the data must be encrypted before being sent over the internet. To address this, a real-time smart camera-based encryption method using chaos is proposed in this work. The encryption logic is created using LabVIEW and integrated into the smart camera. The four-step encryption process includes pixel shuffling, confusion, Improved logistic mapping, and diffusion. Encryption keys are generated using the user’s passcode to set the initial parameters of the logistic mapping for generating key sequences. The resulting encrypted frames are diffused with index key streams to further enhance security. The proposed method has undergone security and performance analyses, demonstrating robustness against various vulnerabilities and minimal computational time. This makes it suitable for real-time encryption scenarios in Industry 4.0.},
  archive      = {J_JRTIP},
  author       = {Sekar, C. and Falmari, Vinod Ramesh and Brindha, M.},
  doi          = {10.1007/s11554-023-01338-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Smart camera with image encryption: A secure solution for real-time monitoring in industry 4.0},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time segmentation network for compact camera module
assembly adhesives based on improved u-net. <em>JRTIP</em>,
<em>20</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01290-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the real-time and accuracy of glue segmentation in compact camera module images, this paper proposes a real-time segmentation network model based on U-Net. To improve the inference speed of the model, the encoder of the network is redesigned into a three-level feature extraction structure, and the upsampling collocation of bilinear interpolation and sub-pixel convolution is used in the decoder. To enhance information fusion in the context of the network, the context guiding block is embedded in the feature extraction branch of the model. On this basis, improvements to convolutional block attention module and embedded in the jump connection to guide upsampling. The experimental results show that the inference speed of the segmentation network in this paper can reach 227.02 frames per second, which is better than real-time segmentation networks, such as ENet and CGNet. The segmentation accuracy of reinforced glue, dust-trapping glue, and escaped air hole glue is closer to high-precision segmentation networks, such as Deeplab V3+ and U-Net. The network model achieves 96.41% Intersection over Union on the segmentation of the reinforcing glue. Exploring the application of this paper’s network to other objects with simple semantic information is an important direction for future research.},
  archive      = {J_JRTIP},
  author       = {Li, Dongjie and Deng, Haipeng and Li, Changfeng and Chen, Hui},
  doi          = {10.1007/s11554-023-01290-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time segmentation network for compact camera module assembly adhesives based on improved U-net},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time efficient semantic segmentation network based on
improved ASPP and parallel fusion module in complex scenes.
<em>JRTIP</em>, <em>20</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01298-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation can help the perception link to better build an understanding of complex scenes, and can assist the unmanned system to better perceive the scene content. To address the problem of detailed information loss and segmentation edge blur in the semantic segmentation task for complex scenes, we propose a modified version of Deeplabv3+ based on the improved ASPP and fusion module. Firstly, we propose an RA-ASPP module combining residual network and asymmetric atrous convolution block (AACB), which further enriches the scale of feature extraction and achieves denser multi-scale feature extraction. It significantly enhances the representation power of the network. Then, we propose a parallel fusion module named convolution combine with bottleneck block (CBB), which combines 1 $$\times$$ 1 convolution and bottleneck block to reduce the information loss in the whole network transmission process. We perform ablation experiments on the PASCAL VOC2012 dataset. When the backbone is Xception, the Mean Intersection over Union (MIoU) of Ours1 is 79.78 $$\%$$ . At the cost of 1.72 frames per second (FPS), its MIoU is 2.81 $$\%$$ faster than Deeplabv3+. The proposed modules significantly improve the accuracy in semantic segmentation and achieve segmentation results comparable to state-of-the-art algorithms. When MobileNetV2 is the backbone, Ours2 achieves 37.54FPS and a MIoU of 73.32 $$\%$$ , which ensures a balance between real-time segmentation speed and accuracy. In summary, our proposed modified module improves the segmentation performance of Deeplabv3+, and the different backbones also provide additional options for semantic segmentation tasks in complex scenes.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Qian, Huaming and Zhou, Yipeng and Yan, Shuya and Feng, Shibao and Yu, Shuang},
  doi          = {10.1007/s11554-023-01298-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time efficient semantic segmentation network based on improved ASPP and parallel fusion module in complex scenes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A novel finetuned YOLOv6 transfer learning model for
real-time object detection. <em>JRTIP</em>, <em>20</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11554-023-01299-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection and object recognition are the most important applications of computer vision. To pursue the task of object detection efficiently, a model with higher detection accuracy is required. Increasing the detection accuracy of the model increases the model’s size and computation cost. Therefore, it becomes a challenge to use deep learning in embedded environments. To overcome this problem, the current research suggests a transfer-learning-based model for real-time object detection that enhances the YOLO algorithm&#39;s effectiveness. The model utilizes YOLOv6 as a baseline model. This study proposes a pruning and finetuning algorithm as well as a transfer learning algorithm for enhancing the proposed model’s efficiency in terms of detection accuracy and inference speed. This paper also focuses on how the proposed model will be able to identify all objects (indoor as well as outdoor) in a scene and provides a voice output to warn the user about nearby and faraway objects. To receive the audio feedback, Google Text-to-Speech (gTTs) library is used. The model is trained on the MS-COCO dataset. The proposed model is compared with the Tensorflow Single Shot Detector model, Faster RCNN model, Mask RCNN model, YOLOv4, and baseline YOLOv6 model. After pruning the YOLOv6 baseline model by 30%, 40%, and 50%, the finetuned YOLOv6 framework hits 37.8% higher average precision (AP) with 1235 frames per second (FPS).},
  archive      = {J_JRTIP},
  author       = {Gupta, Chhaya and Gill, Nasib Singh and Gulia, Preeti and Chatterjee, Jyotir Moy},
  doi          = {10.1007/s11554-023-01299-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel finetuned YOLOv6 transfer learning model for real-time object detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GPU-based parallelisation of a versatile video coding
adaptive loop filter in resource-constrained heterogeneous embedded
platform. <em>JRTIP</em>, <em>20</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01300-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a GPU-based parallelisation of an optimised versatile video decoder (VVC) adaptive loop filter (ALF) filter on a resource-constrained heterogeneous platform. The GPU has been comprehensively utilised to maximise the degree of parallelism, making the programme capable of exploiting the GPU capabilities. The proposed approach enables to accelerate the ALF computation by an average of two times when compared to an already fully optimised version of the software decoder implementation over an embedded platform. Finally, this work presents an analysis of energy consumption, showing that the proposed methodology has a negligible impact on this key parameter.},
  archive      = {J_JRTIP},
  author       = {Saha, Anup and Roma, Nuno and Chavarrías, Miguel and Dias, Tiago and Pescador, Fernando and Aranda, Víctor},
  doi          = {10.1007/s11554-023-01300-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU-based parallelisation of a versatile video coding adaptive loop filter in resource-constrained heterogeneous embedded platform},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust attribute-aware and real-time multi-target
multi-camera tracking system using multi-scale enriched features and
hierarchical clustering. <em>JRTIP</em>, <em>20</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01301-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Camera Multi-Target Tracking (MTMCT) has challenges such as viewpoint and pose variations, scale and illumination changes, and occlusion. Available MTMCT approaches have high computational complexity and are not sufficiently robust in the mentioned challenges. In this work, an Attribute Recognition-based MTMCT(AR-MTMCT) framework is presented for real-time application. This framework performs object detection, re-Identification (Re-Id) feature extraction, and attribute recognition in an end-to-end manner. Applying attributes highly improves MTMC online tracking performance in the mentioned challenges. The pipeline of AR-MTMCT consists of three modules. The first module is a novel one-shot Single-Camera Tracking (SCT) architecture named Attribute Recognition-Multi Object Tracking (AR-MOT) which performs object detection, Re-Id feature extraction, and attributes recognition using one backbone through multi-task learning. Hierarchical clustering is performed in the second module to deal with the detection of several instances of one identity in the overlapping areas of cameras. In the last module, a new data association algorithm is performed using spatial information to reduce matching candidates. We also have proposed an efficient strategy in the data association algorithm to remove lost tracks by making a trade-off between the number of lost tracks and the maximum lost time. Evaluation and training of AR-MTMCT have been done on the large-scale MTA dataset. The proposed system has been improved by 20% and 11%, respectively, compared to the WDA method in IDF1 and IDs metrics. Also, the AR-MTMCT outperforms the state-of-the-art methods by a large margin on decreasing computational complexities.},
  archive      = {J_JRTIP},
  author       = {Moghaddam, Mahnaz and Charmi, Mostafa and Hassanpoor, Hossein},
  doi          = {10.1007/s11554-023-01301-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A robust attribute-aware and real-time multi-target multi-camera tracking system using multi-scale enriched features and hierarchical clustering},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient image steganographic scheme for a real-time
embedded system and its hardware implementation on AMD xilinx zynq-7000
APSoC platform. <em>JRTIP</em>, <em>20</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01302-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal advance in the field of information security, steganography holds a crucial place in concealing sensitive information. It is the technique of hiding secret data within a digital media carrier, such as an image. An image steganographic scheme conceals secret data in a digital image by manipulating pixel values in a way that the data is undetectable to unauthorized parties in communication channels. These schemes are widely used for various real-time applications, including but not limited to content authentication, copyright protection, and biometric data protection. To effectively process image steganographic schemes in real-time applications, it is important to reduce computational delay and increase throughput. Implementing these schemes on a reconfigurable hardware platform is an efficient way to achieve these tasks. In this paper, we present a reconfigurable embedded system for an efficient steganographic scheme that embeds and recovers secret data in digital images using the AMD Xilinx Zynq-7000 all programmable system-on-chip (APSoC) platform. The system demonstrates real-time processing capabilities, delivering a frame rate of 30.7 FPS for a full high-definition RGB image, surpassing recent hardware implementations in terms of speed, resource utilization, and system throughput.},
  archive      = {J_JRTIP},
  author       = {Harb, Salah and Ahmad, M. Omair and Swamy, M. N. S.},
  doi          = {10.1007/s11554-023-01302-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient image steganographic scheme for a real-time embedded system and its hardware implementation on AMD xilinx zynq-7000 APSoC platform},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A color attention mechanism based on YES color space for
skin segmentation. <em>JRTIP</em>, <em>20</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01303-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin segmentation plays an important role in image processing and human–computer interaction tasks. However, it is a challenging task to accurately detect skin regions from various scenes with different illumination or color styles. In addition, in the field of video processing, reducing the computational load and improving the real-time performance of the algorithm has also become an important topic of skin segmentation. Existing deep semantic segmentation networks usually pay too much attention to the detection performance of the model and make the model structure tend to be complex, which brings heavy computational burden. To achieve the trade-off between detection performance and real-time performance of the skin segmentation algorithm, this paper proposes a lightweight skin segmentation network. Compared with existing semantic segmentation networks, this model adopts a simpler structure to improve the real-time performance. In addition, to improve the feature fitting ability of the network without slowing down its inference speed, this paper proposes a color attention mechanism, which locates skin regions in images based on the distribution features of skin colors on the E-R/G color plane generated from the YES color space, and guides the network to update parameters. Experimental results show that this method not only exhibits similar detection performance to existing semantic segmentation networks such as U-Net and DeepLab, but also the computation load of the model is 18.1% lower than Fast-SCNN.},
  archive      = {J_JRTIP},
  author       = {Ding, Shaobo and Liu, Zonghui and Lei, Zhichun},
  doi          = {10.1007/s11554-023-01303-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A color attention mechanism based on YES color space for skin segmentation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel low-power pipelined DCT processor for real-time IoT
applications. <em>JRTIP</em>, <em>20</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01304-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes a novel scalable Discrete Cosine transform (DCT) processor. It is based on a shared-resource enhanced Coordinate Rotation Digital Computer (CORDIC) unit, in a modified Loeffler architecture. All micro-rotation operations have moved to the last stages, and are implemented as one unified block in an overlapped form to reduce the utilization area. Also, the beginning stages consist of multiple-delay feedback butterfly units. The hardware-efficient pipelining method of the processor is another reason to reduce the power consumption. This is important while the purpose is resource-constraint IoT devices whereas establishing strong edge servers is not feasible everywhere. Especially in the case of modern video compression standards which demand higher points for DCT such as high efficiency video coding (HEVC) low-power design is crucial. Ubiquitous computing, Big Data and Cloud Services, IoT-Enabled Web are computation-intensive applications. Thus, power reduction of wireless networks of sensors such as, Bluetooth, RFID Wi-Fi, and smartphones, tablets, camcorders are in great demand. In-order input, and output is the proposed design’s advantage. Furthermore, due to utilization of shared-resource CORDIC-II unit, and reduction of adding, shifting operations, in size, and number, the architecture has high performance in short word lengths (WL) in comparison to state-of-the-art DCT processors.},
  archive      = {J_JRTIP},
  author       = {Khalili Sadaghiani, AbdolVahab and Forouzandeh, Behjat},
  doi          = {10.1007/s11554-023-01304-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Novel low-power pipelined DCT processor for real-time IoT applications},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A swin transformer-functionalized lightweight YOLOv5s for
real-time coal–gangue detection. <em>JRTIP</em>, <em>20</em>(3), 1–16.
(<a href="https://doi.org/10.1007/s11554-023-01305-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite various proposed algorithms predicated upon convolution neural networks to deal with coal–gangue detection under complex production, applying Transformer into the coal–gangue detection network has been rarely executed so far. Here, a lightweight CNN- and Transformer-based coal–gangue detection network is instituted via introducing Swin Transformer blocks to promote feature fusion and achieve accurate position and identification. Transformer enables interacting long-distance semantic information and including more semantic information into low-level features. The α-IoU loss is further leveraged to endow accurate regression of bounding box. Compared with the output heatmap by the original network, it is found that the modified network can accurately capture the area where the target is rather than the irrelevant background area. Images acquired in three illuminances served as test datasets (A1, A2, and A3) to unearth model’s illumination robustness. Outcomes denote that YOLOv5-Swin bears optimal illumination adaptability amid coal–gangue detection. Alongside pristine YOLOv5s, mAP of A1, A2, and A3 jump by 2.53%, 2.4%, 2.84%, respectively, while detection velocity can run at 147 FPS, twice as fast as YOLOv3’s velocity. This method meets the needs of real-time detection, which can accurately and quickly detect coal and gangue.},
  archive      = {J_JRTIP},
  author       = {Wen, Xiao and Li, Bo and Wang, Xuewen and Li, Juanli and Wei, Dailiang and Gao, Jihong and Zhang, Jie},
  doi          = {10.1007/s11554-023-01305-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A swin transformer-functionalized lightweight YOLOv5s for real-time coal–gangue detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning discriminative correlation filters via
saliency-aware channel selection for robust visual object tracking.
<em>JRTIP</em>, <em>20</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01306-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, discriminative correlation filters (DCF) with deep features have achieved excellent results in visual object tracking tasks. These trackers usually use multi-channel features of the fixed layer of the pre-trained network model to represent the target. However, the multi-channel features contain many interfering channels that are not conducive to object representation, resulting in overfitting and high computational complexity. To solve this problem, we research the correlation between multi-channel deep features and target saliency information and propose a novel DCF tracking method based on saliency-aware and adaptive channel selection. Specifically, we adaptively select the most representative feature channels to represent the target by calculating the energy mean ratio of the saliency-aware region to the search region, reducing the feature dimension and improving the tracking efficiency. Then, according to the feedback, the selected channels are given different weights to further enhance the discrimination of the filter. In addition, an adaptive update strategy is designed to alleviate the model degradation problem according to the fluctuation of feature maps in the recent frames. Finally, we use the alternating direction method of multipliers (ADMM) to optimize the proposed tracker model. Extensive experimental results on five well-known tracking benchmark datasets have verified the superiority of the proposed tracker with many state-of-the-art deep features-based trackers, and the running speed of the algorithm can basically meet the real-time requirements.},
  archive      = {J_JRTIP},
  author       = {Ma, Sugang and Zhao, Zhixian and Pu, Lei and Hou, Zhiqiang and Zhang, Lei and Zhao, Xiangmo},
  doi          = {10.1007/s11554-023-01306-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Learning discriminative correlation filters via saliency-aware channel selection for robust visual object tracking},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time traffic sign detection model based on multi-branch
convolutional reparameterization. <em>JRTIP</em>, <em>20</em>(3), 1–16.
(<a href="https://doi.org/10.1007/s11554-023-01307-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent detection of traffic signs has great potential in autonomous driving. Certain elements can make the detection difficult. In the road images captured by vehicle cameras, there could be various types of traffic sign objects some of which are quite similar. On top of that the complex background is adding additional noise which makes it challenging to balance detection speed and accuracy. In this paper, we propose a real-time traffic sign detection network based on the Anchor-free mechanism to solve this problem. First, we present the idea of reparameterization. In the training stage, the Conv $$3 \times 3$$ convolution in the feature extraction network CSPDarknet53 is reconstructed to enhance the extraction ability of traffic sign features. In the inference stage, the multi-branch structure is equivalently converted into a single-way structure to improve the inference speed of the model. Second, to improve the accuracy of locating small objects, we introduce the SIoU position regression loss function, which addresses the challenge of sensitive position regression for small objects. Lastly, test results on TT-100K, GTSDB and CCTSDB datasets show that the model can achieve a performance trade-off between speed and accuracy. This paper proposes a model that achieves an inference speed of 153.8 FPS on the CCTSDB dataset when tested on the GTX 2080ti, and a speed of 23.3 FPS on the Nvidia Jetson Xavier NX, demonstrating its ability to perform fast and efficient traffic sign detection on different hardware platforms.},
  archive      = {J_JRTIP},
  author       = {Huang, Mengtao and Wan, Yiyi and Gao, Zhenwei and Wang, Jiaxuan},
  doi          = {10.1007/s11554-023-01307-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time traffic sign detection model based on multi-branch convolutional reparameterization},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complexity and compression efficiency analysis of libaom AV1
video codec. <em>JRTIP</em>, <em>20</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01308-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent research effort aiming to provide a royalty-free video format resulted in AOMedia Video 1 (AV1), which was launched in 2018. AV1 was developed by the Alliance for Open Media (AOMedia), which groups several major technology companies such as Google, Netflix, Apple, Samsung, Intel, and many others. AV1 is currently one of the most prominent video formats and has introduced several complex coding tools and partitioning structures in comparison to its predecessors. A study of the computational effort required by the different AV1 coding steps and partition structures is essential for understanding its complexity distribution when implementing fast and efficient codecs compatible with this format. Thus, this paper presents two main contributions: first, a profiling analysis aiming at understanding the computational effort required by each individual coding step of AV1; and second, a computational cost and coding efficiency analysis related to the AV1 superblock partitioning process. Experimental results show that the two most complex coding steps of the libaom reference software implementation are the inter-frame prediction and transform, which represent 76.98% and 20.57% of the total encoding time, respectively. Also, the experiments show that disabling ternary and asymmetric quaternary partitions provide the best relationship between coding efficiency and computational cost, increasing the bitrate by only 0.25% and 0.22%, respectively. Disabling all rectangular partitions provides an average time reduction of about 35%. The analyses presented in this paper provide insightful recommendations for the development of fast and efficient AV1-compatible codecs with a methodology that can be easily replicated.},
  archive      = {J_JRTIP},
  author       = {Bender, Isis and Borges, Alex and Agostini, Luciano and Zatt, Bruno and Correa, Guilherme and Porto, Marcelo},
  doi          = {10.1007/s11554-023-01308-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Complexity and compression efficiency analysis of libaom AV1 video codec},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time fire detection method from video for electric
vehicle-charging stations based on improved YOLOX-tiny. <em>JRTIP</em>,
<em>20</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11554-023-01309-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the current mainstream detection methods are difficult to achieve fire detection in outdoor electric vehicle-charging station, this paper proposes a real-time fire detection method from video for electric vehicle charging stations based on improved YOLOX-tiny. CBAM attention mechanism is introduced to concatenate the spatial and channel attention information, to preserve the salient features of different shapes of flames. Depthwise Separable Convolution is used to replace Conventional Convolution to reduce the number of Parameters and FLOPs of the network, and improves the speed of detection and the deployment of the model on the embedded side. CIoU loss function is used to replace bounding box regression loss function of YOLOX-tiny, and the aspect ratio limit mechanism is added to improve the convergence speed of the loss function and make the prediction results more consistent with the actual target. Experiment shows that mAP value of improved YOLOX-tiny is 94.05 $$\%$$ , Precision is 91.76 $$\%$$ , and Recall is 83.27 $$\%$$ on the embedded side, with the video detection speed is 20 fps, which meets the demand for real-time detection for electric vehicle-charging stations.},
  archive      = {J_JRTIP},
  author       = {Ju, Yifan and Gao, Dexin and Zhang, Shiyu and Yang, Qing},
  doi          = {10.1007/s11554-023-01309-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time fire detection method from video for electric vehicle-charging stations based on improved YOLOX-tiny},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning for facial expression recognition: A
comprehensive survey. <em>JRTIP</em>, <em>20</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11554-023-01310-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is utilized in various fields that analyze facial expressions. FER is attracting increasing attention for its role in improving the convenience in human life. It is widely applied in human–computer interaction tasks. However, recently, FER tasks have encountered certain data and training issues. To address these issues in FER, few-shot learning (FSL) has been researched as a new approach. In this paper, we focus on analyzing FER techniques based on FSL and consider the computational complexity and processing time in these models. FSL has been researched as it can solve the problems of training with few datasets and generalizing in a wild-environmental condition. Based on our analysis, we describe certain existing challenges in the use of FSL in FER systems and suggest research directions to resolve these issues. FER using FSL can be time efficient and reduce the complexity in many other real-time processing tasks and is an important area for further research.},
  archive      = {J_JRTIP},
  author       = {Kim, Chae-Lin and Kim, Byung-Gyu},
  doi          = {10.1007/s11554-023-01310-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Few-shot learning for facial expression recognition: A comprehensive survey},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RT-droid: A novel approach for real-time android application
analysis with transfer learning-based CNN models. <em>JRTIP</em>,
<em>20</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01311-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the number, type and complexity of malware is increasing rapidly. Convolution neural network (CNN) based networks continue to be used in software classification based on image. In this study, a CNN model named Real Time-Droid(RT-Droid), which has a very fast malware detection capability and works based on YOLO V5, is introduced. RT-Droid detects android malware with high accuracy and performs this process at near real-time speed. For this process, firstly the features in the android manifest file are extracted and converted to an image in RGB format similar to QR code. Thus, images become processed by CNN-based deep learning models. These images were used to train VGGNet, Faster R-CNN, YOLO V4 and V5 models with the transfer learning technique. The android malware detection performances of the obtained trained models (weights) were examined. In the tests performed with Drebin, Genome and Arslan dataset, the precision value is 98.3%, while the F-score value is 97.0%. In obtaining these values, only 0.019 s per application was needed for analysis. It also requires 25 times less memory space compared to a gray-scale image. Since the small images of the YOLO V5 model can detect objects with very high accuracy and in real time, it provides serious efficiency in processing time. We also compared the results with VGGNet, Faster R-CNN and YOLO V4, which are commonly used CNN models for object detection, and show that it yields results at a higher rate and at least 5.5 times faster than similarly trained networks. Our method detects hacker-generated Android malware very quickly and with high accuracy, while being robust against obfuscated apps.},
  archive      = {J_JRTIP},
  author       = {Tasyurek, Murat and Arslan, Recep Sinan},
  doi          = {10.1007/s11554-023-01311-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RT-droid: A novel approach for real-time android application analysis with transfer learning-based CNN models},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Road disease detection algorithm based on YOLOv5s-DSG.
<em>JRTIP</em>, <em>20</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01312-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection and classification of road damages are critical for the timely maintenance and repair of road surfaces. To address issues in road damage detection, such as single detection type, low detection efficiency, low-resolution detection objects, and difficulty in detecting small target features, this paper proposes an improved road damage detection algorithm YOLOv5s-DSG based on YOLOv5s. First, optimize the depth and width of the network structure to reduce the impact on road damage image detection performance. Second, the Ghost module replaces the traditional convolution to reduce the number of model parameters, making the model lightweight and improving the detection rate. Finally, the Space-to-depth-Conv module is introduced to adapt to low-resolution and small object detection tasks. Numerous experiments on datasets such as Road Damage Dataset 2022 demonstrate that the improved model’s average accuracy increased by 1.1% compared to the original model, FPS increased from 85 to 90, and the parameter quantity decreased by 21.7%. It effectively alleviates problems in recognizing small targets. Compared to existing algorithms, it has significant advantages in road damage detection and classification.},
  archive      = {J_JRTIP},
  author       = {Xiang, Wanni and Wang, Haichen and Xu, Yuan and Zhao, Yixuan and Zhang, Luning and Duan, Yali},
  doi          = {10.1007/s11554-023-01312-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Road disease detection algorithm based on YOLOv5s-DSG},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: A novel finetuned YOLOv6 transfer learning
model for real‑time object detection. <em>JRTIP</em>, <em>20</em>(3), 1.
(<a href="https://doi.org/10.1007/s11554-023-01313-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Gupta, Chhaya and Gill, Nasib Singh and Gulia, Preeti and Chatterjee, Jyotir Moy},
  doi          = {10.1007/s11554-023-01313-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: A novel finetuned YOLOv6 transfer learning model for real‑time object detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving image encoding quality with a low-complexity DCT
approximation using 14 additions. <em>JRTIP</em>, <em>20</em>(3), 1–13.
(<a href="https://doi.org/10.1007/s11554-023-01315-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of images is crucial in image and video compression, especially for resource-constrained systems that prioritize simplicity. To achieve fast and low-energy compression, such systems aim to strike a balance between image quality and computational complexity. While various Discrete Cosine Transform (DCT) approximations have been proposed, only two approximations with 14 additions are currently available. This paper presents a novel 8-point DCT approximation that improves image quality compared to the previous 14-addition transformations. Additionally, a pruned version is derived and shown to be efficient. The proposed approximation achieves an average quality gain of up to 1 dB while maintaining a similar computational structure to the previous transformations, resulting in comparable energy consumption. Therefore, this solution provides a compelling option for resource-constrained systems seeking efficient image compression while preserving high image quality.},
  archive      = {J_JRTIP},
  author       = {Mefoued, Abdelkader and Kouadria, Nasreddine and Harize, Saliha and Doghmane, Noureddine},
  doi          = {10.1007/s11554-023-01315-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improving image encoding quality with a low-complexity DCT approximation using 14 additions},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards reduced dependency and faster unsupervised 3D face
reconstruction. <em>JRTIP</em>, <em>20</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01257-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent monocular 3D face reconstruction methods demonstrate performance improvement regarding 3D face geometry retrieval. However, these methods pose numerous challenges, particularly during testing. One of the significant challenges is the requirement of processed (cropped and aligned) input, which leads to the dependency on the facial landmark coordinates detector. Moreover, input processing time degrades the network’s testing speed, thus increasing the test time. Therefore, we propose a REduced Dependency Fast UnsuperviSEd 3D Face Reconstruction (RED-FUSE) framework, which exploits unprocessed (uncropped and unaligned) face images to estimate reliable 3D face shape and texture, waiving off the requirement for prior facial landmarks information, and improving the network’s estimation speed. More specifically, we utilize a (1) Multi-pipeline training architecture to reconstruct accurate 3D faces from challenging (transformed) unprocessed test inputs without posing additional requirements and (2) Pose transfer module that ensures reliable training for unprocessed challenging images by attaining the inter-pipeline face pose consistency without requiring the respective facial landmark information. We performed qualitative and quantitative analysis of our model on the unprocessed CelebA-test dataset, LFW-test set, NoW selfie challenge set and various open-source images. Our RED-FUSE outperforms a current method on the unprocessed CelebA-test dataset, e.g., for 3D shape-based, color-based, and 2D perceptual errors, the proposed method shows an improvement of $$\mathbf {46.2}\%$$ , $$\mathbf {15.1}\%$$ , and $$\mathbf {27.4}\%$$ , respectively. Moreover, our approach demonstrates a significant improvement of $$\mathbf {29.6}\%$$ on NoW selfie challenge. Furthermore, RED-FUSE requires lesser test time (a reduction from $$\mathbf {7.30}$$ m.sec. to $$\mathbf {1.85}$$ m.sec. per face) and poses minimal test time dependencies, demonstrating the effectiveness of the proposed method.},
  archive      = {J_JRTIP},
  author       = {Tiwari, Hitika and Subramanian, Venkatesh K. and Chen, Yong-Sheng},
  doi          = {10.1007/s11554-023-01257-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Towards reduced dependency and faster unsupervised 3D face reconstruction},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Driver fatigue detection based on comprehensive facial
features and gated recurrent unit. <em>JRTIP</em>, <em>20</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s11554-023-01260-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based driver fatigue detection algorithms have been increasingly used. However, traditional fatigue detection algorithms cannot effectively correlate contextual information of image frames. They perform better in individual image frames. Also, the accuracy and robustness of these algorithms are limited because they only consider particular frames. Therefore, a fatigue detection method based on integrated facial features and Gate Recurrent Unit (GRU) judgment neural network is proposed in this paper. We use a neural network including a GRU layer to efficiently distinguish the contextual information present in multiple image frames arranged in chronological order. Besides, we designed a multi-task convolutional neural network (MTCNN) model to extract comprehensive facial features. After obtaining the facial feature points’ positions, we can calculate the aspect ratio between the upper and lower eyelids, the upper and lower lips, and the eyebrows to the chin. In addition to the above three features, we can also obtain the subject&#39;s three head pose angles by comparing the facial features with the typical 3D face model. Finally, we input the change curves of 6 features in 20 consecutive frames into the judgment network to learn the change rule and create a judgment network. After the learning is completed, the judgment network model will judge the six feature curves in the newly input 20 frames in real-time and output the driver&#39;s fatigue status. This fatigue detection method can take a real-time detection at 55 FPS on the workstation platform (TensorFlow 2.3.0, RTX2070s). On the Nvidia Jetson Xavier AGX embedded platform (TensorFlow lite, ARM 8-cores CPU), the method can take a real-time detection at 26 FPS. The accuracy of this fatigue detection method can reach 97.47%.},
  archive      = {J_JRTIP},
  author       = {Li, Dan and Zhang, Xin and Liu, Xiaofan and Ma, Zhicheng and Zhang, Baolong},
  doi          = {10.1007/s11554-023-01260-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Driver fatigue detection based on comprehensive facial features and gated recurrent unit},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient 2D DCT architecture based on approximate
compressors for image compression with HEVC intra-prediction.
<em>JRTIP</em>, <em>20</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01261-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a design of two-dimensional (2D) discrete cosine transform (DCT) architecture to be used with high-efficiency video coding (HEVC) intra-prediction method in image compression. Since the amount of calculation required by the transform step in HEVC is high and accordingly the power consumption is high, a novel DCT architecture for HEVC is proposed to reduce this calculation complexity and power consumption. This architecture is based on erroneous calculations in the steps, which can be ignored in the quantizing step. For this purpose, approximate 5:3 compressor circuits with different error rates are designed and used instead of addition/subtraction in DCT architecture. This DCT architecture is designed to support 4 × 4, 8 × 8, 16 × 16 and 32 × 32 transform blocks. The designed architecture is performed on FPGA and experiments are conducted. In these experiments, hardware performance parameters are examined, and it is proved that the use of approximate compressor can provide advantages on power consumption and physical area. The efficiency of the proposed architecture is investigated by performing image compression and video coding tests.},
  archive      = {J_JRTIP},
  author       = {Akman, Ali and Cekli, Serap},
  doi          = {10.1007/s11554-023-01261-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient 2D DCT architecture based on approximate compressors for image compression with HEVC intra-prediction},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection method based on lightweight YOLOv4 and
attention mechanism in security scenes. <em>JRTIP</em>, <em>20</em>(2),
1–12. (<a href="https://doi.org/10.1007/s11554-023-01263-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection methods based on deep learning generally suffer from problems such as large size and complex structure, which lead to poor performance of mobile robots in security scenes. It must create a trade-off between speed and accuracy. To address the problem that current deep learning-based object detectors require too much computational resources, we propose a lightweight network model. The new detection model initially uses the modified lightweight network MobileNetV2-CA to replace the YOLOv4 backbone. It can greatly reduce the network complexity while guaranteeing capability. Then, we use the depthwise separable convolution (DSC) to replace the standard convolution in the neck and head part, and add Squeeze-and-Excitation (SE) module to apply the attention mechanism to the channel to avoid excessive consumption. Our method achieves 74.73 $$\%$$ mean average precision (mAP) on PASCAL VOC07+12 dataset and 34.51 $$\%$$ mAP on MS COCO dataset, and the number of model parameters is only 45.14 MB. The amount of parameters is only 18.38 $$\%$$ of the YOLOv4, which realizes a more lightweight detection model. Its detection speed on Titan X is 24.43 frames per second (FPS). Our method can also be compared to state-of-the-art detectors while being lighter.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Qian, Huaming and Zhou, Yipeng and Chu, Shuai},
  doi          = {10.1007/s11554-023-01263-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Object detection method based on lightweight YOLOv4 and attention mechanism in security scenes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time and effective detection of agricultural pest using
an improved YOLOv5 network. <em>JRTIP</em>, <em>20</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01264-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pest detection and control can effectively guarantee the quality and yield of crops. The existing CNN-based object detection methods provide feasible strategies for pest detection; however, their low accuracy and speed have restricted the deployment and application in actual agricultural scenarios. In this paper, an improved YOLOv5 model for real-time and effective agricultural pest detection is proposed. First, a lightweight feature extraction network GhostNet is adopted as the backbone, and an efficient channel attention mechanism is introduced to enhance feature extraction. Then, high-resolution feature maps are added to the bidirectional feature pyramid network to enhance the data flow path of the neck, which enriches semantic information and highlights small pests. Furthermore, to assign dynamic weights to features at different receptive fields and highlight the unequal contributions of different receptive fields to global information, feature fusion with attentional multi-receptive fields is proposed. Finally, the experimental results on a large-scale public pest dataset (Pest24) demonstrate that our method exceeds the original YOLOv5 and other state-of-the-art models. The mAP improves from 71.5 to 74.1%, the detection speed improves from 79.4 FPS to 104.2 FPS, the FLOPs decrease by 42% and the model size is compressed by 39%. The proposed method enables real-time and effective pest detection.},
  archive      = {J_JRTIP},
  author       = {Qi, Fang and Wang, Yuxiang and Tang, Zhe and Chen, Shuhong},
  doi          = {10.1007/s11554-023-01264-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and effective detection of agricultural pest using an improved YOLOv5 network},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TC-YOLOv5: Rapid detection of floating debris on raspberry
pi 4B. <em>JRTIP</em>, <em>20</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01265-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floating debris is a prominent indicator in measuring water quality. However, traditional object detection algorithms cannot meet the requirement of high accuracy due to the complexity of the environment. It is difficult for some deep learning-based object detection algorithms to achieve fast detection due to the limited performance of embedded devices. To address the above issues, this paper proposes TC-YOLOv5, which improves the detection accuracy by integrating the convolutional block attention module and vision transformer. To ensure the efficiency and low resource consumption of the algorithm, we selectively remove some convolutional layers and reduce some redundant calculations. We evaluated the performance of TC-YOLOv5 on a dataset with multiple species of floating debris, which can process an image in an average of 1.18 s on a Raspberry Pi 4B and achieve the mean average precision (mAP@0.5) of 84.2%. The detection accuracy, speed, and floating-point operations (FLOPs) of TC-YOLOv5 are better than some algorithms of the YOLOv5 series, such as YOLOv5n, YOLOv5s, and YOLOv5m. The above data show that TC-YOLOv5 realizes high-precision, low resource consumption, and rapid detection.},
  archive      = {J_JRTIP},
  author       = {Li, Shun and Liu, Shubo and Cai, Zhaohui and Liu, Yuan and Chen, Geng and Tu, Guoqing},
  doi          = {10.1007/s11554-023-01265-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {TC-YOLOv5: Rapid detection of floating debris on raspberry pi 4B},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance comparison of throughput between AVC, HEVC and
VVC hardware CABAC decoder. <em>JRTIP</em>, <em>20</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01266-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a performance comparison of throughput between context-based adaptive binary arithmetic decoding (CABAC) processes adopted in the three recent video codecs: advanced video coding (AVC), high efficiency video coding (HEVC), and versatile video coding (VVC). Consequently, in order to highlight the performance and the modification in three CABAC versions: the three main stages of CABAC decoding Context Selection and Modeling (CSM), Binary Arithmetic Decoding (BAD) and De-binarization (DBZ) are designed, described in VHDL language and implemented on Field Programmable Gate Array (FPGA) device. Firstly, the most efficient CSM is obtained for CABAC VVC with maximum frequency of 183.8 MHz and low power consumption of 0.346 mW. Secondly, the BAD in RM is modified only in the last video standard VVC. The most efficient design of BAD RM is given in the AVC and HEVC version of CABAC with maximum frequency of 261.75 MHz. Thirdly, the BAD in BM and TM are the same adopted in the three CABAC version, with maximum frequencies of 439.657 MHz and 798.861 MHz, respectively. Thirdly, the de-binarization codes are also the same adopted in the three last CABAC versions. Consequently, high frequency of 789.26 MHz is obtained in DBZ but the resources cost and power consumption are greater than that given in CSM and BAD stages. Finally, high throughput of 178.13 bins/s is given by our proposed design of VVC CABAC decoder.},
  archive      = {J_JRTIP},
  author       = {Menasri, Wahiba and Skoudarli, Abdellah},
  doi          = {10.1007/s11554-023-01266-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance comparison of throughput between AVC, HEVC and VVC hardware CABAC decoder},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient convolutional neural networks on raspberry pi for
image classification. <em>JRTIP</em>, <em>20</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s11554-023-01271-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the good performance of deep learning in the field of computer vision (CV), the convolutional neural network (CNN) architectures have become main backbones of image recognition tasks. With the widespread use of mobile devices, neural network models based on platforms with low computing power are gradually being paid attention. However, due to the limitation of computing power, deep learning algorithms are usually not available on mobile devices. This paper proposes a lightweight convolutional neural network TripleNet, which can operate easily on Raspberry Pi. Adopted from the concept of block connections in ThreshNet, the newly proposed network model compresses and accelerates the network model, reduces the amount of parameters of the network, and shortens the inference time of each image while ensuring the accuracy. Our proposed TripleNet and other State-of-the-Art (SOTA) neural networks perform image classification experiments with the CIFAR-10 and SVHN datasets on Raspberry Pi. The experimental results show that, compared with GhostNet, MobileNet, ThreshNet, EfficientNet, and HarDNet, the inference time of TripleNet per image is shortened by 15%, 16%, 17%, 24%, and 30%, respectively. The detail codes of this work are available at https://github.com/RuiyangJu/TripleNet .},
  archive      = {J_JRTIP},
  author       = {Ju, Rui-Yang and Lin, Ting-Yu and Jian, Jia-Hao and Chiang, Jen-Shiun},
  doi          = {10.1007/s11554-023-01271-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient convolutional neural networks on raspberry pi for image classification},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSP-based parallel optimization for real-time video
stitching. <em>JRTIP</em>, <em>20</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01275-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video stitching is a technique that stitches multiple overlapped videos acquired from different cameras, which is widely applied in many applications, including video surveillance, autonomous driving, and virtual reality. Feature-based stitching methods are popular in this area because of their invariance property and efficiency. However, the video stitching pipeline is relatively complicated and the amount of data calculation is large, which impedes its real-time applications. In this paper, we propose a real-time video stitching framework based on vision Digital Signal Processing (DSP). Real-time processing is achieved by the algorithm-level and system-level optimizations. In the algorithm of ORB feature extraction, methods, including look-up table, linear approximation, and single instruction multiple data (SIMD), are adopted to optimize its computation on DSP. In the algorithm of feature matching, the Hamming distance calculation is eliminated for some feature point pairs when their coordinates and angles are not satisfied with certain conditions. Reverse feature matching is proposed to improve registration accuracy. In system-level optimization, the directed acyclic graph (DAG)-based scheduling is proposed to improve the calculation efficiency on dual DSPs, and a ping-pong buffer is utilized to speed up the data transmission between DSP and external memory. Experimental results show that the proposed method can achieve a ten times speedup than that of the CPU, and it can achieve 1536 $$\times$$ 1024@37fps real-time processing on vision DSP.},
  archive      = {J_JRTIP},
  author       = {Huang, Xiaofeng and Tang, Ran and Zhou, Yang and Yin, Haibing and Yan, Chenggang},
  doi          = {10.1007/s11554-023-01275-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DSP-based parallel optimization for real-time video stitching},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconfigurable color medical image encryptor using hardware
accelerated chao(s)-box triplets. <em>JRTIP</em>, <em>20</em>(2), 1–26.
(<a href="https://doi.org/10.1007/s11554-023-01278-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In telemedicine applications, the security of digitized medical images plays a vital role globally. Field Programmable Gate Array (FPGA)-based implementations have many benefits for real-time security applications, such as being able to be changed, working simultaneously, being easy to prototype, and getting to market faster. In this work, hardware-accelerated three-tier architecture to encrypt color medical images under the Digital Imaging and Communication in Medicine (DICOM) modality has been realized on a Cyclone IV EP4CE115F29C7 FPGA. The proposed design achieves three-tiered security through the substitution, diffusion, and permutation processes with concurrent hardware implementation on an FPGA to attain performance benefits. The substitution block utilizes an enhanced S-box constructed using a Zhongtong chaotic system. The diffusion block uses the random sequences generated from a canonical memristor and Rossler’s attractor as a dual key. The permutation process is completed by the keys generated by cellular automata (CA) Rules 90 and 150. The key feature of this security architecture is that it is designed as a concurrent approach for RGB medical images; here, plane-level concurrency is achieved, thereby increasing the throughput. Further, on-the-fly confusion eliminates the unwanted memory requirement. The security strength of the proposed encryption scheme has been evaluated through various metrics such as entropy, correlation, histogram, peak signal-to-noise ratio (PSNR), and keyspace analysis. The synthesis results ensure efficient implementation on FPGA hardware with fewer logic elements (2212) and minimal power dissipation (131.40 mW) to encrypt a 256 256 RGB DICOM image with 8-bit resolution. Furthermore, the S-box’s randomness has been validated using NIST SP 800 and its 22 batteries. Compared with earlier encryption schemes, the proposed work outperforms them in statistical and hardware analyses.},
  archive      = {J_JRTIP},
  author       = {Raj, Vinoth and Janakiraman, Siva and Amirtharajan, Rengarajan},
  doi          = {10.1007/s11554-023-01278-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-26},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reconfigurable color medical image encryptor using hardware accelerated Chao(S)-box triplets},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D3NET (divide and detect drivable area net): Deep learning
based drivable area detection and its embedded application.
<em>JRTIP</em>, <em>20</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01279-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivable area detection is an important component of various levels of autonomous driving starting from advanced driver assistance systems (ADAS) to fully automated vehicles. A drivable area detection system detects the road segment in front of the vehicle for it to drive freely and safely. Using LIght Detection And Ranging (LIDAR) or cameras, these systems need to identify areas free of vehicles, pedestrians and other objects constituting as obstacles for the vehicles movement. As such areas can vary from asphalt to dirt road with or without lane markings and with many obstacle configurations, learning-based approaches have provided effective algorithms using large training data. While accuracy is of high importance, training and runtime complexity of these methods also matter. In this work, we propose a deep learning-based method that detects the drivable area from a single image providing comparable performance with improved training and runtime performance. The model splits the given image in thin slices which are processed by a simple convolutional network regressor to model the drivable with a single parameter. The experiments on benchmark data shows comparable accuracy against the literature while showing improvement in runtime performance. It shows 237 fps operating speed and 92.55% detection performance on a Titan XP GPU while providing similar detection performance at above 30 fps on a low cost Jetson Nano module. Our code is available at https://github.com/Acuno41/D3NET .},
  archive      = {J_JRTIP},
  author       = {Acun, Onur and Küçükmanisa, Ayhan and Genç, Yakup and Urhan, Oğuzhan},
  doi          = {10.1007/s11554-023-01279-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {D3NET (divide and detect drivable area net): Deep learning based drivable area detection and its embedded application},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel parametric based time efficient portable real-time
dehazing system. <em>JRTIP</em>, <em>20</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01283-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research and development on dehazing algorithms have come a long way and the current algorithms work very efficiently in generating clear dehazed images, restoring the images whose contrast gets impaired due to presence of aerosols in the atmosphere. However these algorithms do not work well when applied to dehaze video sequences of hazy scenes because of the time taken to do so, making them unsuitable in real time applications. In this paper, a real-time video dehazing technique has been proposed with a novel haze parameter ‘SATVAL’ which is the ratio of maximum saturation to maximum value of a RGB image applied on image scattering model using a few video frames processing in a second. A frame with a ’SATVAL’ ratio below threshold value is considered to be dehazed or else passed without dehazing. This makes a dehazed video sequence perform accurately in real-time comparable to other contemporary methods. A portable “Raspberry pi model 4B” is used for validation video-on-board or a remote server displaying on a LCD screen. Extensive experimental studies have been carried out to test the effectiveness of the method both at hardware and software levels in comparisons with four existing methods qualitatively and quantitatively. MSE, SSIM, Correlation, PSNR, FPS are the evaluating parameters showing promising output with high quality video in real-time. Finally ten video datasets have been developed for successful implementation of this method.},
  archive      = {J_JRTIP},
  author       = {Ghosh, Avra and Ali, Asfak and Roy, Sangita and Chaudhuri, Sheli Sinha},
  doi          = {10.1007/s11554-023-01283-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Novel parametric based time efficient portable real-time dehazing system},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shuffle-octave-yolo: A tradeoff object detection method for
embedded devices. <em>JRTIP</em>, <em>20</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01284-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying real-time, accurate and efficient object detection algorithms on embedded devices is the basis for robots to perceive the environment. However, it remains a huge challenge to achieve trade-off between detection speed and accuracy on embedded devices due to the limited computational resources. To solve this problem, this paper proposes a fusion mode with “interaction + integration” on the basis of enriching the limited features, and designs a tradeoff object detection method for embedded devices called shuffle-octave-yolo. First, channel shuffle is integrated into the feature extraction module of yolov4-tiny to extract richer features within a limited computation budget. Then, octave convolution (OctConv) that replaced average-pooling with max pooling is employed as the junction of features from the adjacent scales to construct a feature pyramid network based on OctConv (Oct-FPN). Combined with $$1 \times 1$$ pointwise convolution, a feature fusion module with two stages is built in Oct-FPN to conduct efficient feature fusion including interaction and integration. An adjustable fusion ratio is introduced to search the best tradeoff point. Finally, a three-scale prediction structure is adopted to improve the detection performance of small objects. Experimental results on the PASCAL VOC dataset show that shuffle-octave-yolo achieves a mAP (mean average precision) of 65.97% with 30.9 fps (frames per second) on Nvidia Jetson TX2 when fusion ratio is 3:7. It improves accuracy by 11.70% compared to the lightweight network on average, and 5.8 $$\times$$ speedup compared to the original algorithm. The model achieves outstanding trade-off between speed and accuracy on embedded devices.},
  archive      = {J_JRTIP},
  author       = {Chen, Jinwen and Zhang, Xiaoli and Peng, Xiafu and Xu, Dongsheng and Wu, Dongjie and Xin, Rui},
  doi          = {10.1007/s11554-023-01284-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Shuffle-octave-yolo: A tradeoff object detection method for embedded devices},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-material blind beam hardening correction in near
real-time based on non-linearity adjustment of projections.
<em>JRTIP</em>, <em>20</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01285-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beam hardening (BH) is one of the major artifacts that severely reduces the quality of computed tomography (CT) imaging. This BH artifact arises due to the polychromatic nature of the X-ray source and causes cupping and streak artifacts. This work aims to propose a fast and accurate BH correction method that requires no prior knowledge of the materials and corrects first and higher-order BH artifacts. This is achieved by performing a wide sweep of the material based on an experimentally measured look-up table to obtain the closest estimate of the material. Then, the non-linearity effect of the BH is corrected by adding the difference between the estimated monochromatic and the polychromatic simulated projections of the segmented image. The estimated polychromatic projection is accurately derived using the least square estimation (LSE) method by minimizing the difference between the experimental projection and the linear combination of simulated polychromatic projections. As a result, an accurate non-linearity correction term is derived that leads to an accurate BH correction result. The simulated projections in this work are performed using a multi-GPU-accelerated forward projection model which ensures a fast BH correction in near real-time. To evaluate the proposed BH correction method, we have conducted extensive experiments on real-world CT data. It is shown that the proposed method results in images with improved contrast-to-noise ratio (CNR) in comparison to the images corrected from only the scatter artifacts and the BH-corrected images using the state-of-the-art empirical BH correction method.},
  archive      = {J_JRTIP},
  author       = {Alsaffar, Ammar and Sun, Kaicong and Simon, Sven},
  doi          = {10.1007/s11554-023-01285-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-material blind beam hardening correction in near real-time based on non-linearity adjustment of projections},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LCDnet: A lightweight crowd density estimation model for
real-time video surveillance. <em>JRTIP</em>, <em>20</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01286-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic crowd counting using density estimation has gained significant attention in computer vision research. As a result, a large number of crowd counting and density estimation models using convolution neural networks (CNN) have been published in the last few years. These models have achieved good accuracy over benchmark datasets. However, attempts to improve the accuracy often lead to higher complexity in these models. In real-time video surveillance applications using drones with limited computing resources, deep models incur intolerable higher inference delay. In this paper, we propose (i) a Lightweight Crowd Density estimation model (LCDnet) for real-time video surveillance, and (ii) an improved training method using curriculum learning (CL). LCDnet is trained using CL and evaluated over two benchmark datasets i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our evaluation shows that the LCDnet achieves a reasonably good accuracy while significantly reducing the inference time and memory requirement and thus can be deployed over edge devices with very limited computing resources.},
  archive      = {J_JRTIP},
  author       = {Khan, Muhammad Asif and Menouar, Hamid and Hamila, Ridha},
  doi          = {10.1007/s11554-023-01286-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LCDnet: A lightweight crowd density estimation model for real-time video surveillance},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved YOLOX for pedestrian detection in crowded scenes.
<em>JRTIP</em>, <em>20</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01287-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object detection in computer vision has developed rapidly. However, crowded pedestrian detection in object detection remains a challenging problem, especially in one-stage detectors where improved solutions are rare. In this paper, we propose a novel crowded pedestrian detection method called YOLO-CPD which works better than other one-stage models in crowded environments. Our method primarily enhances the ability of the one-stage detector to detect multiple overlapping objects in a single area. The core of our approach is to use boxes difference to adjust the IoU value of the Non-Maximum Suppression (NMS) and to improve the Intersection over Union regression loss (IoU Loss), with an Optimised Score Module (OPSC). Compared to the baseline, YOLO-CPD can improve the Average Precision (AP) by a 5.04% increase, Recall by a 2.17% increase and the log-average Miss Rate ( $$MR^{-2}$$ ) by a 5.12% reduction on the CrowdHuman dataset. In addition, YOLO-CPD also achieved good results in the WiderPerson dataset, demonstrating the strong generalisation capability of our proposed method.},
  archive      = {J_JRTIP},
  author       = {Gao, Fei and Cai, Changxin and Jia, Ruohui and Hu, Xinzhong},
  doi          = {10.1007/s11554-023-01287-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved YOLOX for pedestrian detection in crowded scenes},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optical flow algorithms optimized for speed, energy and
accuracy on embedded GPUs. <em>JRTIP</em>, <em>20</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01288-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded computer vision is a hot field of research that requires trade-offs in order to balance execution time, power consumption and accuracy. In that field, dense optical flow estimation is a major tool used in many applications. Many algorithms have been designed, focusing on accuracy, very few works address trade-offs and implementation on embedded hardware. This paper tackles these trade-offs for embedded GPU through the example of the well-known TV-L $$^{1}$$ algorithm. Thanks to High Level Transforms—operator fusion and pipeline—and taking into account the iterative aspect of these algorithms, we achieve a speedup of $$\times \, 3.7$$ versus OpenCV. Moreover, we show that a 16-bit half precision implementation has a higher accuracy than the 32-bit precision one for the same frame processing time on NVIDIA Jetson boards. Furthermore, this work can be generalized to any kind of iterative stencil-based algorithms.},
  archive      = {J_JRTIP},
  author       = {Romera, Thomas and Petreto, Andrea and Lemaitre, Florian and Bouyer, Manuel and Meunier, Quentin and Lacassagne, Lionel and Etiemble, Daniel},
  doi          = {10.1007/s11554-023-01288-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Optical flow algorithms optimized for speed, energy and accuracy on embedded GPUs},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure, efficient and super-fast chaos-based image
encryption algorithm for real-time applications. <em>JRTIP</em>,
<em>20</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11554-023-01289-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable and ever-increasing use of digital images has turned the necessity of image encryption into an important issue, which has attracted many researchers. Unfortunately, fast encryption schemes for large color images are not well developed and studied. In this study, a secure, efficient and super-fast algorithm is proposed which is compatible with real-time applications. In this algorithm, three Logistic maps are utilized to generate initial values for the Chen system. To initialize three Logistic maps and Chen system parameters, a SHA-512 secret key is provided. Efficient use with fewer runs of chaotic maps has resulted in faster encryption/decryption process as well as less required memory. Moreover, a SHA-512 secret key, which can provide a wide key space, beside using multiple and high-dimensional chaotic maps will bring more security for the proposed algorithm. The simulation results confirm the efficiency and security of the proposed algorithm, while the average speed for tested images is more than 245 Mbps and all the security tests are passed successfully. For Mandrill color image $$512\times 512$$ , the proposed algorithm results in an entropy of 7.999428, a NPCR value of 99.6241, UACI value of 33.4741 and encryption time of 0.0343 s. To compare the obtained results with recently published works from the literature, a comparative study is provided to confirm the significant performance of the proposed method in comparison with the other studied schemes.},
  archive      = {J_JRTIP},
  author       = {Rezaei, Babak and Mobasseri, Mahvash and Enayatifar, Rasul},
  doi          = {10.1007/s11554-023-01289-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A secure, efficient and super-fast chaos-based image encryption algorithm for real-time applications},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamLight: Lightweight networks for object tracking via
attention mechanisms and pixel-level cross-correlation. <em>JRTIP</em>,
<em>20</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01291-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite Siamese-based trackers have achieved great success in recent years, researchers have focused more on the accuracy of trackers than their complexity, which leads to their inapplicability in some scenarios, and the real-time speed can be greatly limited. In this work, we propose a lightweight network method called SiamLight for object tracking. MobileNet-V3 is selected as the backbone network. The PG-corr module is added as the feature fusion module, a strategy that decomposes the template feature into spatial and channel kernels, reducing the matching regions and suppressing the effect of similar interference. In addition, we also add the CSM module, which carries out attention to the channel and spatial simultaneously. CSM module not only reduces the number of parameters but also ensures that it can be integrated into existing network architectures as a plug-and-play module. Finally, multiple separable convolution blocks are added to the classification and regression branches to meet our lightweight parameters and Flops requirements. The experiments on LaSOT, VOT2018, VOT2019, OTB100, and UAV123 benchmarks show that the method has fewer Flops and parameters than state-of-the-art trackers.},
  archive      = {J_JRTIP},
  author       = {Lin, Yu-e and Li, Mengfan and Liang, Xingzhu and Xia, Chenxing},
  doi          = {10.1007/s11554-023-01291-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SiamLight: Lightweight networks for object tracking via attention mechanisms and pixel-level cross-correlation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight algorithm based on YOLOv5 for relative
position detection of hydraulic support at coal mining faces.
<em>JRTIP</em>, <em>20</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01292-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the existing problems that the relative position detection method of hydraulic support in comprehensive coal mining faces, such as complex detection equipment, has poor flexibility and maintainability, we propose a method of hydraulic support relative position detection in coal mining faces based on object detection Lightweight-Ghost-YOLO (LG-YOLO). To better deploy the deep learning model on end-side devices, the GhostNet convolution module and the Ghost residual module are integrated into the YOLOv5s network to reduce the number of parameters and the occupancy of computing resources. Additionally, the Parametric Rectified Linear Unit (PReLU) activation function is integrated to achieve faster inference speed. In the postprocessing stage, Distance Intersection over Union Nonmaximum Suppression (DIoU-NMS) is used to improve the detection accuracy of closely aligned targets. The network model is further compressed by channel pruning and knowledge distillation. Experiments show that compared with YOLOv5s, the proposed algorithm can effectively detect the status of hydraulic supports. Finally, the model size is reduced by 73% and the computational amount is reduced by 69%, which can meet the requirements of end-side device deployment and real-time detection.},
  archive      = {J_JRTIP},
  author       = {Pan, Lihu and Duan, Yuxuan and Zhang, Yingjun and Xie, Binhong and Zhang, Rui},
  doi          = {10.1007/s11554-023-01292-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight algorithm based on YOLOv5 for relative position detection of hydraulic support at coal mining faces},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time fire and flame detection method for electric
vehicle charging station based on machine vision. <em>JRTIP</em>,
<em>20</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01293-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the charging process of electric vehicle (EV), high voltage and high current charging methods are widely used to reduce charging time, resulting in severe battery heating and an increased risk of fire. To improve fire detection efficiency, this paper proposes a real-time fire and smoke detection method for EV charging station based on Machine Vision. The algorithm introduces the Kmeans +  + algorithm in the GhostNet-YOLOv4 model to rescreen anchor boxes for fire smoke targets to optimize the classification quality for the complex and variable features of targets; and introduces the coordinate attention (CA) module after the lightweight backbone network GhostNet to improve the classification quality. In this paper, we use EV charging station monitoring video as a model detection input source to achieve real-time detection of multiple pairs of sites. The experimental results demonstrate that the improved algorithm has a model parameter number of 11.436 M, a mAP value of 87.70%, and a video detection FPS value of 75, which has a good continuous target tracking capability and satisfies the demand for real-time monitoring and is crucial for the safe operation of EV charging station and the emergency extinguishing of fire.},
  archive      = {J_JRTIP},
  author       = {Gao, Dexin and Zhang, Shiyu and Ju, Yifan and Yang, Qing},
  doi          = {10.1007/s11554-023-01293-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time fire and flame detection method for electric vehicle charging station based on machine vision},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time image encryption algorithm based on combined
chaotic map and optimized lifting wavelet transform. <em>JRTIP</em>,
<em>20</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-023-01294-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multimedia technology, the security of image information becomes more and more important. Encryption is a way to protect image information. Chaotic systems are often combined with other algorithms to encrypt images because of their cryptographic properties such as initial value sensitivity and randomness. At present, the common image encryption systems are combined with the traditional scrambling diffusion algorithm, but the number of rounds of scrambling diffusion affects the security and real-time performance at the same time, which makes it impossible to have both. The lifting wavelet transform algorithm is simple, occupies less memory and has high execution efficiency. Based on this, this paper proposes a real-time image encryption algorithm based on a combined chaotic map and optimized lifting wavelet transform to improve the security and efficiency of the system. Instead of using a conventional chaotic system, we propose a new combined chaotic map, which has better randomness and more complex chaotic behavior than a one-dimensional seeded chaotic map. And the speed of image encryption is also faster than that of high-dimensional chaotic maps. At the same time, a novel lifting wavelet transform algorithm is adopted and optimized to reduce the correlation of the data and speed up image processing. The security of the new system is also guaranteed by hashing its key. The experimental results, security analysis and comparison with existing methods all confirm that the proposed algorithm has good performance, high security and complex chaotic behavior.},
  archive      = {J_JRTIP},
  author       = {Mao, Ning and Tong, Xiaojun and Zhang, Miao and Wang, Zhu},
  doi          = {10.1007/s11554-023-01294-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time image encryption algorithm based on combined chaotic map and optimized lifting wavelet transform},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time and accurate defect segmentation of aluminum strip
surface via a lightweight network. <em>JRTIP</em>, <em>20</em>(2), 1–11.
(<a href="https://doi.org/10.1007/s11554-023-01295-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the premise of ensuring the defect segmentation precision of aluminum strip surfaces, improving the segmentation speed to meet the real-time requirements of the production line is an important task. Therefore, a lightweight and efficient network is proposed for the defect segmentation of aluminum strip surfaces. In the network, the lightweight GhostNet with the proposed dilation attention mechanism embedded is used for multi-scale feature extraction. This mechanism focuses more on the critical space and channel features and obtains a large receptive field in the spatial dimension. The Ghost module-based lightweight fusion node is constructed and embedded into the bidirectional feature pyramid network (BiFPN) for more efficient integration of multi-scale features. In addition, a novel lightweight boundary refinement (LBR) block designed as a residual structure is suggested to improve the localization ability near the defect boundaries. The aluminum strip surface dataset with five kinds of common defects is created and adopted to train and test the networks. The evaluation results demonstrate that the mean intersection over union (mIoU) of the proposed network is 85.51%, the speed is 68.86 fps, and the model volume is 9.38 MB. In summary, the proposed network gets a good trade-off between defect segmentation speed and accuracy for aluminum strip surfaces, which provides the potential for real-time segmentation on embedded systems.},
  archive      = {J_JRTIP},
  author       = {Lv, Zehua and Li, Yibo and Qian, Siying},
  doi          = {10.1007/s11554-023-01295-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and accurate defect segmentation of aluminum strip surface via a lightweight network},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view knowledge distillation for efficient semantic
segmentation. <em>JRTIP</em>, <em>20</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01296-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art semantic segmentation models achieve remarkable success in segmentation accuracy. However, the huge model size and computing cost restrict their applications on low-latency online systems or devices. Knowledge distillation has been one popular solution for compressing large-scale segmentation models, which train a small segmentation model from a large teacher model. However, one teacher model’s knowledge may be insufficiently diverse to train an accurate student model. Meanwhile, the student model may inherit bias from the teacher model. This paper proposes a multi-view knowledge distillation framework called MVKD for efficient semantic segmentation. MVKD could aggregate the multi-view knowledge from multiple teacher models and transfer the multi-view knowledge to the student model. In MVKD, we introduce one multi-view co-tuning strategy to acquire uniformity among the multi-view knowledge in features from different teachers. In addition, we propose a multi-view feature distillation loss and a multi-view output distillation loss to transfer the multi-view knowledge in the features and outputs from multiple teachers to the student. We evaluate the proposed MVKD on three benchmark datasets, Cityscapes, CamVid, and Pascal VOC 2012. Experimental results demonstrate the effectiveness of the proposed MVKD in compressing semantic segmentation models.},
  archive      = {J_JRTIP},
  author       = {Wang, Chen and Zhong, Jiang and Dai, Qizhu and Qi, Yafei and Shi, Fengyuan and Fang, Bin and Li, Xue},
  doi          = {10.1007/s11554-023-01296-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-view knowledge distillation for efficient semantic segmentation},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully densely linked and strongly correlated instance
segmentation for street-level images. <em>JRTIP</em>, <em>20</em>(2),
1–13. (<a href="https://doi.org/10.1007/s11554-023-01297-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation of street-level images is an important part of road perception for autonomous vehicles. Different from traditional indoor or outdoor scenes, the segmentation of street-level images needs to ensure the segmentation speed and improve the accuracy as much as possible. The deep learning-based street-level images’ segmentation can achieve high accuracy. However, the complexity and speed of the model are difficult to accept for the development and implementation of autonomous driving system. It is a challenge to design neural networks with higher accuracy while maintaining processing speed. To address this issue, we analyze the advantages and disadvantages of Polar-Mask algorithm, and propose a Polar-Mask-based fully densely linked and strongly correlated instance segmentation network (FCSIS-Polar) which achieves a good balance between precision and speed. Specifically, the original cascaded convolutional layers in Polar-Mask are densely connected to enhance the multi-scale feature extraction. In addition, the category features are co-encoded with the original mask prediction results as a priori information to establish a contour-category correlation. We test our algorithm on the challenging Cityscapes dataset (resolution of 1028 × 728). Experimental results confirm its performance; even in small images, it can achieve 26.4% Mask-mAP segmentation accuracy, which is 2.1% higher than the original model and the mean FPS (Frames Per Second) is 14.2. The proposed algorithm significantly improves the Mask-mAP while ensuring the segmentation speed, which is suitable for the environment perception of vehicles at low speed. At the same time, it can be used in auxiliary driving functions such as automatic parking and collision warning.},
  archive      = {J_JRTIP},
  author       = {Wang, Hao and Shi, Ying and Xie, Changjun and Lin, Chaojun and Hou, Hui and Hua, Jie},
  doi          = {10.1007/s11554-023-01297-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fully densely linked and strongly correlated instance segmentation for street-level images},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShortYOLO-CSP: A decisive incremental improvement for
real-time vehicle detection. <em>JRTIP</em>, <em>20</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01256-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods, particularly in the field of vehicle detection, have significantly increased in recent years. It plays an important role in detecting vehicles. In the case of autonomous driving and traffic control systems, accurate vehicle detection is necessary. To achieve accuracy, the network model&#39;s depth is increased, resulting in increased computational complexity and slow data processing. During severe weather, the majority of detection systems fail. Therefore, this research develops a new approach called ShortYOLO-CSP to enhance the detection of vehicles in challenging environments based on LittleYOLO-SPP. The depth of baseline network layers is re-designed, and Cross-Stage Partial (CSP) connection technique is used to reduce computational cost. The concatenation of the Spatial Pyramid Pooling (SPP) blocks improves the learning abilities of the model by combining spatial characteristics.  To accelerate the training process, Complete Intersection over union (CIoU) loss function for bounding box regression was adopted. The proposed model not only recognizes tiny cars, but also improves the accuracy of detection in a variety of weather situations, according to extensive testing on the combined datasets of PASCAL VOC 2007, 2012, MS COCO 2014, and Indian Vehicle datasets. Additionally, this framework significantly outperforms the LittleYOLO-SPP by increasing mean average precision (mAP) by almost 10%. PASCAL VOC and COCO achieved state-of-the-art outcomes, with mAPs 91.32% and 63.27%, respectively. Furthermore, the suggested strategy also has a considerable impact on accuracy and cuts down on inference time.},
  archive      = {J_JRTIP},
  author       = {Rani, P. Esther and Jamiya, S. Sri},
  doi          = {10.1007/s11554-023-01256-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ShortYOLO-CSP: A decisive incremental improvement for real-time vehicle detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FESSD: SSD target detection based on feature fusion and
feature enhancement. <em>JRTIP</em>, <em>20</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01258-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant breakthroughs have been made in target detection. However, although the existing two-stage target detection algorithm has high precision, the detection velocity is slow to content the real-time requirements. One-stage target detection algorithms can meet real-time requirements but have poor detection capabilities, especially for detecting the small target. In this paper, we propose an end-to-end feature fusion and feature enhancement SSD (FESSD) target detection algorithm to increase the capability of one-stage target detection. Firstly, a deeper ResNet-50 is used to replace VGG16 as the backbone network to obtain richer semantic information. Five extra layers are added to generate feature maps of different sizes for multi-scale target detection. Then, the feature maps are fused by the maximum pooling feature fusion module (MPFFM) and upsampling feature fusion module (UPFFM) to generate a new feature pyramid, which introduces semantic information into the shallow feature mapping. Finally, the feature enhancement module (FEM) is used to expand the receptive field of the output feature map, introduce more context information, and further enhance the feature expression ability of the model. Experimental results on the PASCAL VOC and MS COCO datasets validated the method’s validity.},
  archive      = {J_JRTIP},
  author       = {Qian, Huaming and Wang, Huilin and Feng, Shuai and Yan, Shuya},
  doi          = {10.1007/s11554-023-01258-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FESSD: SSD target detection based on feature fusion and feature enhancement},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepPilot4Pose: A fast pose localisation for MAV indoor
flight using the OAK-d camera. <em>JRTIP</em>, <em>20</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01259-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DeepPilot4Pose, a compact convolutional neural network for visual pose estimation that runs onboard novel smart camera, the OAK-D. We aim at using it for micro aerial vehicle (MAV) localisation, which flies in an indoor environment, where neither GPS nor external sensors are available. This calls for onboard processing, which demands a combination of software and hardware that could run efficiently onboard the MAV. To this end, we exploit the use of this novel sensor that can be carried by the MAV, the OAK-D camera, capable of performing neural inference on its chip in addition to providing colour, monochromatic and depth images. We show that our DeepPilot4Pose can run efficiently on the OAK-D at $$65\,{\text {Hz}}$$ with a localisation performance comparable to that obtained with RGB-D ORB-SLAM using the OAK-D and running onboard the MAV on the Intel Compute Stick at $$12 \,{\text {Hz}}$$ . We have evaluated our approach with benchmark datasets and in real MAV flights in an indoor facility with a challenging visual appearance.},
  archive      = {J_JRTIP},
  author       = {Rojas-Perez, L. Oyuki and Martinez-Carranza, Jose},
  doi          = {10.1007/s11554-023-01259-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DeepPilot4Pose: A fast pose localisation for MAV indoor flight using the OAK-D camera},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VLSI architecture and implementation of HDR camera signal
processor. <em>JRTIP</em>, <em>20</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01262-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The camera imaging built with high dynamic range (HDR) techniques can effectively improve the quality of images and so increase the recognition rate for computer vision systems. This paper presents the parallel architecture with a pipelined schedule to realize a real-time HDR processor based on a high-performance algorithm. With the hardware-oriented design, the processing kernel employs a near approach to reduce the computational circuit. The full HDR chip is realized with the module-by-module design and simulation. The main modules include the inverse module, the dark enhancement circuit, the parameter statistics circuit, the picture type judgment circuit, the mixing circuit and the bright enhancement circuit. Finally, these modules are combined with the pipelined schedule to realize a high-speed HDR core. In total, the latency time of the circuit is 4 line-buffer length added 14 clocks. This circuit is mapping to one FPGA (Field Programmable Gate Array) chip to verify its performance. The results demonstrate that the operation frequency can achieve to near 120 MHz, and data throughput rate is 360 M bytes per second. This chip can output one RGB (Red, Green, Blue) pixel per cycle, which can meet the requirement of high-resolution HDR camera performance.},
  archive      = {J_JRTIP},
  author       = {Hsia, Shih-Chang and Wang, Szu-Hong and Kuo, Ting-Tseng},
  doi          = {10.1007/s11554-023-01262-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {VLSI architecture and implementation of HDR camera signal processor},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time and effective text detection method for
multi-scale and fuzzy text. <em>JRTIP</em>, <em>20</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11554-023-01267-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The text in the natural scene can be in various forms, dynamic blur and geometric perspective greatly affect the efficiency of text detection. Given the above situation, a real-time and effective text detection method is proposed to detect the multi-scale and fuzzy text. This method applies a convolutional attention mechanism to the feature extraction backbone to obtain more valuable text feature maps. To fully utilize the precise text location signals of the low-level features, a bottom-up path augmentation is used simultaneously. Besides, a few layers of the Resnet-50 backbone are cancelled to further shorten information communication path for balancing the speed and accuracy of detection. For text detection results, the four vertex coordinate values of the text boxes are regressed with the assistance of CIoU loss and shrinkage of text labels. Our model can effectively process an image in the fastest time of 112 ms and has a higher comprehensive indicator value than the other comparative models in ICDAR 2013, ICDAR 2015, and MSRA-TD500 datasets.},
  archive      = {J_JRTIP},
  author       = {Tong, Guoxiang and Dong, Ming and Song, Yan},
  doi          = {10.1007/s11554-023-01267-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and effective text detection method for multi-scale and fuzzy text},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time detection algorithm of helmet and reflective vest
based on improved YOLOv5. <em>JRTIP</em>, <em>20</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01268-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the current problems of helmet and reflective vest detection algorithms, such as long inference times, high hardware requirements, and difficulty detecting small objects, an improved real-time detection algorithm based on YOLOv5 is proposed. First, the Mosaic-9 data enhancement method is adopted to expand the number of image splicings from four to nine, enriching the small-scale sample dataset. Minimum rectangular area cropping is used instead of the original random cropping to improve the training efficiency. Then, the EIOU (Efficient Intersection Over Union) loss is used as the loss function, and the focal loss is introduced to focus on the high-quality anchor box, achieving the purpose of accelerating the convergence speed and improving regression accuracy. Next, the CBAM-CA (Convolutional Block Attention Module—Coordinate Attention) hybrid attention mechanism is added to the network model. The purpose of the model is to focus on the small object’s spatial and channel feature information to improve the small object’s detection performance. Finally, a new scale object detection layer is added to learn the multilevel features of the dense object. The experimental results show that the improved model’s mAP (mean average precision) is 94.9%, an improvement of 7.0% over the original model. The size of the improved model is 4.49 MB, and the mean FPS (Frames Per Second) is 62. This algorithm guarantees a small model size while significantly increasing mAP, which is suitable for deployment on low computing power platforms.},
  archive      = {J_JRTIP},
  author       = {Chen, Zhihua and Zhang, Fan and Liu, Hongbo and Wang, Longxuan and Zhang, Qian and Guo, Liulu},
  doi          = {10.1007/s11554-023-01268-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection algorithm of helmet and reflective vest based on improved YOLOv5},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image perceptual hashing for content authentication based on
watson’s visual model and LLE. <em>JRTIP</em>, <em>20</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11554-023-01269-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image perceptual hashing has been widely used in image content authentication. In order to extract hashing sequences that are more consistent with the subjective feeling of human’s eyes, better express the nonlinear relationship and internal structure of the original image, and more effectively distinguish the copy image from similar images, an image hashing algorithm is proposed based on Watson’s Visual Model and LLE for the content-based image authentication. First, images are prepossessed to decrease the effects of noise, interpolation and different image sizes. Then weights of DCT co-efficients of non-overlapping image blocks are adjusted by Watson’s Visual Model, and the Hu invariant moment of each image block is combined to generate an intermediate feature matrix, which is scrambled by chaotic encryption. Third, LLE is performed on the intermediate feature matrix to generate a compact hash. Finally, the compact hash is encrypted again with random numbers and quantified into 0 and 1 sequences to attain the final hash. Experimental results illustrate that when the threshold T = 0.20, the true positive rate for similar images stands at 0.9994, while the false positive rate of different images is merely 0.0017, with the total error rate reaching the least value (0.0023). Furthermore, the AUC value of the proposed algorithm is 0.999995, which is higher than that of the comparison algorithms, indicating that the algorithm has better performance than other state-of-the-art algorithms in terms of various content-preserving attacks.},
  archive      = {J_JRTIP},
  author       = {Xing, Huifen and Che, Hui and Wu, Qilin and Wang, Honghai},
  doi          = {10.1007/s11554-023-01269-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Image perceptual hashing for content authentication based on watson’s visual model and LLE},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end deep learning approach for real-time single
image dehazing. <em>JRTIP</em>, <em>20</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01270-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing methods can restore clean images from hazy images and are popularly used as a preprocessing step to improve performance in various image analysis tasks. In recent times, deep learning-based methods have been used to sharply increase the visual quality of restored images, but they require a long computation time. The processing time of image-dehazing methods is one of the important factors to be considered in order not to affect the latency of the main image analysis tasks such as detection and segmentation. We propose an end-to-end network model for real-time image dehazing. We devised a zoomed convolution group that processes computation-intensive operations with low resolution to decrease the processing time of the network model without performance degradation. Additionally, the zoomed convolution group adopts an efficient channel attention module to improve the performance of the network model. Thus, we designed a network model using a zoomed convolution group to progressively recover haze-free images using a coarse-to-fine strategy. By adjusting the sampling ratio and the number of convolution blocks that make up the convolution group, we distributed small and large computational complexities respectively in the early and later operational stages. The experimental results with the proposed method on a public dataset showed a real-time performance comparable to that of another state-of-the-art (SOTA) method. The proposed network’s peak-signal-to-noise ratio was 0.8 dB lower than that of the SOTA method, but the processing speed was 10.4 times faster.},
  archive      = {J_JRTIP},
  author       = {Jeong, Chi Yoon and Moon, KyeongDeok and Kim, Mooseop},
  doi          = {10.1007/s11554-023-01270-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An end-to-end deep learning approach for real-time single image dehazing},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast segmentation algorithm of PCB image using 2D OTSU
improved by adaptive genetic algorithm and integral image.
<em>JRTIP</em>, <em>20</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-023-01272-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D OTSU achieves good image segmentation performance in thresholding-based segmentation tasks. However, for the real-time detection of printed circuit board (PCB) defects, this method is complicated and cannot meet the real-time requirements. In view of the above phenomenon, this paper proposes an improved 2D OTSU combining adaptive genetic algorithm and integral image algorithm. The adaptive genetic algorithm transforms the threshold selection of 2D OTSU into the optimization of an inter-class variance measure. The integral image algorithm reduces a lot of repeated calculations in the optimization process of an inter-class variance measure. Experimental results show that the proposed algorithm greatly reduces the amount of computation and time on the basis of ensuring the performance of PCB image segmentation. Under the condition of low contrast between line and background and uneven illumination, the proposed algorithm has better segmentation performance on PCB images.},
  archive      = {J_JRTIP},
  author       = {Ma, Jiaocheng and Cheng, Xiaodong},
  doi          = {10.1007/s11554-023-01272-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast segmentation algorithm of PCB image using 2D OTSU improved by adaptive genetic algorithm and integral image},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Butterfly network: A convolutional neural network with a new
architecture for multi-scale semantic segmentation of pedestrians.
<em>JRTIP</em>, <em>20</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01273-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of multi-scale pedestrians is one of the challenging tasks in pedestrian detection applications. Moreover, the task of small-scale pedestrian detection, i.e., accurate localization of pedestrians as low-scale target objects, can help solve the issue of occluded pedestrian detection as well. In this paper, we present a fully convolutional neural network with a new architecture and an innovative, fully detailed supervision for semantic segmentation of pedestrians. The proposed network has been named butterfly network (BF-Net) because of its architecture analogous to a butterfly. The proposed BF-Net preserves the ability of simplicity so that it can process static images with a real-time image processing rate. The sub-path blocks embedded in the architecture of the proposed BF-Net provides a higher accuracy for detecting multi-scale objective targets including the small ones. The other advantage of the proposed architecture is replacing common batch normalization with conditional one. In conclusion, the experimental results of the proposed method demonstrate that the proposed network outperform the other state-of-the-art networks such as U-Net +  + , U-Net3 + , Mask-RCNN, and Deeplabv3 + for the semantic segmentation of the pedestrians.},
  archive      = {J_JRTIP},
  author       = {Alavianmehr, M. A. and Helfroush, M. S. and Danyali, H. and Tashk, A.},
  doi          = {10.1007/s11554-023-01273-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Butterfly network: A convolutional neural network with a new architecture for multi-scale semantic segmentation of pedestrians},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new YOLO-based method for real-time crowd detection from
video and performance analysis of YOLO models. <em>JRTIP</em>,
<em>20</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-023-01276-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As seen in the COVID-19 pandemic, one of the most important measures is physical distance in viruses transmitted from person to person. According to the World Health Organization (WHO), it is mandatory to have a limited number of people in indoor spaces. Depending on the size of the indoors, the number of persons that can fit in that area varies. Then, the size of the indoor area should be measured and the maximum number of people should be calculated accordingly. Computers can be used to ensure the correct application of the capacity rule in indoors monitored by cameras. In this study, a method is proposed to measure the size of a prespecified region in the video and count the people there in real time. According to this method: (1) predetermining the borders of a region on the video, (2) identification and counting of people in this specified region, (3) it is aimed to estimate the size of the specified area and to find the maximum number of people it can take. For this purpose, the You Only Look Once (YOLO) object detection model was used. In addition, Microsoft COCO dataset pre-trained weights were used to identify and label persons. YOLO models were tested separately in the proposed method and their performances were analyzed. Mean average precision (mAP), frame per second (fps), and accuracy rate metrics were found for the detection of persons in the specified region. While the YOLO v3 model achieved the highest value in accuracy rate and mAP (both 0.50 and 0.75) metrics, the YOLO v5s model achieved the highest fps rate among non-Tiny models.},
  archive      = {J_JRTIP},
  author       = {Gündüz, Mehmet Şirin and Işık, Gültekin},
  doi          = {10.1007/s11554-023-01276-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new YOLO-based method for real-time crowd detection from video and performance analysis of YOLO models},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight multimodal feature graph convolutional network
for dangerous driving behavior detection. <em>JRTIP</em>,
<em>20</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01277-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection and identification of dangerous driving behaviors is an effective measure to reduce traffic accidents. Due to the high network delay, limited communication bandwidth, and weak computing power, lightweight detection models that can run on edge devices have been widely investigated and attracted considerable attention. In recent years, the Graph Convolutional Network (GCN), which models the human skeleton as a spatiotemporal graph, has achieved remarkable performance, due to its powerful capability of modeling non-Euclidean structured data. However, there are disadvantages such as the unitary way of extracting information, high model complexity, and inability to integrate environmental information. Therefore, we design a Lightweight Multimodal Feature Graph Convolutional Network (L-MFGCN) model for dangerous driving behavior detection video in an end-to-end manner. First, we propose a Multimodal Feature Graph Convolutional Neural Network (MF-GCN), which captures richer features by extracting critical local spatial and temporal information of joint points, and a multi-information fusion behavior recognition model of “people + objects” by capturing the motion information of related object. Then, the method based on Singular Value Decomposition (SVD) rank reduction is used to compress the model to improve the speed of recognizing an action sample while ensuring sufficient detection accuracy. The proposed model, respectively, achieves 96% and 86.3% accuracy on the x-view benchmark of NTU-RGBD dataset and the homemade Locomotive Driver Dataset, which attains the state-of-the-art performance.},
  archive      = {J_JRTIP},
  author       = {Wei, Xing and Yao, Shang and Zhao, Chong and Hu, Di and Luo, Hui and Lu, Yang},
  doi          = {10.1007/s11554-023-01277-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight multimodal feature graph convolutional network for dangerous driving behavior detection},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeoT: An end-to-end encoder-only transformer object
detector. <em>JRTIP</em>, <em>20</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11554-023-01280-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, with the rapid development of Transformer in object detection tasks, the object detection performance has been significantly improved. However, Transformer-based object detectors generally suffer from high complexity and slow learning convergence, and there is still a certain gap in performance compared to some convolutional neural network (CNN)-based object detectors. Therefore, to improve the existing problems of Transformer in object detection framework and make its detector performance reach the state-of-the-art level, this paper proposes an end-to-end encoder-only Transformer object detector, called DeoT. First, we design a feature pyramid fusion module (FPFM) to generate fusion features with rich semantic information. The proposal of the FPFM not only improves the detection accuracy of objects, but also solves the detection problem of objects of different sizes. Second, we propose an encoder-only Transformer module (E-OTM) to achieve a global representation of features by exploiting deformable multi-head self-attention (DMHSA). Furthermore, we design a Transformer block residual structure (TBRS) in the E-OTM, which refines the output features of the transformer module by using the channel attention and spatial attention in the channel refinement module (CRM) and spatial refinement module (SRM). The proposal of encoder-only Transformer module not only effectively alleviates the complexity and learning convergence problems of the model, but also improves the detection accuracy. We conduct sufficient experiments on the MS COCO object detection dataset and Cityscapes object detection dataset, and achieve 50.9 AP with 34 Epochs on the COCO 2017 tes-dev set, 30.1 AP with 38 FPS on the Cityscapes dataset. Therefore, DeoT not only achieves high efficiency in the training phase, but also ensures real time and accuracy in the detection process.},
  archive      = {J_JRTIP},
  author       = {Ding, Tonghe and Feng, Kaili and Wei, Yanjun and Han, Yu and Li, Tianping},
  doi          = {10.1007/s11554-023-01280-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DeoT: An end-to-end encoder-only transformer object detector},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved lightweight and real-time YOLOv5 network for
detection of surface defects on indocalamus leaves. <em>JRTIP</em>,
<em>20</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-023-01281-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indocalamus leaves are widely used in the Chinese food industry. Surface defect detection plays a crucial role in the post-harvest reprocessing of indocalamus leaves. In this study, we constructed a lightweight convolutional neural network model to detect surface defects on indocalamus leaves. We investigated four categories of surface defects, including damage, black spots, insect spots, and holes, to construct a dataset of surface defects on indocalamus leaves, which contained 4124 images for model training and evaluation. We replaced the original path aggregation network (PANet) in YOLOv5 with a cross-layer feature pyramid network (CFPN), which improved the detection performance by fusing feature maps at different levels. We proposed an improved feature fusion module, named the receptive dilated and deformable convolution field block (RDDCFB), which was integrated into the CFPN for learning within larger spatial and semantic contexts. Furthermore, a new CA mechanism was proposed to improve the feature representation capability of the network by appropriately adjusting the structure of the coordinate attention (CA) mechanism. Extensive experiments using the Pascal VOC and CIFAR-100 datasets demonstrated that this new CA block had superior accuracy and integration capabilities. On MSCOCO2017 validation datasets, experiments show that our module is consistently better than various detectors, including Faster R-CNN, YOLOv3, and YOLOv4. Finally, our quantitative results from the dataset of surface defects on indocalamus leaves indicated the effectiveness of the proposed method. The accuracy and recognition efficiency of the improved YOLOv5 model could reach 97.7% and 97 frames per second, respectively.},
  archive      = {J_JRTIP},
  author       = {Tang, Zhe and Zhou, Lin and Qi, Fang and Chen, Huarong},
  doi          = {10.1007/s11554-023-01281-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved lightweight and real-time YOLOv5 network for detection of surface defects on indocalamus leaves},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-cost low-power approximated VLSI architecture for
high-quality image scaling in mobile devices. <em>JRTIP</em>,
<em>20</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s11554-023-01282-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes three pipeline VLSI architectures for high-quality image scaling. The proposed architectures use low-complexity v-model spatial sharpening filter, modified edge detection, and simplified bilinear interpolation. Low complexity v-model spatial sharpening filter is obtained by modifying the 3 $$\times$$ 3 sharpening kernel to x-model and then to v-model. This sharpening filter enhances the center pixel intensity value relative to the nearest pixels. The modified edge detection technique needs less hardware to find the sharp change in the pixel values. The number of multipliers required for bilinear interpolation is reduced by performing algebraic manipulations. Matlab tool is used analyze the scaled image quality with Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) parameters. The proposed three VLSI architectures are described using Verilog HDL (hardware description language) and synthesized using Cadence genus compiler using GPDK 90 nm technology. One of the proposed architectures requires 8981 (µm2) area and consumes 1.90 (mW) power, which is less when compared to those of the existing architectures.},
  archive      = {J_JRTIP},
  author       = {Midde, Venkata Siva and Jayakumar, E. P.},
  doi          = {10.1007/s11554-023-01282-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-cost low-power approximated VLSI architecture for high-quality image scaling in mobile devices},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
