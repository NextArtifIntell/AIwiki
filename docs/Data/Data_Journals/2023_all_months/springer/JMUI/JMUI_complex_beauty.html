<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui---22">JMUI - 22</h2>
<ul>
<li><details>
<summary>
(2023). Perceptually congruent sonification of auditory line charts.
<em>JMUI</em>, <em>17</em>(4), 285–300. (<a
href="https://doi.org/10.1007/s12193-023-00413-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been increasingly more discussion about sonification design, evaluation strategies, and the future of sonification in general. Concerns relate to the efficacy and usability of sonification methodologies and the lack of evidence supporting design choices. In sonification publications to date, there is little discussion on how the principles of psychoacoustics or auditory scene analysis (ASA) might be used to improve design methodologies. Proposed in this article is the perceptually congruent sonification (PerCS) model that highlights the links between sonification mappings and key perceptual factors that are most likely to affect the interpretability of sonified trends. The model explores the perceptual constraints associated with design choices such as presentation rate, tonality, space, and frequency range. The model highlights the dependencies these mappings have on our perception of frequency, timbre, loudness, and space: the core determinants of stream segregation (a key phenomenon of ASA). This model-centred approach is used to identify perceptually congruent design choices for auditory line charts, a popular and accessible form of parameter-mapping sonification. This article focuses predominately on identifying an efficient and perceptually congruent frequency range for auditory line charts, as there are no contemporary studies on this design choice, with auditory graphing tools leaving little guidance on what might be appropriate. Several psychoacoustically justified ranges are proposed by the PerCS research and are contrasted in a user study to determine the performance of narrow vs wide frequency ranges. Results show that a shorter range between 400 and 1400 Hz performs just as well as a wider range between 65 Hz and 1480 Hz.},
  archive      = {J_JMUI},
  author       = {Fitzpatrick, Joe and Neff, Flaithri},
  doi          = {10.1007/s12193-023-00413-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {285-300},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Perceptually congruent sonification of auditory line charts},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interdisciplinary journey towards an aesthetics of
sonification experience. <em>JMUI</em>, <em>17</em>(4), 263–284. (<a
href="https://doi.org/10.1007/s12193-023-00416-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aesthetic dimension has been proposed as a potential expansion of sonification design, creating listening pieces that reach the goal of effective data communication. However, current views of aesthetics still aim at optimising mapping criteria to convey the ‘right meaning’, maintaining a mostly functional view on what is considered a successful sonification. This paper proposes an interdisciplinary approach to the aesthetics of sonification experience, grounded on theoretical foundations from phenomenology of interaction, post-phenomenology, cross-cultural studies, acoustic ecology and deep listening. From this journey, we present the following design insights: (1) the design of sonifications becomes a design for experience, (2) co-designed during the interaction with each participant; (3) the sonification artefact gains a mediating role that guides the participant’s intentions in the sonification space; (4) the aesthetics of a sonification experience generates a multistable phenomenon, offering new opportunities to experience multiple perspectives over data; (5) the interaction between human participants and the sonic emanations compose a dialogic space. A call for action to reframe the sonification field into novel design spaces is now open, with aesthetics gaining a transformational role in sonification design and interaction.},
  archive      = {J_JMUI},
  author       = {Seiça, Mariana and Roque, Licínio and Martins, Pedro and Cardoso, F. Amílcar},
  doi          = {10.1007/s12193-023-00416-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {263-284},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {An interdisciplinary journey towards an aesthetics of sonification experience},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-dimensional sonification as a surgical guidance tool.
<em>JMUI</em>, <em>17</em>(4), 253–262. (<a
href="https://doi.org/10.1007/s12193-023-00422-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive Sonification is a well-known guidance method in navigation tasks. Researchers have repeatedly suggested the use of interactive sonification in neuronavigation and image-guided surgery. The hope is to reduce clinicians’ cognitive load through a relief of the visual channel, while preserving the precision provided through image guidance. In this paper, we present a surgical use case, simulating a craniotomy preparation with a skull phantom. Through auditory, visual, and audiovisual guidance, non-clinicians successfully find targets on a skull that provides hardly any visual or haptic landmarks. The results show that interactive sonification enables novice users to navigate through three-dimensional space with a high precision. The precision along the depth axis is highest in the audiovisual guidance mode, but adding audio leads to higher durations and longer motion trajectories.},
  archive      = {J_JMUI},
  author       = {Ziemer, Tim},
  doi          = {10.1007/s12193-023-00422-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {253-262},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Three-dimensional sonification as a surgical guidance tool},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based sonification based on the impulse pattern
formulation. <em>JMUI</em>, <em>17</em>(4), 243–251. (<a
href="https://doi.org/10.1007/s12193-023-00423-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most common strategy for interactive sonification is parameter mapping sonification, where sensed or defined data is pre-processed and then used to control one or more variables in a signal processing chain. A well-known but rarely used alternative is model-based sonification, where data is fed into a physical or conceptual model that generates or modifies sound. In this paper, we suggest the Impulse Pattern Formulation (IPF) as a model-based sonification strategy. The IPF can model natural systems and interactions, like the sound production of musical instruments, the reverberation in rooms, and human synchronization to a rhythm. Hence, the IPF has the potential to be easy to interpret and intuitive to interact with. Experiment results show that the IPF is able to produce an intuitively interpretable, natural zero, i.e., a coordinate origin. Coordinate origins are necessary to sonify both polarities of a dimension as well as absolute magnitudes.},
  archive      = {J_JMUI},
  author       = {Linke, Simon and Bader, Rolf and Mores, Robert},
  doi          = {10.1007/s12193-023-00423-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {243-251},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Model-based sonification based on the impulse pattern formulation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: PepperOSC: Enabling interactive sonification
of a robot’s expressive movement. <em>JMUI</em>, <em>17</em>(4), 241.
(<a href="https://doi.org/10.1007/s12193-023-00424-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Latupeirissa, Adrian B. and Bresin, Roberto},
  doi          = {10.1007/s12193-023-00424-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {241},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Correction to: PepperOSC: enabling interactive sonification of a robot’s expressive movement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). PepperOSC: Enabling interactive sonification of a robot’s
expressive movement. <em>JMUI</em>, <em>17</em>(4), 231–239. (<a
href="https://doi.org/10.1007/s12193-023-00414-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design and development of PepperOSC, an interface that connects Pepper and NAO robots with sound production tools to enable the development of interactive sonification in human-robot interaction (HRI). The interface uses Open Sound Control (OSC) messages to stream kinematic data from robots to various sound design and music production tools. The goals of PepperOSC are twofold: (i) to provide a tool for HRI researchers in developing multimodal user interfaces through sonification, and (ii) to lower the barrier for sound designers to contribute to HRI. To demonstrate the potential use of PepperOSC, this paper also presents two applications we have conducted: (i) a course project by two master’s students who created a robot sound model in Pure Data, and (ii) a museum installation of Pepper robot, employing sound models developed by a sound designer and a composer/researcher in music technology using MaxMSP and SuperCollider respectively. Furthermore, we discuss the potential use cases of PepperOSC in social robotics and artistic contexts. These applications demonstrate the versatility of PepperOSC and its ability to explore diverse aesthetic strategies for robot movement sonification, offering a promising approach to enhance the effectiveness and appeal of human-robot interactions.},
  archive      = {J_JMUI},
  author       = {Latupeirissa, Adrian B. and Bresin, Roberto},
  doi          = {10.1007/s12193-023-00414-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {231-239},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {PepperOSC: Enabling interactive sonification of a robot’s expressive movement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-vehicle air gesture design: Impacts of display modality
and control orientation. <em>JMUI</em>, <em>17</em>(4), 215–230. (<a
href="https://doi.org/10.1007/s12193-023-00415-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of visual distraction-caused crashes highlights a need for non-visual displays in the in-vehicle information system (IVIS). Audio-supported air gesture controls can tackle this problem. Twenty-four young drivers participated in our experiment using a driving simulator with six different gesture prototypes—3 modality types (visual-only, visual/auditory, and auditory-only) × 2 control orientation types (horizontal and vertical). Various data were obtained, including lane departures, eye glance behavior, secondary task performance, and driver workload. Results showed that the auditory-only displays showed a significantly lower lane departures and perceived workload. A tradeoff between eyes-on-road time and secondary task completion time for the auditory-only display was also observed, which means the safest, but slowest among the prototypes. Vertical controls (direct manipulation) showed significantly lower workload than horizontal controls (mouse metaphor), but did not differ in performance measures. Experimental results are discussed in the context of multiple resource theory and design guidelines for future implementation.},
  archive      = {J_JMUI},
  author       = {Sterkenburg, Jason and Landry, Steven and FakhrHosseini, Shabnam and Jeon, Myounghoon},
  doi          = {10.1007/s12193-023-00415-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {215-230},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {In-vehicle air gesture design: Impacts of display modality and control orientation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introduction to the special issue on design and perception
of interactive sonification. <em>JMUI</em>, <em>17</em>(4), 213–214. (<a
href="https://doi.org/10.1007/s12193-023-00425-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Ziemer, Tim and Lenzi, Sara and Rönnberg, Niklas and Hermann, Thomas and Bresin, Roberto},
  doi          = {10.1007/s12193-023-00425-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {213-214},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Introduction to the special issue on design and perception of interactive sonification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on the application of gaze visualization interface
on virtual reality training systems. <em>JMUI</em>, <em>17</em>(3),
203–211. (<a href="https://doi.org/10.1007/s12193-023-00409-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although virtual reality (VR) training provides a realistic environment that offers an immersive experience for learners, it can affect the training progress by triggering a cognitive load or distracting learners from focusing on the main content of the training. In this study, the application of a gaze visualization interface to a medical safety training system was proposed to prevent learner distraction, which is an issue usually experienced in traditional VR training systems. An experiment was conducted to analyze the effects of applying the gaze visualization interface to a medical training system on the learning performance. Based on related studies, the following three metrics were selected to evaluate the learning performance: learning flow, achievement, and concentration. After the experiment, flow and achievement were evaluated through surveys and quizzes. These evaluation methods can also distort learners’ memories, making it difficult to analyze objective learning outcomes. Eye-tracking technology is useful for measuring learning performance in multimedia learning environments and can represent learner’s eye movements in objective metrics. Therefore, in this study, eye-tracking technology was used to evaluate concentration. The experimental results suggest that the gaze visualization interface had positive effects on learners’ concentration, learning flow, and achievement.},
  archive      = {J_JMUI},
  author       = {Choi, Haram and Kwon, Joungheum and Nam, Sanghun},
  doi          = {10.1007/s12193-023-00409-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {203-211},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Research on the application of gaze visualization interface on virtual reality training systems},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition via transfer learning in
cooperative game paradigms for enhanced social AI. <em>JMUI</em>,
<em>17</em>(3), 187–201. (<a
href="https://doi.org/10.1007/s12193-023-00410-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) is an effortless task for humans, and such non-verbal communication is intricately related to how we relate to others beyond the explicit content of our speech. Facial expressions can convey how we are feeling, as well as our intentions, and are thus a key point in multimodal social interactions. Recent computational advances, such as promising results from Convolutional Neural Networks (CNN), have drawn increasing attention to the potential of FER to enhance human–agent interaction (HAI) and human–robot interaction (HRI), but questions remain as to how “transferrable” the learned knowledge is from one task environment to another. In this paper, we explore how FER can be deployed in HAI cooperative game paradigms, where a human subject interacts with a virtual avatar in a goal-oriented environment where they must cooperate to survive. The primary question was whether transfer learning (TL) would offer an advantage for FER over pre-trained models based on similar (but the not exact same) task environment. The final results showed that TL was able to achieve significantly improved results (94.3% accuracy), without the need for an extensive task-specific corpus. We discuss how such approaches could be used to flexibly create more life-like robots and avatars, capable of fluid social interactions within cooperative multimodal environments.},
  archive      = {J_JMUI},
  author       = {Sánchez, Paula Castro and Bennett, Casey C.},
  doi          = {10.1007/s12193-023-00410-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {187-201},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Facial expression recognition via transfer learning in cooperative game paradigms for enhanced social AI},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring user-defined gestures for lingual and palatal
interaction. <em>JMUI</em>, <em>17</em>(3), 167–185. (<a
href="https://doi.org/10.1007/s12193-023-00408-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals with motor disabilities can benefit from an alternative means of interacting with the world: using their tongue. The tongue possesses precise movement capabilities within the mouth, allowing individuals to designate targets on the palate. This form of interaction, known as lingual interaction, enables users to perform basic functions by utilizing their tongues to indicate positions. The purpose of this work is to identify the lingual and palatal gestures proposed by end-users. In order to achieve this goal, our initial step was to examine relevant literature on the subject, including clinical studies on the motor capacity of the tongue, devices detecting the movement of the tongue, and current lingual interfaces (e.g.,  using a wheelchair). Then, we conducted a gesture elicitation study (GES) involving 24 ( $$N = 24$$ ) participants, who proposed lingual and palatal gestures to perform 19 Internet of Things (IoT) referents, thus obtaining a corpus of 456 gestures. These gestures were clustered into similarity classes (80 unique gestures) and analyzed by dimension, nature, complexity, thinking time, and goodness-of-fit. Using the Agreement Rate (Ar) methodology, we present a set of 16 gestures for a lingual and palatal interface, which serve as a basis for further comparison with gestures suggested by disabled people.},
  archive      = {J_JMUI},
  author       = {Villarreal-Narvaez, Santiago and Perez-Medina, Jorge Luis and Vanderdonckt, Jean},
  doi          = {10.1007/s12193-023-00408-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {167-185},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring user-defined gestures for lingual and palatal interaction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Understanding virtual drilling perception
using sound, and kinesthetic cues obtained with a mouse and keyboard.
<em>JMUI</em>, <em>17</em>(3), 165. (<a
href="https://doi.org/10.1007/s12193-023-00412-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Ning, Guoxuan and Grant, Brianna and Kapralos, Bill and Quevedo, Alvaro and Collins, KC and Kanev, Kamen and Dubrowski, Adam},
  doi          = {10.1007/s12193-023-00412-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {165},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Correction to: Understanding virtual drilling perception using sound, and kinesthetic cues obtained with a mouse and keyboard},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Understanding virtual drilling perception using sound, and
kinesthetic cues obtained with a mouse and keyboard. <em>JMUI</em>,
<em>17</em>(3), 151–163. (<a
href="https://doi.org/10.1007/s12193-023-00407-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work examined whether a virtual drilling task can be appropriately simulated using combinations of sound, and kinesthetic stimuli obtained with movement of a basic 2D mouse in the absence of a haptic device. An experiment was conducted where participants were asked to drill a virtual block of wood under different sound conditions using a standard computer mouse, or keyboard. Although preliminary and greater work remains, the results of these experiments indicate that kinesthetic cues can be used to provide a suitable perceptual drilling experience without a haptic device. This preliminary work has provided a greater understanding of virtual drilling perception using traditional computer interfaces in the absence of haptic devices. This is particularly important when considering remote learning where trainees may be able to practice various psychomotor-based skills at home with commonly available computer hardware and devices.},
  archive      = {J_JMUI},
  author       = {Ning, Guoxuan and Grant, Brianna and Kapralos, Bill and Quevedo, Alvaro and Collins, KC and Kanev, Kamen and Dubrowski, Adam},
  doi          = {10.1007/s12193-023-00407-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {151-163},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Understanding virtual drilling perception using sound, and kinesthetic cues obtained with a mouse and keyboard},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SonAir: The design of a sonification of radar data for air
traffic control. <em>JMUI</em>, <em>17</em>(3), 137–149. (<a
href="https://doi.org/10.1007/s12193-023-00404-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the increase of digitalization and automation, a new kind of working environment is emerging in the field of air traffic control. Instead of situating the control tower at the airport, it is now possible to remotely control the airport at any given location, i.e. in a remote tower center (RTC). However, by controlling the airport remotely, the situational awareness and sense of presence might be compromised. By using directional sound, a higher situational awareness could potentially be achieved while also offloading the visual perception which is heavily used in air traffic control. Suitable use cases for sonification in air traffic control were found through workshops with air traffic controllers. A sonification design named SonAir was developed based on the outcome of the workshops, and was integrated with an RTC simulator for evaluating to what degree SonAir could support air traffic controllers in their work. The results suggest that certain aspects of SonAir could be useful for air traffic controllers. A continuous sonification where the spatial positioning of aircraft were conveyed was experienced to be partially useful, but the intrusiveness of SonAir should be further considered to fit the air traffic controllers’ needs. An earcon that conveyed when an aircraft enters the airspace and from which direction was considered useful to support situational awareness.},
  archive      = {J_JMUI},
  author       = {Elmquist, Elias and Bock, Alexander and Lundberg, Jonas and Ynnerman, Anders and Rönnberg, Niklas},
  doi          = {10.1007/s12193-023-00404-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {137-149},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {SonAir: The design of a sonification of radar data for air traffic control},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low duration vibro-tactile representation of braille
characters. <em>JMUI</em>, <em>17</em>(3), 121–135. (<a
href="https://doi.org/10.1007/s12193-023-00405-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Braille is a tactile writing/reading system used by people who are visually impaired. Traditionally, Braille has been used on static surfaces like books, signs or keypads, refreshable Braille displays are also available to be used as an interface for digital devices. The linear Braille alphabet proposed in this paper is a novel method to represent Braille characters using the vibration engine of a mobile device operated via vibro-tactile signal sequences. The standard vibration engine is driven to generate different haptic effects on the mobile device in a predefined sequence. Two different tests for distinguishing distinct sequences and characters were conducted over a subject group of 50 clear sighted people. Result of this test show that; signal distinguishability of the method is 99%, and character distinguishability is 97%, while maximum duration for a single character is 1050 ms. A second test on 10 visually impaired users presented a signal distinguishability result of 90% and a character distinguishability result of 82.5%. The method proposed in this paper shows promising results for representing Braille characters on a standard mobile device.},
  archive      = {J_JMUI},
  author       = {Tamer, Özgür and Kirişken, Barbaros and Köklü, Tunca},
  doi          = {10.1007/s12193-023-00405-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {121-135},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A low duration vibro-tactile representation of braille characters},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The cognitive basis for virtual reality rehabilitation of
upper-extremity motor function after neurotraumas. <em>JMUI</em>,
<em>17</em>(3), 105–120. (<a
href="https://doi.org/10.1007/s12193-023-00406-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to present previous works in augmented sensory guidance for motor learning and psychophysiological factors and contextualize how these approaches may facilitate greater optimization of motor rehabilitation after neurotraumas with virtual reality. Through library resources at Stevens Institute of Technology, we searched for related works using multiple electronic databases and search engines with a medical focus (detailed in the paper). Searches were for articles published between 1980 and 2023 examining upper extremity rehabilitation, virtual reality, cognition, and modes and features of sensory feedback (specific search terms detailed in the paper). Strategic activation of sensory modalities for augmented guidance using virtual reality may improve motor training to develop further skill retention in persons suffering from impulsive neurological damage. Features with unique motor learning characteristics to consider with augmented feedback signals include representation, timing, complexity, and intermittency. Furthermore, monitoring psychophysiological factors (e.g., sense of agency, cognitive loading, attention) that represent mental and psychological processes may assist in critically evaluating novel designs in computerized rehabilitation. Virtual reality approaches should better incorporate augmented sensory feedback and leverage psychophysiological factors to advance motor rehabilitation after neurotraumas.},
  archive      = {J_JMUI},
  author       = {Dewil, Sophie and Kuptchik, Shterna and Liu, Mingxiao and Sanford, Sean and Bradbury, Troy and Davis, Elena and Clemente, Amanda and Nataraj, Raviraj},
  doi          = {10.1007/s12193-023-00406-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {105-120},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The cognitive basis for virtual reality rehabilitation of upper-extremity motor function after neurotraumas},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Remote social touch framework: A way to communicate physical
interactions across long distances. <em>JMUI</em>, <em>17</em>(2),
79–104. (<a href="https://doi.org/10.1007/s12193-023-00402-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, living away from loved ones is a common practice due to various reasons such as work, study, or certain health-related concerns (e.g., infection diseases). Such practice aggregates certain negative emotions such as depression due to the loss of physical, mental, and emotional awareness about loved ones. Because of the importance of social touch for one’s wellbeing, this paper reports the research focusing on remote social touch (RST), as a way to stimulate the sense of touch remotely to regain some of the lost awareness. The research identified various dimensions of RST and the process of communicating social touch remotely through a product. This is done through an extensive literature survey, online diary keeping, and interviews. The paper also presents the early proposed RST framework that consists of three elements (actors, product, and communication) and their dimensions, that explain the process of how RST communications can be successfully achieved through a product.},
  archive      = {J_JMUI},
  author       = {Alsamarei, Ali Abdulrazzaq and Bahar Şener},
  doi          = {10.1007/s12193-023-00402-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {79-104},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Remote social touch framework: A way to communicate physical interactions across long distances},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating the influence of agent modality and expression
on agent-mediated fairness behaviours. <em>JMUI</em>, <em>17</em>(2),
65–77. (<a href="https://doi.org/10.1007/s12193-023-00403-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological developments, individuals are increasingly able to delegate tasks to autonomous agents that act on their behalf. This may cause individuals to behave more fairly, as involving an agent representative encourages individuals to strategise ahead and therefore adhere to social norms of fairness. Research suggests that an audio smiling agent may further promote fairness as it provides a signal of honesty and trust. What is still unclear is whether presenting a multimodal smiling agent (by using visual and auditory cues) rather than a unimodal smiling agent as normally available commercially (using only an auditory cue e.g., Siri) could amplify the impact of smiles. In the present study, participants (N = 86) played an ultimatum game either directly with another player (control), through a smiling multimodal and unimodal agent or through a neutral multimodal and unimodal agent. Participants’ task was to offer a number of tickets to the other player from a fixed amount. Results showed that when playing the ultimatum game through a smiling multimodal agent, participants offered more tickets to the other player compared to the control condition and the other agent conditions. Hence, exploiting multisensory perception to enhance an agent’s expression may be key for increasing individuals&#39; pro-social behaviour when interacting through such an agent.},
  archive      = {J_JMUI},
  author       = {Yip, Hiu Lam and Petrini, Karin},
  doi          = {10.1007/s12193-023-00403-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {65-77},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Investigating the influence of agent modality and expression on agent-mediated fairness behaviours},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personality trait estimation in group discussions using
multimodal analysis and speaker embedding. <em>JMUI</em>,
<em>17</em>(2), 47–63. (<a
href="https://doi.org/10.1007/s12193-023-00401-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic estimation of personality traits is essential for many human–computer interface (HCI) applications. This paper focused on improving Big Five personality trait estimation in group discussions via multimodal analysis and transfer learning with the state-of-the-art speaker individuality feature, namely, the identity vector (i-vector) speaker embedding. The experiments were carried out by investigating the effective and robust multimodal features for estimation with two group discussion datasets, i.e., the Multimodal Task-Oriented Group Discussion (MATRICS) (in Japanese) and Emergent Leadership (ELEA) (in European languages) corpora. Subsequently, the evaluation was conducted by using leave-one-person-out cross-validation (LOPCV) and ablation tests to compare the effectiveness of each modality. The overall results showed that the speaker-dependent features, e.g., the i-vector, effectively improved the prediction accuracy of Big Five personality trait estimation. In addition, the experimental results showed that audio-related features were the most prominent features in both corpora.},
  archive      = {J_JMUI},
  author       = {Mawalim, Candy Olivia and Okada, Shogo and Nakano, Yukiko I. and Unoki, Masashi},
  doi          = {10.1007/s12193-023-00401-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {47-63},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Personality trait estimation in group discussions using multimodal analysis and speaker embedding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual reality can mediate the learning phase of upper limb
prostheses supporting a better-informed selection process.
<em>JMUI</em>, <em>17</em>(1), 33–46. (<a
href="https://doi.org/10.1007/s12193-022-00400-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of sophisticated bionic prostheses for upper limb amputees, the selection of the most appropriate device for each individual might become a challenge. Owing to the cost and complexity of such devices and the required time to learn their usage, it would be relevant to facilitate access to a wide range of them for intensive testing in ecological environments. In this paper, we report the development of a virtual reality setup that emulates three representative bionic prostheses and their key grasp selection methods. Twelve able-bodied participants and one transradial amputee participated in a study whose objective was to assess the influence of this setup in quantifying the perceived usability of the virtualised devices, and thus to guide a better-informed selection process of a preferred device based on the most suitable grip selection method for each individual. Results show that the user learning curve with the three simulated devices and their perceived usability was different, and that this had an influence on the final device selection. In particular, more than half of the users changed their favorite device after the experiment in virtual reality. In sum, this paper provides a proof of concept that virtual reality is a valuable tool to support a better-informed selection process of an upper limb myoelectric prosthesis that is personalized to individual preferences and capabilities by mediating its learning phase.},
  archive      = {J_JMUI},
  author       = {Raghibi, Lucas El and Muhoza, Ange Pascal and Evrard, Jeanne and Ghazi, Hugo and van Oldeneel tot Oldenzeel, Grégoire and Sonneville, Victorien and Macq, Benoît and Ronsse, Renaud},
  doi          = {10.1007/s12193-022-00400-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {33-46},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Virtual reality can mediate the learning phase of upper limb prostheses supporting a better-informed selection process},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effects of olfactory cues as interface notifications on
a mobile phone. <em>JMUI</em>, <em>17</em>(1), 21–32. (<a
href="https://doi.org/10.1007/s12193-022-00399-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Olfaction is one of the most important human senses that can help assist human perceptions, and the function of smell is also one of the most important ways that can help human interacting with the environment. Hence, the olfactory cues might become an important design tool to facilitate human-mobile phone interaction. This study therefore adopted olfactory cues as mobile phone prompts to help explore how they affect users’ interactive experience. The experiment of this study implemented a 2 (visual layout) × 3 (olfactory cue) between-subjects design by developing shopping application (app) prototypes. The independent variable of visual layout had two levels, i.e., list layout (LL) and square layout (SL). In addition, the experiment adopted an external device to emit odor to simulate different olfactory cues. The independent variable of olfactory cue had three levels, namely, matched olfactory cue (MOC), mismatched olfactory cue (MMOC), and no olfactory cue (NOC). A total of 72 participants were invited to take part in the experiment via the purposive sampling method. The generated results revealed that: (1) The increase in olfactory cues brought better operational pleasure to the participants; (2) Matching olfactory cues of recommended products (MOC) showed the best interface usability and enhanced the overall operating experience; (3) The user interface of the list layout showed better task operation efficiency than the square layout; (4) An increase in olfactory cues may lower the participants&#39; task efficiency, while no odor cue (NOC) showed the fastest task completion time; and (5) The olfactory cues of rose odor significantly increased the participants’ selection rate for the corresponding product. The olfactory cues have been confirmed in the study to be an important notification to help induce shopping behavior. It may also make online virtual shopping behavior more similar to actual shopping behavior, and improve the overall user experience as well.},
  archive      = {J_JMUI},
  author       = {Huang, Miao and Chen, Chien-Hsiung},
  doi          = {10.1007/s12193-022-00399-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {21-32},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The effects of olfactory cues as interface notifications on a mobile phone},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theory-based approach for assessing cognitive load during
time-critical resource-managing human–computer interactions: An
eye-tracking study. <em>JMUI</em>, <em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12193-022-00398-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computerized systems are taking on increasingly complex tasks. Consequently, monitoring automated computerized systems is becoming increasingly demanding for human operators, which is particularly relevant in time-critical situations. A possible solution might be adapting human–computer interfaces (HCI) to the operators’ cognitive load. Here, we present a novel approach for theory-based measurement of cognitive load based on tracking eye movements of 42 participants while playing a serious game simulating time-critical situations that required resource management at different levels of difficulty. Gaze data was collected within narrow time periods, calculated based on log data interpreted in the light of the time-based resource-sharing model. Our results indicated that eye fixation frequency, saccadic rate, and pupil diameter significantly predicted task difficulty, while performance was best predicted by eye fixation frequency. Subjectively perceived cognitive load was significantly associated with the rate of microsaccades. Moreover our results indicated that more successful players tended to use breaks in gameplay to actively monitor the scene, while players who use these times to rest are more likely to fail the level. The presented approach seems promising for measuring cognitive load in realistic situations, considering adaptation of HCI.},
  archive      = {J_JMUI},
  author       = {Sevcenko, Natalia and Appel, Tobias and Ninaus, Manuel and Moeller, Korbinian and Gerjets, Peter},
  doi          = {10.1007/s12193-022-00398-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Theory-based approach for assessing cognitive load during time-critical resource-managing human–computer interactions: An eye-tracking study},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
