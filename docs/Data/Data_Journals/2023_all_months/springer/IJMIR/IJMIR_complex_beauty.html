<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir---42">IJMIR - 42</h2>
<ul>
<li><details>
<summary>
(2023). Few-shot and meta-learning methods for image understanding:
A survey. <em>IJMIR</em>, <em>12</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s13735-023-00279-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art deep learning systems (e.g., ImageNet image classification) typically require very large training sets to achieve high accuracies. Therefore, one of the grand challenges is called few-shot learning where only a few training samples are required for good performance. In this survey, we illuminate one of the key paradigms in few-shot learning called meta-learning. These meta-learning methods, by simulating the tasks which will be presented at inference through episodic training, can effectively employ previous prior knowledge to guide the learning of new tasks. In this paper, we provide a comprehensive overview and key insights into the meta-learning approaches and categorize them into three branches according to their technical characteristics, namely metric-based, model-based and optimization-based meta-learning. Due to the major importance of the evaluation process, we also present an overview of current widely used benchmarks, as well as performances of recent meta-learning methods on these datasets. Based on over 200 papers in this survey, we conclude with the major challenges and future directions of few-shot learning and meta-learning.},
  archive      = {J_IJMIR},
  author       = {He, Kai and Pu, Nan and Lao, Mingrui and Lew, Michael S.},
  doi          = {10.1007/s13735-023-00279-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Few-shot and meta-learning methods for image understanding: A survey},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-guided temporal modeling for action recognition.
<em>IJMIR</em>, <em>12</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s13735-023-00280-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition is a video understanding task that is carried out to recognize an action of an object in a video. In order to recognize the action, it is necessary to extract motion information through temporal modeling. However, videos typically contain high temporal redundancy, such as iterative events and adjacent frames. This high temporal redundancy weakens information related to actual action, making it difficult for the final classifier to recognize the action. In this article, we focus on preserving helpful information for action recognition by reducing the high temporal redundancy in videos. To achieve this goal, we propose a novel frame selection method called cluster-guided frame selection (CluFrame). Specifically, CluFrame compresses an input video into keyframes of clusters discovered by applying $$k$$ -means clustering to frame-wise features extracted from pre-trained 2D-CNNs in the temporal compression (TC) module. In addition, CluFrame selects keyframes related to the action of the input video by optimizing the TC module based on the action recognition results. Experimental results on five benchmark datasets demonstrate that CluFrame addresses the high temporal redundancy in the video and achieves action recognition accuracy improvement over existing action recognition methods by up to 6.6% and by about 0.7% compared to state-of-the-art frame selection methods.},
  archive      = {J_IJMIR},
  author       = {Kim, Jeong-Hun and Hao, Fei and Leung, Carson Kai-Sang and Nasridinov, Aziz},
  doi          = {10.1007/s13735-023-00280-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Cluster-guided temporal modeling for action recognition},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision fusion for few-shot image classification.
<em>IJMIR</em>, <em>12</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s13735-023-00281-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent few-shot learning methods mostly only use a single classifier to complete image classification. In general, a single classifier is likely to be overfitting because of its inherent drawbacks. However, the recognition rate of categorization will be significantly increased if we can utilize the complementary information of different classifiers. For the few-shot problem, the test samples come from new classes, which makes it difficult for a single classifier to distinguish, and it can be improved via decision fusion. In this paper, we propose decision fusion for few-shot learning (DF-FSL) to overcome the drawbacks of single classifier. To be specific, we assign the task to two classifiers, which are the logical regression classifier and probabilistic collaborative representation-based classifier (ProCRC), then allow the two classifiers to learn together through several iterations. Finally, we evaluate our approach on four benchmark image datasets, which include CIFAR-FS, CUB, miniImageNet and tieredImageNet datasets, and two remote sensing image datasets which are RSD46-WHU and NWPU-RESIS45. The experimental results illustrate the complementarity between different classifiers and show that the performance of our proposed DF-FSL method provides an obvious improvement. And DF-FSL can make great progress in few-shot remote sensing image classification.},
  archive      = {J_IJMIR},
  author       = {Yuan, Tianhao and Liu, Weifeng and Yan, Fei and Liu, Baodi},
  doi          = {10.1007/s13735-023-00281-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Decision fusion for few-shot image classification},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical bidirectional aggregation with prior guided
transformer for few-shot segmentation. <em>IJMIR</em>, <em>12</em>(2),
1–14. (<a href="https://doi.org/10.1007/s13735-023-00282-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed significant interest in few-shot segmentation methods, with the aim of predicting novel categories in a query image given the limited labeled support set. Despite demonstrated successes, some existing methods might suffer from the intra-class inconsistency between query and support samples for local unidirectional information guidance. We propose a hierarchical bidirectional aggregation with prior guided transformer for abundant intra-class common cues. Specifically, we adaptively aggregate support and query features by a non-local bidirectional information flow in a hierarchical manner to derive a closer and deeper correlation. We further introduce the prior affinity map to impart inductive bias and eliminate interfering semantics. Experimental results on three benchmark datasets demonstrate that the proposed method surpasses some previous state-of-the-art approaches well, especially performing favorably in handling challenging situations under 1-shot setting.},
  archive      = {J_IJMIR},
  author       = {Kong, Qiuyu and Jiang, Jie and Yang, Junyan and Wang, Qi},
  doi          = {10.1007/s13735-023-00282-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Hierarchical bidirectional aggregation with prior guided transformer for few-shot segmentation},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-feature collaborative relation-attention networks for
visual question answering. <em>IJMIR</em>, <em>12</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s13735-023-00283-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region and grid features extracted by object detection networks, which contain abundant image information, are widely used in visual question answering (VQA). The regions focus on object-level features, but the grids are better at representing contextual information and fine-grained attributes of images. However, most of the existing VQA models process visual information with one-way attention, failing to capture the internal relations between objects and analyze the feature details. In this work, we propose a novel multi-level collaborative decoder (MLCD) layer based on the encoder–decoder framework to address this issue, which incorporates visual location vectors into attention. Specifically, each MLCD is equipped with three different attention-MLP sub-modules to progressively and accurately mine the intrinsic interactions of features and enhance the influence of image content on prediction results. Additionally, to fully exploit the respective advantages of two features, we propose a novel relativity-augmented cross-attention (RACA) unit and add it to MLCD, in which the features after simple attention are complementarily augmented using global information and self-attributes. To validate the proposed methods, we stack the MLCD layer deeply to constitute our dual-feature collaborative relation-attention network (DFCRAN). We conduct extensive experiments and visualize the results on three benchmark datasets, including COCO-QA, VQA 1.0, and VQA 2.0, to prove the effectiveness of our model and achieve competitive performances compared to the state-of-the-art single models without pre-training.},
  archive      = {J_IJMIR},
  author       = {Yao, Lu and Yang, You and Hu, Juntao},
  doi          = {10.1007/s13735-023-00283-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Dual-feature collaborative relation-attention networks for visual question answering},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recognition of student engagement in classroom from
affective states. <em>IJMIR</em>, <em>12</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s13735-023-00284-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Student engagement is positively related to comprehension in teaching–learning process. Student engagement is widely studied in online learning environments, whereas this research focuses on student engagement recognition in classroom environments using visual cues. To incorporate learning-centered affective states, we curated a dataset with six learning-centered affective states from four public datasets. A graph convolution network (GCN)-based deep learning model with attention was designed and implemented to extract more contributing features from input video for student engagement recognition. The proposed architecture was evaluated on curated as well as four public datasets. An ablation study was conducted on a curated dataset, the best performing model with minority oversampling and focal cross-entropy loss achieved 65.35% accuracy. We also estimated the student engagement in authentic classroom data, and it showed a positive correlation between students’ engagement levels and post-lesson test scores with a Pearson’s coefficient value of 0.64. The proposed method outperformed the existing state-of-the-art methods on two of the public datasets with accuracy scores of 99.20% and 56.17%, and it achieved accuracy scores of 64.92% and 56.17% on other two public datasets which are better than many baseline results on them.},
  archive      = {J_IJMIR},
  author       = {Mandia, Sandeep and Singh, Kuldeep and Mitharwal, Rajendra},
  doi          = {10.1007/s13735-023-00284-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Recognition of student engagement in classroom from affective states},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural style transfer generative adversarial network
(NST-GAN) for facial expression recognition. <em>IJMIR</em>,
<em>12</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s13735-023-00285-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing number of intelligent human–computer systems, more and more research is focusing on human emotion recognition. Facial expressions are an effective modality in emotional recognition, enhancing automatic emotional analysis. Although significant studies have investigated automatic facial expression recognition in the past decades, previous works were mainly produced for controlled environments. Unlike recent pure CNN-based works, we argue that it is practical and feasible to recognize an expression from a facial image. However, the extracted features may capture more identity-related information and are not purely associated with the specific task of expression recognition. To reduce the personal influence of identity-related features by removing identity information from facial images, we propose a neural style transfer generative adversarial network (NST-GAN) in this paper. The objective is to determine the expression information from the input image by removing identity information and transferring it to a synthetic identity. We employ experimental strategies to evaluate the proposed method on three public facial expression databases (CK+, FER-2013, and JAFFE). Extensive experiments prove that our NST-GAN outperforms other methods, setting a new state of the art.},
  archive      = {J_IJMIR},
  author       = {Khemakhem, Faten and Ltifi, Hela},
  doi          = {10.1007/s13735-023-00285-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Neural style transfer generative adversarial network (NST-GAN) for facial expression recognition},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoCoOpter: Pre-train, prompt, and fine-tune the
vision-language model for few-shot image classification. <em>IJMIR</em>,
<em>12</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00286-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification aims at learning to generalize to unseen new categories from a few training samples. Transfer learning is one prominent approach to the task, which first learns a backbone from the base classes and then trains a classifier on new classes with the prior learned knowledge. Typically, the convolutional neural network (CNN) is the preferred backbone. However, when the samples are limited, the representation ability of the feature extracted by CNN will decrease, thus leading to the performance degradation of few-shot image classification. Recently, the pre-trained large-scale vision-language model like CLIP has shown non-trivial potential, which can be used as a backbone for zero or few-shot transfer on a series of downstream tasks with the prompt. To fully explore the few-shot image classification performance of vision-language models, we propose CoCoOpter, a novel “pre-training + prompt tuning + fine-tuning” paradigm based on CLIP. CoCoOpter alleviates the overfitting and ensures generalizability in unseen new categories. Specifically, it learns an input-specific neural network to relieve overfitting by drawing attention away from a specific category to each specific input sample. Then, to establish connection between the visual and textual signals, it introduces the previously learned visual representations to perform automatic prompt tuning in the middle of the pre-trained CLIP, enabling learning input-specified prompt vectors. Moreover, two learnable lightweight neural networks are added at the end of CLIP to guide information propagation between different classes by fine-tuning both the visual and textual features. We perform extensive experiments on 11 image classification datasets. The results show that CoCoOpter is more generalizable in unseen classes and achieves superior few-shot classification performance with a straightforward design.},
  archive      = {J_IJMIR},
  author       = {Yan, Jie and Xie, Yuxiang and Guo, Yanming and Wei, Yingmei and Zhang, Xiaoping and Luan, Xidao},
  doi          = {10.1007/s13735-023-00286-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {CoCoOpter: Pre-train, prompt, and fine-tune the vision-language model for few-shot image classification},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modal interaction-enhanced prompt learning by transformer
decoder for vision-language models. <em>IJMIR</em>, <em>12</em>(2),
1–10. (<a href="https://doi.org/10.1007/s13735-023-00287-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current multimodal retrieval field, CoOp is the preferred approach among many models due to its simplicity and powerful adaptive capability. However, CoOp focuses primarily on optimizing prompts to perform contrast learning, without considering image-text interactions and the impact on the model when visual information is incorporated into the prompts. In this work, we propose a prompt tuning method for simulating image-text interaction based on CoOp: Decoding context optimization (DeCoOp). Through extensive experiments on 11 image classification datasets, seven datasets under the few-shot setting and all 11 datasets under the zero-shot setting are ahead of CoOp in our method. Experiments on four target datasets of ImageNet show a model performance improvement of more than 10%, demonstrating that our approach substantially outperforms the baseline model CoOp in terms of point domain generalization and robustness. In addition, ablation experiments performed on three representative datasets confirmed the effectiveness and further improvement of the accuracy of DeCoOp. Finally, experiments are performed on 11 datasets using different visual backbones, and it is not difficult to find that the gap between our approach and handcrafted prompts is large in all architectures and shows better performance than CoOp.},
  archive      = {J_IJMIR},
  author       = {Liu, Mingyue and Zhao, Honggang and Ma, Longfei and Li, Mingyong},
  doi          = {10.1007/s13735-023-00287-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Modal interaction-enhanced prompt learning by transformer decoder for vision-language models},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep adversarial multi-label cross-modal hashing algorithm.
<em>IJMIR</em>, <em>12</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s13735-023-00288-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, more and more researchers employ the hashing algorithm to improve the large-scale cross-modal retrieval efficiency by mapping the floating-point feature into the compact binary code. However, the cross-modal hashing algorithm usually computes the similarity relationship based on single class labels, while ignoring the multi-label information. To solve the above problem, we propose the deep adversarial multi-label cross-modal hashing algorithm (DAMCH) which takes both multi-label and deep feature into consideration during establishing the cross-modal neighbor matrix. Firstly, we propose the inter- and intra-modal neighbor relationship preserving function to make the Hamming neighbor relationship be consistent with the original neighbor relationship. Secondly, we design linear classification functions to learn binary features’ semantic labels and establish the hash semantic preserving loss function to guarantee the binary features have the same semantic information as the original label. Furthermore, we establish the intra-modal adversarial loss function to minimize the information loss during mapping the floating-point feature into the compact binary code, and propose the inter-modal adversarial loss function to ensure different modal features own the same distribution. Finally, we conduct the cross-modal retrieval comparative experiments and the ablation studies on two public datasets MIRFickr and NUS-WIDE. The experimental results show that DAMCH outperforms the current state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Yang, Xiaohan and Wang, Zhen and Liu, Wenhao and Chang, Xinyi and Wu, Nannan},
  doi          = {10.1007/s13735-023-00288-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Deep adversarial multi-label cross-modal hashing algorithm},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting abnormal behavior in megastore for crime
prevention using a deep neural architecture. <em>IJMIR</em>,
<em>12</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s13735-023-00289-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of employing neural networks in diverse academic, scientific endeavors, and industrial applications is on the rise. In addition to moving forward in neural networks, there has been a great interest in modeling human behavior and activity patterns to recognize particular events. Various methods have so far been proposed for building expert vision systems to understand the scene and draw true semantic inferences from the observed dynamics. However, classifying abnormal or unusual activities in real-time video sequences is still challenging, as the details in video sequences have a time continuity constraint. A cost-effective approach is still demanding, and so this work presents an advanced three-dimensional convolutional network (A3DConvNet) for detecting abnormal behavior of persons by analyzing their actions. The network proposed is 15 layers deep that uses 18 convolutional operations to effectively analyze the video contents and produces spatiotemporal features. The integrated dense layer uses these features for the efficient learning process, and the softmax layer is used as the output layer for labeling the sequences. Additionally, we have created a dataset that carries video clips to represent abnormal behaviors of humans in megastores/shops, which is a consequent contribution of this paper. The dataset includes five complicated activities in the shops/megastores: normal, shoplifting, drinking, eating, and damaging. By analyzing human actions, the proposed algorithm produces an alert if anything like abnormalities are found. Our method&#39;s effectiveness is demonstrated through extensive experiments conducted on the synthesized dataset, achieving an accuracy of up to ~91%.},
  archive      = {J_IJMIR},
  author       = {Ansari, Mohd. Aquib and Singh, Dushyant Kumar and Singh, Vibhav Prakash},
  doi          = {10.1007/s13735-023-00289-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Detecting abnormal behavior in megastore for crime prevention using a deep neural architecture},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Medical image watermarking: A survey on applications,
approach and performance requirement compliance. <em>IJMIR</em>,
<em>12</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s13735-023-00290-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images are fundamentally utilized for rendering diagnostic and treatment to patients. Medical images are a patient body or part captured using medical imaging devices such as CT, X-Ray, PET, MRI, and US. Technological advancement introduces E-Healthcare systems, Telemedicine, and Electronic Health Information Systems (EHIS) enabling medical images to flow over the public network for remote healthcare services. The manipulation or replacement of medical images is fatal to the well-being of a patient, thereby requiring protection using watermarking. Watermarking is a data security approach toward protecting medical images against abuse by unauthorized personnel via providing confidentiality, authentication, and integrity verification. The dynamism and importance of medical image watermarking require constant literature update on trends, issues, and challenges which leading to the forgoing research survey. The survey proposes to highlight trending application areas in medical image watermarking research and evaluation of the recent approach adopted by researchers. Furthermore, the survey evaluates existing work in compliance with the standard benchmark requirement in design and performance and presents a discussion on the way forward to other possible research opportunities in the medical image watermarking domain.},
  archive      = {J_IJMIR},
  author       = {Ayuba, Shehu and Zainon, Wan Mohd Nazmee Wan},
  doi          = {10.1007/s13735-023-00290-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Medical image watermarking: A survey on applications, approach and performance requirement compliance},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved customized CNN model for adaptive recognition of
cerebral palsy people’s handwritten digits in assessment.
<em>IJMIR</em>, <em>12</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s13735-023-00291-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cerebral palsy (CP) is used to describe a group of disorders, characterized by non-progressive, but permanent damage to the developing brain that results in motor deficits, functional difficulties, sensory impairments, and cognitive impairments. AI is rapidly developing with special importance to education and healthcare. Interdisciplinary research perspectives and applications have come up as a helping hand to meet out the needs in different paradigm of Assessment and Biomedicine. At this juncture, building assessment tools for people with motor coordinate impairment is a valuable yet novel research challenge. So, authors implement a handwritten digit recognition system using optical character recognition (OCR) on dataset of CP people’s handwritten digits and based on study the problem statement is defined to detect variation in jerking hands on written to optical digits by proposing a custom convolutional neural network (CNN) architecture on the augmented dataset (CP handwritten data). The proposed research will look into a variety of design preferences for CNN-based handwritten digit recognition, including kernel size, layer count, stride size, and dilation. In addition, it evaluated the confusion matrix for maximum number prediction using various optimization algorithms. The extensive trials were conducted and obtained 85% accuracy for the CP dataset and 97% for the MNIST dataset.},
  archive      = {J_IJMIR},
  author       = {Muthureka, K. and Srinivasulu Reddy, U. and Janet, B.},
  doi          = {10.1007/s13735-023-00291-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {An improved customized CNN model for adaptive recognition of cerebral palsy people’s handwritten digits in assessment},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Content-based image retrieval using handcraft feature fusion
in semantic pyramid. <em>IJMIR</em>, <em>12</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00292-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenge of image retrieval systems is to retrieve similar samples in a way that can be interpreted in a semantic relationship with the user&#39;s query image. In recent years, deep neural networks, due to their remarkable role in extracting the content and semantic features of the image, have been at the center of attention for image retrieval. Processing occurs in deep neural networks, at multiple levels, and with a pyramid approach. This characteristic allows the extraction of semantic and high-level features from the image. On the other hand, the image content features can be extracted with high interpretability using handcraft features. Therefore, in the proposed approach, by fusing features and adding extra information sources, handcraft features are semantically enhanced. In this approach, handcraft features including color and texture are extracted from the semantic pyramid of the deep neural network. The semantic pyramid is the result of the fusion of feature maps in different levels of deep neural networks. Additionally, in this approach, feature vector interpretability is also considered. The t-SNE technique has been used to interpret the discriminability of the feature vector between the classes of the database. Also, the silhouette criterion has been introduced to study the degree of intra-class compatibility and inter-class dataset samples discriminability with feature vector.},
  archive      = {J_IJMIR},
  author       = {Taheri, Fatemeh and Rahbar, Kambiz and Beheshtifard, Ziaeddin},
  doi          = {10.1007/s13735-023-00292-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Content-based image retrieval using handcraft feature fusion in semantic pyramid},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DELIGHT-net: DEep and LIGHTweight network to segment indian
text at word level from wild scenic images. <em>IJMIR</em>,
<em>12</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s13735-023-00293-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition and detection of multioriented text from textual natural scene images are still challenging in the computer vision community. The segmentation on either word level or character level is a vital step in the entire end-to-end performance of the scene text recognition system. Many academicians and researchers have done work in the prominent field of segmenting the words or characters from complex document images as well as handwritten images for various non-Indian scripts. In this paper, we extensively presented a deep learning-based architecture named DELIGHT-Net which is derived from the general UNet architecture to segment the text at the word level from natural scene images. The method is mainly proposed to segment the Devanagari, Gurumukhi, and English scenic words from complete images collected from day-to-day life. To achieve this, we have introduced a new dataset, i.e., National Institute of Technology Jalandhar-Word Segmentation (NITJ-WS) which has around 2200 text blocks extracted from 1500 natural images containing unilingual, bilingual, and trilingual text. The benchmark comparative assessment of our dataset is performed with the proposed model and two state-of-the-art models, i.e., UNet and ResUNet. Statistical and visual results are evaluated using different evaluation parameters, which depict the efficiency of the proposed model. Some possible future directions are also recommended in the manuscript. We hope that our work is a stepping stone for academicians in the field of natural scene text recognition.},
  archive      = {J_IJMIR},
  author       = {Mahajan, Shilpa and Rani, Rajneesh and Trehan, Karan},
  doi          = {10.1007/s13735-023-00293-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DELIGHT-net: DEep and LIGHTweight network to segment indian text at word level from wild scenic images},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interactive attribute-preserving fashion recommendation
with 3D image-based virtual try-on. <em>IJMIR</em>, <em>12</em>(2),
1–15. (<a href="https://doi.org/10.1007/s13735-023-00294-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online shopping experiences should be simplified by incorporating essential features such as virtual try-on clothing and recommending new items based on the customer’s preferences. This is necessary given the rapid growth of the fashion industry and the expansion of shopping technologies. In this paper, we propose a new approach integrating fashion image retrieval and recommendation with a 3D virtual try-on network. We aim to build an interactive attributes-preserving model that allows users to choose the favourite garments and virtually try them on after uploading a frontal image of the whole body. Several deep learning architectures are used to extract and learn the key attributes of garment image, by which each formulated image is subjected to effective human body segmentation and pose estimation procedures. Then, a 3D VTON network is used to generate a 3D image of the user wearing a specific garment, after which the fashion retrieval system recommends and ranks more relevant items. Extensive experiments on multi-domain fashion dataset demonstrate that the proposed framework outperforms the state-of-the-art methods in terms of fashion retrieval and attribute relevancy, achieving a top@30 accuracy of 80.02%, an NDCG@30 of 80.26%, and a top@30 mAP of 87.71%. Additionally, the generated generic image descriptors require very little memory space, enabling rapid online learning and retrieval of large-scale 3D images.},
  archive      = {J_IJMIR},
  author       = {Alzu’bi, Ahmad and Younis, Lojin Bani and Madain, Alia},
  doi          = {10.1007/s13735-023-00294-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {An interactive attribute-preserving fashion recommendation with 3D image-based virtual try-on},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Style-aware adversarial pairwise ranking for image
recommendation systems. <em>IJMIR</em>, <em>12</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00295-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of Machine Learning (ML) models to adversarial attack and their prominence pose security issues, notably in image recommendation systems. The adversarial training method is an excellent strategy for improving the generalization capacity of ML models by creating attacks in the embedding space during training. While there has been a plethora of testing on image recommendation system vulnerabilities and defenses, iterative adversarial training methodologies have received little attention. Furthermore, when browsing visual images, consumers are more interested in the content and how well the image style matches the content. However, when compared to image content, the impact of image styles on the adversarial recommendation community has rarely been examined. In this work, we propose a robust Adversarial Content and Style Bayesian Personalized Ranking (ACSBPR) approach that leverages content and style features for image recommendation. The ACSBPR technique makes three significant contributions: (1) Incorporate content and style features jointly for image recommendation. (2) Present a multi-objective pairwise ranking with Dynamic Negative Sampling to optimize the system and anticipate consumer preferences. (3) To reduce the influence of the attack, we train the ACSBPR objective function using minimax iterative adversarial training. Extensive investigations on the Flickr dataset demonstrate that our strategy achieves better performance when compared to state-of-the-art image recommendation models.},
  archive      = {J_IJMIR},
  author       = {Wu, Zhefu and Zhang, Song and Paul, Agyemang and Fang, Luping},
  doi          = {10.1007/s13735-023-00295-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Style-aware adversarial pairwise ranking for image recommendation systems},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey of multimodal fake news detection
techniques: Advances, challenges, and opportunities. <em>IJMIR</em>,
<em>12</em>(2), 1–27. (<a
href="https://doi.org/10.1007/s13735-023-00296-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating prevalence of disinformation, or “fake news,” on social media platforms represents a growing societal concern with far-reaching implications. Its ubiquitous presence across platforms such as Facebook, Twitter, and Instagram exacerbates the criticality of this issue. Consequently, the development of efficacious counter strategies—such as fact-checking mechanisms and media literacy initiatives—is paramount. This domain of crafting robust defensive strategies against disinformation has burgeoned into an expansive field of investigation, with primary emphasis on the identification and categorization of fake news within the digital media landscape. This study offers a comprehensive survey of the contemporary research landscape on disinformation, or “fake news,” detection, and mitigation strategies. We meticulously analyze the life cycle of disinformation—from inception and propagation to detection—shedding light on both supervised and unsupervised learning techniques such as generative adversarial networks (GANs). The paper provides a comparative analysis of different classification models across a variety of text- and image-based datasets related to fake news. Furthermore, it provides an in-depth discussion of the key evaluation metrics used in assessing the accuracy of news authenticity. Challenges have been put forth concerning the sophisticated task of precisely detecting fake news, an undertaking obscured by its subtle and often deceptive aspects. By offering in-depth insights into the phenomenon of fake news and potential counterstrategies, this survey is anticipated to enrich scholarly understanding, thereby catalyzing the development of innovative solutions to tackle this persistent global issue.},
  archive      = {J_IJMIR},
  author       = {Tufchi, Shivani and Yadav, Ashima and Ahmed, Tanveer},
  doi          = {10.1007/s13735-023-00296-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A comprehensive survey of multimodal fake news detection techniques: Advances, challenges, and opportunities},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPSD: Similarity-preserving self-distillation for video–text
retrieval. <em>IJMIR</em>, <em>12</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00298-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing methods solve cross-modal video and text retrieval via coarse-grained similarity computation based on global representations or fine-grained cross-modal interaction. The former misses sufficient information, while the latter suffers from inferior efficiency in inference. Furthermore, hierarchical features of transformer have not been fully utilized in cross-modal contrastive learning. In this paper, we propose similarity-preserving self-distillation method (SPSD) to achieve video and text alignment by cross-granularity and cross-layer ways. For cross-granularity self-distillation, fine-grained cross-modal similarity based on video and text token-wise interaction is transferred to coarse-grained similarity based on global video and text representations. To utilize hierarchical features of deep video and text transformer encoders, we propose cross-layer self-distillation by regarding cross-modal similarity based on semantic features as teacher to provide soft label for the similarity learning based on low-level features. Besides, we construct hierarchical contrastive loss and cross-granularity self-distillation loss at both feature and semantic levels for training transformer-based video and text encoders. SPSD sufficiently utilizes the fine-grained cross-modal interaction and hierarchical transformer features by generating distillation signals through network itself in training stage. In retrieval inference, cross-modal similarity computation between video and text is based on semantic-level global embeddings. Our SPSD achieves outstanding performance for video–text retrieval on MSRVTT, ActivityNet and LSMDC datasets. Our code is available at https://github.com/Macro-1998/SPSD/ .},
  archive      = {J_IJMIR},
  author       = {Wang, Jiachen and Hua, Yan and Yang, Yingyun and Kou, Hongwei},
  doi          = {10.1007/s13735-023-00298-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {SPSD: Similarity-preserving self-distillation for video–text retrieval},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ornament image retrieval using few-shot learning.
<em>IJMIR</em>, <em>12</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s13735-023-00299-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce OrnamentFIR, a novel ornament dataset related to the fashion industry. In recent years, the retrieval of clothing and footwear articles has received significant interest from researchers. However, because of the design intricacy and lack of a suitable dataset, intricate fashion products, like jewelry, have not gotten much attention. We have assembled the OrnamentFIR dataset to address this issue. By revisiting the publically accessible datasets, namely RingFIR and NecklaceFIR, we create a novel dataset called OrnamentFIR. The dataset includes over $$\sim $$ 4.4 K high-quality images of bangles, over $$\sim $$ 4.8 K high-definition images of necklaces, and more than $$\sim $$ 2.6 K high-quality images of earrings. The dataset is divided into three named classes: ring, necklace, and bangle, with each class having 46, 49, and 56 labeled categories, respectively. Due to the limited amount of data, we employed matching networks, a neural network that uses recent advances in attention and memory to enable rapid learning, to extract the desired image from the dataset. Using the matching networks for one-shot learning technique, we achieve 68% accuracy for RGB photographs, 62% accuracy for segmented images, and 50% accuracy for RGB+Segmented images. For the benefit of researchers, the ornament dataset has been made public. Public access to the dataset and code is provided at https://github.com/iammaidul/OrnamentFIR .},
  archive      = {J_IJMIR},
  author       = {Islam, Sk Maidul and Joardar, Subhankar and Sekh, Arif Ahmed},
  doi          = {10.1007/s13735-023-00299-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Ornament image retrieval using few-shot learning},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-wise reasoning reinforcement learning for
pedestrian attribute retrieval. <em>IJMIR</em>, <em>12</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s13735-023-00300-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian attribute retrieval (PAR) aims at retrieving soft-biometric attributes of pedestrian images from video surveillance. Despite advancements, PAR grapples with challenges, notably the concern of attribute imbalanced distribution. Within this context, we highlight a critical observation: this challenge encompasses both inter-attribute and overlooked intra-attribute imbalanced data distribution. To address the overlooked intra-attribute imbalance problem, we introduce an attribute-wise reasoning reinforcement learning framework (AwRL). AwRL formulates PAR as a Markov decision process (MDP), orchestrating attribute retrieval individually within reinforcement learning episodes. By traversing the entire PAR dataset, each attribute retrieval is calibrated with distinct reward scales, thereby ameliorating the intra-attribute imbalance. Additionally, we develop a novel supervised reinforcement loss function (SR-Loss) to enhance the robustness of the retrieval model. SR-Loss mitigates reinforcement learning’s inherent training instability in the trial-and-error interactions with the environment. The experimental results on three benchmark datasets of PETA, RAP and PA100K demonstrate the effectiveness of our approach, underscoring its capacity to surmount the intra-attribute imbalanced problem.},
  archive      = {J_IJMIR},
  author       = {Wang, Yaodong and Hu, Zhenfei and Ji, Zhong},
  doi          = {10.1007/s13735-023-00300-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Attribute-wise reasoning reinforcement learning for pedestrian attribute retrieval},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConvST-LSTM-net: Convolutional spatiotemporal LSTM networks
for skeleton-based human action recognition. <em>IJMIR</em>,
<em>12</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s13735-023-00301-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) emphases on perceiving and identifying the action behavior done by humans within an image/video. The HAR activities include motion patterns and normal or abnormal activities like standing, walking, sitting, running, playing, falling, fighting, etc. Recently, it sparks the attention of researchers especially in 3D skeleton sequence. The actions of human can be represented via sequence of motions of skeletal keyjoints, although not all the skeleton keyjoints are informative in nature. Various approaches for HAR are used like LSTM, ConvLSTM, Conv-GRU, ST-LSTM, etc. Thus far, ST-LSTM approaches have shown tremendous performance in 3D skeleton sequence tasks but the detection of irrelevant keyjoints produce noise that deteriorates the performance of the model. So, the intent is to bring attention toward improving the efficacy of the model by focusing on informative keyjoint coordinates only. Therefore, the research paper introduces a new class of spatiotemporal LSTM approaches named as ConvST-LSTM-Net (convolutional spatiotemporal long short-term memory network) for skeleton-based action recognition. The prime focus of proposed model is to identify the informative keyjoints in each frame. The result of extensive experimental analysis exhibits that ConvST-LSTM-Net outperforms the state-of-the-art models on various benchmarks dataset, viz. NTU RGB + D 60, UT-Kinetics, UP-Fall Detection, UCF101, and HDMB51 for skeleton sequence data.},
  archive      = {J_IJMIR},
  author       = {Sharma, Abhilasha and Singh, Roshni},
  doi          = {10.1007/s13735-023-00301-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {ConvST-LSTM-net: Convolutional spatiotemporal LSTM networks for skeleton-based human action recognition},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual feature segmentation with reinforcement learning for
continuous sign language recognition. <em>IJMIR</em>, <em>12</em>(2),
1–11. (<a href="https://doi.org/10.1007/s13735-023-00302-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous sign language recognition (CSLR) involves inputting a video that contains unbroken signs and outputting a prediction of the sign gloss sequence. Our research found that the visual features extracted from different signs in a sign language video show a noticeable disparity. As a result, we employed reinforcement learning (RL) to segment the visual features of the video into multiple groups to aid in model training. Compared to previous CSLR methods, our approach results in a more fine-tuned and supervised training process, leading to greater effective gradient backpropagation and improved model performance. We introduce a novel method named “Visual Feature Segmentation with Reinforcement Learning (VFS-RL)” for CSLR. Firstly, we constructed an end-to-end continuous sign language recognition network. Subsequently, we designed an auxiliary task of multi-class recognition to improve the model’s capability for extracting semantic information from sign video, which uses RL to group the video’s visual features. Finally, we conducted experiments on two public CSLR datasets, and the results of our ablation studies demonstrate the effectiveness of our proposed method. Our approach has competitive results compared to other methods in comparison tests.},
  archive      = {J_IJMIR},
  author       = {Fang, Yuchun and Wang, Liangjun and Lin, Shiquan and Ni, Lan},
  doi          = {10.1007/s13735-023-00302-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Visual feature segmentation with reinforcement learning for continuous sign language recognition},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint multi-scale information and long-range dependence for
video captioning. <em>IJMIR</em>, <em>12</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s13735-023-00303-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since deep learning methods have achieved great success in both computer vision and natural language processing, video captioning tasks based on these two fields have also attracted extensive attention. Video captioning is a challenging task, which aims to present video information in the form of natural language to enhance video intelligibility. Most of the current researches in video captioning focus on the behavioral description of the main objects of the video, especially on the holistic understanding of the content. This trend makes most video captioning efforts ignoring the characteristics of smaller objects in the video, resulting in ambiguous, imprecise, or even fundamentally wrong descriptions. In this paper, a novel video captioning method MSLR is proposed, which improves the accuracy of video description by extracting features of video objects with different granularity and preserving long-range temporal dependencies. Specifically, the proposed method performs convolution operations at different scales to obtain different granular spatial features of videos and then fuses them to generate a unified spatial representation. On this basis, a temporal extraction network is further constructed using non-local blocks to preserve the long-range dependencies of videos. Evaluated on two popular benchmark datasets, the experimental results demonstrate the superiority of MSLR over the previous state-of-the-art methods, and the effectiveness of MSLR components is verified through ablation experiments and text evaluation.},
  archive      = {J_IJMIR},
  author       = {Zhai, Zhongyi and Chen, Xiaofeng and Huang, Yishuang and Zhao, Lingzhong and Cheng, Bo and He, Qian},
  doi          = {10.1007/s13735-023-00303-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Joint multi-scale information and long-range dependence for video captioning},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on open-domain image retrieval in the wild.
<em>IJMIR</em>, <em>12</em>(2), 1–2. (<a
href="https://doi.org/10.1007/s13735-023-00304-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMIR},
  author       = {Liu, Yu and Guo, Yanming and Matsui, Yusuke},
  doi          = {10.1007/s13735-023-00304-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-2},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Special issue on open-domain image retrieval in the wild},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight small object detection algorithm based on
improved YOLOv5 for driving scenarios. <em>IJMIR</em>, <em>12</em>(2),
1–14. (<a href="https://doi.org/10.1007/s13735-023-00305-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection has been a longstanding challenge in the field of object detection, and achieving high detection accuracy is crucial for autonomous driving, especially for small objects. This article focuses on researching small object detection algorithms in driving scenarios. To address the need for higher accuracy and fewer parameters in object detection for autonomous driving, we propose LSD-YOLO, a small object detection algorithm with higher average precision and fewer parameters. Building upon YOLOv5, we fully leverage small-scale feature maps to enhance the network’s detection ability for small objects. Additionally, we introduce a new structure called FasterC3 to reduce the network’s latency and parameter volume. To locate attention regions in complex driving scenarios, we integrate Coordinate Attention and explore multiple solutions to determine the optimal approach. Furthermore, we use a spatial pyramid pooling method called LeakySPPF (Wen and Zhang, in: Jin Z, Jiang Y, Buchmann RA, Bi Y, Ghiran A-M, Ma W (eds.) Knowledge Science, Engineering and Management, pp. 39-46. Springer, Cham, 2023) to further improve network speed, achieving up to 15% faster computation. Finally, to better match driving scenarios, we propose a medium-sized dataset called Cone4k to supplement insufficient categories in the VisDrone dataset. Extensive experiments show that our proposed LSD-YOLO(s) achieves an mAP and F1 score of 24.9 and 48.6, respectively, on the VisDrone2021 dataset, resulting in a 4.6% and 3.6% improvement over YOLOv5(s) while reducing parameter volume by 7.5%.},
  archive      = {J_IJMIR},
  author       = {Wen, Zonghui and Su, Jia and Zhang, Yongxiang and Li, Mingyu and Gan, Guoxi and Zhang, Shenmeng and Fan, Deyu},
  doi          = {10.1007/s13735-023-00305-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A lightweight small object detection algorithm based on improved YOLOv5 for driving scenarios},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FOF: A fine-grained object detection and feature extraction
end-to-end network. <em>IJMIR</em>, <em>12</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s13735-023-00306-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, widely used object detection can predict targets present in the training set. However, in fine-grained object detection tasks, such as commodity detection, the introduction of a new target class requires retraining the model, which significantly reduces the flexibility of the algorithm in applications. In response to this problem, we propose an end-to-end fine-grained object detection and feature extraction network (FOF). To detect and identify objects beyond the target category of the training set, the category output in the network head is removed and replaced with a 128-dimensional feature vector. We used the ArcFace loss function to improve feature classification during training. Since there is no category output, an improved non-maximum suppression algorithm, non-maximum suppression-feature similarity, is proposed to distinguish same class and dissimilar class prediction boxes by feature similarity. During the inference, FOF outputs prediction boxes and feature vectors, and matches them with the feature vectors in the feature gallery to determine the detected object category and complete object detection and recognition. Experimental results indicate that FOF achieved high accuracy in both the MS COCO, PASCAL VOC2012, SmartUVM, and a large-scale and fine-grained Retail Product Checkout datasets. In addition, the method exhibits a low equal error rate when identifying new categories, achieving the objective of detecting and identifying new categories without the need to retrain the model.},
  archive      = {J_IJMIR},
  author       = {Shen, Wenzhong and Chen, Jinpeng and Shao, Jie},
  doi          = {10.1007/s13735-023-00306-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {FOF: A fine-grained object detection and feature extraction end-to-end network},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSNet: Position-shift alignment network for image caption.
<em>IJMIR</em>, <em>12</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s13735-023-00307-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Transformer-based models have gained increasing popularity in the field of image captioning. The global attention mechanism of the Transformer facilitates the integration of region and grid features, leading to a significant improvement in accuracy. However, combining two features through direct fusion may lead to inevitable semantic noise, which is caused by non-synergistic issue of the region and grid features; meanwhile, the additional detector to extract region features also decrease the efficiency of the model. In this paper, we introduce a novel position-shift alignment network (PSNet) to exploit the advantages of the two features. Concretely, we embed a simple detector DETR into the model and extracted region features based on grid features to improve model efficiency. Moreover, we propose a P-shift alignment module to address semantic noise caused by non-synergistic issue of the region and grid features. To validate our model, we conduct extensive experiments and visualization on the MS-COCO dataset, and results show that PSNet is qualitatively competitive with existing models under comparable experimental conditions.},
  archive      = {J_IJMIR},
  author       = {Xue, Lixia and Zhang, Awen and Wang, Ronggui and Yang, Juan},
  doi          = {10.1007/s13735-023-00307-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {PSNet: Position-shift alignment network for image caption},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment analysis using deep learning techniques: A
comprehensive review. <em>IJMIR</em>, <em>12</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s13735-023-00308-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of social media platforms and online communication, the necessity of using automated sentiment analysis techniques has significantly increased. Deep learning techniques have emerged in extracting complex patterns and features from unstructured text data, which makes them a powerful tool for sentiment analysis. This research article presents a comprehensive review of sentiment analysis using deep learning techniques. We discuss various aspects of sentiment analysis, including data preprocessing, feature extraction, model architectures, and evaluation metrics. We explore the use of recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformer models in sentiment analysis tasks. We examine the utilization of RNNs, incorporating long short-term memory (LSTM) and gated recurrent unit (GRU), to model sequential dependencies in text data. Furthermore, we discuss the recent advancements in sentiment analysis achieved through a transformer. The findings from this review can facilitate the development of more accurate and efficient sentiment analysis models, enabling organizations to gain valuable insights from large volumes of textual data in several domains, such as social media, market analysis, and customer reviews.},
  archive      = {J_IJMIR},
  author       = {Sahoo, Chinmayee and Wankhade, Mayur and Singh, Binod Kumar},
  doi          = {10.1007/s13735-023-00308-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Sentiment analysis using deep learning techniques: A comprehensive review},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LG-MLFormer: Local and global MLP for image captioning.
<em>IJMIR</em>, <em>12</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00266-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention-based image captioning model exists visual features’ spatial information loss problem, introducing relative position encoding can solve the problem to some extent. However, it will bring additional parameters and greater computational complexity. To solve the above problem, we propose a novel local–global MLFormer (LG-MLFormer) with specifically designed encoder module Local–global multi-layer perceptron (LG-MLP). The LG-MLP can capture the latent correlations between different images and its linear stacking calculation mode can reduce computational complexity. It consists of two independent local MLP (LM) modules and a cross-domain global MLP (CDGM) module. The LM specially designs the mapping dimension between linear layers to realize the self-compensation of visual features’ spatial information without introducing relative position encoding. The CDGM module aggregates cross-domain potential correlations between grid-based features and region-based features to realize the complementary advantages of these global and local semantic associations. Experiments on the Karpathy test split and the online test server reveal that our approach provides superior or comparable performance to the state-of-the-art (SOTA). Trained models and code for reproducing the experiments are publicly available at: https://github.com/wxx1921/LGMLFormer-local-and-global-mlp-for-image-captioning .},
  archive      = {J_IJMIR},
  author       = {Jiang, Zetao and Wang, Xiuxian and Zhai, Zhongyi and Cheng, Bo},
  doi          = {10.1007/s13735-023-00266-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {LG-MLFormer: Local and global MLP for image captioning},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for video-text retrieval: A review.
<em>IJMIR</em>, <em>12</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s13735-023-00267-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-Text Retrieval (VTR) aims to search for the most relevant video related to the semantics in a given sentence, and vice versa. In general, this retrieval task is composed of four successive steps: video and textual feature representation extraction, feature embedding and matching, and objective functions. In the last, a list of samples retrieved from the dataset is ranked based on their matching similarities to the query. In recent years, significant and flourishing progress has been achieved by deep learning techniques, however, VTR is still a challenging task due to the problems like how to learn an efficient spatial-temporal video feature and how to narrow the cross-modal gap. In this survey, we review and summarize over 100 research papers related to VTR, demonstrate state-of-the-art performance on several commonly benchmarked datasets, and discuss potential challenges and directions, with the expectation to provide some insights for researchers in the field of video-text retrieval.},
  archive      = {J_IJMIR},
  author       = {Zhu, Cunjuan and Jia, Qi and Chen, Wei and Guo, Yanming and Liu, Yu},
  doi          = {10.1007/s13735-023-00267-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Deep learning for video-text retrieval: A review},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLIP-based fusion-modal reconstructing hashing for
large-scale unsupervised cross-modal retrieval. <em>IJMIR</em>,
<em>12</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s13735-023-00268-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multi-modal data proliferates, people are no longer content with a single mode of data retrieval for access to information. Deep hashing retrieval algorithms have attracted much attention for their advantages of efficient storage and fast query speed. Currently, the existing unsupervised hashing methods generally have two limitations: (1) Existing methods fail to adequately capture the latent semantic relevance and coexistent information from the different modality data, resulting in the lack of effective feature and hash encoding representation to bridge the heterogeneous and semantic gaps in multi-modal data. (2) Existing unsupervised methods typically construct a similarity matrix to guide the hash code learning, which suffers from inaccurate similarity problems, resulting in sub-optimal retrieval performance. To address these issues, we propose a novel CLIP-based fusion-modal reconstructing hashing for Large-scale Unsupervised Cross-modal Retrieval. First, we use CLIP to encode cross-modal features of visual modalities, and learn the common representation space of the hash code using modality-specific autoencoders. Second, we propose an efficient fusion approach to construct a semantically complementary affinity matrix that can maximize the potential semantic relevance of different modal instances. Furthermore, to retain the intrinsic semantic similarity of all similar pairs in the learned hash codes, an objective function for similarity reconstruction based on semantic complementation is designed to learn high-quality hash code representations. Sufficient experiments were carried out on four multi-modal benchmark datasets (WIKI, MIRFLICKR, NUS-WIDE, and MS COCO), and the proposed method achieves state-of-the-art image-text retrieval performance compared to several representative unsupervised cross-modal hashing methods.},
  archive      = {J_IJMIR},
  author       = {Mingyong, Li and Yewen, Li and Mingyuan, Ge and Longfei, Ma},
  doi          = {10.1007/s13735-023-00268-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {CLIP-based fusion-modal reconstructing hashing for large-scale unsupervised cross-modal retrieval},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end residual learning-based deep neural network model
deployment for human activity recognition. <em>IJMIR</em>,
<em>12</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s13735-023-00269-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition is a theme commonly explored in computer vision. Its applications in various domains include monitoring systems, video processing, robotics, and healthcare sector, etc. Activity recognition is a difficult task since there are structural changes among subjects, as well as inter-class and intra-class correlation between activities. As a result, a continuous intelligent control system for detecting human behavior with grouping of maximum information is necessary. Therefore, in this paper, a novel automatic system to identify human activity on the UTKinect dataset is implemented by using Residual learning-based Network “ResNet-50” and transfer learning to represent more complicated features and improved model robustness. The experimental results have shown an excellent generalization capability when tested on the validation set and obtained high accuracy of 98.60 per cent with a 0.02 loss score. The designed residual learning-based system indicates the efficiency of comparing with the other state-of-the-art models.},
  archive      = {J_IJMIR},
  author       = {Negi, Alok and Kumar, Krishan},
  doi          = {10.1007/s13735-023-00269-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {End-to-end residual learning-based deep neural network model deployment for human activity recognition},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nested-net: A deep nested network for background
subtraction. <em>IJMIR</em>, <em>12</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s13735-023-00270-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is one of the most highly regarded steps in computer vision, especially in video surveillance applications. Although various approaches have been proposed to cope with the different difficulties of this field, many of these methods have not been able to fully tackle complicated situations in realistic scenes due to their sensitivity to many challenges. This paper presents a deep nested background subtraction algorithm based on residual micro-autoencoder blocks. Hence, our method is implemented as a U-net like architecture with more skip connections. The nested network uses residual connections between these micro-autoencoders that can extract significant multi-scale features of a complex scene. We also test and prove that the proposed method can work in various challenging situations. A small set of training samples is included to train this end-to-end network. The experimental results demonstrate that our model outperforms other state-of-the-art methods on two well-known benchmark datasets: CDNet 2014 and SBI 2015.},
  archive      = {J_IJMIR},
  author       = {Gouizi, Fatma and Megherbi, Ahmed Chaouki},
  doi          = {10.1007/s13735-023-00270-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Nested-net: A deep nested network for background subtraction},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Study of alzheimer’s disease brain impairment and methods
for its early diagnosis: A comprehensive survey. <em>IJMIR</em>,
<em>12</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s13735-023-00271-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is one of the most severe kinds of dementia that affects the elderly population. Since this disease is incurable and the changes in brain sub-regions start decades before the symptoms are observed, early detection becomes more challenging. Discriminating similar brain patterns for AD classification is difficult as minute changes in biomarkers are detected in different neuroimaging modality, also in different image projections. Deep learning models have provided excellent performance in analyzing various neuroimaging and clinical data. In this survey, we performed a comparative analysis of 134 papers published between 2017 and 2022 to get 360° knowledge of the AD kind of problem and everything done to examine and deeply analyze factors causing this. Different pre-processing tools and techniques, various datasets, and brain sub-regions affected mainly by AD have been reviewed. Further deep analysis of various biomarkers, feature extraction techniques, Deep learning and Machine learning architectures has been done for the survey. Summarization of the latest research articles with valuable findings has been represented in multiple tables. A novel approach has been used representing classification of biomarkers, pre-processing techniques and AD detection methods in form of figures and classification of AD on the basis of stages showing difference in accuracies between binary and multi-class in form of table. We finally concluded our paper by addressing some challenges faced during classification and provided recommendations that can be considered for future research in diagnosing various stages of AD.},
  archive      = {J_IJMIR},
  author       = {Pallawi, Shruti and Singh, Dushyant Kumar},
  doi          = {10.1007/s13735-023-00271-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Study of alzheimer’s disease brain impairment and methods for its early diagnosis: A comprehensive survey},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video anomaly detection with memory-guided multilevel
embedding. <em>IJMIR</em>, <em>12</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00272-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Playing a vitally important role in the operation of intelligent video surveillance system and smart city, video anomaly detection (VAD) has been widely practiced and studied in both industrial circles and academia. In the present study, a new anomaly detection method is proposed for multi-level memory embedding. According to the novel method, the feature prototype of the sample is stored in the memory pool, which enhances the diversity of the sample feature prototype paradigm. Besides, the memory is embedded in the decoder in a hierarchical integrating manner, which makes the feature information of the object more complete and improves the quality of features. At the end of the model, modeling is performed for the channel relationship between the features of the object in the channel dimension, thus making the model capable of more efficient anomaly detection. This method is verified by conducting evaluation on three publicly available datasets: UCSD Ped2, CUHK Avenue, ShanghaiTech.},
  archive      = {J_IJMIR},
  author       = {Zhou, Liuping and Yang, Jing},
  doi          = {10.1007/s13735-023-00272-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Video anomaly detection with memory-guided multilevel embedding},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple feedback based adversarial collaborative filtering
with aesthetics. <em>IJMIR</em>, <em>12</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-023-00273-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-aware personalized recommendation systems can estimate the potential demand by evaluating consumer personalized preferences. In general, consumer feedback data is deduced from either explicit feedback or implicit feedback. However, explicit and implicit feedback raises the chance of malicious operation or misoperation, which can lead to deviations in recommended outcomes. Adversarial learning, a regularization approach that can resist disturbances, could be a promising choice for enhancing model resilience. We propose a novel adversarial collaborative filtering with aesthetics (ACFA) for the visual recommendation that utilizes adversarial learning to improve resilience and performance in the case of perturbation. The ACFA algorithm applies three types of input to the visual Bayesian personalized ranking: negative, unobserved, and positive feedback. Through feedbacks at various levels, it uses a probabilistic approach to obtain consumer personalized preferences. Since in visual recommendation, the aesthetic data in determining consumer preferences on product is critical, we construct the consumer personalized preferences model with aesthetic elements, and then use them to enhance the sampling quality when training the algorithm. To mitigate the negative effects of feedback noise, We use minimax adversarial learning to learn the ACFA objective function. Experiments using two datasets demonstrate that the ACFA model outperforms state-of-the-art algorithms on two metrics.},
  archive      = {J_IJMIR},
  author       = {Wu, Zhefu and Ma, Yuhang and Cao, Junzhuo and Paul, Agyemang and Li, Xiang},
  doi          = {10.1007/s13735-023-00273-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multiple feedback based adversarial collaborative filtering with aesthetics},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep image retrieval network using max-m-min pooling and
morphological feature generating residual blocks. <em>IJMIR</em>,
<em>12</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s13735-023-00274-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The textural and structural information contained in the images is very important for generating highly discriminative features for the task of image retrieval. Morphological operations are nonlinear mathematical operations that can provide such textural and structural information. In this work, a new residual block based on a module using morphological operations coupled with an edge extraction module is proposed. A novel pooling operation focusing on the edges of the images is also proposed. A deep convolutional network is then designed using the proposed residual block and the new pooling operation that significantly improves its representational capacity. Extensive experiments are carried out to show the effectiveness of the ideas used in the design of the proposed deep image retrieval network. The proposed network is shown to significantly outperform existing state-of-the-art image retrieval networks on various benchmark datasets.},
  archive      = {J_IJMIR},
  author       = {Sabahi, Farzad and Omair Ahmad, M. and Swamy, M.N.S.},
  doi          = {10.1007/s13735-023-00274-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A deep image retrieval network using max-m-min pooling and morphological feature generating residual blocks},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion-aware music tower blocks (EmoMTB ): An intelligent
audiovisual interface for music discovery and recommendation.
<em>IJMIR</em>, <em>12</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-023-00275-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music listening has experienced a sharp increase during the last decade thanks to music streaming and recommendation services. While they offer text-based search functionality and provide recommendation lists of remarkable utility, their typical mode of interaction is unidimensional, i.e., they provide lists of consecutive tracks, which are commonly inspected in sequential order by the user. The user experience with such systems is heavily affected by cognition biases (e.g., position bias, human tendency to pay more attention to first positions of ordered lists) as well as algorithmic biases (e.g., popularity bias, the tendency of recommender systems to overrepresent popular items). This may cause dissatisfaction among the users by disabling them to find novel music to enjoy. In light of such systems and biases, we propose an intelligent audiovisual music exploration system named EmoMTB . It allows the user to browse the entirety of a given collection in a free nonlinear fashion. The navigation is assisted by a set of personalized emotion-aware recommendations, which serve as starting points for the exploration experience. EmoMTB  adopts the metaphor of a city, in which each track (visualized as a colored cube) represents one floor of a building. Highly similar tracks are located in the same building; moderately similar ones form neighborhoods that mostly correspond to genres. Tracks situated between distinct neighborhoods create a gradual transition between genres. Users can navigate this music city using their smartphones as control devices. They can explore districts of well-known music or decide to leave their comfort zone. In addition, EmoMTB   integrates an emotion-aware music recommendation system that re-ranks the list of suggested starting points for exploration according to the user’s self-identified emotion or the collective emotion expressed in EmoMTB ’s Twitter channel. Evaluation of EmoMTB   has been carried out in a threefold way: by quantifying the homogeneity of the clustering underlying the construction of the city, by measuring the accuracy of the emotion predictor, and by carrying out a web-based survey composed of open questions to obtain qualitative feedback from users.},
  archive      = {J_IJMIR},
  author       = {Melchiorre, Alessandro B. and Penz, David and Ganhör, Christian and Lesota, Oleg and Fragoso, Vasco and Fritzl, Florian and Parada-Cabaleiro, Emilia and Schubert, Franz and Schedl, Markus},
  doi          = {10.1007/s13735-023-00275-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Emotion-aware music tower blocks (EmoMTB ): An intelligent audiovisual interface for music discovery and recommendation},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximizing mutual information inside intra- and
inter-modality for audio-visual event retrieval. <em>IJMIR</em>,
<em>12</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s13735-023-00276-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain can process sound and visual information in overlapping areas of the cerebral cortex, which means that audio and visual information are deeply correlated with each other when we explore the world. To simulate this function of the human brain, audio-visual event retrieval (AVER) has been proposed. AVER is about using data from one modality (e.g., audio data) to query data from another. In this work, we aim to improve the performance of audio-visual event retrieval. To achieve this goal, first, we propose a novel network, InfoIIM, which enhance the accuracy of intra-model feature representation and inter-model feature alignment. The backbone of this network is a parallel connection of two VAE models with two different encoders and a shared decoder. Secondly, to enable the VAE to learn better feature representations and to improve intra-modal retrieval performance, we have used InfoMax-VAE instead of the vanilla VAE model. Additionally, we study the influence of modality-shared features on the effectiveness of audio-visual event retrieval. To verify the effectiveness of our proposed method, we validate our model on the AVE dataset, and the results show that our model outperforms several existing algorithms in most of the metrics. Finally, we present our future research directions, hoping to inspire relevant researchers.},
  archive      = {J_IJMIR},
  author       = {Li, Ruochen and Li, Nannan and Wang, Wenmin},
  doi          = {10.1007/s13735-023-00276-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Maximizing mutual information inside intra- and inter-modality for audio-visual event retrieval},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MemeTector: Enforcing deep focus for meme detection.
<em>IJMIR</em>, <em>12</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-023-00277-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image memes and specifically their widely known variation image macros are a special new media type that combines text with images and are used in social media to playfully or subtly express humor, irony, sarcasm and even hate. It is important to accurately retrieve image memes from social media to better capture the cultural and social aspects of online phenomena and detect potential issues (hate-speech, disinformation). Essentially, the background image of an image macro is a regular image easily recognized as such by humans but cumbersome for the machine to do so due to feature map similarity with the complete image macro. Hence, accumulating suitable feature maps in such cases can lead to deep understanding of the notion of image memes. To this end, we propose a methodology, called visual part utilization, that utilizes the visual part of image memes as instances of the regular image class and the initial image memes as instances of the image meme class to force the model to concentrate on the critical parts that characterize an image meme. Additionally, we employ a trainable attention mechanism on top of a standard ViT architecture to enhance the model’s ability to focus on these critical parts and make the predictions interpretable. Several training and test scenarios involving web-scraped regular images of controlled text presence are considered for evaluating the model in terms of robustness and accuracy. The findings indicate that light visual part utilization combined with sufficient text presence during training provides the best and most robust model, surpassing state of the art. Source code and dataset are available at https://github.com/mever-team/memetector .},
  archive      = {J_IJMIR},
  author       = {Koutlis, Christos and Schinas, Manos and Papadopoulos, Symeon},
  doi          = {10.1007/s13735-023-00277-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MemeTector: Enforcing deep focus for meme detection},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optical music recognition for homophonic scores with neural
networks and synthetic music generation. <em>IJMIR</em>, <em>12</em>(1),
1–13. (<a href="https://doi.org/10.1007/s13735-023-00278-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Convolutional Recurrent Neural Networks (CRNN) have been broadly applied to solve these tasks in an end-to-end fashion with successful performance. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements sharing the same horizontal position, disrupting the linear flow of the timeline. In this paper, we study the ability of the state-of-the-art CRNN approach to learn codes that represent this disruption in homophonic scores. In our experiments, we study the lower bounds in the recognition task of real scores when the models are trained with synthetic data. Two relevant conclusions are drawn: (1) Our serialized ways of encoding the music content are appropriate for CRNN-based OMR; (2) the learning process is possible with synthetic data, but there exists a glass ceiling when recognizing real sheet music.},
  archive      = {J_IJMIR},
  author       = {Alfaro-Contreras, María and Iñesta, José M. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s13735-023-00278-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Optical music recognition for homophonic scores with neural networks and synthetic music generation},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
