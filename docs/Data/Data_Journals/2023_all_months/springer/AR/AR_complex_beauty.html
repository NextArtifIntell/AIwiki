<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ar---90">AR - 90</h2>
<ul>
<li><details>
<summary>
(2023a). Correction: Efficiently exploring for human robot
interaction: Partially observable poisson processes. <em>AR</em>,
<em>47</em>(8), 1593. (<a
href="https://doi.org/10.1007/s10514-023-10152-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AR},
  author       = {Jovan, Ferdian and Tomy, Milan and Hawes, Nick and Wyatt, Jeremy},
  doi          = {10.1007/s10514-023-10152-2},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1593},
  shortjournal = {Auton. Robot.},
  title        = {Correction: efficiently exploring for human robot interaction: partially observable poisson processes},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-frame rate homography and visual odometry by tracking
binary features from the focal plane. <em>AR</em>, <em>47</em>(8),
1579–1592. (<a
href="https://doi.org/10.1007/s10514-023-10122-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotics faces a long-standing obstacle in which the speed of the vision system’s scene understanding is insufficient, impeding the robot’s ability to perform agile tasks. Consequently, robots must often rely on interpolation and extrapolation of the vision data to accomplish tasks in a timely and effective manner. One of the primary reasons for these delays is the analog-to-digital conversion that occurs on a per-pixel basis across the image sensor, along with the transfer of pixel-intensity information to the host device. This results in significant delays and power consumption in modern visual processing pipelines. The SCAMP-5—a general-purpose Focal-plane Sensor-processor array (FPSP)—used in this research performs computations in the analog domain prior to analog-to-digital conversion. By extracting features from the image on the focal plane, the amount of data that needs to be digitised and transferred is reduced. This allows for a high frame rate and low energy consumption for the SCAMP-5. The focus of our work is on localising the camera within the scene, which is crucial for scene understanding and for any downstream robotics tasks. We present a localisation system that utilise the FPSP in two parts. First, a 6-DoF odometry system is introduced, which efficiently estimates its position against a known marker at over 400 FPS. Second, our work is extended to implement BIT-VO—6-DoF visual odometry system which operates under an unknown natural environment at 300 FPS.},
  archive      = {J_AR},
  author       = {Murai, Riku and Saeedi, Sajad and Kelly, Paul H. J.},
  doi          = {10.1007/s10514-023-10122-8},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1579-1592},
  shortjournal = {Auton. Robot.},
  title        = {High-frame rate homography and visual odometry by tracking binary features from the focal plane},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic task allocation approaches for coordinated
exploration of subterranean environments. <em>AR</em>, <em>47</em>(8),
1559–1577. (<a
href="https://doi.org/10.1007/s10514-023-10142-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the methods used by team CSIRO Data61 for multi-agent coordination and exploration in the DARPA Subterranean (SubT) Challenge. The SubT competition involved a single operator sending teams of robots to rapidly explore underground environments with severe navigation and communication challenges. Coordination was framed as a multi-robot task allocation (MRTA) problem to allow for a seamless integration of exploration with other required tasks. Methods for extending a consensus-based task allocation approach for an online and highly dynamic mission are discussed. Exploration tasks were generated from frontiers in a map of traversable space, and graph-based heuristics applied to guide the selection of exploration tasks. Results from simulation, field testing, and the final competition are presented. Team CSIRO Data61 tied for most points scored and achieved second place during the final SubT event.},
  archive      = {J_AR},
  author       = {O’Brien, Matthew and Williams, Jason and Chen, Shengkang and Pitt, Alex and Arkin, Ronald and Kottege, Navinda},
  doi          = {10.1007/s10514-023-10142-4},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1559-1577},
  shortjournal = {Auton. Robot.},
  title        = {Dynamic task allocation approaches for coordinated exploration of subterranean environments},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-robot geometric task-and-motion planning for
collaborative manipulation tasks. <em>AR</em>, <em>47</em>(8),
1537–1558. (<a
href="https://doi.org/10.1007/s10514-023-10148-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address multi-robot geometric task-and-motion planning (MR-GTAMP) problems in synchronous, monotone setups. The goal of the MR-GTAMP problem is to move objects with multiple robots to goal regions in the presence of other movable objects. We focus on collaborative manipulation tasks where the robots have to adopt intelligent collaboration strategies to be successful and effective, i.e., decide which robot should move which objects to which positions, and perform collaborative actions, such as handovers. To endow robots with these collaboration capabilities, we propose to first collect occlusion and reachability information for each robot by calling motion-planning algorithms. We then propose a method that uses the collected information to build a graph structure which captures the precedence of the manipulations of different objects and supports the implementation of a mixed-integer program to guide the search for highly effective collaborative task-and-motion plans. The search process for collaborative task-and-motion plans is based on a Monte-Carlo Tree Search (MCTS) exploration strategy to achieve exploration-exploitation balance. We evaluate our framework in two challenging MR-GTAMP domains and show that it outperforms two state-of-the-art baselines with respect to the planning time, the resulting plan length and the number of objects moved. We also show that our framework can be applied to underground mining operations where a robotic arm needs to coordinate with an autonomous roof bolter. We demonstrate plan execution in two roof-bolting scenarios both in simulation and on robots.},
  archive      = {J_AR},
  author       = {Zhang, Hejia and Chan, Shao-Hung and Zhong, Jie and Li, Jiaoyang and Kolapo, Peter and Koenig, Sven and Agioutantis, Zach and Schafrik, Steven and Nikolaidis, Stefanos},
  doi          = {10.1007/s10514-023-10148-y},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1537-1558},
  shortjournal = {Auton. Robot.},
  title        = {Multi-robot geometric task-and-motion planning for collaborative manipulation tasks},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Why ORB-SLAM is missing commonly occurring loop closures?
<em>AR</em>, <em>47</em>(8), 1519–1535. (<a
href="https://doi.org/10.1007/s10514-023-10149-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse, for the first time, the popular loop closing module of a well known and widely used open-source visual SLAM (ORB-SLAM) pipeline. Investigating failures in the loop closure module of visual SLAM is challenging since it consists of multiple building blocks. Our meticulous investigations have revealed a few interesting findings. Contrary to reported results, ORB-SLAM frequently misses large fraction of loop closures on public (KITTI, TUM RGB-D) datasets. One common assumption is, in such scenarios, the visual place recognition (vPR) block of the loop closure module is unable to find a suitable match due to extreme conditions (dynamic scene, viewpoint/scale changes). We report that native vPR of ORB-SLAM is not the sole reason for these failures. Although recent deep vPR alternatives achieve impressive matching performance, replacing native vPR with these deep alternatives will only partially improve loop closure performance of visual SLAM. Our findings suggest that the problem lies with the subsequent relative pose estimation module between the matching pair. ORB-SLAM3 has improved the recall of the original loop closing module. However, even in ORB-SLAM3, the loop closing module is the major reason behind loop closing failures. Surprisingly, using off-the-shelf ORB and SIFT based relative pose estimators (non real-time) manages to close most of the loops missed by ORB-SLAM. This significant performance gap between the two available methods suggests that ORB-SLAM’s pipeline can be further matured by focusing on the relative pose estimators, to improve loop closure performance, rather than investing more resources on improving vPR. We also evaluate deep alternatives for relative pose estimation in the context of loop closures. Interestingly, the performance of deep relocalization methods (e.g. MapNet) is worse than classic methods even in loop closures scenarios. This finding further supports the fundamental limitation of deep relocalization methods recently diagnosed. Finally, we expose bias in well-known public dataset (KITTI) due to which these commonly occurring failures have eluded the community. We augment the KITTI dataset with detailed loop closing labels. In order to compensate for the bias in the public datasets, we provide a challenging loop closure dataset which contains challenging yet commonly occurring indoor navigation scenarios with loop closures. We hope our findings and the accompanying dataset will help the community in further improving the popular ORB-SLAM’s pipeline.},
  archive      = {J_AR},
  author       = {Khaliq, Saran and Anjum, Muhammad Latif and Hussain, Wajahat and Khattak, Muhammad Uzair and Rasool, Momen},
  doi          = {10.1007/s10514-023-10149-x},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1519-1535},
  shortjournal = {Auton. Robot.},
  title        = {Why ORB-SLAM is missing commonly occurring loop closures?},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised dissimilarity-based fault detection method for
autonomous mobile robots. <em>AR</em>, <em>47</em>(8), 1503–1518. (<a
href="https://doi.org/10.1007/s10514-023-10144-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous robots are one of the critical components in modern manufacturing systems. For this reason, the uninterrupted operation of robots in manufacturing is important for the sustainability of autonomy. Detecting possible fault symptoms that may cause failures within a work environment will help to eliminate interrupted operations. When supervised learning methods are considered, obtaining and storing labeled, historical training data in a manufacturing environment with faults is a challenging task. In addition, sensors in mobile devices such as robots are exposed to different noisy external conditions in production environments affecting data labels and fault mapping. Furthermore, relying on a single sensor data for fault detection often causes false alarms for equipment monitoring. Our study takes requirements into consideration and proposes a new unsupervised machine-learning algorithm to detect possible operational faults encountered by autonomous mobile robots. The method suggests using an ensemble of multi-sensor information fusion at the decision level by voting to enhance decision reliability. The proposed technique relies on dissimilarity-based sensor data segmentation with an adaptive threshold control. It has been tested experimentally on an autonomous mobile robot. The experimental results show that the proposed method is effective for detecting operational anomalies. Furthermore, the proposed voting mechanism is also capable of eliminating false positives in case of a single source of information is utilized.},
  archive      = {J_AR},
  author       = {Kasap, Mahmut and Yılmaz, Metin and Çinar, Eyüp and Yazıcı, Ahmet},
  doi          = {10.1007/s10514-023-10144-2},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1503-1518},
  shortjournal = {Auton. Robot.},
  title        = {Unsupervised dissimilarity-based fault detection method for autonomous mobile robots},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-based navigation in real-world environments via
multiple mid-level representations: Fusion models, benchmark and
efficient evaluation. <em>AR</em>, <em>47</em>(8), 1483–1502. (<a
href="https://doi.org/10.1007/s10514-023-10147-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot visual navigation is a relevant research topic. Current deep navigation models conveniently learn the navigation policies in simulation, given the large amount of experience they need to collect. Unfortunately, the resulting models show a limited generalization ability when deployed in the real world. In this work we explore solutions to facilitate the development of visual navigation policies trained in simulation that can be successfully transferred in the real world. We first propose an efficient evaluation tool to reproduce realistic navigation episodes in simulation. We then investigate a variety of deep fusion architectures to combine a set of mid-level representations, with the aim of finding the best merge strategy that maximize the real world performances. Our experiments, performed both in simulation and on a robotic platform, show the effectiveness of the considered mid-level representations-based models and confirm the reliability of the evaluation tool. The 3D models of the environment and the code of the validation tool are publicly available at the following link: https://iplab.dmi.unict.it/EmbodiedVN/ .},
  archive      = {J_AR},
  author       = {Rosano, Marco and Furnari, Antonino and Gulino, Luigi and Santoro, Corrado and Farinella, Giovanni Maria},
  doi          = {10.1007/s10514-023-10147-z},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1483-1502},
  shortjournal = {Auton. Robot.},
  title        = {Image-based navigation in real-world environments via multiple mid-level representations: Fusion models, benchmark and efficient evaluation},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoLoMa: Robust loco-manipulation for quadruped robots with
arms. <em>AR</em>, <em>47</em>(8), 1463–1481. (<a
href="https://doi.org/10.1007/s10514-023-10146-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deployment of robotic systems in the real world requires a certain level of robustness in order to deal with uncertainty factors, such as mismatches in the dynamics model, noise in sensor readings, and communication delays. Some approaches tackle these issues reactively at the control stage. However, regardless of the controller, online motion execution can only be as robust as the system capabilities allow at any given state. This is why it is important to have good motion plans to begin with, where robustness is considered proactively. To this end, we propose a metric (derived from first principles) for representing robustness against external disturbances. We then use this metric within our trajectory optimization framework for solving complex loco-manipulation tasks. Through our experiments, we show that trajectories generated using our approach can resist a greater range of forces originating from any possible direction. By using our method, we can compute trajectories that solve tasks as effectively as before, with the added benefit of being able to counteract stronger disturbances in worst-case scenarios.},
  archive      = {J_AR},
  author       = {Ferrolho, Henrique and Ivan, Vladimir and Merkt, Wolfgang and Havoutis, Ioannis and Vijayakumar, Sethu},
  doi          = {10.1007/s10514-023-10146-0},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1463-1481},
  shortjournal = {Auton. Robot.},
  title        = {RoLoMa: Robust loco-manipulation for quadruped robots with arms},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpaTiaL: Monitoring and planning of robotic tasks using
spatio-temporal logic specifications. <em>AR</em>, <em>47</em>(8),
1439–1462. (<a
href="https://doi.org/10.1007/s10514-023-10145-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many tasks require robots to manipulate objects while satisfying a complex interplay of spatial and temporal constraints. For instance, a table setting robot first needs to place a mug and then fill it with coffee, while satisfying spatial relations such as forks need to placed left of plates. We propose the spatio-temporal framework SpaTiaL that unifies the specification, monitoring, and planning of object-oriented robotic tasks in a robot-agnostic fashion. SpaTiaL is able to specify diverse spatial relations between objects and temporal task patterns. Our experiments with recorded data, simulations, and real robots demonstrate how SpaTiaL provides real-time monitoring and facilitates online planning. SpaTiaL is open source and easily expandable to new object relations and robotic applications.},
  archive      = {J_AR},
  author       = {Pek, Christian and Schuppe, Georg Friedrich and Esposito, Francesco and Tumova, Jana and Kragic, Danica},
  doi          = {10.1007/s10514-023-10145-1},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1439-1462},
  shortjournal = {Auton. Robot.},
  title        = {SpaTiaL: Monitoring and planning of robotic tasks using spatio-temporal logic specifications},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for shared autonomy drone landings.
<em>AR</em>, <em>47</em>(8), 1419–1438. (<a
href="https://doi.org/10.1007/s10514-023-10143-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novice pilots find it difficult to operate and land unmanned aerial vehicles (UAVs), due to the complex UAV dynamics, challenges in depth perception, lack of expertise with the control interface and additional disturbances from the ground effect. Therefore we propose a shared autonomy approach to assist pilots in safely landing a UAV under conditions where depth perception is difficult and safe landing zones are limited. Our approach is comprised of two modules: a perception module that encodes information onto a compressed latent representation using two RGB-D cameras and a policy module that is trained with the reinforcement learning algorithm TD3 to discern the pilot’s intent and to provide control inputs that augment the user’s input to safely land the UAV. The policy module is trained in simulation using a population of simulated users. Simulated users are sampled from a parametric model with four parameters, which model a pilot’s tendency to conform to the assistant, proficiency, aggressiveness and speed. We conduct a user study ( $$n=28$$ ) where human participants were tasked with landing a physical UAV on one of several platforms under challenging viewing conditions. The assistant, trained with only simulated user data, improved task success rate from 51.4 to 98.2% despite being unaware of the human participants’ goal or the structure of the environment a priori. With the proposed assistant, regardless of prior piloting experience, participants performed with a proficiency greater than the most experienced unassisted participants.},
  archive      = {J_AR},
  author       = {Backman, Kal and Kulić, Dana and Chung, Hoam},
  doi          = {10.1007/s10514-023-10143-3},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1419-1438},
  shortjournal = {Auton. Robot.},
  title        = {Reinforcement learning for shared autonomy drone landings},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chasing millimeters: Design, navigation and state estimation
for precise in-flight marking on ceilings. <em>AR</em>, <em>47</em>(8),
1405–1418. (<a
href="https://doi.org/10.1007/s10514-023-10141-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise markings for drilling and assembly are crucial, laborious construction tasks. Aerial robots with suitable end-effectors are capable of markings at the millimeter scale. However, so far, they have only been demonstrated under laboratory conditions where rigid state estimation and navigation assumptions do not impede robustness and accuracy. This paper presents a complete aerial layouting system capable of precise markings on-site under realistic conditions. We use a compliant actuated end-effector on an omnidirectional flying base. Combining a two-stage factor-graph state estimator with a Riemannian Motion Policy-based navigation stack, we avoid the need for a globally consistent state estimate and increase robustness. The policy-based navigation is structured into individual behaviors in different state spaces. Through a comprehensive study, we show that the system creates highly precise markings at a relative precision of 1.5 mm and a global accuracy of 5–6 mm and discuss the results in the context of future construction robotics.},
  archive      = {J_AR},
  author       = {Lanegger, Christian and Pantic, Michael and Bähnemann, Rik and Siegwart, Roland and Ott, Lionel},
  doi          = {10.1007/s10514-023-10141-5},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1405-1418},
  shortjournal = {Auton. Robot.},
  title        = {Chasing millimeters: Design, navigation and state estimation for precise in-flight marking on ceilings},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning with model-based feedforward inputs
for robotic table tennis. <em>AR</em>, <em>47</em>(8), 1387–1403. (<a
href="https://doi.org/10.1007/s10514-023-10140-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We rethink the traditional reinforcement learning approach, which is based on optimizing over feedback policies, and propose a new framework that optimizes over feedforward inputs instead. This not only mitigates the risk of destabilizing the system during training but also reduces the bulk of the learning to a supervised learning task. As a result, efficient and well-understood supervised learning techniques can be applied and are tuned using a validation data set. The labels are generated with a variant of iterative learning control, which also includes prior knowledge about the underlying dynamics. Our framework is applied for intercepting and returning ping-pong balls that are played to a four-degrees-of-freedom robotic arm in real-world experiments. The robot arm is driven by pneumatic artificial muscles, which makes the control and learning tasks challenging. We highlight the potential of our framework by comparing it to a reinforcement learning approach that optimizes over feedback policies. We find that our framework achieves a higher success rate for the returns ( $$100\%$$ vs. $$96\%$$ , on 107 consecutive trials, see https://youtu.be/kR9jowEH7PY ) while requiring only about one tenth of the samples during training. We also find that our approach is able to deal with a variant of different incoming trajectories.},
  archive      = {J_AR},
  author       = {Ma, Hao and Büchler, Dieter and Schölkopf, Bernhard and Muehlebach, Michael},
  doi          = {10.1007/s10514-023-10140-6},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1387-1403},
  shortjournal = {Auton. Robot.},
  title        = {Reinforcement learning with model-based feedforward inputs for robotic table tennis},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UVS: Underwater visual SLAM—a robust monocular visual SLAM
system for lifelong underwater operations. <em>AR</em>, <em>47</em>(8),
1367–1385. (<a
href="https://doi.org/10.1007/s10514-023-10138-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a visual simultaneous localization and mapping (VSLAM/visual SLAM) system called underwater visual SLAM (UVS) system is presented, specifically tailored for camera-only navigation in natural underwater environments. The UVS system is particularly optimized towards precision and robustness, as well as lifelong operations. We build upon Oriented features from accelerated segment test and Rotated Binary robust independent elementary features simultaneous localization and mapping (ORB-SLAM) and improve the accuracy by performing an exact search in the descriptor space during triangulation and the robustness by utilizing a unified initialization method and a motion model. In addition, we present a scale-agnostic station-keeping detection, which aims to optimize the map and poses during station-keeping, and a pruning strategy, which takes into account the point’s age and distance to the active keyframe. An exhaustive evaluation is presented to the reader, using a total of 38 in-air and underwater sequences.},
  archive      = {J_AR},
  author       = {Leonardi, Marco and Stahl, Annette and Brekke, Edmund Førland and Ludvigsen, Martin},
  doi          = {10.1007/s10514-023-10138-0},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1367-1385},
  shortjournal = {Auton. Robot.},
  title        = {UVS: Underwater visual SLAM—a robust monocular visual SLAM system for lifelong underwater operations},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text2Motion: From natural language instructions to feasible
plans. <em>AR</em>, <em>47</em>(8), 1345–1365. (<a
href="https://doi.org/10.1007/s10514-023-10131-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Text2Motion, a language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Given a natural language instruction, our framework constructs both a task- and motion-level plan that is verified to reach inferred symbolic goals. Text2Motion uses feasibility heuristics encoded in Q-functions of a library of skills to guide task planning with Large Language Models. Whereas previous language-based planners only consider the feasibility of individual skills, Text2Motion actively resolves geometric dependencies spanning skill sequences by performing geometric feasibility planning during its search. We evaluate our method on a suite of problems that require long-horizon reasoning, interpretation of abstract goals, and handling of partial affordance perception. Our experiments show that Text2Motion can solve these challenging problems with a success rate of 82%, while prior state-of-the-art language-based planning methods only achieve 13%. Text2Motion thus provides promising generalization characteristics to semantically diverse sequential manipulation tasks with geometric dependencies between skills. Qualitative results are made available at https://sites.google.com/stanford.edu/text2motion .},
  archive      = {J_AR},
  author       = {Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},
  doi          = {10.1007/s10514-023-10131-7},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1345-1365},
  shortjournal = {Auton. Robot.},
  title        = {Text2Motion: From natural language instructions to feasible plans},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-directional interaction force control with an aerial
manipulator under external disturbances. <em>AR</em>, <em>47</em>(8),
1325–1343. (<a
href="https://doi.org/10.1007/s10514-023-10128-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve accuracy and robustness of interactive aerial robots, the knowledge of the forces acting on the platform is of uttermost importance. The robot should distinguish interaction forces from external disturbances in order to be compliant with the firsts and reject the seconds. This represents a challenge since disturbances might be of different nature (physical contact, aerodynamic, modeling errors) and be applied to different points of the robot. This work presents a new $$\hbox {extended Kalman filter (EKF)}$$ based estimator for both external disturbance and interaction forces. The estimator fuses information coming from the system’s dynamic model and it’s state with wrench measurements coming from a Force-Torque sensor. This allows for robust interaction control at the tool’s tip even in presence of external disturbance wrenches acting on the platform. We employ the filter estimates in a novel hybrid force/motion controller to perform force tracking not only along the tool direction, but from any platform’s orientation, without losing the stability of the pose controller. The proposed framework is extensively tested on an omnidirectional aerial manipulator (AM) performing push and slide operations and transitioning between different interaction surfaces, while subject to external disturbances. The experiments are done equipping the AM with two different tools: a rigid interaction stick and an actuated delta manipulator, showing the generality of the approach. Moreover, the estimation results are compared to a state-of-the-art momentum-based estimator, clearly showing the superiority of the EKF approach.},
  archive      = {J_AR},
  author       = {Malczyk, Grzegorz and Brunner, Maximilian and Cuniato, Eugenio and Tognon, Marco and Siegwart, Roland},
  doi          = {10.1007/s10514-023-10128-2},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1325-1343},
  shortjournal = {Auton. Robot.},
  title        = {Multi-directional interaction force control with an aerial manipulator under external disturbances},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Formation control for autonomous fixed-wing air vehicles
with strict speed constraints. <em>AR</em>, <em>47</em>(8), 1299–1323.
(<a href="https://doi.org/10.1007/s10514-023-10126-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a formation-control algorithm for autonomous fixed-wing air vehicles. The desired inter-vehicle positions are time-varying, and we assume that at least one vehicle has access to a measurement its position relative to the leader, which can be a physical or virtual member of the formation. Each vehicle is modeled with extended unicycle dynamics that include orientation kinematics on SO(3), speed dynamics, and strict constraints on speed (i.e., ground speed). The analytic result shows that the vehicles converge exponentially to the desired relative positions with each other and the leader. We also show that each vehicle’s speed satisfies the speed constraints. The formation algorithm is demonstrated in software-in-the-loop (SITL) simulations and experiments with fixed-wing air vehicles. To implement the formation-control algorithm, each vehicle has middle-loop controllers to determine roll, pitch, and throttle commands from the outer-loop formation control. We present SITL simulations with 4 fixed-wing air vehicles that demonstrate formation control with different communication structures. Finally, we present formation-control experiments with up to 3 fixed-wing air vehicles.},
  archive      = {J_AR},
  author       = {Heintz, Christopher and Bailey, Sean C. C. and Hoagg, Jesse B.},
  doi          = {10.1007/s10514-023-10126-4},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1299-1323},
  shortjournal = {Auton. Robot.},
  title        = {Formation control for autonomous fixed-wing air vehicles with strict speed constraints},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning scalable and efficient communication policies for
multi-robot collision avoidance. <em>AR</em>, <em>47</em>(8), 1275–1297.
(<a href="https://doi.org/10.1007/s10514-023-10127-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized multi-robot systems typically perform coordinated motion planning by constantly broadcasting their intentions to avoid collisions. However, the risk of collision between robots varies as they move and communication may not always be needed. This paper presents an efficient communication method that addresses the problem of “when” and “with whom” to communicate in multi-robot collision avoidance scenarios. In this approach, each robot learns to reason about other robots’ states and considers the risk of future collisions before asking for the trajectory plans of other robots. We introduce a new neural architecture for the learned communication policy which allows our method to be scalable. We evaluate and verify the proposed communication strategy in simulation with up to twelve quadrotors, and present results on the zero-shot generalization/robustness capabilities of the policy in different scenarios. We demonstrate that our policy (learned in a simulated environment) can be successfully transferred to real robots.},
  archive      = {J_AR},
  author       = {Serra-Gómez, Álvaro and Zhu, Hai and Brito, Bruno and Böhmer, Wendelin and Alonso-Mora, Javier},
  doi          = {10.1007/s10514-023-10127-3},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1275-1297},
  shortjournal = {Auton. Robot.},
  title        = {Learning scalable and efficient communication policies for multi-robot collision avoidance},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Humans as path-finders for mobile robots using
teach-by-showing navigation. <em>AR</em>, <em>47</em>(8), 1255–1273. (<a
href="https://doi.org/10.1007/s10514-023-10125-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important barriers towards a widespread use of mobile robots in unstructured, human populated and possibly a-priori unknown work environments is the ability to plan a safe path. In this paper, we propose to delegate this activity to a human operator that walks in front of the robot marking with her/his footsteps the path to be followed. The implementation of this approach requires a high degree of robustness in locating the specific person to be followed (the path-finder). We propose a three phases approach to fulfil this goal: 1. Identification and tracking of the person in the image space, 2. Sensor fusion between camera data and laser sensors, 3. Point interpolation with continuous curvature paths. The approach is described in the paper and extensively validated with experimental results.},
  archive      = {J_AR},
  author       = {Antonucci, Alessandro and Bevilacqua, Paolo and Leonardi, Stefano and Paolopoli, Luigi and Fontanelli, Daniele},
  doi          = {10.1007/s10514-023-10125-5},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1255-1273},
  shortjournal = {Auton. Robot.},
  title        = {Humans as path-finders for mobile robots using teach-by-showing navigation},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust bounded control scheme for quadrotor vehicles under
high dynamic disturbances. <em>AR</em>, <em>47</em>(8), 1245–1254. (<a
href="https://doi.org/10.1007/s10514-023-10124-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an optimal bounded robust control algorithm for secure autonomous navigation in quadcopter vehicles is proposed. The controller is developed combining two parts; one dedicated to stabilize the closed-loop system and the second one for dealing and estimating external disturbances as well unknown nonlinearities inherent to the real system’s operations. For bounding the energy used by the system during a mission and, without losing its robustness properties, the quadratic problem formulation is used considering the actuators system constraints. The resulting optimal bounded control scheme improves considerably the stability and robustness of the closed-loop system and at the same time bounds the motor control inputs. The controller is validated in real-time flights and in unconventional conditions for high wind-gusts and Loss of Effectiveness in two rotors. The experimental results demonstrate the good performance of the proposed controller in both scenarios.},
  archive      = {J_AR},
  author       = {Betancourt, J. and Castillo, P. and García, P. and Balaguer, V. and Lozano, R.},
  doi          = {10.1007/s10514-023-10124-6},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1245-1254},
  shortjournal = {Auton. Robot.},
  title        = {Robust bounded control scheme for quadrotor vehicles under high dynamic disturbances},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and control of BRAVER: A bipedal robot actuated via
proprioceptive electric motors. <em>AR</em>, <em>47</em>(8), 1229–1243.
(<a href="https://doi.org/10.1007/s10514-023-10117-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design and control of a high-speed running bipedal robot, BRAVER. The robot, which weighs 8.6 kg and is 0.36 m tall, has six active degrees, all of which are driven by custom back-driveable modular actuators, which enable high-bandwidth force control and proprioceptive torque feedback. We present the details of the hardware design, including the actuator, leg, foot, and onboard control systems, as well as the locomotion controller design for high dynamic tasks and improving robustness. We have demonstrated the performance of BRAVER using a series of experiments, including multi-terrains walking, up and down 15 $$^{\circ }$$ slopes, pushing recovery, and running. The maximum running speed of BRAVER reaches 1.75 m/s.},
  archive      = {J_AR},
  author       = {Zhu, Zhengguo and Zhu, Weiliang and Zhang, Guoteng and Chen, Teng and Li, Yibin and Rong, Xuewen and Song, Rui and Qin, Daoling and Hua, Qiang and Ma, Shugen},
  doi          = {10.1007/s10514-023-10117-5},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1229-1243},
  shortjournal = {Auton. Robot.},
  title        = {Design and control of BRAVER: A bipedal robot actuated via proprioceptive electric motors},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-based neural learning for quadrotor control.
<em>AR</em>, <em>47</em>(8), 1213–1228. (<a
href="https://doi.org/10.1007/s10514-023-10115-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of a simple and adaptive flight controller is a real challenge in aerial robotics. A simple flight controller often generates a poor flight tracking performance. Furthermore, adaptive algorithms might be costly in time and resources or deep learning based methods may cause instability problems, for instance in presence of disturbances. In this paper, we propose an event-based neural learning control strategy that combines the use of a standard cascaded flight controller enhanced by a deep neural network that learns the disturbances in order to improve the tracking performance. The strategy relies on two events: one allowing the improvement of tracking errors and the second to ensure closed-loop system stability. After a validation of the proposed strategy in a ROS/Gazebo simulation environment, its effectiveness is confirmed in real experiments in the presence of wind disturbance.},
  archive      = {J_AR},
  author       = {Carvalho, Estéban and Susbielle, Pierre and Marchand, Nicolas and Hably, Ahmad and Dibangoye, Jilles S.},
  doi          = {10.1007/s10514-023-10115-7},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1213-1228},
  shortjournal = {Auton. Robot.},
  title        = {Event-based neural learning for quadrotor control},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sim-to-real transfer of co-optimized soft robot crawlers.
<em>AR</em>, <em>47</em>(8), 1195–1211. (<a
href="https://doi.org/10.1007/s10514-023-10130-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work provides a complete framework for the simulation, co-optimization, and sim-to-real transfer of the design and control of soft legged robots. Soft robots have “mechanical intelligence”: the ability to passively exhibit behaviors that would otherwise be difficult to program. Exploiting this capacity requires consideration of the coupling between design and control. Co-optimization provides a way to reason over this coupling. Yet, it is difficult to achieve simulations that are both sufficiently accurate to allow for sim-to-real transfer and fast enough for contemporary co-optimization algorithms. We describe a modularized model order reduction algorithm that improves simulation efficiency, while preserving the accuracy required to learn effective soft robot design and control. We propose a reinforcement learning-based co-optimization framework that identifies several soft crawling robots that outperform an expert baseline with zero-shot sim-to-real transfer. We study generalization of the framework to new terrains, and the efficacy of domain randomization as a means to improve sim-to-real transfer.},
  archive      = {J_AR},
  author       = {Schaff, Charles and Sedal, Audrey and Ni, Shiyao and Walter, Matthew R.},
  doi          = {10.1007/s10514-023-10130-8},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1195-1211},
  shortjournal = {Auton. Robot.},
  title        = {Sim-to-real transfer of co-optimized soft robot crawlers},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On robot grasp learning using equivariant models.
<em>AR</em>, <em>47</em>(8), 1175–1193. (<a
href="https://doi.org/10.1007/s10514-023-10112-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world grasp detection is challenging due to the stochasticity in grasp dynamics and the noise in hardware. Ideally, the system would adapt to the real world by training directly on physical systems. However, this is generally difficult due to the large amount of training data required by most grasp learning models. In this paper, we note that the planar grasp function is $$\textrm{SE}(2)$$ -equivariant and demonstrate that this structure can be used to constrain the neural network used during learning. This creates an inductive bias that can significantly improve the sample efficiency of grasp learning and enable end-to-end training from scratch on a physical robot with as few as 600 grasp attempts. We call this method Symmetric Grasp learning (SymGrasp) and show that it can learn to grasp “from scratch” in less that 1.5 h of physical robot time. This paper represents an expanded and revised version of the conference paper Zhu et al. (2022).},
  archive      = {J_AR},
  author       = {Zhu, Xupeng and Wang, Dian and Su, Guanang and Biza, Ondrej and Walters, Robin and Platt, Robert},
  doi          = {10.1007/s10514-023-10112-w},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1175-1193},
  shortjournal = {Auton. Robot.},
  title        = {On robot grasp learning using equivariant models},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pseudo-trilateral adversarial training for domain adaptive
traversability prediction. <em>AR</em>, <em>47</em>(8), 1155–1174. (<a
href="https://doi.org/10.1007/s10514-023-10123-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traversability prediction is a fundamental perception capability for autonomous navigation. Deep neural networks (DNNs) have been widely used to predict traversability during the last decade. The performance of DNNs is significantly boosted by exploiting a large amount of data. However, the diversity of data in different domains imposes significant gaps in the prediction performance. In this work, we make efforts to reduce the gaps by proposing a novel pseudo-trilateral adversarial model that adopts a coarse-to-fine alignment (CALI) to perform unsupervised domain adaptation (UDA). Our aim is to transfer the perception model with high data efficiency, eliminate the prohibitively expensive data labeling, and improve the generalization capability during the adaptation from easy-to-access source domains to various challenging target domains. Existing UDA methods usually adopt a bilateral zero-sum game structure. We prove that our CALI model—a pseudo-trilateral game structure is advantageous over existing bilateral game structures. This proposed work bridges theoretical analyses and algorithm designs, leading to an efficient UDA model with easy and stable training. We further develop a variant of CALI—Informed CALI, which is inspired by the recent success of mixup data augmentation techniques and mixes informative regions based on the results of CALI. This mixture step provides an explicit bridging between the two domains and exposes under-performing classes more during training. We show the superiorities of our proposed models over multiple baselines in several challenging domain adaptation setups. To further validate the effectiveness of our proposed models, we then combine our perception model with a visual planner to build a navigation system and show the high reliability of our model in complex natural environments.},
  archive      = {J_AR},
  author       = {Chen, Zheng and Pushp, Durgakant and Gregory, Jason M. and Liu, Lantao},
  doi          = {10.1007/s10514-023-10123-7},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1155-1174},
  shortjournal = {Auton. Robot.},
  title        = {Pseudo-trilateral adversarial training for domain adaptive traversability prediction},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FuseBot: Mechanical search of rigid and deformable objects
via multi-modal perception. <em>AR</em>, <em>47</em>(8), 1137–1154. (<a
href="https://doi.org/10.1007/s10514-023-10137-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanical search is a robotic problem where a robot needs to retrieve a target item that is partially or fully-occluded from its camera. State-of-the-art approaches for mechanical search either require an expensive search process to find the target item, or they require the item to be tagged with a radio frequency identification tag (e.g., RFID), making their approach beneficial only to tagged items in the environment. We present FuseBot, the first robotic system for RF-Visual mechanical search that enables efficient retrieval of both RF-tagged and untagged items in a pile. Rather than requiring all target items in a pile to be RF-tagged, FuseBot leverages the mere existence of an RF-tagged item in the pile to benefit both tagged and untagged items. Our design introduces two key innovations. The first is RF-Visual Mapping, a technique that identifies and locates RF-tagged items in a pile and uses this information to construct an RF-Visual occupancy distribution map. The second is RF-Visual Extraction, a policy formulated as an optimization problem that minimizes the number of actions required to extract the target object by accounting for the probabilistic occupancy distribution, the expected grasp quality, and the expected information gain from future actions. We built a real-time end-to-end prototype of our system on a UR5e robotic arm with in-hand vision and RF perception modules. We conducted over 200 real-world experimental trials to evaluate FuseBot and compare its performance to a state-of-the-art vision-based system named X-Ray (Danielczuk et al., in: 2020 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE, 2020). Our experimental results demonstrate that FuseBot outperforms X-Ray’s efficiency by more than 40% in terms of the number of actions required for successful mechanical search. Furthermore, in comparison to X-Ray’s success rate of 84%, FuseBot achieves a success rate of 95% in retrieving untagged items, demonstrating for the first time that the benefits of RF perception extend beyond tagged objects in the mechanical search problem.},
  archive      = {J_AR},
  author       = {Boroushaki, Tara and Dodds, Laura and Naeem, Nazish and Adib, Fadel},
  doi          = {10.1007/s10514-023-10137-1},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1137-1154},
  shortjournal = {Auton. Robot.},
  title        = {FuseBot: Mechanical search of rigid and deformable objects via multi-modal perception},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of space-variant and active vision mechanisms
for resource-constrained human inspired robotic vision. <em>AR</em>,
<em>47</em>(8), 1119–1135. (<a
href="https://doi.org/10.1007/s10514-023-10107-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to explore and understand the surrounding environment in an efficient manner, humans have developed a set of space-variant vision mechanisms that allow them to actively attend different locations in the surrounding environment and compensate for memory, neuronal transmission bandwidth and computational limitations in the brain. Similarly, humanoid robots deployed in everyday environments have limited on-board resources, and are faced with increasingly complex tasks that require interaction with objects arranged in many possible spatial configurations. The main goal of this work is to describe and overview biologically inspired, space-variant human visual mechanism benefits, when combined with state-of-the-art algorithms for different visual tasks (e.g. object detection), ranging from low-level hardwired attention vision (i.e. foveal vision) to high-level visual attention mechanisms. We overview the state-of-the-art in biologically plausible space-variant resource-constrained vision architectures, namely for active recognition and localization tasks.},
  archive      = {J_AR},
  author       = {de Figueiredo, Rui Pimentel and Bernardino, Alexandre},
  doi          = {10.1007/s10514-023-10107-7},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1119-1135},
  shortjournal = {Auton. Robot.},
  title        = {An overview of space-variant and active vision mechanisms for resource-constrained human inspired robotic vision},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to summarize and answer questions about a virtual
robot’s past actions. <em>AR</em>, <em>47</em>(8), 1103–1118. (<a
href="https://doi.org/10.1007/s10514-023-10134-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent’s past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization.},
  archive      = {J_AR},
  author       = {DeChant, Chad and Akinola, Iretiayo and Bauer, Daniel},
  doi          = {10.1007/s10514-023-10134-4},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1103-1118},
  shortjournal = {Auton. Robot.},
  title        = {Learning to summarize and answer questions about a virtual robot’s past actions},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TidyBot: Personalized robot assistance with large language
models. <em>AR</em>, <em>47</em>(8), 1087–1102. (<a
href="https://doi.org/10.1007/s10514-023-10139-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people’s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.},
  archive      = {J_AR},
  author       = {Wu, Jimmy and Antonova, Rika and Kan, Adam and Lepert, Marion and Zeng, Andy and Song, Shuran and Bohg, Jeannette and Rusinkiewicz, Szymon and Funkhouser, Thomas},
  doi          = {10.1007/s10514-023-10139-z},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1087-1102},
  shortjournal = {Auton. Robot.},
  title        = {TidyBot: Personalized robot assistance with large language models},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large language models for chemistry robotics. <em>AR</em>,
<em>47</em>(8), 1057–1086. (<a
href="https://doi.org/10.1007/s10514-023-10136-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an approach to automate chemistry experiments using robots by translating natural language instructions into robot-executable plans, using large language models together with task and motion planning. Adding natural language interfaces to autonomous chemistry experiment systems lowers the barrier to using complicated robotics systems and increases utility for non-expert users, but translating natural language experiment descriptions from users into low-level robotics languages is nontrivial. Furthermore, while recent advances have used large language models to generate task plans, reliably executing those plans in the real world by an embodied agent remains challenging. To enable autonomous chemistry experiments and alleviate the workload of chemists, robots must interpret natural language commands, perceive the workspace, autonomously plan multi-step actions and motions, consider safety precautions, and interact with various laboratory equipment. Our approach, CLAIRify, combines automatic iterative prompting with program verification to ensure syntactically valid programs in a data-scarce domain-specific language that incorporates environmental constraints. The generated plan is executed through solving a constrained task and motion planning problem using PDDLStream solvers to prevent spillages of liquids as well as collisions in chemistry labs. We demonstrate the effectiveness of our approach in planning chemistry experiments, with plans successfully executed on a real robot using a repertoire of robot skills and lab tools. Specifically, we showcase the utility of our framework in pouring skills for various materials and two fundamental chemical experiments for materials synthesis: solubility and recrystallization. Further details about CLAIRify can be found at https://ac-rad.github.io/clairify/ .},
  archive      = {J_AR},
  author       = {Yoshikawa, Naruki and Skreta, Marta and Darvish, Kourosh and Arellano-Rubach, Sebastian and Ji, Zhi and Bjørn Kristensen, Lasse and Li, Andrew Zou and Zhao, Yuchi and Xu, Haoping and Kuramshin, Artur and Aspuru-Guzik, Alán and Shkurti, Florian and Garg, Animesh},
  doi          = {10.1007/s10514-023-10136-2},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1057-1086},
  shortjournal = {Auton. Robot.},
  title        = {Large language models for chemistry robotics},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic anomaly detection with large language models.
<em>AR</em>, <em>47</em>(8), 1035–1055. (<a
href="https://doi.org/10.1007/s10514-023-10132-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As robots acquire increasingly sophisticated skills and see increasingly complex and varied environments, the threat of an edge case or anomalous failure is ever present. For example, Tesla cars have seen interesting failure modes ranging from autopilot disengagements due to inactive traffic lights carried by trucks to phantom braking caused by images of stop signs on roadside billboards. These system-level failures are not due to failures of any individual component of the autonomy stack but rather system-level deficiencies in semantic reasoning. Such edge cases, which we call semantic anomalies, are simple for a human to disentangle yet require insightful reasoning. To this end, we study the application of large language models (LLMs), endowed with broad contextual understanding and reasoning capabilities, to recognize such edge cases and introduce a monitoring framework for semantic anomaly detection in vision-based policies. Our experiments apply this framework to a finite state machine policy for autonomous driving and a learned policy for object manipulation. These experiments demonstrate that the LLM-based monitor can effectively identify semantic anomalies in a manner that shows agreement with human reasoning. Finally, we provide an extended discussion on the strengths and weaknesses of this approach and motivate a research outlook on how we can further use foundation models for semantic anomaly detection. Our project webpage can be found at https://sites.google.com/view/llm-anomaly-detection .},
  archive      = {J_AR},
  author       = {Elhafsi, Amine and Sinha, Rohan and Agia, Christopher and Schmerling, Edward and Nesnas, Issa A. D. and Pavone, Marco},
  doi          = {10.1007/s10514-023-10132-6},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1035-1055},
  shortjournal = {Auton. Robot.},
  title        = {Semantic anomaly detection with large language models},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning modular language-conditioned robot policies through
attention. <em>AR</em>, <em>47</em>(8), 1013–1033. (<a
href="https://doi.org/10.1007/s10514-023-10129-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training language-conditioned policies is typically time-consuming and resource-intensive. Additionally, the resulting controllers are tailored to the specific robot they were trained on, making it difficult to transfer them to other robots with different dynamics. To address these challenges, we propose a new approach called Hierarchical Modularity, which enables more efficient training and subsequent transfer of such policies across different types of robots. The approach incorporates Supervised Attention which bridges the gap between modular and end-to-end learning by enabling the re-use of functional building blocks. In this contribution, we build upon our previous work, showcasing the extended utilities and improved performance by expanding the hierarchy to include new tasks and introducing an automated pipeline for synthesizing a large quantity of novel objects. We demonstrate the effectiveness of this approach through extensive simulated and real-world robot manipulation experiments.},
  archive      = {J_AR},
  author       = {Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Ben Amor, Heni and Stepputtis, Simon},
  doi          = {10.1007/s10514-023-10129-1},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {1013-1033},
  shortjournal = {Auton. Robot.},
  title        = {Learning modular language-conditioned robot policies through attention},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProgPrompt: Program generation for situated robot task
planning using large language models. <em>AR</em>, <em>47</em>(8),
999–1012. (<a href="https://doi.org/10.1007/s10514-023-10135-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at progprompt.github.io},
  archive      = {J_AR},
  author       = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  doi          = {10.1007/s10514-023-10135-3},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {999-1012},
  shortjournal = {Auton. Robot.},
  title        = {ProgPrompt: Program generation for situated robot task planning using large language models},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating action knowledge and LLMs for task planning and
situation handling in open worlds. <em>AR</em>, <em>47</em>(8), 981–997.
(<a href="https://doi.org/10.1007/s10514-023-10133-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task planning systems have been developed to help robots use human knowledge (about actions) to complete long-horizon tasks. Most of them have been developed for “closed worlds” while assuming the robot is provided with complete world knowledge. However, the real world is generally open, and the robots frequently encounter unforeseen situations that can potentially break theplanner’s completeness. Could we leverage the recent advances on pre-trained Large Language Models (LLMs) to enable classical planning systems to deal with novel situations? This paper introduces a novel framework, called COWP, for open-world task planning and situation handling. COWP dynamically augments the robot’s action knowledge, including the preconditions and effects of actions, with task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and is grounded to specific domains via action knowledge. For systematic evaluations, we collected a dataset that includes 1085 execution-time situations. Each situation corresponds to a state instance wherein a robot is potentially unable to complete a task using a solution that normally works. Experimental results show that our approach outperforms competitive baselines from the literature in the success rate of service tasks. Additionally, we have demonstrated COWP using a mobile manipulator. Supplementary materials are available at: https://cowplanning.github.io/},
  archive      = {J_AR},
  author       = {Ding, Yan and Zhang, Xiaohan and Amiri, Saeid and Cao, Nieqing and Yang, Hao and Kaminski, Andy and Esselink, Chad and Zhang, Shiqi},
  doi          = {10.1007/s10514-023-10133-5},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {981-997},
  shortjournal = {Auton. Robot.},
  title        = {Integrating action knowledge and LLMs for task planning and situation handling in open worlds},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AuRo special issue on large language models in robotics
guest editorial. <em>AR</em>, <em>47</em>(8), 979–980. (<a
href="https://doi.org/10.1007/s10514-023-10153-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AR},
  doi          = {10.1007/s10514-023-10153-1},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {8},
  pages        = {979-980},
  shortjournal = {Auton. Robot.},
  title        = {AuRo special issue on large language models in robotics guest editorial},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical characterization of ODE models of swarm
behaviors in common foraging scenarios. <em>AR</em>, <em>47</em>(7),
963–977. (<a href="https://doi.org/10.1007/s10514-023-10121-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a large class of real-world problems, such as warehouse transport, at different scales, swarm densities, etc., that can be characterized as Central Place Foraging Problems (CPFPs). We contribute to swarm engineering by designing an Ordinary Differential Equation (ODE) model that strives to capture the underlying behavioral dynamics of the CPFP in these application areas. Our simulation results show that a hybrid ODE modeling approach combining analytic parameter calculations and post-hoc (i.e., after running experiments) parameter fitting can be just as effective as a purely post-hoc approach to computing parameters via simulations, while requiring less tuning and iterative refinement. This makes it easier to design systems with provable bounds on behavior. Additionally, the resulting model parameters are more understandable because their values can be traced back to problem features, such as system size, robot control algorithm, etc. Finally, we perform real-robot experiments to further understand the limits of our model from an engineering standpoint.},
  archive      = {J_AR},
  author       = {Harwell, John and Sylvester, Angel and Gini, Maria},
  doi          = {10.1007/s10514-023-10121-9},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {963-977},
  shortjournal = {Auton. Robot.},
  title        = {An empirical characterization of ODE models of swarm behaviors in common foraging scenarios},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards neuromorphic FPGA-based infrastructures for a
robotic arm. <em>AR</em>, <em>47</em>(7), 947–961. (<a
href="https://doi.org/10.1007/s10514-023-10111-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Muscles are stretched with bursts of spikes that come from motor neurons connected to the cerebellum through the spinal cord. Then, alpha motor neurons directly innervate the muscles to complete the motor command coming from upper biological structures. Nevertheless, classical robotic systems usually require complex computational capabilities and relative high-power consumption to process their control algorithm, which requires information from the robot’s proprioceptive sensors. The way in which the information is encoded and transmitted is an important difference between biological systems and robotic machines. Neuromorphic engineering mimics these behaviors found in biology into engineering solutions to produce more efficient systems and for a better understanding of neural systems. This paper presents the application of a Spike-based Proportional-Integral-Derivative controller to a 6-DoF Scorbot ER-VII robotic arm, feeding the motors with Pulse-Frequency-Modulation instead of Pulse-Width-Modulation, mimicking the way in which motor neurons act over muscles. The presented frameworks allow the robot to be commanded and monitored locally or remotely from both a Python software running on a computer or from a spike-based neuromorphic hardware. Multi-FPGA and single-PSoC solutions are compared. These frameworks are intended for experimental use of the neuromorphic community as a testbed platform and for dataset recording for machine learning purposes.},
  archive      = {J_AR},
  author       = {Canas-Moreno, Salvador and Piñero-Fuentes, Enrique and Rios-Navarro, Antonio and Cascado-Caballero, Daniel and Perez-Peña, Fernando and Linares-Barranco, Alejandro},
  doi          = {10.1007/s10514-023-10111-x},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {947-961},
  shortjournal = {Auton. Robot.},
  title        = {Towards neuromorphic FPGA-based infrastructures for a robotic arm},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLSS: Real-time, decentralized, cooperative, networkless
multi-robot trajectory planning using linear spatial separations.
<em>AR</em>, <em>47</em>(7), 921–946. (<a
href="https://doi.org/10.1007/s10514-023-10104-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory planning for multiple robots in shared environments is a challenging problem especially when there is limited communication available or no central entity. In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments. The algorithm requires relatively few robot capabilities, namely sensing the positions of robots and obstacles without higher-order derivatives and the ability of distinguishing robots from obstacles. There is no communication requirement and the robots’ dynamic limits are taken into account. RLSS generates and solves convex quadratic optimization problems that are kinematically feasible and guarantees collision avoidance if the resulting problems are feasible. We demonstrate the algorithm’s performance in real-time in simulations and on physical robots. We compare RLSS to two state-of-the-art planners and show empirically that RLSS does avoid deadlocks and collisions in forest-like and maze-like environments, significantly improving prior work, which result in collisions and deadlocks in such environments.},
  archive      = {J_AR},
  author       = {Şenbaşlar, Baskın and Hönig, Wolfgang and Ayanian, Nora},
  doi          = {10.1007/s10514-023-10104-w},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {921-946},
  shortjournal = {Auton. Robot.},
  title        = {RLSS: Real-time, decentralized, cooperative, networkless multi-robot trajectory planning using linear spatial separations},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-guided approach for navigation in a minimalistic
foraging robotic swarm. <em>AR</em>, <em>47</em>(7), 905–920. (<a
href="https://doi.org/10.1007/s10514-023-10102-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a biologically inspired design for swarm foraging based on ant’s pheromone deployment, where the swarm is assumed to have very restricted capabilities. The robots do not require global or relative position measurements and the swarm is fully decentralized and needs no infrastructure in place. Additionally, the system only requires one-hop communication over the robot network, we do not make any assumptions about the connectivity of the communication graph and the transmission of information and computation is scalable versus the number of agents. This is done by letting the agents in the swarm act as foragers or as guiding agents (beacons). We present experimental results computed for a swarm of Elisa-3 robots on a simulator, and show how the swarm self-organizes to solve a foraging problem over an unknown environment, converging to trajectories around the shortest path, and test the approach on a real swarm of Elisa-3 robots. At last, we discuss the limitations of such a system and propose how the foraging efficiency can be increased.},
  archive      = {J_AR},
  author       = {Adams, Steven and Jarne Ornia, Daniel and Mazo, Manuel},
  doi          = {10.1007/s10514-023-10102-y},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {905-920},
  shortjournal = {Auton. Robot.},
  title        = {A self-guided approach for navigation in a minimalistic foraging robotic swarm},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HeRo 2.0: A low-cost robot for swarm robotics research.
<em>AR</em>, <em>47</em>(7), 879–903. (<a
href="https://doi.org/10.1007/s10514-023-10100-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state of electronic component miniaturization coupled with the increasing efficiency in hardware and software allow the development of smaller and compact robotic systems. The convenience of using these small, simple, yet capable robots has gathered the research community’s attention towards practical applications of swarm robotics. This paper presents the design of a novel platform for swarm robotics applications that is low cost, easy to assemble using off-the-shelf components, and deeply integrated with the most used robotic framework available today: ROS (Robot Operating System). The robotic platform is entirely open, composed of a 3D printed body and open-source software. We describe its architecture, present its main features, and evaluate its functionalities executing experiments using a couple of robots. Results demonstrate that the proposed mobile robot is capable of performing different swarm tasks, given its small size and reduced cost, being suitable for swarm robotics research and education.},
  archive      = {J_AR},
  author       = {Rezeck, Paulo and Azpúrua, Héctor and Corrêa, Maurício F. S. and Chaimowicz, Luiz},
  doi          = {10.1007/s10514-023-10100-0},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {879-903},
  shortjournal = {Auton. Robot.},
  title        = {HeRo 2.0: A low-cost robot for swarm robotics research},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated group motion control of magnetically actuated
millirobots. <em>AR</em>, <em>47</em>(7), 865–877. (<a
href="https://doi.org/10.1007/s10514-023-10084-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-size robots offer access to spaces that are inaccessible to larger ones. This type of access is crucial in applications such as drug delivery, environmental detection, and collection of small samples. However, there are some tasks that are not possible to perform using only one robot including assembly and manufacturing at small scales, manipulation of micro- and nano- objects, and robot-based structuring of small-scale materials. In this article, we focus on tasks that can be achieved using a group of small-scale robots like pattern formation. These robots are typically externally actuated due to their size limitation. Yet, one faces the challenge of controlling a group of robots using a single global input. In this study, we propose a control algorithm to position individual members of a group in predefined positions. In our previous work, we presented a small-scaled magnetically actuated millirobot. An electromagnetic coil system applied external force and steered the millirobots in various modes of motion such as pivot walking and tumbling. In this paper, we propose two new designs of these millirobots. In the first design, the magnets are placed at the center of body to reduce the magnetic attraction force between the millirobots. In the second design, the millirobots are of identical length with two extra legs acting as the pivot points and varying pivot separation in design to take advantage of variable speed in pivot walking mode while keeping the speed constant in tumbling mode. This paper presents an algorithm for positional control of n millirobots with different lengths to move them from given initial positions to final desired ones. This method is based on choosing a leader that is fully controllable. Then, the motions of other millirobots are regulated by following the leader and determining their appropriate pivot separations in order to implement the intended group motion. Simulations and hardware experiments validate these results.},
  archive      = {J_AR},
  author       = {Razzaghi, Pouria and Al Khatib, Ehab and Hurmuzlu, Yildirim},
  doi          = {10.1007/s10514-023-10084-x},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {865-877},
  shortjournal = {Auton. Robot.},
  title        = {Automated group motion control of magnetically actuated millirobots},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Search and rescue with sparsely connected swarms.
<em>AR</em>, <em>47</em>(7), 849–863. (<a
href="https://doi.org/10.1007/s10514-022-10080-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing and deploying autonomous swarms capable of performing collective tasks in real-world is extremely challenging. One drawback of getting out of the lab is that realistic tasks involve long distances with limited numbers of robots, leading to sparse and intermittent connectivity. As an example, search and rescue requires robots to coordinate in their search, and relay the information of found targets. The search’s effectiveness is greatly reduced if robots must stay close to maintain connectivity. This paper proposes a decentralized search system that only requires sporadic connectivity and allows information diffusion through the swarm whenever possible. Our robots share and update a distributed belief map, to coordinate the search. Once a target is detected, the robots form a communication relay between a base station and the target’s position. We show the applicability of our system both in simulation and with real-world experiments with a small swarm of drones.},
  archive      = {J_AR},
  author       = {Dah-Achinanon, Ulrich and Marjani Bajestani, Seyed Ehsan and Lajoie, Pierre-Yves and Beltrame, Giovanni},
  doi          = {10.1007/s10514-022-10080-7},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {849-863},
  shortjournal = {Auton. Robot.},
  title        = {Search and rescue with sparsely connected swarms},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion-based communication for robotic swarms in exploration
missions. <em>AR</em>, <em>47</em>(7), 833–847. (<a
href="https://doi.org/10.1007/s10514-022-10079-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many people are fascinated by biological swarms, but understanding the behavior and inherent task objectives of a bird flock or ant colony requires training. Whereas several swarm intelligence works focus on mimicking natural swarm behaviors, we argue that this may not be the most intuitive approach to facilitate communication with the operators. Instead, we focus on the legibility of swarm expressive motions to communicate mission-specific messages to the operator. To do so, we leverage swarm intelligence algorithms on chain formation for resilient exploration and mapping combined with acyclic graph formation (AGF) into a novel swarm-oriented programming strategy. We then explore how expressive motions of robot swarms could be designed and test the legibility of nine different expressive motions in an online user study with 98 participants. We found several differences between the motions in communicating messages to the users. These findings represent a promising starting point for the design of legible expressive motions for implementation in decentralized robot swarms.},
  archive      = {J_AR},
  author       = {Boucher, Corentin and Stower, Rebecca and Varadharajan, Vivek Shankar and Zibetti, Elisabetta and Levillain, Florent and St-Onge, David},
  doi          = {10.1007/s10514-022-10079-0},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {833-847},
  shortjournal = {Auton. Robot.},
  title        = {Motion-based communication for robotic swarms in exploration missions},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editor’s note - special issue on robot swarms in the real
world: From design to deployment. <em>AR</em>, <em>47</em>(7), 831. (<a
href="https://doi.org/10.1007/s10514-023-10151-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AR},
  doi          = {10.1007/s10514-023-10151-3},
  journal      = {Autonomous Robots},
  month        = {10},
  number       = {7},
  pages        = {831},
  shortjournal = {Auton. Robot.},
  title        = {Editor’s note - special issue on robot swarms in the real world: From design to deployment},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse reinforcement learning for autonomous navigation via
differentiable semantic mapping and planning. <em>AR</em>,
<em>47</em>(6), 809–830. (<a
href="https://doi.org/10.1007/s10514-023-10118-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on inverse reinforcement learning for autonomous navigation using distance and semantic category observations. The objective is to infer a cost function that explains demonstrated behavior while relying only on the expert’s observations and state-control trajectory. We develop a map encoder, that infers semantic category probabilities from the observation sequence, and a cost encoder, defined as a deep neural network over the semantic features. Since the expert cost is not directly observable, the model parameters can only be optimized by differentiating the error between demonstrated controls and a control policy computed from the cost estimate. We propose a new model of expert behavior that enables error minimization using a closed-form subgradient computed only over a subset of promising states via a motion planning algorithm. Our approach allows generalizing the learned behavior to new environments with new spatial configurations of the semantic categories. We analyze the different components of our model in a minigrid environment. We also demonstrate that our approach learns to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of buildings, sidewalks, and road lanes.},
  archive      = {J_AR},
  author       = {Wang, Tianyu and Dhiman, Vikas and Atanasov, Nikolay},
  doi          = {10.1007/s10514-023-10118-4},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {809-830},
  shortjournal = {Auton. Robot.},
  title        = {Inverse reinforcement learning for autonomous navigation via differentiable semantic mapping and planning},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A learning-based approach to surface vehicle dynamics
modeling for robust multistep prediction. <em>AR</em>, <em>47</em>(6),
797–808. (<a href="https://doi.org/10.1007/s10514-023-10114-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the dynamics of surface vehicles and marine robots is important for developing marine autopilot and autonomous navigation systems. However, this often requires extensive experimental data and intense effort because they are highly nonlinear and involve various uncertainties in real operating conditions. Herein, we propose an efficient data-driven approach for analyzing and predicting the motion of a surface vehicle in a real environment based on deep learning techniques. The proposed multistep model is robust to measurement uncertainty and overcomes compounding errors by eliminating the correlation between the prediction results. Additionally, latent state representation and mixup augmentation are introduced to make the model more consistent and accurate. The performance analysis reveals that the proposed method outperforms conventional methods and is robust against environmental disturbances.},
  archive      = {J_AR},
  author       = {Jang, Junwoo and Lee, Changyu and Kim, Jinwhan},
  doi          = {10.1007/s10514-023-10114-8},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {797-808},
  shortjournal = {Auton. Robot.},
  title        = {A learning-based approach to surface vehicle dynamics modeling for robust multistep prediction},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning latent representations to co-adapt to humans.
<em>AR</em>, <em>47</em>(6), 771–796. (<a
href="https://doi.org/10.1007/s10514-023-10109-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When robots interact with humans in homes, roads, or factories the human’s behavior often changes in response to the robot. Non-stationary humans are challenging for robot learners: actions the robot has learned to coordinate with the original human may fail after the human adapts to the robot. In this paper we introduce an algorithmic formalism that enables robots (i.e., ego agents) to co-adapt alongside dynamic humans (i.e., other agents) using only the robot’s low-level states, actions, and rewards. A core challenge is that humans not only react to the robot’s behavior, but the way in which humans react inevitably changes both over time and between users. To deal with this challenge, our insight is that—instead of building an exact model of the human–robots can learn and reason over high-level representations of the human’s policy and policy dynamics. Applying this insight we develop RILI: Robustly Influencing Latent Intent. RILI first embeds low-level robot observations into predictions of the human’s latent strategy and strategy dynamics. Next, RILI harnesses these predictions to select actions that influence the adaptive human towards advantageous, high reward behaviors over repeated interactions. We demonstrate that—given RILI’s measured performance with users sampled from an underlying distribution—we can probabilistically bound RILI’s expected performance across new humans sampled from the same distribution. Our simulated experiments compare RILI to state-of-the-art representation and reinforcement learning baselines, and show that RILI better learns to coordinate with imperfect, noisy, and time-varying agents. Finally, we conduct two user studies where RILI co-adapts alongside actual humans in a game of tag and a tower-building task. See videos of our user studies here: https://youtu.be/WYGO5amDXbQ},
  archive      = {J_AR},
  author       = {Parekh, Sagar and Losey, Dylan P.},
  doi          = {10.1007/s10514-023-10109-5},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {771-796},
  shortjournal = {Auton. Robot.},
  title        = {Learning latent representations to co-adapt to humans},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven gait model for bipedal locomotion over
continuous changing speeds and inclines. <em>AR</em>, <em>47</em>(6),
753–769. (<a href="https://doi.org/10.1007/s10514-023-10108-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory generation for biped robots is very complex due to the challenge posed by real-world uneven terrain. To address this complexity, this paper proposes a data-driven Gait model that can handle continuously changing conditions. Data-driven approaches are used to incorporate the joint relationships. Therefore, the deep learning methods are employed to develop seven different data-driven models, namely DNN, LSTM, GRU, BiLSTM, BiGRU, LSTM+GRU, and BiLSTM+BiGRU. The dataset used for training the Gait model consists of walking data from 10 able subjects on continuously changing inclines and speeds. The objective function incorporates the standard error from the inter-subject mean trajectory to guide the Gait model to not accurately follow the high variance points in the gait cycle, which helps in providing a smooth and continuous gait cycle. The results show that the proposed Gait models outperform the traditional finite state machine (FSM) and Basis models in terms of mean and maximum error summary statistics. In particular, the LSTM+GRU-based Gait model provides the best performance compared to other data-driven models.},
  archive      = {J_AR},
  author       = {Singh, Bharat and Patel, Suchit and Vijayvargiya, Ankit and Kumar, Rajesh},
  doi          = {10.1007/s10514-023-10108-6},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {753-769},
  shortjournal = {Auton. Robot.},
  title        = {Data-driven gait model for bipedal locomotion over continuous changing speeds and inclines},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rewards from exploratory demonstrations using
probabilistic temporal ranking. <em>AR</em>, <em>47</em>(6), 733–751.
(<a href="https://doi.org/10.1007/s10514-023-10120-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Informative path-planning is a well established approach to visual-servoing and active viewpoint selection in robotics, but typically assumes that a suitable cost function or goal state is known. This work considers the inverse problem, where the goal of the task is unknown, and a reward function needs to be inferred from exploratory example demonstrations provided by a demonstrator, for use in a downstream informative path-planning policy. Unfortunately, many existing reward inference strategies are unsuited to this class of problems, due to the exploratory nature of the demonstrations. In this paper, we propose an alternative approach to cope with the class of problems where these sub-optimal, exploratory demonstrations occur. We hypothesise that, in tasks which require discovery, successive states of any demonstration are progressively more likely to be associated with a higher reward, and use this hypothesis to generate time-based binary comparison outcomes and infer reward functions that support these ranks, under a probabilistic generative model. We formalise this probabilistic temporal ranking approach and show that it improves upon existing approaches to perform reward inference for autonomous ultrasound scanning, a novel application of learning from demonstration in medical imaging while also being of value across a broad range of goal-oriented learning from demonstration tasks.},
  archive      = {J_AR},
  author       = {Burke, Michael and Lu, Katie and Angelov, Daniel and Straižys, Artūras and Innes, Craig and Subr, Kartic and Ramamoorthy, Subramanian},
  doi          = {10.1007/s10514-023-10120-w},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {733-751},
  shortjournal = {Auton. Robot.},
  title        = {Learning rewards from exploratory demonstrations using probabilistic temporal ranking},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cooperative estimation and control of a diffusion-based
spatiotemporal process using mobile sensors and actuators. <em>AR</em>,
<em>47</em>(6), 715–731. (<a
href="https://doi.org/10.1007/s10514-023-10105-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring and controlling a large-scale spatiotemporal process can be costly and dangerous for human operators, which can delegate the task to mobile robots for improved efficiency at a lower cost. The complex evolution of the spatiotemporal process and limited onboard resources of the robots motivate a holistic design of the robots’ actions to complete the tasks efficiently. This paper describes a cooperative framework for estimating and controlling a spatiotemporal process using a team of mobile robots that have limited onboard resources. We model the spatiotemporal process as a 2D diffusion equation that can characterize the intrinsic dynamics of the process with a partial differential equation (PDE). Measurement and actuation of the diffusion process are performed by mobile robots carrying sensors and actuators. The core of the framework is a nonlinear optimization problem, that simultaneously seeks the actuation and guidance of the robots to control the spatiotemporal process subject to the PDE dynamics. The limited onboard resources are formulated as inequality constraints on the actuation and speed of the robots. Extensive numerical studies analyze and evaluate the proposed framework using nondimensionalization and compare the optimal strategy to baseline strategies. The framework is demonstrated on an outdoor multi-quadrotor testbed using hardware-in-the-loop simulations.},
  archive      = {J_AR},
  author       = {Cheng, Sheng and Paley, Derek A.},
  doi          = {10.1007/s10514-023-10105-9},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {715-731},
  shortjournal = {Auton. Robot.},
  title        = {Cooperative estimation and control of a diffusion-based spatiotemporal process using mobile sensors and actuators},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TNES: Terrain traversability mapping, navigation and
excavation system for autonomous excavators on worksite. <em>AR</em>,
<em>47</em>(6), 695–714. (<a
href="https://doi.org/10.1007/s10514-023-10113-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a terrain traversability mapping and navigation system (TNS) for autonomous excavator applications in an unstructured environment. We use an efficient approach to extract terrain features from RGB images and 3D point clouds and incorporate them into a global map for planning and navigation. Our system can adapt to changing environments and update the terrain information in real-time. Moreover, we present a novel dataset, the Complex Worksite Terrain dataset, which consists of RGB images from construction sites with seven categories based on navigability. Our novel algorithms improve the mapping accuracy over previous methods by 4.17–30.48 $$\%$$ and reduce MSE on the traversability map by 13.8–71.4 $$\%$$ . We have combined our mapping approach with planning and control modules in an autonomous excavator navigation system and observe $$49.3\%$$ improvement in the overall success rate. Based on TNS, we demonstrate the first autonomous excavator that can navigate through unstructured environments consisting of deep pits, steep hills, rock piles, and other complex terrain features. In addition, we combine the proposed TNS with the autonomous excavation system (AES), and deploy the new pipeline, TNES, on a more complex construction site. With minimum human intervention, we demonstrate autonomous navigation capability with excavation tasks.},
  archive      = {J_AR},
  author       = {Guan, Tianrui and He, Zhenpeng and Song, Ruitao and Zhang, Liangjun},
  doi          = {10.1007/s10514-023-10113-9},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {695-714},
  shortjournal = {Auton. Robot.},
  title        = {TNES: Terrain traversability mapping, navigation and excavation system for autonomous excavators on worksite},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regulated pure pursuit for robot path tracking. <em>AR</em>,
<em>47</em>(6), 685–694. (<a
href="https://doi.org/10.1007/s10514-023-10097-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated deployment of service robots have spawned a number of algorithm variations to better handle real-world conditions. Many local trajectory planning techniques have been deployed on practical robot systems successfully. While most formulations of Dynamic Window Approach and Model Predictive Control can progress along paths and optimize for additional criteria, the use of pure path tracking algorithms is still commonplace. Decades later, Pure Pursuit and its variants continues to be one of the most commonly utilized classes of local trajectory planners. However, few Pure Pursuit variants have been proposed with schema for variable linear velocities—they either assume a constant velocity or fails to address the point at all. This paper presents a variant of Pure Pursuit designed with additional heuristics to regulate linear velocities, built atop the existing Adaptive variant. The Regulated Pure Pursuit algorithm makes incremental improvements on state of the art by adjusting linear velocities with particular focus on safety in constrained and partially observable spaces commonly negotiated by deployed robots. We present experiments with the Regulated Pure Pursuit algorithm on industrial-grade service robots. We also provide a high-quality reference implementation that is freely included ROS 2 Nav2 framework at https://github.com/ros-planning/navigation2 for fast evaluation.},
  archive      = {J_AR},
  author       = {Macenski, Steve and Singh, Shrijit and Martín, Francisco and Ginés, Jonatan},
  doi          = {10.1007/s10514-023-10097-6},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {685-694},
  shortjournal = {Auton. Robot.},
  title        = {Regulated pure pursuit for robot path tracking},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex environment localization system using complementary
ceiling and ground map information. <em>AR</em>, <em>47</em>(6),
669–683. (<a href="https://doi.org/10.1007/s10514-023-10116-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a robust localization system using complementary information extracted from ceiling and ground plans, particularly applicable to dynamic and complex environments. The ceiling perception provides the robot with stable and time-invariant environmental features independent of the dynamic changes on the ground, whereas the ground perception allows the robot to navigate in the ground plane while avoiding stationary obstacles. We propose an architecture to fuse ground 2D LiDAR scan and ceiling 3D LiDAR scan with our enhanced mapping algorithm associating perception from both sources efficiently. The localization ability and the navigation performance can be promisingly secured even in a harsh environment with our complementary sensed information from the ground and ceiling. The salient feature of our work is that our system can simultaneously map both the ceiling and ground plane efficiently without extra efforts of deploying articulated landmarks and apply such hybrid information effectively, which facilitates the robot to travel through any indoor environment with human crowds without getting lost.},
  archive      = {J_AR},
  author       = {Yu, Chee-An and Chen, Hao-Yun and Wang, Chun-Chieh and Fu, Li-Chen},
  doi          = {10.1007/s10514-023-10116-6},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {669-683},
  shortjournal = {Auton. Robot.},
  title        = {Complex environment localization system using complementary ceiling and ground map information},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGRI-SLAM: A real-time stereo visual SLAM for agricultural
environment. <em>AR</em>, <em>47</em>(6), 649–668. (<a
href="https://doi.org/10.1007/s10514-023-10110-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we proposed a stereo visual simultaneous localisation and mapping (SLAM) system that efficiently works in agricultural scenarios without compromising the performance and accuracy in contrast to the other state-of-the-art methods. The proposed system is equipped with an image enhancement technique for the ORB point and LSD line features recovery, which enables it to work in broader scenarios and gives extensive spatial information from the low-light and hazy agricultural environment. Firstly, the method has been tested on the standard dataset, i.e., KITTI and EuRoC, to validate the localisation accuracy by comparing it with the other state-of-the-art methods, namely VINS-SLAM, PL-SLAM, and ORB-SLAM2. The experimental results evidence that the proposed method obtains superior localisation and mapping accuracy than the other visual SLAM methods. Secondly, the proposed method is tested on the ROSARIO dataset, our low-light agricultural dataset, and O-HAZE dataset to validate the performance in agricultural environments. In such cases, while other methods fail to operate in such complex agricultural environments, our method successfully operates with high localisation and mapping accuracy.},
  archive      = {J_AR},
  author       = {Islam, Rafiqul and Habibullah, Habibullah and Hossain, Tagor},
  doi          = {10.1007/s10514-023-10110-y},
  journal      = {Autonomous Robots},
  month        = {8},
  number       = {6},
  pages        = {649-668},
  shortjournal = {Auton. Robot.},
  title        = {AGRI-SLAM: A real-time stereo visual SLAM for agricultural environment},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware correspondence identification for
collaborative perception. <em>AR</em>, <em>47</em>(5), 635–648. (<a
href="https://doi.org/10.1007/s10514-023-10086-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correspondence identification is essential for multi-robot collaborative perception, which aims to identify the same objects in order to ensure consistent references of the objects by a group of robots/agents in their own fields of view. Although recent deep learning methods have shown encouraging performance on correspondence identification, they suffer from two shortcomings, including the inability to address non-covisibility and the inability to quantify and reduce uncertainty to improve correspondence identification. To address both issues, we propose a novel uncertainty-aware deep graph matching method for correspondence identification in collaborative perception. Our new approach formulates correspondence identification as a deep graph matching problem, which identifies correspondences based on deep graph neural network-based features and explicitly quantify uncertainties in the identified correspondences under the Bayesian framework. In addition, we design a novel loss function that explicitly reduces correspondence uncertainty and perceptual non-covisibility during learning. Finally, we design a novel multi-robot sensor fusion method that integrates the multi-robot observations given the identified correspondences to perform collaborative object localization. We evaluate our approach in the robotics applications of collaborative assembly, multi-robot coordination and connected autonomous driving using high-fidelity simulations and physical robots. Experiments have shown that, our approach achieves the state-of-the-art performance of correspondence identification. Furthermore, the identified correspondences of objects can be well integrated into multi-robot collaboration for object localization.},
  archive      = {J_AR},
  author       = {Gao, Peng and Zhu, Qingzhao and Zhang, Hao},
  doi          = {10.1007/s10514-023-10086-9},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {635-648},
  shortjournal = {Auton. Robot.},
  title        = {Uncertainty-aware correspondence identification for collaborative perception},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empowerment-based solution to robotic manipulation tasks
with sparse rewards. <em>AR</em>, <em>47</em>(5), 617–633. (<a
href="https://doi.org/10.1007/s10514-023-10087-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to provide adaptive and user-friendly solutions to robotic manipulation, it is important that the agent can learn to accomplish tasks even if they are only provided with very sparse instruction signals. To address the issues reinforcement learning algorithms face when task rewards are sparse, this paper proposes an intrinsic motivation approach that can be easily integrated into any standard reinforcement learning algorithm and can allow robotic manipulators to learn useful manipulation skills with only sparse extrinsic rewards. Through integrating and balancing empowerment and curiosity, this approach shows superior performance compared to other state-of-the-art intrinsic exploration approaches during extensive empirical testing. When combined with other strategies for tackling the exploration challenge, e.g. curriculum learning, our approach is able to further improve the exploration efficiency and task success rate. Qualitative analysis also shows that when combined with diversity-driven intrinsic motivations, this approach can help manipulators learn a set of diverse skills which could potentially be applied to other more complicated manipulation tasks and accelerate their learning process.},
  archive      = {J_AR},
  author       = {Dai, Siyu and Xu, Wei and Hofmann, Andreas and Williams, Brian},
  doi          = {10.1007/s10514-023-10087-8},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {617-633},
  shortjournal = {Auton. Robot.},
  title        = {An empowerment-based solution to robotic manipulation tasks with sparse rewards},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point-based metric and topological localisation between
lidar and overhead imagery. <em>AR</em>, <em>47</em>(5), 595–615. (<a
href="https://doi.org/10.1007/s10514-023-10085-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method for solving the localisation of a ground lidar using overhead imagery only. Public overhead imagery such as Google satellite images are readily available resources. They can be used as the map proxy for robot localisation, relaxing the requirement for a prior traversal for mapping as in traditional approaches. While prior approaches have focused on the metric localisation between range sensors and overhead imagery, our method is the first to learn both place recognition and metric localisation of a ground lidar using overhead imagery, and also outperforms prior methods on metric localisation with large initial pose offsets. To bridge the drastic domain gap between lidar data and overhead imagery, our method learns to transform an overhead image into a collection of 2D points, emulating the resulting point-cloud scanned by a lidar sensor situated near the centre of the overhead image. After both modalities are expressed as point sets, point-based machine learning methods for localisation are applied.},
  archive      = {J_AR},
  author       = {Tang, Tim Yuqing and De Martini, Daniele and Newman, Paul},
  doi          = {10.1007/s10514-023-10085-w},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {595-615},
  shortjournal = {Auton. Robot.},
  title        = {Point-based metric and topological localisation between lidar and overhead imagery},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-design of communication and machine inference for cloud
robotics. <em>AR</em>, <em>47</em>(5), 579–594. (<a
href="https://doi.org/10.1007/s10514-023-10093-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, even the most compute-and-power constrained robots can measure complex, high data-rate video and LIDAR sensory streams. Often, such robots, ranging from low-power drones to space and subterranean rovers, need to transmit high-bitrate sensory data to a remote compute server if they are uncertain or cannot scalably run complex perception or mapping tasks locally. However, today’s representations for sensory data are mostly designed for human, not robotic, perception and thus often waste precious compute or wireless network resources to transmit unimportant parts of a scene that are unnecessary for a high-level robotic task. This paper presents an algorithm to learn task-relevant representations of sensory data that are co-designed with a pre-trained robotic perception model’s ultimate objective. Our algorithm aggressively compresses robotic sensory data by up to 11 $$\times $$ more than competing methods. Further, it achieves high accuracy and robust generalization on diverse tasks including Mars terrain classification with low-power deep learning accelerators, neural motion planning, and environmental timeseries classification.},
  archive      = {J_AR},
  author       = {Nakanoya, Manabu and Narasimhan, Sai Shankar and Bhat, Sharachchandra and Anemogiannis, Alexandros and Datta, Akul and Katti, Sachin and Chinchali, Sandeep and Pavone, Marco},
  doi          = {10.1007/s10514-023-10093-w},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {579-594},
  shortjournal = {Auton. Robot.},
  title        = {Co-design of communication and machine inference for cloud robotics},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiSECt: A differentiable simulator for parameter inference
and control in robotic cutting. <em>AR</em>, <em>47</em>(5), 549–578.
(<a href="https://doi.org/10.1007/s10514-023-10094-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic cutting of soft materials is critical for applications such as food processing, household automation, and surgical manipulation. As in other areas of robotics, simulators can facilitate controller verification, policy learning, and dataset generation. Moreover, differentiable simulators can enable gradient-based optimization, which is invaluable for calibrating simulation parameters and optimizing controllers. In this work, we present DiSECt: the first differentiable simulator for cutting soft materials. The simulator augments the finite element method with a continuous contact model based on signed distance fields, as well as a continuous damage model that inserts springs on opposite sides of the cutting plane and allows them to weaken until zero stiffness, enabling crack formation. Through various experiments, we evaluate the performance of the simulator. We first show that the simulator can be calibrated to match resultant forces and deformation fields from a state-of-the-art commercial solver and real-world cutting datasets, with generality across cutting velocities and object instances. We then show that Bayesian inference can be performed efficiently by leveraging the differentiability of the simulator, estimating posteriors over hundreds of parameters in a fraction of the time of derivative-free methods. Next, we illustrate that control parameters in the simulation can be optimized to minimize cutting forces via lateral slicing motions. Finally, we conduct experiments on a real robot arm equipped with a slicing knife to infer simulation parameters from force measurements. By optimizing the slicing motion of the knife, we show on fruit cutting scenarios that the average knife force can be reduced by more than $$40\%$$ compared to a vertical cutting motion. We publish code and additional materials on our project website at https://diff-cutting-sim.github.io .},
  archive      = {J_AR},
  author       = {Heiden, Eric and Macklin, Miles and Narang, Yashraj and Fox, Dieter and Garg, Animesh and Ramos, Fabio},
  doi          = {10.1007/s10514-023-10094-9},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {549-578},
  shortjournal = {Auton. Robot.},
  title        = {DiSECt: A differentiable simulator for parameter inference and control in robotic cutting},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning instance-level n-ary semantic knowledge at scale
for robots operating in everyday environments. <em>AR</em>,
<em>47</em>(5), 529–547. (<a
href="https://doi.org/10.1007/s10514-023-10099-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots operating in everyday environments need to effectively perceive, model, and infer semantic properties of objects. Existing knowledge reasoning frameworks only model binary relations between an object’s class label and its semantic properties, unable to collectively reason about object properties detected by different perception algorithms and grounded in diverse sensory modalities. We bridge the gap between multimodal perception and knowledge reasoning by introducing an n-ary representation that models complex, inter-related object properties. To tackle the problem of collecting n-ary semantic knowledge at scale, we propose transformer neural networks that generalize knowledge from observations of object instances by learning to predict single missing properties or predict joint probabilities of all properties. The learned models can reason at different levels of abstraction, effectively predicting unknown properties of objects in different environmental contexts given different amounts of observed information. We quantitatively validate our approach against prior methods on LINK, a unique dataset we contribute that contains 1457 object instances in different situations, amounting to 15 multimodal properties types and 200 total properties. Compared to the top-performing baseline, a Markov Logic Network, our models obtain a 10% improvement in predicting unknown properties of novel object instances while reducing training and inference time by more than 150 times. Additionally, we apply our work to a mobile manipulation robot, demonstrating its ability to leverage n-ary reasoning to retrieve objects and actively detect object properties. The code and data are available at https://github.com/wliu88/LINK .},
  archive      = {J_AR},
  author       = {Liu, Weiyu and Bansal, Dhruva and Daruna, Angel and Chernova, Sonia},
  doi          = {10.1007/s10514-023-10099-4},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {529-547},
  shortjournal = {Auton. Robot.},
  title        = {Learning instance-level N-ary semantic knowledge at scale for robots operating in everyday environments},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal embodied attribute learning by robots for
object-centric action policies. <em>AR</em>, <em>47</em>(5), 505–528.
(<a href="https://doi.org/10.1007/s10514-023-10098-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots frequently need to perceive object attributes, such as red, heavy, and empty, using multimodal exploratory behaviors, such as look, lift, and shake. One possible way for robots to do so is to learn a classifier for each perceivable attribute given an exploratory behavior. Once the attribute classifiers are learned, they can be used by robots to select actions and identify attributes of new objects, answering questions, such as “Is this object red and empty ?” In this article, we introduce a robot interactive perception problem, called Multimodal Embodied Attribute Learning (meal), and explore solutions to this new problem. Under different assumptions, there are two classes of meal problems. offline-meal problems are defined in this article as learning attribute classifiers from pre-collected data, and sequencing actions towards attribute identification under the challenging trade-off between information gains and exploration action costs. For this purpose, we introduce Mixed Observability Robot Control (morc), an algorithm for offline-meal problems, that dynamically constructs both fully and partially observable components of the state for multimodal attribute identification of objects. We further investigate a more challenging class of meal problems, called online-meal, where the robot assumes no pre-collected data, and works on both attribute classification and attribute identification at the same time. Based on morc, we develop an algorithm called Information-Theoretic Reward Shaping (morc-itrs) that actively addresses the trade-off between exploration and exploitation in online-meal problems. morc and morc-itrs are evaluated in comparison with competitive meal baselines, and results demonstrate the superiority of our methods in learning efficiency and identification accuracy.},
  archive      = {J_AR},
  author       = {Zhang, Xiaohan and Amiri, Saeid and Sinapov, Jivko and Thomason, Jesse and Stone, Peter and Zhang, Shiqi},
  doi          = {10.1007/s10514-023-10098-5},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {505-528},
  shortjournal = {Auton. Robot.},
  title        = {Multimodal embodied attribute learning by robots for object-centric action policies},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial. <em>AR</em>, <em>47</em>(5), 503–504. (<a
href="https://doi.org/10.1007/s10514-023-10119-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AR},
  author       = {Hsieh, M. Ani and Shell, Dylan A.},
  doi          = {10.1007/s10514-023-10119-3},
  journal      = {Autonomous Robots},
  month        = {6},
  number       = {5},
  pages        = {503-504},
  shortjournal = {Auton. Robot.},
  title        = {Editorial},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic stochastic modeling for adaptive sampling of
environmental variables using an AUV. <em>AR</em>, <em>47</em>(4),
483–502. (<a href="https://doi.org/10.1007/s10514-023-10095-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discharge of mine tailings significantly impacts the ecological status of the sea. Methods to efficiently monitor the extent of dispersion is essential to protect sensitive areas. By combining underwater robotic sampling with ocean models, we can choose informative sampling sites and adaptively change the robot’s path based on in situ measurements to optimally map the tailings distribution near a seafill. This paper creates a stochastic spatio-temporal proxy model of dispersal dynamics using training data from complex numerical models. The proxy model consists of a spatio-temporal Gaussian process model based on an advection–diffusion stochastic partial differential equation. Informative sampling sites are chosen based on predictions from the proxy model using an objective function favoring areas with high uncertainty and high expected tailings concentrations. A simulation study and data from real-life experiments are presented.},
  archive      = {J_AR},
  author       = {Berget, Gunhild Elisabeth and Eidsvik, Jo and Alver, Morten Omholt and Johansen, Tor Arne},
  doi          = {10.1007/s10514-023-10095-8},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {483-502},
  shortjournal = {Auton. Robot.},
  title        = {Dynamic stochastic modeling for adaptive sampling of environmental variables using an AUV},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust inverse dynamics by evaluating newton–euler equations
with respect to a moving reference and measuring angular acceleration.
<em>AR</em>, <em>47</em>(4), 465–481. (<a
href="https://doi.org/10.1007/s10514-023-10092-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining stability while walking on arbitrary surfaces or dealing with external perturbations is of great interest in humanoid robotics research. Increasing the system’s autonomous robustness to a variety of postural threats during locomotion is the key despite the need to evaluate noisy sensor signals. The equations of motion are the foundation of all published approaches. In contrast, we propose a more adequate evaluation of the equations of motion with respect to an arbitrary moving reference point in a non-inertial reference frame. Conceptual advantages are, e.g., getting independent of global position and velocity vectors estimated by sensor fusions or calculating the imaginary zero-moment point walking on different inclined ground surfaces. Further, we improve the calculation results by reducing noise-amplifying methods in our algorithm and using specific characteristics of physical robots. We use simulation results to compare our algorithm with established approaches and test it with experimental robot data.},
  archive      = {J_AR},
  author       = {Gießler, Maximilian and Waltersberger, Bernd},
  doi          = {10.1007/s10514-023-10092-x},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {465-481},
  shortjournal = {Auton. Robot.},
  title        = {Robust inverse dynamics by evaluating Newton–Euler equations with respect to a moving reference and measuring angular acceleration},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic hand synergies for in-hand regrasping driven by
object information. <em>AR</em>, <em>47</em>(4), 453–464. (<a
href="https://doi.org/10.1007/s10514-023-10101-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a conditional generative model to represent dexterous grasp postures of a robotic hand and use it to generate in-hand regrasp trajectories. Our model learns to encode the robotic grasp postures into a low-dimensional space, called Synergy Space, while taking into account additional information about the object such as its size and its shape category. We then generate regrasp trajectories through linear interpolation in this low-dimensional space. The result is that the hand configuration moves from one grasp type to another while keeping the object stable in the hand. We show that our model achieves higher success rate on in-hand regrasping compared to previous methods used for synergy extraction, by taking advantage of the grasp size conditional variable.},
  archive      = {J_AR},
  author       = {Dimou, Dimitrios and Santos-Victor, José and Moreno, Plinio},
  doi          = {10.1007/s10514-023-10101-z},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {453-464},
  shortjournal = {Auton. Robot.},
  title        = {Robotic hand synergies for in-hand regrasping driven by object information},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Navigation functions with moving destinations and obstacles.
<em>AR</em>, <em>47</em>(4), 435–451. (<a
href="https://doi.org/10.1007/s10514-023-10088-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic environments challenge existing robot navigation methods, and motivate either stringent assumptions on workspace variation or relinquishing of collision avoidance and convergence guarantees. This paper shows that the latter can be preserved even in the absence of knowledge of how the environment evolves, through a navigation function methodology applicable to sphere-worlds with moving obstacles and robot destinations. Assuming bounds on speeds of robot destination and obstacles, and sufficiently higher maximum robot speed, the navigation function gradient can be used produce robot feedback laws that guarantee obstacle avoidance, and theoretical guarantees of bounded tracking errors and asymptotic convergence to the target when the latter eventually stops moving. The efficacy of the gradient-based feedback controller derived from the new navigation function construction is demonstrated both in numerical simulations as well as experimentally.},
  archive      = {J_AR},
  author       = {Wei, Cong and Chen, Chuchu and Tanner, Herbert G.},
  doi          = {10.1007/s10514-023-10088-7},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {435-451},
  shortjournal = {Auton. Robot.},
  title        = {Navigation functions with moving destinations and obstacles},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed swarm collision avoidance based on angular
calculations. <em>AR</em>, <em>47</em>(4), 425–434. (<a
href="https://doi.org/10.1007/s10514-022-10081-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collision avoidance is one of the most important topics in the robotics field. In this problem, the goal is to move the robots from initial locations to target locations such that they follow the shortest non-colliding paths in the shortest time and with the least amount of energy. Robot navigation among pedestrians is an example application of this problem which is the focus of this paper. This paper presents a distributed and real-time algorithm for solving collision avoidance problems in dense and complex 2D and 3D environments. This algorithm uses angular calculations to select the optimal direction for the movement of each robot and it has been shown that these separate calculations lead to a form of cooperative behavior among agents. We evaluated the proposed approach on various simulation and experimental scenarios and compared the results with ORCA one of the most important algorithms in this field. The results show that the proposed approach is at least 25% faster than ORCA while is also more reliable. The proposed method is shown to enable fully autonomous navigation of a swarm of Crazyflies.},
  archive      = {J_AR},
  author       = {Qazavi, SeyedZahir and Semnani, Samaneh Hosseini},
  doi          = {10.1007/s10514-022-10081-6},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {425-434},
  shortjournal = {Auton. Robot.},
  title        = {Distributed swarm collision avoidance based on angular calculations},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Haptic-guided grasping to minimise torque effort during
robotic telemanipulation. <em>AR</em>, <em>47</em>(4), 405–423. (<a
href="https://doi.org/10.1007/s10514-023-10096-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teleoperating robotic manipulators can be complicated and cognitively demanding for the human operator. Despite these difficulties, teleoperated robotic systems are still popular in several industrial applications, e.g., remote handling of hazardous material. In this context, we present a novel haptic shared control method for minimising the manipulator torque effort during remote manipulative actions in which an operator is assisted in selecting a suitable grasping pose for then displacing an object along a desired trajectory. Minimising torque is important because it reduces the system operating cost and extends the range of objects that can be manipulated. We demonstrate the effectiveness of the proposed approach in a series of representative real-world pick-and-place experiments as well as in a human subjects study. The reported results prove the effectiveness of our shared control vs. a standard teleoperation approach. We also find that haptic-only guidance performs better than visual-only guidance, although combining them together leads to the best overall results.},
  archive      = {J_AR},
  author       = {Rahal, Rahaf and Ghalamzan-E., Amir M. and Abi-Farraj, Firas and Pacchierotti, Claudio and Robuffo Giordano, Paolo},
  doi          = {10.1007/s10514-023-10096-7},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {405-423},
  shortjournal = {Auton. Robot.},
  title        = {Haptic-guided grasping to minimise torque effort during robotic telemanipulation},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visuo-haptic object perception for robots: An overview.
<em>AR</em>, <em>47</em>(4), 377–403. (<a
href="https://doi.org/10.1007/s10514-023-10091-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object perception capabilities of humans are impressive, and this becomes even more evident when trying to develop solutions with a similar proficiency in autonomous robots. While there have been notable advancements in the technologies for artificial vision and touch, the effective integration of these two sensory modalities in robotic applications still needs to be improved, and several open challenges exist. Taking inspiration from how humans combine visual and haptic perception to perceive object properties and drive the execution of manual tasks, this article summarises the current state of the art of visuo-haptic object perception in robots. Firstly, the biological basis of human multimodal object perception is outlined. Then, the latest advances in sensing technologies and data collection strategies for robots are discussed. Next, an overview of the main computational techniques is presented, highlighting the main challenges of multimodal machine learning and presenting a few representative articles in the areas of robotic object recognition, peripersonal space representation and manipulation. Finally, informed by the latest advancements and open challenges, this article outlines promising new research directions.},
  archive      = {J_AR},
  author       = {Navarro-Guerrero, Nicolás and Toprak, Sibel and Josifovski, Josip and Jamone, Lorenzo},
  doi          = {10.1007/s10514-023-10091-y},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {377-403},
  shortjournal = {Auton. Robot.},
  title        = {Visuo-haptic object perception for robots: An overview},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous robot teams for modeling and prediction of
multiscale environmental processes. <em>AR</em>, <em>47</em>(4),
353–376. (<a href="https://doi.org/10.1007/s10514-023-10089-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a framework to enable a team of heterogeneous mobile robots to model and sense a multiscale system. We propose a coupled strategy, where robots of one type collect high-fidelity measurements at a slow time scale and robots of another type collect low-fidelity measurements at a fast time scale, for the purpose of fusing measurements together. The multiscale measurements are fused to create a model of a complex, nonlinear spatiotemporal process. The model helps determine optimal sensing locations and predict the evolution of the process. Key contributions are: (i) consolidation of multiple types of data into one cohesive model, (ii) fast determination of optimal sensing locations for mobile robots, and (iii) adaptation of models online for various monitoring scenarios. We illustrate the proposed framework by modeling and predicting the evolution of an artificial plasma cloud. We test our approach using physical marine robots adaptively sampling a process in a water tank.},
  archive      = {J_AR},
  author       = {Salam, Tahiya and Hsieh, M. Ani},
  doi          = {10.1007/s10514-023-10089-6},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {353-376},
  shortjournal = {Auton. Robot.},
  title        = {Heterogeneous robot teams for modeling and prediction of multiscale environmental processes},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social crowd navigation of a mobile robot based on human
trajectory prediction and hybrid sensing. <em>AR</em>, <em>47</em>(4),
339–351. (<a href="https://doi.org/10.1007/s10514-023-10103-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper propose a hierarchical path planning algorithm that first captures the local crowd movement around the robot using RGB camera combined with LiDAR and predicts the movement of people nearby the robot, and then generates appropriate global path for the robot using the global path planner with the crowd information. After deciding the global path, the low-level control system receives the prediction results of the crowd and high-level global path, and generates the actual speed control commands for the robot after considering the social norms. With the high accuracy of computer vision for human recognition and the high precision of LiDAR, the system is able to accurately track the surrounding human locations. Through high-level path planning, the robot can use different movement strategies in different scenarios, while the crowd prediction allows the robot to generate more efficient and socially acceptable paths. With this system, even in a highly dynamic environment caused by the crowd, the robot can still plan an appropriate path reach the destination without causing psychological discomfort to others successfully.},
  archive      = {J_AR},
  author       = {Chen, Hao-Yun and Huang, Pei-Han and Fu, Li-Chen},
  doi          = {10.1007/s10514-023-10103-x},
  journal      = {Autonomous Robots},
  month        = {4},
  number       = {4},
  pages        = {339-351},
  shortjournal = {Auton. Robot.},
  title        = {Social crowd navigation of a mobile robot based on human trajectory prediction and hybrid sensing},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to: Adaptive submodular inverse reinforcement
learning for spatial search and map exploration. <em>AR</em>,
<em>47</em>(3), 337. (<a
href="https://doi.org/10.1007/s10514-022-10077-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AR},
  author       = {Wu, Ji-Jie and Tseng, Kuo-Shih},
  doi          = {10.1007/s10514-022-10077-2},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {3},
  pages        = {337},
  shortjournal = {Auton. Robot.},
  title        = {Correction to: Adaptive submodular inverse reinforcement learning for spatial search and map exploration},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provident vehicle detection at night for advanced driver
assistance systems. <em>AR</em>, <em>47</em>(3), 313–335. (<a
href="https://doi.org/10.1007/s10514-022-10072-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, computer vision algorithms have become more powerful, which enabled technologies such as autonomous driving to evolve rapidly. However, current algorithms mainly share one limitation: They rely on directly visible objects. This is a significant drawback compared to human behavior, where visual cues caused by objects (e. g., shadows) are already used intuitively to retrieve information or anticipate occurring objects. While driving at night, this performance deficit becomes even more obvious: Humans already process the light artifacts caused by the headlamps of oncoming vehicles to estimate where they appear, whereas current object detection systems require that the oncoming vehicle is directly visible before it can be detected. Based on previous work on this subject, in this paper, we present a complete system that can detect light artifacts caused by the headlights of oncoming vehicles so that it detects that a vehicle is approaching providently (denoted as provident vehicle detection). For that, an entire algorithm architecture is investigated, including the detection in the image space, the three-dimensional localization, and the tracking of light artifacts. To demonstrate the usefulness of such an algorithm, the proposed algorithm is deployed in a test vehicle to use the detected light artifacts to control the glare-free high beam system proactively (react before the oncoming vehicle is directly visible). Using this experimental setting, the provident vehicle detection system’s time benefit compared to an in-production computer vision system is quantified. Additionally, the glare-free high beam use case provides a real-time and real-world visualization interface of the detection results by considering the adaptive headlamps as projectors. With this investigation of provident vehicle detection, we want to put awareness on the unconventional sensing task of detecting objects providently (detection based on observable visual cues the objects cause before they are visible) and further close the performance gap between human behavior and computer vision algorithms to bring autonomous and automated driving a step forward.},
  archive      = {J_AR},
  author       = {Ewecker, Lukas and Asan, Ebubekir and Ohnemus, Lars and Saralajew, Sascha},
  doi          = {10.1007/s10514-022-10072-7},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {3},
  pages        = {313-335},
  shortjournal = {Auton. Robot.},
  title        = {Provident vehicle detection at night for advanced driver assistance systems},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urban localization based on aerial imagery by correcting
projection distortion. <em>AR</em>, <em>47</em>(3), 299–312. (<a
href="https://doi.org/10.1007/s10514-022-10082-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a vehicle localization method that fuses aerial maps and LiDAR measurements in urban canyon environments. The building outlines from an aerial image can be used as appropriate features for matching with the LiDAR data for localization. However, distortions caused by scaled orthographic projection of aerial maps are commonly observed in the images of metropolitan areas, which may significantly degrade the matching and resulting localization performance. In this study, a novel method for correcting such distortions is proposed and used for the vehicle localization by matching the corrected map and LiDAR measurements. Instance and semantic segmentation algorithms were used to distinguish individual buildings and generate corrected outlines of the buildings. A particle filter is applied to determine the pose of the vehicle based on the mutual information between the map and LiDAR measurements. The performance of the proposed algorithm was verified using a dataset obtained in urban areas.},
  archive      = {J_AR},
  author       = {Kim, Jonghwi and Cho, Yonghoon and Kim, Jinwhan},
  doi          = {10.1007/s10514-022-10082-5},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {3},
  pages        = {299-312},
  shortjournal = {Auton. Robot.},
  title        = {Urban localization based on aerial imagery by correcting projection distortion},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient RRT-based motion planning algorithm for
autonomous underwater vehicles under cylindrical sampling constraints.
<em>AR</em>, <em>47</em>(3), 281–297. (<a
href="https://doi.org/10.1007/s10514-023-10083-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quickly finding high-quality paths is of great significance for autonomous underwater vehicles (AUVs) in path planning problems. In this paper, we present a cylinder-based heuristic rapidly exploring random tree (Cyl-HRRT*) algorithm, which is the extension version of the path planner presented in our previous publication. Cyl-HRRT* increases the likelihood of sampling states that can improve the current solution by biasing the states into a cylindrical subset, thus providing better paths for AUVs. A direct greedy sampling method is proposed to explore the space more efficiently and accelerate convergence to the optimum. To reasonably balance the optimization accuracy and the number of iterations, a beacon-based adaptive optimization strategy is presented, which adaptively establishes a cylindrical subset for the next focused sampling according to the current path. Furthermore, the Cyl-HRRT* algorithm is shown to be probabilistically complete and asymptotically optimal. Finally, the Cyl-HRRT* algorithm is comprehensively tested in both simulations and real-world experiments. The results reveal that the path generated by the Cyl-HRRT* algorithm greatly improves the power savings and mobility of the AUV.},
  archive      = {J_AR},
  author       = {Yu, Fujie and Shang, Huaqing and Zhu, Qilong and Zhang, Hansheng and Chen, Yuan},
  doi          = {10.1007/s10514-023-10083-y},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {3},
  pages        = {281-297},
  shortjournal = {Auton. Robot.},
  title        = {An efficient RRT-based motion planning algorithm for autonomous underwater vehicles under cylindrical sampling constraints},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive optimal controller design for an unbalanced UAV
with slung load. <em>AR</em>, <em>47</em>(3), 267–280. (<a
href="https://doi.org/10.1007/s10514-023-10090-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load transportation by Unmanned Aerial Vehicles is a research topic of great interest to the robotic community for its numerous applications in both the civilian and military fields. Attaching a cargo through an elastic cable to a small underactuated UAV such as a quadcopter, which is inherently an unstable system, increases its instability and its underactuated degrees of freedom by three. Moreover, the presence of imperfections in the system such as having the quadcopter’s center of gravity and the cable hanging point arbitrarily shifted from the quadcopter’s geometric centroid further complicates the system. In this paper, a new nonlinear nine degree-of-freedom mathematical model is formulated for a quadcopter when its center of gravity is shifted from its geometric centroid and when a cable-suspended load is attached at an arbitrary position. Thus, a novel adaptive controller based on Linear Quadratic Regulator is designed to control the position and attitude of the quadcopter while minimizing the swinging and radial motions of the suspended load. Subsequently, nonlinear simulations are conducted for three case studies: conventional quadcopter, quadcopter-payload system without imperfections, quadcopter-payload system with imperfections. Finally, the results are presented demonstrating the effectiveness of the proposed control strategy.},
  archive      = {J_AR},
  author       = {Tolba, Mohamed and Shirinzadeh, Bijan and El-Bayoumi, Gamal and Mohamady, Osama},
  doi          = {10.1007/s10514-023-10090-z},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {3},
  pages        = {267-280},
  shortjournal = {Auton. Robot.},
  title        = {Adaptive optimal controller design for an unbalanced UAV with slung load},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). That was not what i was aiming at! Differentiating human
intent and outcome in a physically dynamic throwing task. <em>AR</em>,
<em>47</em>(2), 249–265. (<a
href="https://doi.org/10.1007/s10514-022-10074-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person’s reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1D-CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.},
  archive      = {J_AR},
  author       = {Surendran, Vidullan and Wagner, Alan R.},
  doi          = {10.1007/s10514-022-10074-5},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {249-265},
  shortjournal = {Auton. Robot.},
  title        = {That was not what i was aiming at! differentiating human intent and outcome in a physically dynamic throwing task},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative programming of robotic task decisions and
recovery behaviors. <em>AR</em>, <em>47</em>(2), 229–247. (<a
href="https://doi.org/10.1007/s10514-022-10062-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming by demonstration is reaching industrial applications, which allows non-experts to teach new tasks without manual code writing. However, a certain level of complexity, such as online decision making or the definition of recovery behaviors, still requires experts that use conventional programming methods. Even though, experts cannot foresee all possible faults in a robotic application. To encounter this, we present a framework where user and robot collaboratively program a task that involves online decision making and recovery behaviors. Hereby, a task-graph is created that represents a production task and possible alternative behaviors. Nodes represent start, end or decision states and links define actions for execution. This graph can be incrementally extended by autonomous anomaly detection, which requests the user to add knowledge for a specific recovery action. Besides our proposed approach, we introduce two alternative approaches that manage recovery behavior programming and compare all approaches extensively in a user study involving 21 subjects. This study revealed the strength of our framework and analyzed how users act to add knowledge to the robot. Our findings proclaim to use a framework with a task-graph based knowledge representation and autonomous anomaly detection not only for initiating recovery actions but particularly to transfer those to a robot.},
  archive      = {J_AR},
  author       = {Eiband, Thomas and Willibald, Christoph and Tannert, Isabel and Weber, Bernhard and Lee, Dongheui},
  doi          = {10.1007/s10514-022-10062-9},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {229-247},
  shortjournal = {Auton. Robot.},
  title        = {Collaborative programming of robotic task decisions and recovery behaviors},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMORES-EP, a modular robot with parallel self-assembly.
<em>AR</em>, <em>47</em>(2), 211–228. (<a
href="https://doi.org/10.1007/s10514-022-10078-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-assembly of modular robotic systems enables the construction of complex robotic configurations to adapt to different tasks. This paper presents a framework for SMORES types of modular robots to efficiently self-assemble into tree topologies. These modular robots form kinematic chains that have been shown to be capable of a large variety of manipulation and locomotion tasks, yet they can reconfigure using a mobile reconfiguration. A desired kinematic topology can be mapped onto a planar pattern with the optimal module assignment based on the modules’ locations, then the mobile reconfiguration assembly process can be executed in parallel. A docking controller is developed to guarantee the success of docking processes. A hybrid control architecture is designed to handle a large number of modules and complex behaviors of each individual, and achieve efficient and robust self-assembly actions. The framework is demonstrated in both hardware and simulation on the SMORES-EP platform.},
  archive      = {J_AR},
  author       = {Liu, Chao and Lin, Qian and Kim, Hyun and Yim, Mark},
  doi          = {10.1007/s10514-022-10078-1},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {211-228},
  shortjournal = {Auton. Robot.},
  title        = {SMORES-EP, a modular robot with parallel self-assembly},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware system synthesis, task assignment, and
routing. <em>AR</em>, <em>47</em>(2), 193–210. (<a
href="https://doi.org/10.1007/s10514-022-10076-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and organization of complex robotic systems traditionally requires laborious trial-and-error processes to ensure both hardware and software components are correctly connected with the resources necessary for computation. This paper presents a novel generalization of the quadratic assignment and routing problem, introducing formalisms for selecting components and interconnections to synthesize a complete system capable of providing some user-defined functionality. By introducing mission context, functional requirements, and modularity directly into the assignment problem, we derive a solution where components are automatically selected and placed into an optimal organization of hardware and software, all while respecting restrictions on component viability and required functionality. The ability to generate complete functional systems directly from individual components reduces manual design effort by allowing for a guided exploration of the design space. Additionally, our formulation increases resiliency by quantifying resource margins and enabling adaptation of system structure in response to changing environments, hardware or software failure. The proposed formulation is cast as an integer linear program which is provably $${\mathcal {N}}{\mathcal {P}}$$ -hard. Two case studies are analyzed to highlight the expressiveness and complexity of problems that are addressable by this approach: the first explores the iterative development of a ground-based search-and-rescue robot in varying mission contexts, while the second compares an automated humanoid design against a manual design that was finalist in the DARPA Robotics Challenge. Numerical simulations quantify real-world performance and demonstrate tractable time complexity for the scale of problems encountered in many modern robotic systems.},
  archive      = {J_AR},
  author       = {Ziglar, Jason and Williams, Ryan K. and Wicks, Alfred},
  doi          = {10.1007/s10514-022-10076-3},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {193-210},
  shortjournal = {Auton. Robot.},
  title        = {Context-aware system synthesis, task assignment, and routing},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot re-identification of the speaker by social robots.
<em>AR</em>, <em>47</em>(2), 181–192. (<a
href="https://doi.org/10.1007/s10514-022-10073-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays advanced machine learning, computer vision, audio analysis and natural language understanding systems can be widely used for improving the perceptive and reasoning capabilities of the social robots. In particular, artificial intelligence algorithms for speaker re-identification make the robot aware of its interlocutor and able to personalize the conversation according to the information gathered in real-time and in the past interactions with the speaker. Anyway, this kind of application requires to train neural networks having available only a few samples for each speaker. Within this context, in this paper we propose a social robot equipped with a microphone sensor and a smart deep learning algorithm for few-shot speaker re-identification, able to run in real time over an embedded platform mounted on board of the robot. The proposed system has been experimentally evaluated over the VoxCeleb1 dataset, demonstrating a remarkable re-identification accuracy by varying the number of samples per speaker, the number of known speakers and the duration of the samples, and over the SpReW dataset, showing its robustness in real noisy environments. Finally, a quantitative evaluation of the processing time over the embedded platform proves that the processing pipeline is almost immediate, resulting in a pleasant user experience.},
  archive      = {J_AR},
  author       = {Foggia, Pasquale and Greco, Antonio and Roberto, Antonio and Saggese, Alessia and Vento, Mario},
  doi          = {10.1007/s10514-022-10073-6},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {181-192},
  shortjournal = {Auton. Robot.},
  title        = {Few-shot re-identification of the speaker by social robots},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous exploration with online learning of traversable
yet visually rigid obstacles. <em>AR</em>, <em>47</em>(2), 161–180. (<a
href="https://doi.org/10.1007/s10514-022-10075-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns online learning of terrain properties combining haptic perception with exteroceptive sensing to reason about forces needed to pass through terrains that visually appear as untraversable obstacles. Terrain learning is studied within the context of autonomous exploration. We propose predicting the traversability of potentially obstructing terrains by active perception to establish a connection between the observed geometric environment model and deliberately sampled forces to pass through the terrain using a haptic sensor that probes the terrain in front of the robot. The developed solution uses a Gaussian Process regressor in online learning and force prediction. The robot is navigated by following the information gain to improve traversability and spatial models. The proposed approach has been experimentally verified in fully autonomous exploration with a multi-legged walking robot. The robot is navigated through visually looking obstacles and explores “hidden” areas while following the expected information gain to explore the terrain properties of the mission area.},
  archive      = {J_AR},
  author       = {Prágr, Miloš and Bayer, Jan and Faigl, Jan},
  doi          = {10.1007/s10514-022-10075-4},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {161-180},
  shortjournal = {Auton. Robot.},
  title        = {Autonomous exploration with online learning of traversable yet visually rigid obstacles},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous mapping and spectroscopic analysis of distributed
radiation fields using aerial robots. <em>AR</em>, <em>47</em>(2),
139–160. (<a href="https://doi.org/10.1007/s10514-022-10064-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a strategy for field estimation and informative path planning towards autonomous mapping and radiological characterization of distributed gamma radiation fields within confined GPS-denied environments using aerial robots. First, an online distributed radiation field estimation and spectroscopic analysis framework is presented which combines sequentially acquired measurements to estimate both the field intensity and gradient and propagate the belief over the initially unknown map, while simultaneously classifying each map region with respect to its dominating isotope. As such a process depends on the quality of the acquired measurements and given the limited endurance of small flying robots, we further contribute an informative path planner responsible for iteratively guiding the robot towards the next-best radiation measurement location such that high estimation confidence is achieved in short time. A divided global and local planning architecture is proposed enabling the robot to guide itself towards the radiologically most interesting areas quickly and acquire sufficient measurements within those. We further develop a tailor-made collision-tolerant micro flying robot that is equipped with a lightweight scintillator and silicon photomultiplier combination, alongside GPS-denied localization and mapping capabilities. A set of experimental studies are presented involving the autonomous characterization of distributed radiation fields containing live uranium ore and radium sources within GPS-denied industrial settings.},
  archive      = {J_AR},
  author       = {Mascarich, Frank and Kulkarni, Mihir and De Petris, Paolo and Wilson, Taylor and Alexis, Kostas},
  doi          = {10.1007/s10514-022-10064-7},
  journal      = {Autonomous Robots},
  month        = {2},
  number       = {2},
  pages        = {139-160},
  shortjournal = {Auton. Robot.},
  title        = {Autonomous mapping and spectroscopic analysis of distributed radiation fields using aerial robots},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Efficiently exploring for human robot interaction:
Partially observable poisson processes. <em>AR</em>, <em>47</em>(1),
121–138. (<a href="https://doi.org/10.1007/s10514-022-10070-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a mobile robot exploring an office building with the aim of observing as much human activity as possible over several days. It must learn where and when people are to be found, count the observed activities, and revisit popular places at the right time. In this paper we present a series of Bayesian estimators for the levels of human activity that improve on simple counting. We then show how these estimators can be used to drive efficient exploration for human activities. The estimators arise from modelling the human activity counts as a partially observable Poisson process (POPP). This paper presents novel extensions to POPP for the following cases: (i) the robot’s sensors are correlated, (ii) the robot’s sensor model, itself built from data, is also unreliable, (iii) both are combined. It also combines the resulting Bayesian estimators with a simple, but effective solution to the exploration-exploitation trade-off faced by the robot in a real deployment. A series of 15 day robot deployments show how our approach boosts the number of human activities observed by 70% relative to a baseline and produces more accurate estimates of the level of human activity in each place and time.},
  archive      = {J_AR},
  author       = {Jovan, Ferdian and Tomy, Milan and Hawes, Nick and Wyatt, Jeremy},
  doi          = {10.1007/s10514-022-10070-9},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {121-138},
  shortjournal = {Auton. Robot.},
  title        = {Efficiently exploring for human robot interaction: Partially observable poisson processes},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater visual mapping of curved ship hull surface using
stereo vision. <em>AR</em>, <em>47</em>(1), 109–120. (<a
href="https://doi.org/10.1007/s10514-022-10071-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an underwater visual mapping method for the three-dimensional (3D) reconstruction of a moderately curved ship hull surface using a stereo vision system. The proposed approach estimates the local hull surface by extracting 3D point clouds from stereo image pairs, and the relative poses between the pairs are calculated by matching the corresponding point cloud points. A surfel model is extracted from each point cloud set by fitting a plane model, and a smoothness factor is applied between nearby surfels for a smooth 3D surface reconstruction. The camera trajectory and the surfel map are optimized through the graph-based simultaneous localization and mapping (SLAM) framework. A 3D surface mesh is generated with the optimized surfel poses, and the corresponding images are projected into the surface plane, texturing the surface. The performance of the proposed approach is shown with experimental data obtained from an actual ship hull inspection.},
  archive      = {J_AR},
  author       = {Chung, Dongha and Kim, Jinwhan},
  doi          = {10.1007/s10514-022-10071-8},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {109-120},
  shortjournal = {Auton. Robot.},
  title        = {Underwater visual mapping of curved ship hull surface using stereo vision},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AUV navigation using cues in the sand ripples. <em>AR</em>,
<em>47</em>(1), 95–107. (<a
href="https://doi.org/10.1007/s10514-022-10069-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsea navigation by autonomous underwater vehicles (AUVs) is a demanding task that involves the integration of inertial sensors, gyrocompasses, Doppler velocity loggers, and reference from acoustic beacons. In this paper, we propose to augment this information by providing an external measurement of heading change. We rely on the direction of sand ripples, which are abundant on the seabed near the shore and whose direction is, locally, constant. Thus, any apparent change in their directivity, as detected by the AUV, would reflect as a change in the vehicle’s heading. Considering this, we developed a mechanism that detects regions of interest (ROIs) containing sand ripples within a synthetic aperture sonar (SAS) image, segments the ROI into highlight and shadow, and evaluates the angle difference between ROIs within two consecutive SAS images. For detection of sand ripples and estimation of angle difference, we employ two deep neural networks, while for segmentation we formulate a fuzzy-logic clustering. Taking advantage of a transfer learning approach, we trained the deep networks on simulated SAS images and on a large database of 2088 real SAS images, which we share for reproducibility. Results from real SAS images from three different sites show a good trade-off between precision and recall for sand-ripple detection, and an error of a few degrees in the heading change estimation, which well exceeds a geometrical-based benchmark. We also show performance from a real-time experiment for which we implemented our method on an AUV and estimated its heading change on-the-fly.},
  archive      = {J_AR},
  author       = {Shalev, Hadar and Nagar, Liav and Abu, Avi and Testolin, Alberto and Diamant, Roee},
  doi          = {10.1007/s10514-022-10069-2},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {95-107},
  shortjournal = {Auton. Robot.},
  title        = {AUV navigation using cues in the sand ripples},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized swarms of unmanned aerial vehicles for search
and rescue operations without explicit communication. <em>AR</em>,
<em>47</em>(1), 77–93. (<a
href="https://doi.org/10.1007/s10514-022-10066-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a distributed autonomous flocking behavior of Unmanned Aerial Vehicles (UAVs) in demanding outdoor conditions, motivated by search and rescue applications. We propose a novel approach for decentralized swarm navigation in the direction of a candidate object of interest (OOI) based on real-time detections from onboard RGB cameras. A novel self-adaptive communication strategy secures an efficient change of swarm azimuth to a higher priority direction based on the real-time detections. We introduce a local visual communication channel that establishes a network connection between neighboring robots without explicit communication to achieve high reliability and scalability of the system. As a case study, this novel method is applied for the deployment of a UAV swarm towards detected OOI for closer inspection and verification. The results of simulations and real-world experiments have verified the intended behavior of the swarm system for the detection of true positive and false positive OOI, as well as for cooperative environment exploration.},
  archive      = {J_AR},
  author       = {Horyna, Jiri and Baca, Tomas and Walter, Viktor and Albani, Dario and Hert, Daniel and Ferrante, Eliseo and Saska, Martin},
  doi          = {10.1007/s10514-022-10066-5},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {77-93},
  shortjournal = {Auton. Robot.},
  title        = {Decentralized swarms of unmanned aerial vehicles for search and rescue operations without explicit communication},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locomotion generation for quadruped robots on challenging
terrains via quadratic programming. <em>AR</em>, <em>47</em>(1), 51–76.
(<a href="https://doi.org/10.1007/s10514-022-10068-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a locomotion generation method for quadruped robots, which first computes an optimal trajectory of the robot’s center of mass (CoM) and then its whole-body motion through inverse kinematics. As the core component, the computing of the CoM trajectory, which is parameterized as polynomials, is based on the robot’s centroidal dynamics and it is observed that several terms in the centroidal dynamics are minor and can be omitted in the locomotion generation. Then, as a basic form of the proposed method, the CoM trajectory optimization is written as a quadratic programming (QP) problem for the case of given step sequences, timings and footholds. Furthermore, the uncertainty of the robot’s CoM, which is described as a convex polyhedron around a nominal CoM position, and the reachability of the robot’s feet, which is approximated as another convex polyhedron, can be added to the QP problem as linear inequality constraints. Ultimately, the planning of step sequences, timings, and footholds is all incorporated, leading to a single mixed-integer quadratic programming problem. Numerical and hardware experiments have been conducted and show that the proposed method can generate various walking motions for a quadruped robot to travel over challenging terrains.},
  archive      = {J_AR},
  author       = {Jiang, Xinyang and Chi, Wanchao and Zheng, Yu and Zhang, Shenghao and Ling, Yonggen and Xu, Jiafeng and Zhang, Zhengyou},
  doi          = {10.1007/s10514-022-10068-3},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {51-76},
  shortjournal = {Auton. Robot.},
  title        = {Locomotion generation for quadruped robots on challenging terrains via quadratic programming},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel framework for generalizing dynamic movement
primitives under kinematic constraints. <em>AR</em>, <em>47</em>(1),
37–50. (<a href="https://doi.org/10.1007/s10514-022-10067-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel framework for generalizing a desired trajectory pattern, encoded using Dynamic Movement Primitives (DMP), subject to kinematic constraints. DMP have been extensively used in robotics for encoding and reproducing kinematic behaviours, thanks to their generalization, stability and robustness properties. However, incorporating kinematic constraints has not yet been fully addressed. To this end, we design an optimization framework, based on the DMP formulation from our previous work, for generalizing trajectory patterns, encoded with DMP subject to kinematic constraints, considering also time-varying target and time duration, via-point and obstacle constraints. Simulations highlight these properties and comparisons are drawn with other approaches for enforcing constraints on DMP. The usefulness and applicability of the proposed framework is showcased in experimental scenarios, including a handover, where the target and time duration vary, and placing scenarios, where obstacles are dynamically introduced in the scene.},
  archive      = {J_AR},
  author       = {Sidiropoulos, Antonis and Papageorgiou, Dimitrios and Doulgeri, Zoe},
  doi          = {10.1007/s10514-022-10067-4},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {37-50},
  shortjournal = {Auton. Robot.},
  title        = {A novel framework for generalizing dynamic movement primitives under kinematic constraints},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active object perception using bayesian classifiers and
haptic exploration. <em>AR</em>, <em>47</em>(1), 19–36. (<a
href="https://doi.org/10.1007/s10514-022-10065-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To recognise objects using only tactile sensing, humans employ various haptic exploratory procedures (EPs). Because the time, effort, and information acquisition costs of different EPs vary, choosing the best EP for accurate and efficient perception is usually based on prior knowledge or experience, also known as active exploration. An active EP selection algorithm based on a Gaussian mixture modal and Bayesian classifier has been developed to empower robots with similar intelligence. To choose the best EP for the next perception iteration, the information gain and total time cost of all actions required to identify the object are both considered. Six EPs were realised using a designed robotic arm platform, allowing eight features representing the object’s surface and geometric properties to be extracted. To evaluate the algorithm, offline data and real-world experiments were used, with the random method as a comparison. According to the results, the active method outperformed the random method with higher accuracy and in significantly less time. It had an average of weighted information gain of 132.6 and a time cost ratio (spent/total time) of only 0.3.},
  archive      = {J_AR},
  author       = {Sun, Teng and Liu, Hongbin and Miao, Zhonghua},
  doi          = {10.1007/s10514-022-10065-6},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {19-36},
  shortjournal = {Auton. Robot.},
  title        = {Active object perception using bayesian classifiers and haptic exploration},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous search of an airborne release in urban
environments using informed tree planning. <em>AR</em>, <em>47</em>(1),
1–18. (<a href="https://doi.org/10.1007/s10514-022-10063-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of autonomous vehicles for source localisation is a key enabling tool for disaster response teams to safely and efficiently deal with chemical emergencies. Whilst much work has been performed on source localisation using autonomous systems, most previous works have assumed an open environment or employed simplistic obstacle avoidance, separate from the estimation procedure. In this paper, we explore the coupling of the path planning task for both source term estimation and obstacle avoidance in an adaptive framework. The proposed system intelligently produces potential gas sampling locations that will reliably inform the estimation engine by not sampling in the wake of buildings as frequently. Then a tree search is performed to generate paths toward the estimated source location that traverse around any obstacles and still allow for exploration of potentially superior sampling locations.The proposed informed tree planning algorithm is then tested against the standard Entrotaxis and Entrotaxis-Jump techniques in a series of high fidelity simulations. The proposed system is found to reduce source estimation error far more efficiently than its competitors in a feature rich environment, whilst also exhibiting vastly more consistent and robust results.},
  archive      = {J_AR},
  author       = {Rhodes, Callum and Liu, Cunjia and Westoby, Paul and Chen, Wen-Hua},
  doi          = {10.1007/s10514-022-10063-8},
  journal      = {Autonomous Robots},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Auton. Robot.},
  title        = {Autonomous search of an airborne release in urban environments using informed tree planning},
  volume       = {47},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
