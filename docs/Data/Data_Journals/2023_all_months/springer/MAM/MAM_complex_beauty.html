<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---34">MAM - 34</h2>
<ul>
<li><details>
<summary>
(2023). Natural and artificial intelligence: A comparative analysis
of cognitive aspects. <em>MAM</em>, <em>33</em>(4), 791–815. (<a
href="https://doi.org/10.1007/s11023-023-09646-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving from a behavioral definition of intelligence, which describes it as the ability to adapt to the surrounding environment and deal effectively with new situations (Anastasi, 1986), this paper explains to what extent the performance obtained by ChatGPT in the linguistic domain can be considered as intelligent behavior and to what extent they cannot. It also explains in what sense the hypothesis of decoupling between cognitive and problem-solving abilities, proposed by Floridi (2017) and Floridi and Chiriatti (2020) should be interpreted. The problem of symbolic grounding (Harnad, 1990) is then addressed to show the problematic relationship between ChatGPT and the natural environment, and thus the impossibility for it to understand the symbols it manipulates. To explain the reasons why ChatGPT does not succeed in this task, an investigation is carried out and a possible solution to the problem in the artificial domain is proposed by making a comparison with the natural ability of living beings to ground their own meanings from some basic cognitive-sensory aspects, which, it is explained, are directly related to the emergence of self-awareness in humans. Thus, the question is raised whether a possible and concrete solution to the Symbol Grounding Problem would involve in the artificial domain the development of cognitive abilities fully comparable to those of humans. Finally, I explain the difficulties that would have to be overcome before such a level could be reached, since human cognitive capacities are intimately linked to intersubjectivity and intercorporeality.},
  archive      = {J_MAM},
  author       = {Abbate, Francesco},
  doi          = {10.1007/s11023-023-09646-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {791-815},
  shortjournal = {Minds Mach.},
  title        = {Natural and artificial intelligence: A comparative analysis of cognitive aspects},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Encoding ethics to compute value-aligned norms.
<em>MAM</em>, <em>33</em>(4), 761–790. (<a
href="https://doi.org/10.1007/s11023-023-09649-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Norms have been widely enacted in human and agent societies to regulate individuals’ actions. However, although legislators may have ethics in mind when establishing norms, moral values are only sometimes explicitly considered. This paper advances the state of the art by providing a method for selecting the norms to enact within a society that best aligns with the moral values of such a society. Our approach to aligning norms and values is grounded in the ethics literature. Specifically, from the literature’s study of the relations between norms, actions, and values, we formally define how actions and values relate through the so-called value judgment function and how norms and values relate through the so-called norm promotion function. We show that both functions provide the means to compute value alignment for a set of norms. Moreover, we detail how to cast our decision-making problem as an optimisation problem: finding the norms that maximise value alignment. We also show how to solve our problem using off-the-shelf optimisation tools. Finally, we illustrate our approach with a specific case study on the European Value Study.},
  archive      = {J_MAM},
  author       = {Serramia, Marc and Rodriguez-Soto, Manel and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A. and Bistaffa, Filippo and Boddington, Paula and Wooldridge, Michael and Ansotegui, Carlos},
  doi          = {10.1007/s11023-023-09649-7},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {761-790},
  shortjournal = {Minds Mach.},
  title        = {Encoding ethics to compute value-aligned norms},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The principle-at-risk analysis (PaRA): Operationalising
digital ethics by bridging principles and operations of a digital ethics
advisory panel. <em>MAM</em>, <em>33</em>(4), 737–760. (<a
href="https://doi.org/10.1007/s11023-023-09654-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent attempts to develop and apply digital ethics principles to address the challenges of the digital transformation leave organisations with an operationalisation gap. To successfully implement such guidance, they must find ways to translate high-level ethics frameworks into practical methods and tools that match their specific workflows and needs. Here, we describe the development of a standardised risk assessment tool, the Principle-at-Risk Analysis (PaRA), as a means to close this operationalisation gap for a key level of the ethics infrastructure at many organisations – the work of an interdisciplinary ethics panel. The PaRA tool serves to guide and harmonise the work of the Digital Ethics Advisory Panel at the multinational science and technology company Merck KGaA in alignment with the principles outlined in the company’s Code of Digital Ethics. We examine how such a tool can be used as part of a multifaceted approach to operationalise high-level principles at an organisational level and provide general requirements for its implementation. We showcase its application in an example case dealing with the comprehensibility of consent forms in a data-sharing context at Syntropy, a collaborative technology platform for clinical research.},
  archive      = {J_MAM},
  author       = {Nemat, André T. and Becker, Sarah J. and Lucas, Simon and Thomas, Sean and Gadea, Isabel and Charton, Jean Enno},
  doi          = {10.1007/s11023-023-09654-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {737-760},
  shortjournal = {Minds Mach.},
  title        = {The principle-at-risk analysis (PaRA): Operationalising digital ethics by bridging principles and operations of a digital ethics advisory panel},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Yet another impossibility theorem in algorithmic fairness.
<em>MAM</em>, <em>33</em>(4), 715–735. (<a
href="https://doi.org/10.1007/s11023-023-09645-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a surge in research addressing the question which properties predictive algorithms ought to satisfy in order to be considered fair. Three of the most widely discussed criteria of fairness are the criteria called equalized odds, predictive parity, and counterfactual fairness. In this paper, I will present a new impossibility result involving these three criteria of algorithmic fairness. In particular, I will argue that there are realistic circumstances under which any predictive algorithm that satisfies counterfactual fairness will violate both other fairness criteria, that is, equalized odds and predictive parity. As will be shown, this impossibility result forces us to give up one of four intuitively plausible assumptions about algorithmic fairness. I will explain and motivate each of the four assumptions and discuss which of them can plausibly be given up in order to circumvent the impossibility.},
  archive      = {J_MAM},
  author       = {Beigang, Fabian},
  doi          = {10.1007/s11023-023-09645-x},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {715-735},
  shortjournal = {Minds Mach.},
  title        = {Yet another impossibility theorem in algorithmic fairness},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of naturalness in concept learning: A computational
study. <em>MAM</em>, <em>33</em>(4), 695–714. (<a
href="https://doi.org/10.1007/s11023-023-09652-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the learnability of natural concepts in the context of the conceptual spaces framework. Previous work proposed that natural concepts are represented by the cells of optimally partitioned similarity spaces, where optimality was defined in terms of a number of constraints. Among these is the constraint that optimally partitioned similarity spaces result in easily learnable concepts. While there is evidence that systems of concepts generally regarded as natural satisfy a number of the proposed optimality constraints, the connection between naturalness and learnability has been less well studied. To fill this gap, we conduct a computational study employing two standard models of concept learning. Applying these models to the learning of color concepts, we examine whether natural color concepts are more readily learned than nonnatural ones. Our findings warrant a positive answer to this question for both models employed, thus lending empirical support to the notion that learnability is a distinctive characteristic of natural concepts.},
  archive      = {J_MAM},
  author       = {Douven, Igor},
  doi          = {10.1007/s11023-023-09652-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {695-714},
  shortjournal = {Minds Mach.},
  title        = {The role of naturalness in concept learning: A computational study},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The ethics of online controlled experiments (a/b testing).
<em>MAM</em>, <em>33</em>(4), 667–693. (<a
href="https://doi.org/10.1007/s11023-023-09644-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online controlled experiments, also known as A/B tests, have become ubiquitous. While many practical challenges in running experiments at scale have been thoroughly discussed, the ethical dimension of A/B testing has been neglected. This article fills this gap in the literature by introducing a new, soft ethics and governance framework that explicitly recognizes how the rise of an experimentation culture in industry settings brings not only unprecedented opportunities to businesses but also significant responsibilities. More precisely, the article (a) introduces a set of principles to encourage ethical and responsible experimentation to protect users, customers, and society; (b) argues that ensuring compliance with the proposed principles is a complex challenge unlikely to be addressed by resorting to a one-solution response; (c) discusses the relevance and effectiveness of several mechanisms and policies in educating, governing, and incentivizing companies conducting online controlled experiments; and (d) offers a list of prompting questions specifically designed to help and empower practitioners by stimulating specific ethical deliberations and facilitating coordination among different groups of stakeholders.},
  archive      = {J_MAM},
  author       = {Polonioli, Andrea and Ghioni, Riccardo and Greco, Ciro and Juneja, Prathm and Tagliabue, Jacopo and Watson, David and Floridi, Luciano},
  doi          = {10.1007/s11023-023-09644-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {667-693},
  shortjournal = {Minds Mach.},
  title        = {The ethics of online controlled experiments (A/B testing)},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online altruism: What it is and how it differs from other
kinds of altruism. <em>MAM</em>, <em>33</em>(4), 641–666. (<a
href="https://doi.org/10.1007/s11023-023-09648-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Altruism is a well-studied phenomenon in the social sciences, but online altruism has received relatively little attention. In this article, we examine several cases of online altruism, and analyse the key characteristics of the phenomenon, in particular comparing and contrasting it against models of traditional donor behaviour. We suggest a novel definition of online altruism, and provide an in-depth, mixed-method study of a significant case, represented by the r/Assistance subreddit. We argue that online altruism can be characterized by its differing experiences compared to traditional giving, from a giver’s point of view, and unique mechanisms and actions made possible by the internet. These findings explain why people give to anonymous strangers online and provide a new perspective on altruism that is important in building a more altruistic internet and society.},
  archive      = {J_MAM},
  author       = {Lou, Katherine and Floridi, Luciano},
  doi          = {10.1007/s11023-023-09648-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {641-666},
  shortjournal = {Minds Mach.},
  title        = {Online altruism: What it is and how it differs from other kinds of altruism},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contestable AI by design: Towards a framework. <em>MAM</em>,
<em>33</em>(4), 613–639. (<a
href="https://doi.org/10.1007/s11023-022-09611-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the use of AI systems continues to increase, so do concerns over their lack of fairness, legitimacy and accountability. Such harmful automated decision-making can be guarded against by ensuring AI systems are contestable by design: responsive to human intervention throughout the system lifecycle. Contestable AI by design is a small but growing field of research. However, most available knowledge requires a significant amount of translation to be applicable in practice. A proven way of conveying intermediate-level, generative design knowledge is in the form of frameworks. In this article we use qualitative-interpretative methods and visual mapping techniques to extract from the literature sociotechnical features and practices that contribute to contestable AI, and synthesize these into a design framework.},
  archive      = {J_MAM},
  author       = {Alfrink, Kars and Keller, Ianus and Kortuem, Gerd and Doorn, Neelke},
  doi          = {10.1007/s11023-022-09611-z},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {613-639},
  shortjournal = {Minds Mach.},
  title        = {Contestable AI by design: Towards a framework},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Realising meaningful human control over automated driving
systems: A multidisciplinary approach. <em>MAM</em>, <em>33</em>(4),
587–611. (<a href="https://doi.org/10.1007/s11023-022-09608-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a framework to realise “meaningful human control” over Automated Driving Systems. The framework is based on an original synthesis of the results of the multidisciplinary research project “Meaningful Human Control over Automated Driving Systems” lead by a team of engineers, philosophers, and psychologists at Delft University of the Technology from 2017 to 2021. Meaningful human control aims at protecting safety and reducing responsibility gaps. The framework is based on the core assumption that human persons and institutions, not hardware and software and their algorithms, should remain ultimately—though not necessarily directly—in control of, and thus morally responsible for, the potentially dangerous operation of driving in mixed traffic. We propose an Automated Driving System to be under meaningful human control if it behaves according to the relevant reasons of the relevant human actors (tracking), and that any potentially dangerous event can be related to a human actor (tracing). We operationalise the requirements for meaningful human control through multidisciplinary work in philosophy, behavioural psychology and traffic engineering. The tracking condition is operationalised via a proximal scale of reasons and the tracing condition via an evaluation cascade table. We review the implications and requirements for the behaviour and skills of human actors, in particular related to supervisory control and driver education. We show how the evaluation cascade table can be applied in concrete engineering use cases in combination with the definition of core components to expose deficiencies in traceability, thereby avoiding so-called responsibility gaps. Future research directions are proposed to expand the philosophical framework and use cases, supervisory control and driver education, real-world pilots and institutional embedding},
  archive      = {J_MAM},
  author       = {de Sio, Filippo Santoni and Mecacci, Giulio and Calvert, Simeon and Heikoop, Daniel and Hagenzieker, Marjan and van Arem, Bart},
  doi          = {10.1007/s11023-022-09608-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {587-611},
  shortjournal = {Minds Mach.},
  title        = {Realising meaningful human control over automated driving systems: A multidisciplinary approach},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Democratizing AI from a sociotechnical perspective.
<em>MAM</em>, <em>33</em>(4), 563–586. (<a
href="https://doi.org/10.1007/s11023-023-09651-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) technologies offer new ways of conducting decision-making tasks that influence the daily lives of citizens, such as coordinating traffic, energy distributions, and crowd flows. They can sort, rank, and prioritize the distribution of fines or public funds and resources. Many of the changes that AI technologies promise to bring to such tasks pertain to decisions that are collectively binding. When these technologies become part of critical infrastructures, such as energy networks, citizens are affected by these decisions whether they like it or not, and they usually do not have much say in them. The democratic challenge for those working on AI technologies with collectively binding effects is both to develop and deploy technologies in such a way that the democratic legitimacy of the relevant decisions is safeguarded. In this paper, we develop a conceptual framework to help policymakers, project managers, innovators, and technologists to assess and develop approaches to democratize AI. This framework embraces a broad sociotechnical perspective that highlights the interactions between technology and the complexities and contingencies of the context in which these technologies are embedded. We start from the problem-based and practice-oriented approach to democracy theory as developed by political theorist Mark Warren. We build on this approach to describe practices that can enhance or challenge democracy in political systems and extend it to integrate a sociotechnical perspective and make the role of technology explicit. We then examine how AI technologies can play a role in these practices to improve or inhibit the democratic nature of political systems. We focus in particular on AI-supported political systems in the energy domain.},
  archive      = {J_MAM},
  author       = {Noorman, Merel and Swierstra, Tsjalling},
  doi          = {10.1007/s11023-023-09651-z},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {563-586},
  shortjournal = {Minds Mach.},
  title        = {Democratizing AI from a sociotechnical perspective},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spotting when algorithms are wrong. <em>MAM</em>,
<em>33</em>(4), 541–562. (<a
href="https://doi.org/10.1007/s11023-022-09591-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of sociotechnical systems often have no way to independently verify whether the system output which they use to make decisions is correct; they are epistemically dependent on the system. We argue that this leads to problems when the system is wrong, namely to bad decisions and violations of the norm of practical reasoning. To prevent this from occurring we suggest the implementation of defeaters: information that a system is unreliable in a specific case (undercutting defeat) or independent information that the output is wrong (rebutting defeat). Practically, we suggest to design defeaters based on the different ways in which a system might produce erroneous outputs, and analyse this suggestion with a case study of the risk classification algorithm used by the Dutch tax agency.},
  archive      = {J_MAM},
  author       = {Buijsman, Stefan and Veluwenkamp, Herman},
  doi          = {10.1007/s11023-022-09591-0},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {541-562},
  shortjournal = {Minds Mach.},
  title        = {Spotting when algorithms are wrong},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avatars as proxies. <em>MAM</em>, <em>33</em>(3), 525–539.
(<a href="https://doi.org/10.1007/s11023-023-09643-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Avatars will represent us online, in virtual worlds, and in technologically supported hybrid environments. We and our avatars will stand not in an identity relation but in a proxy relation, an arrangement that is significant not least because our proxies’ actions can be counted as our own. However, this proxy relation between humans and avatars is not well understood and its consequences under-explored. In this paper I explore the relation and its potential ethical consequences.},
  archive      = {J_MAM},
  author       = {Sweeney, Paula},
  doi          = {10.1007/s11023-023-09643-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {525-539},
  shortjournal = {Minds Mach.},
  title        = {Avatars as proxies},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Can deep CNNs avoid infinite regress/circularity in content
constitution? <em>MAM</em>, <em>33</em>(3), 507–524. (<a
href="https://doi.org/10.1007/s11023-023-09642-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The representations of deep convolutional neural networks (CNNs) are formed from generalizing similarities and abstracting from differences in the manner of the empiricist theory of abstraction (Buckner, Synthese 195:5339–5372, 2018). The empiricist theory of abstraction is well understood to entail infinite regress and circularity in content constitution (Husserl, Logical Investigations. Routledge, 2001). This paper argues these entailments hold a fortiori for deep CNNs. Two theses result: deep CNNs require supplementation by Quine’s “apparatus of identity and quantification” in order to (1) achieve concepts, and (2) represent objects, as opposed to “half-entities” corresponding to similarity amalgams (Quine, Quintessence, Cambridge, 2004, p. 107). Similarity amalgams are also called “approximate meaning[s]” (Marcus &amp; Davis, Rebooting AI, Pantheon, 2019, p. 132). Although Husserl inferred the “complete abandonment of the empiricist theory of abstraction” (a fortiori deep CNNs) due to the infinite regress and circularity arguments examined in this paper, I argue that the statistical learning of deep CNNs may be incorporated into a Fodorian hybrid account that supports Quine’s “sortal predicates, negation, plurals, identity, pronouns, and quantifiers” which are representationally necessary to overcome the regress/circularity in content constitution and achieve objective (as opposed to similarity-subjective) representation (Burge, Origins of Objectivity. Oxford, 2010, p. 238). I base myself initially on Yoshimi’s (New Frontiers in Psychology, 2011) attempt to explain Husserlian phenomenology with neural networks but depart from him due to the arguments and consequently propose a two-system view which converges with Weiskopf’s proposal (“Observational Concepts.” The Conceptual Mind. MIT, 2015. 223–248).},
  archive      = {J_MAM},
  author       = {Lopes, Jesse},
  doi          = {10.1007/s11023-023-09642-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {507-524},
  shortjournal = {Minds Mach.},
  title        = {Can deep CNNs avoid infinite Regress/Circularity in content constitution?},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The non-theory-driven character of computer simulations and
their role as exploratory strategies. <em>MAM</em>, <em>33</em>(3),
487–505. (<a href="https://doi.org/10.1007/s11023-023-09641-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, I focus on the role of computer simulations as exploratory strategies. I begin by establishing the non-theory-driven nature of simulations. This refers to their ability to characterize phenomena without relying on a predefined conceptual framework that is provided by an implemented mathematical model. Drawing on Steinle’s notion of exploratory experimentation and Gelfert’s work on exploratory models, I present three exploratory strategies for computer simulations: (1) starting points and continuation of scientific inquiry, (2) varying the parameters, and (3) scientific prototyping.},
  archive      = {J_MAM},
  author       = {Durán, Juan M.},
  doi          = {10.1007/s11023-023-09641-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {487-505},
  shortjournal = {Minds Mach.},
  title        = {The non-theory-driven character of computer simulations and their role as exploratory strategies},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reasoning with concepts: A unifying framework. <em>MAM</em>,
<em>33</em>(3), 451–485. (<a
href="https://doi.org/10.1007/s11023-023-09640-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few decades, cognitive science has identified several forms of reasoning that make essential use of conceptual knowledge. Despite significant theoretical and empirical progress, there is still no unified framework for understanding how concepts are used in reasoning. This paper argues that the theory of conceptual spaces is capable of filling this gap. Our strategy is to demonstrate how various inference mechanisms which clearly rely on conceptual information—including similarity, typicality, and diagnosticity-based reasoning—can be modeled using principles derived from conceptual spaces. Our first topic analyzes the role of expectations in inductive reasoning and their relation to the structure of our concepts. We examine the relationship between using generic expressions in natural language and common-sense reasoning as a second topic. We propose that the strength of a generic can be described by distances between properties and prototypes in conceptual spaces. Our third topic is category-based induction. We demonstrate that the theory of conceptual spaces can serve as a comprehensive model for this type of reasoning. The final topic is analogy. We review some proposals in this area, present a taxonomy of analogical relations, and show how to model them in terms of distances in conceptual spaces. We also briefly discuss the implications of the model for reasoning with concepts in artificial systems.},
  archive      = {J_MAM},
  author       = {Gärdenfors, Peter and Osta-Vélez, Matías},
  doi          = {10.1007/s11023-023-09640-2},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {451-485},
  shortjournal = {Minds Mach.},
  title        = {Reasoning with concepts: A unifying framework},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machines learn better with better data ontology: Lessons
from philosophy of induction and machine learning practice.
<em>MAM</em>, <em>33</em>(3), 429–450. (<a
href="https://doi.org/10.1007/s11023-023-09639-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As scientists start to adopt machine learning (ML) as one research tool, the security of ML and the knowledge generated become a concern. In this paper, I explain how supervised ML can be improved with better data ontology, or the way we make categories and turn information into data. More specifically, we should design data ontology in such a way that is consistent with the knowledge that we have about the target phenomenon so that such ontology can help us make the inductive leap. I do so by thinking through a thought experiment, Goodman’s New Riddle of Induction (Fact, fiction, and forecast, Harvard University Press, 1955). Goodman’s riddle helps flesh out three problems of induction: (1) the problem of equal goodies, that there are often too many equally good inductive results given the same data; (2) the problem of diverging performance, that these equally good results can give opposite predictions in the future; and (3) the problem of mediocrity, that when averaged across all equally possible datasets and tasks, no inductive algorithm outperforms any other. I show that all these three problems are manifested as real obstacles in ML practice, namely, the Rashomon effect (Breiman in Stat Sci 16(3):199–231, 2001), the problem of underspecification (D’Amour et al. in J Mach Learn Res, 2020, https://doi.org/10.48550/arXiv.2011.03395 ), and the No Free Lunch theorem (Wolpert in Neural Comput 8(7):1341–90, 1996, https://doi.org/10.1162/neco.1996.8.7.1341 ). Lastly, I argue that proper data ontology can help mitigate these problems and I demonstrate how using concrete examples from climate science. This research highlights the links between philosophers’ discussions of induction and implications in ML practice.},
  archive      = {J_MAM},
  author       = {Li, Dan},
  doi          = {10.1007/s11023-023-09639-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {429-450},
  shortjournal = {Minds Mach.},
  title        = {Machines learn better with better data ontology: Lessons from philosophy of induction and machine learning practice},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An alternative to cognitivism: Computational phenomenology
for deep learning. <em>MAM</em>, <em>33</em>(3), 397–427. (<a
href="https://doi.org/10.1007/s11023-023-09638-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a non-representationalist framework for deep learning relying on a novel method computational phenomenology, a dialogue between the first-person perspective (relying on phenomenology) and the mechanisms of computational models. We thereby propose an alternative to the modern cognitivist interpretation of deep learning, according to which artificial neural networks encode representations of external entities. This interpretation mainly relies on neuro-representationalism, a position that combines a strong ontological commitment towards scientific theoretical entities and the idea that the brain operates on symbolic representations of these entities. We proceed as follows: after offering a review of cognitivism and neuro-representationalism in the field of deep learning, we first elaborate a phenomenological critique of these positions; we then sketch out computational phenomenology and distinguish it from existing alternatives; finally we apply this new method to deep learning models trained on specific tasks, in order to formulate a conceptual framework of deep-learning, that allows one to think of artificial neural networks’ mechanisms in terms of lived experience.},
  archive      = {J_MAM},
  author       = {Beckmann, Pierre and Köstner, Guillaume and Hipólito, Inês},
  doi          = {10.1007/s11023-023-09638-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {397-427},
  shortjournal = {Minds Mach.},
  title        = {An alternative to cognitivism: Computational phenomenology for deep learning},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing artificial human-like arithmetical intelligence
(and why). <em>MAM</em>, <em>33</em>(3), 379–396. (<a
href="https://doi.org/10.1007/s11023-023-09636-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Why would we want to develop artificial human-like arithmetical intelligence, when computers already outperform humans in arithmetical calculations? Aside from arithmetic consisting of much more than mere calculations, one suggested reason is that AI research can help us explain the development of human arithmetical cognition. Here I argue that this question needs to be studied already in the context of basic, non-symbolic, numerical cognition. Analyzing recent machine learning research on artificial neural networks, I show how AI studies could potentially shed light on the development of human numerical abilities, from the proto-arithmetical abilities of subitizing and estimating to counting procedures. Although the current results are far from conclusive and much more work is needed, I argue that AI research should be included in the interdisciplinary toolbox when we try to explain the development and character of numerical cognition and arithmetical intelligence. This makes it relevant also for the epistemology of mathematics.},
  archive      = {J_MAM},
  author       = {Pantsar, Markus},
  doi          = {10.1007/s11023-023-09636-y},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {379-396},
  shortjournal = {Minds Mach.},
  title        = {Developing artificial human-like arithmetical intelligence (and why)},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable AI and causal understanding: Counterfactual
approaches considered. <em>MAM</em>, <em>33</em>(2), 347–377. (<a
href="https://doi.org/10.1007/s11023-023-09637-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The counterfactual approach to explainable AI (XAI) seeks to provide understanding of AI systems through the provision of counterfactual explanations. In a recent systematic review, Chou et al. (Inform Fus 81:59–83, 2022) argue that the counterfactual approach does not clearly provide causal understanding. They diagnose the problem in terms of the underlying framework within which the counterfactual approach has been developed. To date, the counterfactual approach has not been developed in concert with the approach for specifying causes developed by Pearl (Causality: Models, reasoning, and inference. Cambridge University Press, 2000) and Woodward (Making things happen: A theory of causal explanation. Oxford University Press, 2003). In this paper, I build on Chou et al.’s work by applying the Pearl-Woodward approach. I argue that the standard counterfactual approach to XAI is capable of delivering causal understanding, but that there are limitations on its capacity to do so. I suggest a way to overcome these limitations.},
  archive      = {J_MAM},
  author       = {Baron, Sam},
  doi          = {10.1007/s11023-023-09637-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {347-377},
  shortjournal = {Minds Mach.},
  title        = {Explainable AI and causal understanding: Counterfactual approaches considered},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The puzzling resilience of multiple realization.
<em>MAM</em>, <em>33</em>(2), 321–345. (<a
href="https://doi.org/10.1007/s11023-023-09635-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the multiple realization argument, mental states or processes can be realized in diverse and heterogeneous physical systems; and that fact implies that mental state or process kinds cannot be identified with particular kinds of physical states or processes. More specifically, mental processes cannot be identified with brain processes. Moreover, the argument provides a general model for the autonomy of the special sciences. The multiple realization argument is widely influential, but over the last thirty years it has also faced serious objections. Despite those objections, most philosophers regard the fact of multiple realization and the cogency of the multiple realization argument as plainly correct. Why is that? What is it about the multiple realization argument that makes it so resilient? One reason is that the multiple realization argument is deeply intertwined with a view that minds are, in some sense, computational. But we argue that the sense in which minds are computational does not support the conclusion that they are ipso facto multiply realized. We argue that the sense in which brains compute does not imply that brains implement multiply realizable computational processes, and it does not provide a general model for the autonomy of the special sciences.},
  archive      = {J_MAM},
  author       = {Polger, Thomas W. and Shapiro, Lawrence A.},
  doi          = {10.1007/s11023-023-09635-z},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {321-345},
  shortjournal = {Minds Mach.},
  title        = {The puzzling resilience of multiple realization},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of a priori belief in the design and analysis of
fault-tolerant distributed systems. <em>MAM</em>, <em>33</em>(2),
293–319. (<a href="https://doi.org/10.1007/s11023-023-09631-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The debate around the notions of a priori knowledge and a posteriori knowledge has proven crucial for the development of many fields in philosophy, such as metaphysics, epistemology, metametaphysics etc. We advocate that the recent debate on the two notions is also fruitful for man-made distributed computing systems and for the epistemic analysis thereof. Following a recently proposed modal and fallibilistic account of a priori knowledge, we elaborate the corresponding concept of a priori belief: We propose a rich taxonomy of types of a priori beliefs and their role for the different agents that participate in the system engineering process, which match the existing view exceedingly well and are particularly promising for explaining and dealing with unexpected behaviors in fault-tolerant distributed systems. Developing such a philosophical foundation will provide a sound basis for eventually implementing our ideas in a suitable epistemic reasoning and analysis framework and, hence, constitutes a mandatory first step for developing methods and tools to cope with the various challenges that emerge in such systems.},
  archive      = {J_MAM},
  author       = {Cignarale, Giorgio and Schmid, Ulrich and Tahko, Tuomas and Kuznets, Roman},
  doi          = {10.1007/s11023-023-09631-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {293-319},
  shortjournal = {Minds Mach.},
  title        = {The role of a priori belief in the design and analysis of fault-tolerant distributed systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The blueprint for an AI bill of rights: In search of
enaction, at risk of inaction. <em>MAM</em>, <em>33</em>(2), 285–292.
(<a href="https://doi.org/10.1007/s11023-023-09625-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US is promoting a new vision of a “Good AI Society” through its recent AI Bill of Rights. This offers a promising vision of community-oriented equity unique amongst peer countries. However, it leaves the door open for potential rights violations. Furthermore, it may have some federal impact, but it is non-binding, and without concrete legislation, the private sector is likely to ignore it.},
  archive      = {J_MAM},
  author       = {Hine, Emmie and Floridi, Luciano},
  doi          = {10.1007/s11023-023-09625-1},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {285-292},
  shortjournal = {Minds Mach.},
  title        = {The blueprint for an AI bill of rights: In search of enaction, at risk of inaction},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta’s oversight board: A review and critical assessment.
<em>MAM</em>, <em>33</em>(2), 261–284. (<a
href="https://doi.org/10.1007/s11023-022-09613-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the announcement and establishment of the Oversight Board (OB) by the technology company Meta as an independent institution reviewing Facebook and Instagram’s content moderation decisions, the OB  has been subjected to scholarly scrutiny ranging from praise to criticism. However, there is currently no overarching framework for understanding the OB’s various strengths and weaknesses. Consequently, this article analyses, organises, and supplements academic literature, news articles, and Meta and OB documents to understand the OB’s strengths and weaknesses and how it can be improved. Significant strengths include its ability to enhance the transparency of content moderation decisions and processes, to effect reform indirectly through policy recommendations, and its assertiveness in interpreting its jurisdiction and overruling Meta. Significant weaknesses include its limited jurisdiction, limited impact, Meta’s control over the OB’s precedent, and its lack of diversity. The analysis of  a recent OB case in Ethiopia shows these strengths and weaknesses in practice. The OB’s relationship with Meta and governments will lead to challenges and opportunities shaping its future development. Reforms to the OB should improve the OB’s control over its precedent, apply OB precedent to currently disputed cases, and clarify the standards for invoking OB precedent. Finally, these reforms provide the foundation for an additional improvement to address the OB’s institutional weaknesses, by involving users in determining whether the OB’s precedent should be applied to decide current content moderation disputes.},
  archive      = {J_MAM},
  author       = {Wong, David and Floridi, Luciano},
  doi          = {10.1007/s11023-022-09613-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {261-284},
  shortjournal = {Minds Mach.},
  title        = {Meta’s oversight board: A review and critical assessment},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous force beyond armed conflict. <em>MAM</em>,
<em>33</em>(1), 251–260. (<a
href="https://doi.org/10.1007/s11023-023-09627-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proposals by the San Francisco Police Department (SFPD) to use bomb disposal robots for deadly force against humans have met with widespread condemnation. Media coverage of the furore has tended, incorrectly, to conflate these robots with autonomous weapon systems (AWS), the AI-based weapons used in armed conflict. These two types of systems should be treated as distinct since they have different sets of social, ethical, and legal implications. However, the conflation does raise a pressing question: what if the SFPD had proposed using AWS for law enforcement purposes? This article argues that current debate on AWS takes place within a ‘killing paradigm’ that leaves us ill-placed to understand the implications of using autonomous force outside of armed conflict. This is because ‘lethality’ is taken as the paradigmatic form of harm, meaning that other harms are understood in a way that is derivative of lethal harm. The article calls for more research on how other sorts of goods, such as freedom from domination, are imperilled by the use of autonomous force. The article also suggests that bringing research on AWS into dialogue with the sorts of concerns that have typically motivated political theory—e.g. power, coercion, the state—provides a fruitful starting point for addressing these issues.},
  archive      = {J_MAM},
  author       = {Blanchard, Alexander},
  doi          = {10.1007/s11023-023-09627-z},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {251-260},
  shortjournal = {Minds Mach.},
  title        = {Autonomous force beyond armed conflict},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: The switch, the ladder, and the matrix:
Models for classifying AI systems. <em>MAM</em>, <em>33</em>(1), 249.
(<a href="https://doi.org/10.1007/s11023-023-09626-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Mökander, Jakob and Sheth, Margi and Watson, David S. and Floridi, Luciano},
  doi          = {10.1007/s11023-023-09626-0},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {249},
  shortjournal = {Minds Mach.},
  title        = {Correction to: the switch, the ladder, and the matrix: models for classifying AI systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). The switch, the ladder, and the matrix: Models for
classifying AI systems. <em>MAM</em>, <em>33</em>(1), 221–248. (<a
href="https://doi.org/10.1007/s11023-022-09620-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organisations that design and deploy artificial intelligence (AI) systems increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. One major obstacle organisations face when attempting to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the purpose of implementing AI governance in practice. We find that attempts to classify AI systems proposed in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, input data, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the vocabulary needed to demarcate the material scope of their AI governance frameworks.},
  archive      = {J_MAM},
  author       = {Mökander, Jakob and Sheth, Margi and Watson, David S. and Floridi, Luciano},
  doi          = {10.1007/s11023-022-09620-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {221-248},
  shortjournal = {Minds Mach.},
  title        = {The switch, the ladder, and the matrix: Models for classifying AI systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How a minimal learning agent can infer the existence of
unobserved variables in a complex environment. <em>MAM</em>,
<em>33</em>(1), 185–219. (<a
href="https://doi.org/10.1007/s11023-022-09619-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to a mainstream position in contemporary cognitive science and philosophy, the use of abstract compositional concepts is amongst the most characteristic indicators of meaningful deliberative thought in an organism or agent. In this article, we show how the ability to develop and utilise abstract conceptual structures can be achieved by a particular kind of learning agent. More specifically, we provide and motivate a concrete operational definition of what it means for these agents to be in possession of abstract concepts, before presenting an explicit example of a minimal architecture that supports this capability. We then proceed to demonstrate how the existence of abstract conceptual structures can be operationally useful in the process of employing previously acquired knowledge in the face of new experiences, thereby vindicating the natural conjecture that the cognitive functions of abstraction and generalisation are closely related.},
  archive      = {J_MAM},
  author       = {Eva, Benjamin and Ried, Katja and Müller, Thomas and Briegel, Hans J.},
  doi          = {10.1007/s11023-022-09619-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {185-219},
  shortjournal = {Minds Mach.},
  title        = {How a minimal learning agent can infer the existence of unobserved variables in a complex environment},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enactivism meets mechanism: Tensions &amp; congruities in
cognitive science. <em>MAM</em>, <em>33</em>(1), 153–184. (<a
href="https://doi.org/10.1007/s11023-022-09618-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enactivism advances an understanding of cognition rooted in the dynamic interaction between an embodied agent and their environment, whilst new mechanism suggests that cognition is explained by uncovering the organised components underlying cognitive capacities. On the face of it, the mechanistic model’s emphasis on localisable and decomposable mechanisms, often neural in nature, runs contrary to the enactivist ethos. Despite appearances, this paper argues that mechanistic explanations of cognition, being neither narrow nor reductive, and compatible with plausible iterations of ideas like emergence and downward causation, are congruent with enactivism. Attention to enactivist ideas, moreover, may serve as a heuristic for mechanistic investigations of cognition. Nevertheless, I show how enactivism and approaches that prioritise mechanistic modelling may diverge in starting assumptions about the nature of cognitive phenomena, such as where the constitutive boundaries of cognition lie.},
  archive      = {J_MAM},
  author       = {Lee, Jonny},
  doi          = {10.1007/s11023-022-09618-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {153-184},
  shortjournal = {Minds Mach.},
  title        = {Enactivism meets mechanism: Tensions &amp; congruities in cognitive science},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dilemma for dispositional answers to kripkenstein’s
challenge. <em>MAM</em>, <em>33</em>(1), 135–152. (<a
href="https://doi.org/10.1007/s11023-023-09629-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kripkenstein’s challenge is usually described as being essentially about the use of a word in new kinds of cases ‒ the old kinds of cases being commonly considered as non-problematic. I show that this way of conceiving the challenge is neither true to Kripke’s intentions nor philosophically defensible: the Kripkean skeptic can question my answering “125” to the question “What is 68 plus 57?” even if that problem is one I have already encountered and answered. I then argue that once the real nature of Kripkenstein’s challenge is properly appreciated, one extremely popular strategy to try to meet it, what usually goes by the name of “semantic dispositionalism”, loses much of its appeal. Along the way, I also explain that Kripkenstein’s challenge is actually two distinct challenges ‒ one concerning the mental state of meaning, or intending, something by a sign and the other concerning the meaning (referentially conceived) of linguistic expressions.},
  archive      = {J_MAM},
  author       = {Guardo, Andrea},
  doi          = {10.1007/s11023-023-09629-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {135-152},
  shortjournal = {Minds Mach.},
  title        = {A dilemma for dispositional answers to kripkenstein’s challenge},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A model solution: On the compatibility of predictive
processing and embodied cognition. <em>MAM</em>, <em>33</em>(1),
113–134. (<a href="https://doi.org/10.1007/s11023-022-09617-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive processing (PP) and embodied cognition (EC) have emerged as two influential approaches within cognitive science in recent years. Not only have PP and EC been heralded as “revolutions” and “paradigm shifts” but they have motivated a number of new and interesting areas of research. This has prompted some to wonder how compatible the two views might be. This paper looks to weigh in on the issue of PP-EC compatibility. After outlining two recent proposals, I argue that further clarity can be achieved on the issue by considering a model of scientific progress. Specifically, I suggest that Larry Laudan’s “problem solving model” can provide important insights into a number of outstanding challenges that face existing accounts of PP-EC compatibility. I conclude by outlining additional implications of the problem solving model for PP and EC more generally.},
  archive      = {J_MAM},
  author       = {Kersten, Luke},
  doi          = {10.1007/s11023-022-09617-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {113-134},
  shortjournal = {Minds Mach.},
  title        = {A model solution: On the compatibility of predictive processing and embodied cognition},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computers as interactive machines: Can we build an
explanatory abstraction? <em>MAM</em>, <em>33</em>(1), 83–112. (<a
href="https://doi.org/10.1007/s11023-023-09624-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the question of what current computers are from the point of view of human-computer interaction. In the early days of computing, the Turing machine (TM) has been the cornerstone of the understanding of computers. The TM defines what can be computed and how computation can be carried out. However, in the last decades, computers have evolved and increasingly become interactive systems, reacting in real-time to external events in an ongoing loop. We argue that the TM does not provide a mechanistic explanation for interactive computing. The reason is that the fundamental phenomena relevant to interactive computing are out of the scope of classical computability theory. Part of the explanatory power of the TM relies on what we propose to call an execution model. An execution model belongs to a level of abstraction where it is possible to describe both the functional architecture and the execution in mechanistic terms. An updated execution model is warranted to provide the minimal mechanistic description for interactive computation as a counterpart of what the TM could explain regarding Church-Turing computation. It would support an explanation of the ubiquitous computing devices we know - those interacting with humans, e.g., through digital interfaces. We show that such a model is not available within interactive models of computation and that relevant abstractions and concerns are available in computer engineering but need to be identified and gathered. To fill this void, we propose to reflect on the level of abstraction required to support the mechanistic description of an interactive execution and propose some preliminary requirements.},
  archive      = {J_MAM},
  author       = {Martin, Alice and Magnaudet, Mathieu and Conversy, Stéphane},
  doi          = {10.1007/s11023-023-09624-2},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {83-112},
  shortjournal = {Minds Mach.},
  title        = {Computers as interactive machines: Can we build an explanatory abstraction?},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attitudinal tensions in the joint pursuit of explainable and
trusted AI. <em>MAM</em>, <em>33</em>(1), 55–82. (<a
href="https://doi.org/10.1007/s11023-023-09628-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is frequently demanded that AI-based Decision Support Tools (AI-DSTs) ought to be both explainable to, and trusted by, those who use them. The joint pursuit of these two principles is ordinarily believed to be uncontroversial. In fact, a common view is that AI systems should be made explainable so that they can be trusted, and in turn, accepted by decision-makers. However, the moral scope of these two principles extends far beyond this particular instrumental connection. This paper argues that if we were to account for the rich and diverse moral reasons that ground the call for explainable AI, and fully consider what it means to “trust” AI in a descriptively rich sense of the term, we would uncover a deep and persistent tension between the two principles. For explainable AI to usefully serve the pursuit of normatively desirable goals, decision-makers must carefully monitor and critically reflect on the content of an AI-DST’s explanation. This entails a deliberative attitude. Conversely, calls for trust in AI-DSTs imply the disposition to put questions about their reliability out of mind. This entails an unquestioning attitude. As such, the joint pursuit of explainable and trusted AI calls on decision-makers to simultaneously adopt incompatible attitudes towards their AI-DST, which leads to an intractable implementation gap. We analyze this gap and explore its broader implications: suggesting that we may need alternate theoretical conceptualizations of what explainability and trust entail, and/or alternate decision-making arrangements that separate the requirements for trust and deliberation to different parties.},
  archive      = {J_MAM},
  author       = {Narayanan, Devesh and Tan, Zhi Ming},
  doi          = {10.1007/s11023-023-09628-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {55-82},
  shortjournal = {Minds Mach.},
  title        = {Attitudinal tensions in the joint pursuit of explainable and trusted AI},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grounding the vector space of an octopus: Word meaning from
raw text. <em>MAM</em>, <em>33</em>(1), 33–54. (<a
href="https://doi.org/10.1007/s11023-023-09622-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most, if not all, philosophers agree that computers cannot learn what words refers to from raw text alone. While many attacked Searle’s Chinese Room thought experiment, no one seemed to question this most basic assumption. For how can computers learn something that is not in the data? Emily Bender and Alexander Koller (2020) recently presented a related thought experiment—the so-called Octopus thought experiment, which replaces the rule-based interlocutor of Searle’s thought experiment with a neural language model. The Octopus thought experiment was awarded a best paper prize and was widely debated in the AI community. Again, however, even its fiercest opponents accepted the premise that what a word refers to cannot be induced in the absence of direct supervision. I will argue that what a word refers to is probably learnable from raw text alone. Here’s why: higher-order concept co-occurrence statistics are stable across languages and across modalities, because language use (universally) reflects the world we live in (which is relatively stable). Such statistics are sufficient to establish what words refer to. My conjecture is supported by a literature survey, a thought experiment, and an actual experiment.},
  archive      = {J_MAM},
  author       = {Søgaard, Anders},
  doi          = {10.1007/s11023-023-09622-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {33-54},
  shortjournal = {Minds Mach.},
  title        = {Grounding the vector space of an octopus: Word meaning from raw text},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The turing test is a thought experiment. <em>MAM</em>,
<em>33</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s11023-022-09616-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Turing test has been studied and run as a controlled experiment and found to be underspecified and poorly designed. On the other hand, it has been defended and still attracts interest as a test for true artificial intelligence (AI). Scientists and philosophers regret the test’s current status, acknowledging that the situation is at odds with the intellectual standards of Turing’s works. This article refers to this as the Turing Test Dilemma, following the observation that the test has been under discussion for over seventy years and still is widely seen as either too bad or too good to be a valuable experiment for AI. An argument that solves the dilemma is presented, which relies on reconstructing the Turing test as a thought experiment in the modern scientific tradition. It is argued that Turing’s exposition of the imitation game satisfies Mach’s characterization of the basic method of thought experiments and that Turing’s uses of his test satisfy Popper’s conception of the critical and heuristic uses of thought experiments and Kuhn’s association of thought experiments to conceptual change. It is emphasized how Turing methodically varied the imitation game design to address specific challenges posed to him by other thinkers and how his test illustrates a property of the phenomenon of intelligence and suggests a hypothesis on machine learning. This reconstruction of the Turing test provides a rapprochement to the conflicting views on its value in the literature.},
  archive      = {J_MAM},
  author       = {Gonçalves, Bernardo},
  doi          = {10.1007/s11023-022-09616-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Minds Mach.},
  title        = {The turing test is a thought experiment},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
