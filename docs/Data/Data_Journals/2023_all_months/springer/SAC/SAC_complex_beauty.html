<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---142">SAC - 142</h2>
<ul>
<li><details>
<summary>
(2023). Bayesian analysis for matrix-variate logistic regression
with/without response misclassification. <em>SAC</em>, <em>33</em>(6),
1–12. (<a href="https://doi.org/10.1007/s11222-023-10286-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate logistic regression is useful in facilitating the relationship between the binary response and matrix-variates which arise commonly from medical imaging research. However, inference based on such a model is impaired by the presence of the response misclassification and spurious covariates It is imperative to account for the misclassification effects and select active covatiates when employing matrix-variate logistic regression to handle such data. In this paper, we develop Bayesian inferential methods with the horse-shoe prior. We numerically examine the biases induced from the naive analysis which ignores misclassification of responses. The performance of the proposed methods is justified empirically and their usage is illustrated by the application to the Lee Silverman Voice Treatment (LSVT) Companion data.},
  archive      = {J_SAC},
  author       = {Fang, Junhan and Yi, Grace Y.},
  doi          = {10.1007/s11222-023-10286-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian analysis for matrix-variate logistic regression with/without response misclassification},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modern non-linear function-on-function regression.
<em>SAC</em>, <em>33</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s11222-023-10299-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, function-on-function direct neural networks and function-on-function basis neural networks. Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.},
  archive      = {J_SAC},
  author       = {Rao, Aniruddha Rajendra and Reimherr, Matthew},
  doi          = {10.1007/s11222-023-10299-z},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Modern non-linear function-on-function regression},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized model-based clustering of complex functional data.
<em>SAC</em>, <em>33</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10288-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an $$L_1$$ penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
  archive      = {J_SAC},
  author       = {Pronello, Nicola and Ignaccolo, Rosaria and Ippoliti, Luigi and Fontanella, Sara},
  doi          = {10.1007/s11222-023-10288-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Penalized model-based clustering of complex functional data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density estimation for toroidal data using semiparametric
mixtures. <em>SAC</em>, <em>33</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10305-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Toroidal data is an extension of circular data on a torus and plays a critical part in various scientific fields. This article studies the density estimation of multivariate toroidal data based on semiparametric mixtures. One of the major challenges of semiparametric mixture modelling in a multi-dimensional space is that one can not directly maximize the likelihood over the unrestricted component density as it will result in a degenerate estimate with an unbounded likelihood. To overcome this problem, we propose to fix the maximum of the component density, which subsequently bounds the maximum of the mixture and its likelihood function, hence providing a satisfactory density estimate. The product of univariate circular distributions are utilized to form multivariate toroidal densities as candidates for mixture components. Numerical studies show that the mixture-based density estimator is superior in general to the kernel density estimator.},
  archive      = {J_SAC},
  author       = {Xu, Danli and Wang, Yong},
  doi          = {10.1007/s11222-023-10305-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Density estimation for toroidal data using semiparametric mixtures},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Laplace based bayesian inference for ordinary differential
equation models using regularized artificial neural networks.
<em>SAC</em>, <em>33</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s11222-023-10289-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation and associated uncertainty quantification is an important problem in dynamical systems characterised by ordinary differential equation (ODE) models that are often nonlinear. Typically, such models have analytically intractable trajectories which result in likelihoods and posterior distributions that are similarly intractable. Bayesian inference for ODE systems via simulation methods require numerical approximations to produce inference with high accuracy at a cost of heavy computational power and slow convergence. At the same time, Artificial Neural Networks (ANN) offer tractability that can be utilized to construct an approximate but tractable likelihood and posterior distribution. In this paper we propose a hybrid approach, where Laplace-based Bayesian inference is combined with an ANN architecture for obtaining approximations to the ODE trajectories as a function of the unknown initial values and system parameters. Suitable choices of customized loss functions are proposed to fine tune the approximated ODE trajectories and the subsequent Laplace approximation procedure. The effectiveness of our proposed methods is demonstrated using an epidemiological system with non-analytical solutions—the Susceptible-Infectious-Removed (SIR) model for infectious diseases—based on simulated and real-life influenza datasets. The novelty and attractiveness of our proposed approach include (i) a new development of Bayesian inference using ANN architectures for ODE based dynamical systems, and (ii) a computationally fast posterior inference by avoiding convergence issues of benchmark Markov Chain Monte Carlo methods. These two features establish the developed approach as an accurate alternative to traditional Bayesian computational methods, with improved computational cost.},
  archive      = {J_SAC},
  author       = {Kwok, Wai M. and Streftaris, George and Dass, Sarat C.},
  doi          = {10.1007/s11222-023-10289-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Laplace based bayesian inference for ordinary differential equation models using regularized artificial neural networks},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extreme value modeling with errors-in-variables in detection
and attribution of changes in climate extremes. <em>SAC</em>,
<em>33</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10290-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized extreme value (GEV) regression provides a framework for modeling extreme events across various fields by incorporating covariates into the location parameter of GEV distributions. When the covariates are subject to errors-in-variables (EIV) or measurement error, ignoring the EIVs leads to biased estimation and degraded inferences. This problem arises in detection and attribution analyses of changes in climate extremes because the covariates are estimated with uncertainty. It has not been studied even for the case of independent EIVs, let alone the case of dependent EIVs, due to the complex structure of GEV. Here we propose a general Monte Carlo corrected score method and extend it to address temporally correlated EIVs in GEV modeling with application to the detection and attribution analyses for climate extremes. Through extensive simulation studies, the proposed method provides an unbiased estimator and valid inference. In the application to the detection and attribution analyses of temperature extremes in central regions of China, with the proposed method, the combined anthropogenic and natural signal is detected in the change in the annual minimum of daily maximum and the annual minimum of daily minimum.},
  archive      = {J_SAC},
  author       = {Lau, Yuen Tsz Abby and Wang, Tianying and Yan, Jun and Zhang, Xuebin},
  doi          = {10.1007/s11222-023-10290-8},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Extreme value modeling with errors-in-variables in detection and attribution of changes in climate extremes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistency factor for the MCD estimator at the student-t
distribution. <em>SAC</em>, <em>33</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10296-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that trimmed estimators of multivariate scatter, such as the Minimum Covariance Determinant (MCD) estimator, are inconsistent unless an appropriate factor is applied to them in order to take the effect of trimming into account. This factor is widely recommended and applied when uncontaminated data are assumed to come from a multivariate normal model. We address the problem of computing a consistency factor for the MCD estimator in a heavy-tail scenario, when uncontaminated data come from a multivariate Student-t distribution. We derive a remarkably simple computational formula for the appropriate factor and show that it reduces to an even simpler analytic expression in the bivariate case. Exploiting our formula, we then develop a robust Monte Carlo procedure for estimating the usually unknown number of degrees of freedom of the assumed and possibly contaminated multivariate Student-t model, which is a necessary ingredient for obtaining the required consistency factor. Finally, we provide substantial simulation evidence about the proposed procedure and apply it to data from image processing and financial markets.},
  archive      = {J_SAC},
  author       = {Barabesi, Lucio and Cerioli, Andrea and García-Escudero, Luis Angel and Mayo-Iscar, Agustín},
  doi          = {10.1007/s11222-023-10296-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Consistency factor for the MCD estimator at the student-t distribution},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization via rejection-free partial neighbor search.
<em>SAC</em>, <em>33</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10300-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated Annealing using Metropolis steps at decreasing temperatures is widely used to solve complex combinatorial optimization problems (Kirkpatrick et al. in Science 220(4598):671–680, 1983). To improve its efficiency, we can use the Rejection-Free version of the Metropolis algorithm, which avoids the inefficiency of rejections by considering all the neighbors at every step (Rosenthal et al. in Comput Stat 36(4):2789–2811, 2021). To prevent the algorithm from becoming stuck in local extreme areas, we propose an enhanced version of Rejection-Free called Partial Neighbor Search, which only considers random parts of the neighbors while applying Rejection-Free. We demonstrate the superior performance of the Rejection-Free Partial Neighbor Search algorithm compared to the Simulation Annealing and Rejection-Free with several examples, such as the QUBO question, the Knapsack problem, the 3R3XOR problem, and the quadratic programming.},
  archive      = {J_SAC},
  author       = {Chen, Sigeng and Rosenthal, Jeffrey S. and Dote, Aki and Tamura, Hirotaka and Sheikholeslami, Ali},
  doi          = {10.1007/s11222-023-10300-9},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Optimization via rejection-free partial neighbor search},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parsimonious mixtures for the analysis of tensor-variate
data. <em>SAC</em>, <em>33</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s11222-023-10291-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real data is taking on more and more complex structures, raising the necessity for more flexible and parsimonious statistical methodologies. Tensor-variate (or multi-way) structures are a typical example of such kind of data. Unfortunately, real data often present atypical observations that make the traditional normality assumption inadequate. Thus, in this paper, we first introduce two new tensor-variate distributions, both heavy-tailed generalizations of the tensor-variate normal distribution. Then, we use these distributions for model-based clustering via finite mixture models. To introduce parsimony in the models, we use the eigen-decomposition of the components’ scale matrices, obtaining two families of parsimonious tensor-variate mixture models. As a by-product, we also introduce the parsimonious version of tensor-variate normal mixtures. As for parameter estimation, we illustrate variants of the well-known EM algorithm. Since the number of parsimonious models depends on the order of the tensors, we implement strategies intending to shorten the initialization and fitting processes. These procedures are investigated via simulated analyses. Finally, we fitted our parsimonious models to two real datasets having a 4-way and a 5-way structure, respectively.},
  archive      = {J_SAC},
  author       = {Tomarchio, Salvatore D. and Punzo, Antonio and Bagnato, Luca},
  doi          = {10.1007/s11222-023-10291-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Parsimonious mixtures for the analysis of tensor-variate data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial joint models through bayesian structured piecewise
additive joint modelling for longitudinal and time-to-event data.
<em>SAC</em>, <em>33</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10293-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and time-to-event data simultaneously model longitudinal and time-to-event information to avoid bias by combining usually a linear mixed model with a proportional hazards model. This model class has seen many developments in recent years, yet joint models including a spatial predictor are still rare and the traditional proportional hazards formulation of the time-to-event part of the model is accompanied by computational challenges. We propose a joint model with a piecewise exponential formulation of the hazard using the counting process representation of a hazard and structured additive predictors able to estimate (non-)linear, spatial and random effects. Its capabilities are assessed in a simulation study comparing our approach to an established one and highlighted by an example on physical functioning after cardiovascular events from the German Ageing Survey. The Structured Piecewise Additive Joint Model yielded good estimation performance, also and especially in spatial effects, while being double as fast as the chosen benchmark approach and performing stable in an imbalanced data setting with few events.},
  archive      = {J_SAC},
  author       = {Rappl, Anja and Kneib, Thomas and Lang, Stefan and Bergherr, Elisabeth},
  doi          = {10.1007/s11222-023-10293-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Spatial joint models through bayesian structured piecewise additive joint modelling for longitudinal and time-to-event data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing symmetry for bivariate copulas using bernstein
polynomials. <em>SAC</em>, <em>33</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10297-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, tests of symmetry for bivariate copulas are introduced and studied using empirical Bernstein copula process. Three statistics are proposed and their asymptotic properties are established. Besides, a multiplier bootstrap Bernstein version is investigated for implementation purpose. The simulation study demonstrated the superior performance of the Bernstein tests compared to tests based on empirical copulas. Furthermore, in real data applications, these tests consistently yielded similar conclusions across a diverse range of scenarios.},
  archive      = {J_SAC},
  author       = {Lyu, Guanjie and Belalia, Mohamed},
  doi          = {10.1007/s11222-023-10297-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Testing symmetry for bivariate copulas using bernstein polynomials},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Switched diffusion processes for non-convex optimization and
saddle points search. <em>SAC</em>, <em>33</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10315-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and investigate stochastic processes designed to find local minimizers and saddle points of non-convex functions, exploring the landscape more efficiently than the standard noisy gradient descent. The processes switch between two behaviours, a noisy gradient descent and a noisy saddle point search. It is proven to be well-defined and to converge to a stationary distribution in the long time. Numerical experiments are provided on low-dimensional toy models and for Lennard–Jones clusters.},
  archive      = {J_SAC},
  author       = {Journel, Lucas and Monmarché, Pierre},
  doi          = {10.1007/s11222-023-10315-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Switched diffusion processes for non-convex optimization and saddle points search},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost free hyper-parameter selection/averaging for bayesian
inverse problems with vanilla and rao-blackwellized SMC samplers.
<em>SAC</em>, <em>33</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10294-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian inverse problems, one aims at characterizing the posterior distribution of a set of unknowns, given indirect measurements. For non-linear/non-Gaussian problems, analytic solutions are seldom available: Sequential Monte Carlo samplers offer a powerful tool for approximating complex posteriors, by constructing an auxiliary sequence of densities that smoothly reaches the posterior. Often the posterior depends on a scalar hyper-parameter, for which limited prior information is available. In this work, we show that properly designed Sequential Monte Carlo (SMC) samplers naturally provide an approximation of the marginal likelihood associated with this hyper-parameter for free, i.e. at a negligible additional computational cost. The proposed method proceeds by constructing the auxiliary sequence of distributions in such a way that each of them can be interpreted as a posterior distribution corresponding to a different value of the hyper-parameter. This can be exploited to perform selection of the hyper-parameter in Empirical Bayes (EB) approaches, as well as averaging across values of the hyper-parameter according to some hyper-prior distribution in Fully Bayesian (FB) approaches. For FB approaches, the proposed method has the further benefit of allowing prior sensitivity analysis at a negligible computational cost. In addition, the proposed method exploits particles at all the (relevant) iterations, thus alleviating one of the known limitations of SMC samplers, i.e. the fact that all samples at intermediate iterations are typically discarded. We show numerical results for two distinct cases where the hyper-parameter affects only the likelihood: a toy example, where an SMC sampler is used to approximate the full posterior distribution; and a brain imaging example, where a Rao-Blackwellized SMC sampler is used to approximate the posterior distribution of a subset of parameters in a conditionally linear Gaussian model.},
  archive      = {J_SAC},
  author       = {Viani, Alessandro and Johansen, Adam M. and Sorrentino, Alberto},
  doi          = {10.1007/s11222-023-10294-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Cost free hyper-parameter selection/averaging for bayesian inverse problems with vanilla and rao-blackwellized SMC samplers},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian hierarchical models incorporating measurement error
for interrupted time series design. <em>SAC</em>, <em>33</em>(6), 1–15.
(<a href="https://doi.org/10.1007/s11222-023-10295-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interrupted time series (ITS) design is a quasi-experimental approach for evaluating the impact of an intervention in public health research using segmented linear regression (SLR) models. Usually, aggregated data across multiple time points before and after an intervention are compared to detect a change in intercept, slope, or both; however, the use of aggregated data can lead to an ecological fallacy, imprecise estimates, loss of power, and spurious inferences. We formulated three models with/without measurement error in the dependent variable to address different data limitations resulted from aggregation. Adopting the Bayesian hierarchical methodology for three standard SLR models from an ITS design, we compared performances of the varying pre-intervention intercept model (VPIM), varying intercept model (VIM) and measurement error model (MEM) with a non-hierarchical model (NHM) using real-life data and simulation studies. The MEM first estimates true value using observed data and standard deviation, then regresses the independent variables on the estimated true values. The results demonstrated the suitability of the hierarchical models through sustained improvement in model performance and parameter estimates over the NHM. The VPIM and VIM provided precise estimates modeling the population of clusters and pooling information across parameters. The measurement error assumption, along with the Bayesian model’s hierarchical formulation and generative nature, helped stabilize the unstable values of the dependent variable based on observed data and standard deviations.},
  archive      = {J_SAC},
  author       = {Rana, Masud and Kosar, Justin and Peermohamed, Shaqil},
  doi          = {10.1007/s11222-023-10295-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian hierarchical models incorporating measurement error for interrupted time series design},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient estimation for distributed subset
selection. <em>SAC</em>, <em>33</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10302-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large scale both of the sample size and dimensions, modern data is usually stored in a distributed system, which poses unprecedented challenges in computation and statistical inference. Best subset selection is widely known as a benchmark method for handling high-dimensional data. However, there still is a lack of the study of the efficient algorithm for the best subset selection in the distributed system. To this end, we propose a new communication-efficient method to deal with the best subset selection in the distributed system. The proposed method restricts the information communication among local machines in a moderate active set, and leads not only to an efficient computation but also a cheaper cost of communication in a network of the distributed system. Moreover, we propose a new generalized information criterion for tuning the sparsity level on the central machine. Under mild conditions, we establish the consistency of estimation and variable selection for the proposed estimator. We demonstrate the superiority of the proposed method through several numerical studies and a real data application in adolescent health.},
  archive      = {J_SAC},
  author       = {Chen, Yan and Dong, Ruipeng and Wen, Canhong},
  doi          = {10.1007/s11222-023-10302-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Communication-efficient estimation for distributed subset selection},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reduced-rank approach to predicting multiple binary
responses through machine learning. <em>SAC</em>, <em>33</em>(6), 1–15.
(<a href="https://doi.org/10.1007/s11222-023-10314-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of simultaneously predicting multiple binary responses by utilizing a shared set of covariates. Our approach incorporates machine learning techniques for binary classification, without making assumptions about the underlying observations. Instead, our focus lies on a group of predictors, aiming to identify the one that minimizes prediction error. Unlike previous studies that primarily address estimation error, we directly analyze the prediction error of our method using PAC-Bayesian bounds techniques. In this paper, we introduce a pseudo-Bayesian approach capable of handling incomplete response data. Our strategy is efficiently implemented using the Langevin Monte Carlo method. Through simulation studies and a practical application using real data, we demonstrate the effectiveness of our proposed method, producing comparable or sometimes superior results compared to the current state-of-the-art method.},
  archive      = {J_SAC},
  author       = {Mai, The Tien},
  doi          = {10.1007/s11222-023-10314-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A reduced-rank approach to predicting multiple binary responses through machine learning},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised segmentation of PolSAR data with complex
wishart and <span class="math display">$${\varvec{\mathcal
{G}}}^{\varvec{0}}_{\varvec{m}}$$</span> distributions and shannon
entropy. <em>SAC</em>, <em>33</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10298-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarimetric synthetic aperture radar (PolSAR) systems are powerful remote sensing instruments. Despite their ability to acquire images in all weather conditions, day and night, and to detect dielectric properties and structures, PolSAR images are corrupted by multidimensional speckle. Such contamination, resulting from the use of coherent illumination, does not obey the classical hypothesis of additive Gaussian noise and therefore require special treatment. Moreover, the data are structured as positive-definite Hermitian matrices. The Wishart and $${\mathcal {G}}^0_m$$ laws, both defined on the set of all positive definite Hermitian matrices, successfully describe fully PolSAR returns. The Shannon entropy is a scalar that measures the contrast between two PolSAR observations in this context. We obtain the Shannon entropy of these models and, using the $$\textbf{h}$$ - $$\phi $$ entropy formalism, its asymptotic distribution. With these two elements, and using the Stochastic Expectation-Maximisation algorithm, we obtain better segmentations than those provided by the k-means and k-medoids with nine-dimensional inputs. We show applications to actual PolSAR data. Our results show that $${\mathcal {G}}^0_m$$ entropy-based segmentation is better suited for images where the class features are not well separated, while its Wishart counterpart gives the best results in scenarios with more separable regions.},
  archive      = {J_SAC},
  author       = {Nascimento, Abraão D. C. and Ferreira, Jodavid A. and Frery, Alejandro C.},
  doi          = {10.1007/s11222-023-10298-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Unsupervised segmentation of PolSAR data with complex wishart and $${\varvec{\mathcal {G}}}^{\varvec{0}}_{\varvec{m}}$$ distributions and shannon entropy},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating bayesian inference for stochastic epidemic
models using incidence data. <em>SAC</em>, <em>33</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10311-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the case of performing Bayesian inference for stochastic epidemic compartment models, using incomplete time course data consisting of incidence counts that are either the number of new infections or removals in time intervals of fixed length. We eschew the most natural Markov jump process representation for reasons of computational efficiency, and focus on a stochastic differential equation representation. This is further approximated to give a tractable Gaussian process, that is, the linear noise approximation (LNA). Unless the observation model linking the LNA to data is both linear and Gaussian, the observed data likelihood remains intractable. It is in this setting that we consider two approaches for marginalising over the latent process: a correlated pseudo-marginal method and analytic marginalisation via a Gaussian approximation of the observation model. We compare and contrast these approaches using synthetic data before applying the best performing method to real data consisting of removal incidence of oak processionary moth nests in Richmond Park, London. Our approach further allows comparison between various competing compartment models.},
  archive      = {J_SAC},
  author       = {Golightly, Andrew and Wadkin, Laura E. and Whitaker, Sam A. and Baggaley, Andrew W. and Parker, Nick G. and Kypraios, Theodore},
  doi          = {10.1007/s11222-023-10311-6},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Accelerating bayesian inference for stochastic epidemic models using incidence data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable methods for computing sharp extreme event
probabilities in infinite-dimensional stochastic systems. <em>SAC</em>,
<em>33</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s11222-023-10307-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and compare computational techniques for sharp extreme event probability estimates in stochastic differential equations with small additive Gaussian noise. In particular, we focus on strategies that are scalable, i.e. their efficiency does not degrade upon temporal and possibly spatial refinement. For that purpose, we extend algorithms based on the Laplace method for estimating the probability of an extreme event to infinite dimensional path space. The method estimates the limiting exponential scaling using a single realization of the random variable, the large deviation minimizer. Finding this minimizer amounts to solving an optimization problem governed by a differential equation. The probability estimate becomes sharp when it additionally includes prefactor information, which necessitates computing the determinant of a second derivative operator to evaluate a Gaussian integral around the minimizer. We present an approach in infinite dimensions based on Fredholm determinants, and develop numerical algorithms to compute these determinants efficiently for the high-dimensional systems that arise upon discretization. We also give an interpretation of this approach using Gaussian process covariances and transition tubes. An example model problem, for which we provide an open-source python implementation, is used throughout the paper to illustrate all methods discussed. To study the performance of the methods, we consider examples of stochastic differential and stochastic partial differential equations, including the randomly forced incompressible three-dimensional Navier–Stokes equations.},
  archive      = {J_SAC},
  author       = {Schorlepp, Timo and Tong, Shanyin and Grafke, Tobias and Stadler, Georg},
  doi          = {10.1007/s11222-023-10307-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Scalable methods for computing sharp extreme event probabilities in infinite-dimensional stochastic systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-step closed-form estimator for generalized linear model
with categorical explanatory variables. <em>SAC</em>, <em>33</em>(6),
1–13. (<a href="https://doi.org/10.1007/s11222-023-10313-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameters of generalized linear models are generally estimated by the maximum likelihood estimator (MLE), computed using a Newton–Raphson type algorithm that can be time-consuming for a large number of variables or modalities, or a large sample size. Explicit estimators exist for these models but they are not always asymptotically efficient, especially for simple effects models, although they are fast to calculate compared to the MLE. The article proposes a fast and asymptotically efficient estimation of the parameters of generalized linear models with categorical explanatory variables. It is based on a one-step procedure where a single step of the gradient descent is performed on the log-likelihood function initialized from the explicit estimators. This work presents the theoretical results obtained, the simulations carried out and an application to car insurance pricing.},
  archive      = {J_SAC},
  author       = {Brouste, Alexandre and Dutang, Christophe and Hovsepyan, Lilit and Rohmer, Tom},
  doi          = {10.1007/s11222-023-10313-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {One-step closed-form estimator for generalized linear model with categorical explanatory variables},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold-constrained gaussian process inference for
time-varying parameters in dynamic systems. <em>SAC</em>,
<em>33</em>(6), 1–30. (<a
href="https://doi.org/10.1007/s11222-023-10319-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of parameters in ordinary differential equations (ODEs) is an important and challenging task when modeling dynamic systems in biomedical research and other scientific areas, especially with the presence of time-varying parameters. This article proposes a fast and accurate method, TVMAGI (Time-Varying MAnifold-constrained Gaussian process Inference), to estimate both time-constant and time-varying parameters in the ODE using noisy and sparse observation data. TVMAGI imposes a Gaussian process model over the time series of system components as well as time-varying parameters, and restricts the derivative process to satisfy ODE conditions. Consequently, TVMAGI does not require any conventional numerical integration such as Runge–Kutta and thus achieves substantial savings in computation time. By incorporating the ODE structures through manifold constraints, TVMAGI enjoys a principled statistical construct under the Bayesian paradigm, which further enables it to handle systems with missing data or unobserved components. The Gaussian process prior also alleviates the identifiability issue often associated with the time-varying parameters in ODE. Unlike existing approaches, TVMAGI can be applied to general nonlinear systems without specific structural assumptions. Three simulation examples, including an infectious disease compartmental model, are provided to illustrate the robustness and efficiency of our method compared with numerical integration and Bayesian filtering methods.},
  archive      = {J_SAC},
  author       = {Sun, Yan and Yang, Shihao},
  doi          = {10.1007/s11222-023-10319-y},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Manifold-constrained gaussian process inference for time-varying parameters in dynamic systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning binary undirected graph in low dimensional regime.
<em>SAC</em>, <em>33</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s11222-023-10321-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a random sample drawn from a Multivariate Bernoulli Variable (MBV), we consider the problem of estimating the structure of the undirected graph for which the distribution is pairwise Markov and the parameters’ vector of its exponential form. We propose a simple method that provides a closed form estimator of the parameters’ vector and through its support also provides an estimate of the undirected graph associated with the MBV distribution. The estimator is proved to be asymptotically consistent but it is feasible only in low-dimensional regimes. Synthetic examples illustrate its performance compared with another method that represents state of the art in literature. Finally, the proposed procedure is used to analyze a data set in the pediatric allergology area showing its practical efficiency.},
  archive      = {J_SAC},
  author       = {De Canditiis, Daniela},
  doi          = {10.1007/s11222-023-10321-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Learning binary undirected graph in low dimensional regime},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A randomized multi-index sequential monte carlo method.
<em>SAC</em>, <em>33</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10249-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating expectations with respect to a target distribution with an unknown normalizing constant, and where even the unnormalized target needs to be approximated at finite resolution. Under such an assumption, this work builds upon a recently introduced multi-index sequential Monte Carlo (SMC) ratio estimator, which provably enjoys the complexity improvements of multi-index Monte Carlo (MIMC) and the efficiency of SMC for inference. The present work leverages a randomization strategy to remove bias entirely, which simplifies estimation substantially, particularly in the MIMC context, where the choice of index set is otherwise important. Under reasonable assumptions, the proposed method provably achieves the same canonical complexity of MSE $$^{-1}$$ as the original method (where MSE is mean squared error), but without discretization bias. It is illustrated on examples of Bayesian inverse and spatial statistics problems.},
  archive      = {J_SAC},
  author       = {Liang, Xinzhu and Yang, Shangda and Cotter, Simon L. and Law, Kody J. H.},
  doi          = {10.1007/s11222-023-10249-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A randomized multi-index sequential monte carlo method},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A test for the absence of aliasing or white noise in
two-dimensional locally stationary wavelet processes. <em>SAC</em>,
<em>33</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10269-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Either intentionally or unintentionally, sub-sampling is a common occurrence in image processing and can lead to aliasing if the highest frequency in the underlying process is higher than the Nyquist frequency. Several techniques have already been suggest in order to prevent aliasing from occurring (for example applying anti-aliasing filters), however there is little work describing methods to detect for it. Recently, Eckley and Nason (Biometrika 105(4), 833–848, 2018) developed a test for the absence of aliasing and/or white noise in locally stationary wavelet processes. Following Eckley and Nason (Biometrika 105(4), 833–848, 2018), we derive the corresponding theoretical consequences of sub-sampling a two-dimensional locally stationary wavelet process and develop a procedure to test for the absence of aliasing and/or white noise confounding at a fixed point, demonstrating its effectiveness and use through appropriate simulation studies and an example. In addition, we outline some possibilities for extending these methods further, from images to videos.},
  archive      = {J_SAC},
  author       = {Palasciano, Henry Antonio and Nason, Guy P.},
  doi          = {10.1007/s11222-023-10269-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A test for the absence of aliasing or white noise in two-dimensional locally stationary wavelet processes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible non-parametric regression models for compositional
response data with zeros. <em>SAC</em>, <em>33</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10277-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional data arise in many real-life applications and versatile methods for properly analyzing this type of data in the regression context are needed. When parametric assumptions do not hold or are difficult to verify, non-parametric regression models can provide a convenient alternative method for prediction. To this end, we consider an extension to the classical k- Nearest Neighbours (k-NN) regression, that yields a highly flexible non-parametric regression model for compositional data. A similar extension of kernel regression is proposed by adopting the Nadaraya–Watson estimator. Both extensions involve a power transformation termed the $$\alpha $$ -transformation. Unlike many of the recommended regression models for compositional data, zeros values (which commonly occur in practice) are not problematic and they can be incorporated into the proposed models without modification. Extensive simulation studies and real-life data analyses highlight the advantage of using these non-parametric regressions for complex relationships between compositional response data and Euclidean predictor variables. Both the extended K-NN and kernel regressions can lead to more accurate predictions compared to current regression models which assume a, sometimes restrictive, parametric relationship with the predictor variables. In addition, the extended k-NN regression, in contrast to current regression techniques, enjoys a high computational efficiency rendering it highly attractive for use with large sample data sets.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail and Alenazi, Abdulaziz and Stewart, Connie},
  doi          = {10.1007/s11222-023-10277-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Flexible non-parametric regression models for compositional response data with zeros},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wavelet monte carlo: A principle for sampling from complex
distributions. <em>SAC</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10256-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Wavelet Monte Carlo (WMC), a new method for generating independent samples from complex target distributions. The methodology is based on wavelet decomposition of the difference between the target density and a user-specified initial density, and exploits both wavelet theory and survival analysis. In practice, WMC can process only a finite range of wavelet scales. We prove that the resulting $$L_1$$ approximation error converges to zero geometrically as the scale range tends to $$(-\infty ,+\infty )$$ . This provides a principled approach to trading off accuracy against computational efficiency. We offer practical suggestions for addressing some issues of implementation, but further development is needed for a computationally efficient methodology. We illustrate the methodology in one- and two-dimensional examples, and discuss challenges and opportunities for application in higher dimensions.},
  archive      = {J_SAC},
  author       = {Gilks, Walter R. and Cironis, Lukas and Barber, Stuart},
  doi          = {10.1007/s11222-023-10256-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Wavelet monte carlo: A principle for sampling from complex distributions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EuMMD: Efficiently computing the MMD two-sample test
statistic for univariate data. <em>SAC</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10271-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum mean discrepancy (MMD) test is a nonparametric kernelised two-sample test that, when using a characteristic kernel, can detect any distributional change between two samples. However, when the total number of $$d$$ -dimensional observations is $$n$$ , direct computation of the test statistic is $$\mathcal {O}(dn^2 )$$ . While approximations with lower computational complexity are known, more efficient methods for computing the exact test statistic are unknown. This paper provides an exact method for computing the MMD test statistic for the univariate case in $$\mathcal {O}(n\log n)$$ using the Laplacian kernel. Furthermore, this exact method is extended to an approximate method for $$d$$ -dimensional real-valued data also with complexity log-linear in the number of observations. Experiments show that this approximate method can have good statistical performance when compared to the exact test, particularly in cases where $$d&gt; n$$ .},
  archive      = {J_SAC},
  author       = {Bodenham, Dean A. and Kawahara, Yoshinobu},
  doi          = {10.1007/s11222-023-10271-x},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {EuMMD: Efficiently computing the MMD two-sample test statistic for univariate data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent uniform samplers on multivariate binary spaces.
<em>SAC</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10276-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider sampling from a probability distribution on $${0,1}^M$$ , or an equivalent high-dimensional binary space. A number of important applications rely on sampling from such distributions, including Bayesian variable selection problems and fitting Bayesian regression trees. Direct sampling is prohibitive when the dimension is large due to the fact that there are $$2^M$$ possible states. One approach to sampling such distributions is to use a Metropolis–Hastings algorithm, which can require choosing a decent proposal mechanism, with a default choice being the single-component switch proposal move. This is problematic when multiple modes exist. In this paper, we propose a latent variable uniform sampling algorithm, such as a latent slice sampler, which allows for large moves and proposal paths which give non-negligible probabilities for moving between modes, even when the probabilities of states between these modes is low. A number of illustrations are presented, focusing primarily on demonstrating the advantages over current generic samplers.},
  archive      = {J_SAC},
  author       = {Li, Yanxin and Linero, Antonio and Walker, Stephen G.},
  doi          = {10.1007/s11222-023-10276-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Latent uniform samplers on multivariate binary spaces},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved baselines for causal structure learning on
interventional data. <em>SAC</em>, <em>33</em>(5), 1–33. (<a
href="https://doi.org/10.1007/s11222-023-10257-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal structure learning (CSL) refers to the estimation of causal graphs from data. Causal versions of tools such as ROC curves play a prominent role in empirical assessment of CSL methods and performance is often compared with “random” baselines (such as the diagonal in an ROC analysis). However, such baselines do not take account of constraints arising from the graph context and hence may represent a “low bar”. In this paper, motivated by examples in systems biology, we focus on assessment of CSL methods for multivariate data where part of the graph structure is known via interventional experiments. For this setting, we put forward a new class of baselines called graph-based predictors (GBPs). In contrast to the “random” baseline, GBPs leverage the known graph structure, exploiting simple graph properties to provide improved baselines against which to compare CSL methods. We discuss GBPs in general and provide a detailed study in the context of transitively closed graphs, introducing two conceptually simple baselines for this setting, the observed in-degree predictor (OIP) and the transitivity assuming predictor (TAP). While the former is straightforward to compute, for the latter we propose several simulation strategies. Moreover, we study and compare the proposed predictors theoretically, including a result showing that the OIP outperforms in expectation the “random” baseline on a subclass of latent network models featuring positive correlation among edge probabilities. Using both simulated and real biological data, we show that the proposed GBPs outperform random baselines in practice, often substantially. Some GBPs even outperform standard CSL methods (whilst being computationally cheap in practice). Our results provide a new way to assess CSL methods for interventional data.},
  archive      = {J_SAC},
  author       = {Richter, Robin and Bhamidi, Shankar and Mukherjee, Sach},
  doi          = {10.1007/s11222-023-10257-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Stat. Comput.},
  title        = {Improved baselines for causal structure learning on interventional data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unlabelled landmark matching via bayesian data selection,
and application to cell matching across imaging modalities.
<em>SAC</em>, <em>33</em>(5), 1–25. (<a
href="https://doi.org/10.1007/s11222-023-10259-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of landmark matching between two unlabelled point sets, in particular where the number of points in each cloud may differ, and where points in each cloud may not have a corresponding match. We invoke a Bayesian framework to identify the transformation of coordinates that maps one cloud to the other, alongside correspondence of the points. This problem necessitates a novel methodology for Bayesian data selection, simultaneous inference of model parameters, and selection of the data which leads to the best fit of the model to the majority of the data. We apply this to a problem in developmental biology where the landmarks correspond to segmented cell centres, where potential death or division of cells can lead to discrepancies between the point-sets from each image. We validate the efficacy of our approach using in silico tests and a microinjected fluorescent marker experiment. Subsequently we apply our approach to the matching of cells between real time imaging and immunostaining experiments, facilitating the combination of single-cell data between imaging modalities. Furthermore our approach to Bayesian data selection is broadly applicable across data science, and has the potential to change the way we think about fitting models to data.},
  archive      = {J_SAC},
  author       = {Forsyth, Jessica E. and Al-Anbaki, Ali H. and Plusa, Berenika and Cotter, Simon L.},
  doi          = {10.1007/s11222-023-10259-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Unlabelled landmark matching via bayesian data selection, and application to cell matching across imaging modalities},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goodness-of-fit tests for multivariate skewed distributions
based on the characteristic function. <em>SAC</em>, <em>33</em>(5),
1–18. (<a href="https://doi.org/10.1007/s11222-023-10260-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We employ a general Monte Carlo method to test composite hypotheses of goodness-of-fit for several popular multivariate models that can accommodate both asymmetry and heavy tails. Specifically, we consider weighted L2-type tests based on a discrepancy measure involving the distance between empirical characteristic functions and thus avoid the need for employing corresponding population quantities which may be unknown or complicated to work with. The only requirements of our tests are that we should be able to draw samples from the distribution under test and possess a reasonable method of estimation of the unknown distributional parameters. Monte Carlo studies are conducted to investigate the performance of the test criteria in finite samples for several families of skewed distributions. Real-data examples are also included to illustrate our method.},
  archive      = {J_SAC},
  author       = {Karling, Maicon J. and Genton, Marc G. and Meintanis, Simos G.},
  doi          = {10.1007/s11222-023-10260-0},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Goodness-of-fit tests for multivariate skewed distributions based on the characteristic function},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A point mass proposal method for bayesian state-space model
fitting. <em>SAC</em>, <em>33</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10268-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-space models (SSMs) are commonly used to model time series data where the observations depend on an unobserved latent process. However, inference on the model parameters of an SSM can be challenging, especially when the likelihood of the data given the parameters is not available in closed-form. One approach is to jointly sample the latent states and model parameters via Markov chain Monte Carlo (MCMC) and/or sequential Monte Carlo approximation. These methods can be inefficient, mixing poorly when there are many highly correlated latent states or parameters, or when there is a high rate of sample impoverishment in the sequential Monte Carlo approximations. We propose a novel block proposal distribution for Metropolis-within-Gibbs sampling on the joint latent state and parameter space. The proposal distribution is informed by a deterministic hidden Markov model (HMM), defined such that the usual theoretical guarantees of MCMC algorithms apply. We discuss how the HMMs are constructed, the generality of the approach arising from the tuning parameters, and how these tuning parameters can be chosen efficiently in practice. We demonstrate that the proposed algorithm using HMM approximations provides an efficient alternative method for fitting state-space models, even for those that exhibit near-chaotic behavior.},
  archive      = {J_SAC},
  author       = {Llewellyn, Mary and King, Ruth and Elvira, Víctor and Ross, Gordon},
  doi          = {10.1007/s11222-023-10268-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A point mass proposal method for bayesian state-space model fitting},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized multiple change-point detection in the structure
of multivariate, possibly high-dimensional, data sequences.
<em>SAC</em>, <em>33</em>(5), 1–29. (<a
href="https://doi.org/10.1007/s11222-023-10261-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive emergence of big data techniques has led to an increasing interest in the development of change-point detection algorithms that can perform well in a multivariate, possibly high-dimensional setting. In the current paper, we propose a new method for the consistent estimation of the number and location of multiple generalized change-points in multivariate, possibly high-dimensional, noisy data sequences. The number of change-points is allowed to increase with the sample size and the dimensionality of the given data sequence. Having a number of univariate signals, which constitute the unknown multivariate signal, our algorithm can deal with general structural changes; we focus on changes in the mean vector of a multivariate piecewise-constant signal, as well as changes in the linear trend of any of the univariate component signals. Our proposed algorithm, labeled Multivariate Isolate–Detect (MID) allows for consistent change-point detection in the presence of frequent changes of possibly small magnitudes in a computationally fast way.},
  archive      = {J_SAC},
  author       = {Anastasiou, Andreas and Papanastasiou, Angelos},
  doi          = {10.1007/s11222-023-10261-z},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Generalized multiple change-point detection in the structure of multivariate, possibly high-dimensional, data sequences},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional order-free multivariate spatial disease
mapping. <em>SAC</em>, <em>33</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s11222-023-10263-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the amount of research on disease mapping in recent years, the use of multivariate models for areal spatial data remains limited due to difficulties in implementation and computational burden. These problems are exacerbated when the number of areas is very large. In this paper, we introduce an order-free multivariate scalable Bayesian modelling approach to smooth mortality (or incidence) risks of several diseases simultaneously. The proposal partitions the spatial domain into smaller subregions, fits multivariate models in each subdivision and obtains the posterior distribution of the relative risks across the entire spatial domain. The approach also provides posterior correlations among the spatial patterns of the diseases in each partition that are combined through a consensus Monte Carlo algorithm to obtain correlations for the whole study region. We implement the proposal using integrated nested Laplace approximations (INLA) in the R package bigDM and use it to jointly analyse colorectal, lung, and stomach cancer mortality data in Spanish municipalities. The new proposal allows for the analysis of large datasets and yields superior results compared to fitting a single multivariate model. Additionally, it facilitates statistical inference through local homogeneous models, which may be more appropriate than a global homogeneous model when dealing with a large number of areas.},
  archive      = {J_SAC},
  author       = {Vicente, Gonzalo and Adin, Aritz and Goicoa, Tomás and Ugarte, María Dolores},
  doi          = {10.1007/s11222-023-10263-x},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional order-free multivariate spatial disease mapping},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedded topics in the stochastic block model. <em>SAC</em>,
<em>33</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10265-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication networks such as emails or social networks are now ubiquitous and their analysis has become a strategic field. In many applications, the goal is to automatically extract relevant information by looking at the nodes and their connections. Unfortunately, most of the existing methods focus on analysing the presence or absence of edges and textual data is often discarded. However, all communication networks actually come with textual data on the edges. In order to take into account this specificity, we consider in this paper networks for which two nodes are linked if and only if they share textual data. We introduce a deep latent variable model allowing embedded topics to be handled called ETSBM to simultaneously perform clustering on the nodes while modelling the topics used between the different clusters. ETSBM extends both the stochastic block model (SBM) and the embedded topic model (ETM) which are core models for studying networks and corpora, respectively. The inference is done using a variational-Bayes expectation-maximisation algorithm combined with a stochastic gradient descent. The methodology is evaluated on synthetic data and on a real world dataset.},
  archive      = {J_SAC},
  author       = {Boutin, Rémi and Bouveyron, Charles and Latouche, Pierre},
  doi          = {10.1007/s11222-023-10265-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Embedded topics in the stochastic block model},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural networks for scalar input and functional output.
<em>SAC</em>, <em>33</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10287-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The regression of a functional response on a set of scalar predictors can be a challenging task, especially if there is a large number of predictors, or the relationship between those predictors and the response is nonlinear. In this work, we propose a solution to this problem: a feed-forward neural network (NN) designed to predict a functional response using scalar inputs. First, we transform the functional response to a finite-dimensional representation and construct an NN that outputs this representation. Then, we propose to modify the output of an NN via the objective function and introduce different objective functions for network training. The proposed models are suited for both regularly and irregularly spaced data, and a roughness penalty can be further applied to control the smoothness of the predicted curve. The difficulty in implementing both those features lies in the definition of objective functions that can be back-propagated. In our experiments, we demonstrate that our models outperform the conventional function-on-scalar regression model in multiple scenarios while computationally scaling better with the dimension of the predictors.},
  archive      = {J_SAC},
  author       = {Wu, Sidi and Beaulac, Cédric and Cao, Jiguo},
  doi          = {10.1007/s11222-023-10287-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Neural networks for scalar input and functional output},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and accurate inference for mixtures of mallows
models with spearman distance. <em>SAC</em>, <em>33</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10266-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mallows model (MM) occupies a central role in parametric modelling of ranking data to learn preferences of a population of judges. Despite the wide range of metrics for rankings that can be considered in the model specification, the choice is typically limited to the Kendall, Cayley or Hamming distances, due to the closed-form expression of the related model normalizing constant. This work instead focuses on the Mallows model with Spearman distance (MMS). A novel approximation of the normalizing constant is introduced to allow inference even with a large number of items. This allows us to develop and implement an efficient and accurate EM algorithm for estimating finite mixtures of MMS aimed at i) enlarging the applicability to samples drawn from heterogeneous populations, and ii) dealing with partial rankings affected by diverse forms of censoring. These novelties encompass the critical inferential steps that traditionally limited the use of this distance in practice, and render the MMS comparable (or even preferable) to the MMs with other metrics in terms of computational burden. The inferential ability of the EM scheme and the effectiveness of the approximation are assessed by extensive simulation studies. Finally, we show that the application to three real-world datasets endorses our proposals also in the comparison with competing mixtures of ranking models.},
  archive      = {J_SAC},
  author       = {Crispino, Marta and Mollica, Cristina and Astuti, Valerio and Tardella, Luca},
  doi          = {10.1007/s11222-023-10266-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Efficient and accurate inference for mixtures of mallows models with spearman distance},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient estimation of multiple expectations with the same
sample by adaptive importance sampling and control variates.
<em>SAC</em>, <em>33</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10270-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some classical uncertainty quantification problems require the estimation of multiple expectations. Estimating all of them accurately is crucial and can have a major impact on the analysis to perform, and standard existing Monte Carlo methods can be costly to do so. We propose here a new procedure based on importance sampling and control variates for estimating more efficiently multiple expectations with the same sample. We first show that there exists a family of optimal estimators combining both importance sampling and control variates, which however cannot be used in practice because they require the knowledge of the values of the expectations to estimate. Motivated by the form of these optimal estimators and some interesting properties, we therefore propose an adaptive algorithm. The general idea is to adaptively update the parameters of the estimators for approaching the optimal ones. We suggest then a quantitative stopping criterion that exploits the trade-off between approaching these optimal parameters and having a sufficient budget left. This left budget is then used to draw a new independent sample from the final sampling distribution, allowing to get unbiased estimators of the expectations. We show how to apply our procedure to sensitivity analysis, by estimating Sobol’ indices and quantifying the impact of the input distributions. Finally, realistic test cases show the practical interest of the proposed algorithm, and its significant improvement over estimating the expectations separately.},
  archive      = {J_SAC},
  author       = {Demange-Chryst, Julien and Bachoc, François and Morio, Jérôme},
  doi          = {10.1007/s11222-023-10270-y},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Efficient estimation of multiple expectations with the same sample by adaptive importance sampling and control variates},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toroidal PCA via density ridges. <em>SAC</em>,
<em>33</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10273-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal Component Analysis (PCA) is a well-known linear dimension-reduction technique designed for Euclidean data. In a wide spectrum of applied fields, however, it is common to observe multivariate circular data (also known as toroidal data), rendering spurious the use of PCA on it due to the periodicity of its support. This paper introduces Toroidal Ridge PCA (TR-PCA), a novel construction of PCA for bivariate circular data that leverages the concept of density ridges as a flexible first principal component analog. Two reference bivariate circular distributions, the bivariate sine von Mises and the bivariate wrapped Cauchy, are employed as the parametric distributional basis of TR-PCA. Efficient algorithms are presented to compute density ridges for these two distribution models. A complete PCA methodology adapted to toroidal data (including scores, variance decomposition, and resolution of edge cases) is introduced and implemented in the companion R package ridgetorus. The usefulness of TR-PCA is showcased with a novel case study involving the analysis of ocean currents on the coast of Santa Barbara.},
  archive      = {J_SAC},
  author       = {García-Portugués, Eduardo and Prieto-Tirado, Arturo},
  doi          = {10.1007/s11222-023-10273-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Toroidal PCA via density ridges},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of combinatorial optimisation algorithms for
c-optimal experimental designs with correlated observations.
<em>SAC</em>, <em>33</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10280-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show how combinatorial optimisation algorithms can be applied to the problem of identifying c-optimal experimental designs when there may be correlation between and within experimental units and evaluate the performance of relevant algorithms. We assume the data generating process is a generalised linear mixed model and show that the c-optimal design criterion is a monotone supermodular function amenable to a set of simple minimisation algorithms. We evaluate the performance of three relevant algorithms: the local search, the greedy search, and the reverse greedy search. We show that the local and reverse greedy searches provide comparable performance with the worst design outputs having variance $$&lt;10\%$$ greater than the best design, across a range of covariance structures. We show that these algorithms perform as well or better than multiplicative methods that generate weights to place on experimental units. We extend these algorithms to identifying modle-robust c-optimal designs.},
  archive      = {J_SAC},
  author       = {Watson, Samuel I. and Pan, Yi},
  doi          = {10.1007/s11222-023-10280-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Evaluation of combinatorial optimisation algorithms for c-optimal experimental designs with correlated observations},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed adaptive nearest neighbor classifier: Algorithm
and theory. <em>SAC</em>, <em>33</em>(5), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10267-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data is of an extraordinarily large size or physically stored in different locations, the distributed nearest neighbor (NN) classifier is an attractive tool for classification. We propose a novel distributed adaptive NN classifier for which the number of nearest neighbors is a tuning parameter stochastically chosen by a data-driven criterion. An early stopping rule is proposed when searching for the optimal tuning parameter, which not only speeds up the computation but also improves the finite sample performance of the proposed algorithm. Convergence rate of excess risk of the distributed adaptive NN classifier is investigated under various sub-sample size compositions. In particular, we show that when the sub-sample sizes are sufficiently large, the proposed classifier achieves the nearly optimal convergence rate. Effectiveness of the proposed approach is demonstrated through simulation studies as well as an empirical application to a real-world dataset.},
  archive      = {J_SAC},
  author       = {Liu, Ruiqi and Xu, Ganggang and Shang, Zuofeng},
  doi          = {10.1007/s11222-023-10267-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Distributed adaptive nearest neighbor classifier: Algorithm and theory},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). False discovery rate envelopes. <em>SAC</em>,
<em>33</em>(5), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10275-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False discovery rate (FDR) is a common way to control the number of false discoveries in multiple testing. There are a number of approaches available for controlling FDR. However, for functional test statistics, which are discretized into m highly correlated hypotheses, the methods must account for changes in distribution across the functional domain and correlation structure. Further, it is of great practical importance to visualize the test statistic together with its rejection or acceptance region. Therefore, the aim of this paper is to find, based on resampling principles, a graphical envelope that controls FDR and detects the outcomes of all individual hypotheses by a simple rule: the hypothesis is rejected if and only if the empirical test statistic is outside of the envelope. Such an envelope offers a straightforward interpretation of the test results, similarly as the recently developed global envelope testing which controls the family-wise error rate. Two different adaptive single threshold procedures are developed to fulfill this aim. Their performance is studied in an extensive simulation study. The new methods are illustrated by three real data examples.},
  archive      = {J_SAC},
  author       = {Mrkvička, Tomáš and Myllymäki, Mari},
  doi          = {10.1007/s11222-023-10275-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {False discovery rate envelopes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On simulation of continuous determinantal point processes.
<em>SAC</em>, <em>33</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10272-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review how to simulate continuous determinantal point processes (DPPs) and improve the current simulation algorithms in several important special cases as well as detail how certain types of conditional simulation can be carried out. Importantly we show how to speed up the simulation of the widely used Fourier based projection DPPs, which arise as approximations of more general DPPs. The algorithms are implemented and published as open source software.},
  archive      = {J_SAC},
  author       = {Lavancier, Frédéric and Rubak, Ege},
  doi          = {10.1007/s11222-023-10272-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {On simulation of continuous determinantal point processes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized linear models for massive data via
doubly-sketching. <em>SAC</em>, <em>33</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10274-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models are a popular analytics tool with interpretable results and broad applicability, but require iterative estimation procedures that impose data transfer and computational costs that can be problematic under some infrastructure constraints. We propose a doubly-sketched approximation of the iteratively re-weighted least squares algorithm to estimate generalized linear model parameters using a sequence of surrogate datasets. The procedure sketches once to reduce data transfer costs, and sketches again to reduce data computation costs, yielding wall-clock time savings. Regression coefficients and standard errors are produced, with comparison against literature methods. Asymptotic properties of the proposed procedure are shown, with empirical results from simulated and real-world datasets. The efficacy of the proposed method is investigated across a variety of commodity computational infrastructure configurations accessible to practitioners. A highlight of the present work is the estimation of a Poisson-log generalized linear model across almost 1.7 billion observations on a personal computer in 25 min.},
  archive      = {J_SAC},
  author       = {Hou-Liu, Jason and Browne, Ryan P.},
  doi          = {10.1007/s11222-023-10274-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Generalized linear models for massive data via doubly-sketching},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate blocked gibbs sampling for bayesian neural
networks. <em>SAC</em>, <em>33</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10285-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, minibatch MCMC sampling for feedforward neural networks is made more feasible. To this end, it is proposed to sample subgroups of parameters via a blocked Gibbs sampling scheme. By partitioning the parameter space, sampling is possible irrespective of layer width. It is also possible to alleviate vanishing acceptance rates for increasing depth by reducing the proposal variance in deeper layers. Increasing the length of a non-convergent chain increases the predictive accuracy in classification tasks, so avoiding vanishing acceptance rates and consequently enabling longer chain runs have practical benefits. Moreover, non-convergent chain realizations aid in the quantification of predictive uncertainty. An open problem is how to perform minibatch MCMC sampling for feedforward neural networks in the presence of augmented data.},
  archive      = {J_SAC},
  author       = {Papamarkou, Theodore},
  doi          = {10.1007/s11222-023-10285-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Approximate blocked gibbs sampling for bayesian neural networks},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Use generalized linear models or generalized partially
linear models? <em>SAC</em>, <em>33</em>(5), 1–10. (<a
href="https://doi.org/10.1007/s11222-023-10278-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose test statistics based on the penalized spline to decide between generalized linear models and generalized partially linear models. The numerical performance of the proposed statistics is comparable to that of their kernel-based competitors, which have been shown to be asymptotically normal in the literature (Härdle et al. in J Am Stat Assoc 93:1461–1474, 1998). We also numerically explore the possibility of using the proposed statistics for goodness of fit checking for GLM. The proposed proposed procedures are illustrated to analyze two datasets.},
  archive      = {J_SAC},
  author       = {Li, Xinmin and Liang, Haozhe and Härdle, Wolfgang and Liang, Hua},
  doi          = {10.1007/s11222-023-10278-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Use generalized linear models or generalized partially linear models?},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal designs for generalized linear mixed models based on
the penalized quasi-likelihood method. <em>SAC</em>, <em>33</em>(5),
1–13. (<a href="https://doi.org/10.1007/s11222-023-10279-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While generalized linear mixed models are useful, optimal design questions for such models are challenging due to complexity of the information matrices. For longitudinal data, after comparing three approximations for the information matrices, we propose an approximation based on the penalized quasi-likelihood method. We evaluate this approximation for logistic mixed models with time as the single predictor variable. Assuming that the experimenter controls at which time observations are to be made, the approximation is used to identify locally optimal designs based on the commonly used A- and D-optimality criteria. The method can also be used for models with random block effects. Locally optimal designs found by a Particle Swarm Optimization algorithm are presented and discussed. As an illustration, optimal designs are derived for a study on self-reported disability in older women. Finally, we also study the robustness of the locally optimal designs to mis-specification of the covariance matrix for the random effects.},
  archive      = {J_SAC},
  author       = {Shi, Yao and Yu, Wanchunzi and Stufken, John},
  doi          = {10.1007/s11222-023-10279-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Optimal designs for generalized linear mixed models based on the penalized quasi-likelihood method},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trans-cGAN: Transformer-unet-based generative adversarial
networks for cross-modality magnetic resonance image synthesis.
<em>SAC</em>, <em>33</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s11222-023-10282-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging is a widely used medical imaging technology, which can provide different contrasts between the tissues in human body. To use the complementary information from multiple imaging modalities and shorten the time of MR scanning, cross-modality magnetic resonance image synthesis has recently aroused extensive interests in literature. Most existing methods improve the quality of image synthesis by artificially designing loss on the basis of minimizing pixel-wise intensity error, even though it can improve the quality of the synthesized image, it is still difficult to balance the hyperparameters of different loss functions. In this paper, we propose a generative adversarial network (Trans-cGAN) based on transformer and unet for cross-modality magnetic resonance image synthesis. Specifically, transformer block is added into the network to enhance the model’s understanding of the overall semantics of the image as well as to implicitly integrate the edge information. Moreover, spectral normalization is added to the generator and discriminator to avoid mode collapse and ensure the stability of training. Experimental results demonstrate that the proposed Trans-cGAN is superior to the most of the cross-modality MR image synthesis methods in both qualitative and quantitative evaluations. Furthermore, Trans-cGAN also shows excellent generality in the generic image synthesis task of the benchmark datasets of maps and cityscapes.},
  archive      = {J_SAC},
  author       = {Li, Yan and Han, Na and Qin, Yuxiang and Zhang, Jing and Su, Jinxia},
  doi          = {10.1007/s11222-023-10282-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Trans-cGAN: Transformer-unet-based generative adversarial networks for cross-modality magnetic resonance image synthesis},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage approach for bayesian joint models: Reducing
complexity while maintaining accuracy. <em>SAC</em>, <em>33</em>(5),
1–11. (<a href="https://doi.org/10.1007/s11222-023-10281-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several joint models for longitudinal and survival data have been proposed in recent years. In particular, many authors have preferred to employ the Bayesian approach to model more complex structures, make dynamic predictions, or use model averaging. However, Markov chain Monte Carlo methods are computationally very demanding and may suffer convergence problems, especially for complex models with random effects, which is the case for most joint models. These issues can be overcome by estimating the parameters of each submodel separately, leading to a natural reduction in the complexity of the joint modelling, but often producing biased estimates. Hence, we propose a novel two-stage approach that uses the estimations from the longitudinal submodel to specify an informative prior distribution for the random effects when estimating them within the survival submodel. In addition, as a bias correction mechanism, we incorporate the longitudinal likelihood function in the second stage, where its fixed effects are set according to the estimation using only the longitudinal submodel. Based on simulation studies and real applications, we empirically compare our proposal with joint specification and standard two-stage approaches considering different types of longitudinal responses (continuous, count and binary) that share information with a Weibull proportional hazard model. The results show that our estimator is more accurate than its two-stage competitor and as good as jointly estimating all parameters. Moreover, the novel two-stage approach significantly reduces the computational time compared to the joint specification.},
  archive      = {J_SAC},
  author       = {Alvares, Danilo and Leiva-Yamaguchi, Valeria},
  doi          = {10.1007/s11222-023-10281-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {A two-stage approach for bayesian joint models: Reducing complexity while maintaining accuracy},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust supervised learning with coordinate gradient descent.
<em>SAC</em>, <em>33</em>(5), 1–39. (<a
href="https://doi.org/10.1007/s11222-023-10283-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of supervised learning with linear methods when both features and labels can be corrupted, either in the form of heavy tailed data and/or corrupted rows. We introduce a combination of coordinate gradient descent as a learning algorithm together with robust estimators of the partial derivatives. This leads to robust statistical learning methods that have a numerical complexity nearly identical to non-robust ones based on empirical risk minimization. The main idea is simple: while robust learning with gradient descent requires the computational cost of robustly estimating the whole gradient to update all parameters, a parameter can be updated immediately using a robust estimator of a single partial derivative in coordinate gradient descent. We prove upper bounds on the generalization error of the algorithms derived from this idea, that control both the optimization and statistical errors with and without a strong convexity assumption of the risk. Finally, we propose an efficient implementation of this approach in a new Python library called linlearn, and demonstrate through extensive numerical experiments that our approach introduces a new interesting compromise between robustness, statistical performance and numerical efficiency for this problem.},
  archive      = {J_SAC},
  author       = {Merad, Ibrahim and Gaïffas, Stéphane},
  doi          = {10.1007/s11222-023-10283-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-39},
  shortjournal = {Stat. Comput.},
  title        = {Robust supervised learning with coordinate gradient descent},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model averaging for support vector classifier by
cross-validation. <em>SAC</em>, <em>33</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10284-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector classification (SVC) is a well-known statistical technique for classification problems in machine learning and other fields. An important question for SVC is the selection of covariates (or features) for the model. Many studies have considered model selection methods. As is well-known, selecting one winning model over others can entail considerable instability in predictive performance due to model selection uncertainties. This paper advocates model averaging as an alternative approach, where estimates obtained from different models are combined in a weighted average. We propose a model weighting scheme and provide the theoretical underpinning for the proposed method. In particular, we prove that our proposed method yields a model average estimator that achieves the smallest hinge risk among all feasible combinations asymptotically. To remedy the computational burden due to a large number of feasible models, we propose a screening step to eliminate the uninformative features before combining the models. Results from real data applications and a simulation study show that the proposed method generally yields more accurate estimates than existing methods.},
  archive      = {J_SAC},
  author       = {Zou, Jiahui and Yuan, Chaoxia and Zhang, Xinyu and Zou, Guohua and Wan, Alan T. K.},
  doi          = {10.1007/s11222-023-10284-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Model averaging for support vector classifier by cross-validation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fréchet distance-based cluster analysis for
multi-dimensional functional data. <em>SAC</em>, <em>33</em>(4), 1–19.
(<a href="https://doi.org/10.1007/s11222-023-10237-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional functional data analysis has become a contemporary research topic in medical research as patients’ various records are measured over time. We propose two clustering methods using the Fréchet distance for multi-dimensional functional data. The first method extends an existing K-means type approach from one-dimensional to multi-dimensional longitudinal data. The second method enforces sparsity on functional variables while grouping observed trajectories and enables us to assess the contribution from each variable. Both methods utilize the generalized Fréchet distance to measure the distance between trajectories with irregularly spaced and asynchronous measurements. We demonstrate the effectiveness of the proposed methods through a comparative study using various simulation examples. Then, we apply the sparse clustering method to multi-dimensional thyroid cancer data collected in South Korea. It produces interpretable clusters and weighs the importance of functional variables.},
  archive      = {J_SAC},
  author       = {Kang, Ilsuk and Choi, Hosik and Yoon, Young Joo and Park, Junyoung and Kwon, Soon-Sun and Park, Cheolwoo},
  doi          = {10.1007/s11222-023-10237-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fréchet distance-based cluster analysis for multi-dimensional functional data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A similarity-based bayesian mixture-of-experts model.
<em>SAC</em>, <em>33</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10238-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new nonparametric mixture-of-experts model for multivariate regression problems, inspired by the probabilistic k-nearest neighbors algorithm. Using a conditionally specified model, predictions for out-of-sample inputs are based on similarities to each observed data point, yielding predictive distributions represented by Gaussian mixtures. Posterior inference is performed on the parameters of the mixture components as well as the distance metric using a mean-field variational Bayes algorithm accompanied with a stochastic gradient-based optimization procedure. The proposed method is especially advantageous in settings where inputs are of relatively high dimension in comparison to the data size, where input–output relationships are complex, and where predictive distributions may be skewed or multimodal. Computational studies on five datasets, of which two are synthetically generated, illustrate clear advantages of our mixture-of-experts method for high-dimensional inputs, outperforming competitor models both in terms of validation metrics and visual inspection.},
  archive      = {J_SAC},
  author       = {Zhang, Tianfang and Bokrantz, Rasmus and Olsson, Jimmy},
  doi          = {10.1007/s11222-023-10238-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {A similarity-based bayesian mixture-of-experts model},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence rate of multiple-try metropolis independent
sampler. <em>SAC</em>, <em>33</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10241-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple-try Metropolis method is an interesting extension of the classical Metropolis–Hastings algorithm. However, theoretical understanding about its usefulness and convergence behavior is still lacking. We here derive the exact convergence rate for the multiple-try Metropolis Independent sampler (MTM-IS) via an explicit eigen analysis. As a by-product, we prove that an naive application of the MTM-IS is less efficient than using the simpler approach of “thinned” independent Metropolis–Hastings method at the same computational cost. We further explore more variants and find it possible to design more efficient algorithms by applying MTM to part of the target distribution or creating correlated multiple trials.},
  archive      = {J_SAC},
  author       = {Yang, Xiaodong and Liu, Jun S.},
  doi          = {10.1007/s11222-023-10241-3},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Convergence rate of multiple-try metropolis independent sampler},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A PRticle filter algorithm for nonparametric estimation of
multivariate mixing distributions. <em>SAC</em>, <em>33</em>(4), 1–14.
(<a href="https://doi.org/10.1007/s11222-023-10242-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive recursion (PR) is a fast, recursive algorithm that gives a smooth estimate of the mixing distribution under the general mixture model. However, the PR algorithm requires evaluation of a normalizing constant at each iteration. When the support of the mixing distribution is of relatively low dimension, this is not a problem since quadrature methods can be used and are very efficient. But when the support is of higher dimension, quadrature methods are inefficient and there is no obvious Monte Carlo-based alternative. In this paper, we propose a new strategy, which we refer to as PRticle filter, wherein we augment the basic PR algorithm with a filtering mechanism that adaptively reweights an initial set of particles along the updating sequence which are used to obtain Monte Carlo approximations of the normalizing constants. Convergence properties of the PRticle filter approximation are established and its empirical accuracy is demonstrated with simulation studies and a marked spatial point process data analysis.},
  archive      = {J_SAC},
  author       = {Dixit, Vaidehi and Martin, Ryan},
  doi          = {10.1007/s11222-023-10242-2},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A PRticle filter algorithm for nonparametric estimation of multivariate mixing distributions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quadratic upper bound algorithm for regression analysis of
credit risk under the proportional hazards model with case-cohort data.
<em>SAC</em>, <em>33</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10248-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A case-cohort design is a cost-effective biased-sampling scheme in studies on survival data. We study the regression analysis of credit risk by fitting the proportional hazards model to data collected via the case-cohort design. Using the minorization-maximization principle, we develop a new quadratic upper-bound algorithm for the calculation of estimators and obtain the convergence of the algorithm. The proposed algorithm involves the inversion of the derived upper-bound matrix only one time in the whole process and the upper-bound matrix is independent of parameters. These features make the proposed algorithm have simple update and low per-iterative cost, especially to large-dimensional problems. Rcpp is an R package which enables users to write R extensions with C++. In this paper, we write the program of the proposed algorithm via Rcpp and improve the efficiency of R program execution and realize the fast computing. We conduct simulation studies to illustrate the performance of the proposed algorithm. We analyze a real data example from a mortgage dataset for evaluating credit risk.},
  archive      = {J_SAC},
  author       = {Huang, Chen and Ding, Jieli and Feng, Yanqin},
  doi          = {10.1007/s11222-023-10248-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A quadratic upper bound algorithm for regression analysis of credit risk under the proportional hazards model with case-cohort data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive online variance estimation in particle filters: The
ALVar estimator. <em>SAC</em>, <em>33</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10243-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach—the ALVar estimator—to estimation of asymptotic variance in sequential Monte Carlo methods, or, particle filters. The method, which adjusts adaptively the lag of the estimator proposed in Olsson and Douc (Bernoulli 25(2):1504–1535) applies to very general distribution flows and particle filters, including auxiliary particle filters with adaptive resampling. The algorithm operates entirely online, in the sense that it is able to monitor the variance of the particle filter in real time and with, on the average, constant computational complexity and memory requirements per iteration. Crucially, it does not require the calibration of any algorithmic parameter. Estimating the variance only on the basis of the genealogy of the propagated particle cloud, without additional simulations, the routine requires only minor code additions to the underlying particle algorithm. Finally, we prove that the ALVar estimator is consistent for the true asymptotic variance as the number of particles tends to infinity and illustrate numerically its superiority to existing approaches.},
  archive      = {J_SAC},
  author       = {Mastrototaro, Alessandro and Olsson, Jimmy},
  doi          = {10.1007/s11222-023-10243-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive online variance estimation in particle filters: The ALVar estimator},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian spatiotemporal modeling for inverse problems.
<em>SAC</em>, <em>33</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10253-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problems with spatiotemporal observations are ubiquitous in scientific studies and engineering applications. In these spatiotemporal inverse problems, observed multivariate time series are used to infer parameters of physical or biological interests. Traditional solutions for these problems often ignore the spatial or temporal correlations in the data (static model), or simply model the data summarized over time (time-averaged model). In either case, the data information that contains the spatiotemporal interactions is not fully utilized for parameter learning, which leads to insufficient modeling in these problems. In this paper, we apply Bayesian models based on spatiotemporal Gaussian processess (STGP) to inverse problems with spatiotemporal data and show that the spatial and temporal information provides more effective parameter estimation and uncertainty quantification (UQ). We demonstrate the merit of Bayesian spatiotemporal modeling for inverse problems compared with traditional static and time-averaged approaches using a time-dependent advection–diffusion partial different equation (PDE) and three chaotic ordinary differential equations (ODE). We also provide theoretic justification for the superiority of spatiotemporal modeling to fit the trajectories even if it appears cumbersome (e.g. for chaotic dynamics).},
  archive      = {J_SAC},
  author       = {Lan, Shiwei and Li, Shuyi and Pasha, Mirjeta},
  doi          = {10.1007/s11222-023-10253-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian spatiotemporal modeling for inverse problems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference of multivariate exponential hawkes processes with
inhibition and application to neuronal activity. <em>SAC</em>,
<em>33</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10264-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate Hawkes process is a past-dependent point process used to model the relationship of event occurrences between different phenomena. Although the Hawkes process was originally introduced to describe excitation effects, which means that one event increases the chances of another occurring, there has been a growing interest in modelling the opposite effect, known as inhibition. In this paper, we focus on how to infer the parameters of a multidimensional exponential Hawkes process with both excitation and inhibition effects. Our first result is to prove the identifiability of this model under a few sufficient assumptions. Then we propose a maximum likelihood approach to estimate the interaction functions, which is, to the best of our knowledge, the first exact inference procedure in the frequentist framework. Our method includes a variable selection step in order to recover the support of interactions and therefore to infer the connectivity graph. A benefit of our method is to provide an explicit computation of the log-likelihood, which enables in addition to perform a goodness-of-fit test for assessing the quality of estimations. We compare our method to standard approaches, which were developed in the linear framework and are not specifically designed for handling inhibiting effects. We show that the proposed estimator performs better on synthetic data than alternative approaches. We also illustrate the application of our procedure to a neuronal activity dataset, which highlights the presence of both exciting and inhibiting effects between neurons.},
  archive      = {J_SAC},
  author       = {Bonnet, Anna and Martinez Herrera, Miguel and Sangnier, Maxime},
  doi          = {10.1007/s11222-023-10264-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Inference of multivariate exponential hawkes processes with inhibition and application to neuronal activity},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast look-up method for bayesian mean-parameterised
conway–maxwell–poisson regression models. <em>SAC</em>, <em>33</em>(4),
1–16. (<a href="https://doi.org/10.1007/s11222-023-10244-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data that are subject to both under and overdispersion at some hierarchical level cannot be readily accommodated by classic models such as Poisson or negative binomial regression models. The mean-parameterised Conway–Maxwell–Poisson distribution allows for both types of dispersion within the same model, but is doubly intractable with an embedded normalising constant. We propose a look-up method where pre-computing values of the rate parameter dramatically reduces computing times and renders the proposed model a practicable alternative when faced with such bidispersed data. The approach is demonstrated and verified using a simulation study and applied to three datasets: an underdispersed small dataset on takeover bids, a medium dataset on yellow cards issued by referees in the English Premier League prior to and during the Covid-19 pandemic, and a large Test match cricket bowling dataset, the latter two of which each exhibit over and underdispersion at the individual level.},
  archive      = {J_SAC},
  author       = {Philipson, Pete and Huang, Alan},
  doi          = {10.1007/s11222-023-10244-0},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A fast look-up method for bayesian mean-parameterised Conway–Maxwell–Poisson regression models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for mediation analysis with massive data.
<em>SAC</em>, <em>33</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10255-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past few years, mediation analysis has gained increasing popularity across various research fields. The primary objective of mediation analysis is to examine the direct impact of exposure on outcome, as well as the indirect effects that occur along the pathways from exposure to outcome. There has been a great number of articles that applied mediation analysis to data from hundreds or thousands of individuals. With the rapid development of technology, the volume of avaliable data increases exponentially, which brings new challenges to researchers. Directly conducting statistical analysis for large datasets is often computationally infeasible. Nonetheless, there is a paucity of findings regarding mediation analysis in the context of big data. In this paper, we propose utilizing subsampled double bootstrap and divide-and-conquer algorithms to conduct statistical mediation analysis on large-scale datasets. The proposed algorithms offer a significant enhancement in computational efficiency over traditional bootstrap confidence interval and Sobel test, while simultaneously ensuring desirable confidence interval coverage and power. We conducted extensive numerical simulations to evaluate the performance of our method. The practical applicability of our approach is demonstrated through two real-world data examples.},
  archive      = {J_SAC},
  author       = {Zhang, Haixiang and Li, Xin},
  doi          = {10.1007/s11222-023-10255-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A framework for mediation analysis with massive data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A data-driven and model-based accelerated hamiltonian monte
carlo method for bayesian elliptic inverse problems. <em>SAC</em>,
<em>33</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10262-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a Bayesian inverse problem modeled by elliptic partial differential equations (PDEs). Specifically, we propose a data-driven and model-based approach to accelerate the Hamiltonian Monte Carlo (HMC) method in solving large-scale Bayesian inverse problems. The key idea is to exploit (model-based) and construct (data-based) intrinsic approximate low-dimensional structure of the underlying problem which consists of two components—a training component that computes a set of data-driven basis to achieve significant dimension reduction in the solution space, and a fast solving component that computes the solution and its derivatives for a newly sampled elliptic PDE with the constructed data-driven basis. Hence we develop an effective data and model-based approach for the Bayesian inverse problem and overcome the typical computational bottleneck of HMC—repeated evaluation of the Hamiltonian involving the solution (and its derivatives) modeled by a complex system, a multiscale elliptic PDE in our case. Finally, we present numerical examples to demonstrate the accuracy and efficiency of the proposed method.},
  archive      = {J_SAC},
  author       = {Li, Sijing and Zhang, Cheng and Zhang, Zhiwen and Zhao, Hongkai},
  doi          = {10.1007/s11222-023-10262-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A data-driven and model-based accelerated hamiltonian monte carlo method for bayesian elliptic inverse problems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Phylourny: Efficiently calculating elimination tournament
win probabilities via phylogenetic methods. <em>SAC</em>,
<em>33</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s11222-023-10246-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of knockout tournaments represents an area of large public interest and active academic as well as industrial research. Here, we show how one can leverage the computational analogies between calculating the phylogenetic likelihood score used in the area of molecular evolution to efficiently calculate, instead of approximate via simulations, the exact per-team tournament win probabilities, given a pairwise win probability matrix between all teams. We implement and make available our method as open-source code and show that it is two orders of magnitude faster than simulations and two or more orders of magnitude faster than calculating the exact per-team win probabilities naïvely, without taking into account the substantial computational savings induced by the tournament tree structure. Furthermore, we showcase novel prediction approaches that now become feasible due to this order of magnitude improvement in calculating tournament win probabilities. We demonstrate how to quantify prediction uncertainty by calculating 100,000 distinct tournament win probabilities for a tournament with 16 teams under slight variations of a reasonable pairwise win probability matrix within one minute on a standard laptop. We also conduct an analogous analysis for a tournament with 64 teams.},
  archive      = {J_SAC},
  author       = {Bettisworth, Ben and Jordan, Alexander I. and Stamatakis, Alexandros},
  doi          = {10.1007/s11222-023-10246-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Phylourny: Efficiently calculating elimination tournament win probabilities via phylogenetic methods},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatically adapting the number of state particles in SMC
<span class="math display"><sup>2</sup></span>. <em>SAC</em>,
<em>33</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10250-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Monte Carlo squared (SMC $$^2$$ ) methods can be used for parameter inference of intractable likelihood state-space models. These methods replace the likelihood with an unbiased particle filter estimate, similarly to particle Markov chain Monte Carlo (MCMC). As with particle MCMC, the efficiency of SMC $$^2$$ greatly depends on the variance of the likelihood estimator, and therefore on the number of state particles used within the particle filter. We introduce novel methods to adaptively select the number of state particles within SMC $$^2$$ using the expected squared jumping distance to trigger the adaptation, and modifying the exchange importance sampling method of Chopin et al. (J R Stat Soc: Ser B (Stat Method) 75(3):397–426, 2012) to replace the current set of state particles with the new set of state particles. The resulting algorithm is fully automatic, and can significantly improve current methods. Code for our methods is available at https://github.com/imkebotha/adaptive-exact-approximate-smc .},
  archive      = {J_SAC},
  author       = {Botha, Imke and Kohn, Robert and South, Leah and Drovandi, Christopher},
  doi          = {10.1007/s11222-023-10250-2},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Automatically adapting the number of state particles in SMC $$^2$$},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effect of intrinsic dimension on the bayes-error of
projected quadratic discriminant classification. <em>SAC</em>,
<em>33</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10251-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensionality is a common hurdle in machine learning and pattern classification; mitigating its effects has attracted extensive research efforts. It has been found in a recent NeurIPS paper that, when the data possesses a low effective dimension, the predictive performance of a discriminative quadratic classifier with nuclear norm regularisation enjoys a reduced (logarithmic) dependence on the ambient dimension and depends on the effective dimension instead, while other regularisers are insensitive to the effective dimension. In this paper, we show that dependence on the effective dimension is also exhibited by the Bayes error of the generative Quadratic Discriminant Analysis (QDA) classifier, without any explicit regularisation, under three linear dimensionality reduction schemes. Specifically, we derive upper bounds on the Bayes error of QDA, which adapt to the effective dimension, and entirely bypass any dependence on the ambient dimension. Our findings complement previous results on compressive QDA that were obtained under compressive sensing type assumptions on the covariance structure. In contrast, our bounds make no a-priori assumptions on the covariance structure, in turn they tighten in the presence of benign traits of the covariance. We corroborate our findings with numerical experiments.},
  archive      = {J_SAC},
  author       = {Palias, Efstratios and Kabán, Ata},
  doi          = {10.1007/s11222-023-10251-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {The effect of intrinsic dimension on the bayes-error of projected quadratic discriminant classification},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable computations for nonstationary gaussian processes.
<em>SAC</em>, <em>33</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10252-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonstationary Gaussian process models can capture complex spatially varying dependence structures in spatial data. However, the large number of observations in modern datasets makes fitting such models computationally intractable with conventional dense linear algebra. In addition, derivative-free or even first-order optimization methods can be slow to converge when estimating many spatially varying parameters. We present here a computational framework that couples an algebraic block-diagonal plus low-rank covariance matrix approximation with stochastic trace estimation to facilitate the efficient use of second-order solvers for maximum likelihood estimation of Gaussian process models with many parameters. We demonstrate the effectiveness of these methods by simultaneously fitting 192 parameters in the popular nonstationary model of Paciorek and Schervish using 107,600 sea surface temperature anomaly measurements.},
  archive      = {J_SAC},
  author       = {Beckman, Paul G. and Geoga, Christopher J. and Stein, Michael L. and Anitescu, Mihai},
  doi          = {10.1007/s11222-023-10252-0},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Scalable computations for nonstationary gaussian processes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial replacement imputation estimation for partially
linear models with complex missing pattern covariates. <em>SAC</em>,
<em>33</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10258-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common problem in clinical data collection, which causes difficulty in the statistical analysis of such data. In this article, we consider the problem under a framework of a semiparametric partially linear model when observations are subject to complex missing pattern covariates. One natural question in the partially linear model is the choice of model structure, that is, how to decide which covariates are linear and which are nonlinear. If the correct model structure of the partially linear model is available, we propose to use a new imputation method called Partial Replacement IMputation Estimation (PRIME), which can overcome problems caused by incomplete data in the partially linear model. In the more challenging setting where the model structure is unknown a priori, we use PRIME in conjunction with model averaging (PRIME-MA) to adapt to the unknown model structure in the partially linear model. In simulation studies, we use various error distributions, sample sizes, missing data rates, covariate correlations, and noise levels, and PRIME outperforms other methods in almost all cases. With an unknown correct model structure, PRIME-MA has satisfactory performance in terms of prediction. Moreover, we conduct a study of influential factors in the Chinese Provincial Legal Funding Dataset from the Harvard Dataverse, which shows that our method performs better than the other models.},
  archive      = {J_SAC},
  author       = {Zhan, Zishu and Li, Xiangjie and Zhang, Jingxiao},
  doi          = {10.1007/s11222-023-10258-8},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Partial replacement imputation estimation for partially linear models with complex missing pattern covariates},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The forward–backward envelope for sampling with the
overdamped langevin algorithm. <em>SAC</em>, <em>33</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s11222-023-10254-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyse a proximal method based on the idea of forward–backward splitting for sampling from distributions with densities that are not necessarily smooth. In particular, we study the non-asymptotic properties of the Euler–Maruyama discretization of the Langevin equation, where the forward–backward envelope is used to deal with the non-smooth part of the dynamics. An advantage of this envelope, when compared to widely-used Moreu–Yoshida one and the MYULA algorithm, is that it maintains the MAP estimator of the original non-smooth distribution. We also study a number of numerical experiments that support our theoretical findings.},
  archive      = {J_SAC},
  author       = {Eftekhari, Armin and Vargas, Luis and Zygalakis, Konstantinos C.},
  doi          = {10.1007/s11222-023-10254-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {The forward–backward envelope for sampling with the overdamped langevin algorithm},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection using a smooth information criterion for
distributional regression models. <em>SAC</em>, <em>33</em>(3), 1–16.
(<a href="https://doi.org/10.1007/s11222-023-10204-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern variable selection procedures make use of penalization methods to execute simultaneous model selection and estimation. A popular method is the least absolute shrinkage and selection operator, the use of which requires selecting the value of a tuning parameter. This parameter is typically tuned by minimizing the cross-validation error or Bayesian information criterion, but this can be computationally intensive as it involves fitting an array of different models and selecting the best one. In contrast with this standard approach, we have developed a procedure based on the so-called “smooth IC” (SIC) in which the tuning parameter is automatically selected in one step. We also extend this model selection procedure to the distributional regression framework, which is more flexible than classical regression modelling. Distributional regression, also known as multiparameter regression, introduces flexibility by taking account of the effect of covariates through multiple distributional parameters simultaneously, e.g., mean and variance. These models are useful in the context of normal linear regression when the process under study exhibits heteroscedastic behaviour. Reformulating the distributional regression estimation problem in terms of penalized likelihood enables us to take advantage of the close relationship between model selection criteria and penalization. Utilizing the SIC is computationally advantageous, as it obviates the issue of having to choose multiple tuning parameters.},
  archive      = {J_SAC},
  author       = {O’Neill, Meadhbh and Burke, Kevin},
  doi          = {10.1007/s11222-023-10204-8},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Variable selection using a smooth information criterion for distributional regression models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On estimating the structure factor of a point process, with
applications to hyperuniformity. <em>SAC</em>, <em>33</em>(3), 1–28. (<a
href="https://doi.org/10.1007/s11222-023-10219-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperuniformity is the study of stationary point processes with a sub-Poisson variance in a large window. In other words, counting the points of a hyperuniform point process that fall in a given large region yields a small-variance Monte Carlo estimation of the volume. Hyperuniform point processes have received a lot of attention in statistical physics, both for the investigation of natural organized structures and the synthesis of materials. Unfortunately, rigorously proving that a point process is hyperuniform is usually difficult. A common practice in statistical physics and chemistry is to use a few samples to estimate a spectral measure called the structure factor. Its decay around zero provides a diagnostic of hyperuniformity. Different applied fields use however different estimators, and important algorithmic choices proceed from each field’s lore. This paper provides a systematic survey and derivation of known or otherwise natural estimators of the structure factor. We also leverage the consistency of these estimators to contribute the first asymptotically valid statistical test of hyperuniformity. We benchmark all estimators and hyperuniformity diagnostics on a set of examples. In an effort to make investigations of the structure factor and hyperuniformity systematic and reproducible, we further provide the Python toolbox structure_factor, containing all the estimators and tools that we discuss.},
  archive      = {J_SAC},
  author       = {Hawat, Diala and Gautier, Guillaume and Bardenet, Rémi and Lachièze-Rey, Raphaël},
  doi          = {10.1007/s11222-023-10219-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {On estimating the structure factor of a point process, with applications to hyperuniformity},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frugal gaussian clustering of huge imbalanced datasets
through a bin-marginal approach. <em>SAC</em>, <em>33</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10221-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering conceptually reveals all its interest when the dataset size considerably increases since there is the opportunity to discover tiny but possibly high value clusters which were out of reach with more modest sample sizes. However, clustering is practically faced to computer limits with such high data volume, since possibly requiring extremely high memory and computation resources. In addition, the classical subsampling strategy, often adopted to overcome these limitations, is expected to heavily failed for discovering clusters in the highly imbalanced cluster case. Our proposal first consists in drastically compressing the data volume by just preserving its bin-marginal values, thus discarding the bin-cross ones. Despite this extreme information loss, we then prove generic identifiability property for the diagonal mixture model and also introduce a specific EM-like algorithm associated to a composite likelihood approach. This latter is extremely more frugal than a regular but unfeasible EM algorithm expected to be used on our bin-marginal data, while preserving all consistency properties. Finally, numerical experiments highlight that this proposed method outperforms subsampling both in controlled simulations and in various real applications where imbalanced clusters may typically appear, such as image segmentation, hazardous asteroids recognition and fraud detection.},
  archive      = {J_SAC},
  author       = {Antonazzo, Filippo and Biernacki, Christophe and Keribin, Christine},
  doi          = {10.1007/s11222-023-10221-7},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Frugal gaussian clustering of huge imbalanced datasets through a bin-marginal approach},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based importance sampling via stochastic optimal
control for stochastic reaction networks. <em>SAC</em>, <em>33</em>(3),
1–17. (<a href="https://doi.org/10.1007/s11222-023-10222-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore efficient estimation of statistical quantities, particularly rare event probabilities, for stochastic reaction networks. Consequently, we propose an importance sampling (IS) approach to improve the Monte Carlo (MC) estimator efficiency based on an approximate tau-leap scheme. The crucial step in the IS framework is choosing an appropriate change of probability measure to achieve substantial variance reduction. This task is typically challenging and often requires insights into the underlying problem. Therefore, we propose an automated approach to obtain a highly efficient path-dependent measure change based on an original connection in the stochastic reaction network context between finding optimal IS parameters within a class of probability measures and a stochastic optimal control formulation. Optimal IS parameters are obtained by solving a variance minimization problem. First, we derive an associated dynamic programming equation. Analytically solving this backward equation is challenging, hence we propose an approximate dynamic programming formulation to find near-optimal control parameters. To mitigate the curse of dimensionality, we propose a learning-based method to approximate the value function using a neural network, where the parameters are determined via a stochastic optimization algorithm. Our analysis and numerical experiments verify that the proposed learning-based IS approach substantially reduces MC estimator variance, resulting in a lower computational complexity in the rare event regime, compared with standard tau-leap MC estimators.},
  archive      = {J_SAC},
  author       = {Ben Hammouda, Chiheb and Ben Rached, Nadhir and Tempone, Raúl and Wiechert, Sophia},
  doi          = {10.1007/s11222-023-10222-6},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Learning-based importance sampling via stochastic optimal control for stochastic reaction networks},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlation-based sparse inverse cholesky factorization for
fast gaussian-process inference. <em>SAC</em>, <em>33</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10231-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are widely used as priors for unknown functions in statistics and machine learning. To achieve computationally feasible inference for large datasets, a popular approach is the Vecchia approximation, which is an ordered conditional approximation of the data vector that implies a sparse Cholesky factor of the precision matrix. The ordering and sparsity pattern are typically determined based on Euclidean distance of the inputs or locations corresponding to the data points. Here, we propose instead to use a correlation-based distance metric, which implicitly applies the Vecchia approximation in a suitable transformed input space. The correlation-based algorithm can be carried out in quasilinear time in the size of the dataset, and so it can be applied even for iterative inference on unknown parameters in the correlation structure. The correlation-based approach has two advantages for complex settings: It can result in more accurate approximations, and it offers a simple, automatic strategy that can be applied to any covariance, even when Euclidean distance is not applicable. We demonstrate these advantages in several settings, including anisotropic, nonstationary, multivariate, and spatio-temporal processes. We also illustrate our method on multivariate spatio-temporal temperature fields produced by a regional climate model.},
  archive      = {J_SAC},
  author       = {Kang, Myeongjong and Katzfuss, Matthias},
  doi          = {10.1007/s11222-023-10231-5},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Correlation-based sparse inverse cholesky factorization for fast gaussian-process inference},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational tobit gaussian process regression. <em>SAC</em>,
<em>33</em>(3), 1–26. (<a
href="https://doi.org/10.1007/s11222-023-10225-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a variational inference-based framework for training a Gaussian process regression model subject to censored observational data. Data censoring is a typical problem encountered during the data gathering procedure and requires specialized techniques to perform inference since the resulting probabilistic models are typically analytically intractable. In this article we exploit the variational sparse Gaussian process inducing variable framework and local variational methods to compute an analytically tractable lower bound on the true log marginal likelihood of the probabilistic model which can be used to perform Bayesian model training and inference. We demonstrate the proposed framework on synthetically-produced, noise-corrupted observational data, as well as on a real-world data set, subject to artificial censoring. The resulting predictions are comparable to existing methods to account for data censoring, but provides a significant reduction in computational cost.},
  archive      = {J_SAC},
  author       = {Basson, Marno and Louw, Tobias M. and Smith, Theresa R.},
  doi          = {10.1007/s11222-023-10225-3},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Variational tobit gaussian process regression},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional concurrent hidden markov model. <em>SAC</em>,
<em>33</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10226-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers a functional concurrent hidden Markov model. The proposed model consists of two components. One is a transition model for elucidating how potential covariates influence the transition probability from one state to another. The other is a conditional functional linear concurrent regression model for characterizing the state-specific effects of functional covariates. A distribution-free random effect is introduced to the conditional model to describe the dependency of individual functional observations. The soft-thresholding operator and the adaptive group lasso are introduced to simultaneously accommodate the local and global sparsity of the functional coefficients. A Bayesian approach is developed to jointly conduct estimation, variable selection, and the detection of zero-effect regions. This proposed approach incorporates the dependent Dirichlet process with stick-breaking prior for accommodating the unspecified distribution of the random effect and a blocked Gibbs sampler for efficient posterior sampling. Finally, the empirical performance of the proposed method is evaluated through simulation studies, and the utility of the methodology is demonstrated by an application to the analysis of air pollution and meteorological data.},
  archive      = {J_SAC},
  author       = {Zhou, Xiaoxiao and Song, Xinyuan},
  doi          = {10.1007/s11222-023-10226-2},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Functional concurrent hidden markov model},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and generalizable tuning strategies for stochastic
gradient MCMC. <em>SAC</em>, <em>33</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-023-10233-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient Markov chain Monte Carlo (SGMCMC) is a popular class of algorithms for scalable Bayesian inference. However, these algorithms include hyperparameters such as step size or batch size that influence the accuracy of estimators based on the obtained posterior samples. As a result, these hyperparameters must be tuned by the practitioner and currently no principled and automated way to tune them exists. Standard Markov chain Monte Carlo tuning methods based on acceptance rates cannot be used for SGMCMC, thus requiring alternative tools and diagnostics. We propose a novel bandit-based algorithm that tunes the SGMCMC hyperparameters by minimizing the Stein discrepancy between the true posterior and its Monte Carlo approximation. We provide theoretical results supporting this approach and assess various Stein-based discrepancies. We support our results with experiments on both simulated and real datasets, and find that this method is practical for a wide range of applications.},
  archive      = {J_SAC},
  author       = {Coullon, Jeremie and South, Leah and Nemeth, Christopher},
  doi          = {10.1007/s11222-023-10233-3},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient and generalizable tuning strategies for stochastic gradient MCMC},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse matrix formulation of model-based ensemble kalman
filter. <em>SAC</em>, <em>33</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11222-023-10228-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a computationally efficient variant of the model-based ensemble Kalman filter (EnKF). We propose two changes to the original formulation. First, we phrase the setup in terms of precision matrices instead of covariance matrices, and introduce a new prior for the precision matrix which ensures it to be sparse. Second, we propose to split the state vector into several blocks and formulate an approximate updating procedure for each of these blocks. We study in a simulation example the computational speedup and the approximation error resulting from using the proposed approach. The speedup is substantial for high dimensional state vectors, allowing the proposed filter to be run on much larger problems than can be done with the original formulation. In the simulation example the approximation error resulting from using the introduced block updating is negligible compared to the Monte Carlo variability inherent in both the original and the proposed procedures.},
  archive      = {J_SAC},
  author       = {Gryvill, Håkon and Tjelmeland, Håkon},
  doi          = {10.1007/s11222-023-10228-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {A sparse matrix formulation of model-based ensemble kalman filter},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the optimality of the oja’s algorithm for online PCA.
<em>SAC</em>, <em>33</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11222-023-10229-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we analyze the behavior of the Oja’s algorithm for online/streaming principal component subspace estimation. It is proved that with high probability it performs an efficient, gap-free, global convergence rate to approximate an principal component subspace for any sub-Gaussian distribution. Moreover, it is the first time to show that the convergence rate, namely the upper bound of the approximation, exactly matches the lower bound of an approximation obtained by the offline/classical PCA up to a constant factor.},
  archive      = {J_SAC},
  author       = {Liang, Xin},
  doi          = {10.1007/s11222-023-10229-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {On the optimality of the oja’s algorithm for online PCA},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic variable metric proximal gradient with variance
reduction for non-convex composite optimization. <em>SAC</em>,
<em>33</em>(3), 1–30. (<a
href="https://doi.org/10.1007/s11222-023-10230-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel algorithm, the Perturbed Proximal Preconditioned SPIDER algorithm (3P-SPIDER), designed to solve finite sum non-convex composite optimization. It is a stochastic Variable Metric Forward–Backward algorithm, which allows approximate preconditioned forward operator and uses a variable metric proximity operator as the backward operator; it also proposes a mini-batch strategy with variance reduction to address the finite sum setting. We show that 3P-SPIDER extends some Stochastic preconditioned Gradient Descent-based algorithms and some Incremental Expectation Maximization algorithms to composite optimization and to the case the forward operator can not be computed in closed form. We also provide an explicit control of convergence in expectation of 3P-SPIDER, and study its complexity in order to satisfy the approximate epsilon-stationary condition. Our results are the first to combine the non-convex composite optimization setting, a variance reduction technique to tackle the finite sum setting by using a minibatch strategy and, to allow deterministic or random approximations of the preconditioned forward operator. Finally, through an application to inference in a logistic regression model with random effects, we numerically compare 3P-SPIDER to other stochastic forward–backward algorithms and discuss the role of some design parameters of 3P-SPIDER.},
  archive      = {J_SAC},
  author       = {Fort, Gersende and Moulines, Eric},
  doi          = {10.1007/s11222-023-10230-6},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic variable metric proximal gradient with variance reduction for non-convex composite optimization},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally and symmetrically identified bayesian multinomial
probit model. <em>SAC</em>, <em>33</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10232-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian multinomial probit models have been widely used to analyze discrete choice data. Existing methods have some shortcomings in parameter identification or sensitivity of posterior inference to labeling of choice objects. The main task of this study is to simultaneously deal with these problems. First we propose a globally and symmetrically identified multinomial probit model with covariance matrix positive semidefinite. However, it is difficult to design an efficient Bayesian algorithm to fit the model directly because it is infeasible to sample a positive semidefinite matrix from a regular distribution. Then we develop a projected model for the above proposed model by projection technique. This projected model is equivalent to the former one, but equips with a positive definite covariance matrix. Finally, based on the latter model, we develop an efficient Bayesian algorithm to fit it by using modern Markov chain Monte Carlo techniques. Through simulation studies and an analysis of clothes detergent purchases data, we demonstrated that our approach not only solved the identifiability problem, but also showed robustness and satisfactory estimation accuracy, while the computation costs were comparable.},
  archive      = {J_SAC},
  author       = {Pan, Maolin and Gu, Minggao and Wu, Xianyi and Fan, Xiaodan},
  doi          = {10.1007/s11222-023-10232-4},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Globally and symmetrically identified bayesian multinomial probit model},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The first-passage-time moments for the hougaard process and
its birnbaum–saunders approximation. <em>SAC</em>, <em>33</em>(3), 1–15.
(<a href="https://doi.org/10.1007/s11222-023-10235-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hougaard processes, which include gamma and inverse Gaussian processes as special cases, as well as the moments of the corresponding first-passage-time (FPT) distributions are commonly used in many applications. Because the density function of a Hougaard process involves an intractable infinite series, the Birnbaum–Saunders (BS) distribution is often used to approximate its FPT distribution. This article derives the finite moments of FPT distributions based on Hougaard processes and provides a theoretical justification for BS approximation in terms of convergence rates. Further, we show that the first moment of the FPT distribution for a Hougaard process approximated by the BS distribution is larger and provide a sharp upper bound for the difference using an exponential integral. The conditions for convergence coincidentally elucidate the classical convergence results of Hougaard distributions. Some numerical examples are proposed to support the validity and precision of the theoretical results.},
  archive      = {J_SAC},
  author       = {Peng, Chien-Yu and Dong, Yi-Shian and Fan, Tsai-Hung},
  doi          = {10.1007/s11222-023-10235-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {The first-passage-time moments for the hougaard process and its Birnbaum–Saunders approximation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian parametrized method for interval-valued
regression models. <em>SAC</em>, <em>33</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10234-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued data, as typical symbolic data, provide a feasible way to deal with massive data sets. Although a lot of literature has been focused on researching interval-valued regression models, few works are devoted to exploring Bayesian methods for interval-valued data. In this paper, we propose a novel Bayesian parametrized method for interval-valued data by transforming an interval into a reference point, and further establish a Bayesian linear regression model. The advantage of the Bayesian parametrized method is to make use of full information within intervals and meanwhile it can solve the potential problem of multicollinearity in the parametrized method. We assume the prior distribution is normal with zero mean, and employ the EM algorithm to obtain the empirical Bayes estimates. The results of experimental and real data sets show that the Bayesian parametrized method has a superior forecasting advantage when the sample size is small and the random error is large.},
  archive      = {J_SAC},
  author       = {Xu, Min and Qin, Zhongfeng},
  doi          = {10.1007/s11222-023-10234-2},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian parametrized method for interval-valued regression models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection and regularization via arbitrary
rectangle-range generalized elastic net. <em>SAC</em>, <em>33</em>(3),
1–21. (<a href="https://doi.org/10.1007/s11222-023-10240-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the arbitrary rectangle-range generalized elastic net penalty method, abbreviated to ARGEN, for performing constrained variable selection and regularization in high-dimensional sparse linear models. As a natural extension of the nonnegative elastic net penalty method, ARGEN is proved to have both variable selection consistency and estimation consistency under some conditions. The asymptotic behavior in distribution of the ARGEN estimators have been studied in this framework. We also propose an algorithm called MU-QP-RR-W- $$l_1$$ to efficiently solve the ARGEN problem. By conducting simulation study we show that ARGEN outperforms the elastic net in a number of settings. Finally an application of S &amp;P 500 index tracking with constraints on the stock allocations is performed to provide general guidance for adapting ARGEN to solve real-world problems.},
  archive      = {J_SAC},
  author       = {Ding, Yujia and Peng, Qidi and Song, Zhengming and Chen, Hansen},
  doi          = {10.1007/s11222-023-10240-4},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Variable selection and regularization via arbitrary rectangle-range generalized elastic net},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum likelihood estimation of the weibull distribution
with reduced bias. <em>SAC</em>, <em>33</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11222-023-10236-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this short note, we derive a new bias adjusted maximum likelihood estimate for the shape parameter of the Weibull distribution with complete data and type I censored data. The proposed estimate of the shape parameter is significantly less biased and more efficient than the corresponding maximum likelihood estimate, while being simple to compute using existing maximum likelihood software procedures.},
  archive      = {J_SAC},
  author       = {Makalic, Enes and Schmidt, Daniel F.},
  doi          = {10.1007/s11222-023-10236-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Maximum likelihood estimation of the weibull distribution with reduced bias},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The limited-memory recursive variational gaussian
approximation (l-RVGA). <em>SAC</em>, <em>33</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10239-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of computing a Gaussian approximation to the posterior distribution of a parameter given a large number N of observations and a Gaussian prior when the dimension of the parameter d is also large. To address this problem we build on a recently introduced recursive algorithm for variational Gaussian approximation of the posterior, called recursive variational Gaussian approximation (RVGA), which is a single pass algorithm, free of parameter tuning. In this paper, we consider the case where the parameter dimension d is high, and we propose a novel version of RVGA that scales linearly in the dimension d (as well as in the number of observations N), and which only requires linear storage capacity in d. This is afforded by using a novel recursive expectation maximization (EM) algorithm applied for factor analysis introduced herein, to approximate at each step the covariance matrix of the Gaussian distribution conveying the uncertainty in the parameter. The approach is successfully illustrated on the problems of high dimensional least-squares and logistic regression, and generalized to a large class of nonlinear models.},
  archive      = {J_SAC},
  author       = {Lambert, Marc and Bonnabel, Silvère and Bach, Francis},
  doi          = {10.1007/s11222-023-10239-x},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {The limited-memory recursive variational gaussian approximation (L-RVGA)},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). General jackknife empirical likelihood and its applications.
<em>SAC</em>, <em>33</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10245-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is to give a novel Jackknife Empirical Likelihood for both smooth and unsmooth statistical functionals. Specifically, we consider the estimation of unknown statistical functionals under the framework of general delete-d jackknife and a usage of the subsampling method which can reduce the computational burden. Moreover, the corresponding statistical inference issues and asymptotic properties of the proposed method are also investigated. Several related application examples are carefully conducted to check the superiority of our new proposed method. Finally, the finite sample simulation studies are also included.},
  archive      = {J_SAC},
  author       = {Liu, Yiming and Wang, Shaochen and Zhou, Wang},
  doi          = {10.1007/s11222-023-10245-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {General jackknife empirical likelihood and its applications},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed statistical optimization for non-randomly stored
big data with application to penalized learning. <em>SAC</em>,
<em>33</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-023-10247-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed optimization for big data has recently attracted enormous attention. However, the existing algorithms are all based on one critical randomness condition, i.e., the big data are randomly distributed on different machines. This is seldom in practice, and violating this condition can seriously degrade the estimation accuracy. To fix this problem, we propose a pilot dataset surrogate loss function based optimization framework, which can realize communication-efficient distributed optimization for non-randomly distributed big data. Furthermore, we also apply it to penalized high-dimensional sparse learning problems by combining it with the penalty functions. Theoretical properties and numerical results all confirm the good performance of the proposed methods.},
  archive      = {J_SAC},
  author       = {Wang, Kangning and Li, Shaomin},
  doi          = {10.1007/s11222-023-10247-x},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Distributed statistical optimization for non-randomly stored big data with application to penalized learning},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting matérn smoothness parameters using automatic
differentiation. <em>SAC</em>, <em>33</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10127-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Matérn covariance function is ubiquitous in the application of Gaussian processes to spatial statistics and beyond. Perhaps the most important reason for this is that the smoothness parameter $$\nu $$ gives complete control over the mean-square differentiability of the process, which has significant implications for the behavior of estimated quantities such as interpolants and forecasts. Unfortunately, derivatives of the Matérn covariance function with respect to $$\nu $$ require derivatives of the modified second-kind Bessel function $${\mathcal {K}}_{\nu }$$ with respect to $$\nu $$ . While closed form expressions of these derivatives do exist, they are prohibitively difficult and expensive to compute. For this reason, many software packages require fixing $$\nu $$ as opposed to estimating it, and all existing software packages that attempt to offer the functionality of estimating $$\nu $$ use finite difference estimates for $$\partial _\nu {\mathcal {K}}_{\nu }$$ . In this work, we introduce a new implementation of $${\mathcal {K}}_{\nu }$$ that has been designed to provide derivatives via automatic differentiation (AD), and whose resulting derivatives are significantly faster and more accurate than those computed using finite differences. We provide comprehensive testing for both speed and accuracy and show that our AD solution can be used to build accurate Hessian matrices for second-order maximum likelihood estimation in settings where Hessians built with finite difference approximations completely fail.},
  archive      = {J_SAC},
  author       = {Geoga, Christopher J. and Marin, Oana and Schanen, Michel and Stein, Michael L.},
  doi          = {10.1007/s11222-022-10127-w},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Fitting matérn smoothness parameters using automatic differentiation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptation of the tuning parameter in general bayesian
inference with robust divergence. <em>SAC</em>, <em>33</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s11222-023-10205-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel methodology for robust Bayesian estimation with robust divergence (e.g., density power divergence or $$\gamma $$ -divergence), indexed by tuning parameters. It is well known that the posterior density induced by robust divergence gives highly robust estimators against outliers if the tuning parameter is appropriately and carefully chosen. In a Bayesian framework, one way to find the optimal tuning parameter would be using evidence (marginal likelihood). However, we theoretically and numerically illustrate that evidence induced by the density power divergence does not work to select the optimal tuning parameter since robust divergence is not regarded as a statistical model. To overcome the problems, we treat the exponential of robust divergence as an unnormalisable statistical model, and we estimate the tuning parameter by minimising the Hyvarinen score. We also provide adaptive computational methods based on sequential Monte Carlo samplers, enabling us to obtain the optimal tuning parameter and samples from posterior distributions simultaneously. The empirical performance of the proposed method through simulations and an application to real data are also provided.},
  archive      = {J_SAC},
  author       = {Yonekura, Shouto and Sugasawa, Shonosuke},
  doi          = {10.1007/s11222-023-10205-7},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Adaptation of the tuning parameter in general bayesian inference with robust divergence},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new flexible bayesian hypothesis test for multivariate
data. <em>SAC</em>, <em>33</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-023-10214-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hypothesis testing procedure for comparing the multivariate distributions of several treatment groups against a control group. This test is derived from a flexible model for the group distributions based on a random binary vector such that, if its jth element equals one, then the jth treatment group is merged with the control group. The group distributions’ flexibility comes from a dependent Dirichlet process, while the latent vector prior distribution ensures a multiplicity correction to the testing procedure. We explore the posterior consistency of the Bayes factor and provide a Monte Carlo simulation study comparing the performance of our procedure with state-of-the-art alternatives. Our results show that the presented method performs better than competing approaches. Finally, we apply our proposal to two classical experiments. The first one studies the effects of tuberculosis vaccines on multiple health outcomes for rabbits, and the second one analyzes the effects of two drugs on weight gain for rats. In both applications, we find relevant differences between the control group and at least one treatment group.},
  archive      = {J_SAC},
  author       = {Gutiérrez, Iván and Gutiérrez, Luis and Alvares, Danilo},
  doi          = {10.1007/s11222-023-10214-6},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A new flexible bayesian hypothesis test for multivariate data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On predictive inference for intractable models via
approximate bayesian computation. <em>SAC</em>, <em>33</em>(2), 1–20.
(<a href="https://doi.org/10.1007/s11222-022-10163-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian computation (ABC) is commonly used for parameter estimation and model comparison for intractable simulator-based statistical models whose likelihood function cannot be evaluated. In this paper we instead investigate the feasibility of ABC as a generic approximate method for predictive inference, in particular, for computing the posterior predictive distribution of future observations or missing data of interest. We consider three complementary ABC approaches for this goal, each based on different assumptions regarding which predictive density of the intractable model can be sampled from. The case where only simulation from the joint density of the observed and future data given the model parameters can be used for inference is given particular attention and it is shown that the ideal summary statistic in this setting is minimal predictive sufficient instead of merely minimal sufficient (in the ordinary sense). An ABC prediction approach that takes advantage of a certain latent variable representation is also investigated. We additionally show how common ABC sampling algorithms can be used in the predictive settings considered. Our main results are first illustrated by using simple time-series models that facilitate analytical treatment, and later by using two common intractable dynamic models.},
  archive      = {J_SAC},
  author       = {Järvenpää, Marko and Corander, Jukka},
  doi          = {10.1007/s11222-022-10163-6},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {On predictive inference for intractable models via approximate bayesian computation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stochastic approximation ECME algorithm to semi-parametric
scale mixtures of centred skew normal regression models. <em>SAC</em>,
<em>33</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-023-10223-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many situations we are interested in modeling data where there is no a clear relationship between the response and the covariates. In the literature there are few related proposals based on the additive partially linear models and normal distribution. It is also common to find situations where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy tails. In these situations it is more suitable to consider models based on the general class of scale mixture of skew-normal distributions, mainly under the respective centered reparameterization, due to the some inferential issues. In this paper, we developed a class of additive partially linear models based on scale mixture of skew-normal under the centered parameterization. We explore a hierarchical representation and set up an algorithm for maximum likelihood estimation based on the stochastic-approximation-expectation-maximization and expectation-conditional-maximization-either algorithms. A Monte Carlo experiment is conducted to evaluate the performance of these estimators in small and moderate samples. Furthermore, we developed residuals and influence diagnostic tools. The methodology is illustrated with the analysis of a real data set.},
  archive      = {J_SAC},
  author       = {de Freitas, João Victor B. and Azevedo, Caio L. N. and Nobre, Juvêncio S.},
  doi          = {10.1007/s11222-023-10223-5},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {A stochastic approximation ECME algorithm to semi-parametric scale mixtures of centred skew normal regression models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State-dependent importance sampling for estimating
expectations of functionals of sums of independent random variables.
<em>SAC</em>, <em>33</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10202-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the expectations of functionals applied to sums of random variables (RVs) is a well-known problem encountered in many challenging applications. Generally, closed-form expressions of these quantities are out of reach. A naive Monte Carlo simulation is an alternative approach. However, this method requires numerous samples for rare event problems. Therefore, it is paramount to use variance reduction techniques to develop fast and efficient estimation methods. In this work, we use importance sampling (IS), known for its efficiency in requiring fewer computations to achieve the same accuracy requirements. We propose a state-dependent IS scheme based on a stochastic optimal control formulation, where the control is dependent on state and time. We aim to calculate rare event quantities that could be written as an expectation of a functional of the sums of independent RVs. The proposed algorithm is generic and can be applied without restrictions on the univariate distributions of RVs or the functional applied to the sum. We apply this approach to the log-normal distribution to compute the left tail and cumulative distribution of the ratio of independent RVs. For each case, we numerically demonstrate that the proposed state-dependent IS algorithm compares favorably to most well-known estimators dealing with similar problems.},
  archive      = {J_SAC},
  author       = {Ben Amar, Eya and Ben Rached, Nadhir and Haji-Ali, Abdul-Lateef and Tempone, Raúl},
  doi          = {10.1007/s11222-022-10202-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {State-dependent importance sampling for estimating expectations of functionals of sums of independent random variables},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical depth in abstract metric spaces. <em>SAC</em>,
<em>33</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10216-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of depth has proved very important for multivariate and functional data analysis, as it essentially acts as a surrogate for the notion of ranking of observations which is absent in more than one dimension. Motivated by the rapid development of technology, in particular the advent of ‘Big Data’, we extend here that concept to general metric spaces, propose a natural depth measure and explore its properties as a statistical depth function. Working in a general metric space allows the depth to be tailored to the data at hand and to the ultimate goal of the analysis, a very desirable property given the polymorphic nature of modern data sets. This flexibility is thoroughly illustrated by several real data analyses.},
  archive      = {J_SAC},
  author       = {Geenens, Gery and Nieto-Reyes, Alicia and Francisci, Giacomo},
  doi          = {10.1007/s11222-023-10216-4},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Statistical depth in abstract metric spaces},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile-distribution functions and their use for
classification, with application to naïve bayes classifiers.
<em>SAC</em>, <em>33</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-023-10224-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a flexible parametric framework for the estimation of quantile functions. This involves the specification of an analytical quantile-distribution function. It is shown to adapt well to a wide range of distributions under reasonable assumptions. We derive a least-square type estimator, leading to computationally efficient inference. By-products include a test for comparing two distributions, a variable selection method, and an innovative naïve Bayes classifier. Properties of the estimator, of the asymptotic test and of the classifier are investigated through theoretical results and simulation studies, and illustrated through a real data example.},
  archive      = {J_SAC},
  author       = {Redivo, Edoardo and Viroli, Cinzia and Farcomeni, Alessio},
  doi          = {10.1007/s11222-023-10224-4},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Quantile-distribution functions and their use for classification, with application to naïve bayes classifiers},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subgraph nomination: Query by example subgraph retrieval in
networks. <em>SAC</em>, <em>33</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10206-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the subgraph nomination inference task, in which example subgraphs of interest are used to query a network for similarly interesting subgraphs. This type of problem appears time and again in real world problems connected to, for example, user recommendation systems and structural retrieval tasks in social and biological/connectomic networks. We formally define the subgraph nomination framework with an emphasis on the notion of a user-in-the-loop in the subgraph nomination pipeline. In this setting, a user can provide additional post-nomination light supervision that can be incorporated into the retrieval task. After introducing and formalizing the retrieval task, we examine the nuanced effect that user-supervision can have on performance, both analytically and across real and simulated data examples.},
  archive      = {J_SAC},
  author       = {Al-Qadhi, Al-Fahad and Priebe, Carey E. and Helm, Hayden S. and Lyzinski, Vince},
  doi          = {10.1007/s11222-023-10206-6},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Subgraph nomination: Query by example subgraph retrieval in networks},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive step size rules for stochastic optimization in
large-scale learning. <em>SAC</em>, <em>33</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-023-10218-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of the step size in stochastic optimization has been confirmed both theoretically and empirically during the past few decades and reconsidered in recent years, especially for large-scale learning. Different rules of selecting the step size have been discussed since the arising of stochastic approximation methods. The first part of this work reviews the studies on several representative techniques of setting the step size, covering heuristic rules, meta-learning procedure, adaptive step size technique and line search technique. The second component of this work proposes a novel class of accelerating stochastic optimization methods by resorting to the Barzilai–Borwein (BB) technique with a diagonal selection rule for the metric, particularly, termed as DBB. We first explore the theoretical and empirical properties of variance reduced stochastic optimization algorithms with DBB. Especially, we study the theoretical and numerical properties of the resulting method under strongly convex and non-convex cases respectively. To great show the efficacy of the step size schedule of DBB, we extend it into more general stochastic optimization methods. The theoretical and empirical properties of such the case also developed under different cases. Extensive numerical results in machine learning are offered, suggesting that the proposed algorithms show much promise.},
  archive      = {J_SAC},
  author       = {Yang, Zhuang and Ma, Li},
  doi          = {10.1007/s11222-023-10218-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive step size rules for stochastic optimization in large-scale learning},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A SUR version of the bichon criterion for excursion set
estimation. <em>SAC</em>, <em>33</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10208-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many model inversion problems occur in industry. These problems consist in finding the set of parameter values such that a certain quantity of interest respects a constraint, for example remains below a threshold. In general, the quantity of interest is the output of a simulator, costly in computation time. An effective way to solve this problem is to replace the simulator by a Gaussian process regression, with an experimental design enriched sequentially by a well chosen acquisition criterion. Different inversion-adapted criteria exist such as the Bichon criterion (also known as expected feasibility function) and deviation number . There also exist a class of enrichment strategies (stepwise uncertainty reduction—SUR) which select the next point by measuring the expected uncertainty reduction induced by its selection. In this paper we propose a SUR version of the Bichon criterion. An explicit formulation of the criterion is given and test comparisons show good performances on classical test functions.},
  archive      = {J_SAC},
  author       = {Duhamel, Clément and Helbert, Céline and Munoz Zuniga, Miguel and Prieur, Clémentine and Sinoquet, Delphine},
  doi          = {10.1007/s11222-023-10208-4},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A SUR version of the bichon criterion for excursion set estimation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond homophilic dyadic interactions: The impact of network
formation on individual outcomes. <em>SAC</em>, <em>33</em>(2), 1–17.
(<a href="https://doi.org/10.1007/s11222-023-10215-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models of social interaction have been mostly focusing on the dyad, the smallest possible social structure, as a unit of network analysis. In the context of friendship networks, we argue that the triad could also be seen as a building block to ensure cohesion and stability of larger group structures. By explicitly modeling the mechanism behind network formation, individual attributes (such as gender and ethnicity) are often dissociated from purely structural network effects (such as popularity) acknowledging the presence of more complex configurations. Allowing structural configurations to emerge when nodes share similar attribute values, real-world networks are more adequately described. We present a comprehensive set of network statistics that allow for continuous attributes to be accounted for. We also draw on the important literature on endogenous social effects to further explore the role of network structures on individual outcomes. A series of Monte Carlo experiments and an empirical example analyzing students’ friendship networks illustrate the importance of properly modeling attribute-based structural effects. In addition, we model unobserved nodal heterogeneity in the network formation process to control for possible friendship selection bias on educational outcomes. A critical issue discussed is whether friendships are related to homogeneity across several attributes or by a balance between homophily on some, such as gender and race, but heterophily on others, such as socio-economic factors.},
  archive      = {J_SAC},
  author       = {Weng, Huibin and Parent, Olivier},
  doi          = {10.1007/s11222-023-10215-5},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Beyond homophilic dyadic interactions: The impact of network formation on individual outcomes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). De-noising boosting methods for variable selection and
estimation subject to error-prone variables. <em>SAC</em>,
<em>33</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11222-023-10209-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting is one of the most powerful statistical learning methods that combines multiple weak learners into a strong learner. The main idea of boosting is to sequentially apply the algorithm to enhance its performance. Recently, boosting methods have been implemented to handle variable selection. However, little work has been available to deal with complex data such as measurement error in covariates. In this paper, we adopt the boosting method to do variable selection, especially in the presence of measurement error. We develop two different approximated correction approaches to deal with different types of responses, and meanwhile, eliminate measurement error effects. In addition, the proposed algorithms are easy to implement and are able to derive precise estimators. Throughout numerical studies under various settings, the proposed method outperforms other competitive approaches.},
  archive      = {J_SAC},
  author       = {Chen, Li-Pang},
  doi          = {10.1007/s11222-023-10209-3},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {De-noising boosting methods for variable selection and estimation subject to error-prone variables},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic variational inference for scalable non-stationary
gaussian process regression. <em>SAC</em>, <em>33</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-023-10210-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A natural extension to standard Gaussian process (GP) regression is the use of non-stationary Gaussian processes, an approach where the parameters of the covariance kernel are allowed to vary in time or space. The non-stationary GP is a flexible model that relaxes the strong prior assumption of standard GP regression, that the covariance properties of the inferred functions are constant across the input space. Non-stationary GPs typically model varying covariance kernel parameters as further lower-level GPs, thereby enabling sampling-based inference. However, due to the high computational costs and inherently sequential nature of MCMC sampling, these methods do not scale to large datasets. Here we develop a variational inference approach to fitting non-stationary GPs that combines sparse GP regression methods with a trajectory segmentation technique. Our method is scalable to large datasets containing potentially millions of data points. We demonstrate the effectiveness of our approach on both synthetic and real world datasets.},
  archive      = {J_SAC},
  author       = {Paun, Ionut and Husmeier, Dirk and Torney, Colin J.},
  doi          = {10.1007/s11222-023-10210-w},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic variational inference for scalable non-stationary gaussian process regression},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unbalanced distributed estimation and inference for the
precision matrix in gaussian graphical models. <em>SAC</em>,
<em>33</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10211-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the estimation of Gaussian graphical models in the unbalanced distributed framework. It provides an effective approach when the available machines are of different powers or when the existing dataset comes from different sources with different sizes and cannot be aggregated in one single machine. In this paper, we propose a new aggregated estimator of the precision matrix and justify such an approach by both theoretical and practical arguments. The limit distribution and convergence rate for this estimator are provided under sparsity conditions on the true precision matrix and controlling for the number of machines. Furthermore, a procedure for performing statistical inference is proposed. On the practical side, using a simulation study and a real data example, we show that the performance of the distributed estimator is similar to that of the non-distributed estimator that uses the full data.},
  archive      = {J_SAC},
  author       = {Nezakati, Ensiyeh and Pircalabelu, Eugen},
  doi          = {10.1007/s11222-023-10211-9},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Unbalanced distributed estimation and inference for the precision matrix in gaussian graphical models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum softly-penalized likelihood for mixed effects
logistic regression. <em>SAC</em>, <em>33</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-023-10217-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood estimation in logistic regression with mixed effects is known to often result in estimates on the boundary of the parameter space. Such estimates, which include infinite values for fixed effects and singular or infinite variance components, can cause havoc to numerical estimation procedures and inference. We introduce an appropriately scaled additive penalty to the log-likelihood function, or an approximation thereof, which penalizes the fixed effects by the Jeffreys’ invariant prior for the model with no random effects and the variance components by a composition of negative Huber loss functions. The resulting maximum penalized likelihood estimates are shown to lie in the interior of the parameter space. Appropriate scaling of the penalty guarantees that the penalization is soft enough to preserve the optimal asymptotic properties expected by the maximum likelihood estimator, namely consistency, asymptotic normality, and Cramér-Rao efficiency. Our choice of penalties and scaling factor preserves equivariance of the fixed effects estimates under linear transformation of the model parameters, such as contrasts. Maximum softly-penalized likelihood is compared to competing approaches on two real-data examples, and through comprehensive simulation studies that illustrate its superior finite sample performance.},
  archive      = {J_SAC},
  author       = {Sterzinger, Philipp and Kosmidis, Ioannis},
  doi          = {10.1007/s11222-023-10217-3},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Maximum softly-penalized likelihood for mixed effects logistic regression},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic time series forecasts with autoregressive
transformation models. <em>SAC</em>, <em>33</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-023-10212-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic forecasting of time series is an important matter in many applications and research fields. In order to draw conclusions from a probabilistic forecast, we must ensure that the model class used to approximate the true forecasting distribution is expressive enough. Yet, characteristics of the model itself, such as its uncertainty or its feature-outcome relationship are not of lesser importance. This paper proposes Autoregressive Transformation Models (ATMs), a model class inspired by various research directions to unite expressive distributional forecasts using a semi-parametric distribution assumption with an interpretable model specification. We demonstrate the properties of ATMs both theoretically and through empirical evaluation on several simulated and real-world forecasting datasets.},
  archive      = {J_SAC},
  author       = {Rügamer, David and Baumann, Philipp F. M. and Kneib, Thomas and Hothorn, Torsten},
  doi          = {10.1007/s11222-023-10212-8},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Probabilistic time series forecasts with autoregressive transformation models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast epigraph and hypograph-based approach for clustering
functional data. <em>SAC</em>, <em>33</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10213-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering techniques for multivariate data are useful tools in Statistics that have been fully studied in the literature. However, there is limited literature on clustering methodologies for functional data. Our proposal consists of a clustering procedure for functional data using techniques for clustering multivariate data. The idea is to reduce a functional data problem into a multivariate one by applying the epigraph and hypograph indexes to the original curves and to their first and/or second derivatives. All the information given by the functional data is therefore transformed to the multivariate context, being informative enough for the usual multivariate clustering techniques to be efficient. The performance of this new methodology is evaluated through a simulation study and is also illustrated through real data sets. The results are compared to some other clustering procedures for functional data.},
  archive      = {J_SAC},
  author       = {Pulido, Belén and Franco-Pereira, Alba M. and Lillo, Rosa E.},
  doi          = {10.1007/s11222-023-10213-7},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A fast epigraph and hypograph-based approach for clustering functional data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast bayesian inference of block nearest neighbor gaussian
models for large data. <em>SAC</em>, <em>33</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-023-10227-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the development of a spatial block-Nearest Neighbor Gaussian process (blockNNGP) for location-referenced large spatial data. The key idea behind this approach is to divide the spatial domain into several blocks which are dependent under some constraints. The cross-blocks capture the large-scale spatial dependence, while each block captures the small-scale spatial dependence. The resulting blockNNGP enjoys Markov properties reflected on its sparse precision matrix. It is embedded as a prior within the class of latent Gaussian models, thus fast Bayesian inference is obtained using the integrated nested Laplace approximation. The performance of the blockNNGP is illustrated on simulated examples, a comparison of our approach with other methods for analyzing large spatial data and applications with Gaussian and non-Gaussian real data.},
  archive      = {J_SAC},
  author       = {Quiroz, Zaida C. and Prates, Marcos O. and Dey, Dipak K. and Rue, H.åvard},
  doi          = {10.1007/s11222-023-10227-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fast bayesian inference of block nearest neighbor gaussian models for large data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-index antithetic stochastic gradient algorithm.
<em>SAC</em>, <em>33</em>(2), 1–37. (<a
href="https://doi.org/10.1007/s11222-023-10220-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational statistics, machine learning and optimisation. Recent years have brought an influx of interest in SGAs, and the non-asymptotic analysis of their bias is by now well-developed. However, relatively little is known about the optimal choice of the random approximation (e.g mini-batching) of the gradient in SGAs as this relies on the analysis of the variance and is problem specific. While there have been numerous attempts to reduce the variance of SGAs, these typically exploit a particular structure of the sampled distribution by requiring a priori knowledge of its density’s mode. In this paper, we construct a Multi-index Antithetic Stochastic Gradient Algorithm (MASGA) whose implementation is independent of the structure of the target measure. Our rigorous theoretical analysis demonstrates that for log-concave targets, MASGA achieves performance on par with Monte Carlo estimators that have access to unbiased samples from the distribution of interest. In other words, MASGA is an optimal estimator from the mean square error-computational cost perspective within the class of Monte Carlo estimators. To illustrate the robustness of our approach, we implement MASGA also in some simple non-log-concave numerical examples, however, without providing theoretical guarantees on our algorithm’s performance in such settings.},
  archive      = {J_SAC},
  author       = {Majka, Mateusz B. and Sabate-Vidales, Marc and Szpruch, Łukasz},
  doi          = {10.1007/s11222-023-10220-8},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-37},
  shortjournal = {Stat. Comput.},
  title        = {Multi-index antithetic stochastic gradient algorithm},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On proportional volume sampling for experimental design in
general spaces. <em>SAC</em>, <em>33</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-022-10115-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal design for linear regression is a fundamental task in statistics. For finite design spaces, recent progress has shown that random designs drawn using proportional volume sampling (PVS for short) lead to polynomial-time algorithms with approximation guarantees that outperform i.i.d. sampling. PVS strikes the balance between design nodes that jointly fill the design space, while marginally staying in regions of high mass under the solution of a relaxed convex version of the original problem. In this paper, we examine some of the statistical implications of a new variant of PVS for (possibly Bayesian) optimal design. Using point process machinery, we treat the case of a generic Polish design space. We show that not only are known A-optimality approximation guarantees preserved, but we obtain similar guarantees for D-optimal design that tighten recent results. Moreover, we show that our PVS variant can be sampled in polynomial time. Unfortunately, in spite of its elegance and tractability, we demonstrate on a simple example that the practical implications of general PVS are likely limited. In the second part of the paper, we focus on applications and investigate the use of PVS as a subroutine for stochastic search heuristics. We demonstrate that PVS is a robust addition to the practitioner’s toolbox, especially when the regression functions are nonstandard and the design space, while low-dimensional, has a complicated shape (e.g., nonlinear boundaries, several connected components).},
  archive      = {J_SAC},
  author       = {Poinas, Arnaud and Bardenet, Rémi},
  doi          = {10.1007/s11222-022-10115-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {On proportional volume sampling for experimental design in general spaces},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian learning via neural schrödinger–föllmer flows.
<em>SAC</em>, <em>33</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-022-10172-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we explore a new framework for approximate Bayesian inference in large datasets based on stochastic control. We advocate stochastic control as a finite time and low variance alternative to popular steady-state methods such as stochastic gradient Langevin dynamics. Furthermore, we discuss and adapt the existing theoretical guarantees of this framework and establish connections to already existing VI routines in SDE-based models.},
  archive      = {J_SAC},
  author       = {Vargas, Francisco and Ovsianas, Andrius and Fernandes, David and Girolami, Mark and Lawrence, Neil D. and Nüsken, Nikolas},
  doi          = {10.1007/s11222-022-10172-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian learning via neural Schrödinger–Föllmer flows},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian a-optimal two-phase designs with a single blocking
factor in each phase. <em>SAC</em>, <em>33</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10126-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase experiments are widely used in many areas of science (e.g., agriculture, industrial engineering, food processing, etc.). For example, consider a two-phase experiment in plant breeding. Often, the first phase of this experiment is run in a field involving several blocks. The samples obtained from the first phase are then analyzed in several machines (or days, etc.) in a laboratory in the second phase. There might be field-block-to-field-block and machine-to-machine (or day-to-day, etc.) variation. Thus, it is practical to consider these sources of variation as blocking factors. Clearly, there are two possible strategies to analyze this kind of two-phase experiment, i.e., blocks are treated as fixed or random. While there are a few studies regarding fixed block effects, there are still a limited number of studies with random block effects and when information of block effects is uncertain. Hence, it is beneficial to consider a Bayesian approach to design for such an experiment, which is the main goal of this work. In this paper, we construct a design for a two-phase experiment that has a single treatment factor, a single blocking factor in each phase, and a response that can only be observed in the second phase.},
  archive      = {J_SAC},
  author       = {Vo-Thanh, Nha and Piepho, Hans-Peter},
  doi          = {10.1007/s11222-022-10126-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian A-optimal two-phase designs with a single blocking factor in each phase},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing two-level <span
class="math display"><em>Q</em><sub><em>B</em></sub></span> -optimal
screening designs using mixed-integer programming and heuristic
algorithms. <em>SAC</em>, <em>33</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10168-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-level screening designs are widely applied in manufacturing industry to identify influential factors of a system. These designs have each factor at two levels and are traditionally constructed using standard algorithms, which rely on a pre-specified linear model. Since the assumed model may depart from the truth, two-level $$Q_B$$ -optimal designs have been developed to provide efficient parameter estimates for several potential models. These designs also have an overarching goal that models that are more likely to be the best for explaining the data are estimated more efficiently than the rest. However, there is no effective algorithm for constructing them. This article proposes two methods: a mixed-integer programming algorithm that guarantees convergence to the two-level $$Q_B$$ -optimal designs; and, a heuristic algorithm that employs a novel formula to find good designs in short computing times. Using numerical experiments, we show that our mixed-integer programming algorithm is attractive to find small optimal designs, and our heuristic algorithm is the most computationally-effective approach to construct both small and large designs, when compared to benchmark heuristic algorithms.},
  archive      = {J_SAC},
  author       = {Vazquez, Alan R. and Wong, Weng Kee and Goos, Peter},
  doi          = {10.1007/s11222-022-10168-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Constructing two-level $$Q_B$$ -optimal screening designs using mixed-integer programming and heuristic algorithms},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic search intervals for the smoothing parameter in
penalized splines. <em>SAC</em>, <em>33</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10178-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of smoothing parameter is central to the estimation of penalized splines. The best value of the smoothing parameter is often the one that optimizes a smoothness selection criterion, such as generalized cross-validation error (GCV) and restricted likelihood (REML). To correctly identify the global optimum rather than being trapped in an undesired local optimum, grid search is recommended for optimization. Unfortunately, the grid search method requires a pre-specified search interval that contains the unknown global optimum, yet no guideline is available for providing this interval. As a result, practitioners have to find it by trial and error. To overcome such difficulty, we develop novel algorithms to automatically find this interval. Our automatic search interval has four advantages. (i) It specifies a smoothing parameter range where the associated penalized least squares problem is numerically solvable. (ii) It is criterion-independent so that different criteria, such as GCV and REML, can be explored on the same parameter range. (iii) It is sufficiently wide to contain the global optimum of any criterion, so that for example, the global minimum of GCV and the global maximum of REML can both be identified. (iv) It is computationally cheap compared with the grid search itself, carrying no extra computational burden in practice. Our method is ready to use through our recently developed R package gps ( $$\ge $$ version 1.1). It may be embedded in more advanced statistical modeling methods that rely on penalized splines.},
  archive      = {J_SAC},
  author       = {Li, Zheyuan and Cao, Jiguo},
  doi          = {10.1007/s11222-022-10178-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Automatic search intervals for the smoothing parameter in penalized splines},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure-based hyperparameter selection with bayesian
optimization in multidimensional scaling. <em>SAC</em>, <em>33</em>(1),
1–18. (<a href="https://doi.org/10.1007/s11222-022-10197-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the structure optimized proximity scaling (STOPS) framework for hyperparameter selection in parametrized multidimensional scaling and extensions (proximity scaling; PS). The selection process for hyperparameters is based on the idea that we want the configuration to show a certain structural quality (c-structuredness). A number of structures and how to measure them are discussed. We combine the structural quality by means of c-structuredness indices with the PS badness-of-fit measure in a multi-objective scalarization approach, yielding the Stoploss objective. Computationally we suggest a profile-type algorithm that first solves the PS problem and then uses Stoploss in an outer step to optimize over the hyperparameters. Bayesian optimization with treed Gaussian processes as a an apt and efficient strategy for carrying out the outer optimization is recommended. This way, hyperparameter tuning for many instances of PS is covered in a single conceptual framework. We illustrate the use of the STOPS framework with three data examples.},
  archive      = {J_SAC},
  author       = {Rusch, Thomas and Mair, Patrick and Hornik, Kurt},
  doi          = {10.1007/s11222-022-10197-w},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Structure-based hyperparameter selection with bayesian optimization in multidimensional scaling},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On randomized sketching algorithms and the tracy–widom law.
<em>SAC</em>, <em>33</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10148-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing body of work exploring the integration of random projection into algorithms for numerical linear algebra. The primary motivation is to reduce the overall computational cost of processing large datasets. A suitably chosen random projection can be used to embed the original dataset in a lower-dimensional space such that key properties of the original dataset are retained. These algorithms are often referred to as sketching algorithms, as the projected dataset can be used as a compressed representation of the full dataset. We show that random matrix theory, in particular the Tracy–Widom law, is useful for describing the operating characteristics of sketching algorithms in the tall-data regime when the sample size n is much greater than the number of variables d. Asymptotic large sample results are of particular interest as this is the regime where sketching is most useful for data compression. In particular, we develop asymptotic approximations for the success rate in generating random subspace embeddings and the convergence probability of iterative sketching algorithms. We test a number of sketching algorithms on real large high-dimensional datasets and find that the asymptotic expressions give accurate predictions of the empirical performance.},
  archive      = {J_SAC},
  author       = {Ahfock, Daniel and Astle, William J. and Richardson, Sylvia},
  doi          = {10.1007/s11222-022-10148-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {On randomized sketching algorithms and the Tracy–Widom law},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape modeling with spline partitions. <em>SAC</em>,
<em>33</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10170-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape modelling (with methods that output shapes) is a new and important task in Bayesian nonparametrics and bioinformatics. In this work, we focus on Bayesian nonparametric methods for capturing shapes by partitioning a space using curves. In related work, the classical Mondrian process is used to partition spaces recursively with axis-aligned cuts, and is widely lied in multi-dimensional and relational data. The Mondrian process outputs hyper-rectangles. Recently, the random tessellation process was introduced as a generalization of the Mondrian process, partitioning a domain with non-axis aligned cuts in an arbitrary dimensional space, and outputting polytopes. Motivated by these processes, in this work, we propose a novel parallelized Bayesian nonparametric approach to partition a domain with curves, enabling complex data-shapes to be acquired. We apply our method to HIV-1-infected human macrophage image dataset, and also simulated datasets sets to illustrate our approach. We compare to support vector machines, random forests and state-of-the-art computer vision methods such as simple linear iterative clustering super pixel image segmentation. We develop an R package that is available at https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions .},
  archive      = {J_SAC},
  author       = {Ge, Shufei and Wang, Shijia and Elliott, Lloyd},
  doi          = {10.1007/s11222-022-10170-7},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Shape modeling with spline partitions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse logistic functional principal component analysis for
binary data. <em>SAC</em>, <em>33</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10190-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional binary datasets occur frequently in real practice, whereas discrete characteristics of the data can bring challenges to model estimation. In this paper, we propose a sparse logistic functional principal component analysis (SLFPCA) method to handle functional binary data. The SLFPCA looks for local sparsity of the eigenfunctions to obtain convenience in interpretation. We formulate the problem through a penalized Bernoulli likelihood with both roughness penalty and sparseness penalty terms. An innovative algorithm is developed for the optimization of the penalized likelihood using majorization-minimization algorithm. The proposed method is accompanied by R package SLFPCA for implementation. The theoretical results indicate both consistency and sparsistency of the proposed method. We conduct a thorough numerical experiment to demonstrate the advantages of the SLFPCA approach. Our method is further applied to a physical activity dataset.},
  archive      = {J_SAC},
  author       = {Zhong, Rou and Liu, Shishi and Li, Haocheng and Zhang, Jingxiao},
  doi          = {10.1007/s11222-022-10190-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Sparse logistic functional principal component analysis for binary data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction scoring of data-driven discoveries for
reproducible research. <em>SAC</em>, <em>33</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-022-10154-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive modeling uncovers knowledge and insights regarding a hypothesized data generating mechanism (DGM). Results from different studies on a complex DGM, derived from different data sets, and using complicated models and algorithms, are hard to quantitatively compare due to random noise and statistical uncertainty in model results. This has been one of the main contributors to the replication crisis in the behavioral sciences. The contribution of this paper is to apply prediction scoring to the problem of comparing two studies, such as can arise when evaluating replications or competing evidence. We examine the role of predictive models in quantitatively assessing agreement between two datasets that are assumed to come from two distinct DGMs. We formalize a distance between the DGMs that is estimated using cross validation. We argue that the resulting prediction scores depend on the predictive models created by cross validation. In this sense, the prediction scores measure the distance between DGMs, along the dimension of the particular predictive model. Using human behavior data from experimental economics, we demonstrate that prediction scores can be used to evaluate preregistered hypotheses and provide insights comparing data from different populations and settings. We examine the asymptotic behavior of the prediction scores using simulated experimental data and demonstrate that leveraging competing predictive models can reveal important differences between underlying DGMs. Our proposed cross-validated prediction scores are capable of quantifying differences between unobserved data generating mechanisms and allow for the validation and assessment of results from complex models.},
  archive      = {J_SAC},
  author       = {Smith, Anna L. and Zheng, Tian and Gelman, Andrew},
  doi          = {10.1007/s11222-022-10154-7},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Prediction scoring of data-driven discoveries for reproducible research},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust discrete choice models with t-distributed kernel
errors. <em>SAC</em>, <em>33</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-022-10182-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers in discrete choice response data may result from misclassification and misreporting of the response variable and from choice behaviour that is inconsistent with modelling assumptions (e.g. random utility maximisation). In the presence of outliers, standard discrete choice models produce biased estimates and suffer from compromised predictive accuracy. Robust statistical models are less sensitive to outliers than standard non-robust models. This paper analyses two robust alternatives to the multinomial probit (MNP) model. The two models are robit models whose kernel error distributions are heavy-tailed t-distributions to moderate the influence of outliers. The first model is the multinomial robit (MNR) model, in which a generic degrees of freedom parameter controls the heavy-tailedness of the kernel error distribution. The second model, the generalised multinomial robit (Gen-MNR) model, is more flexible than MNR, as it allows for distinct heavy-tailedness in each dimension of the kernel error distribution. For both models, we derive Gibbs samplers for posterior inference. In a simulation study, we illustrate the finite sample properties of the proposed Bayes estimators and show that MNR and Gen-MNR produce more accurate estimates if the choice data contain outliers through the lens of the non-robust MNP model. In a case study on transport mode choice behaviour, MNR and Gen-MNR outperform MNP by substantial margins in terms of in-sample fit and out-of-sample predictive accuracy. The case study also highlights differences in elasticity estimates across models.},
  archive      = {J_SAC},
  author       = {Krueger, Rico and Bierlaire, Michel and Gasos, Thomas and Bansal, Prateek},
  doi          = {10.1007/s11222-022-10182-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Robust discrete choice models with t-distributed kernel errors},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible tree-structured regression models for discrete
event times. <em>SAC</em>, <em>33</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-022-10196-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete hazard models are widely applied for the analysis of time-to-event outcomes that are intrinsically discrete or grouped versions of continuous event times. Commonly, one assumes that the effect of explanatory variables on the hazard can be described by a linear predictor function. This, however, may be not appropriate when non-linear effects or interactions between the explanatory variables occur in the data. To address this issue, we propose a novel class of discrete hazard models that utilizes recursive partitioning techniques and allows to include the effects of explanatory variables in a flexible data-driven way. We introduce a tree-building algorithm that inherently performs variable selection and facilitates the inclusion of non-linear effects and interactions, while the favorable additive form of the predictor function is kept. In a simulation study, the proposed class of models is shown to be competitive with alternative approaches, including a penalized parametric model and Bayesian additive regression trees, in terms of predictive performance and the ability to detect informative variables. The modeling approach is illustrated by two real-world applications analyzing data of patients with odontogenic infection and lymphatic filariasis.},
  archive      = {J_SAC},
  author       = {Spuck, Nikolai and Schmid, Matthias and Heim, Nils and Klarmann-Schulz, Ute and Hörauf, Achim and Berger, Moritz},
  doi          = {10.1007/s11222-022-10196-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Flexible tree-structured regression models for discrete event times},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient simulation of p-tempered <span
class="math display"><em>α</em></span> -stable OU processes.
<em>SAC</em>, <em>33</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10165-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop efficient methods for simulating processes of Ornstein–Uhlenbeck type related to the class of p-tempered $$\alpha $$ -stable ( $$\textrm{TS}^p_\alpha $$ ) distributions. Our results hold for both the univariate and multivariate cases and we consider both the case where the $$\textrm{TS}^p_\alpha $$ distribution is the stationary law and where it is the distribution of the background driving Lévy process. In the latter case, we also derive an explicit representation for the transition law as this was previous known only in certain special cases and only for $$p=1$$ and $$\alpha \in [0,1)$$ . Simulation results suggest that our methods work well in practice.},
  archive      = {J_SAC},
  author       = {Grabchak, Michael and Sabino, Piergiacomo},
  doi          = {10.1007/s11222-022-10165-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Efficient simulation of p-tempered $$\alpha $$ -stable OU processes},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic model training under restrictive time constraints.
<em>SAC</em>, <em>33</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s11222-022-10166-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a hyperparameter optimisation algorithm, Automated Budget Constrained Training, which balances the quality of a model with the computational cost required to tune it. The relationship between hyperparameters, model quality and computational cost must be learnt and this learning is incorporated directly into the optimisation problem. At each training epoch, the algorithm decides whether to terminate or continue training, and, in the latter case, what values of hyperparameters to use. This decision weighs optimally potential improvements in the quality with the additional training time and the uncertainty about the learnt quantities. The performance of our algorithm is verified on a number of machine learning problems encompassing random forests and neural networks. Our approach is rooted in the theory of Markov decision processes with partial information and we develop a numerical method to compute the value function and an optimal strategy.},
  archive      = {J_SAC},
  author       = {Cironis, Lukas and Palczewski, Jan and Aivaliotis, Georgios},
  doi          = {10.1007/s11222-022-10166-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Stat. Comput.},
  title        = {Automatic model training under restrictive time constraints},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical hilbert space approximate bayesian gaussian
processes for probabilistic programming. <em>SAC</em>, <em>33</em>(1),
1–28. (<a href="https://doi.org/10.1007/s11222-022-10167-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are powerful non-parametric probabilistic models for stochastic functions. However, the direct implementation entails a complexity that is computationally intractable when the number of observations is large, especially when estimated with fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we focus on a low-rank approximate Bayesian Gaussian processes, based on a basis function approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is a detailed analysis of the performance, and practical recommendations for how to select the number of basis functions and the boundary factor. Intuitive visualizations and recommendations, make it easier for users to improve approximation accuracy and computational performance. We also propose diagnostics for checking that the number of basis functions and the boundary factor are adequate given the data. The approach is simple and exhibits an attractive computational complexity due to its linear structure, and it is easy to implement in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the probabilistic programming language Stan are presented together with the underlying Stan model code.},
  archive      = {J_SAC},
  author       = {Riutort-Mayol, Gabriel and Bürkner, Paul-Christian and Andersen, Michael R. and Solin, Arno and Vehtari, Aki},
  doi          = {10.1007/s11222-022-10167-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Practical hilbert space approximate bayesian gaussian processes for probabilistic programming},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to: Variational inference and sparsity in
high-dimensional deep gaussian mixture models. <em>SAC</em>,
<em>33</em>(1), 1. (<a
href="https://doi.org/10.1007/s11222-022-10179-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Kock, Lucas and Klein, Nadja and J.Nott, David},
  doi          = {10.1007/s11222-022-10179-y},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: Variational inference and sparsity in high-dimensional deep gaussian mixture models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sticky PDMP samplers for sparse and local inference
problems. <em>SAC</em>, <em>33</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s11222-022-10180-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a new class of efficient Monte Carlo methods based on continuous-time piecewise deterministic Markov processes (PDMPs) suitable for inference in high dimensional sparse models, i.e. models for which there is prior knowledge that many coordinates are likely to be exactly 0. This is achieved with the fairly simple idea of endowing existing PDMP samplers with “sticky” coordinate axes, coordinate planes etc. Upon hitting those subspaces, an event is triggered during which the process sticks to the subspace, this way spending some time in a sub-model. This results in non-reversible jumps between different (sub-)models. While we show that PDMP samplers in general can be made sticky, we mainly focus on the Zig-Zag sampler. Compared to the Gibbs sampler for variable selection, we heuristically derive favourable dependence of the Sticky Zig-Zag sampler on dimension and data size. The computational efficiency of the Sticky Zig-Zag sampler is further established through numerical experiments where both the sample size and the dimension of the parameter space are large.},
  archive      = {J_SAC},
  author       = {Bierkens, Joris and Grazzi, Sebastiano and Meulen, Frank van der and Schauer, Moritz},
  doi          = {10.1007/s11222-022-10180-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Sticky PDMP samplers for sparse and local inference problems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LASSO for streaming data with adaptative filtering.
<em>SAC</em>, <em>33</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11222-022-10181-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data is ubiquitous in modern machine learning, and so the development of scalable algorithms to analyze this sort of information is a topic of current interest. On the other hand, the problem of $$l_1$$ -penalized least-square regression, commonly referred to as LASSO, is a quite popular data mining technique, which is commonly used for feature selection. In this work, we develop a homotopy-based solver for LASSO, on a streaming data context, that massively speeds up its convergence by extracting the most information out of the solution prior receiving the latest batch of data. Since these batches may show a non-stationary behavior, our solver also includes an adaptive filter that improves the predictability of our method in this scenario. Besides different theoretical properties, we additionally compare empirically our solver to the state-of-the-art: LARS, coordinate descent and Garrigues and Ghaoui’s data streaming homotopy. The obtained results show our approach to massively reduce the computational time require to convergence for the previous approaches, reducing up to 3, 4 and 5 orders of magnitude of running time with respect to LARS, coordinate descent and Garrigues and Ghaoui’s homotopy, respectively.},
  archive      = {J_SAC},
  author       = {Capó, Marco and Pérez, Aritz and Lozano, José A.},
  doi          = {10.1007/s11222-022-10181-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {LASSO for streaming data with adaptative filtering},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic cluster embedding. <em>SAC</em>, <em>33</em>(1),
1–14. (<a href="https://doi.org/10.1007/s11222-022-10186-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbor embedding (NE) aims to preserve pairwise similarities between data items and has been shown to yield an effective principle for data visualization. However, even the best existing NE methods such as stochastic neighbor embedding (SNE) may leave large-scale patterns hidden, for example clusters, despite strong signals being present in the data. To address this, we propose a new cluster visualization method based on the Neighbor Embedding principle. We first present a family of Neighbor Embedding methods that generalizes SNE by using non-normalized Kullback–Leibler divergence with a scale parameter. In this family, much better cluster visualizations often appear with a parameter value different from the one corresponding to SNE. We also develop an efficient software that employs asynchronous stochastic block coordinate descent to optimize the new family of objective functions. Our experimental results demonstrate that the method consistently and substantially improves the visualization of data clusters compared with the state-of-the-art NE approaches. The code of our method is publicly available at https://github.com/rozyangno/sce .},
  archive      = {J_SAC},
  author       = {Yang, Zhirong and Chen, Yuwei and Sedov, Denis and Kaski, Samuel and Corander, Jukka},
  doi          = {10.1007/s11222-022-10186-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic cluster embedding},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variance reduction for metropolis–hastings samplers.
<em>SAC</em>, <em>33</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10183-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general framework that constructs estimators with reduced variance for random walk Metropolis and Metropolis-adjusted Langevin algorithms. The resulting estimators require negligible computational cost and are derived in a post-process manner utilising all proposal values of the Metropolis algorithms. Variance reduction is achieved by producing control variates through the approximate solution of the Poisson equation associated with the target density of the Markov chain. The proposed method is based on approximating the target density with a Gaussian and then utilising accurate solutions of the Poisson equation for the Gaussian case. This leads to an estimator that uses two key elements: (1) a control variate from the Poisson equation that contains an intractable expectation under the proposal distribution, (2) a second control variate to reduce the variance of a Monte Carlo estimate of this latter intractable expectation. Simulated data examples are used to illustrate the impressive variance reduction achieved in the Gaussian target case and the corresponding effect when target Gaussianity assumption is violated. Real data examples on Bayesian logistic regression and stochastic volatility models verify that considerable variance reduction is achieved with negligible extra computational cost.},
  archive      = {J_SAC},
  author       = {Alexopoulos, Angelos and Dellaportas, Petros and Titsias, Michalis K.},
  doi          = {10.1007/s11222-022-10183-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Variance reduction for Metropolis–Hastings samplers},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GParareal: A time-parallel ODE solver using gaussian process
emulation. <em>SAC</em>, <em>33</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10195-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential numerical methods for integrating initial value problems (IVPs) can be prohibitively expensive when high numerical accuracy is required over the entire interval of integration. One remedy is to integrate in a parallel fashion, “predicting” the solution serially using a cheap (coarse) solver and “correcting” these values using an expensive (fine) solver that runs in parallel on a number of temporal subintervals. In this work, we propose a time-parallel algorithm (GParareal) that solves IVPs by modelling the correction term, i.e. the difference between fine and coarse solutions, using a Gaussian process emulator. This approach compares favourably with the classic parareal algorithm and we demonstrate, on a number of IVPs, that GParareal can converge in fewer iterations than parareal, leading to an increase in parallel speed-up. GParareal also manages to locate solutions to certain IVPs where parareal fails and has the additional advantage of being able to use archives of legacy solutions, e.g. solutions from prior runs of the IVP for different initial conditions, to further accelerate convergence of the method — something that existing time-parallel methods do not do.},
  archive      = {J_SAC},
  author       = {Pentland, Kamran and Tamborrino, Massimiliano and Sullivan, T. J. and Buchanan, James and Appel, L. C.},
  doi          = {10.1007/s11222-022-10195-y},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {GParareal: A time-parallel ODE solver using gaussian process emulation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Moments and random number generation for the truncated
elliptical family of distributions. <em>SAC</em>, <em>33</em>(1), 1–20.
(<a href="https://doi.org/10.1007/s11222-022-10200-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an algorithm to generate random numbers from any member of the truncated multivariate elliptical family of distributions with a strictly decreasing density generating function. Based on the ideas of Neal (Ann stat 31(3):705–767, 2003) and Ho et al. (J Stat Plan Inference 142(1):25–40, 2012), we construct an efficient sampling method by means of a slice sampling algorithm with Gibbs sampler steps. We also provide a faster approach to approximate the first and the second moment for the truncated multivariate elliptical distributions where Monte Carlo integration is used for the truncated partition and explicit expressions for the non-truncated part (Galarza et al., in J Multivar Anal 189(104):944, 2022). Examples and an application to environmental spatial data illustrate its usefulness. Methods are available for free in the new R library relliptical.},
  archive      = {J_SAC},
  author       = {Valeriano, Katherine A. L. and Galarza, Christian E. and Matos, Larissa A.},
  doi          = {10.1007/s11222-022-10200-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Moments and random number generation for the truncated elliptical family of distributions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Truncated poisson–dirichlet approximation for dirichlet
process hierarchical models. <em>SAC</em>, <em>33</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10201-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dirichlet process was introduced by Ferguson in 1973 to use with Bayesian nonparametric inference problems. A lot of work has been done based on the Dirichlet process, making it the most fundamental prior in Bayesian nonparametric statistics. Since the construction of Dirichlet process involves an infinite number of random variables, simulation-based methods are hard to implement, and various finite approximations for the Dirichlet process have been proposed to solve this problem. In this paper, we construct a new random probability measure called the truncated Poisson–Dirichlet process. It sorts the components of a Dirichlet process in descending order according to their random weights, then makes a truncation to obtain a finite approximation for the distribution of the Dirichlet process. Since the approximation is based on a decreasing sequence of random weights, it has a lower truncation error comparing to the existing methods using stick-breaking process. Then we develop a blocked Gibbs sampler based on Hamiltonian Monte Carlo method to explore the posterior of the truncated Poisson–Dirichlet process. This method is illustrated by the normal mean mixture model and Caron–Fox network model. Numerical implementations are provided to demonstrate the effectiveness and performance of our algorithm.},
  archive      = {J_SAC},
  author       = {Zhang, Junyi and Dassios, Angelos},
  doi          = {10.1007/s11222-022-10201-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Truncated Poisson–Dirichlet approximation for dirichlet process hierarchical models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Moment-based density estimation of confidential micro-data:
A computational statistics approach. <em>SAC</em>, <em>33</em>(1), 1–20.
(<a href="https://doi.org/10.1007/s11222-022-10203-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing access to synthetic micro-data in place of confidential data to protect the privacy of participants is common practice. For the synthetic data to be useful for analysis, it is necessary that the density function of the synthetic data closely approximate the confidential data. Hence, accurately estimating the density function based on sample micro-data is important. Existing kernel-based, copula-based, and machine learning methods of joint density estimation may not be viable. Applying the multivariate moments’ problem to sample-based density estimation has long been considered impractical due to the computational complexity and intractability of optimal parameter selection of the density estimate when the true joint density function is unknown. This paper introduces a generalised form of the sample moment-based density estimate, which can be used to estimate joint density functions when only the information of empirical moments is available. We demonstrate optimal parametrisation of the moment-based density estimate based solely on sample data by employing a computational strategy for parameter selection. We compare the performance of the moment-based estimate to that of existing non-parametric and parametric density estimation methods. The results show that using empirical moments can provide a reasonable, robust non-parametric approximation of a joint density function that is comparable to existing non-parametric methods. We provide an example of synthetic data generation from the moment-based density estimate and show that the resulting synthetic data provides a reasonable disclosure-protected alternative for public release.},
  archive      = {J_SAC},
  author       = {Wakefield, Bradley and Lin, Yan-Xia and Sarathy, Rathin and Muralidhar, Krishnamurty},
  doi          = {10.1007/s11222-022-10203-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Moment-based density estimation of confidential micro-data: A computational statistics approach},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of greedy algorithms for minimising a
maximum mean discrepancy. <em>SAC</em>, <em>33</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11222-022-10184-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the performance of several iterative algorithms for the quantisation of a probability measure $$\mu $$ , based on the minimisation of a Maximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy MMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the finite-sample-size approximation error, measured by the MMD, decreases as 1/n for SBQ and also for kernel herding and greedy MMD minimisation when using a suitable step-size sequence. The upper bound on the approximation error is slightly better for SBQ, but the other methods are significantly faster, with a computational cost that increases only linearly with the number of points selected. This is illustrated by two numerical examples, with the target measure $$\mu $$ being uniform (a space-filling design application) and with $$\mu $$ a Gaussian mixture. They suggest that the bounds derived in the paper are overly pessimistic, in particular for SBQ. The sources of this pessimism are identified but seem difficult to counter.},
  archive      = {J_SAC},
  author       = {Pronzato, Luc},
  doi          = {10.1007/s11222-022-10184-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Performance analysis of greedy algorithms for minimising a maximum mean discrepancy},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-free global likelihood subsampling for massive data.
<em>SAC</em>, <em>33</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10185-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing studies for subsampling heavily depend on a specified model. If the assumed model is not correct, the performance of the subsample may be poor. This paper focuses on a model-free subsampling method, called global likelihood subsampling, such that the subsample is robust to different model choices. It leverages the idea of the global likelihood sampler, which is an effective and robust sampling method from a given continuous distribution. Furthermore, we accelerate the algorithm for large-scale datasets and extend it to deal with high-dimensional data with relatively low computational complexity. Simulations and real data studies are conducted to apply the proposed method to regression and classification problems. It illustrates that this method is robust against different modeling methods and has promising performance compared with some existing model-free subsampling methods for data compression.},
  archive      = {J_SAC},
  author       = {Yi, Si-Yu and Zhou, Yong-Dao},
  doi          = {10.1007/s11222-022-10185-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Model-free global likelihood subsampling for massive data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct sampling with a step function. <em>SAC</em>,
<em>33</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10188-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The direct sampling method proposed by Walker et al. (JCGS 2011) can generate draws from weighted distributions possibly having intractable normalizing constants. The method may be of interest as a tool in situations which require drawing from an unfamiliar distribution. However, the original algorithm can have difficulty producing draws in some situations. The present work restricts attention to a univariate setting where the weight function and base distribution of the weighted target density meet certain criteria. Here, a variant of the direct sampler is proposed which uses a step function to approximate the density of a particular augmented random variable on which the method is based. Knots for the step function can be placed strategically to ensure the approximation is close to the underlying density. Variates may then be generated reliably while largely avoiding the need for manual tuning or rejections. A rejection sampler based on the step function allows exact draws to be generated from the target with lower rejection probability in exchange for increased computation. Several applications of the proposed sampler illustrate the method: generating draws from the Conway-Maxwell Poisson distribution, a Gibbs sampler which draws the dependence parameter in a random effects model with conditional autoregression structure, and a Gibbs sampler which draws the degrees-of-freedom parameter in a regression with t-distributed errors.},
  archive      = {J_SAC},
  author       = {Raim, Andrew M.},
  doi          = {10.1007/s11222-022-10188-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Direct sampling with a step function},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and universal estimation of latent variable models
using extended variational approximations. <em>SAC</em>, <em>33</em>(1),
1–16. (<a href="https://doi.org/10.1007/s11222-022-10189-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear latent variable models (GLLVMs) are a class of methods for analyzing multi-response data which has gained considerable popularity in recent years, e.g., in the analysis of multivariate abundance data in ecology. One of the main features of GLLVMs is their capacity to handle a variety of responses types, such as (overdispersed) counts, binomial and (semi-)continuous responses, and proportions data. On the other hand, the inclusion of unobserved latent variables poses a major computational challenge, as the resulting marginal likelihood function involves an intractable integral for non-normally distributed responses. This has spurred research into a number of approximation methods to overcome this integral, with a recent and particularly computationally scalable one being that of variational approximations (VA). However, research into the use of VA for GLLVMs has been hampered by the fact that fully closed-form variational lower bounds have only been obtained for certain combinations of response distributions and link functions. In this article, we propose an extended variational approximations (EVA) approach which widens the set of VA-applicable GLLVMs dramatically. EVA draws inspiration from the underlying idea behind the Laplace approximation: by replacing the complete-data likelihood function with its second order Taylor approximation about the mean of the variational distribution, we can obtain a fully closed-form approximation to the marginal likelihood of the GLLVM for any response type and link function. Through simulation studies and an application to a species community of testate amoebae, we demonstrate how EVA results in a “universal” approach to fitting GLLVMs, which remains competitive in terms of estimation and inferential performance relative to both standard VA (where any intractable integrals are either overcome through reparametrization or quadrature) and a Laplace approximation approach, while being computationally more scalable than both methods in practice.},
  archive      = {J_SAC},
  author       = {Korhonen, Pekka and Hui, Francis K. C. and Niku, Jenni and Taskinen, Sara},
  doi          = {10.1007/s11222-022-10189-w},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Fast and universal estimation of latent variable models using extended variational approximations},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallelized integrated nested laplace approximations for
fast bayesian inference. <em>SAC</em>, <em>33</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10192-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing demand for performing larger-scale Bayesian inference tasks, arising from greater data availability and higher-dimensional model parameter spaces. In this work we present parallelization strategies for the methodology of integrated nested Laplace approximations (INLA), a popular framework for performing approximate Bayesian inference on the class of Latent Gaussian models. Our approach makes use of nested thread-level parallelism, a parallel line search procedure using robust regression in INLA’s optimization phase and the state-of-the-art sparse linear solver PARDISO. We leverage mutually independent function evaluations in the algorithm as well as advanced sparse linear algebra techniques. This way we can flexibly utilize the power of today’s multi-core architectures. We demonstrate the performance of our new parallelization scheme on a number of different real-world applications. The introduction of parallelism leads to speedups of a factor 10 and more for all larger models. Our work is already integrated in the current version of the open-source R-INLA package, making its improved performance conveniently available to all users.},
  archive      = {J_SAC},
  author       = {Gaedke-Merzhäuser, Lisa and van Niekerk, Janet and Schenk, Olaf and Rue, Håvard},
  doi          = {10.1007/s11222-022-10192-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Parallelized integrated nested laplace approximations for fast bayesian inference},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting renewal states in chains of variable length via
intrinsic bayes factors. <em>SAC</em>, <em>33</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10191-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chains with variable length are useful parsimonious stochastic models able to generate most stationary sequence of discrete symbols. The idea is to identify the suffixes of the past, called contexts, that are relevant to predict the future symbol. Sometimes a single state is a context, and looking at the past and finding this specific state makes the further past irrelevant. States with such property are called renewal states and they can be used to split the chain into independent and identically distributed blocks. In order to identify renewal states for chains with variable length, we propose the use of Intrinsic Bayes Factor to evaluate the hypothesis that some particular state is a renewal state. In this case, the difficulty lies in integrating the marginal posterior distribution for the random context trees for general prior distribution on the space of context trees, with Dirichlet prior for the transition probabilities, and Monte Carlo methods are applied. To show the strength of our method, we analyzed artificial datasets generated from different models and one example coming from the field of Linguistics.},
  archive      = {J_SAC},
  author       = {Freguglia, Victor and Garcia, Nancy L.},
  doi          = {10.1007/s11222-022-10191-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Detecting renewal states in chains of variable length via intrinsic bayes factors},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection using conditional AIC for linear mixed
models with data-driven transformations. <em>SAC</em>, <em>33</em>(1),
1–17. (<a href="https://doi.org/10.1007/s11222-022-10198-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data analysts use linear mixed models, they usually encounter two practical problems: (a) the true model is unknown and (b) the Gaussian assumptions of the errors do not hold. While these problems commonly appear together, researchers tend to treat them individually by (a) finding an optimal model based on the conditional Akaike information criterion (cAIC) and (b) applying transformations on the dependent variable. However, the optimal model depends on the transformation and vice versa. In this paper, we aim to solve both problems simultaneously. In particular, we propose an adjusted cAIC by using the Jacobian of the particular transformation such that various model candidates with differently transformed data can be compared. From a computational perspective, we propose a step-wise selection approach based on the introduced adjusted cAIC. Model-based simulations are used to compare the proposed selection approach to alternative approaches. Finally, the introduced approach is applied to Mexican data to estimate poverty and inequality indicators for 81 municipalities.},
  archive      = {J_SAC},
  author       = {Lee, Yeonjoo and Rojas-Perilla, Natalia and Runge, Marina and Schmid, Timo},
  doi          = {10.1007/s11222-022-10198-9},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Variable selection using conditional AIC for linear mixed models with data-driven transformations},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modularized bayesian analyses and cutting feedback in
likelihood-free inference. <em>SAC</em>, <em>33</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11222-023-10207-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been much recent interest in modifying Bayesian inference for misspecified models so that it is useful for specific purposes. One popular modified Bayesian inference method is “cutting feedback” which can be used when the model consists of a number of coupled modules, with only some of the modules being misspecified. Cutting feedback methods represent the full posterior distribution in terms of conditional and sequential components, and then modify some terms in such a representation based on the modular structure for specification or computation of a modified posterior distribution. The main goal of this is to avoid contamination of inferences for parameters of interest by misspecified modules. Computation for cut posterior distributions is challenging, and here we consider cutting feedback for likelihood-free inference based on Gaussian mixture approximations to the joint distribution of parameters and data summary statistics. We exploit the fact that marginal and conditional distributions of a Gaussian mixture are Gaussian mixtures to give explicit approximations to marginal or conditional posterior distributions so that we can easily approximate cut posterior analyses. The mixture approach allows repeated approximation of posterior distributions for different data based on a single mixture fit. This is important for model checks which aid in the decision of whether to “cut”. A semi-modular approach to likelihood-free inference where feedback is partially cut is also developed. The benefits of the method are illustrated on two challenging examples, a collective cell spreading model and a continuous time model for asset returns with jumps.},
  archive      = {J_SAC},
  author       = {Chakraborty, Atlanta and Nott, David J. and Drovandi, Christopher C. and Frazier, David T. and Sisson, Scott A.},
  doi          = {10.1007/s11222-023-10207-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Modularized bayesian analyses and cutting feedback in likelihood-free inference},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian parameter inference for partially observed
stochastic differential equations driven by fractional brownian motion.
<em>SAC</em>, <em>33</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11222-022-10193-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider Bayesian parameter inference for partially observed fractional Brownian motion models. The approach we follow is to time-discretize the hidden process and then to design Markov chain Monte Carlo (MCMC) algorithms to sample from the posterior density on the parameters given data. We rely on a novel representation of the time discretization, which seeks to sample from an approximation of the posterior and then corrects via importance sampling; the approximation reduces the time (in terms of total observation time T) by $$\mathcal {O}(T)$$ . This method is extended by using a multilevel MCMC method which can reduce the computational cost to achieve a given mean square error versus using a single time discretization. Our methods are illustrated on simulated and real data.},
  archive      = {J_SAC},
  author       = {Maama, Mohamed and Jasra, Ajay and Ombao, Hernando},
  doi          = {10.1007/s11222-022-10193-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian parameter inference for partially observed stochastic differential equations driven by fractional brownian motion},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational inference with vine copulas: An efficient
approach for bayesian computer model calibration. <em>SAC</em>,
<em>33</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11222-022-10194-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements of computer architectures, the use of computational models proliferates to solve complex problems in many scientific applications such as nuclear physics and climate research. However, the potential of such models is often hindered because they tend to be computationally expensive and consequently ill-fitting for uncertainty quantification. Furthermore, they are usually not calibrated with real-time observations. We develop a computationally efficient algorithm based on variational Bayes inference (VBI) for calibration of computer models with Gaussian processes. Unfortunately, the standard fast-to-compute gradient estimates based on subsampling are biased under the calibration framework due to the conditionally dependent data which diminishes the efficiency of VBI. In this work, we adopt a pairwise decomposition of the data likelihood using vine copulas that separate the information on dependence structure in data from their marginal distributions and leads to computationally efficient gradient estimates that are unbiased and thus scalable calibration. We provide an empirical evidence for the computational scalability of our methodology together with average case analysis and describe all the necessary details for an efficient implementation of the proposed algorithm. We also demonstrate the opportunities given by our method for practitioners on a real data example through calibration of the Liquid Drop Model of nuclear binding energies.},
  archive      = {J_SAC},
  author       = {Kejzlar, Vojtech and Maiti, Tapabrata},
  doi          = {10.1007/s11222-022-10194-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Variational inference with vine copulas: An efficient approach for bayesian computer model calibration},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropic herding. <em>SAC</em>, <em>33</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s11222-022-10199-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herding is a deterministic algorithm used to generate data points regarded as random samples satisfying input moment conditions. This algorithm is based on a high-dimensional dynamical system and rooted in the maximum entropy principle of statistical inference. We propose an extension, entropic herding, which generates a sequence of distributions instead of points. We derived entropic herding from an optimization problem obtained using the maximum entropy principle. Using the proposed entropic herding algorithm as a framework, we discussed a closer connection between the herding and maximum entropy principle. Specifically, we interpreted the original herding algorithm as a tractable version of the entropic herding, the ideal output distribution of which is mathematically represented. We further discussed how the complex behavior of the herding algorithm contributes to optimization. We argued that the proposed entropic herding algorithm extends the herding to probabilistic modeling. In contrast to the original herding, the entropic herding can generate a smooth distribution such that both efficient probability density calculation and sample generation become possible. To demonstrate the viability of these arguments in this study, numerical experiments were conducted, including a comparison with other conventional methods, on both synthetic and real data.},
  archive      = {J_SAC},
  author       = {Yamashita, Hiroshi and Suzuki, Hideyuki and Aihara, Kazuyuki},
  doi          = {10.1007/s11222-022-10199-8},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Entropic herding},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
