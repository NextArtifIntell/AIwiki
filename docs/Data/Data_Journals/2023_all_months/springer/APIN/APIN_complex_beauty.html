<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin---1190">APIN - 1190</h2>
<ul>
<li><details>
<summary>
(2023). A graph convolution-based heterogeneous fusion network for
multimodal sentiment analysis. <em>APIN</em>, <em>53</em>(24),
30455–30468. (<a
href="https://doi.org/10.1007/s10489-023-05151-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis leverages various modalities, including text, audio, and video, to determine human sentiment tendencies, which holds significance in fields such as intention understanding and opinion analysis. However, there are two critical challenges in multimodal sentiment analysis: one is how to effectively extract and integrate information from various modalities, which is important for reducing the heterogeneity gap among modalities; the other is how to overcome the problem of information forgetting while modelling long sequences, which leads to significant information loss and adversely affect the fusion performance of modalities. Based on the above issues, this paper proposes a multimodal heterogeneity fusion network based on graph convolutional neural networks (HFNGC). A shared convolutional aggregation mechanism is used to overcome the semantic gap among modalities and reduce the noise effect caused by modality heterogeneity. In addition, the model applies Dynamic Routing to convert modality features into graph structures. By learning semantic information in the graph representation space, our model can improve the capability of remote-dependent learning. Furthermore, the model integrates complementary information among modalities and explores the intra- and inter-modal interactions during the modality fusion stage. To validate the effectiveness of our model, we conduct experiments on two benchmark datasets. The experimental results demonstrate that our method outperforms the existing methods, exhibiting strong generalisation capability and high competitiveness.},
  archive      = {J_APIN},
  author       = {Zhao, Tong and Peng, Junjie and Huang, Yansong and Wang, Lan and Zhang, Huiran and Cai, Zesu},
  doi          = {10.1007/s10489-023-05151-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30455-30468},
  shortjournal = {Appl. Intell.},
  title        = {A graph convolution-based heterogeneous fusion network for multimodal sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving action quality assessment with across-staged
temporal reasoning on imbalanced data. <em>APIN</em>, <em>53</em>(24),
30443–30454. (<a
href="https://doi.org/10.1007/s10489-023-05166-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action quality assessment is a significant research domain in computer vision, aimed at evaluating the accuracy of human movement and providing feedback and guidance for training and rehabilitation. However, the uneven nature of the data, which has a significant impact on the labels with less samples, is not taken into consideration by the generally used approaches in this field. To address this issue, we propose using kernel density estimation (KDE) to recalculate the label density and weight the loss function by the reciprocal of the square root of each label density. Additionally, we divide the entire motion into three sub-stages, including the takeoff, aerial movement, and entry for diving, and connect the three stages using an across-staged temporal reasoning module (ASTRM). Our approach achieves a performance of 0.9222 Spearman correlation coefficient ( $$\rho $$ ) and 0.3304 ( $$\times $$ 100) Relative $$\ell _2$$ -distance ( $$\mathrm R$$ - $$\ell _2$$ ) on the FineDiving dataset, demonstrating competitiveness compared to other methods. Furthermore, numerous comprehensive ablation experiments validate the effectiveness of the methods and modules we adopted.},
  archive      = {J_APIN},
  author       = {Lian, Pu-Xiang and Shao, Zhi-Gang},
  doi          = {10.1007/s10489-023-05166-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30443-30454},
  shortjournal = {Appl. Intell.},
  title        = {Improving action quality assessment with across-staged temporal reasoning on imbalanced data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Dynamic interactive learning network for audio-visual event
localization. <em>APIN</em>, <em>53</em>(24), 30431–30442. (<a
href="https://doi.org/10.1007/s10489-023-05146-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual event (AVE) localization aims to detect whether an event exists in each video segment and predict its category. Only when the event is audible and visible can it be recognized as an AVE. However, sometimes the information from auditory and visual modalities is asymmetrical in a video sequence, leading to incorrect predictions. To address this challenge, we introduce a dynamic interactive learning network designed to dynamically explore the intra- and inter-modal relationships depending on the other modality for better AVE localization. Specifically, our approach involves a dynamic fusion attention of intra- and inter-modalities module, enabling the auditory and visual modalities to focus more on regions deemed informative by the other modality while focusing less on regions that the other modality considers noise. In addition, we introduce an audio-visual difference loss to reduce the distance between auditory and visual representations. Our proposed method has been demonstrated to have superior performance by extensive experimental results on the AVE dataset. The source code will be available at https://github.com/hanliang/DILN .},
  archive      = {J_APIN},
  author       = {Chen, Jincai and Liang, Han and Wang, Ruili and Zeng, Jiangfeng and Lu, Ping},
  doi          = {10.1007/s10489-023-05146-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30431-30442},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic interactive learning network for audio-visual event localization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphs get personal: Learning representation with contextual
pretraining for collaborative filtering. <em>APIN</em>, <em>53</em>(24),
30416–30430. (<a
href="https://doi.org/10.1007/s10489-023-05144-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interactions of users and items in recommender systems can be naturally modeled as user-item bipartite graphs. The iterative propagation of graph neural networks (GNN) can explicitly exploit high-order connectivity from those user-item interactions. Apart from these advantages, there are two limitations in GNN-based recommendation systems that might lead to performance degradation: 1) Existing GNN methods only depend on the graph topology but ignore the insightful relationship between the connected nodes. The edge representation in GNNs cannot effectively express the personalized information of users and items, and can only represent the structural connection information of the graph. 2) The representations of nodes and edges were initialized randomly, these initial representations participate in subsequent node propagation and updating computations in the graph neural network. It directly affects the final representation of the user and item nodes in the network and result in bad recommendation performence. To address these issues, this study proposes a graph attention network with contextual pretraining (GAT-CP) for content-based collaborative filtering. It explicitly exploits the user-item graph structure twofold. First, an contextual personalized sentiment analysis task was applied by fine-tuning the BERT model to initialize the representations of nodes and edges by investigating the user preference for products based on the reviews of the users. Second, the obtained edge representations were used as the propagation constraints to assign different weights to the edges in GAT. Comparative results show the significant performance gains of GAT-CP and the necessity of node and edge initialization with contextual tasks. The code for this paper is available at: https://github.com/Yellow4Submarine7/GAT_AP},
  archive      = {J_APIN},
  author       = {Shen, Tiesunlong and Zhang, You and Wang, Jin and Zhang, Xuejie},
  doi          = {10.1007/s10489-023-05144-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30416-30430},
  shortjournal = {Appl. Intell.},
  title        = {Graphs get personal: Learning representation with contextual pretraining for collaborative filtering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Emotion-cause pair extraction with bidirectional
multi-label sequence tagging. <em>APIN</em>, <em>53</em>(24),
30400–30415. (<a
href="https://doi.org/10.1007/s10489-023-05140-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion-cause pair extraction (ECPE) is a challenging natural language understanding task that aims to extract all potential emotion-cause clause pairs in a document. Existing works have adopted an end-to-end sequence tagging framework to solve this task. However, these approaches struggle to solve the overlapping pairs problem, which means more than one emotion-cause pairs share the same emotion or cause clause. And they suffer from poor generalization ability when the document contains multiple emotion-cause pairs. To address the mentioned issues, we propose a Bidirectional Multi-Label Sequence Tagging (BMST) framework in this paper. Specifically, the proposed method can leverage two sequence tagging modules to achieve bidirectional emotion-cause pair extraction, each of which integrates relevant distance information into the label scheme and employs a multi-label classification strategy. To improve the generalization ability of the model, we propose a joint reasoning mechanism that can exploit the two sequence tagging probabilities to interactively decode emotion-cause pairs. Besides, we design a distance-aware graph attention network to model the relationship between clauses of different distances. Experimental results show that BMST achieves state-of-the-art performance, demonstrating the effectiveness of our model.},
  archive      = {J_APIN},
  author       = {Liu, Jintao and Zhang, Zequn and Guo, Zhi and Jin, Li and Li, Xiaoyu and Wei, Kaiwen and Sun, Xian},
  doi          = {10.1007/s10489-023-05140-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30400-30415},
  shortjournal = {Appl. Intell.},
  title        = {Emotion-cause pair extraction with bidirectional multi-label sequence tagging},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransETA: Transformer networks for estimated time of arrival
with local congestion representation. <em>APIN</em>, <em>53</em>(24),
30384–30399. (<a
href="https://doi.org/10.1007/s10489-023-05139-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimated time of arrival (ETA) is an estimate of the vehicle travel time from the origin to destination in the roadworks. From the perspective of travel planning or resource allocation, accurate ETA is significantly important. In recent years, deep learning-based methods represented by recurrent neural networks has been widely used in travel time prediction tasks, but such methods cannot effectively learn data association at different moments. At the same time, the existing methods do not effectively leverage local traffic information. Targeting these challenges, this paper proposes a new model TransETA to predict vehicle travel time. The model includes three modules: the input feature transformation module uses graph convolutional network (GCN) to extract the local congestion feature, the deep forest module mainly deals with static trajectory data, and ETA-Transformer module processes the feature extraction of dynamic trajectory data. Finally, we conducted experiments on two large trajectory datasets. The experimental results show that the proposed hybrid deep learning method, TransETA, outperforms the state-of-the-art models. On the Chengdu and Porto datasets, our proposed method shows an improvement of 6s and 9s in mean absolute error compared to the current best performing method, respectively. Also the average absolute percentage error is reduced by 2.34% and 3.64% respectively. The effectiveness of each module was approved through ablation experiments. Specifically, local congestion information representation can effectively improve the accuracy of the prediction. ETA-Transformer module is more effective in extracting spatio-temporal feature correlation than the LSTM-based method.},
  archive      = {J_APIN},
  author       = {Lin, Shu and Xu, Yanyan and Zhao, Shengjian and Wang, Yibing and Xu, Jungang},
  doi          = {10.1007/s10489-023-05139-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30384-30399},
  shortjournal = {Appl. Intell.},
  title        = {TransETA: Transformer networks for estimated time of arrival with local congestion representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revision of prioritized <span class="math display">ℰℒ</span>
ontologies. <em>APIN</em>, <em>53</em>(24), 30359–30383. (<a
href="https://doi.org/10.1007/s10489-023-05074-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigated the evolution of prioritized $$\varvec{\mathcal {E}}\varvec{\mathcal {L}}$$ ontologies in the presence of new information that can be certain or uncertain. We propose an extension of $$\varvec{\mathcal {E}}\varvec{\mathcal {L}}$$ description logic, named $$\varvec{\mathcal {E}}\varvec{\mathcal {L}}_{\bot }^{+}$$ , within possibility theory to encode such knowledge. This extension provides a natural way to deal with the ordinal scale and represent knowledge in a way that can handle incomplete information and conflicting data. We provided a polynomial algorithm for computing the possibilistic entailment. Then, we defined the evolution process at the semantic and syntactic levels. Interestingly enough, we show that the syntactical algorithm is done in polynomial time.},
  archive      = {J_APIN},
  author       = {Mohamed, Rim and Loukil, Zied and Gargouri, Faiez and Bouraoui, Zied},
  doi          = {10.1007/s10489-023-05074-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30359-30383},
  shortjournal = {Appl. Intell.},
  title        = {Revision of prioritized $$\mathcal {E}\mathcal {L}$$ ontologies},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multi-task semi-supervised medical image
segmentation method based on multi-branch cross pseudo supervision.
<em>APIN</em>, <em>53</em>(24), 30343–30358. (<a
href="https://doi.org/10.1007/s10489-023-05158-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a crucial task in many clinical applications, such as tumor detection and surgical planning. However, the annotation process for medical images is often both time-consuming and expensive, which requires professional knowledge and experience. This study aims to develop a medical image segmentation method based on semi-supervised learning, which can improve the performance of segmentation by effectively using labeled data and unlabeled data. We proposed a novel multi-task semi-supervised method based on multi-branch cross pseudo supervision, called MS2MPS, which can efficiently utilize unlabeled data for semi-supervised medical image segmentation. The proposed method consists of two multi-task backbone networks with multiple output branches, which were used to simultaneously generate segmentation probability maps (SPM) and signed distance maps (SDM) to get more constraints and information. Moreover, a multi-branch cross pseudo supervised (MPS) approach was proposed to promote the high similarity between predictions of the same input image by multiple perturbation networks. Experiments on the public medical image segmentation dataset Automated Cardiac Diagnosis Challenge (ACDC) and Left Atrium (LA) dataset demonstrate that our approach is superior to existing some semi-supervised segmentation methods in terms of Dice similarity coefficient (DSC), 95% Hausdorff distance (HD95), and Jaccard Similarity Coefficient (JSC). When trained with 10% labeled data, compared to the best results in all compared methods, our approach improved the DSC by 2.63% and 1.54% on the ACDC and LA datasets, increased the JSC by 23.15% and 0.79%, and simultaneously reduced the HD95 by 12.57% and 7.77% on the respective datasets. Our proposed semi-supervised medical image segmentation approach is an effective and practical solution for medical image analysis, particularly in scenarios where labeled data is limited or expensive.},
  archive      = {J_APIN},
  author       = {Xiao, Yueyue and Chen, Chunxiao and Fu, Xue and Wang, Liang and Yu, Jie and Zou, Yuan},
  doi          = {10.1007/s10489-023-05158-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30343-30358},
  shortjournal = {Appl. Intell.},
  title        = {A novel multi-task semi-supervised medical image segmentation method based on multi-branch cross pseudo supervision},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A holistic approach for improving milling machine cutting
tool wear prediction. <em>APIN</em>, <em>53</em>(24), 30329–30342. (<a
href="https://doi.org/10.1007/s10489-023-04793-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive maintenance (PdM) has tremendous potential for reducing total operational costs and turnaround time in industrial manufacturing and maintenance services. In recent years, many deep learning based prediction methods have emerged and advanced the state-of-the-art in the PdM area. This paper proposes a holistic tool wear prediction framework that orchestrates convolutional neural networks for multisensor sequence learning, synthetic features to augment small training datasets, and a meticulously designed training regulation strategy. The effectiveness of the proposed framework is evaluated with computer numerical control (CNC) machine milling datasets available in the public domain. The results show that our method, sCNN-Ex, outperforms state-of-the-art tool wear prediction methods. sCNN-Ex reduces the prediction mean absolute percentage error from existing 32% to 21.7% for steel cases in the NASA milling dataset.},
  archive      = {J_APIN},
  author       = {Feng, Yeli},
  doi          = {10.1007/s10489-023-04793-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30329-30342},
  shortjournal = {Appl. Intell.},
  title        = {A holistic approach for improving milling machine cutting tool wear prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypersphere anchor loss for k-nearest neighbors.
<em>APIN</em>, <em>53</em>(24), 30319–30328. (<a
href="https://doi.org/10.1007/s10489-023-05148-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective feature spaces for KNN (K-Nearest Neighbor) classifiers is critical for their performance. Existing KNN loss functions designed to optimize CNNs in $$\mathbb {R}^n$$ feature spaces for specific KNN classifiers greatly boost the performance. However, these loss functions need to compute the pairwise distances within each batch, which requires large computational resource, GPU and memory. This paper aims to exploit lightweight KNN loss functions in order to reduce the computational cost while achieving comparable to or even better performance than existing KNN loss functions. To this end, an anchor loss function is proposed that assigns each category an anchor vector in KNN feature spaces and introduces the distances between training samples and anchor vectors in the NCA (Neighborhood Component Analysis) function. The proposed anchor loss function largely reduces the required computation by existing KNN loss functions. In addition, instead of optimizing CNNs in $$\mathbb {R}^n$$ feature spaces, this paper proposed to optimize them in hypersphere feature spaces for faster convergence and better performance. The proposed anchor loss optimized in the hypersphere feature space is called HAL (Hypersphere Anchor Loss). Experiments on various image classification benchmarks show that HAL reduces the computational cost and achieves better performance: on CIFAR-10 and Fashion-MNIST datasets, compared with existing KNN loss functions, HAL improves the accuracy by over $$1\%$$ , and the computational cost decreases to less than $$10\%$$ .},
  archive      = {J_APIN},
  author       = {Ye, Xiang and He, Zihang and Wang, Heng and Li, Yong},
  doi          = {10.1007/s10489-023-05148-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30319-30328},
  shortjournal = {Appl. Intell.},
  title        = {Hypersphere anchor loss for K-nearest neighbors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning to solve graph metric dimension problem based on
graph contrastive learning. <em>APIN</em>, <em>53</em>(24), 30300–30318.
(<a href="https://doi.org/10.1007/s10489-023-05130-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely used to solve graph and combinatorial optimization problems. However, proper model deployment is critical for training a model and solving all problems. Existing frameworks mainly use reinforcement learning to learn to solve combinatorial optimization problems, in which a partial solution of the problem is regarded as an environmental state and each vertex of the corresponding graph is regarded as an action. As a result, using the sample data in model training effectively is challenging for different graphs. This study proposes a sampling-based, data-driven and distributed independent graph learning framework, based on decoupling graph structure learning and problem solving processes. To some extent, it facilitates industrial applications. Specifically, the framework consists of two independent parts: extracting graph structure and learning to solve the problem. Under this framework, the graph contrastive learning(GCL) is used to finish the graph structure learning process. Then by means of state-value aggregation on all of nodes in graphs, a global reinforcement learning method is established to learn to solve the graph problem, associated with repair policies to get improvement of performance. Experiments on synthetic graph datasets show that the graph contrastive learning is beneficial or has some advantages for training stability and improving the accuracy of solving the graph problem, and that the repair policies are stable for solution search. However, it also demonstrates that the graph neural network is not necessarily needed in the process of learning to solve the graph problem. Moreover, learning to solve MDP still has some challenges, such as decreasing learning performance with increasing edge existence probability of graphs, and it is unknown what kind of reward function is appropriate for solving MDP.},
  archive      = {J_APIN},
  author       = {Wu, Jian and Wang, Li and Yang, Weihua and Zhao, Haixia and Wang, Rui and Cao, Jianji and Wei, Fuhong},
  doi          = {10.1007/s10489-023-05130-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30300-30318},
  shortjournal = {Appl. Intell.},
  title        = {Learning to solve graph metric dimension problem based on graph contrastive learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OBRUN algorithm for the capacity-constrained joint
replenishment and delivery problem with trade credits. <em>APIN</em>,
<em>53</em>(24), 30266–30299. (<a
href="https://doi.org/10.1007/s10489-023-05055-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the joint replenishment and delivery problem taking trade credits the delay in payment permitted by suppliers and capacity-constrained into consideration (CJRD-TC). The CJRD-TC aims to minimize the total system costs by finding a reasonable replenishment and delivery schedule policy. In this paper, an opposite-based RUNge Kutta optimization algorithm (OBRUN) is proposed to compare with other intelligent algorithms, including the differential evolution (DE), RUNge Kutta optimization algorithm (RUN), sine cosine algorithm (SCA), and adaptive differential evolution with optimal external archive (JADE). The proposed algorithm is tested on 14 optimization functions. The values of average, standard deviation, and the best are the lowest for 12 functions, which demonstrates OBRUN is superior to other algorithms. We also provide graphs of search history, 2D views of functions, trajectory curve, average fitness history, and convergence curve of OBRUN, the balance between exploration and exploitation over the course of iterations can be observed. The convergence curves of five algorithms are also depicted, which show OBRUN has a fast convergence rate and accuracy than other algorithms. We conduct numerical experiments, parameter tuning, and sensitivity analysis for each of the small-scale, medium-scale, and large-scale number of items. The OBRUN can always find better solutions with the lowest mean values and the best-found total cost results than other algorithms. Indicators of the average optimality gap and the optimality gap dominate by the value of zero. These computational results have demonstrated OBRUN is a suitable and effective candidate for managers to solve the practical CJRD-TC problems.},
  archive      = {J_APIN},
  author       = {Wang, Lin and Pi, Yingying and Peng, Lu and Wang, Sirui and Zhang, Ziqing and Liu, Rui},
  doi          = {10.1007/s10489-023-05055-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30266-30299},
  shortjournal = {Appl. Intell.},
  title        = {OBRUN algorithm for the capacity-constrained joint replenishment and delivery problem with trade credits},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent team formation and player selection: A
data-driven approach for football coaches. <em>APIN</em>,
<em>53</em>(24), 30250–30265. (<a
href="https://doi.org/10.1007/s10489-023-05150-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a data-driven approach called Intelligent Team Formation and Player Selection (ITFPS), aimed at assisting football coaches in making informed decisions about team formation and player selection. The proposed approach utilizes deep neural networks to evaluate the suitability of each player for different positions on the field. The problem is then formulated as finding the maximum weighted matching in a complete bipartite graph, with the objective of achieving the best possible alignment between team members and the positions designated by the coach. The Hungarian algorithm is employed to facilitate this matching process. Furthermore, the approach allows coaches to select a system of distinct representatives for each position, based on the specific qualities expected from players in a given match. The effectiveness of the approach is demonstrated through tests conducted on four teams from the 2021–2022 English Premier League. The results indicate that ITFPS produces decisions comparable to those made by successful coaches. By optimizing team formations and enabling the utilization of rotating formations, this approach not only enhances team performance but also empowers coaches to make strategic decisions while fully leveraging the potential of their players. ITFPS serves as an intelligent assistant for coaches, complementing their strategic perspectives.},
  archive      = {J_APIN},
  author       = {Nouraie, Mahdi and Eslahchi, Changiz and Baca, Arnold},
  doi          = {10.1007/s10489-023-05150-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30250-30265},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent team formation and player selection: A data-driven approach for football coaches},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From methods to datasets: A detailed study on facial emotion
recognition. <em>APIN</em>, <em>53</em>(24), 30219–30249. (<a
href="https://doi.org/10.1007/s10489-023-05052-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human ideas and sentiments are mirrored in facial expressions. Facial expression recognition (FER) is a crucial type of visual data that can be utilized to deduce a person’s emotional state. It gives the spectator a plethora of social cues, such as the viewer’s focus of attention, emotion, motivation, and intention. It’s said to be a powerful instrument for silent communication. AI-based facial recognition systems can be deployed at different areas like bus stations, railway stations, airports, or stadiums to help security forces identify potential threats. There has been a lot of research done in this area. But, it lacks a detailed review of the literature that highlights and analyses the previous work in FER (including work on compound emotion and micro-expressions), and a comparative analysis of different models applied to available datasets, further identifying aligned future directions. So, this paper includes a comprehensive overview of different models that can be used in the field of FER and a comparative study of the traditional methods based on hand-crafted feature extraction and deep learning methods in terms of their advantages and disadvantages which distinguishes our work from existing review studies.This paper also brings you to an eye on the analysis of different FER systems, the performance of different models on available datasets, evaluation of the classification performance of traditional and deep learning algorithms in the context of facial emotion recognition which reveals a good understanding of the classifier’s characteristics. Along with the proposed models, this study describes the commonly used datasets showing the year-wise performance achieved by state-of-the-art methods which lacks in the existing manuscripts. At last, the authors itemize recognized research gaps and challenges encountered by researchers which can be considered in future research work.},
  archive      = {J_APIN},
  author       = {Nidhi and Verma, Bindu},
  doi          = {10.1007/s10489-023-05052-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30219-30249},
  shortjournal = {Appl. Intell.},
  title        = {From methods to datasets: A detailed study on facial emotion recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pedestrian detection based on channel feature fusion and
enhanced semantic segmentation. <em>APIN</em>, <em>53</em>(24),
30203–30218. (<a
href="https://doi.org/10.1007/s10489-023-04957-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, pedestrian detection is widely applied to autonomous driving and intelligent transportation and robots, etc. But the balance between accuracy and speed is still not reached. In complex background with high pedestrian density and serious occlusion, missing detection or false detection may occur by pedestrian detection models based on center and scale prediction (CSP). An improved pedestrian detection method based on channel feature fusion and enhanced semantic segmentation is presented. A feature fusion module based on squeeze and excitation is proposed in feature extraction. Multi-scale feature maps are fused to obtain faster detection speed and higher detection accuracy. An enhanced semantic segmentation module is presented in detection head to solve missing detection for long-distance pedestrians. CIOU (Complete Intersection Over Union) loss function is used to improve the confidence levels of pedestrians. Experiments on different networks, scales of feature fusion and detection methods are carried out to verify the performance of proposed approach. The experimental results show that the proposed model can detect pedestrians with high accuracy in occluded, dense and long-distance scenes. The detection speed can be accelerated while keeping low missed detection rate and less computational cost. It is shown that the approach can achieve high accuracy and robustness especially in complex background.},
  archive      = {J_APIN},
  author       = {Zong, Xinlu and Xu, Yuan and Ye, Zhiwei and Chen, Zhen},
  doi          = {10.1007/s10489-023-04957-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30203-30218},
  shortjournal = {Appl. Intell.},
  title        = {Pedestrian detection based on channel feature fusion and enhanced semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEMD-ConvLSTM: A model for short-term prediction of
two-dimensional wind speed in the south china sea. <em>APIN</em>,
<em>53</em>(24), 30186–30202. (<a
href="https://doi.org/10.1007/s10489-023-05042-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of sea surface wind speed is crucial for marine activities such as marine search and rescue, marine shipping, and marine fishing. Because of the gustiness of sea surface winds, the wind speed data have strong non-stationarity and non-linearity, and it is still challenging to predict sea surface winds accurately and stably in a short time. Most previous studies have only considered the problem of non-smoothness of wind speed in single-point wind speed prediction, and this study extends the method to solve the non-smoothness of data in two dimensions. We proposed a hybrid model based on ensemble empirical mode decomposition (EEMD) and Convolutional long short-term memory network (ConvLSTM). Specifically, this study employs the Pearson correlation coefficient (PCC) to select the most periodic and most predictable subsequences of the EEMD signal decomposition, then fuse them into the spatio-temporal prediction of ConvLSTM. The method eliminates the influence of noise in the two-dimensional wind speed prediction and grasps the wind speed variation pattern more accurately. The proposed model has the best prediction effect based on the experimental findings, and this advantage becomes increasingly evident as time increases. This shows that EEMD signal decomposition is a good way to solve the short-term prediction problem for two-dimensional wind speed.},
  archive      = {J_APIN},
  author       = {Sun, Handan and Song, Tao and Li, Ying and Yang, Kunlin and Xu, Danya and Meng, Fan},
  doi          = {10.1007/s10489-023-05042-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30186-30202},
  shortjournal = {Appl. Intell.},
  title        = {EEMD-ConvLSTM: A model for short-term prediction of two-dimensional wind speed in the south china sea},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic feature-guided and correlation-aggregated salient
object detection. <em>APIN</em>, <em>53</em>(24), 30169–30185. (<a
href="https://doi.org/10.1007/s10489-023-05141-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current salient object detection (SOD) methods employ an encoder-decoder architecture based on fully convolutional neural networks. However, the subjective nature of the saliency object detection task and the local nature of convolutional processing may result in missing global contextual information. In addition, feature fusion without information filtering may introduce more noise and thus weaken the localization ability of prominent objects. Therefore, we propose a transformer-based semantic feature-guided and correlation-aggregated salient object detection (SFC-SOD) method. Specifically, the method takes a pyramid vision transformer (PVT) as the encoder backbone to extract features and designs a top-level feature guidance (TFG) module in the decoder to explore the correlation between the highest-level features and the low-level features. The low-level features are guided in the channel dimension to enhance the expression of the low-level features. Based on the features obtained from TFG, the adaptive feature fusion (AFF) module is designed to efficiently utilize the essential features of different layers for fusion to obtain salient critical information while reducing redundant features. After feature fusion, the top-down correlation-aggregation (TCA) module is introduced to further enhance and refine the salient features by using the high-level output results to guide the lower-level features to establish global dependencies, thus achieving better saliency results. The results of extensive experiments conducted on six widely used datasets show the superior performance of the proposed SFC-SOD by comparing it with several state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Luo, Jincheng and Li, Yongjun and Li, Bo and Zhang, Xinru and Li, Chaoyue and Chenjin, Zhimin and Zhang, Dongming},
  doi          = {10.1007/s10489-023-05141-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30169-30185},
  shortjournal = {Appl. Intell.},
  title        = {Semantic feature-guided and correlation-aggregated salient object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer network with decoupled spatial–temporal
embedding for traffic flow forecasting. <em>APIN</em>, <em>53</em>(24),
30148–30168. (<a
href="https://doi.org/10.1007/s10489-023-05126-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, there has been significant research on applying Transformer models to time series prediction, yielding promising results. Simultaneously, researchers have begun exploring the utilization of Transformers for traffic prediction in order to mitigate the nonlinear spatial–temporal correlation inherent in traffic data. Some of these studies have attempted to characterize spatial–temporal features by incorporating embedding structures, with the goal of improving performance of the model. However, existing methods have not adequately addressed the issue of spatial–temporal correlation. To address these limitations, we propose the Transformer Network with Decoupled Spatial–Temporal Embedding (DSTET) model for traffic flow prediction. The key aspect of our model is its ability to effectively decouple the spatial and temporal embedding through the implementation of the Decoupled Spatial–Temporal Embedding structure. This structure enhances the characterization of spatial–temporal features, ultimately improving the performance of traffic prediction based on the Transformer model. Through experiments conducted on six real-world traffic datasets, our model consistently outperforms multiple baseline models, demonstrating its capability to address the identified problems. Moreover, we substantiate the efficacy of the suggested components via ablation experiments and furnish a thorough analysis of the attention weight matrix to clarify the functioning of the model.},
  archive      = {J_APIN},
  author       = {Sun, Wei and Cheng, Rongzhang and Jiao, Yingqi and Gao, Junbo and Zheng, Zhedian and Lu, Nan},
  doi          = {10.1007/s10489-023-05126-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30148-30168},
  shortjournal = {Appl. Intell.},
  title        = {Transformer network with decoupled spatial–temporal embedding for traffic flow forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating hand-crafted features into deep learning
models for motor imagery EEG-based classification. <em>APIN</em>,
<em>53</em>(24), 30133–30147. (<a
href="https://doi.org/10.1007/s10489-023-05134-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery (MI) is a mental process that produces two types of event-related potentials called event-related desynchronization (ERD) and event-related synchronization (ERS). We can record ERD and ERS in an electroencephalogram (EEG) and use them to identify a MI execution. However, the classification of MI is a challenging task because ERD and ERS exhibit inter- and intra-subject variability. Recently, researchers have proposed deep learning models to solve this problem. Although they achieve cutting-edge results, the amount of data available for training constrains their learning ability. To address this issue, we propose to incorporate hand-crafted features, which have a strong inductive bias, into deep learning models at different levels of depth, which have a soft inductive bias, without making them lose their ability to discover new features from data. Our approach enables the design of models that benefit from deep learning and traditional machine learning models for MI EEG-based classification. In this manner, it is possible to build compact machine learning models that perform better than pure deep learning models in a small data setting. Results of experiments on the public datasets 2a and 2b of the BCI Competition IV demonstrate that a model built following our proposed strategy achieves state-of-the-art accuracy on MI EEG-based classification.},
  archive      = {J_APIN},
  author       = {Bustios, Paul and Garcia Rosa, João Luís},
  doi          = {10.1007/s10489-023-05134-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30133-30147},
  shortjournal = {Appl. Intell.},
  title        = {Incorporating hand-crafted features into deep learning models for motor imagery EEG-based classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Gene selection in a single cell gene decision space based
on class-consistent technology and fuzzy rough iterative computation
model. <em>APIN</em>, <em>53</em>(24), 30113–30132. (<a
href="https://doi.org/10.1007/s10489-023-05115-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores gene selection in a single cell gene decision space (scgd-space) based on class-consistent technology and fuzzy rough iterative computation model (FRIC-model). Gene expression data (ge-data) exhibit characteristics such as limited sample size, high dimensionality, and noise. Due to their high dimensionality, gene selection must be carried out before clustering and classifying them. The existing gene selection methods based on equivalence relation are not effective for ge-data owing to the strictness of the equality between gene expression values. In order to overcome this weakness, class-consistent technology of replacing equality with approximate equality between gene expression values is first proposed. Then, “the class consistency between gene expression values is fed back to the gene set” is considered with the help of class-consistent technology, and fuzzy symmetric relations on the cell set of a scgd-space are induced. In addition, fuzzy rough approximations in a scgd-space are defined. Next, FRIC-model is given. This model employs the iterative computation strategy to define fuzzy rough approximations and dependency functions. A gene selection algorithm based on this model is designed. Finally, the designed algorithm is testified in several publicly open ge-data sets to estimate its performance. The experimental results show that the designed algorithm is more effective than some existing algorithms.},
  archive      = {J_APIN},
  author       = {Zhang, Jie and Yu, Guangji and Huang, Dan and Wang, Yuxian},
  doi          = {10.1007/s10489-023-05115-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30113-30132},
  shortjournal = {Appl. Intell.},
  title        = {Gene selection in a single cell gene decision space based on class-consistent technology and fuzzy rough iterative computation model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Oriented transformer for infectious disease case
prediction. <em>APIN</em>, <em>53</em>(24), 30097–30112. (<a
href="https://doi.org/10.1007/s10489-023-05101-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of infectious disease cases plays a crucial role in achieving effective infection prevention and control. However, the inherent variability of incubation periods and progression dynamics of infectious diseases pose significant challenges to the accuracy of predicting multiple diseases. Multiple representation fusion (MRF) methods would improve the performance of prediction models, due to their capability to capture diverse temporal dependencies that reflect potential disease transmission patterns. But the traditional fusion approach for infectious disease prediction still faces many challenges, including the requirement of auxiliary data, vulnerability to disease evolution, and lack of intuitive explanation. To address these challenges, this paper proposes an oriented transformer (ORIT) for infectious diseases case prediction. Contrary to traditional MRF structures that integrate representations from multiple data sources, the MRF in the proposed ORIT combines multi-orientation context vectors solely by capturing multi-dimensional temporal relationships within disease case data. Furthermore, this paper considers the heterogeneity of the incubation period in the prediction of different infectious disease cases. Lastly, this paper conducts comprehensive experiments to evaluate the proposed method using two real datasets of infectious diseases, and compares it with 21 well-known prediction methods. The experimental results verify the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Zhijin and Zhang, Pesiong and Huang, Yaohui and Chao, Guoqing and Xie, Xijiong and Fu, Yonggang},
  doi          = {10.1007/s10489-023-05101-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30097-30112},
  shortjournal = {Appl. Intell.},
  title        = {Oriented transformer for infectious disease case prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot intelligent fault diagnosis based on an improved
meta-relation network. <em>APIN</em>, <em>53</em>(24), 30080–30096. (<a
href="https://doi.org/10.1007/s10489-023-05128-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, fault diagnosis methods based on machine learning and deep learning have achieved excellent results in fault diagnosis and are characterized by powerful automatic feature extraction and accurate identification capabilities. In many real-world scenarios, gathering enough samples of each fault type can be time-consuming and difficult. The scarcity of samples may significantly degrade the performance of these learning-based methods, making it extremely challenging to train a robust fault diagnosis classifier. In this paper, a few-shot fault diagnosis method based on the improved meta-relation network (IMRN) model is proposed to overcome the challenge of implementing fault diagnosis with limited data samples. First, a multiscale feature encoder module that utilizes two one-dimensional convolutional neural networks with different kernel sizes is used to automatically extract signal features from the original support dataset and query dataset. Then, a metric meta-learner module is designed to obtain relation scores between support samples and query samples. Finally, the feature vector output by the feature encoder module is input to the metric meta-learner module to determine the category of query samples by comparing the relation scores between the query dataset and support dataset, thus implementing the classification of fault categories. Experiments are conducted on three public datasets (TE, PU and CWRU), and the experimental results show that the proposed method outperforms other benchmark few-shot learning methods in terms of accuracy and exhibits remarkable robustness and adaptability in fault diagnosis.},
  archive      = {J_APIN},
  author       = {Zheng, Xiaoqing and Yue, Changyuan and Wei, Jiang and Xue, Anke and Ge, Ming and Kong, Yaguang},
  doi          = {10.1007/s10489-023-05128-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30080-30096},
  shortjournal = {Appl. Intell.},
  title        = {Few-shot intelligent fault diagnosis based on an improved meta-relation network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual representations with texts domain generalization for
semantic segmentation. <em>APIN</em>, <em>53</em>(24), 30069–30079. (<a
href="https://doi.org/10.1007/s10489-023-05125-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, Domain generalization for semantic segmentation relying on deep neural networks has made little progress. Most of the current methods are mainly divided into domain randomization, standardization, and whitening. We propose a novel approach to achieve domain generalization for semantic segmentation: leveraging cross-modal information to supervise the model training and improve the generalization ability of the network. We align visual features with textual features in a subspace and enhance the contrast between categories. Our method enables the network to learn rich semantic knowledge from text features and clearer category boundaries. Our experiments also prove that our method can effectively improve the generalization ability of the network. We are the first to exploit multi-modal information for domain-generalized semantic segmentation.},
  archive      = {J_APIN},
  author       = {Yue, Wanlin and Zhou, Zhiheng and Cao, Yinglie and Wu, Weikang},
  doi          = {10.1007/s10489-023-05125-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30069-30079},
  shortjournal = {Appl. Intell.},
  title        = {Visual representations with texts domain generalization for semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot object detection with contrastive semantic
association network. <em>APIN</em>, <em>53</em>(24), 30056–30068. (<a
href="https://doi.org/10.1007/s10489-023-05117-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot object detection (ZSD) is dedicated to the task of precisely localizing and identifying unfamiliar objects that have not been encountered before. In this paper, a contrastive semantic association network is proposed to address the knowledge transfer challenge from seen classes to unseen ones in ZSD. It enables efficient information propagation through similarity-based connections, thereby establishing a clearer link between seen and unseen categories. Moreover, a visual-semantic contrastive learning technique is developed to mitigate the node convergence issue caused by the graph structure of the proposed network. By emphasizing the visual and semantic distinctiveness across different categories, the proposed model leverages semantic information and graph structure knowledge to enhance the generalization capability of seen and unseen feature projection. Extensive experiments demonstrate the superior performance of our model compared to other zero-shot object detection methods, showcasing notable improvement in mean average precision (mAP) on the MS-COCO dataset. The code and models are publicly available at: https://github.com/lihh1023/CSA-ZSD/tree/master .},
  archive      = {J_APIN},
  author       = {Li, Haohe and Wang, Chong and Liu, Weijie and Gong, Yilin and Dai, Xinmiao},
  doi          = {10.1007/s10489-023-05117-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30056-30068},
  shortjournal = {Appl. Intell.},
  title        = {Zero-shot object detection with contrastive semantic association network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Select and calibrate the low-confidence: Dual-channel
consistency based graph convolutional networks. <em>APIN</em>,
<em>53</em>(24), 30041–30055. (<a
href="https://doi.org/10.1007/s10489-023-05110-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Graph Convolutional Networks (GCNs) have achieved excellent results in various graph-related tasks, their performance at low label rates is still unsatisfactory. Previous studies in Semi-Supervised Learning (SSL) for graph primarily focused on utilizing network predictions to generate pseudo-labels or instruct message propagation, often resulting in incorrect predictions owing to over-confidence. To address this issue, we propose a novel approach called Dual-Channel Consistency based Graph Convolutional Networks (DCC-GCN) for semi-supervised node classification. The key idea behind DCC-GCN is to leverage the extraction of embeddings from both node features and topological structures by employing GCN encoders in two separate channels. We consider samples with consistent predictions from these two channels as high-confidence samples, while those with differing predictions are labeled as low-confidence samples. DCC-GCN calibrate the feature embeddings of low-confidence samples by aggregating high-confidence samples from their respective neighborhoods. DCC-GCN can significantly improve the classification accuracy of low-confidence samples, thus improving the overall accuracy. Experiments on seven graph datasets demonstrate that DCC-GCN outperforms prior SSL methods, improving node classification accuracy by a considerable margin.},
  archive      = {J_APIN},
  author       = {Shi, Shuhao and Chen, Jian and Qiao, Kai and Yang, Shuai and Wang, Linyuan and Yan, Bin},
  doi          = {10.1007/s10489-023-05110-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30041-30055},
  shortjournal = {Appl. Intell.},
  title        = {Select and calibrate the low-confidence: Dual-channel consistency based graph convolutional networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global relationship memory network for retinal capillary
segmentation on optical coherence tomography angiography images.
<em>APIN</em>, <em>53</em>(24), 30027–30040. (<a
href="https://doi.org/10.1007/s10489-023-05107-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic retinal capillary segmentation is a necessary prerequisite for quantitatively analyzing retinal vessels. In recent years, active research has been using deep learning-based methods in this field. However, deep learning methods inevitably lose spatial information of vessels when downsampling, thereby limiting the segmentation performance for fine vessels. Additionally, existing methods must pay more attention to the dynamic correlations between feature mappings in deep learning frameworks, resulting in inefficient acquisition of multi-scale decoder features. To address these limitations, we propose a Global Relationship Memory Network (GRM-Net) that considers the relationship between frequency domain and decoder hierarchy. Specifically, we first design a frequency relation learning module to preserve fine details of vessels during downsampling. This module decouples encoder features into frequency domain features of different dimensions and employs globally learnable filters to better guide the network’s attention towards vessels of different sizes and shapes. Secondly, we investigate a hierarchical relation selection module that leverages gate mechanisms to dynamically adjust the collaboration between two adjacent decoder blocks, thereby adaptively aggregating multi-scale decoder features to address the issue of underutilized decoding information. Comparative experimental results on two retinal vessel datasets validate the effectiveness of the proposed GRM-Net segmentation method. Compared to other state-of-the-art methods (Unet, CS-Net, DeeplabV3, MiniSeg, and OCTA-Net), this method achieves more remarkable segmentation results, preserving more details in the tiniest retinal capillaries. Code is available at https://github.com/WeiliJiang/Global-Relationship-Memory-Network .},
  archive      = {J_APIN},
  author       = {Jiang, Weili and Jiang, Weijing and An, Lin and Qin, Jia and Chen, Lushi and Ou, Chubin},
  doi          = {10.1007/s10489-023-05107-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30027-30040},
  shortjournal = {Appl. Intell.},
  title        = {Global relationship memory network for retinal capillary segmentation on optical coherence tomography angiography images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DO-SLAM: Research and application of semantic SLAM system
towards dynamic environments based on object detection. <em>APIN</em>,
<em>53</em>(24), 30009–30026. (<a
href="https://doi.org/10.1007/s10489-023-05070-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) is one of the research hotspots in the field of robotics, and it is also a prerequisite for autonomous robot navigation. The localization accuracy and stability of traditional SLAM based on static scene assumption declines due to the interference of dynamic objects. To solve the problem, this paper proposes a semantic SLAM system for dynamic environments based on object detection named DO-SLAM. Firstly, DO-SLAM uses YOLOv5 to identify objects in dynamic environments and obtains the semantic information; Then, combined with the outlier detection mechanism proposed in this paper, the dynamic objects are effectively determined and the feature points on the dynamic objects are eliminated; The combination method can reduce the interference of dynamic objects to SLAM, and improve the stability and localization accuracy of the system. At the same time, a static dense point cloud map is constructed for high-level tasks. Finally, the effectiveness of DO-SLAM is verified on the TUM RGB-D dataset. The results show that the Absolute Trajectory Error (ATE) and Relative Pose Error (RPE) are reduced, indicating that DO-SLAM can reduce the interference of dynamic objects.},
  archive      = {J_APIN},
  author       = {Wei, Yaoguang and Zhou, Bingqian and Duan, Yunhong and Liu, Jincun and An, Dong},
  doi          = {10.1007/s10489-023-05070-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {30009-30026},
  shortjournal = {Appl. Intell.},
  title        = {DO-SLAM: Research and application of semantic SLAM system towards dynamic environments based on object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A personalized paper recommendation method based on
knowledge graph and transformer encoder with a self-attention mechanism.
<em>APIN</em>, <em>53</em>(24), 29991–30008. (<a
href="https://doi.org/10.1007/s10489-023-05108-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paper recommendation with personalized methods helps researchers to track the latest academic trends and master cutting-edge academic trends efficiently. Meanwhile, the methods of previous paper recommendation suffer from three problems: data sparsity of content-based and collaborative filtering methods; Graph-based recommendations do not fully consider the personalized information of authors and their articles; Cold start based on deep learning. To overcome those difficulties, we propose a personalized paper recommendation method based on a knowledge graph and Transformer encoder (KGTE) with a self-attention mechanism. Firstly, we add auxiliary information (article title, publication year, citation times, and abstract) as attributes to the nodes of knowledge graph(KG), which contain author, digital object unique identifier(DOI) and keywords. Secondly, BERT is used to represent the semantic information features of the article and Transformer is introduced to fully integrate the feature context. After that, by using RippleNet, we traverse the knowledge graph, filter the user preference distribution and form a set of pre recommended nodes with multi_hop nodes. Finally, the prediction layer sorts the set and gets a Top_n paper recommendation. In the experiments on the DBLP and Aminer datasets, the precision value of KGTE improved by an average of 2.59% over the existing baseline methods DER and 4.23% improvement in NDCG.},
  archive      = {J_APIN},
  author       = {Gao, Li and Lan, Yu and Yu, Zhen and Zhu, Jian-min},
  doi          = {10.1007/s10489-023-05108-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29991-30008},
  shortjournal = {Appl. Intell.},
  title        = {A personalized paper recommendation method based on knowledge graph and transformer encoder with a self-attention mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). METER: Multi-task efficient transformer for no-reference
image quality assessment. <em>APIN</em>, <em>53</em>(24), 29974–29990.
(<a href="https://doi.org/10.1007/s10489-023-05104-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in computer vision. Current NR-IQA methods based on convolutional neural networks typically employ deeply-stacked convolutions to learn local features pertinent to image quality, neglecting the importance of non-local information and distortion types. As a remedy, we introduce in this paper an end-to-end multi-task efficient transformer (METER) for the NR-IQA task, consisting of a multi-scale semantic feature extraction (MSFE) backbone module, a distortion type identification (DTI) module, and an adaptive quality prediction (AQP) module. METER identifies the distortion type using the DTI module to facilitate extraction of distortion-specific features via the MSFE module. METER scores image quality in an adaptive manner by adjusting the weights and biases of adaptive fully-connected (AFC) layers in the AQP module, increasing generalizability to images captured in different natural environments. Experimental results demonstrate that METER significantly outperforms existing methods for accuracy and efficiency across five public datasets: LIVEC, BID, KonIQ, LIVE, and CSIQ, and exhibits remarkable performance with Pearson’s linear correlation coefficients: 0.923, 0.912, 0.937, 0.978, and 0.982 on respective datasets when compared to human subjective scores. Additionally, METER also attains higher efficiency (-53.9% Params and -87.7% FLOPs) compared to the existing transformer-based methods, making it valuable for real-world applications. METER: Multi-task Efficient Transformer for No-Reference Image Quality Assessment. Pengli Zhu,Siyuan Liu,Yancheng Liu,Pew-Thian Yap},
  archive      = {J_APIN},
  author       = {Zhu, Pengli and Liu, Siyuan and Liu, Yancheng and Yap, Pew-Thian},
  doi          = {10.1007/s10489-023-05104-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29974-29990},
  shortjournal = {Appl. Intell.},
  title        = {METER: Multi-task efficient transformer for no-reference image quality assessment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection under budget constraint in medical
applications: Analysis of penalized empirical risk minimization methods.
<em>APIN</em>, <em>53</em>(24), 29943–29973. (<a
href="https://doi.org/10.1007/s10489-023-05063-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial step when building supervised predictive models. In many medical applications, features are associated with costs. For example, the diagnostic value extracted by a clinical test is associated with its own cost. Costs can also refer to a non-financial aspects, such as a decision between an invasive exploratory surgery and a simple blood test. Traditional feature selection methods, which ignore costs, aim to choose a subset of features that maximize the accuracy of the corresponding model. However, such a model can be impractical as the total cost of making a prediction may exceed the assumed user-specified budget. In cost-constrained methods, it is necessary to take into account both the relevance of the feature and its cost. We focus on embedded feature selection methods based on a very general penalized empirical risk minimization framework that includes various loss functions. The most natural $$\ell _0$$ -type penalty is computationally intractable and therefore we analyze other penalties such as: the cost-sensitive lasso and adaptive lasso, non-convex penalties and the method based on knockoffs. The experiments performed on real medical datasets, including large database MIMIC, indicate that non-convex penalties give promising results, in particular they allow to achieve high accuracy, especially when the assumed budget is low. Our model achieved AUC 0.88 for the MIMIC-II dataset in which we predict the occurrence of liver diseases based on clinical features with a budget equal to (5%) of the cost of all available features, which is significantly better than the AUC for the traditional method. Moreover, the fraction of feature cost wasted for noisy features in our method is usually lower than for cost-sensitive lasso.},
  archive      = {J_APIN},
  author       = {Klonecki, Tomasz and Teisseyre, Paweł},
  doi          = {10.1007/s10489-023-05063-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29943-29973},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection under budget constraint in medical applications: Analysis of penalized empirical risk minimization methods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probability rough set and portfolio optimization integrated
three-way predication decisions approach to stock price. <em>APIN</em>,
<em>53</em>(24), 29918–29942. (<a
href="https://doi.org/10.1007/s10489-023-05085-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the stock market, accurate trend judgment and reasonable asset distribution are effective ways to obtain ideal return. However, the real stock market is affected by the objective economic environment, investors’ expected return and other potential factors, which makes the classical portfolio strategy face more challenges and pressures. How to build a reliable portfolio strategy in an uncertain environment will be a scientific problem worthy of in-depth discussion. To address this issue, this paper combines machine learning with rough set to establish a new rough set theory prediction model, quantitatively dividing the stock data into three categories and targetedly predicting the future trend according to the complexity. Based on the proposed prediction model, a new portfolio strategy is proposed by integrating the mean-variance model. Firstly, for reducing the volatility and noise of the original data of stock price, outlier processing (OP) and wavelet denoising (WD) are utilized. Secondly, for the sake of pertinently forecasting the future trend of different characteristic stock price, a three-way prediction (TWP) decisions approach is constructed based on multiscale permutation entropy (MPE), probabilistic rough set (PRS), variational modal decomposition (VMD) and deep learning. Finally, 20 stocks of Shanghai and Shenzhen stock exchanges are taken as research samples to verify the scientificity and rationality of the portfolio strategy. The results show that the proposed approach not only provides scientific support and reference for investors’ investment decisions, but also provides a new investment strategy theory and method for the investment decisions of the stock market.},
  archive      = {J_APIN},
  author       = {Bai, Juncheng and Guo, Jianfeng and Sun, Bingzhen and Guo, Yuqi and Chen, Youwei and Xiao, Xia},
  doi          = {10.1007/s10489-023-05085-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29918-29942},
  shortjournal = {Appl. Intell.},
  title        = {Probability rough set and portfolio optimization integrated three-way predication decisions approach to stock price},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting broken receiver tubes in CSP plants using
intelligent sampling and dual loss. <em>APIN</em>, <em>53</em>(24),
29902–29917. (<a
href="https://doi.org/10.1007/s10489-023-05093-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concentrated solar power (CSP) is one of the growing technologies that is leading the process of change from fossil fuels to renewable energies for electricity production. The sophistication and size of the systems require an increase in maintenance tasks to ensure reliability, availability, maintainability, and safety. Currently, automatic detection of broken glass envelopes of receiver tubes in CSP plants using parabolic trough collector systems has two main drawbacks: 1) the devices in use need to be manually placed near the receiver tube, 2) the machine learning–based solutions have only been tested in constrained environments. We address both gaps by combining the data collected by an unmanned aerial vehicle with data provided by sensors placed within 7 real CSP plants. The resulting dataset is the first of this type and can help standardize research activities for the problem of fault detection in this type of power plant. Our work proposes supervised machine-learning algorithms for detecting broken envelopes of the receiver tubes in CSP plants. The proposed solution takes the class imbalance problem into account, boosting the accuracy of the algorithms for the minority class without harming the overall performance of the models. For a deep residual network, we solve an imbalance and a balance problem at the same time, which increases the recall of the minority class by 5% with no harm to the F1 score. Additionally, the random under-sampling technique boosts the performance of traditional machine learning models. The histogram gradient boost classifier was found to be the algorithm with the highest increase (3%) in the F1 score. To the best of our knowledge, this paper is the first to provide an automated solution to this problem using data from operating plants, drones, and highly unbalanced datasets.},
  archive      = {J_APIN},
  author       = {Pérez-Cutiño, M. A. and Valverde, J. and Díaz-Báñez, J. M},
  doi          = {10.1007/s10489-023-05093-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29902-29917},
  shortjournal = {Appl. Intell.},
  title        = {Detecting broken receiver tubes in CSP plants using intelligent sampling and dual loss},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decomposition-based multiobjective evolutionary algorithm
with density estimation-based dynamical neighborhood strategy.
<em>APIN</em>, <em>53</em>(24), 29863–29901. (<a
href="https://doi.org/10.1007/s10489-023-05105-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiobjective evolutionary algorithm based on decomposition (MOEA/D) decomposes a multiobjective optimization problem (MOP) into several scalar subproblems and then optimizes them cooperatively in their respective neighborhoods. Since the neighborhood size remains constant during the evolution process, striking a balance between diversity and convergence is a challenging for the conventional MOEA/D. In this study, a density estimation-based dynamical neighborhood (DEDN) strategy is proposed and integrated into MOEA/D to form MOEA/D-DEDN. In the MOEA/D-DEDN, an angle-based evolutionary state evaluation (AESE) scheme is first developed to evaluate the evolutionary state of the algorithm. Second, a distance-based density estimation (DDE) scheme is designed to calculate the population density for all the subproblems. Finally, the neighborhood size and penalty parameters of each subproblem are adjusted based on the AESE scheme and DDE schemes during the evolutionary process to overcome the disadvantages of computational resource waste and premature convergence. The performance of the proposed MOEA/D-DEDN is validated using the ZDT, DTLZ, and UF test suits in terms of IGD, HV, and Spacing metrics. The experimental results show that MOEA/D-DEDN has a significant improvement over the traditional MOEA/D and six state-of-the-art MOEA/D variants. Furthermore, to verify its effectiveness and usefulness, the proposed MOEA/D-DEDN is applied to address MOPs for three engineering applications: trajectory planning for parafoil UAVs, structural optimization for space trusses, and parameters optimization for wastewater treatment processes.},
  archive      = {J_APIN},
  author       = {Qin, Yuanhui and Ren, Jian and Yang, Dan and Zhou, Hongbiao and Zhou, Hengrui and Ma, Congguo},
  doi          = {10.1007/s10489-023-05105-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29863-29901},
  shortjournal = {Appl. Intell.},
  title        = {Decomposition-based multiobjective evolutionary algorithm with density estimation-based dynamical neighborhood strategy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Arabic text detection: A survey of recent progress
challenges and opportunities. <em>APIN</em>, <em>53</em>(24),
29845–29862. (<a
href="https://doi.org/10.1007/s10489-023-04992-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Arabic language plays a crucial role in the world after becoming the sixth official language of the United Nations (UN). In the last ten years, there has been a rising growth in the number of Arabic texts, which requires algorithmic to be more effective and efficient to represent Arabic Text (AT), detecting patterns, and classifying text into the right class. Many algorithms are available for English text, but it is not the same for Arabic because of the complexity of morphology and diversity of the Arabic dialects. This study provides a survey of research in the field of Arabic Text Detection (ATD) published from 2017 to 2023. In addition, it has been conducted in a two-fold manner. Firstly, we survey based on eleven topics related to ATD. Secondly, we survey based on three stages of ATD namely pre-processing, representation, and detection. We explore all available datasets and open sources related to AT. It is revealed through the reviewed research that there are many topics of still interest to address. Furthermore, based on our observation deep-based methods yield better results only because they comprehend both the context and semantics of the language. However, they are also slower than traditional representations. Thus, hybrid models seem to be a promising way forward. Finally, we highlight new directions and discuss the open challenges and opportunities which assist researchers in identifying future work.},
  archive      = {J_APIN},
  author       = {Muaad, Abdullah Y. and Raza, Shaina and Naseem, Usman and Davanagere, Hanumanthappa J. Jayappa},
  doi          = {10.1007/s10489-023-04992-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29845-29862},
  shortjournal = {Appl. Intell.},
  title        = {Arabic text detection: A survey of recent progress challenges and opportunities},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-sensory system for UAVs detection using bayesian
inference. <em>APIN</em>, <em>53</em>(24), 29818–29844. (<a
href="https://doi.org/10.1007/s10489-023-05027-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles UAVs have revolutionized a wide range of activities and businesses, creating new opportunities for commercial and military applications. However, they also pose potential risks for terrorist and criminal activities. Therefore, researchers have been examining potential drone threats and considering how to address security and privacy concerns related to this technology. This research topic has gained significant attention in recent years due to the rapid proliferation of commercial and recreational drones, as well as the associated risks to airspace safety and security. Various detection technologies have been developed, including radar, optical, and acoustic sensing systems. However, each detector has its own limitations, such as reduced effectiveness in low-light conditions, fog, and noise. To address these limitations, we have developed a drone detection technique that utilizes multiple detectors, including visual, acoustic, and magnetic field sensors applying artificial intelligence, to compensate for their shortcomings. In this approach, each detector makes an independent decision, which may be either consistent or conflicting with the other detectors. In our study, we employed the Bayesian Inference technique to optimize decision-making in cases where there was conflict among the decisions made by the multi-sensors. We used indicators such as the Ephemeris indicator (EI) and the Acoustic ambiance indicator (AI) to generate settings to determine the degree of conflict. The drone detection process was fully automated, and the conflict decision was optimized using this approach. Our results indicated that the automatic drone detection with Bayesian inference had the best performance for drone identification in terms of accuracy, specificity, and sensitivity, as well as the highest accuracy in preventing unwanted drone interventions.},
  archive      = {J_APIN},
  author       = {Saadaoui, Fatima Zohra and Cheggaga, Nawal and Djabri, Nour El Houda},
  doi          = {10.1007/s10489-023-05027-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29818-29844},
  shortjournal = {Appl. Intell.},
  title        = {Multi-sensory system for UAVs detection using bayesian inference},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Exploring complex multivariate probability distributions
with simple and robust bayesian network topology for classification.
<em>APIN</em>, <em>53</em>(24), 29799–29817. (<a
href="https://doi.org/10.1007/s10489-023-05098-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network classifier (BNC) allows efficient and effective inference under condition of uncertainty for classification, and it depicts the interdependencies among random variables using directed acyclic graph (DAG). However, learning an optimal BNC is NP-hard, and complicated DAGs may lead to biased estimates of multivariate probability distributions and subsequent degradation in classification performance. In this study, we suggest using the entropy function as the scoring metric, and then apply greedy search strategy to improve the fitness of learned DAG to training data at each iteration. The proposed algorithm, called One $$+$$ Bayesian Classifier (O $$^{+}$$ BC), can represent high-dependence relationships in its robust DAG with a limited number of directed edges. We compare the performance of O $$^{+}$$ BC with other six state-of-the-art single and ensemble BNCs. The experimental results reveal that O $$^{+}$$ BC demonstrates competitive or superior performance in terms of zero-one loss, bias-variance decomposition, Friedman and Nemenyi tests.},
  archive      = {J_APIN},
  author       = {Wang, Lanni and Wang, Limin and Guo, Lu and Li, Qilong and Li, Xiongfei},
  doi          = {10.1007/s10489-023-05098-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29799-29817},
  shortjournal = {Appl. Intell.},
  title        = {Exploring complex multivariate probability distributions with simple and robust bayesian network topology for classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A meta-heuristic feature selection algorithm combining
random sampling accelerator and ensemble using data perturbation.
<em>APIN</em>, <em>53</em>(24), 29781–29798. (<a
href="https://doi.org/10.1007/s10489-023-05123-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms have been extensively utilized in feature selection tasks because they can obtain the global optimal solution. However, the meta-heuristic algorithm will take too much time in the face of a large number of samples. Although most of the studies compromise to approximate optimal solutions for avoiding time-consuming problems, a new problem with reduced classification performance, especially classification stability, is then generated. Aiming to above problems, this paper proposes a new feature selection framework. First, this framework exploits a voting ensemble strategy to improve classification stability by reducing the impact of misclassified labels on the overall classification results. Second, the framework uses a data perturbation strategy to enhance classification accuracy. In particular, the data perturbation strategy is able to generate more neighborhood relationships in the dataset, which could reveal the distribution of various features of the samples. A voting ensemble of different feature distributions is capable of extracting more information from the dataset, then the initially misclassified samples are more likely to be returned to the correct classification. Third, the framework takes a random sampling accelerator into account to solve the problem of excessive time consumption by reducing the size of the search sample space. Finally, for the sake of verifying the effectiveness of the proposed framework, four meta-heuristic feature selection methods based on a neighborhood rough set are compared on 20 datasets. The experimental results indicate that our framework could improve classification performance and accelerate feature selection, particularly in confronting large sample sizes.},
  archive      = {J_APIN},
  author       = {Zhang, Shuaishuai and Liu, Keyu and Xu, Taihua and Yang, Xibei and Zhang, Ao},
  doi          = {10.1007/s10489-023-05123-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29781-29798},
  shortjournal = {Appl. Intell.},
  title        = {A meta-heuristic feature selection algorithm combining random sampling accelerator and ensemble using data perturbation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the potentials of online machine learning for
predictive maintenance: A case study in the railway industry.
<em>APIN</em>, <em>53</em>(24), 29758–29780. (<a
href="https://doi.org/10.1007/s10489-023-05092-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses data-driven predictive maintenance, an area in which machine learning has received considerable attention. Traditionally, a machine learning model is trained on static data before being put into production to predict failures on incoming data. However, new data typically present novelties that were not included in the training data, such as unexpected anomalies or faults. Such novelties reduce the model accuracy and require model retraining, which we consider to be a suboptimal practice.Therefore, we propose to leverage online machine learning as an adaptive and continuous alternative to implement efficient predictive maintenance on systems that produce data continuously. The literature on predictive maintenance concentrates primarily on failure prediction, whereas there are multiple stages in a standard predictive maintenance framework, such as data preprocessing and diagnostics, that require attention. In this study, we propose a modular pipeline consisting of three modules to execute many stages inside a predictive maintenance solution. Each module represents one of our original contributions. Firstly, because a system generates repeating patterns in the form of cycles when performing its functions, we construct an online active learning-based framework to extract these cycles from a stream of sensor data (cycle extraction with InterCE). Secondly, we implement an autoencoder for encoding the extracted cycles into feature vectors (feature learning with LSTM-AE). Thirdly, we develop an adaptive scoring function to compute the health of any system at any time using online clustering on the stream of feature vectors (health detection with CheMoc). These three contributions establish our framework for processing raw sensor data for predictive maintenance. We evaluate our methods using a real-world data set provided by SNCF, the French national railway company. For each experiment, we simulate a data stream consisting of sequentially arriving data from the provided data set to test our online algorithms. The experimental results demonstrate that (i) InterCE is able to extract cycles from a high-speed stream with greater accuracy than a hand-crafted expert system, (ii) LSTM-AE can identify meaningful features from the extracted cycles, and (iii) CheMoc can discover clusters that represent physical anomalies of the systems and capture the health evolution of the monitored systems. Due to a lack of ground-truth data at the time of writing, we have not implemented the prognostics method and will reserve this for future works. This study confirms the potential of online machine learning as an adaptive and lifelong learning solution for predictive maintenance.},
  archive      = {J_APIN},
  author       = {Le-Nguyen, Minh-Huong and Turgis, Fabien and Fayemi, Pierre-Emmanuel and Bifet, Albert},
  doi          = {10.1007/s10489-023-05092-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29758-29780},
  shortjournal = {Appl. Intell.},
  title        = {Exploring the potentials of online machine learning for predictive maintenance: A case study in the railway industry},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conflict-based negotiation strategy for human-agent
negotiation. <em>APIN</em>, <em>53</em>(24), 29741–29757. (<a
href="https://doi.org/10.1007/s10489-023-05001-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Day by day, human-agent negotiation becomes more and more vital to reach a socially beneficial agreement when stakeholders need to make a joint decision together. Developing agents who understand not only human preferences but also attitudes is a significant prerequisite for this kind of interaction. Studies on opponent modeling are predominantly based on automated negotiation and may yield good predictions after exchanging hundreds of offers. However, this is not the case in human-agent negotiation in which the total number of rounds does not usually exceed tens. For this reason, an opponent model technique is needed to extract the maximum information gained with limited interaction. This study presents a conflict-based opponent modeling technique and compares its prediction performance with the well-known approaches in human-agent and automated negotiation experimental settings. According to the results of human-agent studies, the proposed model outpr erforms them despite the diversity of participants’ negotiation behaviors. Besides, the conflict-based opponent model estimates the entire bid space much more successfully than its competitors in automated negotiation sessions when a small portion of the outcome space was explored. This study may contribute to developing agents that can perceive their human counterparts’ preferences and behaviors more accurately, acting cooperatively and reaching an admissible settlement for joint interests.},
  archive      = {J_APIN},
  author       = {Keskin, Mehmet Onur and Buzcu, Berk and Aydoğan, Reyhan},
  doi          = {10.1007/s10489-023-05001-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29741-29757},
  shortjournal = {Appl. Intell.},
  title        = {Conflict-based negotiation strategy for human-agent negotiation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CCSMP: An efficient closed contiguous sequential pattern
mining algorithm with a pattern relation graph. <em>APIN</em>,
<em>53</em>(24), 29723–29740. (<a
href="https://doi.org/10.1007/s10489-023-05118-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The closed contiguous sequential pattern combines the advantages of closedness constraints and contiguity constraints and in recent years has been widely used in the fields of sequence classification, traffic trajectory visualization and football player trajectory analysis. Most of the previously developed closed contiguous sequential pattern mining algorithms pose some challenges. For instance, CCSpan, BP-CCSM, and LCCspm cannot mine the large-scale sequence database with reasonable time and memory usage, while C3Ro, which can mine patterns with multiple constraints, does not consider the specificity induced by the contiguity constraint of the pattern. To address these problems and improve the efficiency of mining closed contiguous sequential patterns, in this paper, we present an algorithm called CCSMP based on the pattern relation graph. Pattern relation graph is a novel data structure that has some key properties related to closed contiguous sequential pattern mining. In the experimental section, we not only conducted extensive experiments on real datasets to evaluate the performance and scalability of CCSMP but also analyzed the running time of each step of CCSMP to verify the effectiveness of the pattern relation graph. The experimental results show that CCSMP outperforms the existing state-of-the-art algorithm in most cases and that the use of the pattern relation graph can significantly reduce the time for closure checking.},
  archive      = {J_APIN},
  author       = {Hu, Haichuan and Zhang, Jingwei and Xia, Ruiqing and Liu, Shichao},
  doi          = {10.1007/s10489-023-05118-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29723-29740},
  shortjournal = {Appl. Intell.},
  title        = {CCSMP: An efficient closed contiguous sequential pattern mining algorithm with a pattern relation graph},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive label assignment in vehicle detection.
<em>APIN</em>, <em>53</em>(24), 29713–29722. (<a
href="https://doi.org/10.1007/s10489-023-05023-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection is a critical task that involves identifying and localizing vehicles in a traffic scenario. However, the traditional approach of one-to-one set matching for label assignment, where each ground-truth bounding box is assigned to one specific query, can lead to sparse positive samples. To address this issue, we drew inspiration from contrastive learning and employed contrasting samples generated by feature augmentation, rather than supplementing the complex one-to-many matching in label assignment. Our proposed approach was evaluated on the publicly available GM traffic dataset and Hangzhou traffic dataset, and the results demonstrate that our approach outperforms other state-of-the-art methods, with average precision (AP) improvements of 1.0% and 1.1%, respectively. Overall, our approach effectively handles the sparsity of positive samples in vehicle detection and achieves better performance than existing methods.},
  archive      = {J_APIN},
  author       = {Sun, Erjun and Zhou, Di and Xu, Zhaocheng and Sun, Jie and Wang, Xun},
  doi          = {10.1007/s10489-023-05023-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29713-29722},
  shortjournal = {Appl. Intell.},
  title        = {Contrastive label assignment in vehicle detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-way group decisions with evidential reasoning in
incomplete hesitant fuzzy information systems for liver disease
diagnosis. <em>APIN</em>, <em>53</em>(24), 29693–29712. (<a
href="https://doi.org/10.1007/s10489-023-05116-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver diseases have emerged as a growing concern in modern times, encompassing various disorders that can harm the liver and impede its function. The employment of data acquisition instruments introduces novel obstacles to the identification and diagnosis of these diseases, such as ambiguity in information modeling, potential risks in decision-making when interpreting diagnostic data, and the demand for comprehensible tools to facilitate informed choices. Therefore, the goal of this paper is to investigate a three-way group decision (3WGD) scheme with evidential reasoning (ER) in incomplete hesitant fuzzy information systems (I-HF-ISs) for liver disease diagnosis. Specifically, the form of multigranulation (MG) I-HF-ISs is established to describe realistic incomplete, imprecise and hesitant information existed in liver disease diagnosis. By utilizing the HF similarity principle and MG three-way decisions (3WD), an adjustable MG HF probability rough set (PRS) concept is developed. The ER method is then employed to determine the optimal threshold. Subsequently, an HF MAGDM approach is established for multi-attribute group decision-making (MAGDM) using adjustable MG HF PRSs and the ER method. Ultimately, the rationality of the proposed methodology is demonstrated through a real-life example using two UCI data sets for liver diseases. The approach’s effectiveness is substantiated through experimental analyses.},
  archive      = {J_APIN},
  author       = {Ding, Juanjuan and Li, Deyu and Zhang, Chao and Lin, Mingwei},
  doi          = {10.1007/s10489-023-05116-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29693-29712},
  shortjournal = {Appl. Intell.},
  title        = {Three-way group decisions with evidential reasoning in incomplete hesitant fuzzy information systems for liver disease diagnosis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate time-series classification using memory and
attention for long and short-term dependence <span
class="math display"><sup>⋆</sup></span>. <em>APIN</em>,
<em>53</em>(24), 29677–29692. (<a
href="https://doi.org/10.1007/s10489-023-05079-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is one of the most challenging research topics in data mining, and can finds its wide applications in biomedical engineering and clinical prediction. In recent years, deep learning (DL) has shown impressive performance in TSC research due to its end-to-end capabilities. In contrast to univariate TSC (UTSC), multivariable TSC (MTSC) is more prevalent and its effectiveness depends on long and short-term dependencies to a certain degree. However, mainstream DL models mainly construct various complex frameworks to extract features, and with the over-fitting problem. This paper proposes a comprehensive DL structure, namely the Multivariable Multi-Scale Attention Gated Cycle Unit Fully Convolutional Network (MMAGRU-FCN), which can effectively address the long and short-term dependence in MTSC by integrating memory and attention mechanisms at multiple scales. The proposed model can achieve adaptive characteristic calibration and capture significant features simultaneously. Extensive experiments demonstrate the superior performance of our model over state-of-the-art DL networks in terms of faster convergence, better stability, and in the case that parameters are close to the minimum threshold of comparison. Moreover, the proposed model consistently achieves the highest classification accuracy across different time series lengths. Finally, we verify the performance of the proposed model for various classification scenarios.},
  archive      = {J_APIN},
  author       = {Yuan, Jianjun and Wu, Fujun and Wu, Hong},
  doi          = {10.1007/s10489-023-05079-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29677-29692},
  shortjournal = {Appl. Intell.},
  title        = {Multivariate time-series classification using memory and attention for long and short-term dependence $$^{\star }$$},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint relational triple extraction based on potential
relation detection and conditional entity mapping. <em>APIN</em>,
<em>53</em>(24), 29656–29676. (<a
href="https://doi.org/10.1007/s10489-023-05111-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint relational triple extraction treats entity recognition and relation extraction as a joint task to extract relational triples, and this is a critical task in information extraction and knowledge graph construction. However, most existing joint models still fall short in terms of extracting overlapping triples. Moreover, these models ignore the trigger words of potential relations during the relation detection process. To address the two issues, a joint model based on Potential Relation Detection and Conditional Entity Mapping is proposed, named PRDCEM. Specifically, the proposed model consists of three components, i.e., potential relation detection, candidate entity tagging, and conditional entity mapping, corresponding to three subtasks. First, a non-autoregressive decoder that contains a cross-attention mechanism is applied to detect potential relations. In this way, different potential relations are associated with the corresponding trigger words in the given sentence, and the semantic representations of the trigger words are fully utilized to encode potential relations. Second, two distinct sequence taggers are employed to extract candidate subjects and objects. Third, an entity mapping module incorporating conditional layer normalization is designed to align the candidate subjects and objects. As such, each candidate subject and each potential relation are combined to form a condition that is incorporated into the sentence, which can effectively extract overlapping triples. Finally, the negative sampling strategy is employed in the entity mapping module to mitigate the error propagation from the previous two components. In a comparison with 15 baselines, the experimental results obtained on two widely used public datasets demonstrate that PRDCEM can effectively extract overlapping triples and achieve improved performance.},
  archive      = {J_APIN},
  author       = {Zhou, Xiong and Zhang, Qinghua and Gao, Man and Wang, Guoyin},
  doi          = {10.1007/s10489-023-05111-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29656-29676},
  shortjournal = {Appl. Intell.},
  title        = {Joint relational triple extraction based on potential relation detection and conditional entity mapping},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention adaptive instance normalization style transfer for
vascular segmentation using deep learning. <em>APIN</em>,
<em>53</em>(24), 29638–29655. (<a
href="https://doi.org/10.1007/s10489-023-05033-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have demonstrated substantial progress in medical image segmentation. However, these models require large datasets for training, which can prove to be clinically difficult. Medical imaging datasets exhibit domain shift problems due to different imaging techniques, scanners, and data privacy issues and, conventional deep neural networks lack generalization capabilities, as their effectiveness decreases with different data distributions. This paper presents a deep learning-based attention-adaptive instance normalization style transfer technique to address the challenges encountered when segmenting blood vessels. The proposed methodology combines adaptive instance normalization style transfer with a dense extreme inception network and convolution block attention module to achieve the best observed vessel segmentation performance. A simple yet effective method is proposed, and it improves the generalization performance of deep neural networks in vascular segmentation. The network is trained on natural images and tested on medical images, thereby overcoming the need for a large dataset or labelled ground truth to train for vessel segmentation. The proposed technique uses experimental results from five distinct medical datasets to demonstrate higher cross-domain generalization capabilities than the state-of-the-art baselines available in the current literature, and the segmentation performance is compared qualitatively and quantitatively with other models. The results demonstrate the feasibility of generalizing our approach to various datasets. This approach overcomes the constraints of traditional deep learning algorithms, which require enormous volumes of medical data along with manually-labelled ground truth. The predictions by the proposed approach are based on natural image training and can be reliably used to detect and identify cardiac and retinal abnormalities without prior medical imaging information.},
  archive      = {J_APIN},
  author       = {Mulay, Supriti and Ram, Keerthi and Sivaprakasam, Mohanasankar},
  doi          = {10.1007/s10489-023-05033-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29638-29655},
  shortjournal = {Appl. Intell.},
  title        = {Attention adaptive instance normalization style transfer for vascular segmentation using deep learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BiG2S: A dual task graph-to-sequence model for the
end-to-end template-free reaction prediction. <em>APIN</em>,
<em>53</em>(24), 29620–29637. (<a
href="https://doi.org/10.1007/s10489-023-05048-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrosynthesis and reaction outcome prediction are fundamental problems in organic chemistry and computer-aided synthesis planning (CASP), which are also crucial parts of computer-aided drug design. In recent years, deep learning has spawned a branch of methods which use machine translation frameworks with SMILES data representation to solve these problems. With the successive introduction of additional inverted reaction data as well as the molecular graph representation, the accuracy and validity of machine-transaction-based approaches have been further improved. In this work, we propose a bidirectional graph-to-sequence model (BiG2S) that combines the benefits of inverted reaction training and graph representation. The proposed approach has the ability to provide high-quality retrosynthesis and forward synthesis prediction simultaneously on various datasets, which achieves $$5.5\%$$ top-1 accuracy with only $$0.1\%$$ invalid results on USPTO-50k retrosynthesis task, and maintain $$85.0\%$$ top-1 accuracy for outcome prediction with the same model.},
  archive      = {J_APIN},
  author       = {Hu, Haozhe and Jiang, Yongquan and Yang, Yan and Chen, Jim X.},
  doi          = {10.1007/s10489-023-05048-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29620-29637},
  shortjournal = {Appl. Intell.},
  title        = {BiG2S: A dual task graph-to-sequence model for the end-to-end template-free reaction prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic comparison of different machine learning models
for the spatial estimation of air pollution. <em>APIN</em>,
<em>53</em>(24), 29604–29619. (<a
href="https://doi.org/10.1007/s10489-023-05109-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollutants harm human health and the environment. Nowadays, deploying an air pollution monitoring network in many urban areas could provide real-time air quality assessment. However, these networks are usually sparsely distributed and the sensor calibration problems that may appear over time lead to missing and wrong measurements. There is an increasing interest in developing air quality modelling methods to minimize measurement errors, predict spatial and temporal air quality, and support more spatially-resolved health effect analysis. This research aims to evaluate the ability of three feed-forward neural network architectures for the spatial prediction of air pollutant concentrations using the measures of an air quality monitoring network. In addition to these architectures, Support Vector Machines and geostatistical methods (Inverse Distance Weighting and Ordinary Kriging) were also implemented to compare the performance of neural network models. The evaluation of the methods was performed using the historical values of seven air pollutants (Nitrogen monoxide, Nitrogen dioxide, Sulphur dioxide, Carbon monoxide, Ozone, and particulate matters with size less than or equal to 2.5 $$\upmu $$ m and to 10 $$\upmu $$ m) from an urban air quality monitoring network located at the metropolitan area of Madrid (Spain). To assess and compare the predictive ability of the models, three estimation accuracy indicators were calculated: the Root Mean Squared Error, the Mean Absolute Error, and the coefficient of determination. FFNN-based models are superior to geostatistical methods and slightly better than Support Vector Machines for fitting the spatial correlation of air pollutant measurements.},
  archive      = {J_APIN},
  author       = {Cerezuela-Escudero, Elena and Montes-Sanchez, Juan Manuel and Dominguez-Morales, Juan Pedro and Duran-Lopez, Lourdes and Jimenez-Moreno, Gabriel},
  doi          = {10.1007/s10489-023-05109-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29604-29619},
  shortjournal = {Appl. Intell.},
  title        = {A systematic comparison of different machine learning models for the spatial estimation of air pollution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomalous variable-length subsequence detection in time
series: Mathematical formulation and a novel evolutionary algorithm
based on clustering and swarm intelligence. <em>APIN</em>,
<em>53</em>(24), 29585–29603. (<a
href="https://doi.org/10.1007/s10489-023-05066-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable-length anomalous subsequence detection in time series has many important applications in the real world, yet the methods presented in existing studies are computationally expensive, as the detection techniques are mostly brute-force approaches. In this work, we formalize the detection problem into a subsequence segmentation problem (SSP) optimization task, in which the time series is segmented by a set of cutting points into subsequences with minimized total distances to the representative motif. The anomalous subsequences can then be accurately located by reducing the dissimilarity among all subsequences, and this technique, when compared to existing techniques, can reduce the number of comparisons required for search. We further introduce a new clustering-based and swarm intelligence-based evolutionary algorithm (CBSI) in this work to solve the highly complex SSP efficiently. The proposed method balances the scopes of exploration and exploitation under a local-global search strategy. The CBSI clusters the solutions in the search space into groups, allowing frequent information sharing among solutions in the same cluster for their exploitation within their own search spaces. Furthermore, the best local solutions are promoted by the global-search strategy to explore the remaining search regions. Through a comparison with existing state-of-the-art techniques in solving both synthetic and real-world problems, we show that any optimization methods under our proposed SSP bring significant computational savings and comparable searching accuracy compared to existing techniques for the detection task. Our proposed CBSI also has the highest searching capability compared to existing and related optimization methods. The experimental results also highlight the scalability of our study to longer time series, larger anomaly sizes and wider search ranges.},
  archive      = {J_APIN},
  author       = {Sutrisno, Hendri and Phoa, Frederick Kin Hing},
  doi          = {10.1007/s10489-023-05066-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29585-29603},
  shortjournal = {Appl. Intell.},
  title        = {Anomalous variable-length subsequence detection in time series: Mathematical formulation and a novel evolutionary algorithm based on clustering and swarm intelligence},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent convolutional model based on gated spiking neural
p system for stereo matching networks. <em>APIN</em>, <em>53</em>(24),
29570–29584. (<a
href="https://doi.org/10.1007/s10489-023-05091-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning techniques has introduced extensive research improvements to various aspects in the processing pipeline of the stereo matching problem. Due to the high requirements of 3D convolution on computing resources and the domain sensitivity of 2D convolution, some stereo matching networks have begun to shift from full convolutional structures to recurrent structures, using the hidden state update mechanism of recurrent units to achieve global consistency of disparity-related information. In this paper, a new recurrent convolutional model is constructed based on a two-dimensional spiking neural computational system, and three types of recurrent units are designed by setting different parameters. The newly designed recurrent units are applied to a recent recurrent stereo matching network for better disparity propagation. Starting from the definition of two-dimensional gated spiking neural P systems, the spiking mechanism of a single neuron is extended to multi-neurons arranged in a two-dimensional array which are locally topologically connected. Its state update mechanism is parameterized in a form that can be back-propagated, thus realizing a new type of recurrent convolutional model. The proposed model can be embedded into existing recurrent stereo matching networks. Experimental results demonstrate that it can effectively reduce the computational load of the baseline method and achieve comparable accuracy to existing state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Guo, Chenggang and Peng, Hong and Wang, Jun},
  doi          = {10.1007/s10489-023-05091-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29570-29584},
  shortjournal = {Appl. Intell.},
  title        = {Recurrent convolutional model based on gated spiking neural p system for stereo matching networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CRUN: A super lightweight and efficient network for
single-image super resolution. <em>APIN</em>, <em>53</em>(24),
29557–29569. (<a
href="https://doi.org/10.1007/s10489-023-05077-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, single-image super-resolution (SISR) has made significant progress using convolutional neural networks. However, to achieve better performance, many methods deepen the network depth and stack a large number of parameters. Unfortunately, this leads to high computational complexity and is difficult to apply on ordinary intelligent devices used by the public. To address this issue, we propose a lightweight SISR network that uses thin convolutional groups to significantly reduce network size. We also optimize the network structure to compensate for the loss in performance caused by the reduction in the number of parameters. Our proposed CRUN(Cascade Compound Residual UNet Network) model utilizes cascade residuals structure as the backbone to deepen the network and increase its receptive field. We then use skip connections and dense connection structures to design deep feature extraction blocks that reduce the network’s dependence on feature dimensions. Moreover, we introduce a Multi-scale Feature Enhancement Block (MFEB) to fuse features at different levels, compensating for the weakness of thin convolutional groups in obtaining features. Our experimental results show that the CRUN model achieves superior performance compared to state-of-the-art lightweight models while requiring relatively fewer resources. This improvement makes our proposed model more practical and suitable for usage on everyday devices.},
  archive      = {J_APIN},
  author       = {Huang, Xingji and Mao, Yuxing and Li, Jian and Wu, Shunxin and Chen, Xueshuo and Lu, Hang},
  doi          = {10.1007/s10489-023-05077-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {24},
  pages        = {29557-29569},
  shortjournal = {Appl. Intell.},
  title        = {CRUN: A super lightweight and efficient network for single-image super resolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WaveCNNs-AT: Wavelet-based deep CNNs of adaptive threshold
for signal recognition. <em>APIN</em>, <em>53</em>(23), 28819–28831. (<a
href="https://doi.org/10.1007/s10489-023-05047-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are widely used for feature extraction in signal recognition. A critical issue in convolutional neural networks is the loss of information which increases with the depth of the network. To reduce information loss, this paper proposes a downsampling operator based on wavelet transforms, in which all wavelet components are fused as an output. The corresponding weights of the components are computed via signal back-propagation of the neural network. Meanwhile, a denoising rule of soft-threshold is devised for all wavelet components to achieve multi-channel adaptive denoising of the signal in the frequency domain. More precisely, the thresholds in each frequency band are optimised using a gradient descent algorithm so that the reconstructed signal carries information from each frequency band of the original signal while the noise is filtered out. Based on this, a 1-level wavelet feature extraction structure consisting of 1-stride convolution and full-component wavelet downsampling operation is designed. Finally, with a number of experiments on signal recognition, the proposed algorithm achieves favourable performance compared with state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yang, Wangzhuo and Chen, Bo and Shen, Yijun and Yu, Li},
  doi          = {10.1007/s10489-023-05047-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28819-28831},
  shortjournal = {Appl. Intell.},
  title        = {WaveCNNs-AT: Wavelet-based deep CNNs of adaptive threshold for signal recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new similarity measure to increase coverage of rating
predictions for collaborative filtering. <em>APIN</em>, <em>53</em>(23),
28804–28818. (<a
href="https://doi.org/10.1007/s10489-023-05041-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-based collaborative filtering (UBCF) is a popular technique in recommendation systems, where rating estimations for unrated items are based on the rating information of each neighboring user. However, conventional similarity measures for identifying the neighboring users often overlook the user’s contribution in rating predictions for unrated items. This leads to limited coverage in rating predictions. This study addresses this limitation by introducing a novel similarity metric that effectively incorporates the degree of user contribution to unrated item predictions, thereby enhancing coverage. Additionally, the proposed approach assigns varying weights to items based on their rating frequency, further improving coverage. Extensive experiments were conducted on six benchmark datasets to validate the proposed approach. The results showed that the proposed similarity measure improved the recommendations and significantly expanded the coverage of predictable items. Furthermore, the proposed similarity enhances diversity in recommendations, contributing to a more robust and effective recommendation system.},
  archive      = {J_APIN},
  author       = {Kim, Kyoungok},
  doi          = {10.1007/s10489-023-05041-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28804-28818},
  shortjournal = {Appl. Intell.},
  title        = {A new similarity measure to increase coverage of rating predictions for collaborative filtering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatio-temporal grammar graph attention network with
adaptive edge information for traffic flow prediction. <em>APIN</em>,
<em>53</em>(23), 28787–28803. (<a
href="https://doi.org/10.1007/s10489-023-05020-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is an essential part of the intelligent traffic management system, which can help managers plan and maintain traffic order and individuals choose better travel routes. Due to the complex spatio-temporal correlation of large-scale transportation networks, it is challenging to build accurate and efficient prediction models. To address this issue, this paper proposes a spatio-temporal grammar graph attention network with adaptive edge information for traffic flow prediction. The external structure of the prediction model uses a grammar graph structure based on three grammar rules to capture the interactive relationship between various traffic parameters. The internal structure uses a graph attention network with adaptive edge information for synchronous extraction of the spatio-temporal dependence of historical traffic information. The two real data sets simulation results show that the model’s prediction accuracy is better than the existing prediction methods in different scale traffic networks.},
  archive      = {J_APIN},
  author       = {Zhang, Zhao and Jiao, Xiaohong},
  doi          = {10.1007/s10489-023-05020-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28787-28803},
  shortjournal = {Appl. Intell.},
  title        = {A spatio-temporal grammar graph attention network with adaptive edge information for traffic flow prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive graph contrastive learning for community detection.
<em>APIN</em>, <em>53</em>(23), 28768–28786. (<a
href="https://doi.org/10.1007/s10489-023-05046-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph contrastive learning (GCL) has received considerable interest in graph representation learning for its robustness in capturing complex relationships between nodes in an unsupervised manner, making it suitable for unsupervised graph learning tasks such as community detection. However, most GCL approaches have two limitations when applied to community detection. First, the random augmentation strategy employed by them may destroy a graph’s community structure due to the random added/removed edges or attributes. Second, nodes with similar topology or attributes may be selected as the negative samples of a target node according to their sample selection strategy, leading to the wrong assignment of the target node’s community. In this paper, we propose an adaptive-graph-contrastive-learning-based community detection (AGCLCD) algorithm to address the problems. At its core, AGCLCD introduces an adaptive graph augmentation strategy to preserve a graph’s original community structure in augmentation. Furthermore, we develop a composite contrastive pair selection scheme to choose the nodes sharing similar topology and attributes with a target node as its positive samples to ensure that the representation vectors of nodes in the same community are highly relevant. Comprehensive experiments on real-world and synthetic networks demonstrate that AGCLCD achieves higher accuracy and effectiveness than state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Guo, Kun and Lin, Jiaqi and Zhuang, Qifeng and Zeng, Ruolan and Wang, Jingbin},
  doi          = {10.1007/s10489-023-05046-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28768-28786},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive graph contrastive learning for community detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable-wise generative adversarial transformer in
multivariate time series anomaly detection. <em>APIN</em>,
<em>53</em>(23), 28745–28767. (<a
href="https://doi.org/10.1007/s10489-023-05029-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple variable time series anomaly detection plays a significant role in fields such as AIOps and intelligent healthcare. However, time series often lack labels, and anomalies are infrequent compared to normal sequences. Moreover, high-dimensional nonlinear correlations and substantial distribution variations among variables exist in multivariate time series. These challenges make it difficult for the model to accurately model time series. While contemporary reconstruction-based models address challenges of data scarcity and time series modeling through GAN frameworks, little attention has been given to mitigating the substantial differences in variable distributions among multiple time series. This paper proposes the Variable-wise Generative Adversarial Transformer (VGAT) for achieving unsupervised anomaly detection, with WGAN as the training framework and Transformer as the fundamental structure for the Duplicator and Discriminator. Firstly, VGAT makes the Generator act as the Duplicator, reconstructing samples through direct replication, simplifying the model structure, and enhancing efficiency. Secondly, VGAT employs the Soft-DTW regularization to facilitate a more authentic replication of normal sequences by the Duplicator. Most importantly, VGAT introduces the Variable-wise Gate, dynamically calculating a gate for each variable in the multivariate time series to balance the differences in variable distributions. After VGAT generates anomaly scores, the SPOT algorithm based on Extreme Value Theory (EVT) automatically computes the threshold, enhancing the adaptability to different data of the model. VGAT demonstrates state-of-the-art (SOTA) performance, surpassing baselines on public datasets. Notably, as the dimensionality of the series increases, VGAT exhibits more significant improvements. On WADI dataset, VGAT achieves an impressive 68.19% enhancement in F1-score, showcasing remarkable progress compared to the current SOTA model.},
  archive      = {J_APIN},
  author       = {Yang, Xuekang and Li, Hui and Feng, Xingyu and Jin, Zixiong},
  doi          = {10.1007/s10489-023-05029-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28745-28767},
  shortjournal = {Appl. Intell.},
  title        = {Variable-wise generative adversarial transformer in multivariate time series anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning based time-varying formation control
for quadrotor unmanned aerial vehicles system with input saturation.
<em>APIN</em>, <em>53</em>(23), 28730–28744. (<a
href="https://doi.org/10.1007/s10489-023-05050-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a reinforcement learning sliding mode strategy for the time-varying formation control of quadrotor unmanned aerial vehicles system. Previous research relied on sliding mode control for swarm coordination, but encountered significant challenges, including the chattering phenomenon and increased energy consumption. To overcome these issues, this work introduces a reinforcement learning algorithm that leverages a critic neural network to approximate the performance index function and an actor neural network to redesign the switching control strategy within the traditional sliding mode framework. Additionally, the incorporation of control thresholds ensures the smooth operation of the system when executing time-varying formation tasks. According to the Lyapunov method, the stability is analyzed. The evaluation results indicate that with the proposed reinforcement learning sliding mode controller, the quadrotor unmanned aerial vehicles system can achieve time-varying formation control well. At the same time, the chattering phenomenon and power consumption characteristics are significantly improved.},
  archive      = {J_APIN},
  author       = {Ma, Chi and Cao, Yizhe and Dong, Dianbiao},
  doi          = {10.1007/s10489-023-05050-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28730-28744},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning based time-varying formation control for quadrotor unmanned aerial vehicles system with input saturation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting skeleton-based gait events with attention-guided
residual deep learning model for human identification. <em>APIN</em>,
<em>53</em>(23), 28711–28729. (<a
href="https://doi.org/10.1007/s10489-023-05019-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human identification using unobtrusive visual features is a daunting task in smart environments. Gait is among adequate biometric features when the camera cannot correctly capture the human face due to environmental factors. In recent years, gait-based human identification using skeleton data has been intensively studied using a variety of feature extractors and more sophisticated deep learning models. Although skeleton data is susceptible to changes in covariate variables, resulting in noisy data, most existing algorithms employ a single feature extraction technique for all frames to generate frame-level feature maps. This results in degraded performance and additional features, necessitating increased computing power. This paper proposes a robust feature extractor that extracts a quantitative summary of gait event-specific information, thereby reducing the total number of features throughout the gait cycle. In addition, a novel Attention-guided LSTM-based deep learning model with residual connections is proposed to learn the extracted features for gait recognition. The proposed approach outperforms the state-of-the-art works on five publicly available datasets on various benchmark evaluation protocols and metrics. Further, the CMC test revealed that the proposed model obtained higher than 97% Accuracy in lower-level ranks on these datasets.},
  archive      = {J_APIN},
  author       = {M, Rashmi and Guddeti, Ram Mohana Reddy},
  doi          = {10.1007/s10489-023-05019-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28711-28729},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting skeleton-based gait events with attention-guided residual deep learning model for human identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-enhanced multi-task recommendation in hyperbolic
space. <em>APIN</em>, <em>53</em>(23), 28694–28710. (<a
href="https://doi.org/10.1007/s10489-023-05045-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning has recently inspired a series of fruitful research in the field of recommendation due to its ability to handle complex scenarios by associating information between tasks. However, various information suitable for multi-task recommender systems usually produces different degrees of noise. For example, parameter information suitable for one task will affect other tasks, and the real data that is difficult to observe correctly in Euclidean space may be regarded as noise data. To tackle this problem, we propose a novel knowledge-enhanced multi-task recommendation algorithm in hyperbolic space named KMRH. The algorithm employs the alternate training method to alleviate the parameter noise problem in complex recommendation scenarios. Specifically, we design a novel knowledge enhancement strategy in the Poincaré sphere, which exploits hyperbolic embeddings to capture knowledge graphs of complex structured data. Finally, we adopt spatial distance as a metric to distinguish positive and negative samples at different locations, thereby limiting the detrimental impact of noise components on the recommendation model. Extensive experiments on three benchmark datasets demonstrate that our proposed algorithm achieves significant improvements over other state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Zhu, Junlin and Zhang, Yihao and Wang, Yulin and Liao, Weiwen and Chen, Ruizhen and Yuan, Meng},
  doi          = {10.1007/s10489-023-05045-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28694-28710},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge-enhanced multi-task recommendation in hyperbolic space},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large capacity generative image steganography via image
style transfer and feature-wise deep fusion. <em>APIN</em>,
<em>53</em>(23), 28675–28693. (<a
href="https://doi.org/10.1007/s10489-023-04993-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with modification-based steganography, the coverless steganography models have stronger anti-detection performance. However, the limitations of low embedding capacity and image quality are existing in current coverless steganographic models. The image style transfer is a generation task that translates a style to another while maintaining the structure and semantics of the original image. The stable structures that can be used as cover to hide secret for steganography. In this paper, a generative steganographic model based on style transfer and feature-wise deep fusion is proposed, which can achieve a large embedding capacity and high generation quality. In embedding stage, the channel reduction net is designed to distill structural features from image, and the fusion net is proposed to fuse the secret matrix and distilled structural features. The embedded structural features are participated in style transfer to finish steganography. In secret recovery stage, the two-level extraction model is adopted. The structural features are extracted from stego image by residual network, then the embedded secret message is distilled from the extracted features. Compared with existing steganographic model, the proposed model is convenient applied without code-book or database, and the experimental results show that the model achieves a larger capacity, higher security and visual quality.},
  archive      = {J_APIN},
  author       = {Sun, Youqiang and Liu, Jianyi and Zhang, Ru},
  doi          = {10.1007/s10489-023-04993-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28675-28693},
  shortjournal = {Appl. Intell.},
  title        = {Large capacity generative image steganography via image style transfer and feature-wise deep fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A contrastive autoencoder with multi-resolution
segment-consistency discrimination for multivariate time series anomaly
detection. <em>APIN</em>, <em>53</em>(23), 28655–28674. (<a
href="https://doi.org/10.1007/s10489-023-04985-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most reconstruction-based multivariate time series (MTS) anomaly detection methods tend to learn point-wise information, failing to extract a robust overall representation. Some studies have tried to introduce contrastive learning to alleviate this problem, but two key challenges remain: (1) Most data augmentation approaches follow the inductive bias from computer vision, which may destroy the time series patterns. (2) The instance discrimination proxy task of traditional contrastive learning will cause the generation of numerous false negative samples and the loss of common information when applied to MTS anomaly detection. In this paper, a contrastive autoencoder with multi-resolution segment-consistency discrimination (MRSCD) is proposed for MTS anomaly detection. Firstly, a time series data augmentation method is proposed, which constructs positive and negative samples by down-sampling the original time series in different resolutions and orders. Then an encoder is used to extract different levels of temporal information from the multi-resolution sample pairs which are randomly combined by positive and negative samples with the same resolution. Finally, a proxy task of segment-consistency discrimination is proposed for contrastive autoencoder to distinguish positive and negative sample pairs through the order of the down-sampled segments, enabling the model to extract inter-segment contextual information at the same time as learning point-wise information. The experimental results on five public datasets show that MRSCD significantly outperforms the baseline methods in three evaluation metrics.},
  archive      = {J_APIN},
  author       = {Xue, Bing and Gao, Xin and Zhai, Feng and Li, Baofeng and Yu, Jiahao and Fu, Shiyuan and Chen, Lingli and Meng, Zhihang},
  doi          = {10.1007/s10489-023-04985-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28655-28674},
  shortjournal = {Appl. Intell.},
  title        = {A contrastive autoencoder with multi-resolution segment-consistency discrimination for multivariate time series anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Response index: Quantitative evaluation index of
translational equivariance. <em>APIN</em>, <em>53</em>(23), 28642–28654.
(<a href="https://doi.org/10.1007/s10489-023-05021-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translational equivariance, one of the properties of Convolutional neural networks(CNNs), directly reflects the coherence of the influence of input at each position on the output. By looking for changes in variability such as translational equivariance, it is possible to determine whether the direction of model fit is correct. A controllable location target is designed to verify the translationlal equivariance of a CNN and then the effect of the CNN’s parameters on positioning errors was investigated. Furthermore, A quantitative method called response index(ResIndex) is proposed in this paper. When the parameters of a CNN are determined, the distribution of the input signal response at each position in the heatmap can be obtained via simple algebraic calculations. Here we demonstrate that translational equivariance is primarily affected by the convolution boundary effect,which can be quantitatively assessed by the ResIndex. Experimental evidence for the Pearson correlation coefficient between the MSE and ResIndex demonstrates that our ResIndex is strongly negatively correlated with the MSE, with the mean Pearson correlation coefficient is -0.9282 on the CIFAR-10 and -0.7837 on COCO. For the first time, a unified quantitative evaluation index called the ResIndex is proposed to measure the translational equivariance of CNN. A complete mathematical derivation and a time-saving calculation method are given.},
  archive      = {J_APIN},
  author       = {Yang, Peng and Kong, Lingqin and Liu, Ming and Tang, Ge and Dong, Liquan and Zhao, Yuejin and Chu, Xuhong and Hui, Mei},
  doi          = {10.1007/s10489-023-05021-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28642-28654},
  shortjournal = {Appl. Intell.},
  title        = {Response index: Quantitative evaluation index of translational equivariance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Stochastic variance reduced gradient with hyper-gradient
for non-convex large-scale learning. <em>APIN</em>, <em>53</em>(23),
28627–28641. (<a
href="https://doi.org/10.1007/s10489-023-05025-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-convex optimization, which can better capture the problem structure, has received considerable attention in the applications of machine learning, image/signal processing, statistics, etc. With faster convergence rate, there have been tremendous studies on developing stochastic variance reduced algorithms to solve these non-convex optimization problems. However, as a crucial hyper-parameter for stochastic variance reduced algorithms, that how to select an appropriate step size is less researched in solving non-convex optimization problems. To address this gap, we propose a new class of stochastic variance reduced algorithms based on hyper-gradient, which has the ability to automatically obtain the online step size. Specifically, we focus on the variance-reduced stochastic optimization algorithms, the stochastic variance reduced gradient (SVRG) algorithm, which computes a full gradient periodically. We analyze theoretically the convergence of the proposed algorithm for non-convex optimization problems. Moreover, we show that the proposed algorithm enjoys the same complexities as state-of-the-art algorithms for solving non-convex problems in terms of finding an approximate stationary point. Thorough numerical results on empirical risk minimization with non-convex loss functions validate the efficacy of our method.},
  archive      = {J_APIN},
  author       = {Yang, Zhuang},
  doi          = {10.1007/s10489-023-05025-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28627-28641},
  shortjournal = {Appl. Intell.},
  title        = {Stochastic variance reduced gradient with hyper-gradient for non-convex large-scale learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of fault evolution and remaining useful life for
rolling bearings with spalling fatigue using digital twin technology.
<em>APIN</em>, <em>53</em>(23), 28611–28626. (<a
href="https://doi.org/10.1007/s10489-023-05010-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying fault severity is a critical part of rolling bearing health management. There are numerous methods for evaluating the severity of rolling bearing faults that utilize various signals to predict the remaining useful life (RUL). However, the fault severity assessment methods based on digital twins still cannot accurately track the fault evolution. To address this issue, this paper proposes a data/physics-driven digital twin model for predicting the RUL of rolling bearings. Firstly, using theoretical damage mechanics, the evolution of fatigue damage of rolling bearings throughout their entire life cycle is described, for which a spalling fatigue evolution model is constructed. Secondly, fatigue defects predicted by the spalling fatigue evolution model are introduced into a dynamic model to construct a bearing high-fidelity model. Thirdly, a local-to-global dynamic updating framework is designed for the interactive coupling of real-time monitoring data of rolling bearings and corresponding simulated data, as well as updating the high-fidelity model parameters. Finally, a neural network is used to predict the RUL of rolling bearings. The approach is verified using test data of rolling bearings. The results demonstrate that the proposed digital twin model can more accurately address the problem of RUL prediction of rolling bearings. The remaining useful life prediction process of rolling bearing is based on digital twin.},
  archive      = {J_APIN},
  author       = {Meng, Weiying and Wang, Yutong and Zhang, Xiaochen and Li, Sihui and Bai, Xu and Hou, Lingling},
  doi          = {10.1007/s10489-023-05010-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28611-28626},
  shortjournal = {Appl. Intell.},
  title        = {Prediction of fault evolution and remaining useful life for rolling bearings with spalling fatigue using digital twin technology},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hybridformer: An efficient and robust new hybrid network
for chip image segmentation. <em>APIN</em>, <em>53</em>(23),
28592–28610. (<a
href="https://doi.org/10.1007/s10489-023-04975-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent monitoring of machine chips can help the machine tool to remove chip accumulation in time. Based on chip shape information, real-time monitoring of the machine tool is also possible. Although image segmentation algorithms have been widely used in industry, they still need to improve. These methods still have some limitations in effectively combining global and local information of industrial images, and there needs to be an image segmentation algorithm specifically for machine tool chips. In this paper, we propose a deep learning-based algorithm for lightweight chip image segmentation named Hybridformer. First, the algorithm generates scale-aware semantic features using feature maps of different scales as the input part. Then, we use the cross specification and SelfNorm to improve the generalization robustness of the model in terms of distribution bias. Finally, the network extracts global contextual information fusing EfficientNetv2 features to achieve accurate localization. Widespread experimental results show that Hybridformer outperforms both CNN and Transformer algorithms in terms of comprehensive performance on multiple chip image datasets, and it can achieve a good balance between accuracy and size of parameters.},
  archive      = {J_APIN},
  author       = {Zhang, Chuang and Liu, Xiuping and Ning, Xiaoge and Bai, Yuwei},
  doi          = {10.1007/s10489-023-04975-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28592-28610},
  shortjournal = {Appl. Intell.},
  title        = {Hybridformer: An efficient and robust new hybrid network for chip image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Component preserving laplacian eigenmaps for data
reconstruction and dimensionality reduction. <em>APIN</em>,
<em>53</em>(23), 28570–28591. (<a
href="https://doi.org/10.1007/s10489-023-05012-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laplacian Eigenmaps (LE) is a widely used dimensionality reduction and data reconstruction method. When the data has multiple connected components, the LE method has two obvious deficiencies. First, it might reconstruct each component as a single point, resulting in loss of information within the component. Second, it only focuses on local features but ignores the location information between components, which might cause the reconstructed components to overlap or to completely change their relative positions. To solve these two problems, this article first modifies the optimization objective of the LE method, by characterizing the relative positions between components of data with the similarity between high-density core points, and then solves the optimization problem by using a gradient descent method to avoid the over-compression of data points in the same connected component. A series of experiments on synthetic data and real-world data verify the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Meng, Hua and Zhang, Hanlin and Ding, Yu and Ma, Shuxia and Long, Zhiguo},
  doi          = {10.1007/s10489-023-05012-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28570-28591},
  shortjournal = {Appl. Intell.},
  title        = {Component preserving laplacian eigenmaps for data reconstruction and dimensionality reduction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware hierarchical reinforcement learning for
long-horizon tasks. <em>APIN</em>, <em>53</em>(23), 28555–28569. (<a
href="https://doi.org/10.1007/s10489-023-05022-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical reinforcement learning excels at dividing difficult task goals into easily achievable subgoals. It provides an effective means to solve long-horizon planning tasks that are trapped in high-dimensional complex environments. However, because it is challenging to train multiple levels of policies simultaneously, hierarchical reinforcement learning often suffers from the training non-stationary problem. Existing work analyzing the training non-stationary problem focuses on the noisy data created by the changes in low-level policy, which makes the high-level policy with aleatoric uncertainty. But the uncertain factors leading to the instability of high-level policy training are manifold. First, the randomness of the environments also generates noise in the high-level replay buffer, forming aleatoric uncertainty. Second, the limited transitions due to the agent’s insufficient exploration ability constitute the high-level policy’s epistemic uncertainty. In this paper, we first comprehensively examine the causes of the instability of hierarchical reinforcement learning training to address the uncertainty of high-level policy networks. On this basis, we propose uncertainty-aware hierarchical reinforcement learning (UAHRL), a novel framework to solve long-horizon tasks with stable learning. UAHRL constructs an action uncertainty estimation network based on deep ensembles to capture both uncertainties. The calculated uncertainties are then considered in the high-level training process to reduce non-stationary phenomena. The experiment results demonstrate that UAHRL outperforms the state-of-the-art hierarchical reinforcement learning algorithms in terms of sampling efficiency while also performing better on a series of long-horizon tasks with continuous action and state space.},
  archive      = {J_APIN},
  author       = {Hu, Wenning and Wang, Hongbin and He, Ming and Wang, Nianbin},
  doi          = {10.1007/s10489-023-05022-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28555-28569},
  shortjournal = {Appl. Intell.},
  title        = {Uncertainty-aware hierarchical reinforcement learning for long-horizon tasks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid transformer-CNN with boundary-awareness network for
3D medical image segmentation. <em>APIN</em>, <em>53</em>(23),
28542–28554. (<a
href="https://doi.org/10.1007/s10489-023-05032-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D volumetric medical image segmentation is a crucial task in computer-aided diagnosis applications, but it remains challenging due to low contrast and boundary ambiguity between organs and surrounding tissues. Considering that accurate boundary voxels are of importance for organ segmentation, which relies on rich detailed features information. The most recent convolutional neural networks and transformer networks have attempted to enhance 2D boundaries during feature extraction. Few approaches focus on boundary voxels preservation for 3D scenarios. To address these issues, we propose the Hybrid Transformer-CNN with Boundary-awareness(HTCB-Net) network, which follows an encoder-decoder segmentation paradigm with learnable boundary modules. The 3D swin-transformer encoder is embedded with auxiliary object-related boundary map by designing a learnable boundary extracting module(BEM), which assists model in obtaining rich and discriminative feature representations. Our boundary map obtained from BEM supervises explicitly feature extraction process. Subsequently, boundary preserving module(BPM) adopts a novel fusion strategy, which integrates extracted boundary map and the corresponding encoder features. Through this module, the boundary position awareness is combined for feature enhancement with spatial complement and channel attention. We evaluate the performance of the proposed method with quantitative experiments on three public available datasets in both CT and MRI modalities: OAI-ZIB, Spleen, Pancreas. The comparative experimental results demonstrate that our HTCB-Net preserves more precise 3D boundaries and obtains significant improvements, particularly in terms of Average Symmetric Surface Distance(ASSD).},
  archive      = {J_APIN},
  author       = {He, Jianfei and Xu, Canhui},
  doi          = {10.1007/s10489-023-05032-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28542-28554},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid transformer-CNN with boundary-awareness network for 3D medical image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCL-IKD: Intermediate knowledge distillation via supervised
contrastive representation learning. <em>APIN</em>, <em>53</em>(23),
28520–28541. (<a
href="https://doi.org/10.1007/s10489-023-05036-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation, which extracts dark knowledge from a deep teacher model to drive the learning of a shallow student model, is helpful in several tasks, including model compression and regularization. While previous research has focused on architecture-driven solutions for extracting information from the teacher models, these solutions are focused on a single task and fail to extract rich dark knowledge from large teacher networks in the presence of capacity gaps for broader applications. Hence, in this paper, we propose a supervised contrastive learning-based intermediate knowledge distillation (SCL-IKD) technique that is more effective in distilling knowledge from teacher networks to train a student model for classification tasks. SCL-IKD, unlike other approaches, is model agnostic and may be used in a variety of teacher-student cross-architectures. Investigations on several datasets reveal that SCL-IKD can achieve $$3-4\%$$ better top-1 accuracy over several state-of-the-art baselines. Furthermore, compared to the baselines, SCL-IKD is found better to handle capacity gaps between teacher and student models and is significantly more robust to symmetric noisy labels and data availability.},
  archive      = {J_APIN},
  author       = {Sharma, Saurabh and Lodhi, Shikhar Singh and Chandra, Joydeep},
  doi          = {10.1007/s10489-023-05036-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28520-28541},
  shortjournal = {Appl. Intell.},
  title        = {SCL-IKD: Intermediate knowledge distillation via supervised contrastive representation learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approach to occluded face recognition based on dynamic
image-to-class warping using structural similarity index. <em>APIN</em>,
<em>53</em>(23), 28501–28519. (<a
href="https://doi.org/10.1007/s10489-023-05026-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition in uncontrolled environments is a challenging problem in computer vision due to occlusion, pose, and illumination changes. While machine learning techniques address occluded face recognition, they require retraining when updating gallery images. The Dynamic Image-to-Class Warping (DICW) technique offers real-time recognition without training, maintaining the natural order of facial features (forehead, eyes, nose, mouth, and chin) to avoid disruptions caused by occlusion. DICW separates face image patches and integrates them into an ordered sequence through raster scanning. It computes the image-to-class distance between query and target faces using optimal warping paths along temporal and within-class dimensions. This paper proposes an improved face recognition approach using DICW and the Structural SIMilarity (SSIM) index, mitigating variations in illumination and contrast to match structural information. A technique for automatic face recognition from video sequences with DICW is also presented. Experiments on the AR Face Database, Chokepoint Database, and uncontrolled environment video sequences show that the proposed approach significantly improves the recognition rates for occluded images. The proposed approach achieved an improvement of around 5-6% in all considered cases compared to other state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Naseem, Shadab and Rathore, Santosh Singh and Kumar, Sandeep and Gangopadhyay, Sugata and Jain, Ankita},
  doi          = {10.1007/s10489-023-05026-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28501-28519},
  shortjournal = {Appl. Intell.},
  title        = {An approach to occluded face recognition based on dynamic image-to-class warping using structural similarity index},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enhanced wasserstein generative adversarial network with
gramian angular fields for efficient stock market prediction during
market crash periods. <em>APIN</em>, <em>53</em>(23), 28479–28500. (<a
href="https://doi.org/10.1007/s10489-023-05016-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the beginning of 2020, the COVID-19 pandemic caused a sharp decline in equity market indices, which remained stagnant for a considerable period. This resulted in significant losses for many investors. Despite extensive research on stock market prediction and the development of various effective models, there has been no specific effort to create a stable model during a financial crisis. Several studies have been conducted to forecast stock market trends and prices using advanced techniques like machine learning, deep learning, generative adversarial networks, and reinforcement learning. However, none of the existing forecasting models address the issue of market crashes, leading to substantial losses. We propose a GAF-EWGAN, a stacking ensemble model that combines enhanced WGANs with Gramian Angular Fields. This model demonstrates a high level of resilience during stock market crashes, effectively preventing investors from experiencing losses and generating significant profits. The GAF-EWGAN model achieved an average annual return of 16.49% across 20 selected stocks. Financial indicators indicate its reliability for real-world transactions.},
  archive      = {J_APIN},
  author       = {Ghasemieh, Alireza and Kashef, Rasha},
  doi          = {10.1007/s10489-023-05016-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28479-28500},
  shortjournal = {Appl. Intell.},
  title        = {An enhanced wasserstein generative adversarial network with gramian angular fields for efficient stock market prediction during market crash periods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining profitable alpha factors via convolution kernel
learning. <em>APIN</em>, <em>53</em>(23), 28460–28478. (<a
href="https://doi.org/10.1007/s10489-023-05014-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An automatic alpha factor mining method is proposed in this paper to assist expert traders in finding profitable alpha factors efficiently. Unlike finding qualified alpha factors by directly enumerating all possible combinations, the mining task is formulated as an iterative convolution kernel learning problem. Each kernel to be learned is associated with a unique alpha factor. To better solve the learning problem, the sparsity is introduced at the mutation step of the learning process to find simple and interpretable solutions efficiently and relieve the overfitting risks in real-world trading. A theorem is proposed to prove that the designed learning process can complete automatically in finite iterations as all convolution kernel vectors converge to zero vectors. In addition, a score function based on win rate, expected return and trade frequency is designed to evaluate the performance of market entry signals generated by the alpha factors practically. The convolution kernels with high score values are recorded and exported as the mined alpha factors. The experiment results show that the proposed method can achieve superior performance on both the China government bond dataset and the gold dataset.},
  archive      = {J_APIN},
  author       = {Shen, Zhenyi and Mao, Xiahong and Yang, Xiaohu and Zhao, Dan},
  doi          = {10.1007/s10489-023-05014-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28460-28478},
  shortjournal = {Appl. Intell.},
  title        = {Mining profitable alpha factors via convolution kernel learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STAR++: Rethinking spatio-temporal cross attention
transformer for video action recognition. <em>APIN</em>,
<em>53</em>(23), 28446–28459. (<a
href="https://doi.org/10.1007/s10489-023-04978-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video action recognition needs to model any differences by subdividing the spatio-temporal features to distinguish various actions. We propose rethinking spatio-temporal cross attention transformer (STAR++), a multi-modal transformer-based model that uses both RGB and skeleton information as an extended version of STAR-Transformer. STAR++ unifies the encoder-decoder structure of the base spatio-temporal cross attention transformer (STAR-Transformer) into an encoder structure and applies a new method of using interval attention as spatio-temporal cross attention. STAR++ provides interval attention from local features to global features as the layer deepens, allowing it to learn appropriately based on the transformer properties, improving the performance. In addition, STAR++ additionally proposes a deformable 3D token selection that can dynamically select and learn tokens for an attention operation such that tokens can be efficiently learned. The proposed STAR++ demonstrated competitive performance when compared with other state-of-the-art models using Penn action and NTU-RGB+D 60, 120, which are action recognition benchmark datasets. In addition, an ablation study was conducted to confirm that each proposed module has an essential effect on the performance improvement.},
  archive      = {J_APIN},
  author       = {Ahn, Dasom and Kim, Sangwon and Ko, Byoung Chul},
  doi          = {10.1007/s10489-023-04978-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28446-28459},
  shortjournal = {Appl. Intell.},
  title        = {STAR++: Rethinking spatio-temporal cross attention transformer for video action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Question difficulty estimation via enhanced directional
modality association transformer. <em>APIN</em>, <em>53</em>(23),
28434–28445. (<a
href="https://doi.org/10.1007/s10489-023-04988-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the difficulty of a question in video QAs is one of the important reasoning steps to answer the question. However, no previous question difficulty estimators consider the association between multiple modalities though video QA is intrinsically a multi-modal task involving both text and video. To solve this problem, this paper proposes a novel question difficulty estimator using an enhanced directional modality attention transformer (DiMAT++). The proposed estimator adopts a CNN backbone network and a transformer to express a video modality and RoBERTa to express a text modality. However, these modalities are insufficient to classify the difficulty level of a question correctly, since they affect each other during performing video QAs. Therefore, in the proposed estimator, DiMAT++ captures directional associations from text modality to video modality and vice versa. DiMAT, the previous version of DiMAT++, does not represent the sequential information for each modality though it is designed to express the directional associations. Thus, DiMAT++ revises it to accept the sequential representations of each modality as its input. The effectiveness of the proposed estimator is verified with two benchmark video QA data sets. The experimental results indicate that the proposed estimator outperforms three baselines of heterogeneous attention mechanism (HAM), multi-modal fusion transformer (MMFT), and DiMAT, which proves that DiMAT++ is effective in improving the performance of video question difficulty estimation.},
  archive      = {J_APIN},
  author       = {Kim, Bong-Min and Park, Gyu-Min and Park, Seong-Bae},
  doi          = {10.1007/s10489-023-04988-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28434-28445},
  shortjournal = {Appl. Intell.},
  title        = {Question difficulty estimation via enhanced directional modality association transformer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Temporal knowledge graph reasoning triggered by memories.
<em>APIN</em>, <em>53</em>(23), 28418–28433. (<a
href="https://doi.org/10.1007/s10489-023-05015-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of inferring missing facts using the temporal knowledge graph (TKG) is important and has been widely studied. Extrapolation in TKG inference is more challenging because no direct historical facts are available for predicting future events. Previous methods apply recursive embedding learning to solve the extrapolation problem. However, these methods primarily focus on recent historical facts and overlook the potential knowledge hidden in earlier facts. Moreover, the recursive embedding learning process can lead to imprecise knowledge propagation. To address these issues, we propose a memory-triggered decision-making (MTDM) network. It takes advantage of earlier historical facts to establish initial node representations and then updates node representations with recent historical facts. In particular, to enhance the prediction performance and efficiency, we update node representations of each timestamp in parallel using the proposed Res-GCN and then establish temporal correlations among these representations based on the designed control gating unit. To mitigate the issue of imprecise knowledge propagation, we leverage the facts most relevant to the missing knowledge to directly make predictions. This strategy improves the predictions by focusing on the most informative and contextual-free information. Additionally, we introduce the dissolution constraint, which enables us to analyse the process of event dissolution. Extensive experiments demonstrate that MTDM outperforms existing methods in terms of prediction accuracy and computational efficiency.},
  archive      = {J_APIN},
  author       = {Zhao, Mengnan and Zhang, Lihe and Kong, Yuqiu and Yin, Baocai},
  doi          = {10.1007/s10489-023-05015-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28418-28433},
  shortjournal = {Appl. Intell.},
  title        = {Temporal knowledge graph reasoning triggered by memories},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TCLN: A transformer-based conv-LSTM network for multivariate
time series forecasting. <em>APIN</em>, <em>53</em>(23), 28401–28417.
(<a href="https://doi.org/10.1007/s10489-023-04980-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of multivariate time series forecasting (MTSF) problems has high significance in many areas, such as industrial forecasting and traffic flow forecasting. Traditional forecasting models pay more attention to the temporal features of variables and lack depth in extracting spatial and spatiotemporal features between variables. In this paper, a novel model based on the Transformer, convolutional neural network (CNN), and long short-term memory (LSTM) network is proposed to address the issues. The model first extracts the spatial feature vectors through the proposed Multi-kernel CNN. Then it fully extracts the temporal information by the Encoder layer that consists of the Transformer encoder layer and the LSTM network, which can also obtain the potential spatiotemporal correlation. To extract more feature information, we stack multiple Encoder layers. Finally, the output is decoded by the Decoder layer composed of the ReLU activation function and the Linear layer. To further improve the model’s robustness, we also integrate an autoregressive model. In model evaluation, the proposed model achieves significant performance improvements over the current benchmark methods for MTSF tasks on four datasets. Further experiments demonstrate that the model can be used for long-horizon forecasting and achieve satisfactory results on the yield forecasting of test items (our private dataset, TIOB).},
  archive      = {J_APIN},
  author       = {Ma, Shusen and Zhang, Tianhao and Zhao, Yun-Bo and Kang, Yu and Bai, Peng},
  doi          = {10.1007/s10489-023-04980-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28401-28417},
  shortjournal = {Appl. Intell.},
  title        = {TCLN: A transformer-based conv-LSTM network for multivariate time series forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification model-based assisted preselection and
environment selection approach for evolutionary expensive bilevel
optimization. <em>APIN</em>, <em>53</em>(23), 28377–28400. (<a
href="https://doi.org/10.1007/s10489-023-04916-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilevel evolutionary algorithms (BLEAs) are a plausible approach for bilevel optimization. However, these algorithms require many fitness evaluations (FEs) and might become unusable if the fitness evaluations are computationally expensive. Therefore, reducing the number of FEs is crucial for designing a BLEA for expensive bilevel optimization (EBLOP). The surrogate-assisted optimization and knowledge transfer mechanisms used in BLEAs have been proven to reduce the number of FEs. This paper proposes a surrogate-assisted bilevel improved multioperator differential evolution algorithm (SA-BL-IMODE), which integrates a classification model-based assisted preselection and environment selection strategy (CPES) for EBLOP. In CPES, promising candidate solutions are prescreened by a classification model, filtering out some unpromising candidate solutions without performing FEs, thus improving the algorithm’s performance. Moreover, the classification model also assists environment selection by directly discarding the unpromising offspring solutions before FEs are performed, thus reducing the number of FEs in each iteration. Additionally, an enhanced direct neighbor solution transfer (EDST) mechanism is proposed to identify and utilize the interactions between upper-level and lower-level variables for acquiring knowledge, improving knowledge quality, and reducing the number of FEs further. Experimental studies on two test suite benchmark problems are conducted, and the proposed method is compared with nine state-of-the-art algorithms. The experimental results demonstrate the effectiveness of the proposed mechanisms and show that SA-BL-IMODE has a significant advantage over existing algorithms for expensive bilevel optimization.},
  archive      = {J_APIN},
  author       = {Lin, Libin and Liu, Ting and Leng, Jiewu and Yao, Shaowen and Zhang, Hao and Wei, Lijun and Liu, Qiang},
  doi          = {10.1007/s10489-023-04916-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28377-28400},
  shortjournal = {Appl. Intell.},
  title        = {Classification model-based assisted preselection and environment selection approach for evolutionary expensive bilevel optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contradiction neutralization for interpreting multi-layered
neural networks. <em>APIN</em>, <em>53</em>(23), 28349–28376. (<a
href="https://doi.org/10.1007/s10489-023-04883-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper aims to propose a new method for neutralizing contradictions in neural networks. Neural networks exhibit numerous contradictions in the form of contrasts, differences, and errors, making it extremely challenging to find a compromise between them. In this context, neutralization is introduced not to resolve these contradictions, but to weaken them by transforming them into more manageable and concrete forms. In this paper, contradictions are neutralized or weakened through four neutralization methods: comprehensive, nullified, compressive, and collective. Comprehensive neutralization involves increasing the neutrality of all components in a neural network. Nullified neutralization is employed to weaken contradictions among different computational and optimization procedures. Compressive neutralization aims to simplify multi-layered neural networks while preserving the original internal information as much as possible. Collective neutralization is achieved by considering as many final networks as possible under different conditions, inputs, learning steps, and so on. The proposed method was applied to two data sets, one of which consisted of irregular forms resulting from natural language processing. The experimental results demonstrate that comprehensive neutralization could enhance the neutrality of all components and represent features across a broader range of components, thereby improving generalization. Nullified neutralization enabled a compromise between neutrality maximization and error minimization. Through compressive and collective neutralization of a large number of compressed weights, it became possible to interpret compressed and collective weights. In particular, inputs that were considered relatively unimportant by conventional methods emerged as highly significant. Finally, these results were compared with those obtained in the field of the human-centered approach to provide a clearer understanding of the significance of contradiction resolution, applied to neural networks.},
  archive      = {J_APIN},
  author       = {Kamimura, Ryotaro},
  doi          = {10.1007/s10489-023-04883-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28349-28376},
  shortjournal = {Appl. Intell.},
  title        = {Contradiction neutralization for interpreting multi-layered neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-class partial hinge loss for partial label learning.
<em>APIN</em>, <em>53</em>(23), 28333–28348. (<a
href="https://doi.org/10.1007/s10489-023-04954-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important branch of weakly supervised learning, partial label learning (PLL) tackles the problem where each training instance is associated with a set of candidate labels, among only one is correct. Most existing PLL algorithms elaborately designed loss functions and update strategies to learn potential ground-truth labels among candidate labels with deep neural networks. However, these algorithms are susceptible to the cumulative error caused by noisy label propagation when updating label confidences, this will make the deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, we propose a general framework multi-class partial hinge loss (MPHL) for PLL, which can disambiguate the candidate labels by optimizing the margin between the maximum modeling output from partial labels and that from non-partial ones. More importantly, the partial hinge loss can adaptively optimize the separation hyperplane to reduce the influence of cumulative error. Meanwhile, we introduce graph laplacian regularization to full mine the relationship between candidate labels of similar instances to constrain the separation hyperplane to improve the robustness of disambiguation. Extensive experimental results demonstrate that the multi-class partial hinge loss significantly outperforms the state-of-the-art counterparts.},
  archive      = {J_APIN},
  author       = {Fan, Jinfu and Jiang, Zhencun and Xian, Yuanqing and Wang, Zhongjie},
  doi          = {10.1007/s10489-023-04954-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28333-28348},
  shortjournal = {Appl. Intell.},
  title        = {A multi-class partial hinge loss for partial label learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new bargaining solution for finite offer spaces.
<em>APIN</em>, <em>53</em>(23), 28310–28332. (<a
href="https://doi.org/10.1007/s10489-023-05009-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bargaining problem deals with the question of how far a negotiating agent should concede to its opponent. Classical solutions to this problem, such as the Nash bargaining solution (NBS), are based on the assumption that the set of possible negotiation outcomes forms a continuous space. Recently, however, we proposed a new solution to this problem for scenarios with finite offer spaces de Jonge and Zhang (Auton Agents Multi-Agent Syst 34(1):1–41, 2020). Our idea was to model the bargaining problem as a normal-form game, which we called the concession game, and then pick one of its Nash equilibria as the solution. Unfortunately, however, this game in general has multiple Nash equilibria and it was not clear which of them should be picked. In this paper we fill this gap by defining a new solution to the general problem of how to choose between multiple Nash equilibria, for arbitrary 2-player normal-form games. This solution is based on the assumption that the agent will play either ‘side’ of the game (i.e. as row-player or as column-player) equally often, or with equal probability. We then apply this to the concession game, which ties up the loose ends of our previous work and results in a proper, well-defined, solution to the bargaining problem. The striking conclusion, is that for rational and purely self-interested agents, in most cases the optimal strategy is to agree to the deal that maximizes the sum of the agents’ utilities and not the product of their utilities as the NBS prescribes.},
  archive      = {J_APIN},
  author       = {de Jonge, Dave},
  doi          = {10.1007/s10489-023-05009-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28310-28332},
  shortjournal = {Appl. Intell.},
  title        = {A new bargaining solution for finite offer spaces},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing LSTM and bi-LSTM models for crop yield prediction
and comparison of their performance with traditional machine learning
techniques. <em>APIN</em>, <em>53</em>(23), 28291–28309. (<a
href="https://doi.org/10.1007/s10489-023-05005-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advance prediction of crop yield is very critical in the context of ensuring food security as the region specific challenges in social and environmental conditions often infringe plan of policy makers. This study presents a generic methodology to configure and fine tune the state-of-the-art Long Short-Term Memory (LSTM) based Deep Learning (DL) model through hyperparameter optimization for prediction of yield (annual crop production) in Wheat, Groundnut and Barely over India based on multiple independent input variables identified using multicollinearity test. The Monte Carlo cross-validation method is used to validate the optimized LSTM models. Results from the LSTM model tuning showed that among the 4 optimizers tested, Adam was found to perform better irrespective of the crop and Bi-LSTM outperformed sLSTM in terms of prediction accuracy. The percentage reduction in error with Bi-LSTM compared to sLSTM in predicting wheat and groundnut crop yield was 39% and 13% respectively while in case of barley crop, error reduction was marginal (0.34%). The performance of optimized Bi-LSTM model is compared with the performance of traditional machine learning (ML) models such as support vector regression (SVR) and SVR polynomial {2nd and 3rd order}, Auto Regressive Integrated Moving Average (ARIMA) and ARIMAX (ARIMA with exogenous variables) and Vector Auto-regression (VAR). The Bi-LSTM model is found to be superior to ML models; the percentage reduction in mean absolute scaled error with the Bi-LSTM compared to the best performing ML model was 94%, 72%, and 71% in predicting wheat, groundnut and barley yield respectively. This study showed that by choosing proper explanatory (independent) variable and hyperparameter optimization, a simple (single layer) structure of deep neural network (LSTM) outperformed traditional ML models in terms of accuracy for crop yield prediction application.},
  archive      = {J_APIN},
  author       = {Kiran Kumar, V. and Ramesh, K. V. and Rakesh, V.},
  doi          = {10.1007/s10489-023-05005-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28291-28309},
  shortjournal = {Appl. Intell.},
  title        = {Optimizing LSTM and bi-LSTM models for crop yield prediction and comparison of their performance with traditional machine learning techniques},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fault diagnosis method for few-shot industrial processes
based on semantic segmentation and hybrid domain transfer learning.
<em>APIN</em>, <em>53</em>(23), 28268–28290. (<a
href="https://doi.org/10.1007/s10489-023-04979-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis of industrial processes plays an important role in avoiding heavy losses and ensuring production safety. Complex industrial processes often have many working conditions, and the actual industrial process often concentrates on certain working conditions. As a result, the running time of some working conditions is shorter, so the data of these conditions are difficult to obtain. However, almost all fault diagnosis methods based on Deep Learning (DL) requires a large amount of data. Therefore, it is a big challenge to realize the fault diagnosis of few-shot industrial processes. In order to solve these problems, this paper proposes a Deep Feature Transfer Fusion (DFTF) framework based on hybrid domain transfer learning. The purpose is to take few-shot working conditions as the target domain and carry out fault diagnosis for them. As the features of industrial process images are more complex, this paper introduces Pyramid Hybrid Vision Transformer (PHVT) model, which have stronger feature extraction capabilities and spatial perception, as feature extraction module. In order to improve the transferability of the model, this paper introduces the In-Cross Domain Hybrid Transfer Learning (ICTL) method. By fusing the general object features from ResNet50 which pre-trained under public dataset of common object and the features extracted from PHVT which pre-trained under industrial dataset of multiple working conditions, the adaptability of the model to different scenes is enhanced. The experimental results based on Pronto dataset of Process System Engineering lab of Cranfield University show that the proposed transfer learning method has excellent performance.},
  archive      = {J_APIN},
  author       = {Tian, Ying and Wang, Yiwei and Peng, Xin and Zhang, Wei},
  doi          = {10.1007/s10489-023-04979-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28268-28290},
  shortjournal = {Appl. Intell.},
  title        = {A fault diagnosis method for few-shot industrial processes based on semantic segmentation and hybrid domain transfer learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TDG4MSF: A temporal decomposition enhanced graph neural
network for multivariate time series forecasting. <em>APIN</em>,
<em>53</em>(23), 28254–28267. (<a
href="https://doi.org/10.1007/s10489-023-04987-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting is an important issue in industries, agriculture, finance, and other applications. There are many challenging problems in it such as non-linear and complicated relationships among the series. The entanglement of latent multiple different sequence patterns maybe one of the most reasons for the time series complex behavior, and decomposition can help reveal the hidden evolution law. Graph is a good modelling tool for multiple entities and graph neural network has showed better learning ability for spatial dependence, but its high memory consumption requires valid solutions. Inspired by the above two points, we propose a Temporal Decomposition enhanced Graph neural network for Multivariate time Series Forecasting, namely TDG4MSF, which mainly consists of four components: temporal decomposition enhanced representation learning, graph structure learning, gated GNNML-based representation learning and MLP-based forecasting. A progressive quadratic decomposition architecture is designed in the temporal decomposition enhanced representation learning that extracts the different periodic patterns of time series. Graph structure learning is used to construct adjacency matrix to represent the spatial topological structure. The temporal decomposition enhanced representation and adjacency matrix are fed into gated GNNML to integrating temporal and spatial information between variables. A MLP-based forecasting is utilized to make multivariate time series prediction. Experimental results show the effectiveness of our model in short- and medium-term prediction scenarios are superior to the state-of-the-art methods, which help make exact decisions timely. Code is available at: https://github.com/TYUT-Theta/MHZN.git .},
  archive      = {J_APIN},
  author       = {Miao, Hao and Zhang, Yilin and Ning, Zefei and Jiang, Zhuolun and Wang, Li},
  doi          = {10.1007/s10489-023-04987-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28254-28267},
  shortjournal = {Appl. Intell.},
  title        = {TDG4MSF: A temporal decomposition enhanced graph neural network for multivariate time series forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated few-shot learning for cough classification with
edge devices. <em>APIN</em>, <em>53</em>(23), 28241–28253. (<a
href="https://doi.org/10.1007/s10489-023-05006-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically classifying cough sounds is one of the most critical tasks for the diagnosis and treatment of respiratory diseases. However, collecting a huge amount of labeled cough dataset is challenging mainly due to high laborious expenses, data scarcity, and privacy concerns. In this work, our aim is to develop a framework that can effectively perform cough classification even in situations when enormous cough data is not available, while also addressing privacy concerns. Specifically, we formulate a new problem to tackle these challenges and adopt few-shot learning and federated learning to design a novel framework, termed F2LCough, for solving the newly formulated problem. We illustrate the superiority of our method compared with other approaches on COVID-19 Thermal Face &amp; Cough dataset, in which F2LCough achieves an average F1-Score of 86%. Our results show the feasibility of few-shot learning combined with federated learning to build a classification model of cough sounds. This new methodology is able to classify cough sounds in data-scarce situations and maintain privacy properties. The outcomes of this work can be a fundamental framework for building support systems for the detection and diagnosis of cough-related diseases.},
  archive      = {J_APIN},
  author       = {Hoang, Ngan Dao and Tran-Anh, Dat and Luong, Manh and Tran, Cong and Pham, Cuong},
  doi          = {10.1007/s10489-023-05006-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28241-28253},
  shortjournal = {Appl. Intell.},
  title        = {Federated few-shot learning for cough classification with edge devices},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable local interpretable model-agnostic explanations based
on a variational autoencoder. <em>APIN</em>, <em>53</em>(23),
28226–28240. (<a
href="https://doi.org/10.1007/s10489-023-04942-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For humans to trust in artificial intelligence (AI) systems, it is essential for machine learning (ML) models to be interpretable to users. For example, the judicial process requires that AI conclusions must be rigorous and absolutely interpretable. In this paper, we propose a novel approach, VAE-SLIME, for providing stable local interpretable model-agnostic explanations (SLIME) based on a variational autoencoder (VAE). LIME is a technique that explains the predictions of any classifier in an interpretable and faithful manner. Despite the great success of LIME, the most popular method in this category, it has several disadvantages due to its random perturbation-based sampling method. The VAE-SLIME proposed in this paper is specifically designed to address the lack of stability and local fidelity exhibited by LIME for tabular data. VAE-SLIME first employs fixed noise to replace the random Gaussian noise used by the reparameterization trick of the VAE. Then, it uses this new VAE model instead of random perturbation method to generate stable samples. By considering the sequential relationship and flipping of features, a novel explanation stability evaluation metric, the feature sequence stability index (FSSI), is introduced to accurately evaluate the stability of explanations. In a comparison with 6 state-of-the-art approaches on 7 commonly used tabular datasets, the experimental results show beyond doubt that the explanations produced by our approach are most stable, and its local fidelity is 65.17% higher than that of other approaches on average.},
  archive      = {J_APIN},
  author       = {Xiang, Xu and Yu, Hong and Wang, Ye and Wang, Guoyin},
  doi          = {10.1007/s10489-023-04942-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28226-28240},
  shortjournal = {Appl. Intell.},
  title        = {Stable local interpretable model-agnostic explanations based on a variational autoencoder},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for multi-agent formation navigation
with scalability. <em>APIN</em>, <em>53</em>(23), 28207–28225. (<a
href="https://doi.org/10.1007/s10489-023-05007-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the multi-agent formation obstacle avoidance (MAFOA) problem using multi-agent deep reinforcement learning (MADRL). MAFOA control aims to achieve and maintain a desired formation while avoiding collisions among agents or with obstacles. It is a research hotspot in multi-agent cooperation due to its wide applications and challenges. However, current MADRL methods face two major difficulties in solving this problem: 1) the high complexity and uncertainty of the environment when there are many agents; 2) the lack of scalability when the number of agents varies. To overcome these difficulties, we propose: 1) A local multi-agent deep deterministic policy gradient algorithm that allows each agent to learn from its local neighbors’ strategies during training and act independently during execution; 2) A reinforcement learning framework based on local information that uses partial observation as input and adapts to different numbers of agents; 3) A hybrid control method that switches between reinforcement learning and PID control to ensure formation stability. We evaluate our method on the multiagent particle environment environment and compare it with other algorithms to demonstrate its feasibility and superiority for solving the MAFOA problem.},
  archive      = {J_APIN},
  author       = {Gong, Yalei and Xiong, Hongyun and Li, MengMeng and Wang, Haibo and Nian, Xiaohong},
  doi          = {10.1007/s10489-023-05007-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28207-28225},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning for multi-agent formation navigation with scalability},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transaction-aware inverse reinforcement learning for trading
in stock markets. <em>APIN</em>, <em>53</em>(23), 28186–28206. (<a
href="https://doi.org/10.1007/s10489-023-04959-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training automated trading agents is a long-standing topic that has been widely discussed in artificial intelligence for the quantitative finance. Reinforcement learning (RL) is designed to solve the sequential decision-making tasks, like the stock trading. The output of the RL is the policy which can be presented as the probability values of the possible actions based on a given state. The policy is optimized by the reward function. However, even if the profit is considered as the natural reward function, a trading agent equipped with an RL model has several serious problems. Specifically, profit is only obtained after executing sell action, different profits exist at the same time step due to the varying-length transactions and the hold action deals with two opposite states, empty or nonempty position. To alleviate these shortcomings, in this paper, we introduce a new trading action called wait for the empty position status and design the appropriate rewards to all actions. Based on the new action space and reward functions, a novel approach named Transaction-aware Inverse Reinforcement Learning (TAIRL) is proposed. TAIRL rewards all trading actions for avoiding the reward bias and dilemma. TAIRL is evaluated by backtesting on 12 stocks of US, UK and China stock markets, and compared against other state-of-art RL methods and moving average trading methods. The experimental results show that the agent of TAIRL achieves the state-of-art performance in profitability and anti-risk ability.},
  archive      = {J_APIN},
  author       = {Sun, Qizhou and Gong, Xueyuan and Si, Yain-Whar},
  doi          = {10.1007/s10489-023-04959-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28186-28206},
  shortjournal = {Appl. Intell.},
  title        = {Transaction-aware inverse reinforcement learning for trading in stock markets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A multi-graph convolutional network based wearable human
activity recognition method using multi-sensors. <em>APIN</em>,
<em>53</em>(23), 28169–28185. (<a
href="https://doi.org/10.1007/s10489-023-04997-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable human activity recognition (WHAR) using multi-sensors is a promising research area in ubiquitous and wearable computing. Existing WHAR methods usually interact features learned from multi-sensor data by using convolutional neural networks or fully connected networks, which may ignore the prior relationships among multi-sensors. In this paper, we propose a novel method, called MG-WHAR, which employs graphs to model the relationships among multi-sensors. Specifically, we construct three types of graphs: a body structure based graph, a sensor modality based graph, and a data pattern based graph. In each graph, the nodes represent sensors, and the edges are set according to the relationships among sensors. MG-WHAR, utilizing a multi-graph convolutional network, conducts feature interactions by leveraging the relationships among multi-sensors. This strategy not only enhances model performance but also results in a model with fewer parameters. Compared to the state-of-the-art WHAR methods, our method increases weighted F1-score by 3.2% on Opportunity dataset, 1.9% on Realdisp dataset, and 2.6% on DSADS dataset, while maintaining lower computational complexity.},
  archive      = {J_APIN},
  author       = {Chen, Ling and Luo, Yingsong and Peng, Liangying and Hu, Rong and Zhang, Yi and Miao, Shenghuan},
  doi          = {10.1007/s10489-023-04997-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28169-28185},
  shortjournal = {Appl. Intell.},
  title        = {A multi-graph convolutional network based wearable human activity recognition method using multi-sensors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust stacked capsule autoencoder with hybrid
adversarial training. <em>APIN</em>, <em>53</em>(23), 28153–28168. (<a
href="https://doi.org/10.1007/s10489-023-05002-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule networks (CapsNets) are new neural networks that classify images based on the spatial relationships of features. By analyzing the pose of features and their relative positions, it is more capable of recognizing images after affine transformation. The stacked capsule autoencoder (SCAE) is a state-of-the-art CapsNet that achieved unsupervised classification of CapsNets for the first time. However, the security vulnerabilities and the robustness of the SCAE have rarely been explored. In this paper, we propose an evasion attack against SCAE, where the attacker can generate adversarial perturbations by reducing the contribution of the object capsules related to the original category of the image in the SCAE. Adversarial perturbations are then applied to the original images, and the perturbed images are misclassified with a high probability. For such an evasion attack, we further propose a defense method called hybrid adversarial training (HAT), which makes use of adversarial training and adversarial distillation to achieve better robustness of SCAE against the evasion attack. We evaluate the defense method and the experimental results show that the SCAE trained with HAT ensures that the model can maintain relatively high classification accuracy under the evasion attack and achieve similar classification accuracy to that of the original SCAE model on clean samples. The source code is available at https://github.com/FrostbiteXSW/SCAE_Defense .},
  archive      = {J_APIN},
  author       = {Dai, Jiazhu and Xiong, Siwei},
  doi          = {10.1007/s10489-023-05002-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28153-28168},
  shortjournal = {Appl. Intell.},
  title        = {Towards robust stacked capsule autoencoder with hybrid adversarial training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STemGAN: Spatio-temporal generative adversarial network for
video anomaly detection. <em>APIN</em>, <em>53</em>(23), 28133–28152.
(<a href="https://doi.org/10.1007/s10489-023-04940-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection and interpretation of abnormal events have become crucial tasks in large-scale video surveillance systems. The challenges arise from the lack of a clear definition of abnormality, which restricts the usage of supervised methods. To this end, we propose a novel unsupervised anomaly detection method, Spatio-Temporal Generative Adversarial Network (STemGAN). This framework consists of a generator and discriminator that learns from the video context, utilizing both spatial and temporal information to predict future frames. The generator follows an Autoencoder (AE) architecture, having a dual-stream encoder for extracting appearance and motion information, and a decoder having a Channel Attention (CA) module to focus on dynamic foreground features. In addition, we provide a transfer-learning method that enhances the generalizability of STemGAN. We use benchmark Anomaly Detection (AD) datasets to compare the performance of our approach with the existing state-of-the-art approaches using standard evaluation metrics, i.e., AUC (Area Under Curve) and EER (Equal Error Rate). The empirical results show that our proposed STemGAN outperforms the existing state-of-the-art methods achieving an AUC score of 97.5% on UCSDPed2, 86.0% on CUHK Avenue, 90.4% on Subway-entrance, and 95.2% on Subway-exit.},
  archive      = {J_APIN},
  author       = {Singh, Rituraj and Saini, Krishanu and Sethi, Anikeit and Tiwari, Aruna and Saurav, Sumeet and Singh, Sanjay},
  doi          = {10.1007/s10489-023-04940-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28133-28152},
  shortjournal = {Appl. Intell.},
  title        = {STemGAN: Spatio-temporal generative adversarial network for video anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rich feature representation and aggregation for
accurate visual tracking. <em>APIN</em>, <em>53</em>(23), 28114–28132.
(<a href="https://doi.org/10.1007/s10489-023-04998-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking is a key component of computer vision and has a wide range of practical applications. Recently, the tracking-by-segmentation framework has been widely applied in visual tracking due to its astonishing performance on accuracy. It attempts to learn from the framework of video object segmentation to realize accurate tracking. Although segmentation-based trackers are effective for target scale estimation, the segmentation network makes the trackers have high requirements for the extracted target features due to the need for pixel-level segmentation. Therefore, in this article, we propose a novel feature representation and aggregation network and introduce it into the tracking-by-segmentation framework to extract and integrate rich features for accurate and robust segmentation tracking. To be specific, firstly, the proposed approach models three complementary feature representations, including contextual semantic, local position, and structural patch feature representations, through cross-attention, cross-correlation and dilated involution mechanisms respectively. Secondly, these features are fused by a simple feature aggregation network. Thirdly, the fusion features are fed into the segmentation network to obtain accurate target state estimation. In addition, to adapt the segmentation network to the appearance changes and partial occlusion, we introduce a template update strategy and a bounding box refinement module for robust segmentation and tracking. The extensive experimental results on twelve challenging tracking benchmarks show that the proposed tracker outperforms most of the state-of-the-art trackers and achieves very promising tracking performance on the OTB100 and VOT2018 benchmarks.},
  archive      = {J_APIN},
  author       = {Yang, Yijin and Gu, Xiaodong},
  doi          = {10.1007/s10489-023-04998-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28114-28132},
  shortjournal = {Appl. Intell.},
  title        = {Learning rich feature representation and aggregation for accurate visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principal relation component reasoning-enhanced social
relation recognition. <em>APIN</em>, <em>53</em>(23), 28099–28113. (<a
href="https://doi.org/10.1007/s10489-023-05003-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social relationships (SRs) are the basis of human life. Hence, the ability to accurately recognize interpersonal relations in public spaces based on visual observations helps policymakers improve mental health programs and address social challenges. The key to image-based computer-vision research on SR recognition (SRR) is a deep-learning mechanism that can predict SRs based on the contents of visual scenery images. Current methods explore logical constraints using relatively simple scenes with small groups of people. However, this is insufficient when desiring to form relation graphs of multiple groups simultaneously from complex scenes. Generally, complex scenes contain a principal relationship that applies to the largest proportion of people, and secondary relationships apply to smaller proportions. To effectively explore relational situations in complex scenes, we propose a new distributed reasoning strategy that accounts for principal and secondary SRs. First, our novel model enhances principal relation component reasoning, and a new contrastive learning algorithm supplements the principal relationship with secondary types. A shifted-window transformer is applied to extract interactive human relation features and local-global features to support more accurate and comprehensive relation prediction. Extensive experiments demonstrate that each part of the proposed model improves the accuracy of SRR and that the whole model outperforms state-of-the-art methods on public datasets.},
  archive      = {J_APIN},
  author       = {Tang, Wang and Qing, Linbo and Li, Lindong and Guo, Li and Peng, Yonghong},
  doi          = {10.1007/s10489-023-05003-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28099-28113},
  shortjournal = {Appl. Intell.},
  title        = {Principal relation component reasoning-enhanced social relation recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-directional matrix completion for highly incomplete
multi-label learning via co-embedding predictive side information.
<em>APIN</em>, <em>53</em>(23), 28074–28098. (<a
href="https://doi.org/10.1007/s10489-023-05004-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by real-world applications such as recommendation systems and social networks where only “likes” or “friendships” are observed, we consider a challenging multi-label learning problem where the observed label consists only of positive and unlabeled entries and the feature matrix contains missing entries. The problem is an enhanced instance of PU (positive-unlabeled) learning. Due to highly incomplete data, traditional multi-label learning algorithms are not directly available in such a scenario. In this paper, we propose a bi-directional matrix completion approach that exploits the matrix low-rank property to recover missing feature entries and label entries. Specifically, we introduce a low-rank co-embedding framework that integrates a low-rank matrix complete model and prediction model to jointly recover missing entries. In our framework, the prediction model can be conducted efficiently by dividing the low-rank matrix into a part capturing feature information and a part capturing information outside the feature space. Furthermore, we incorporate label embedding and graph regularized embedding together to improve matrix completion performance, which not only takes feature graph-structured information into account but also simultaneously exploits label semantic structured information. We provide an efficient alternative minimization scheme to solve the proposed problem. The experiments on transductive and inductive incomplete multi-label learning demonstrate the effectiveness of our proposed approach.},
  archive      = {J_APIN},
  author       = {Xia, Yuelong and Tang, Mingjing and Wang, Pei},
  doi          = {10.1007/s10489-023-05004-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28074-28098},
  shortjournal = {Appl. Intell.},
  title        = {Bi-directional matrix completion for highly incomplete multi-label learning via co-embedding predictive side information},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive graph regularized non-negative matrix factorization
with self-weighted learning for data clustering. <em>APIN</em>,
<em>53</em>(23), 28054–28073. (<a
href="https://doi.org/10.1007/s10489-023-04868-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, fully exploiting the local structure of the original data space can effectively improve the clustering performance of nonnegative matrix factorization (NMF). Therefore, graph-based NMF algorithms have been widely studied and applied. However, traditional graph-based NMF methods generally employ predefined models to construct similarity graphs, so that the clustering results depend heavily on the quality of the similarity graph. Furthermore, most of these methods follow the ideal assumption that the importance of different features is equal in the process of learning the similarity matrix, which results in irrelevant features being valued. To alleviate the above issues, this paper develops an adaptive graph regularized nonnegative matrix factorization with self-weighted learning (SWAGNMF) method. Firstly, the proposed method learns the similarity matrix flexibly and adaptively to explore the local structure of samples based on the assumption that data points with smaller distances should have a higher probability of adjacency. Furthermore, the self-weight matrix assigns different weights automatically according to the importance of features in the process of constructing similarity graph, i.e., discriminative features are assigned more significant weights than redundant features, which can effectively suppress irrelevant features and enhance the robustness of our model. Finally, considering the duality between samples and features, the proposed method is capable of exploring the local structures of both the data space and the feature space. An effective alternative optimization algorithm is proposed, and convergence is theoretically guaranteed. Extensive experiments on benchmark and synthetic datasets show that the proposed method outperforms compared state-of-the-art clustering methods.},
  archive      = {J_APIN},
  author       = {Ma, Ziping and Wang, Jingyu and Li, Huirong and Huang, Yulei},
  doi          = {10.1007/s10489-023-04868-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28054-28073},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive graph regularized non-negative matrix factorization with self-weighted learning for data clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DIG: Dual interaction and guidance network for salient
object detection. <em>APIN</em>, <em>53</em>(23), 28039–28053. (<a
href="https://doi.org/10.1007/s10489-023-04982-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of salient object detection (SOD) research is to obtain rich and effective features for characterizing objects. Recent progress on SOD mainly aims at exploiting how to effectively extract and integrate multilevel object features in a SOD network. In some cases, existing approaches mainly consider the cross-layer or cross-modal information fusion in the decoder. What’s more, they extract global context information from the last encoder layer and directly integrate them with outputs of each preceding layer, which pay less attention to refine the global context features and explore ways to overcome the scale variability of salient objects. These may limit the ability of the model to represent salient object to a certain extent. Based on this observation, we propose a novel Dual Interaction and Guidance module (namely DIG) for better leveraging the information of various encoder-outputs and global context information. Specifically, a Dual Interaction Module (DIM) is proposed to realize two-level-granularity feature interaction through a nested residual-like structure. According to the global context features, we design a Global Context Feature Enhancement (GCIE) and Global Guidance Module (GGM) to mine the deep information hidden in the original global information and effective guide feature fusion respectively. Embedding the DIG into a encode-decoder salient detection framework has greatly improved the characterization ability to salient objects. Through extensive experimental results on six benchmark datasets demonstrate that the proposed algorithm performs favorably against most state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Jia, Ning and Chen, Yufei and Liu, Xianhui and Wang, Hui},
  doi          = {10.1007/s10489-023-04982-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28039-28053},
  shortjournal = {Appl. Intell.},
  title        = {DIG: Dual interaction and guidance network for salient object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving relation classification effectiveness by alternate
distillation. <em>APIN</em>, <em>53</em>(23), 28021–28038. (<a
href="https://doi.org/10.1007/s10489-023-04964-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of neural networks, more and more complex and excellent relation classification models are constantly proposed. Although they can be compressed by some model compression methods at the cost of effectiveness, they are still insufficient to deploy on resource-constrained devices. Knowledge distillation can transfer the excellent predictive abilities of superior models to lightweight models, but the gap between models limits its effects. Due to the huge gaps between relation classification models, it is painstakingly difficult to select and train a superior teacher model to guide student models when we use knowledge distillation to get a lightweight model. Therefore, how to obtain a lightweight relation classification model with high effectiveness is still a hot research topic. In this paper, we construct an alternate distillation framework with three modules. The weight adaptive external distillation module is built based on an adaptive weighting module based on cosine similarity. The progressive internal distillation module allows the model to be its own teacher to guide its own training. Finally, a combination module based on the attention mechanism combines the above two modules. On SemEval-2010 Task 8 and WiKi80 datasets, we demonstrate the great effect of our approach on improving the relation classification effectiveness of lightweight models. The complex relation classification models compressed at the cost of effectiveness are still insufficient to deploy on resource-constrained devices. Besides, due to the significant differences between relation classification models, it is challenging to find a suitable teacher model for knowledge distillation. In this paper, we propose an alternate distillation framework (including external distillation and internal distillation) to obtain lightweight relation classification models with high effectiveness. Our approach effectively transfers the excellent predictive capability of complex models to lightweight models even when there is a significant gap between them},
  archive      = {J_APIN},
  author       = {Wang, Zhaoguo and Li, Kai and Ye, Yuxin},
  doi          = {10.1007/s10489-023-04964-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {28021-28038},
  shortjournal = {Appl. Intell.},
  title        = {Improving relation classification effectiveness by alternate distillation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical interactive multi-granularity co-attention
embedding to improve the small infrared target detection. <em>APIN</em>,
<em>53</em>(23), 27998–28020. (<a
href="https://doi.org/10.1007/s10489-023-04958-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small Infrared Target Detection (SITD) aims to separate the information of small targets from complex background clutter. Infrared radiation decays with distance, making the appearance information of small targets very limited and highly blurry, and easily submerging in complex background environments. Due to the fact that local areas similar to infrared dim and small targets which may spread throughout the entire background, it is crucial to mine the diversity differences between the target and background to obtain accurate target contrast information for robust detection. Therefore, a deep network framework that covers feature enhancement, interaction, and comparison is proposed in this thesis for SITD. Specially, the patch-based attention module is first designed to enhance the embedding of local context information from different layer features. Then, multi-granularity co-attention embedding modules are designed in a parallel manner to explore interactions and alignments at corresponding spatial locations in the cross-layer features, thus enhancing the consistent representation of small infrared targets. Next, hierarchical attention fusion module is constructed to discover the association and contrast among cross-layer aggregation features of three different granularity in corresponding spatial positions, thus highlighting the distinguishability between small targets and complex backgrounds with strong interference. Moreover, the optimization of multi-objective loss is designed not only to assist the feature learning of small targets on different layer space, but also to improve the distribution consistency of cross-layer aligned features. Extensive experiments on two publicly datasets and a constructed maritime dataset show that our method is effective and can achieve better detection effect compared with the current state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Lv, Guangrui and Dong, Lili and Xu, Wenhai},
  doi          = {10.1007/s10489-023-04958-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {27998-28020},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical interactive multi-granularity co-attention embedding to improve the small infrared target detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Particle filter meets hybrid octrees: An octree-based ground
vehicle localization approach without learning. <em>APIN</em>,
<em>53</em>(23), 27982–27997. (<a
href="https://doi.org/10.1007/s10489-023-04622-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an accurate lidar-based outdoor localization method that requires few computational resources, is robust in challenging environments (urban, off-road, seasonal variations) and whose performances are equivalent for two different sensor technologies: scanning LiDAR and flash LiDAR. The method is based on the matching between a pre-built 3D map and the LiDAR measurements. Our contribution lies in the combined use of a particle filter with a hybrid octree to reduce the memory footprint of the map and significantly decrease the computational load for online localization. The design of the algorithm allows it to run on both CPU and GPU with equivalent performance. We have evaluated our approach on the KITTI dataset and obtained good results compared to the state of the art. This paper introduces the baseline performance on a multi-seasonal dataset we are publicly releasing to the community. We have shown that the same localization algorithms and parameters can perform well in urban environments and can be extended to off-road environments. We have also evaluated the robustness of our method when masking angular sectors of the LiDAR field of view to reproduce edge-cases scenarios in urban environments where the LiDAR field is partially occulted by another vehicle (bus, truck). Finally, experiments have been carried out with two distinctive scanning and flash LiDAR technologies. The performance achieved with the flash LiDAR is close to the scanning LiDAR despite different resolutions and sensing modalities. The positioning performance is significant with 10cm and 0.12° angular RMSE for both technologies. We validated our approach in an off-road environment from a front view field of view with only 768 LiDAR points.},
  archive      = {J_APIN},
  author       = {Vauchey, Vincent and Dupuis, Yohan and Merriaux, Pierre and Savatier, Xavier},
  doi          = {10.1007/s10489-023-04622-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {27982-27997},
  shortjournal = {Appl. Intell.},
  title        = {Particle filter meets hybrid octrees: An octree-based ground vehicle localization approach without learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate gradient scaling for directly training spiking
neural networks. <em>APIN</em>, <em>53</em>(23), 27966–27981. (<a
href="https://doi.org/10.1007/s10489-023-04966-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are considered to be biologically plausible and can yield high energy efficiency when implemented on neuromorphic hardware due to their highly sparse asynchronous binary event-driven nature. Recently, surrogate gradient (SG) approaches have enabled SNNs to be trained from scratch with backpropagation (BP) algorithms under a deep learning framework. However, a popular SG approach known as straight-through estimator (STE), which only propagates the same gradient information, does not take into account the activation differences between the membrane potentials and output spikes. To address this issue, we propose surrogate gradient scaling (SGS), which scales up or down the gradient information of the membrane potential according to the sign of the gradient of the spiking neuron output and the difference between the membrane potential and the output of the spiking neuron. This SGS approach can also be applied to unimodal functions that propagate different gradient information from the output spikes to the input membrane potential. In addition, SNNs trained directly from scratch suffer from poor generalization performance, and we introduce Lipschitz regularization (LR), which is incorporated into the loss function. It not only improves the generalization performance of SNNs but also makes them more robust to noise. Extensive experimental results on several popular benchmark datasets (CIFAR10, CIFAR100 and CIFAR10-DVS) show that our approach not only outperforms the SOTA but also has lower inference latency. Remarkably, our SNNs can lead to 34 $$\times $$ , 29 $$\times $$ , and 17 $$\times $$ computation energy savings compared to standard Artificial neural networks (ANNs) on above three datasets.},
  archive      = {J_APIN},
  author       = {Chen, Tao and Wang, Shu and Gong, Yu and Wang, Lidan and Duan, Shukai},
  doi          = {10.1007/s10489-023-04966-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {27966-27981},
  shortjournal = {Appl. Intell.},
  title        = {Surrogate gradient scaling for directly training spiking neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Remote photoplethysmography (rPPG) based learning fatigue
detection. <em>APIN</em>, <em>53</em>(23), 27951–27965. (<a
href="https://doi.org/10.1007/s10489-023-04926-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG), which uses a facial video to measure skin reflection variations, is a contactless method for monitoring human cardiovascular activity. Due to its simplicity, convenience and potential in large-scale application, rPPG has gained more attention over the decade. However, the accuracy, reliability, and computational complexity have not reached the expected standards, thus rPPG has a very limited application in the educational field. In order to alleviate this issue, this study proposes an rPPG-based learning fatigue detection system, which consists of the following three modules. First, we propose an rPPG extraction module, which realizes real-time pervasive biomedical signal monitoring. Second, we propose an rPPG reconstruction module, which evaluates heart rate using a hybrid of 1D and 2D deep convolutional neural network approach. Third, we propose a learning fatigue classification module based on multi-source feature fusion, which classifies a learner’s state into non-fatigue and fatigue. In order to verify the performance, the proposed system is tested on a self-collected dataset. Experimental results demonstrate that (i) the accuracy of heart rate evaluation is better than the cutting-edge methods; and (ii) based on both the subject-dependent and independent cross validations, the proposed system succeeded in not only learning person-independent features for fatigue detection but also detecting early fatigue with very high accuracy.},
  archive      = {J_APIN},
  author       = {Zhao, Liang and Zhang, Xinyu and Niu, Xiaojing and Sun, Jianwen and Geng, Ruonan and Li, Qing and Zhu, Xiaoliang and Dai, Zhicheng},
  doi          = {10.1007/s10489-023-04926-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {27951-27965},
  shortjournal = {Appl. Intell.},
  title        = {Remote photoplethysmography (rPPG) based learning fatigue detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive local recalibration network for scene recognition.
<em>APIN</em>, <em>53</em>(23), 27935–27950. (<a
href="https://doi.org/10.1007/s10489-023-04963-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is a computer vision task that categorizes scenes from photographs. In this paper, we introuduce the Adaptive Local Recalibration Network (ALR-Net), a novel scene recognition method based on convolutional neural networks (CNNs). In comparison to the object classification task, the scene classification images have a more dispersed distribution of information. To solve this issue, we suggest an attention mechanism for locating the discriminative regions for scene recognition. Along with normal data augmentation, we use the regions to guide two additional data augmentation approaches, namely adaptive cropping and adaptive hiding, in order to capture local information more efficiently and specifically. Attention maps are also used to adaptively recalibrate scene feature maps so that discriminative regions receive more attention than others. In addition, we bring in a scene distribution label for each image, which is used to assist the training of attention maps. Extensive studies on two scene recognition benchmarks verified the proposed model’s effectiveness: MIT67 (88.37%) and SUN397 (74.24%).},
  archive      = {J_APIN},
  author       = {Wang, Jiale and Zou, Lian and Fan, Cien and Jiang, Hao and Chen, Liqiong and Cheng, Mofan and Yu, Hu and Liu, Yifeng},
  doi          = {10.1007/s10489-023-04963-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {23},
  pages        = {27935-27950},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive local recalibration network for scene recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain consensual contrastive learning for few-shot
universal domain adaptation. <em>APIN</em>, <em>53</em>(22),
27191–27206. (<a
href="https://doi.org/10.1007/s10489-023-04890-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional unsupervised domain adaptation (UDA) aims to transfer the learned knowledge from a fully labeled source domain to another unlabeled target domain on the same label set. The strong assumptions of full annotations on the source domain and a closed label set of the two domains might not hold in real-world applications. In this paper, we investigate a practical but challenging domain adaptation scenario, termed few-shot universal domain adaptation (FUniDA), where only a few labeled data are available in the source domain and the label sets of the source and target domains are different. Existing few-shot UDA (FUDA) methods and universal domain adaptation (UniDA) methods cannot address this novel domain adaptation setting well. The FUDA methods would misalign the unknown samples of the target domain and the private samples of the source domain, and the UniDA methods cannot perform well with only a small number of labeled source samples. To address these challenges, we propose a novel domain consensual contrastive learning (DCCL) framework for FUniDA. Specifically, DCCL comprises two major components: 1) in-domain consensual contrastive learning aims to learn discriminative features from few labeled source data, and 2) cluster matching and cross-domain consensual contrastive learning aim to align the features of common samples in the source and target domains while keeping the private samples as private. We conduct extensive experiments on five standard benchmark datasets, including Office-31, Office-Home, VisDA-17, DomainNet, and ImageCLEF-DA. The results demonstrate that the proposed DCCL achieves state-of-the-art performance with remarkable gains.},
  archive      = {J_APIN},
  author       = {Liao, Haojin and Wang, Qiang and Zhao, Sicheng and Xing, Tengfei and Hu, Runbo},
  doi          = {10.1007/s10489-023-04890-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27191-27206},
  shortjournal = {Appl. Intell.},
  title        = {Domain consensual contrastive learning for few-shot universal domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSDTI: An interpretable cross-attention network with
GNN-based drug molecule aggregation for drug-target interaction
prediction. <em>APIN</em>, <em>53</em>(22), 27177–27190. (<a
href="https://doi.org/10.1007/s10489-023-04977-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug-target interaction (DTI) is a critical and complex process that plays a vital role in drug discovery and design. In deep learning-based DTI methods, graph neural networks (GNNs) are employed for drug molecule modeling, attention mechanisms are utilized to simulate the interaction between drugs and targets. However, existing methods still face two limitations in these aspects. First, GNN primarily focus on local neighboring nodes, making it difficult to capture the global 3D structure and edge information. Second, the current attention-based methods for modeling drug-target interactions lack interpretability and do not fully utilize the deep representations of drugs and targets. To address the aforementioned issues, we propose an interpretable network architecture called CSDTI. It utilizes a cross-attention mechanism to capture the interaction features between drugs and targets. Meanwhile, we design a drug molecule aggregator to capture high-order dependencies within the drug molecular graph. These features are then utilized simultaneously for downstream tasks. Through rigorous experiments, we have demonstrated that CSDTI outperforms state-of-the-art methods in terms of performance metrics such as AUC, precision, and recall in DTI prediction tasks. Furthermore, the visualization mapping of attention weights indicates that CSDTI can provide chemical insights even without external knowledge.},
  archive      = {J_APIN},
  author       = {Pan, Yaohua and Zhang, Yijia and Zhang, Jing and Lu, Mingyu},
  doi          = {10.1007/s10489-023-04977-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27177-27190},
  shortjournal = {Appl. Intell.},
  title        = {CSDTI: An interpretable cross-attention network with GNN-based drug molecule aggregation for drug-target interaction prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fine-grained causality extraction model incorporating
relative location coding. <em>APIN</em>, <em>53</em>(22), 27163–27176.
(<a href="https://doi.org/10.1007/s10489-023-04970-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular methods of causality extraction work well for simple and explicit single causal relations, but it remains challenging to extract causal relations from the complex sentences of natural texts due to ambiguity concerning the locations of the causal subject and object as well as the complexity of the relevant dependencies. To solve these problems, this paper proposes a five-tuple annotation scheme that defines a scoring function to iteratively parse out the causal pairs from multiple entity pairs in a sentence, and to thus transform the task of extracting causal relations into that of automatic annotation. First, this study uses this scheme to propose a multi-headed, self-attentive mechanism that incorporates encoded information on relative position to increase the capability of the model to perceive causal features. Second, the authors combine information from a dependency tree while assigning the appropriate weights, and finally use a bidirectional GCN network to parse the weights of features of the tree from multiple perspectives and splice the dependency-related features. This joint model of extraction improves the bounds of cause–effect pairs of entities while considering the dependency relationships between them, which renders the extracted, fine-grained causal terms more accurate. Experiments on the SemEval 2010 task 8 and the ADE datasets show that our approach significantly outperforms prevalent methods in terms of the accuracy of solving complex causal extraction compared with state-of-the-art approaches to modeling.},
  archive      = {J_APIN},
  author       = {Wan, Weibing and Chen, Yang and Gao, Yongbin and Shao, Chen and Zhao, Yuming},
  doi          = {10.1007/s10489-023-04970-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27163-27176},
  shortjournal = {Appl. Intell.},
  title        = {A fine-grained causality extraction model incorporating relative location coding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-objective parameter setting problem of a genetic
algorithm: An empirical study on traveling salesperson problem.
<em>APIN</em>, <em>53</em>(22), 27148–27162. (<a
href="https://doi.org/10.1007/s10489-023-04972-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic Algorithm (GA) is a widely used metaheuristic for addressing challenging optimization problems. Selecting suitable settings for GA parameters can lead to a considerable improvement in its performance, but this is often difficult due to the larger number of alternatives available and variable performance depending on the problem solved. Furthermore, consideration of multiple performance criteria in parameter selection adds an additional layer of complexity. Therefore, practitioners often rely on their previous experiences or perform trial-and-error experiments when determining suitable parameter settings. In this study, we define the problem of finding suitable settings for GAs as a bi-objective optimization problem with an aim to find efficient settings that will maximize the approximation quality and minimize the run time of the GA. We conduct an empirical study on a GA that solves Traveling Salesperson Problem (TSP). Two evolutionary algorithms are utilized in a nested manner to address the bi-objective parameter setting problem: a GA to solve the TSP instances and a multi-objective evolutionary algorithm to find the bi-objective efficient settings of the GA. We find efficient settings for each instance through an empirical study with 31 TSP instances and identify settings that perform well regardless of the instance solved. The results provide valuable insights into the behavior of efficient settings, demonstrating that some operators and parameters are robust to changes in problem size, while others require adjustment.},
  archive      = {J_APIN},
  author       = {Akduran, Yavuzhan and Dasdemir, Erdi and Testik, Murat Caner},
  doi          = {10.1007/s10489-023-04972-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27148-27162},
  shortjournal = {Appl. Intell.},
  title        = {Bi-objective parameter setting problem of a genetic algorithm: An empirical study on traveling salesperson problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAC-bayesian offline meta-reinforcement learning.
<em>APIN</em>, <em>53</em>(22), 27128–27147. (<a
href="https://doi.org/10.1007/s10489-023-04911-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-reinforcement learning (Meta-RL) utilizes shared structure among tasks to enable rapid adaptation to new tasks with only a little experience. However, most existing Meta-RL algorithms lack theoretical generalization guarantees or offer such guarantees under restrictive assumptions (e.g., strong assumptions on the data distribution). This paper for the first time conducts a theoretical analysis for estimating the generalization performance of the Meta-RL learner using the PAC-Bayesian theory. The application of PAC-Bayesian theory to Meta-RL poses a challenge due to the existence of dependencies in the training data, which renders the independent and identically distributed (i.i.d.) assumption invalid. To address this challenge, we propose a dependency graph-based offline decomposition (DGOD) approach, which decomposes non-i.i.d. Meta-RL data into multiple offline i.i.d. datasets by utilizing the techniques of offline sampling and graph decomposition. With the DGOD approach, we derive the practical PAC-Bayesian offline Meta-RL generalization bounds and design an algorithm with generalization guarantees to optimize them, called PAC-Bayesian Offline Meta-Actor-Critic (PBOMAC). The results of experiments conducted on several challenging Meta-RL benchmarks demonstrate that our algorithm performs well in avoiding meta-overfitting and outperforms recent state-of-the-art Meta-RL algorithms without generalization bounds.},
  archive      = {J_APIN},
  author       = {Sun, Zheng and Jing, Chenheng and Guo, Shangqi and An, Lingling},
  doi          = {10.1007/s10489-023-04911-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27128-27147},
  shortjournal = {Appl. Intell.},
  title        = {PAC-bayesian offline meta-reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). INNES: An intelligent network penetration testing model
based on deep reinforcement learning. <em>APIN</em>, <em>53</em>(22),
27110–27127. (<a
href="https://doi.org/10.1007/s10489-023-04946-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penetration testing (PT) is a crucial way to ensure the security of computer systems. However, it requires a high threshold and can only be implemented by trained experts. Automated tools can reduce the pressure of talent shortages, and reinforcement learning (RL) is a promising approach for achieving automated PT. Due to the unreasonable characterization of the PT process and the low efficiency of RL data, the applicability of the model is limited, and it is difficult to reuse, which hinders its practical application. In this paper, we propose an INNES (INtelligent peNEtration teSting) model based on deep reinforcement learning (DRL). First, the model characterizes the key elements of PT more reasonably based on the Markov decision process (MDP), fully considering the commonality of the PT process in different scenarios to improve its applicability. Second, the DQN_valid algorithm is designed to constrain the agent’s action space, to improve the agent’s decision-making accuracy, and avoid invalid exploration, according to the feature that enables the effective action space to gradually increase during the PT process. The experimental results show that our model is not only effective for automated PT in the network environment but also has portability, which provides a possible future direction for practical application of intelligent PT based on RL.},
  archive      = {J_APIN},
  author       = {Li, Qianyu and Hu, Miao and Hao, Hao and Zhang, Min and Li, Yang},
  doi          = {10.1007/s10489-023-04946-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27110-27127},
  shortjournal = {Appl. Intell.},
  title        = {INNES: An intelligent network penetration testing model based on deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic music emotion classification model for movie
soundtrack subtitling based on neuroscientific premises. <em>APIN</em>,
<em>53</em>(22), 27096–27109. (<a
href="https://doi.org/10.1007/s10489-023-04967-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of music to induce emotions has been arousing a lot of interest in recent years, especially due to the boom in music streaming platforms and the use of automatic music recommenders. Music Emotion Recognition approaches are based on combining multiple audio features extracted from digital audio samples and different machine learning techniques. In these approaches, neuroscience results on musical emotion perception are not considered. The main goal of this research is to facilitate the automatic subtitling of music. The authors approached the problem of automatic musical emotion detection in movie soundtracks considering these characteristics and using scientific musical databases, which have become a reference in neuroscience research. In the experiments, the Constant-Q-Transform spectrograms, the ones that best represent the relationships between musical tones from the point of view of human perception, are combined with Convolutional Neural Networks. Results show an efficient emotion classification model for 2-second musical audio fragments representative of intense basic feelings of happiness, sadness, and fear. Those emotions are the most interesting to be identified in the case of movie music captioning. The quality metrics have demonstrated that the results of the different models differ significantly and show no homogeneity. Finally, these results pave the way for an accessible and automatic captioning of music, which could automatically identify the emotional intent of the different segments of the movie soundtrack.},
  archive      = {J_APIN},
  author       = {Lucia-Mulas, Maria Jose and Revuelta-Sanz, Pablo and Ruiz-Mezcua, Belen and Gonzalez-Carrasco, Israel},
  doi          = {10.1007/s10489-023-04967-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27096-27109},
  shortjournal = {Appl. Intell.},
  title        = {Automatic music emotion classification model for movie soundtrack subtitling based on neuroscientific premises},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High utility pattern mining algorithm over data streams
using ext-list. <em>APIN</em>, <em>53</em>(22), 27072–27095. (<a
href="https://doi.org/10.1007/s10489-023-04925-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility pattern has received a lot of research and attention because of their wide range of application scenarios. How to efficiently mine high utility patterns over data streams has become an important issue in the field of data mining. To solve the problem that the traditional utility list structure has too many join operations and the join operation is not efficient, which leads to the low spatio-temporal efficiency of the algorithm and the problem that the sliding window model repeatedly generates the same resultset, a new algorithm for high utility pattern mining over data streams is proposed, named HUPM_Stream. A location-indexed list structure, Ext-list, is designed to reduce the time complexity of the utility list join operation, and an improved remaining utility pruning strategy IRS is proposed to reduce the number of utility list join operations, and a hash table structure-based resultset maintenance strategy HRS is designed to effectively reduce the search space of the algorithm and avoid repeatedly generating the same resultset during the sliding process of the window. A large number of experimental results show that the proposed algorithm has better performance on dense datasets.},
  archive      = {J_APIN},
  author       = {Han, Meng and Li, Muhang and Chen, Zhiqiang and Wu, Hongxin and Zhang, Xilong},
  doi          = {10.1007/s10489-023-04925-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27072-27095},
  shortjournal = {Appl. Intell.},
  title        = {High utility pattern mining algorithm over data streams using ext-list.},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). FedPJF: Federated contrastive learning for
privacy-preserving person-job fit. <em>APIN</em>, <em>53</em>(22),
27060–27071. (<a
href="https://doi.org/10.1007/s10489-023-04775-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The person-job fit algorithm has become a crucial task in the online recruitment industry for matching resumes with suitable jobs and making recommendations. However, individuals’ resumes and users’ interaction records contain personal privacy information, which resumes and records cannot use in a public environment. This paper proposes a person-job fit framework with federated learning and privacy protection. It can utilize the user data stored locally on the user-side to train the model and ensure that the user’s private data are not uploaded to the server-side. In addition, it prevents users’ uploaded gradients from revealing private information by applying adaptive differential privacy techniques. However, federated learning suffers from a nonindependent homogeneous distribution of user data, which can lead to a user-side learned job representation with its own bias. Therefore, we introduce the idea of contrastive learning to guide user-side training to alleviate user-side bias and improve model performance. Experiments on real datasets show that our framework ensures higher performance while preserving user privacy data compared to typical person-job fit methods.},
  archive      = {J_APIN},
  author       = {Zhang, Yunchong and Liu, Baisong and Qian, Jiangbo},
  doi          = {10.1007/s10489-023-04775-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27060-27071},
  shortjournal = {Appl. Intell.},
  title        = {FedPJF: Federated contrastive learning for privacy-preserving person-job fit},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source identification of weak audio signals using attention
based convolutional neural network. <em>APIN</em>, <em>53</em>(22),
27044–27059. (<a
href="https://doi.org/10.1007/s10489-023-04973-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the source of a weak sound signal can be difficult, particularly in busy or noisy surroundings. The hand-engineered characteristics and algorithms used in traditional methods for source separation and localization are irresistible to changes in the signal or the noise. In environmental sound classification(ESC) the effectiveness of representative features collected from the environmental sounds is crucial to the classification performance. However, semantically meaningless frames and silent frames frequently hinder the performance of ESC. In order to address this problem, we proposed a new context-aware attention-based neural network for weak environmental sound source identification. Our method is based on the idea that the attention mechanism will be able to concentrate on important elements of the input signal and suppress irrelevant ones, enabling the network to locate the weak sound’s source more effectively. To solve the limited capacity problem faced by the attention maps of the attention model we are using context information as additional input. The identification of weak signals and finding corresponding context information is also a challenging problem because of the degradation or noise in the signal. To solve this issue we proposed a novel algorithm based on MFCC for context feature generation. Additionally, the robustness and generalizability of the classification model are improved by using multiple feature extraction techniques which reduces the reliance on any single feature extraction technique. We test our methodology with experiments on datasets of simulated weak sound signals with varying amounts of noise and clutter. We evaluated the performance of our attention-based neural network in comparison to a number of established techniques. Our findings demonstrate that, especially in noisy and congested environments, the proposed model outperforms the baselines in terms of source identification accuracy. Overall, our work illustrates the efficiency of employing an attention-based neural network based on context information for the identification of weak sound sources and implies that this strategy may be a promising approach for further research in this area.},
  archive      = {J_APIN},
  author       = {Presannakumar, Krishna and Mohamed, Anuj},
  doi          = {10.1007/s10489-023-04973-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27044-27059},
  shortjournal = {Appl. Intell.},
  title        = {Source identification of weak audio signals using attention based convolutional neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recipe recommendations for individual users and groups in a
cooking assistance app. <em>APIN</em>, <em>53</em>(22), 27027–27043. (<a
href="https://doi.org/10.1007/s10489-023-04909-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are commonly-used tools to assist people in making decisions. However, most research has focused on the domain of recommendations for audio-visual content and e-commerce, whereas the specific characteristics of recommendations for recipes and cooking did not receive enough attention. Since meals are often consumed in group (with friends or family), there is a need for group recommendations, taking into account the preferences of all group members. Also cuisine, allergies, disliked ingredients, diets, dish type, and required time to prepare are important factors for recipe selection. For 13 algorithms, we evaluated the recommendations for individuals and for groups using a dataset of recipe ratings. The best algorithm and a baseline algorithm based on popularity were selected for our mobile kitchen experience and recipe application, which assists users in the cooking process and provides recipe recommendations. Although significant differences between both algorithms were witnessed in the offline evaluation with the dataset, the differences were less noticeable in the online evaluation with real users. Because of the cold-start problem, the advanced algorithm failed to reach its full accuracy potential, but excelled in other quality features such as diversity, perceived usefulness, and confidence. We also witnessed a better evaluation (about half a star) of the recommendations by the more advanced cooks.},
  archive      = {J_APIN},
  author       = {De Pessemier, Toon and Vanhecke, Kris and All, Anissa and Van Hove, Stephanie and De Marez, Lieven and Martens, Luc and Joseph, Wout and Plets, David},
  doi          = {10.1007/s10489-023-04909-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27027-27043},
  shortjournal = {Appl. Intell.},
  title        = {Recipe recommendations for individual users and groups in a cooking assistance app},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to train your pre-trained GAN models. <em>APIN</em>,
<em>53</em>(22), 27001–27026. (<a
href="https://doi.org/10.1007/s10489-023-04807-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GAN) show excellent performance in various problems of computer vision, computer graphics, and machine learning, but require large amounts of data and huge computational resources. There is also the issue of unstable training. If the generator and discriminator diverge during the training process, the GAN is subsequently difficult to converge. In order to tackle these problems, various transfer learning methods have been introduced; however, mode collapse, which is a form of overfitting, often arises. Moreover, there were limitations in learning the distribution of the training data. In this paper, we provide a comprehensive review of the latest transfer learning methods as a solution to the problem, propose the most effective method of fixing some layers of the generator and discriminator, and discuss future prospects. The model to be used for the experiment is StyleGAN, and the performance evaluation uses Fréchet Inception Distance (FID), coverage, and density. Results of the experiment revealed that the proposed method did not overfit. The model was able to learn the distribution of the training data relatively well compared to the previously proposed methods. Moreover, it outperformed existing methods at the Stanford Cars, Stanford Dogs, Oxford Flower, Caltech-256, CUB-200–2011, and Insect-30 datasets.},
  archive      = {J_APIN},
  author       = {Park, Sung-Wook and Kim, Jun-Yeong and Park, Jun and Jung, Se-Hoon and Sim, Chun-Bo},
  doi          = {10.1007/s10489-023-04807-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {27001-27026},
  shortjournal = {Appl. Intell.},
  title        = {How to train your pre-trained GAN models},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data representation learning via dictionary learning and
self-representation. <em>APIN</em>, <em>53</em>(22), 26988–27000. (<a
href="https://doi.org/10.1007/s10489-023-04902-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning is an effective feature learning method, leading to many remarkable results in data representation and classification tasks. However, dictionary learning is performed on the original data representation. In some cases, the capability of representation and discriminability of the learned dictionaries may need to be performed better, i.e., with only sparse but not low rank. In this paper, we propose a novel efficient data representation learning method by combining dictionary learning and self-representation, which utilizes both properties of sparsity in dictionary learning and low-rank in low-rank representation (LRR) simultaneously. Thus both the sparse and low-rank properties of the data representation can be naturally captured by our method. To obtain the solution of our proposed method effectively, we also innovatively introduce a more generalized data representation model in this paper. To our best knowledge, its closed-form solution is first derived analytically through our rigorous mathematical analysis. Experimental results show that our method not only can be used for data pre-processing but also can realize better dictionary learning. The samples in the same class can have similar representations by our method, and the discriminability of the learned dictionary can also be enhanced.},
  archive      = {J_APIN},
  author       = {Zeng, Deyu and Sun, Jing and Wu, Zongze and Ding, Chris and Ren, Zhigang},
  doi          = {10.1007/s10489-023-04902-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26988-27000},
  shortjournal = {Appl. Intell.},
  title        = {Data representation learning via dictionary learning and self-representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric algebra-based multiscale encoder-decoder networks
for 3D motion prediction. <em>APIN</em>, <em>53</em>(22), 26967–26987.
(<a href="https://doi.org/10.1007/s10489-023-04908-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human motion prediction is one of the essential and challenging problems in computer vision, which has attracted extensive research attention in the past decades. Many previous methods sought to predict the motion state of the next moment using the traditional recurrent neural network in Euclidean space. However, most methods did not explicitly exploit the relationships or constraints between different body components, which carry crucial information for motion prediction. In addition, human motion representation in Euclidean space has high distortion and shows a weak semantic expression when using deep learning models. Based on these observations, we propose a novel Geometric Algebra-based Multiscale Encoder-Decoder network (GAMEDnet) to predict the future 3D poses. In the encoder, the core module is a novel multiscale Geometric Algebra-based multiscale feature extractor(GA-MFE) , which extracts motion features given the multiscale human motion graph. In the decoder, we propose a novel GA-Graph-based Gated Recurrent Unit (GAG-GRU) to sequentially produce predictions. Extensive experiments are conducted to show that the proposed GAMEDnet outperforms state-of-the-art methods in both short and long-term motion prediction on the datasets of Human 3.6M, CMU Mocap.},
  archive      = {J_APIN},
  author       = {Zhong, Jianqi and Cao, Wenming},
  doi          = {10.1007/s10489-023-04908-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26967-26987},
  shortjournal = {Appl. Intell.},
  title        = {Geometric algebra-based multiscale encoder-decoder networks for 3D motion prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fractional-order genetic-particle swarm
optimization otsu algorithm for image segmentation. <em>APIN</em>,
<em>53</em>(22), 26949–26966. (<a
href="https://doi.org/10.1007/s10489-023-04969-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-dimensional (2D) Otsu algorithm used in image segmentation considers both the gray scale information of the image and the spatial information contained in the neighborhood of pixels. The algorithm is quite effective, but a number of variations have been proposed to improve its performance. In this paper, a new adaptive fractional-order (FO) genetic-particle swarm optimization (FOGPSO) version is proposed. The FOGPSO associates the particle selection operations of a genetic algorithm (GA) and particle swarm optimization (PSO). Crossover and genetic mutation are used to avoid PSO falling into a local optimum. Fractional calculus operators are adopted in the updating scheme of velocity and position, while the order of the derivative is adaptively changed according to the state of the particles. Compared with the original PSO, the integer PSO, the fractional PSO (FOPSO), other improved versions of the PSO for Otsu algorithms, and other existing methods for 2D Otsu algorithms, the proposed method shows great superiority. Indeed, experimental results reveal that both qualitatively and quantitatively, through suitable indices, as the regional contrast, the intersection over union (IOU) and peak signal to noise ratio (PSNR), the FOGPSO outperforms the other methods, thus verifying the effectiveness of the new algorithm.},
  archive      = {J_APIN},
  author       = {Chen, Liping and Gao, Jinhui and Lopes, António M. and Zhang, Zhiqiang and Chu, Zhaobi and Wu, Ranchao},
  doi          = {10.1007/s10489-023-04969-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26949-26966},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive fractional-order genetic-particle swarm optimization otsu algorithm for image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Truthful meta-explanations for local interpretability of
machine learning models. <em>APIN</em>, <em>53</em>(22), 26927–26948.
(<a href="https://doi.org/10.1007/s10489-023-04944-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Machine Learning-based systems’ integration into a wide range of tasks has expanded as a result of their performance and speed. Although there are numerous advantages to employing ML-based systems, if they are not interpretable, they should not be used in critical or high-risk applications. To address this issue, researchers and businesses have been focusing on finding ways to improve the explainability of complex ML systems, and several such methods have been developed. Indeed, there are so many developed techniques that it is difficult for practitioners to choose the best among them for their applications, even when using evaluation metrics. As a result, the demand for a selection tool, a meta-explanation technique based on a high-quality evaluation metric, is apparent. In this paper, we present a local meta-explanation technique which builds on top of the truthfulness metric, which is a faithfulness-based metric. We demonstrate the effectiveness of both the technique and the metric by concretely defining all the concepts and through experimentation.},
  archive      = {J_APIN},
  author       = {Mollas, Ioannis and Bassiliades, Nick and Tsoumakas, Grigorios},
  doi          = {10.1007/s10489-023-04944-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26927-26948},
  shortjournal = {Appl. Intell.},
  title        = {Truthful meta-explanations for local interpretability of machine learning models},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling for trial production with a parallel machine and
multitasking scheduling model. <em>APIN</em>, <em>53</em>(22),
26907–26926. (<a
href="https://doi.org/10.1007/s10489-023-04845-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trial production is essential for some manufacturing systems to adjust production lines before mass production. Scheduling for trial production promotes carrying out mass production activities faster. In this paper, a set of job families with setup time should be processed on several identical parallel machines, in which jobs belonging to the same family are processed according to multitasking scheduling. The objective is to minimize the total weighted completion time. Structural properties of the investigated problem are proposed. Subsequently, a novel branch-and-price algorithm (B &amp;P) is proposed. In detail, a label-setting algorithm is presented for fast solving the pricing problem of B &amp;P, and the structural properties are utilized to speed up the algorithm. Computational results show that the proposed B &amp;P has an excellent performance in both optimality and efficiency. Within the limit time of 1800 seconds, B &amp;P is able to optimally solve 100.00% of small-size problems and 91.11% of large-size problems. The average gap of B &amp;P in percentage between the upper bound and theoretical lower bound is close to 0.00%. Results of the final case study demonstrate that, in general, the proposed algorithm improves the efficiency of the trial production plan.},
  archive      = {J_APIN},
  author       = {Gao, Jinsheng and Zhu, Xiaomin and Zhang, Runtong},
  doi          = {10.1007/s10489-023-04845-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26907-26926},
  shortjournal = {Appl. Intell.},
  title        = {Scheduling for trial production with a parallel machine and multitasking scheduling model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning-based power control and
bandwidth allocation policy for weighted cost minimization in wireless
networks. <em>APIN</em>, <em>53</em>(22), 26885–26906. (<a
href="https://doi.org/10.1007/s10489-023-04929-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) can dispatch its powerful servers close by to assist with the computation workloads that intelligent wireless terminals have offloaded. The MEC server’s physical location is closer to the intelligent wireless terminals, which can satisfy the low latency and high reliability demands. In this paper, we formulate an MEC framework with multiple vehicles and service devices that considers the priority and randomness of arriving workloads from roadside units (RSUs), cameras, laser radars (Lidar) and the time-varying channel state between the service device and MEC server (MEC-S). To minimize the long-term weighted average cost of the proposed MEC system, we transit this issue (cost minimization problem) into the Markov decision process (MDP). Furthermore, considering the difficulty realizing the state transition probability matrix, the dimensional complexity of the state space, and the continuity of the action space, we propose a deterministic policy gradient (MADDPG)-based bandwidth partition and power allocation optimization policy. The proposed MADDPG-based policy is a model-free deep reinforcement learning (DRL) method, which can effectively deal with continuous action space and further guide multi-agent to execute decision-making. The comprehensive results verify that the proposed MADDPG-based optimization scheme has fine convergence and performance that is better than that of the other four baseline algorithms.},
  archive      = {J_APIN},
  author       = {Ke, Hongchang and Wang, Hui and Sun, Hongbin},
  doi          = {10.1007/s10489-023-04929-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26885-26906},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning-based power control and bandwidth allocation policy for weighted cost minimization in wireless networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved versions of crow search algorithm for solving
global numerical optimization problems. <em>APIN</em>, <em>53</em>(22),
26840–26884. (<a
href="https://doi.org/10.1007/s10489-023-04732-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent decades, research in Artificial Intelligence (AI) has developed a broad range of approaches and methods that can be utilized or adapted to address complex optimization problems. As real-world problems get increasingly complicated, this requires an effective optimization method. Various meta-heuristic algorithms have been developed and applied in the optimization domain. This paper used and ameliorated a promising meta-heuristic approach named Crow Search Algorithm (CSA) to address numerical optimization problems. Although CSA can efficiently optimize many problems, it needs more searchability and early convergence. Its positioning updating process was improved by supporting two adaptive parameters: flight length (fl) and awareness probability (AP) to tackle these curbs. This is to manage the exploration and exploitation conducts of CSA in the search space. This process takes advantage of the randomization of crows in CSA and the adoption of well-known growth functions. These functions were recognized as exponential, power, and S-shaped functions to develop three different improved versions of CSA, referred to as Exponential CSA (ECSA), Power CSA (PCSA), and S-shaped CSA (SCSA). In each of these variants, two different functions were used to amend the values of fl and AP. A new dominant parameter was added to the positioning updating process of these algorithms to enhance exploration and exploitation behaviors further. The reliability of the proposed algorithms was evaluated on 67 benchmark functions, and their performance was quantified using relevant assessment criteria. The functionality of these algorithms was illustrated by tackling four engineering design problems. A comparative study was made to explore the efficacy of the proposed algorithms over the standard one and other methods. Overall results showed that ECSA, PCSA, and SCSA have convincing merits with superior performance compared to the others.},
  archive      = {J_APIN},
  author       = {Sheta, Alaa and Braik, Malik and Al-Hiary, Heba and Mirjalili, Seyedali},
  doi          = {10.1007/s10489-023-04732-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26840-26884},
  shortjournal = {Appl. Intell.},
  title        = {Improved versions of crow search algorithm for solving global numerical optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A text-specific domain adaptive network for scene text
detection in the wild. <em>APIN</em>, <em>53</em>(22), 26827–26839. (<a
href="https://doi.org/10.1007/s10489-023-04873-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection has drawn increasing attention due to its potential scalability to large-scale applications. Currently, a well-trained scene text detection model on a source domain usually has unsatisfactory performance when it is migrated to e large domain shift between them. To bridge this gap, this paper proposes a novel network integrates both text-specific Faster R-CNN (ts-FRCNN) and domain adaptation (ts-DA) into one framework. Compared to conventional FRCNN, ts-FRCNN designs a text-specific RPN to generate more accurate region proposals by considering the inherent characters of scene text, as well as text-specific RoI pooling to extract purer and sufficient fine-grained text features by adopting an adaptive asymmetric griding strategy. Compared to conventional domain adaptation, ts-DA adopts a triple-level alignment strategy to reduce the domain shift at the image, word and character levels, and builds a triple-consistency regularization among them, which significantly promotes domain-invariant text feature learning. We conduct extensive experiments on three representative transfer learning tasks: common-to-extreme scenes, real-to-real scenes and synthetic-to-real scenes. The experimental results demonstrate that our model consistently outperforms the previous methods.},
  archive      = {J_APIN},
  author       = {He, Xuan and Yuan, Jin and Li, Mengyao and Wang, Runmin and Wang, Haidong and Li, Zhiyong},
  doi          = {10.1007/s10489-023-04873-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26827-26839},
  shortjournal = {Appl. Intell.},
  title        = {A text-specific domain adaptive network for scene text detection in the wild},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense-scale dynamic network with filter-varying atrous
convolution for semantic segmentation. <em>APIN</em>, <em>53</em>(22),
26810–26826. (<a
href="https://doi.org/10.1007/s10489-023-04935-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolution neural networks (DCNNs) in deep learning have been widely used in semantic segmentation. However, the filters of most regular convolutions in DCNNs are spatially invariant to local transformations, which reduces localization accuracy and hinders the improvement of semantic segmentation. Dynamic convolution with pixel-level filters can enhance the localization accuracy through its region-awareness, but these are sensitive to objects with large-scale variations in semantic segmentation. To simultaneously address the low localization accuracy and objects with large-scale variations, we propose a filter-varying atrous convolution (FAC) to efficiently enlarge the per-pixel receptive fields pertaining to various objects. FAC mainly consists of a conditional-filter-generating network (CFGN) and a dynamic local filtering operation (DLFO). In the CFGN, a class probability map is used to generate the corresponding filters, making the FAC genuinely dynamic. In the DLFO, by replacing the sliding convolution operation one by one with a one-time dot product operation, the efficiency of the algorithm is greatly improved. Also, a dense scale module (DSM) is constructed to generate denser scales and larger receptive fields for exploring long-range contextual information. Finally, a dense-scale dynamic network (DsDNet) simultaneously enhances the localization accuracy and reduces the effect of large-scale variations of the object, by assigning FAC to different spatial locations at dense scales. In addition, to accelerate network convergence and improve segmentation accuracy, our network employs two pixel-wise cross-entropy loss functions. One is between the Backbone and DSM, and the other is at the network’s end. Extensive experiments on Cityscapes, PASCAL VOC 2012, and ADE20K datasets verify that the performance of our DsDNet is superior to the non-dynamic and multi-scale convolution neural networks.},
  archive      = {J_APIN},
  author       = {Li, Zhiqiang and Jiang, Jie and Chen, Xi and Laganière, Robert and Li, Qingli and Liu, Min and Qi, Honggang and Wang, Yong and Zhang, Min},
  doi          = {10.1007/s10489-023-04935-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26810-26826},
  shortjournal = {Appl. Intell.},
  title        = {Dense-scale dynamic network with filter-varying atrous convolution for semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial representation teaching with
perturbation-agnostic student-teacher structure for semi-supervised
learning. <em>APIN</em>, <em>53</em>(22), 26797–26809. (<a
href="https://doi.org/10.1007/s10489-023-04950-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency regularization (CR) is representative semi-supervised learning (SSL) technique that maintains the consistency of predictions from multiple views on the same unlabeled data during the training. In recent SSL studies, the approaches of self-supervised learning with CR, which conducts the pre-training based on unsupervised learning and fine-tuning based on supervised learning, have provided excellent classification accuracy. However, the data augmentation used to generate multiple views in CR has a limitation for expanding the training data distribution. In addition, the existing self-supervised learning using CR cannot provide the high-density clustering result for each class of the labeled data in a representation space, thus it is vulnerable to outlier samples of the unlabeled data with strong augmentation. Consequently, the unlabeled data with augmentation for SSL may not improve the classification performance but rather degrade it. To solve these, we propose a new training methodology called adversarial representation teaching (ART), which consists of the labeled sample-guided representation teaching and adversarial noise-based CR. In our method, the adversarial attack-robust teacher model guides the student model to form a high-density distribution in representation space. This allows for maximizing the improvement by the strong embedding augmentation in the student model for SSL. For the embedding augmentation, the adversarial noise attack on the representation is proposed to successfully expand a class-wise subspace, which cannot be achieved by the existing adversarial attack or embedding expansion. Experimental results showed that the proposed method provided outstanding classification accuracy up to 1.57% compared to the existing state-of-the-art methods under SSL conditions. Moreover, ART significantly outperforms the classification accuracies up to 1.57%, 0.53%, and 0.3% over our baseline method on the CIFAR-10, SVHN, and ImageNet datasets, respectively.},
  archive      = {J_APIN},
  author       = {Park, Jae Hyeon and Kim, Ju Hyun and Ngo, Ba Hung and Kwon, Jung Eun and Cho, Sung In},
  doi          = {10.1007/s10489-023-04950-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26797-26809},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial representation teaching with perturbation-agnostic student-teacher structure for semi-supervised learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persistence initialization: A novel adaptation of the
transformer architecture for time series forecasting. <em>APIN</em>,
<em>53</em>(22), 26781–26796. (<a
href="https://doi.org/10.1007/s10489-023-04927-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is an important problem, with many real world applications. Transformer models have been successfully applied to natural language processing tasks, but have received relatively little attention for time series forecasting. Motivated by the differences between classification tasks and forecasting, we propose PI-Transformer, an adaptation of the Transformer architecture designed for time series forecasting, consisting of three parts: First, we propose a novel initialization method called Persistence Initialization, with the goal of increasing training stability of forecasting models by ensuring that the initial outputs of an untrained model are identical to the outputs of a simple baseline model. Second, we use ReZero normalization instead of Layer Normalization, in order to further tackle issues related to training stability. Third, we use Rotary positional encodings to provide a better inductive bias for forecasting. Multiple ablation studies show that the PI-Transformer is more accurate, learns faster, and scales better than regular Transformer models. Finally, PI-Transformer achieves competitive performance on the challenging M4 dataset, both when compared to the current state of the art, and to recently proposed Transformer models for time series forecasting.},
  archive      = {J_APIN},
  author       = {Haugsdal, Espen and Aune, Erlend and Ruocco, Massimiliano},
  doi          = {10.1007/s10489-023-04927-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26781-26796},
  shortjournal = {Appl. Intell.},
  title        = {Persistence initialization: A novel adaptation of the transformer architecture for time series forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IDO: Instance dual-optimization for weakly supervised object
detection. <em>APIN</em>, <em>53</em>(22), 26763–26780. (<a
href="https://doi.org/10.1007/s10489-023-04956-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it utilizes only image-level annotations to train object detectors and greatly reduces the labor and capital cost of fine labeling. Nevertheless, the absence of instance-level annotations leads to two phenomena: partial regions and missing instances. We believe these are mainly caused by two issues: 1) Noisy instances exist in the training samples, which can confuse the detector. 2) Global salient information is missing, resulting in little attention being received in the low-confidence region. To solve the above two problems, we propose an instance dual-optimization framework called IDO. First, an instance-wise selection strategy (IWSS) based on curriculum learning is proposed for instance denoising and for improving the robustness of the model. Second, CAM-generated spatial attention (CGSA) is carefully designed to optimize the features of instances. Without introducing additional hyperparameters, our CGSA complements the low class-confidence region with more global salient information, which assists the model in acquiring a more complete region of the target and identifying more neglected targets. Finally, we empirically demonstrate that our proposal can achieve comparable results to those of other state-of-the-art methods on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO.},
  archive      = {J_APIN},
  author       = {Ren, Zhida and Tang, Yongqiang and Zhang, Wensheng},
  doi          = {10.1007/s10489-023-04956-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26763-26780},
  shortjournal = {Appl. Intell.},
  title        = {IDO: Instance dual-optimization for weakly supervised object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial variational autoencoder for attributed graph
embedding with high-frequency noise filtering. <em>APIN</em>,
<em>53</em>(22), 26750–26762. (<a
href="https://doi.org/10.1007/s10489-023-04961-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph embedding aims to learn low-dimensional representations by fully exploiting both topological structure and rich attributes. The embeddings contain more useful information, thus yielding better performance in subsequent analysis tasks. Therefore, attributed network embedding has become an important research direction in recent years. However, most existing algorithms ignore the high-frequency noise contained in the features and do not normalize the embedding, which limits their performance in downstream tasks. To solve the above problem, we design an Adversarial Attributed Graph Embedding algorithm (AAGE) in conjunction with a Laplacian smoothing filter. In particular, the AAGE employs a variational autoencoder (VAE) to learn the embedding of nodes and attributes. Unlike most variational autoencoder-based algorithms, we first apply a carefully-designed Laplacian filter to smooth the features before performing autoencoder, secondly normalize the obtained embeddings of nodes and attributes during the autoencoder process, and finally devise a generative adversarial training method to enhance the robustness of the learned representations. To verify the potential of the proposed AAGE, we evaluate its performance on the tasks of node classification and node clustering on three real-world attributed networks. The results show that AAGE significantly outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhou, Hui and Liu, Xin and Li, Xiangju and Zhao, Zhongying and Li, Chao},
  doi          = {10.1007/s10489-023-04961-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26750-26762},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial variational autoencoder for attributed graph embedding with high-frequency noise filtering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dictionary cache transformer for hyperspectral image
classification. <em>APIN</em>, <em>53</em>(22), 26725–26749. (<a
href="https://doi.org/10.1007/s10489-023-04934-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral anomalies, limited training samples, and noisy training labels pose significant challenges to accurately classifying hyperspectral images (HSIs). To address these issues, we propose a novel dictionary cache transformer (DiCT) for HSIs classification, leveraging a combination of a group self-attention mechanism and a dictionary cache module. Specifically, the group self-attention mechanism binds the features local to the pixel into a group, thus alleviating the interference caused by pixel spectral anomalies. Furthermore, we capture representative structural information from different samples using their discriminative features to construct the dictionary cache module. The dictionary cache module enhances features by fusing sample features and the most similar element in the dictionary cache, thus improving the model’s resilience to noisy training labels. Experiments on five HSIs datasets demonstrate the proposed DiCT’s superiority in classification performance and robustness to noisy training labels.},
  archive      = {J_APIN},
  author       = {Zhou, Heng and Zhang, Xin and Zhang, Chunlei and Ma, Qiaoyu and Jiang, Yanan},
  doi          = {10.1007/s10489-023-04934-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26725-26749},
  shortjournal = {Appl. Intell.},
  title        = {Dictionary cache transformer for hyperspectral image classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TADSRNet: A triple-attention dual-scale residual network for
super-resolution image quality assessment. <em>APIN</em>,
<em>53</em>(22), 26708–26724. (<a
href="https://doi.org/10.1007/s10489-023-04932-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) has been extensively investigated in recent years. However, due to the absence of trustworthy and precise perceptual quality standards, it is challenging to objectively measure the performance of different SR approaches. In this paper, we propose a novel triple attention dual-scale residual network called TADSRNet for no-reference super-resolution image quality assessment (NR-SRIQA). Firstly, we simulate the human visual system (HVS) and construct a triple attention mechanism to acquire more significant portions of SR images through cross-dimensionality, making it simpler to identify visually sensitive regions. Then a dual-scale convolution module (DSCM) is constructed to capture quality-perceived features at different scales. Furthermore, in order to collect more informative feature representation, a residual connection is added to the network to compensate for perceptual features. Extensive experimental results demonstrate that the proposed TADSRNet can predict visual quality with greater accuracy and better consistency with human perception compared with existing IQA methods. The code will be available at https://github.com/kbzhang0505/TADSRNet .},
  archive      = {J_APIN},
  author       = {Quan, Xing and Zhang, Kaibing and Li, Hui and Fan, Dandan and Hu, Yanting and Chen, Jinguang},
  doi          = {10.1007/s10489-023-04932-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26708-26724},
  shortjournal = {Appl. Intell.},
  title        = {TADSRNet: A triple-attention dual-scale residual network for super-resolution image quality assessment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual ant colony optimization for electric vehicle charging
infrastructure planning. <em>APIN</em>, <em>53</em>(22), 26690–26707.
(<a href="https://doi.org/10.1007/s10489-023-04772-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Charging infrastructure planning (CIPL) is key to popularizing electric vehicles and reducing carbon emissions. CIPL consists of two subproblems: charging station siting and charging pile allocation. The existing methods independently solve the two subproblems and ignore their interaction, which restricts the rationality of CIPL. To address this issue, this paper proposes a dual ant colony optimization for CIPL (DACO-CIPL). In each iteration, under the guidance of heuristic information and pheromones, the upper and lower ant colonies construct solutions for charging station siting and charging pile allocation in turn, respectively. Then, a global pheromone update strategy is performed to update the pheromones of each ant colony according to the historical best solutions, which realizes information transmission from the lower ant colony to the upper ant colony. In addition, whenever the upper ant colony finishes constructing solutions, a pheromone enhancement strategy is used to strengthen the pheromones of the lower ant colony according to the solutions of the upper ant colony, which realizes information transmission from the upper ant colony to the lower ant colony. DACO-CIPL is compared with several algorithms on multiple test instances. The experimental results show that DACO-CIPL has superior performance and more reasonable options for CIPL.},
  archive      = {J_APIN},
  author       = {Ji, Junzhong and Liu, Yuefeng and Yang, Cuicui},
  doi          = {10.1007/s10489-023-04772-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26690-26707},
  shortjournal = {Appl. Intell.},
  title        = {Dual ant colony optimization for electric vehicle charging infrastructure planning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IMVAN: Integrative multimodal variational autoencoder and
network fusion for biomarker identification and cancer subtype
classification. <em>APIN</em>, <em>53</em>(22), 26672–26689. (<a
href="https://doi.org/10.1007/s10489-023-04936-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous research has been conducted to define the molecular and clinical aspects of various tumors from a multi-omics point of view. However, there are significant obstacles in integrating multi-omics via Machine Learning (ML) for biomarker identification and cancer subtype classification. In this research, iMVAN, an integrated Multimodal Variational Autoencoder and Network fusion, is presented for biomarker discovery and classification of cancer subtypes. First, MVAE is used on multi-omics data consisting of Copy Number Variation (CNV), mRNA, and Reverse Protein Phase Array (rppa) to discover the biomarkers associated with distinct cancer subtypes. Then, multi-omics integration is accomplished by fusing similarity networks. Ultimately, the MVAE latent data and network fusion are given to a Simplified Graph Convolutional Network (SGC) for categorizing cancer subtypes. The suggested study extracts the top 100 features, which are then submitted to the KEGG analysis and survival analysis test. The survival study identifies nine biomarkers, including AGT, CDH1, CALML5, ERBB2, CCND1, FZD6, BRAF, AR, and MSH6, as poor prognostic markers. In addition, the cancer subtypes are classified, and the performance is assessed. The experimental findings demonstrate that the iMVAN performed well, with an accuracy of 87%.},
  archive      = {J_APIN},
  author       = {Dhillon, Arwinder and Singh, Ashima and Bhalla, Vinod Kumar},
  doi          = {10.1007/s10489-023-04936-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26672-26689},
  shortjournal = {Appl. Intell.},
  title        = {IMVAN: Integrative multimodal variational autoencoder and network fusion for biomarker identification and cancer subtype classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving image classification of one-dimensional
convolutional neural networks using hilbert space-filling curves.
<em>APIN</em>, <em>53</em>(22), 26655–26671. (<a
href="https://doi.org/10.1007/s10489-023-04945-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have significantly contributed to recent advances in machine learning and computer vision. Although initially designed for image classification, the application of CNNs has stretched far beyond the context of images alone. Some exciting applications, e.g., in natural language processing and image segmentation, implement one-dimensional CNNs, often after a pre-processing step that transforms higher-dimensional input into a suitable data format for the networks. However, local correlations within data can diminish or vanish when one converts higher-dimensional data into a one-dimensional string. The Hilbert space-filling curve can minimize this loss of locality. Here, we study this claim rigorously by comparing an analytical model that quantifies locality preservation with the performance of several neural networks trained with and without Hilbert mappings. We find that Hilbert mappings offer a consistent advantage over the traditional flatten transformation in test accuracy and training speed. The results also depend on the chosen kernel size, agreeing with our analytical model. Our findings quantify the importance of locality preservation when transforming data before training a one-dimensional CNN and show that the Hilbert space-filling curve is a preferential transformation to achieve this goal.},
  archive      = {J_APIN},
  author       = {Verbruggen, Bert and Ginis, Vincent},
  doi          = {10.1007/s10489-023-04945-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26655-26671},
  shortjournal = {Appl. Intell.},
  title        = {Improving image classification of one-dimensional convolutional neural networks using hilbert space-filling curves},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Morphing aircraft acceleration and deceleration task
morphing strategy using a reinforcement learning method. <em>APIN</em>,
<em>53</em>(22), 26637–26654. (<a
href="https://doi.org/10.1007/s10489-023-04876-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a design scheme for a whole morphing strategy based on the reinforcement learning (RL) method. A novel morphing aircraft is designed, and its nonlinear dynamic equations are established based on the calculated aerodynamic data. Further, a soft actor critic (SAC) approach is utilized to design the scheme, whose structure consists of the environment, the agent, and the reward function. In the environment design part, the incremental backstepping approach is employed to design the morphing aircraft controller. The safety and feasibility of deployment are verified. In the agent design part, in addition to using the entropy regularization RL algorithm, the generalization ability of the agent is enhanced in three ways: adding environmental noise, adding control command randomness, and adding output momentum terms. For the reward function, a structure with dynamic and steady-state performance is designed to accurately describe the aircraft dynamics. Finally, the designed SAC strategy is verified under the acceleration and deceleration tasks and compared with a GA and PPO strategy. Simulation results validate the effectiveness and superiority of the designed SAC scheme.},
  archive      = {J_APIN},
  author       = {Ming, Ruichen and Liu, Xiaoxiong and Li, Yu and Yin, Yi and Zhang, WeiGuo},
  doi          = {10.1007/s10489-023-04876-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26637-26654},
  shortjournal = {Appl. Intell.},
  title        = {Morphing aircraft acceleration and deceleration task morphing strategy using a reinforcement learning method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view stereo network with point attention.
<em>APIN</em>, <em>53</em>(22), 26622–26636. (<a
href="https://doi.org/10.1007/s10489-023-04806-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, learning-based multi-view stereo (MVS) reconstruction has gained superiority when compared with traditional methods. In this paper, we introduce a novel point-attention network, with an attention mechanism, based on the point cloud structure. During the reconstruction process, our method with an attention mechanism can guide the network to pay more attention to complex areas such as thin structures and low-texture surfaces. We first infer a coarse depth map using a modified classical MVS deep framework and convert it into the corresponding point cloud. Then, we add the high-frequency features and different-resolution features of the raw images to the point cloud. Finally, our network guides the weight distribution of points in different dimensions through the attention mechanism and computes the depth displacement of each point iteratively as the depth residual, which is added to the coarse depth prediction to obtain the final high-resolution depth map. Experimental results show that our proposed point-attention architecture can achieve a significant improvement in some scenes without reasonable geometrical assumptions on the DTU dataset and the Tanks and Temples dataset, suggesting that our method has a strong generalization ability.},
  archive      = {J_APIN},
  author       = {Zhao, Rong and Gu, Zhuoer and Han, Xie and He, Ligang and Sun, Fusheng and Jiao, Shichao},
  doi          = {10.1007/s10489-023-04806-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26622-26636},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view stereo network with point attention},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple yet effective joint guidance learning for few-shot
semantic segmentation. <em>APIN</em>, <em>53</em>(22), 26603–26621. (<a
href="https://doi.org/10.1007/s10489-023-04937-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully-supervised semantic segmentation methods are difficult to generalize to novel objects, and their fine-tuning often requires a sufficient number of fully-labeled images. Few-shot semantic segmentation (FSS) has recently attracted lots of attention due to its excellent capability for segmenting the novel object with only a few labeled images. Most of recent approaches follow the prototype learning paradigm and have made a significant improvement in segmentation performance. However, there exist two critical bottleneck problems to be solved. (1) Previous methods mainly focus on mining the foreground information of the target object, and class-specific prototypes are generated by solely leveraging average operation on the whole support image, which may lead to information loss, underutilization, or semantic confusion of the object. (2) Most existing methods unilaterally guide the object segmentation in the query image with support images, which may result in semantic misalignment due to the diversity of objects in the support and query sets. To alleviate the above challenging problems, we propose a simple yet effective joint guidance learning architecture to generate and align more compact and robust prototypes from two aspects. (1) We propose a coarse-to-fine prototype generation module to generate coarse-grained foreground prototypes and fine-grained background prototypes. (2) We design a joint guidance learning module for the prototype evaluation and optimization on both support and query images. Extensive experiments show that the proposed method can achieve superior segmentation results on PASCAL-5 $$^{i}$$ and COCO-20 $$^{i}$$ datasets.},
  archive      = {J_APIN},
  author       = {Chang, Zhaobin and Lu, Yonggang and Ran, Xingcheng and Gao, Xiong and Zhao, Hong},
  doi          = {10.1007/s10489-023-04937-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26603-26621},
  shortjournal = {Appl. Intell.},
  title        = {Simple yet effective joint guidance learning for few-shot semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage approach for identifying and interpreting
self-admitted technical debt. <em>APIN</em>, <em>53</em>(22),
26592–26602. (<a
href="https://doi.org/10.1007/s10489-023-04941-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major current focus in software quality is how to identify and interpret Self-admitted technical debt(SATD). While many methods have been proposed to identify SATD, these methods are neither interpretable nor generic. There remains a need for an efficient method that can interpret SATD. In this paper, we propose a two-stage approach to identify and interpret SATD using interpretable methods. In the first stage, the decision tree model is combined into an integrated model to identify SATD better. We apply SHAP, LIME, and Anchors models in the second stage to interpret the result. The experiments of 10 projects show that our method not only can effectively detect and explain SATD both in within-project and cross-project experiments, but also has a good explanation for self-generated data outside the dataset.},
  archive      = {J_APIN},
  author       = {Yin, Ming and Wang, Jiaze and Zhu, Dan and Gao, Cunzhi},
  doi          = {10.1007/s10489-023-04941-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26592-26602},
  shortjournal = {Appl. Intell.},
  title        = {A two-stage approach for identifying and interpreting self-admitted technical debt},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic relation learning for link prediction in knowledge
hypergraphs. <em>APIN</em>, <em>53</em>(22), 26580–26591. (<a
href="https://doi.org/10.1007/s10489-023-04710-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction for knowledge graphs (KGs), which aims to predict missing facts, has been broadly studied in binary relational KGs. However, real world data contains a large number of high-order interaction patterns, which is difficult to describe using only binary relations. In this work, we propose a relation-based dynamic learning model RD-MPNN, based on the message passing neural network model, to learn higher-order interactions and address the link prediction problem in knowledge hypergraphs. Different from existing methods, we consider the positional information of entities within a hyper-relation to differentiate each entity’s role in the hyper-relation. Furthermore, we complete the representation learning of hyper-relations by dynamically updating hyper-relations with entity information. Extensive evaluations on two representative knowledge hypergraph datasets demonstrate that our model outperforms the state-of-the-art methods. We also compare the performance of models at differing arities (the number of entities within a relation), to show that RD-MPNN demonstrates outstanding performance metrics for complex hypergraphs (arity&gt;2).},
  archive      = {J_APIN},
  author       = {Zhou, Xue and Hui, Bei and Zeira, Ilana and Wu, Hao and Tian, Ling},
  doi          = {10.1007/s10489-023-04710-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26580-26591},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic relation learning for link prediction in knowledge hypergraphs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VTP: Volumetric transformer for multi-view multi-person 3D
pose estimation. <em>APIN</em>, <em>53</em>(22), 26568–26579. (<a
href="https://doi.org/10.1007/s10489-023-04805-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Volumetric Transformer Pose Estimator (VTP), the first 3D volumetric transformer framework for multi-view multi-person 3D human pose estimation. VTP aggregates features from 2D keypoints in all camera views and directly learns the spatial relationships in the 3D voxel space in an end-to-end fashion. The aggregated 3D features are passed through 3D convolutions before being flattened into sequential embeddings and fed into a transformer. A residual structure is designed to further improve the performance. In addition, the sparse Sinkhorn attention is empowered to reduce the memory cost, which is a major bottleneck for volumetric representations, while also achieving excellent performance. The output of the transformer is again concatenated with 3D convolutional features by a residual design. The proposed VTP framework integrates the high performance of the transformer with volumetric representations, which can be used as a good alternative to the convolutional backbones. Experiments on the Shelf, Campus and CMU Panoptic benchmarks show promising results in terms of both Mean Per Joint Position Error (MPJPE) and Percentage of Correctly estimated Parts (PCP). Our code will be available.},
  archive      = {J_APIN},
  author       = {Chen, Yuxing and Gu, Renshu and Huang, Ouhan and Jia, Gangyong},
  doi          = {10.1007/s10489-023-04805-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26568-26579},
  shortjournal = {Appl. Intell.},
  title        = {VTP: Volumetric transformer for multi-view multi-person 3D pose estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Dual-stream GNN fusion network for hyperspectral
classification. <em>APIN</em>, <em>53</em>(22), 26542–26567. (<a
href="https://doi.org/10.1007/s10489-023-04960-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised Graph Neural Networks (GNNs), as an effective data representation learning framework, have been applied to hyperspectral image (HSI) classification. Although GNNs can quickly capture the structural information of HSIs, traditional GNN-based methods use the entire graph as input, which consumes huge computational resources. Moreover, using a single form of GNN variant cannot effectively extract the correlation between two different dimensions, leading to insufficient improvement in the impact of noise and spectral differences in HSIs. To address these issues, a Dual-Stream GNN Fusion Network (named DGFNet) is proposed in this paper. This end-to-end network uses subcubes as input to reduce computational costs. The spatial branch uses Graph Attention Network (GAT) to capture the inherent relationships within the HSIs, and the risk of overfitting is reduced by combining a novel graph pooling and local guidance module to preserve important features in subcubes. The spectral branch weights the correlations among bands to obtain more reasonable spectral features. Finally, these spatial, spectral, and structural features are fused and classified using linear layers. Extensive comparative experiments on four benchmark datasets have demonstrated that our model achieves competitive performance, and its low computational cost advantage makes it more feasible and practical compared to most GNN-based methods.},
  archive      = {J_APIN},
  author       = {Li, Weiming and Liu, Qikang and Fan, Shuaishuai and Xu, Cong’an and Bai, Hongyang},
  doi          = {10.1007/s10489-023-04960-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26542-26567},
  shortjournal = {Appl. Intell.},
  title        = {Dual-stream GNN fusion network for hyperspectral classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new boundary-degree-based oversampling method for
imbalanced data. <em>APIN</em>, <em>53</em>(22), 26518–26541. (<a
href="https://doi.org/10.1007/s10489-023-04846-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data constitute a significant challenge in practical applications, as standard classifiers are usually designed to work on data with balanced class label distributions. One of effective methods to solve the imbalanced problem is boundary oversampling method, which only focuses on the classification of boundary samples. However, most boundary oversampling methods roughly select boundary samples for oversampling without considering the potentially useful boundary characteristics inherent in majority (negative) class. To overcome this limitation, we propose a novel boundary-degree-based oversampling method (BDO) in this paper. The originality of BDO stemps from quantifying the degree to which each negative sample can be regarded as a boundary sample in terms of probability using information entropy. Applying the sigma rule on the quantified boundary degree, negative boundary samples are determined to indirectly select minority (positive) boundary samples for oversampling. In this way, a substantial amount of information hidden in the negative class can be mined. To further transfer the mined information to help oversample, BDO iteratively synthesizes aided boundary points along a fraudulent gradient. Oversampling finally is performed on both positive boundary samples and the aided boundary points. Experimental results completed on 15 benchmark imbalanced datasets, two multi-label datasets and one large-scale dataset in terms of G-mean, F-measure, AUC, accuracy, TPR and TNR show that BDO exhibits better performance, which is competitive with some commonly considered methods.},
  archive      = {J_APIN},
  author       = {Chen, Yueqi and Pedrycz, Witold and Yang, Jie},
  doi          = {10.1007/s10489-023-04846-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26518-26541},
  shortjournal = {Appl. Intell.},
  title        = {A new boundary-degree-based oversampling method for imbalanced data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TMS: Temporal multi-scale in time-delay neural network for
speaker verification. <em>APIN</em>, <em>53</em>(22), 26497–26517. (<a
href="https://doi.org/10.1007/s10489-023-04953-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The speaker encoder is an important front-end module that explores discriminative speaker features for many speech applications requiring speaker information. Current speaker encoders aggregate multi-scale features from utterances using multi-branch network architectures. However, naively adding many branches through a fully convolutional operation cannot efficiently improve its capability to capture multi-scale features due to the problem of rapid increase of model parameters and computational complexity. Therefore, in current network architectures, only a few branches corresponding to a limited number of temporal scales are designed for capturing speaker features. To address this problem, this paper proposes an effective temporal multi-scale (TMS) model where multi-scale branches could be efficiently designed in a speaker encoder while negligibly increasing computational costs. The TMS model is based on a time-delay neural network (TDNN), where the network architecture is separated into channel-modeling and temporal multi-branch modeling operators. In the TMS model, adding temporal multi-scale elements in the temporal multi-branch operator only slightly increases the model’s parameters, thus saving more of the computational budget to add branches with large temporal scales. After model training, we further develop a systemic re-parameterization method to convert the multi-branch network topology into a single-path-based topology to increase the inference speed.We conducted automatic speaker verification (ASV) experiments under in-domain (VoxCeleb) and out-of-domain (CNCeleb) conditions to investigate the proposed TMS model’s performance.Experimental results show that the TMS-method-based model outperformed state-of-the-art ASV models (e.g., ECAPA-TDNN) and improved robustness. Moreover, the proposed model achieved a 29%–46% increase in the inference speed compared to ECAPA-TDNN.},
  archive      = {J_APIN},
  author       = {Zhang, Ruiteng and Wei, Jianguo and Lu, Xugang and Lu, Wenhuan and Jin, Di and Zhang, Lin and Xu, Junhai and Dang, Jianwu},
  doi          = {10.1007/s10489-023-04953-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26497-26517},
  shortjournal = {Appl. Intell.},
  title        = {TMS: Temporal multi-scale in time-delay neural network for speaker verification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attribute-weighted isometric embedding method for
categorical encoding on mixed data. <em>APIN</em>, <em>53</em>(22),
26472–26496. (<a
href="https://doi.org/10.1007/s10489-023-04899-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed data containing categorical and numerical attributes are widely available in real-world. Before analysing such data, it is typically necessary to process (transform/embed/represent) them into high-quality numerical data. The conditional probability transformation method (CPT) can provide acceptable performance in the majority of cases, but it is not satisfactory for datasets with strong attribute association. Inspired by the one dependence value difference metric method, the concept of relaxing the attributes conditional independence has been applied to CPT, but this approach has the drawback of dramatically-expanding the attribute dimensionality. We employ the isometric embedding method to tackle the problem of dimensionality expansion. In addition, an attribute weighting method based on the must-link and cannot-link constraints is designed to optimize the data transformation quality. Combining these methods, we propose an attribute-weighted isometric embedding (AWIE) for categorical encoding on mixed data. Extensive experimental results obtained on 16 datasets demonstrate that AWIE significantly improves upon the classification performance (increasing the F1-score by 2.54%, attaining 6/16 best results, and reaching average ranks of 1.94/8), compared with 28 competitors.},
  archive      = {J_APIN},
  author       = {Liang, Zupeng and Ji, Shengfen and Li, Qiude and Hu, Sigui and Yu, Yang},
  doi          = {10.1007/s10489-023-04899-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26472-26496},
  shortjournal = {Appl. Intell.},
  title        = {An attribute-weighted isometric embedding method for categorical encoding on mixed data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting signs of model change with continuous model
selection based on descriptive dimensionality. <em>APIN</em>,
<em>53</em>(22), 26454–26471. (<a
href="https://doi.org/10.1007/s10489-023-04780-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the issue of detecting changes of models that lie behind a data stream. The model refers to an integer-valued structural information such as the number of free parameters in a parametric model. Specifically we are concerned with the problem of how we can detect signs of model changes earlier than they are actualized. To this end, we employ continuous model selection on the basis of the notion of descriptive dimensionality (Ddim). It is a real-valued model dimensionality, which is designed for quantifying the model dimensionality in the model transition period. Continuous model selection is to determine the real-valued model dimensionality in terms of Ddim from a given data. We propose a novel methodology for detecting signs of model changes by tracking the rise-up/descent of Ddim in a data stream. We apply this methodology to detecting signs of changes of the number of clusters in a Gaussian mixture model and those of the order in an auto regression model. With synthetic and real data sets, we empirically demonstrate its effectiveness by showing that it is able to visualize well how rapidly model dimensionality moves in the transition period and to raise early warning signals of model changes earlier than they are detected with existing methods.},
  archive      = {J_APIN},
  author       = {Yamanishi, Kenji and Hirai, So},
  doi          = {10.1007/s10489-023-04780-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26454-26471},
  shortjournal = {Appl. Intell.},
  title        = {Detecting signs of model change with continuous model selection based on descriptive dimensionality},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial graph attention network-based object tracking with
adaptive cosine window. <em>APIN</em>, <em>53</em>(22), 26439–26453. (<a
href="https://doi.org/10.1007/s10489-023-04839-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most popular Siamese trackers optimize the classification map from the tracking head using a fixed cosine window penalty. However, this fixed operation, which sets the weight and center of the cosine window to fixed values, can lead to tracking errors when there are similar interferences or the target is out of view. In addition, traditional graph attention networks determine attention weights only based on the cosine similarity between nodes, ignoring the relationship between the positions of nodes in the template and search region. To address these issues, this paper proposes a spatial graph attention network-based object tracking with adaptive cosine window in tracking head. The adaptive cosine window combines spatial-temporal information and adjusts the cosine window, using a positional bias Kalman filter to predict the offset of the target in the search region. The location-based attention mask module considers both the similarity between nodes and their positions in the template and search region, rather than just node similarity, which reduces the impact of similar surroundings. The attention weights between nodes are constrained using a position matrix based on Gaussian functions. Extensive experiments on four challenging public datasets (GOT-10k, UAV123, OTB-100, and LaSOT) show that our tracker outperforms other state-of-the-art trackers.},
  archive      = {J_APIN},
  author       = {Fan, Liu-Yi and Jiang, Xiao-Yan and Huang, Bo and Zhang, Juan and Gao, Yong-Bin},
  doi          = {10.1007/s10489-023-04839-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26439-26453},
  shortjournal = {Appl. Intell.},
  title        = {Spatial graph attention network-based object tracking with adaptive cosine window},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New attention strategy for negative sampling in knowledge
graph embedding. <em>APIN</em>, <em>53</em>(22), 26418–26438. (<a
href="https://doi.org/10.1007/s10489-023-04901-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of knowledge graph embedding (KGE), self-adversarial negative sampling is a recently proposed technique based on the attention mechanism, which pays more attention to the negative triplets with higher embedding scores. Unfortunately, the technique also pays more attention to those false negative triplets with higher embedding scores, which is obviously unreasonable and often leads to a performance downgrade of the KGE model. To alleviate the downgrade, this paper proposes a new attention strategy, aiming at gradually decreasing the attention of high-score false negative triplets. In the new strategy, the attention difference between high-score and low-score negative triplets will be narrowed as the KGE model performance improves, which is more reasonable during training. Experimental results on TransE, DistMult, RotatE, and PairRE show that our proposed strategy indeed has a significant performance improvement for KGE models on the task of linking prediction and triplet classification.},
  archive      = {J_APIN},
  author       = {Cen, Si and Wang, Xizhao and Zou, Xiaoying and Liu, Chao and Dai, Guoquan},
  doi          = {10.1007/s10489-023-04901-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26418-26438},
  shortjournal = {Appl. Intell.},
  title        = {New attention strategy for negative sampling in knowledge graph embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retinal artery/vein classification by multi-channel
multi-scale fusion network. <em>APIN</em>, <em>53</em>(22), 26400–26417.
(<a href="https://doi.org/10.1007/s10489-023-04939-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic artery/vein (A/V) classification in retinal fundus images plays a significant role in detecting vascular abnormalities and could speed up the diagnosis of various systemic diseases. Deep-learning methods have been extensively employed in this task. However, due to the lack of annotated data and the serious data imbalance, the performance of the existing methods is constricted. To address these limitations, we propose a novel multi-channel multi-scale fusion network (MMF-Net) that employs the enhancement of vessel structural information to constrain the A/V classification. First, the newly designed multi-channel (MM) module could extract the vessel structure from the original fundus image by the frequency filters, increasing the proportion of blood vessel pixels and reducing the influence caused by the background pixels. Second, the MMF-Net introduces a multi-scale transformation (MT) module, which could efficiently extract the information from the multi-channel feature representations. Third, the MMF-Net utilizes a multi-feature fusion (MF) module to improve the robustness of A/V classification by splitting and reorganizing the pixel feature from different scales. We validate our results on several public benchmark datasets. The experimental results show that the proposed method could achieve the best result compared with the existing state-of-the-art methods, which demonstrate the superior performance of the MMF-Net. The highly optimized Python implementations of our method is released at: https://github.com/chenchouyu/MMF_Net .},
  archive      = {J_APIN},
  author       = {Yi, Junyan and Chen, Chouyu and Yang, Gang},
  doi          = {10.1007/s10489-023-04939-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26400-26417},
  shortjournal = {Appl. Intell.},
  title        = {Retinal artery/vein classification by multi-channel multi-scale fusion network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early prediction of sepsis using a high-order markov dynamic
bayesian network (HMDBN) classifier. <em>APIN</em>, <em>53</em>(22),
26384–26399. (<a
href="https://doi.org/10.1007/s10489-023-04920-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is among the leading causes of morbidity, mortality and high costs in the ICU. The early prediction and intervention of sepsis is a challenging task under strict time and cost constraints. In this paper, a novel High-order Markov Dynamic Bayesian Network (HMDBN) classifier with discrete features is presented for early prediction of sepsis at a high-order time point. The model structure is learned from the unrolled DBN by performing the K2 algorithm, and the features ‘disappeared’ in the prediction are eliminated using the VE method. Based on a few vital signs and laboratory results, an intuitive causal graph and indicating system are constructed to realize continuous prediction and probabilistic interpretation in real-time. Compared with other ten classical machine learning classifiers on evaluation metrics, HMDBN models have the highest AUROC scores on both internal tests and external validations for sepsis early prediction, and provide identifiable and interpretable results that allowing clinicians to immediately understand the reason for the prediction.},
  archive      = {J_APIN},
  author       = {Zhang, Siwen and Duan, Yongrui and Hou, Fenggang and Yan, Guoliang and Li, Shufang and Wang, Haihui and Zhou, Liang},
  doi          = {10.1007/s10489-023-04920-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26384-26399},
  shortjournal = {Appl. Intell.},
  title        = {Early prediction of sepsis using a high-order markov dynamic bayesian network (HMDBN) classifier},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Joint modeling of user and item preferences with
interaction frequency and attention for knowledge graph-based
recommendation. <em>APIN</em>, <em>53</em>(22), 26364–26383. (<a
href="https://doi.org/10.1007/s10489-023-04914-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, knowledge graphs (KGs) have attracted considerable attention in recommendation research based on auxiliary information. The rich semantic knowledge in a KG can enrich user and item representations and provide more accurate recommendations. Unfortunately, most previous studies failed to incorporate user/item interaction frequency, which plays a critical role in analyzing users’ historical behaviors, into their recommendation models. Furthermore, the importance levels of users’ historical preference features and users’ KG preference features for modeling user preferences are different. Therefore, we propose an approach for jointly modeling the preferences of users and items with interaction frequency and attention (JMPIA), which first leverages an attention network with interaction frequency to obtain users’ and items’ historical preference representations and conducts preference propagation in a KG to obtain users’ KG preference representations for each hop. Then, we leverage an attention aggregator with the ReLU activation function to aggregate these representations to capture more accurate user preferences, thereby promoting recommendation. Finally, we conducted a comprehensive performance evaluation on two real-world datasets. The experimental results obtained on these two datasets demonstrate that the proposed JMPIA approach outperforms the state-of-the-art KG-based methods. These results validate the effectiveness of using an attention network with interaction frequency to derive preferences from users’ historical interaction information and combining them with the rich information in a KG to integrate user preferences.},
  archive      = {J_APIN},
  author       = {Li, Zheng and Liu, Jiahao and Yang, Wei and Liu, Chun},
  doi          = {10.1007/s10489-023-04914-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26364-26383},
  shortjournal = {Appl. Intell.},
  title        = {Joint modeling of user and item preferences with interaction frequency and attention for knowledge graph-based recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mapping natural language procedures descriptions to linear
temporal logic templates: An application in the surgical robotic domain.
<em>APIN</em>, <em>53</em>(22), 26351–26363. (<a
href="https://doi.org/10.1007/s10489-023-04882-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language annotations and manuals can provide useful procedural information and relations for the highly specialized scenario of autonomous robotic task planning. In this paper, we propose and publicly release AUTOMATE, a pipeline for automatic task knowledge extraction from expert-written domain texts. AUTOMATE integrates semantic sentence classification, semantic role labeling, and identification of procedural connectors, in order to extract templates of Linear Temporal Logic (LTL) relations that can be directly implemented in any sufficiently expressive logic programming formalism for autonomous reasoning, assuming some low-level commonsense and domain-independent knowledge is available. This is the first work that bridges natural language descriptions of complex LTL relations and the automation of full robotic tasks. Unlike most recent similar works that assume strict language constraints in substantially simplified domains, we test our pipeline on texts that reflect the expressiveness of natural language used in available textbooks and manuals. In fact, we test AUTOMATE in the surgical robotic scenario, defining realistic language constraints based on a publicly available dataset. In the context of two benchmark training tasks with texts constrained as above, we show that automatically extracted LTL templates, after translation to a suitable logic programming paradigm, achieve comparable planning success in reduced time, with respect to logic programs written by expert programmers.},
  archive      = {J_APIN},
  author       = {Bombieri, Marco and Meli, Daniele and Dall’Alba, Diego and Rospocher, Marco and Fiorini, Paolo},
  doi          = {10.1007/s10489-023-04882-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26351-26363},
  shortjournal = {Appl. Intell.},
  title        = {Mapping natural language procedures descriptions to linear temporal logic templates: An application in the surgical robotic domain},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). More effective and efficient exploration via more refined
gradient information. <em>APIN</em>, <em>53</em>(22), 26329–26350. (<a
href="https://doi.org/10.1007/s10489-023-04955-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploration is a crucial component of reinforcement learning. Although many works employ random noise to explore, few have employed exploration based on action gradients. The common use of random noise exploration methods has several weaknesses; for instance, they encounter the problem of dimension disaster. In contrast, action gradient-based methods only use first-order information, thus utilizing information poorly. To address this gap, we introduce a novel reinforcement learning framework that employs an exploration teacher to teach an agent to explore. This framework uses both first-order and second-order information of the action gradient. Additionally, the attention prediction method in our framework aims to address the problem of Q-value overestimation. Based on our new framework and the TD3 algorithm, we propose an off-policy deterministic actor-critic algorithm, named more Effective and Efficient Exploration via more Refined Gradient Information (3E-RGI). Our 3E-RGI algorithm uses a new stage-wise updating method to increase training efficiency. We compare 3E-RGI with five other off-policy algorithms on six DMControl suite environments. The experiment shows that our algorithm outperforms current reinforcement learning algorithms in various continuous control problems from the DMControl suite.},
  archive      = {J_APIN},
  author       = {Chen, Xiu-yan and Liu, Jian-Wei},
  doi          = {10.1007/s10489-023-04955-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26329-26350},
  shortjournal = {Appl. Intell.},
  title        = {More effective and efficient exploration via more refined gradient information},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics informed neural network for dynamic stress
prediction. <em>APIN</em>, <em>53</em>(22), 26313–26328. (<a
href="https://doi.org/10.1007/s10489-023-04923-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural failures are often caused by catastrophic events such as earthquakes and winds. As a result, it is crucial to predict dynamic stress distributions during highly disruptive events in real time. Currently available high-fidelity methods, such as Finite Element Models (FEMs), suffer from their inherent high complexity. Therefore, to reduce computational cost while maintaining accuracy, a Physics Informed Neural Network (PINN), PINN-Stress model, is proposed to predict the entire sequence of stress distribution based on Finite Element simulations using a partial differential equation (PDE) solver. Using automatic differentiation, we embed a PDE into a deep neural network’s loss function to incorporate information from measurements and PDEs. The PINN-Stress model can predict the sequence of stress distribution in almost real-time and can generalize better than the model without PINN.},
  archive      = {J_APIN},
  author       = {Bolandi, Hamed and Sreekumar, Gautam and Li, Xuyang and Lajnef, Nizar and Boddeti, Vishnu Naresh},
  doi          = {10.1007/s10489-023-04923-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26313-26328},
  shortjournal = {Appl. Intell.},
  title        = {Physics informed neural network for dynamic stress prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A time series classification method combining graph
embedding and the bag-of-patterns algorithm. <em>APIN</em>,
<em>53</em>(22), 26297–26312. (<a
href="https://doi.org/10.1007/s10489-023-04859-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data mining techniques have attracted extensive attention from researchers worldwide. Of these techniques, time series classification is an important part of time series mining. Among the many time series classification algorithms, methods based on the bag-of-patterns algorithm have attracted much attention from researchers because of their high accuracy and execution efficiency. However, when using these methods, only the frequency of different patterns is considered. Features such as the position of patterns in a sequence are not mined. Therefore, the aim of this paper is to determine how to solve the problem that the positional relationships among patterns are ignored when using the bag-of-patterns algorithm. To solve this issue, we introduce the graph embedding technique, and an attempt is made to capture the positional relationships among the patterns of time series from the graph perspective. To verify the performance of the method, we perform extensive experiments with the UCR time series archive, and the experimental results demonstrate that our proposed method generally improves the classification ability of models based on the bag-of-patterns algorithm.},
  archive      = {J_APIN},
  author       = {Ma, Xiaoxuan and Yu, Mengping and Huang, Huan and Hou, Rui and Dong, Mianxiong and Ota, Kaoru and Zeng, Deze},
  doi          = {10.1007/s10489-023-04859-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {22},
  pages        = {26297-26312},
  shortjournal = {Appl. Intell.},
  title        = {A time series classification method combining graph embedding and the bag-of-patterns algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence-adapted meta-interaction for unsupervised person
re-identification. <em>APIN</em>, <em>53</em>(21), 25525–25542. (<a
href="https://doi.org/10.1007/s10489-023-04863-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most unsupervised person re-identification (ReID) approaches combine clustering-based pseudo-label prediction with feature learning, and perform the two steps in an alternating fashion for training ReID models. However, incorrect/noisy pseudo-labels are often present due to various variations (e.g., human pose, illumination, and viewpoint, etc.). Such noisy pseudo-labels may harm the trained ReID models. In order to use diverse variations/information while minimizing negative influence of the noisy pseudo-labels, we propose a confidence-adapted meta-interaction (CAMI) method by explicitly exploring the interaction between the believable supervision (reliable pseudo-labels) and the diverse information. Specifically, CAMI iteratively trains the ReID model in a meta-learning manner, in which the training images are dynamically divided into a reliable set and an unreliable set. At each iteration, the pseudo-labels of images are predicted by clustering and the training images are divided by the proposed confidence-adapted sample disentanglement (CASD) method. To adapt the changes of the pseudo-labels and gradually refine the division, the CASD method dynamically predicts the pseudo-label confidence. It divides the training images into the reliable set (with high confidence pseudo-labels) and the unreliable set (with low confidence pseudo-labels), respectively. Then a meta-interaction method is proposed for training the ReID model, which consists of a meta-training step to use the believable supervision of the reliable set and a meta-testing step to use the diverse information of the unreliable set. Meanwhile, a bridge model is dynamically built to refine the unreliable set based on the believable supervision from the reliable set. The CAMI is evaluated by two unsupervised person ReID settings, including the image-based and the video-based. The experimental results on four datasets demonstrate the superiority of the proposed CAMI.},
  archive      = {J_APIN},
  author       = {Li, Xiaobao and Li, Qingyong and Xue, Wenyuan and Liu, Yang and Liang, Fengjiao and Wang, Wen},
  doi          = {10.1007/s10489-023-04863-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25525-25542},
  shortjournal = {Appl. Intell.},
  title        = {Confidence-adapted meta-interaction for unsupervised person re-identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual channel group-aware graph convolutional networks for
collaborative filtering. <em>APIN</em>, <em>53</em>(21), 25511–25524.
(<a href="https://doi.org/10.1007/s10489-023-04860-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the existing collaborative filtering approaches, graph-based collaborative filtering models that adopt graph convolutional networks for representation learning have achieved state-of-the-art results. However, most graph convolutions on the user-item graph treat all neighbors equally, while different users may have different interests and different items can belong to different categories, aggregating information about all neighbors without discrimination can lead to performance degradation. In addition, the user-item interaction graph is essentially a heterogeneous graph, but most of the existing models regard it as homogeneous, which also can lead to sub-optimal results. To address the above issues, we design a dual channel group-aware graph convolution model, called DG-GCN. Specifically, it first performs message passing on the user-item interaction graph to leverage the direct and higher-order connectivity information for further grouping, and then groups users and items separately through dual group-aware modules based on their latent interests and categories. This scheme allows nodes to learn from more similar homogeneous nodes, thus preventing noisy information from participating in the propagation process. Experimental results show that DG-GCN outperforms the state-of-the-art GCN model on all five benchmark datasets, with up to 5 $$\%$$ and 4 $$\%$$ relative improvements over LightGCN on Recall@20 and NDCG@20, respectively.},
  archive      = {J_APIN},
  author       = {Zhao, Jinsong and Huang, Kaiwen and Li, Ping},
  doi          = {10.1007/s10489-023-04860-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25511-25524},
  shortjournal = {Appl. Intell.},
  title        = {Dual channel group-aware graph convolutional networks for collaborative filtering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid-driven remaining useful life prediction method
combining asymmetric dual-channel autoencoder and nonlinear wiener
process. <em>APIN</em>, <em>53</em>(21), 25490–25510. (<a
href="https://doi.org/10.1007/s10489-023-04855-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining Useful Life (RUL) prediction is an essential aspect of Prognostics and Health Management (PHM), facilitating the assessment of mechanical components’ health statuses and their times to failure. Currently, most deep learning-based RUL prediction methods can achieve accurate RUL point estimations. However, due to sample variability and degradation randomness, point estimations may contain uncertainties. To obtain both RUL prediction values and their corresponding uncertainty estimations, this paper proposes a novel hybrid-driven prediction method that effectively combines an Asymmetric Dual-Channel AutoEncoder and the Nonlinear Wiener Process (ADCAE-NWP). To achieve comprehensive feature extraction, two feature extraction channels are parallelly combined in the encoder. Moreover, to reduce the space-time overhead of the model training process, an asymmetric form of the autoencoder is composed by using only the fully connected layer in the decoder. Subsequently, the ADCAE model is trained to construct health indicators in an unsupervised manner. Finally, the RUL Probability Density Functions (PDFs) are calculated using the NWP. RUL predictions containing uncertainty estimations are obtained by calculating expectations over confidence intervals. The proposed model is experimentally validated and compared on two datasets, and the results demonstrate that the proposed scheme achieves better prediction performance than competing approaches.},
  archive      = {J_APIN},
  author       = {Duan, Yuhang and Liu, Zhen and Li, Honghui and Zhang, Chun and Zhang, Ning},
  doi          = {10.1007/s10489-023-04855-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25490-25510},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid-driven remaining useful life prediction method combining asymmetric dual-channel autoencoder and nonlinear wiener process},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective embedding algorithm for blind image
watermarking technique based on hessenberg decomposition. <em>APIN</em>,
<em>53</em>(21), 25467–25489. (<a
href="https://doi.org/10.1007/s10489-023-04903-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For digital image copyright protection, watermarking techniques are a promising solution and are of interest to many researchers. In watermarking schemes based on matrix transformation, the embedding element and embedding formula play a very important role in maintaining the quality of a watermark image and the robustness of the watermark. In this paper, a blind image watermarking scheme based on Hessenberg decomposition, where the improvement focuses on the embedding element and embedding formula, is proposed. First, the structure of the Hessenberg factorization is analysed to obtain the most suitable embedding element. Accordingly, this is the first time that the element on the second row and the second column of the upper Hessenberg matrix is selected as an embedding element in a Hessenberg-based image watermarking scheme because of its energy concentration and stability. Second, an improved embedding formula is proposed to address the limitations of previous studies. In the proposed formula, constraint conditions are added to limit the change in all blocks, and a scaling factor is applied to guarantee a trade-off between invisibility and robustness. Here, the scaling factor is carefully calculated by repeating various experiments under different image attacks to achieve an optimal value. Therefore, our proposed embedding formula not only minimizes the modification of the host image after embedding but also helps maintain the robustness of the extracted watermark. Third, to increase the security of the proposed scheme, the watermark image is encoded by the Arnold transform before it is embedded into the host image. The experimental results show that the proposed approach defeats the compared methods in terms of invisibility and execution time. Moreover, the proposed scheme can resist most common attacks when the average normalized correlation value is higher than 0.93 and the extracted watermarks are always clearly recognized.},
  archive      = {J_APIN},
  author       = {Nha, Phuong Thi and Thanh, Ta Minh and Phong, Nguyen Tuan},
  doi          = {10.1007/s10489-023-04903-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25467-25489},
  shortjournal = {Appl. Intell.},
  title        = {An effective embedding algorithm for blind image watermarking technique based on hessenberg decomposition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental analysis of similarity measurements for
multivariate time series and its application to the stock market.
<em>APIN</em>, <em>53</em>(21), 25450–25466. (<a
href="https://doi.org/10.1007/s10489-023-04874-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity measurement takes on critical significance in strategies that seek similar stocks based on historical data to make predictions. Stock data refers to a multidimensional time series with features of non-linearity and high noise, posing a challenge to the practical design of similarity measurement. However, the existing similarity measurements cannot better address the negative effects of the singularity of data and correlations of data in multidimensional stock price series, such that the performance of stock prediction will be reduced. In this study, a novel method named dynamic multi-factor similarity measurement (DMFSM) is proposed to accurately describe the similarity between a pair of multidimensional time series. DMFSM is capable of eliminating effects exerted by singularity and correlations of data using dynamic time warping (DTW) with Mahalanobis distance embedded and weights of series nodes in multidimensional time series. To validate the efficiency of DMFSM, several experiments were performed on a total of 675 stocks, which comprised 290 stocks from the Shanghai Stock Exchange, 285 stocks from the Shenzhen Stock Exchange, as well as 100 stocks from the Growth Enterprise Market of the Shenzhen Stock Exchange. The experiment results for mean absolute error of predictions indicated that DMFSM (0.018) outperformed similarity measurements (e.g., Euclidean distance (0.023), DTW (0.054), and dynamic multi-perspective personalized similarity measurement (0.023)).},
  archive      = {J_APIN},
  author       = {Xiang, Zhong-Liang and Wang, Rui and Yu, Xiang-Ru and Li, Bo and Yu, Yuan},
  doi          = {10.1007/s10489-023-04874-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25450-25466},
  shortjournal = {Appl. Intell.},
  title        = {Experimental analysis of similarity measurements for multivariate time series and its application to the stock market},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDSCNet: An instance segmentation network for efficient
monitoring of goose breeding conditions. <em>APIN</em>, <em>53</em>(21),
25435–25449. (<a
href="https://doi.org/10.1007/s10489-023-04743-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improve the scientific level of the goose breeding industry and help the development of intelligent agriculture. Instance Segmentation has a pivotal role when the breeders make decisions about geese breeding. It can be used for disease prevention, body size estimation and behavioural prediction, etc. However, instance segmentation requires high performance computing devices to run smoothly due to its rich output. To ameliorate this problem, this paper constructs a novel encoder-decoder module and proposes the SDSCNet model. The reasonable use of depth-separable convolution in the module reduces the number and size of model parameters and increase execution speed. Finally, SDSCNet model enables real-time identification and segmentation of individual geese with the accuracy reached 0.933.We compare this model with numerous mainstream instance segmentation models, and the final results demonstrate the excellent performance of our model.Furthermore, deploying SDSCNet model on the embedded device Raspberry Pi 4 Model B can achieve effective detection of continuous moving scenes.},
  archive      = {J_APIN},
  author       = {Li, Jiao and Su, Houcheng and Li, Jianing and Xie, Tianyu and Chen, Yijie and Yuan, Jianan and Jiang, Kailin and Duan, Xuliang},
  doi          = {10.1007/s10489-023-04743-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25435-25449},
  shortjournal = {Appl. Intell.},
  title        = {SDSCNet: An instance segmentation network for efficient monitoring of goose breeding conditions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernelized global-local discriminant information
preservation for unsupervised domain adaptation. <em>APIN</em>,
<em>53</em>(21), 25412–25434. (<a
href="https://doi.org/10.1007/s10489-023-04706-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual recognition has become inevitable in applications such as object detection, biometric tracking, autonomous vehicles, and social media platforms. The images have multiple factors such as image resolution, illumination, perspective and noise, resulting in a significant mismatch between the training and testing domains. Unsupervised Domain adaptation (DA) has proven an effective way to reduce the differences by adapting the knowledge from a richly labeled source domain to an unlabeled target domain. But the real-time datasets are non-linear and high-dimensional. Though kernelization can handle the non-linearity in data, the dimension needs to be reduced as the salient features of the data lie in a low-dimensional subspace. Current dimensionality reduction approaches in DA preserve either the global or local part of information of manifold data. Specifically, the data manifold’s static (subject-invariant) and dynamic (intra-subject variant) information need to be considered during knowledge transfer. Therefore, to preserve both parts of information Globality-Locality Preserving Projection (GLPP) method is applied to the labeled source domain. The other objectives are preserving the discriminant information and variance of target data, and minimizing the distribution and subspace differences between the domains. With all these objectives, we propose a unique method known as Kernelized Global-Local Discriminant Information Preservation for unsupervised DA (KGLDIP). KGLDIP aims to reduce the discrimination discrepancy geometrically and statistically between the two domains after calculating two projection matrices for each domain. Intensive experiments are conducted using five standard datasets and the analysis reveals that the proposed algorithm excels the other state-of-the-art DA approaches.},
  archive      = {J_APIN},
  author       = {R, Lekshmi and Sanodiya, Rakesh Kumar and Jose, Babita Roslind and Mathew, Jimson},
  doi          = {10.1007/s10489-023-04706-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25412-25434},
  shortjournal = {Appl. Intell.},
  title        = {Kernelized global-local discriminant information preservation for unsupervised domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DyGCN-LSTM: A dynamic GCN-LSTM based encoder-decoder
framework for multistep traffic prediction. <em>APIN</em>,
<em>53</em>(21), 25388–25411. (<a
href="https://doi.org/10.1007/s10489-023-04871-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent transportation systems (ITS) are gaining attraction in large cities for better traffic management. Traffic forecasting is an important part of ITS, but a difficult one due to the intricate spatiotemporal relationships of traffic between different locations. Despite the fact that remote or far sensors may have temporal and spatial similarities with the predicting sensor, existing traffic forecasting research focuses primarily on modeling correlations between neighboring sensors while disregarding correlations between remote sensors. Furthermore, existing methods for capturing spatial dependencies, such as graph convolutional networks (GCNs), are unable to capture the dynamic spatial dependence in traffic systems. Self-attention-based techniques for modeling dynamic correlations of all sensors currently in use overlook the hierarchical features of roads and have quadratic computational complexity. Our paper presents a new Dynamic Graph Convolution LSTM Network (DyGCN-LSTM) to address the aforementioned limitations. The novelty of DyGCN-LSTM is that it can model the underlying non-linear spatial and temporal correlations of remotely located sensors at the same time. Experimental investigations conducted using four real-world traffic data sets show that the suggested approach is superior to state-of-the-art benchmarks by $$\varvec{25\%}$$ in terms of RMSE.},
  archive      = {J_APIN},
  author       = {Kumar, Rahul and Mendes Moreira, João and Chandra, Joydeep},
  doi          = {10.1007/s10489-023-04871-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25388-25411},
  shortjournal = {Appl. Intell.},
  title        = {DyGCN-LSTM: A dynamic GCN-LSTM based encoder-decoder framework for multistep traffic prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-adaptable radar-based people counting via few-shot
learning. <em>APIN</em>, <em>53</em>(21), 25359–25387. (<a
href="https://doi.org/10.1007/s10489-023-04778-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many industrial or healthcare contexts, keeping track of the number of people is essential. Radar systems, with their low overall cost and power consumption, enable privacy-friendly monitoring in many use cases. Yet, radar data are hard to interpret and incompatible with most computer vision strategies. Many current deep learning-based systems achieve high monitoring performance but are strongly context-dependent. In this work, we show how context generalization approaches can let the monitoring system fit unseen radar scenarios without adaptation steps. We collect data via a 60 GHz frequency-modulated continuous wave in three office rooms with up to three people and preprocess them in the frequency domain. Then, using meta learning, specifically the Weighting-Injection Net, we generate relationship scores between the few training datasets and query data. We further present an optimization-based approach coupled with weighting networks that can increase the training stability when only very few training examples are available. Finally, we use pool-based sampling active learning to fine-tune the model in new scenarios, labeling only the most uncertain data. Without adaptation needs, we achieve over 80% and 70% accuracy by testing the meta learning algorithms in new radar positions and a new office, respectively.},
  archive      = {J_APIN},
  author       = {Mauro, Gianfranco and Martinez-Rodriguez, Ignacio and Ott, Julius and Servadei, Lorenzo and Wille, Robert and P. Cuellar, Manuel and Morales-Santos, Diego P.},
  doi          = {10.1007/s10489-023-04778-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25359-25387},
  shortjournal = {Appl. Intell.},
  title        = {Context-adaptable radar-based people counting via few-shot learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards evolving software recommendation with time-sliced
social and behavioral information. <em>APIN</em>, <em>53</em>(21),
25343–25358. (<a
href="https://doi.org/10.1007/s10489-023-04852-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software recommendations play a crucial role in helping developers discover potential functional requirements and improve development efficiencies. As new requirements emerge in the software development process, developers’ preferences tend to change over time and social relationships. However, the existing works fall short of capturing the evolution of developers’ interests. To overcome these problems, evolving software recommendation with time-sliced social and behavioral information is proposed for capturing the dynamic interests of developers. Specifically, the different behaviors of developers are considered and graph structure features on projects are extracted by gated graph neural networks. Then, the graph attention networks are introduced to model rich developer-project interactions and social aggregation. Finally, the integration of time-sliced representations on the developer and project sides is employed through gated recurrent units to capture the dynamic interests of developers. Extensive experiments conducted on three datasets demonstrate the superiority of the proposed model over representative baseline methods across various evaluation metrics.},
  archive      = {J_APIN},
  author       = {Chen, Hongqi and Feng, Zhiyong and Chen, Shizhan and Xue, Xiao and Wu, Hongyue and Sun, Yingchao and Xu, Yanwei and Han, Gaoyong},
  doi          = {10.1007/s10489-023-04852-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25343-25358},
  shortjournal = {Appl. Intell.},
  title        = {Towards evolving software recommendation with time-sliced social and behavioral information},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent auto-encoder with multi-resolution ensemble and
predictive coding for multivariate time-series anomaly detection.
<em>APIN</em>, <em>53</em>(21), 25330–25342. (<a
href="https://doi.org/10.1007/s10489-023-04764-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large-scale time-series data can easily be found in real-world applications, multivariate time-series anomaly detection has played an essential role in diverse industries. It enables productivity improvement and maintenance cost reduction by preventing malfunctions and detecting anomalies based on time-series data. However, multivariate time-series anomaly detection is challenging because real-world time-series data exhibit complex temporal dependencies. For this task, it is crucial to learn a rich representation that effectively contains the nonlinear temporal dynamics of normal behavior. In this study, we propose an unsupervised multivariate time-series anomaly detection model named RAE-MEPC which learns informative normal representations based on multi-resolution ensemble reconstruction and predictive coding. We introduce multi-resolution ensemble encoding to capture the multi-scale dependency from the input time series. The encoder hierarchically aggregates the multi-scale temporal features extracted from the sub-encoders with different encoding lengths. From these encoded features, the reconstruction decoder reconstructs the input time series based on multi-resolution ensemble decoding where lower-resolution information helps to decode sub-decoders with higher-resolution outputs. Predictive coding is further introduced to encourage the model to learn more temporal dependencies of the time series. Experiments on real-world benchmark datasets show that the proposed model outperforms the benchmark models for multivariate time-series anomaly detection.},
  archive      = {J_APIN},
  author       = {Choi, Heejeong and Kim, Subin and Kang, Pilsung},
  doi          = {10.1007/s10489-023-04764-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25330-25342},
  shortjournal = {Appl. Intell.},
  title        = {Recurrent auto-encoder with multi-resolution ensemble and predictive coding for multivariate time-series anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). STRAN: Student expression recognition based on
spatio-temporal residual attention network in classroom teaching videos.
<em>APIN</em>, <em>53</em>(21), 25310–25329. (<a
href="https://doi.org/10.1007/s10489-023-04858-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to obtain the state of students’ listening in class objectively and accurately, we can obtain students’ emotions through their expressions in class and cognitive feedback through their behaviors in class, and then integrate the two to obtain a comprehensive assessment results of classroom status. However, when obtaining students’ classroom expressions, the major problem is how to accurately and efficiently extract the expression features from the time dimension and space dimension of the class videos. In order to solve the above problems, we propose a class expression recognition model based on spatio-temporal residual attention network (STRAN), which could extract facial expression features through convolution operation in both time and space dimensions on the basis of limited resources, shortest time consumption and optimal performance. Specifically, STRAN firstly uses the residual network with the three-dimensional convolution to solve the problem of network degradation when the depth of the convolutional neural network increases, and the convergence speed of the whole network is accelerated at the same number of layers. Secondly, the spatio-temporal attention mechanism is introduced so that the network can effectively focus on the important video frames and the key areas within the frames. In order to enhance the comprehensiveness and correctness of the final classroom evaluation results, we use deep convolutional neural network to capture students’ behaviors while obtaining their classroom expressions. Then, an intelligent classroom state assessment method(Weight_classAssess) combining students’ expressions and behaviors is proposed to evaluate the classroom state. Finally, on the basis of the public datasets CK+ and FER2013, we construct two more comprehensive synthetic datasets CK+_Class and FER2013_Class, which are more suitable for the scene of classroom teaching, by adding some collected video sequences of students in class and images of students’ expressions in class. The proposed method is compared with the existing methods, and the results show that STRAN can achieve 93.84% and 80.45% facial expression recognition rates on CK+ and CK+_Class datasets, respectively. The accuracy rate of classroom intelligence assessment of students based on Weight_classAssess also reaches 78.19%, which proves the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Chen, Zheng and Liang, Meiyu and Xue, Zhe and Yu, Wanying},
  doi          = {10.1007/s10489-023-04858-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25310-25329},
  shortjournal = {Appl. Intell.},
  title        = {STRAN: Student expression recognition based on spatio-temporal residual attention network in classroom teaching videos},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage progressive shadow removal network.
<em>APIN</em>, <em>53</em>(21), 25296–25309. (<a
href="https://doi.org/10.1007/s10489-023-04856-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing image shadows has been a challenging task in computer vision due to its diversity and complexity. Shadow removal techniques have been greatly enhanced by deep learning and shadow image datasets, but state-of-the-art methods generally consider the information of the shadow and its neighborhood, ignoring the correlation of the features between the shadow and non-shadow regions. It leads to the resulting image presenting poor overall consistency and unnatural boundary between the original shadow and non-shadow areas. To obtain a consistent and natural shadow removal result, a two-stage progressive shadow removal network is proposed. The first stage performs a multi-exposure fusion network (MEFN) to roughly recover the shadow region features, while in the second stage, a fine-recovery network (FRN) is performed to extract the correlation among the global image contexts, accompanied by a detail feature fusion step. This coarse-to-fine process improves the overall effect of shadow removal, in terms of image quality and boundary consistency. Extensive experiments on the widely used ISTD, ISTD+ and SRD datasets show that the proposed shadow removal network outperforms most of the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Xu, Zile and Chen, Xin},
  doi          = {10.1007/s10489-023-04856-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25296-25309},
  shortjournal = {Appl. Intell.},
  title        = {A two-stage progressive shadow removal network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint learning of graph and latent representation for
unsupervised feature selection. <em>APIN</em>, <em>53</em>(21),
25282–25295. (<a
href="https://doi.org/10.1007/s10489-023-04893-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data samples in real-world applications are not only related to high-dimensional features, but also related to each other. To fully exploit the interconnection between data samples, some recent methods embed latent representation learning into unsupervised feature selection and are proven effective. Despite superior performance, we observe that existing methods first predefine a similarity graph, and then perform latent representation learning based feature selection with this graph. Since fixed graph is obtained from the original feature space containing noisy features and the graph construction process is independent of the feature selection task, this makes the prefixed graph unreliable and ultimately hinders the efficiency of feature selection. To solve this problem, we propose joint learning of graph and latent representation for unsupervised feature selection (JGLUFS). Different from previous methods, we integrate adaptive graph construction into a feature selection method based on the latent representation learning, which not only reduces the impact of external conditions on the quality of graph but also enhances the connection between graph learning and latent representation learning for benefiting the feature selection task. These three basic tasks, including graph learning, latent representation learning and feature selection, cooperate with each other and lead to a better solution. An efficient algorithm with guaranteed convergence is carefully designed to solve the optimization problem of the algorithm. Extensive clustering experiments verify the competitiveness of JGLUFS compared to several state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Xie, Xijiong and Cao, Zhiwen and Sun, Feixiang},
  doi          = {10.1007/s10489-023-04893-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25282-25295},
  shortjournal = {Appl. Intell.},
  title        = {Joint learning of graph and latent representation for unsupervised feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GFF-net: Graph-based feature fusion network for diagnosing
plus disease in retinopathy of prematurity. <em>APIN</em>,
<em>53</em>(21), 25259–25281. (<a
href="https://doi.org/10.1007/s10489-023-04766-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinopathy of prematurity (ROP) is a retinal proliferative disorder, and it is the primary cause of childhood blindness. Accurate and convenient automatic diagnostic tools are required to assist ophthalmologists in diagnosing ROP. Existing methods only extract information from fundus image captured from posterior angle, while images captured from other angles are ignored, which limits the performance of the algorithm. In this paper, we propose a graph-based feature fusion network (GFF-Net) that can jointly analyze multiple images and make full use of the relevant information between these images to diagnose the plus disease in ROP. The convolutional features of different fundus images are connected into a graph, where the edges of the graph model the correlation between these images. A graph-based feature fusion module is proposed to aggregate features from the constructed feature graph and produce the final prediction. We compared the proposed GFF-Net with state-of-the-art methods on a clinical dataset and a low-quality “attack dataset&quot;. The GFF-Net achieved superior performance compared to other methods on both datasets. The results show that the proposed GFF-Net could be more effective than existing methods in clinical practice.},
  archive      = {J_APIN},
  author       = {Huang, Kaide and Dong, Wentao and Li, Jie and Chen, Yuanyuan and Zhong, Jie and Yi, Zhang},
  doi          = {10.1007/s10489-023-04766-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25259-25281},
  shortjournal = {Appl. Intell.},
  title        = {GFF-net: Graph-based feature fusion network for diagnosing plus disease in retinopathy of prematurity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised nested dirichlet finite mixture model for
clustering. <em>APIN</em>, <em>53</em>(21), 25232–25258. (<a
href="https://doi.org/10.1007/s10489-023-04888-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dirichlet distribution is widely used in the context of mixture models. Despite its flexibility, it still suffers from some limitations, such as its restrictive covariance matrix and its direct proportionality between its mean and variance. In this work, a generalization over the Dirichlet distribution, namely the Nested Dirichlet distribution, is introduced in the context of finite mixture model providing more flexibility and overcoming the mentioned drawbacks, thanks to its hierarchical structure. The model learning is based on the generalized expectation-maximization algorithm, where parameters are initialized with the method of moments and estimated through the iterative Newton-Raphson method. Moreover, the minimum message length criterion is proposed to determine the best number of components that describe the data clusters by the finite mixture model. The Nested Dirichlet distribution is proven to be part of the exponential family, which offers several advantages, such as the calculation of several probabilistic distances in closed forms. The performance of the Nested Dirichlet mixture model is compared to the Dirichlet mixture model, the generalized Dirichlet mixture model, and the Convolutional Neural Network as a deep learning network. The excellence of the powerful proposed framework is validated through this comparison via challenging datasets. The hierarchical feature of the model is applied to real-world challenging tasks such as hierarchical cluster analysis and hierarchical feature learning, showing a significant improvement in terms of accuracy.},
  archive      = {J_APIN},
  author       = {Alkhawaja, Fares and Bouguila, Nizar},
  doi          = {10.1007/s10489-023-04888-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25232-25258},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised nested dirichlet finite mixture model for clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ternary symmetric fusion network for camouflaged object
detection. <em>APIN</em>, <em>53</em>(21), 25216–25231. (<a
href="https://doi.org/10.1007/s10489-023-04898-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflage object detection (COD) is designed to locate objects that are “seamlessly” embedded in the surrounding environment. Camouflaged object detection is a challenging task due to the high intrinsic similarities between objects and their backgrounds, as well as the low boundary contrast between them. To address this problem, this paper proposes a new ternary symmetric fusion network (TSFNet), which can detect camouflaged objects by fully fusing features of different levels and scales. Specifically, the network proposed in this paper mainly contains two key modules: the location-attention search (LAS) module and the ternary symmetric interaction fusion (TSIF) module. The location-attention search module makes full use of contextual information to position potential target objects from a global perspective while enhancing feature representation and guiding feature fusion. The ternary symmetric interaction fusion module consists of three branches: bilateral branches gather rich contextual information of multi-level features, and a middle branch provides fusion attention coefficients for the other two branches. The strategy can effectively achieve information fusion between low- and high-level features, and then achieve the refinement of edge details. Experimental results show that the method is an effective COD model and outperforms existing models. Compared with the existing model SINetV2, TSFNet significantly improves the performance by 3.5% weighted F-measure and 8.1% MAE on the COD10K.},
  archive      = {J_APIN},
  author       = {Deng, Yangyang and Ma, Jianxin and Li, Yajun and Zhang, Min and Wang, Li},
  doi          = {10.1007/s10489-023-04898-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25216-25231},
  shortjournal = {Appl. Intell.},
  title        = {Ternary symmetric fusion network for camouflaged object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced generative adversarial networks for bearing
imbalanced fault diagnosis of rotating machinery. <em>APIN</em>,
<em>53</em>(21), 25201–25215. (<a
href="https://doi.org/10.1007/s10489-023-04870-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional rolling bearing fault diagnosis approaches require a large amount of fault data in advance, while some specific fault data is difficult to obtain in engineering scenarios. This imbalanced fault data problem seriously affects the accuracy of fault diagnosis. To improve the accuracy under imbalanced data conditions, we propose a novel data augmentation method of Enhanced Generative Adversarial Networks with Data Selection Module (EGAN-DSM). Firstly, a network enhancement module is designed, which quantifies antagonism between the generator and discriminator through loss value. And the module determines whether to iteratively enhance the networks with weak adversarial ability. Secondly, a Data Selected Module (DSM) is constructed using Hilbert space distance for screening generated data, and the screened data is mixed with original imbalanced data to reconstruct balanced data sets. Then, Deep Convolutional Neural Networks with Wide First-layer Kernels (WDCNN) is used for fault diagnosis. Finally, the method is verified by data measured on a rotating machine experimental platform. The results show that our method has high fault diagnosis accuracy under the condition of imbalanced data.},
  archive      = {J_APIN},
  author       = {Hou, Yandong and Ma, Jiulong and Wang, Jinjin and Li, Tianzhi and Chen, Zhengquan},
  doi          = {10.1007/s10489-023-04870-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25201-25215},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced generative adversarial networks for bearing imbalanced fault diagnosis of rotating machinery},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview learning of homogeneous neighborhood of nodes for
the node representation of heterogeneous graph. <em>APIN</em>,
<em>53</em>(21), 25184–25200. (<a
href="https://doi.org/10.1007/s10489-023-04907-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview learning has caught the interest of many graph researchers because it can learn richer information about graphs from different views. Recently, multiview learning, as a novel paradigm in learning, has been widely applied to learn nodes representation of heterogeneous graphs, such as MVSE, HeMI, etc., they only utilize the local homogeneous neighborhood information of nodes, which degrades the quality of nodes representation. We are aware that the heterogeneous graph representation aims to drive the representation of a node to be near the homogeneous neighbors that are similar to it in the heterogeneous graph and far wary from heterogeneous neighbors. Besides, in the heterogeneous graph, linked nodes are more likely to be dissimilar, but remote nodes may have some similarities. Therefore, we can move the locality of a node to discover more homogenous neighbors’ information to improve the quality of node representation. In this work, we propose an unsupervised heterogeneous graph embedding technique that is simple yet efficient; and devise a systematic way to learn node embeddings from the local and global views of the homogeneous neighborhood of nodes by introducing a regularization framework that minimizes the disagreements among the local and global node embeddings under the specific meta-path. Inspired by Personal PageRank graph diffusion, we expand an infinite meta path-based restart random walk to obtain global homogenous neighbors of nodes and construct a meta path-based diffusion matrix to represent the relation between global homogenous neighbors and nodes. Finally, we employ mini-batch gradient descent to train our model to reduce computational consumption. Experimental findings demonstrate that our approach outperforms a wide variety of baselines on different datasets when it comes to node classification and node clustering tasks, with a particularly impressive 7.22% improvement over the best baseline on the ACM dataset.},
  archive      = {J_APIN},
  author       = {Li, Dongjie and Li, Dong and Liu, Hao},
  doi          = {10.1007/s10489-023-04907-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25184-25200},
  shortjournal = {Appl. Intell.},
  title        = {Multiview learning of homogeneous neighborhood of nodes for the node representation of heterogeneous graph},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSKGE: A time-saving knowledge graph embedding framework
based on structure enhancement and semantic guidance. <em>APIN</em>,
<em>53</em>(21), 25171–25183. (<a
href="https://doi.org/10.1007/s10489-023-04896-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In knowledge graph embedding, an attempt is made to embed the objective facts and relationships expressed in the form of triplets into multidimensional vector space, facilitating various applications, such as link prediction and question answering. Structure embedding models focus on the graph structure while the importance of language semantics in inferring similar entities and relations is ignored. Semantic embedding models use pretrained language models to learn entity and relation embeddings based on text information, but they do not fully exploit graph structures that reflect relation patterns and mapping attributes. Structure and semantic information in knowledge graphs represent different hierarchical properties that are indispensable for comprehensive knowledge representation. In this paper, we propose a general knowledge graph embedding framework named SSKGE, which considers both the graph structure and language semantics and learns these two complementary characteristics to integrate entity and relation representations. To compensate for semantic embedding approaches that ignore the graph structure, we first design a structure loss function to explicitly model the graph structure attributes. Second, we leverage a pretrained language model that has been fine-tuned by the structure loss to guide the structure embedding approaches in enhancing the semantic information they lack and obtaining universal knowledge representations. Specifically, guidance is provided by a distance function that makes the spatial distribution of the two types of graph embeddings have a certain similarity. SSKGE significantly reduces the time cost of using a pretrained language model to complete a knowledge graph. Common knowledge graph embedding models such as TransE, DistMult, ComplEx, RotatE, PairRE, and HousE have achieved better results with multiple datasets, including FB15k, FB15k-237, WN18, and WN18RR, using the SSKGE framework. Extensive experiments and analyses have verified the effectiveness and practicality of SSKGE.},
  archive      = {J_APIN},
  author       = {Wang, Tao and Shen, Bo and Zhong, Yu},
  doi          = {10.1007/s10489-023-04896-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25171-25183},
  shortjournal = {Appl. Intell.},
  title        = {SSKGE: A time-saving knowledge graph embedding framework based on structure enhancement and semantic guidance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised robust graph neural networks against noisy
graphs and noisy labels. <em>APIN</em>, <em>53</em>(21), 25154–25170.
(<a href="https://doi.org/10.1007/s10489-023-04836-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we first explore a novel problem of training the robust Graph Neural Networks (GNNs) against noisy graphs and noisy labels. To the problem, we propose a general Self-supervised Robust Graph Neural Network framework that consists of three modules: graph structure learning, sample selection, and self-supervised learning. Specifically, we first employ a graph structure learning approach to obtain an optimal graph structure. Next, using this structure, we use a clustering algorithm to generate pseudo-labels that represent the clusters. We then design a sample selection strategy based on these pseudo-labels to select nodes with clean labels. Additionally, we introduce a self-supervised learning technique where low-level layer parameters are shared with GNNs to predict pseudo-labels. We jointly train the graph structure learning module, the GNNs model, and the self-supervised model. Finally, we conduct extensive experiments on four real-world datasets, demonstrating the superiority of our methods compared with state-of-the-art methods for semi-supervised node classification under noisy graphs and noisy labels.},
  archive      = {J_APIN},
  author       = {Yuan, Jinliang and Yu, Hualei and Cao, Meng and Song, Jianqing and Xie, Junyuan and Wang, Chongjun},
  doi          = {10.1007/s10489-023-04836-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25154-25170},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised robust graph neural networks against noisy graphs and noisy labels},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From multi-label learning to cross-domain transfer: A
model-agnostic approach. <em>APIN</em>, <em>53</em>(21), 25135–25153.
(<a href="https://doi.org/10.1007/s10489-023-04841-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning it has been widely assumed in the literature that, to obtain best accuracy, the dependence among the labels should be explicitly modeled. This premise led to a proliferation of methods offering techniques to learn and predict labels together (joint modeling). Even though it is now acknowledged that in many contexts a model of dependence is not required for optimal performance, such models continue to outperform independent models in some of those very contexts, suggesting alternative explanations for their performance beyond label dependence. In this article we turn the original premise of multi-label learning on its head, and approach the problem of joint-modeling specifically under the absence of any measurable dependence among task labels. The insights from this study allow us to design a method for cross-domain transfer learning which, unlike most contemporary methods of this type, is model-agnostic (any base model class can be considered) and does not require any access to source data. The results we obtain have important implications and we provide clear directions for future work, both in the areas of multi-label and transfer learning.},
  archive      = {J_APIN},
  author       = {Read, Jesse},
  doi          = {10.1007/s10489-023-04841-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25135-25153},
  shortjournal = {Appl. Intell.},
  title        = {From multi-label learning to cross-domain transfer: A model-agnostic approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discriminative multiple-manifold network for image set
classification. <em>APIN</em>, <em>53</em>(21), 25119–25134. (<a
href="https://doi.org/10.1007/s10489-023-04900-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because the distinct advantages of manifold-learning methods for feature extraction, Riemannian manifolds have been used extensively in image recognition tasks in recent years. However, large intra-class variation is a major problem for researchers. Although recent studies have alleviated this problem by extending nonlinear learning to Riemannian manifolds, important classification information may be lost when describing image sets with a single manifold. To address this problem, we design a novel discriminative multiple-manifold network (DMMNet) for image recognition. The Grassmann and symmetric positive definite manifold are first used simultaneously to model each image set. Then, the symmetric positive definite manifold and Grassmann manifold nonlinear learning blocks are devised to transform the data into low-dimensional manifold matrices to enhance their discriminability. A feature fusion layer is designed to fuse the complementary features of different manifolds, and a point-to-point hidden layer is devised to focus on more important feature maps. To acquire the importance of the manifold feature maps, a feature selection method based on manifold feature importance is designed to learn the corresponding weight coefficients. Finally, an output layer is used to map the optimal feature subset into the Euclidean space, such that the kernel discriminant analysis and K-nearest neighbor algorithm are applied. The proposed DMMNet is compared with state-of-the-art techniques on four datasets. Extensive experimental results demonstrate the effectiveness of the proposed DMMNet.},
  archive      = {J_APIN},
  author       = {Wu, Hao and Wang, Weigang and Xia, Zishan and Chen, Yonghao and Liu, Yuanjian and Chen, Jianfei},
  doi          = {10.1007/s10489-023-04900-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25119-25134},
  shortjournal = {Appl. Intell.},
  title        = {A discriminative multiple-manifold network for image set classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new algorithm using integer programming relaxation for
privacy-preserving in utility mining. <em>APIN</em>, <em>53</em>(21),
25106–25118. (<a
href="https://doi.org/10.1007/s10489-023-04913-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility itemset mining (HUIM) is an effective technique for discovering significant information in data. However, data containing sensitive and private information may cause privacy concerns. Therefore, privacy preserving utility mining (PPUM) has recently become a critical research area. PPUM is the process of transforming a quantitative transactional database into a sanitised one, thus ensuring that utility mining algorithms cannot discover sensitive information. The sanitisation process can have several side effects, including the loss of non-sensitive information and the introduction of redundant information. Additionally, the running times of heuristic algorithms for sanitising data are high. To minimise negative effects and lower the execution time of the hiding process, we propose the G-ILP algorithm with a GPU parallel programming method for preprocessing and a new efficient constraint satisfaction problem for hiding data. The experimental evaluations of G-ILP show the algorithm’s efficiency in terms of running time and its ability to minimise side effects in large datasets.},
  archive      = {J_APIN},
  author       = {Nguyen, Duc and Tran, Minh-Thai and Le, Bac},
  doi          = {10.1007/s10489-023-04913-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25106-25118},
  shortjournal = {Appl. Intell.},
  title        = {A new algorithm using integer programming relaxation for privacy-preserving in utility mining},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Online object-level SLAM with dual bundle adjustment.
<em>APIN</em>, <em>53</em>(21), 25092–25105. (<a
href="https://doi.org/10.1007/s10489-023-04854-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-level landmarks enable the SLAM system to construct robust object-keyframe constraints of bundle adjustment and improve the pose estimation performance. In this paper, we present a real-time online object-level SLAM. The dual Bundle Adjustment (BA) optimization method, including high and low frequencies, is proposed to optimize the estimated pose. The High-frequency BA (HBA) module is used to quickly estimate the camera pose by matching landmarks of keyframes and feature points of the current frame. Then, the estimated camera pose is used in the Low-frequency BA (LBA) module to improve the trajectory accuracy. The LBA module integrates the object-level landmarks into the pose graph to optimize the camera pose of local mapping. Moreover, we build an additional object detection thread to extract object 2D bounding boxes online. While this paper improves the data association through the depth projection of point-line features and the Euclidean distance of object centroid. Experimental results show that our proposed algorithm effectively reduce the drift error of camera pose estimation and improve the accuracy by a large margin on different datasets.},
  archive      = {J_APIN},
  author       = {Liu, Jiaqi and Gao, Yongbin and Jiang, Xiaoyan and Fang, Zhijun},
  doi          = {10.1007/s10489-023-04854-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25092-25105},
  shortjournal = {Appl. Intell.},
  title        = {Online object-level SLAM with dual bundle adjustment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Steering the spread of influence adaptively in social
networks via a discrete scheduled particle swarm optimization.
<em>APIN</em>, <em>53</em>(21), 25070–25091. (<a
href="https://doi.org/10.1007/s10489-023-04884-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical influence maximization (IM) in social network analysis aims at selecting a group of influential nodes as seed so that the number of nodes activated by the set is maximized when the spreading process completes. The single stage seeding problem would ignite all the seeds at the very beginning of the diffusion and let the influence diffuses passively with no additional supervision. However, the effect of scheduled seeding activation operations on improving the marginal profit tends to be ignored. More importantly, the practical scenarios that need external seeding activities can not be well depicted. In this paper, seed activation strategies are investigated for the adaptive influence maximization (AIM) problem under general feedback models. A novel bio-inspired meta-heuristic policy named discrete scheduled particle swarm optimization (DSPSO) is proposed to tackle the intractable problem confronted by AIM, i.e., which nodes need to be ignited at the right time during the process of influence spreading? Following the framework, the proposed policy selects optimal size of nodes into the seed set in each round to ensure the continuation of the spreading process. Dynamic encoding mechanism for the particle individuals is constructed. To make a full exploration of the solution space, a local search strategy specifically for the discrete network topology is implemented on the historical best individuals of the swarm. Extensive experiments on four social networks under different activation feedback models demonstrate the advantage of the scheduled seeding mechanism over the single stage seeding approaches, and show that the proposed DSPSO outperforms the existing greedy strategy as well as the topological heuristics.},
  archive      = {J_APIN},
  author       = {Tang, Jianxin and Song, Shihui and Lan, Jimao and Zhang, Li and Zhao, Fuqing},
  doi          = {10.1007/s10489-023-04884-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25070-25091},
  shortjournal = {Appl. Intell.},
  title        = {Steering the spread of influence adaptively in social networks via a discrete scheduled particle swarm optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exponential information fractal dimension weighted risk
priority number method for failure mode and effects analysis.
<em>APIN</em>, <em>53</em>(21), 25058–25069. (<a
href="https://doi.org/10.1007/s10489-023-04912-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient assessment technique, failure mode and effects analysis (FMEA) plays a critical role in conducting risk analyses of production and decision-making. In FMEA, the determination of the risk priority number (RPN) is the most important stage used to prioritize potential failure modes. However, the elicited RPN oftentimes contains uncertainty. Information fractal dimension is a measure of uncertainty and complexity for probability distributions. Therefore, this paper aims to propose a novel exponential information fractal dimension weighted RPN (IFDRPN). Probability distributions of assessments are provided by experts. Then, the information fractal dimension is calculated with the consideration of the uncertainty of each piece of assessment information. Furthermore, the relative importance of experts based on variable backgrounds and authorities is also taken into consideration. The corresponding FMEA is capable of measuring uncertainty and complexity in opinions for failure modes to facilitate assessment. An application of rotor blades for an aircraft turbine, along with the risk assessment in hydropower finance project, are used to demonstrate the effectiveness of the proposed FMEA approach.},
  archive      = {J_APIN},
  author       = {Liu, Ruijie and Li, Zhen and Deng, Yong},
  doi          = {10.1007/s10489-023-04912-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25058-25069},
  shortjournal = {Appl. Intell.},
  title        = {Exponential information fractal dimension weighted risk priority number method for failure mode and effects analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TNFRM: A recommendation model based on temporal interest
fluctuation with neural networks and fuzzy clustering. <em>APIN</em>,
<em>53</em>(21), 25042–25057. (<a
href="https://doi.org/10.1007/s10489-023-04776-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of personalized recommendation technology for big data, online data resources have undergone a rapid expansion, presenting features such as complex structure and diverse forms. For the purpose of improving recommendation accuracy, we adopt the idea of divide and conquer to construct a novel hybrid temporal prediction model to adapt to the features of different user interest data fluctuations. Firstly, the users’ data is divided into small and large ranges through fluctuation thresholds. Meanwhile, we introduce a global dynamic factor to adjust the temporal weight decay of the sequence data, and then use the neural network to regress and predict the small range fluctuation series data, at the same time, the large range fluctuation series data is predicted by dividing the fuzzy relationship through the membership degree of fuzzy clustering. Finally, through simulation recommendation experiments, model ablation experiments, and iterative performance experiments by using 8 baseline models and 7 datasets, the results show that the F1 index of the proposed model has increased by an average of 13.21%, the NDCG index has increased by an average of 15.96%, and the convergence iterations are within 600 times. More accurate prediction results can be received through extraction of data feature for different interest fluctuations, which is helpful for optimizing personalized information recommendation services.},
  archive      = {J_APIN},
  author       = {Ding, Hao},
  doi          = {10.1007/s10489-023-04776-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25042-25057},
  shortjournal = {Appl. Intell.},
  title        = {TNFRM: A recommendation model based on temporal interest fluctuation with neural networks and fuzzy clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structurally incoherent adaptive weighted low-rank matrix
decomposition for image classification. <em>APIN</em>, <em>53</em>(21),
25028–25041. (<a
href="https://doi.org/10.1007/s10489-023-04875-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address image classification challenges caused by noisy disturbances, we propose a new algorithm called structurally incoherent adaptive weighted low-rank matrix decomposition (SIAWLR). This method divides the raw image matrix into a low-rank denoised matrix, which retains all the information of images, and a sparse error matrix that captures the noise components. The incorporation of structural incoherence in the low-rank matrix and the utilization of adaptive weights in the error matrix significantly enhance the classification performance. To solve the SIAWLR, we propose an integrated algorithm consisting of two steps. Firstly, we employ the augmented lagrangian alternating direction method (ALADM) (Shen et al., Optim Methods Softw 29(2), 239–263, 2014) to solve the SIAWLR. Subsequently, we classify the images based on the obtained low-rank matrix. In comparison to other methods, SIAWLR exhibits computational attractiveness as it requires fewer parameters, often determined through cross validation. We conduct experiments comparing the proposed method with four other methods on three datasets. The experimental results consistently demonstrate that SIAWLR outperforms the other methods in terms of classification accuracy.},
  archive      = {J_APIN},
  author       = {Li, Zhaoyang and Yang, Yuehan},
  doi          = {10.1007/s10489-023-04875-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25028-25041},
  shortjournal = {Appl. Intell.},
  title        = {Structurally incoherent adaptive weighted low-rank matrix decomposition for image classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Enabling inductive knowledge graph completion via
structure-aware attention network. <em>APIN</em>, <em>53</em>(21),
25003–25027. (<a
href="https://doi.org/10.1007/s10489-023-04768-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion (KGC) aims at complementing missing entities and relations in a knowledge graph (KG). Popular KGC approaches based on KG embedding are typically limited to the transductive setting, i.e., all entities must be seen during training, which is impractical for real-world KGs where new entities are emerging daily. Recent inductive KG embedding approaches propose to train a neighborhood aggregator in conjunction with entity and relation embeddings, which helps to embed unseen entities via existing neighbors. However, existing methods do not fully take advantage of the structural information of neighbors and are unable to handle triplets involving unseen relations. In this paper, we work further and propose a novel and unified inductive KGC model, namely structure-aware attention network (SAAN), which can efficiently generate embeddings of unseen entities and relations by aggregating neighbors with structure-aware attention weights. Unlike conventional embedding-based attention methods, SAAN can naturally learn importance weights by modeling structural correlations between nodes in an embedding-independent manner and can be applied to any existing KG embedding model. Experimental results on both transductive and inductive KGC tasks show that our model significantly outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Jingchao and Li, Weimin and Liu, Wei and Wang, Can and Jin, Qun},
  doi          = {10.1007/s10489-023-04768-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {25003-25027},
  shortjournal = {Appl. Intell.},
  title        = {Enabling inductive knowledge graph completion via structure-aware attention network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible and lightweight deep learning weather forecasting
model. <em>APIN</em>, <em>53</em>(21), 24991–25002. (<a
href="https://doi.org/10.1007/s10489-023-04824-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical weather prediction is an established weather forecasting technique in which equations describing wind, temperature, pressure and humidity are solved using the current atmospheric state as input. This study examines deep learning to forecast weather given historical data from two London-based locations. Two distinct Bi-LSTM recurrent neural network models were developed in the TensorFlow deep learning framework and trained to make predictions in the next 24 and 72 h, given the past 120 h. The first trained neural network predicted temperature at Kew Gardens with a forecast accuracy of $$\pm$$ 2 $${}^{\circ }$$ C in 73% of instances in a whole unseen year, and a root mean squared errors of 1.45 $${}^{\circ }$$ C. The second network predicted 72-h air temperature and relative humidity at Heathrow with root mean squared errors 2.26 $${}^{\circ }$$ C and 14% respectively and 80% of the temperature predictions were within $$\pm$$ 3 $${}^{\circ }$$ C while 80% of relative humidity predictions were within $$\pm$$ 20%. Both networks were trained with five years of historical data, with cloud training times of over a minute (24-h network) and three minutes (72-h).},
  archive      = {J_APIN},
  author       = {Zenkner, Gabriel and Navarro-Martinez, Salvador},
  doi          = {10.1007/s10489-023-04824-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24991-25002},
  shortjournal = {Appl. Intell.},
  title        = {A flexible and lightweight deep learning weather forecasting model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multi-scale parametric optimization of deep
neural networks. <em>APIN</em>, <em>53</em>(21), 24963–24990. (<a
href="https://doi.org/10.1007/s10489-023-04745-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, sensitivity analysis has been utilized to determine the importance of input variables to a deep neural network (DNN). However, the quantification of sensitivity for each neuron in a network presents a significant challenge. In this article, a selective method for calculating neuron sensitivity in layers of neurons concerning network output is proposed. This approach incorporates scaling factors that facilitate the evaluation and comparison of neuron importance. Additionally, a hierarchical multi-scale optimization framework is proposed, where layers with high-importance neurons are selectively optimized. Unlike the traditional backpropagation method that optimizes the whole network at once, this alternative approach focuses on optimizing the more important layers. This paper provides fundamental theoretical analysis and motivating case study results for the proposed neural network treatment. The framework is shown to be effective in network optimization when applied to simulated and UCI Machine Learning Repository datasets. This alternative training generates local minima close to or even better than those obtained with the backpropagation method, utilizing the same starting points for comparative purposes within a multi-start optimization procedure. Moreover, the proposed approach is observed to be more efficient for large-scale DNNs. These results validate the proposed algorithmic framework as a rigorous and robust new optimization methodology for training (fitting) neural networks to input/output data series of any given system.},
  archive      = {J_APIN},
  author       = {Zhang, Sushen and Vassiliadis, Vassilios S. and Dorneanu, Bogdan and Arellano-Garcia, Harvey},
  doi          = {10.1007/s10489-023-04745-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24963-24990},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical multi-scale parametric optimization of deep neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HELViT: Highly efficient lightweight vision transformer for
remote sensing image scene classification. <em>APIN</em>,
<em>53</em>(21), 24947–24962. (<a
href="https://doi.org/10.1007/s10489-023-04725-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image scene classification methods based on convolutional neural networks (CNN) have been extremely successful. However, the limitations of CNN itself make it difficult to acquire global information. The traditional Vision Transformer can effectively capture long-distance dependencies for acquiring global information, but it is computationally intensive. In addition, each class of scene in remote sensing images has a large quantity of the similar background or foreground features. To effectively leverage those similar features and reduce the computation, a highly efficient lightweight vision transformer (HELViT) is proposed. HELViT is a hybrid model combining CNN and Transformer and consists of the Convolution and Attention Block (CAB), the Convolution and Token Merging Block (CTMB). Specifically, in CAB module, the embedding layer in the original Vision Transformer is replaced with a modified MBConv (MBConv $$^*$$ ), and the Fast Multi-Head Self Attention (F-MHSA) is used to change the quadratic complexity of the self-attention mechanism to linear. To further decreasing the model’s computational cost, CTMB employs the adaptive token merging (ATOME) to fuse some related foreground or background features. The experimental results on the UCM, AID and NWPU datasets show that the proposed model displays better results in terms of accuracy and efficiency than the state-of-the-art remote sensing scene classification methods. On the most challenging NWPU dataset, HELViT achieves the highest accuracy of 94.64%/96.84% with 4.6G GMACs for 10%/20% training samples, respectively.},
  archive      = {J_APIN},
  author       = {Guo, Dongen and Wu, Zechen and Feng, Jiangfan and Zhou, Zhuoke and Shen, Zhen},
  doi          = {10.1007/s10489-023-04725-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24947-24962},
  shortjournal = {Appl. Intell.},
  title        = {HELViT: Highly efficient lightweight vision transformer for remote sensing image scene classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A missing value filling model based on feature fusion
enhanced autoencoder. <em>APIN</em>, <em>53</em>(21), 24931–24946. (<a
href="https://doi.org/10.1007/s10489-023-04892-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the big data era, the data quality problem is becoming more critical. Among many factors, data with missing values is one primary issue, and thus developing effective imputation models is a key topic in the research community. Recently, a major research direction is to employ neural network models such as self-organizing mappings or automatic encoders for filling missing values. However, these classical methods can hardly discover interrelated features and common features simultaneously among data attributes. Especially, it is a very typical problem for classical autoencoders that they often learn invalid constant mappings, which dramatically hurts the filling performance. To solve the above-mentioned problems, we propose a missing-value-filling model based on a feature-fusion-enhanced autoencoder. We first incorporate into an autoencoder a hidden layer that consists of de-tracking neurons and radial basis function neurons, which can enhance the ability of learning interrelated features and common features. Besides, we develop a missing value filling strategy based on dynamic clustering that is incorporated into an iterative optimization process. This design can enhance the multi-dimensional feature fusion ability and thus improves the dynamic collaborative missing-value-filling performance. The effectiveness of the proposed model is validated by extensive experiments compared to a variety of baseline methods on thirteen data sets.},
  archive      = {J_APIN},
  author       = {Liu, Xinyao and Du, Shengdong and Li, Tianrui and Teng, Fei and Yang, Yan},
  doi          = {10.1007/s10489-023-04892-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24931-24946},
  shortjournal = {Appl. Intell.},
  title        = {A missing value filling model based on feature fusion enhanced autoencoder},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DynaQ: Online learning from imbalanced multi-class streams
through dynamic sampling. <em>APIN</em>, <em>53</em>(21), 24908–24930.
(<a href="https://doi.org/10.1007/s10489-023-04886-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online supervised learning from fast-evolving data streams, particularly in domains such as health, the environment, and manufacturing, is a crucial research area. However, these domains often experience class imbalance, which can skew class distributions. It is essential for online learning algorithms to analyze large datasets in real-time while accurately modeling rare or infrequent classes that may appear in bursts. While methods have been proposed to handle binary class imbalance, there is a lack of attention to multi-class imbalanced settings with varying degrees of imbalance in evolving streams. In this paper, we present the Dynamic Queues (DynaQ) algorithm for online learning in multi-class imbalanced settings to fill this knowledge gap. Our approach utilizes a batch-based resampling method that creates an instance queue for each class to balance the number of instances. We maintain a queue threshold and remove older samples during training. Additionally, we dynamically oversample minority classes based on one of four rate parameters: recall, F1-score, $$\kappa _m$$ , and Euclidean distance. Our learning algorithm consists of an ensemble that uses sliding windows and a soft voting schema while incorporating a drift detection mechanism. Our experimental results demonstrate the superiority of the DynaQ approach over state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Sadeghi, Farnaz and Viktor, Herna L. and Vafaie, Parsa},
  doi          = {10.1007/s10489-023-04886-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24908-24930},
  shortjournal = {Appl. Intell.},
  title        = {DynaQ: Online learning from imbalanced multi-class streams through dynamic sampling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OSAF-net: A one-stage anchor-free detector for small-target
crop pest detection. <em>APIN</em>, <em>53</em>(21), 24895–24907. (<a
href="https://doi.org/10.1007/s10489-023-04862-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class crop pest detection in massive images is a practically challenging problem. Recently, convolutional neural networks (CNN) based approaches have shown promise in detecting crop pests, but there are still significant obstacles to overcome. Two primary challenges include the highly similar physical appearance of some categories, making it difficult to distinguish the specific categories manually, and the multi-scale characteristics of pest objects, leading to numerous false negative detections, especially for small pests. To address the above problems, we propose a one-stage anchor-free detection network (OSAF-Net) with strong performance and robustness. Firstly, a dynamic training sample selection (DTSS) method is devised to capture high-quality training examples that contain multi-scale contextual information to improve the detection performance of pests with diverse scales. Secondly, to mitigate the disturbance from the similar physical appearance of pests, a dynamic detection head (DDH) is introduced to accurately obtain more representative semantic features to locate and distinguish pest objects. The proposed DTSS and DDH methods are stable for implementation. They can be combined with existing state-of-the-art detection network architectures. Extensive experiments conducted on two datasets, CropPest24 and MPD2018, demonstrate that our proposed method has a competitive performance, achieving AP50 of 77.3% on CropPest24 and 81.3% on MPD2018.},
  archive      = {J_APIN},
  author       = {Wang, Rujing and Dong, Shifeng and Jiao, Lin and Du, Jianming and Huang, Ziliang and Zheng, Shijian and Kang, Chenrui},
  doi          = {10.1007/s10489-023-04862-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24895-24907},
  shortjournal = {Appl. Intell.},
  title        = {OSAF-net: A one-stage anchor-free detector for small-target crop pest detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernelized multi-granulation fuzzy rough set over hybrid
attribute decision system and application to stroke risk prediction.
<em>APIN</em>, <em>53</em>(21), 24876–24894. (<a
href="https://doi.org/10.1007/s10489-023-04850-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for quantitative scientific prediction models and methods under uncertain hybrid information environment is helpful to the scientific decision-making of practical management problems. In this paper, we discuss the uncertain decision making problem with hybrid attributes and fuzzy decision objects. Considering the characteristic of hybrid attribute information, we introduce kernel functions to abstract the similarities from different types of attributes. On this basis, the kernel-based upper and lower approximations of arbitrary fuzzy decision objects over hybrid attribute information system are given under a multi-granularity framework, i.e., kernelized multi-granulation fuzzy rough sets, called KMGFRS in this paper. Then, the attribute reduction on hybrid attribute decision system based on KMGFRS is discussed. In addition, we combine KMGFRS and fuzzy k-nearest neighbors (FkNN) to propose a new prediction method without prior information and apply it to stroke risk assessment and prediction in clinical decision-making. Finally, we use real clinical data and UCI datasets for experiment analysis, and the results verify the applicability and validity of the model.},
  archive      = {J_APIN},
  author       = {Wang, Ting and Sun, Bingzhen and Jiang, Chao},
  doi          = {10.1007/s10489-023-04850-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24876-24894},
  shortjournal = {Appl. Intell.},
  title        = {Kernelized multi-granulation fuzzy rough set over hybrid attribute decision system and application to stroke risk prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic incident detection using edge-cloud collaboration
based deep learning scheme for intelligent transportation systems.
<em>APIN</em>, <em>53</em>(21), 24864–24875. (<a
href="https://doi.org/10.1007/s10489-023-04673-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic incident detection not only plays an important role in traffic safety management, but also contributes to the operation of intelligent transportation systems. Although the emerging information technologies and artificial intelligence approaches are paving the way for high-precision incident detection, existing incident detection methods fail to handle the unbalanced incident data with excessive zero observations. Also, issues related to network delays and privacy leakage of centralized computing are prevalent. To fill the above gaps, this study proposes a novel automatic incident detection paradigm using an edge-cloud collaboration mechanism. In particular, a Spatio-Temporal Variational Digraph Auto-Encoder model is developed to distinguish the incidents in dynamic traffic flows. To be specific, the model encoder includes two components. The first module, deployed in an edge server, is designed to extract the local contexts from the real-time traffic flow. The dynamic traffic flows will be projected into a spatio-temporal digraph, and in turn addressed by a graph convolutional network for extraction of the deep-seated features. Similarly, the second module is deployed in a central server to capture the spatio-temporal global contexts from historical traffic flows. Finally, the above-concerned contexts are integrated and fed into a model decoder to measure the likelihood of incidents. To testify the proposed paradigm and model, real-world datasets were applied. The experimental results revealed the proposed model outperforms state-of-the-art models in terms of detection accuracy, achieving 26.3% improvement over the best-performing baseline. Furthermore, the proposed paradigm is more efficient in respondence compared with traditional centralized computing, realizing 8x processing speed for the same detection task.},
  archive      = {J_APIN},
  author       = {Lu, Yuhuan and Lin, Qinghai and Chi, Haiyang and Chen, Jin-Yong},
  doi          = {10.1007/s10489-023-04673-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24864-24875},
  shortjournal = {Appl. Intell.},
  title        = {Automatic incident detection using edge-cloud collaboration based deep learning scheme for intelligent transportation systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient distributional reinforcement learning with
kullback-leibler divergence regularization. <em>APIN</em>,
<em>53</em>(21), 24847–24863. (<a
href="https://doi.org/10.1007/s10489-023-04867-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the issues of stability and data-efficiency in reinforcement learning (RL). A novel RL approach, Kullback-Leibler divergence-regularized distributional RL (KL-C51) is proposed to integrate the advantages of both stability in the distributional RL and data-efficiency in the Kullback-Leibler (KL) divergence-regularized RL in one framework. KL-C51 derived the Bellman equation and the TD errors regularized by KL divergence in a distributional perspective and explored the approximated strategies of properly mapping the corresponding Boltzmann softmax term into distributions. Evaluated not only by several benchmark tasks with different complexity from OpenAI Gym but also by six Atari 2600 games from the Arcade Learning Environment, the proposed method clearly illustrates the positive effect of the KL divergence regularization to the distributional RL including exclusive exploration behaviors and smooth value function update, and demonstrates an improvement in both learning stability and data-efficiency compared with other related baseline approaches.},
  archive      = {J_APIN},
  author       = {Li, Renxing and Shang, Zhiwei and Zheng, Chunhua and Li, Huiyun and Liang, Qing and Cui, Yunduan},
  doi          = {10.1007/s10489-023-04867-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24847-24863},
  shortjournal = {Appl. Intell.},
  title        = {Efficient distributional reinforcement learning with kullback-leibler divergence regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multigranulation rough set model based on variable
precision neighborhood and its applications. <em>APIN</em>,
<em>53</em>(21), 24822–24846. (<a
href="https://doi.org/10.1007/s10489-023-04826-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As combinations of neighborhood rough sets and multigranulation rough sets (MRSs), optimistic and pessimistic neighborhood MRSs can handle complex information systems and characterize problems from multiple perspectives. Nevertheless, they require complete inclusion between neighborhood granules and target concepts, which may weaken their fault tolerance. To overcome the challenge, this paper proposes neighborhood MRSs based on variable precision neighborhood (VMRSs), which allow a certain degree of misclassification and noise in data. In VMRSs, we assign different weights to different attribute subsets to distinguish their importance in learning. In addition to investigating the properties of the VMRS model, we focus on the methods of obtaining its required multiple attribute subsets and their weights. Next, we introduce two applications of the VMRS model. One is using it to construct an indicator for evaluating attribute clustering and attribute subset weighting methods. The other is employing it for attribute reduction. Based on distribution distances, we develop a heuristic algorithm framework for obtaining the proposed reducts. The mechanism and applications of VMRSs are explained through a case study on a medical diagnosis issue. Finally, the experiments on real datasets illuminate the effectiveness and superiority of the methods and algorithms in the paper.},
  archive      = {J_APIN},
  author       = {Chen, Jiayue and Zhu, Ping},
  doi          = {10.1007/s10489-023-04826-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24822-24846},
  shortjournal = {Appl. Intell.},
  title        = {A multigranulation rough set model based on variable precision neighborhood and its applications},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relative order constraint for monocular depth estimation.
<em>APIN</em>, <em>53</em>(21), 24804–24821. (<a
href="https://doi.org/10.1007/s10489-023-04851-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation, which is playing an increasingly important role in 3D scene understanding, has been attracting increasing attention in the computer vision field in recent years. The latest monocular depth estimation methods based on deep learning have achieved significant performance by exploring various network architectures. However, compared with designing larger and more complex model architectures for monocular depth estimation, leveraging scene geometry relations to boost the performance of monocular depth estimation models has been less studied. To explore further utilization of scene geometry relations on monocular depth estimation, we propose a geometry-aware constraint that makes use of relative order information to improve the performance of monocular depth estimation models. Specifically, we first design a relative order descriptor (ROD) to construct the relative order description on single scene location. Then, based on the ROD, the relative order map (ROM) is built to represent the relative order information of the whole scene. Finally, a loss term relative order loss (ROL), which relies on ROM to supervise the training process of the monocular depth estimation model is presented. Our proposed method can help monocular depth estimation models to predict more accurate depth maps. Moreover, with the geometry constraint from our method, the monocular depth estimation model can provide prediction results where high-quality scene structure can be better preserved. We conduct extensive experiments on the popular datasets NYU Depth V2 and KITTI. The experimental results demonstrate the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Liu, Chunpu and Zuo, Wangmeng and Yang, Guanglei and Li, Wanlong and Wen, Feng and Zhang, Hongbo and Zang, Tianyi},
  doi          = {10.1007/s10489-023-04851-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24804-24821},
  shortjournal = {Appl. Intell.},
  title        = {Relative order constraint for monocular depth estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stable data-augmented reinforcement learning method with
ensemble exploration and exploitation. <em>APIN</em>, <em>53</em>(21),
24792–24803. (<a
href="https://doi.org/10.1007/s10489-023-04816-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from visual observations is a significant yet challenging problem in Reinforcement Learning (RL). Two respective problems, representation learning and task learning, need to solve to infer an optimal policy. Some methods have been proposed to utilize data augmentation in reinforcement learning to directly learn from images. Although these methods can improve generation in RL, they are often found to make the task learning unsteady and can even lead to divergence. We investigate the causes of instability and find it is usually rooted in high-variance of Q-functions. In this paper, we propose an easy-to-implement and unified method to solve above-mentioned problems, Data-augmented Reinforcement Learning with Ensemble Exploration and Exploitation (DAR-EEE). Bootstrap ensembles are incorporated into data augmented reinforcement learning and provide uncertainty estimation of both original and augmented states, which can be utilized to stabilize and accelerate the task learning. Specially, a novel strategy called uncertainty-weighted exploitation is designed for rational utilization of transition tuples. Moreover, an efficient exploration method using the highest upper confidence is used to balance exploration and exploitation. Our experimental evaluation demonstrates the improved sample efficiency and final performance of our method on a range of difficult image-based control tasks. Especially, our method has achieved the new state-of-the-art performance on Reacher-easy and Cheetah-run tasks.},
  archive      = {J_APIN},
  author       = {Zuo, Guoyu and Tian, Zhipeng and Huang, Gao},
  doi          = {10.1007/s10489-023-04816-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24792-24803},
  shortjournal = {Appl. Intell.},
  title        = {A stable data-augmented reinforcement learning method with ensemble exploration and exploitation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile estimation for encrypted data. <em>APIN</em>,
<em>53</em>(21), 24782–24791. (<a
href="https://doi.org/10.1007/s10489-023-04837-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data-based studies continue to increase, the need for privacy protection has become a crucial issue. One proposed solution to address this obstacle is homomorphic encryption (HE); however, the complexity of handling ciphertexts used in HE poses a serious challenge due to the extended calculation time of elementary operations. As a result, it has much more complex than handling plaintexts, limiting various subsequent data analyses. This paper proposes a quantile estimation method for encrypted data, where quantiles are core statistics for understanding the data distribution in statistical analysis. We developed an HE-friendly method for large homomorphic encrypted data using an approximate quantile loss function. Numerical studies show that the proposed method significantly improves the calculation time for simulated and real homomorphically encrypted data. Specifically, the proposed method takes approximately 26 minutes for calculating a dataset of four million, which is about 14 times faster than the sorting method. Furthermore, we applied the proposed method to construct boxplots for homomorphically encrypted data.},
  archive      = {J_APIN},
  author       = {Park, Minje and Kim, Jaeseon and Shin, Sungchul and Park, Cheolwoo and Jeon, Jong-June and Kwon, SoonSun and Choi, Hosik},
  doi          = {10.1007/s10489-023-04837-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24782-24791},
  shortjournal = {Appl. Intell.},
  title        = {Quantile estimation for encrypted data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of synthetic data generation for intelligent
climate control in greenhouses. <em>APIN</em>, <em>53</em>(21),
24765–24781. (<a
href="https://doi.org/10.1007/s10489-023-04783-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are witnessing the digitalization era, where artificial intelligence (AI)/machine learning (ML) models are mandatory to transform this data deluge into actionable information. However, these models require large, high-quality datasets to predict high reliability/accuracy. Even with the maturity of Internet of Things (IoT) systems, there are still numerous scenarios where there is not enough quantity and quality of data to successfully develop AI/ML-based applications that can meet market expectations. One such scenario is precision agriculture, where operational data generation is costly and unreliable due to the extreme and remote conditions of numerous crops. In this paper, we investigated the generation of synthetic data as a method to improve predictions of AI/ML models in precision agriculture. We used generative adversarial networks (GANs) to generate synthetic temperature data for a greenhouse located in Murcia (Spain). The results reveal that the use of synthetic data significantly improves the accuracy of the AI/ML models targeted compared to using only ground truth data.},
  archive      = {J_APIN},
  author       = {Morales-García, Juan and Bueno-Crespo, Andrés and Terroso-Sáenz, Fernando and Arcas-Túnez, Francisco and Martínez-España, Raquel and Cecilia, José M.},
  doi          = {10.1007/s10489-023-04783-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24765-24781},
  shortjournal = {Appl. Intell.},
  title        = {Evaluation of synthetic data generation for intelligent climate control in greenhouses},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared and visible image fusion based on VPDE model and
VGG network. <em>APIN</em>, <em>53</em>(21), 24739–24764. (<a
href="https://doi.org/10.1007/s10489-023-04692-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared (IR) and visible (VIS) image fusion techniques are widely applied to many high-level vision tasks, such as object detection, recognition, and tracking. However, most existing image fusion algorithms exhibit varying degrees of edge-step effect and texture information degradation in their fused images. To improve the fusion quality, an IR and VIS image fusion method based on a variational partial differential equation (VPDE) model and a VGG network is proposed. A productive smoothing segmentation is integrated into the energy function of the VPDE model, which is based on a novel regularization function. To decompose source images into low-frequency and high-frequency components, the new VPDE model is employed. To fuse low-frequency components, a probabilistic parameter model based on space-alternating generalized expectation-maximization (SAGE) is utilized rather than the traditional average fusion rule. Then, multi-layer features of the high-frequency components are extracted using a VGG network. To generate several candidates of the fused detail content, the $$l_1$$ -norm and weighted average rule are adopted, and the final details are obtained by using the maximum selection strategy. Finally, fused images are obtained by reconstructing the fused low-frequency and high-frequency components. Extensive experiments on the TNO and RoadScene datasets demonstrate that the proposed technique effectively eliminates artifacts as well as the step effect. In the subjective comparison, the proposed method can highlight the salient objects of the fused images while strengthening the texture information. In terms of the evaluation metrics, the proposed method outperforms 13 state-of-the-art methods in objective comparison in addition to the subjective evaluation.},
  archive      = {J_APIN},
  author       = {Luo, Donghua and Liu, Gang and Bavirisetti, Durga Prasad and Cao, Yisheng},
  doi          = {10.1007/s10489-023-04692-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24739-24764},
  shortjournal = {Appl. Intell.},
  title        = {Infrared and visible image fusion based on VPDE model and VGG network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A texture-based method for predicting molecular markers and
survival outcome in lower grade glioma. <em>APIN</em>, <em>53</em>(21),
24724–24738. (<a
href="https://doi.org/10.1007/s10489-023-04844-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture-based convolutional neural networks (CNNs) have shown great promise in predicting various types of cancer, including lower grade glioma (LGG) through radiomics analysis. However, the use of CNN-based radiomics requires a large training set to avoid overfitting. To overcome this problem, the study proposes a novel panel of radiomic/texture features based on principal component analysis (PCA) applied to pretrained CNN features. The study used extracted PCA-CNN radiomic features from multimodal magnetic resonance imaging (MRI) images as input to a random forest (RF) classifier to predict immune cell markers, the gene status, and the survival outcome for LGG patients (n = 83). The results of the experiments demonstrate that RF with PCA-CNN radiomic features improved the classification performance, achieving the highest significant classification between short- and long-term survival outcomes. Notably, the area under the curve for PCA-CNN radiomic features with RF was 78.53% (p = 0.0008), which was significantly better than using gene status 63.14% (p = 0.23), clinical variables 52.60% (p = 0.32), standard radiomic features 72.56% (p = 0.02), immune cell markers 65.67% (p = 0.007), conditional entropy 74.54% (p = 0.0058), Gaussian mixture model-CNN 74.94% (p = 0.0053), or using 3D CNN classification directly without RF 72.61% (p = 0.01). The proposed PCA-CNN-based radiomic model outperformed state-of-the-art techniques to predict the survival outcome of LGG patients.},
  archive      = {J_APIN},
  author       = {Chaddad, Ahmad and Hassan, Lama and Katib, Yousef},
  doi          = {10.1007/s10489-023-04844-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24724-24738},
  shortjournal = {Appl. Intell.},
  title        = {A texture-based method for predicting molecular markers and survival outcome in lower grade glioma},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Siamese transformer RGBT tracking. <em>APIN</em>,
<em>53</em>(21), 24709–24723. (<a
href="https://doi.org/10.1007/s10489-023-04741-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese-based RGBT trackers have attracted wide attention because of their high efficiency. However, there is a lack of an effective multimodal fusion module and information interaction between the search area and template area, which leads to poor performance of these siamese-based RGBT trackers. To solve this problem, inspire by the global information modeling capability of the transformer, we construct a siamese-based transformer RGBT tracker consisting of a single unified transformer module. Specifically, we propose a unified transformer fusion module to achieve feature extraction and global information interaction in the siamese RGBT tracker, i.e., the interaction between the search area and template area, the interaction between different modalities. It consists of self-attention and cross-attention, which are used to extract features and information interaction respectively. In addition, to alleviate the impact of multimodal fusion on the efficiency of template update in the tracking stage, we propose a feature-level template update strategy, which effectively improves tracking efficiency. To verify the effectiveness of our tracker, we evaluate it on five benchmark datasets including GTOT, RGBT210, RGBT234, LasHeR and VTUAV, and the results show that our tracker achieves excellent performance compared to the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Futian and Wang, Wenqi and Liu, Lei and Li, Chenglong and Tang, Jing},
  doi          = {10.1007/s10489-023-04741-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24709-24723},
  shortjournal = {Appl. Intell.},
  title        = {Siamese transformer RGBT tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active pairwise distance learning for efficient labeling of
large datasets by human experts. <em>APIN</em>, <em>53</em>(21),
24689–24708. (<a
href="https://doi.org/10.1007/s10489-023-04516-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many machine learning applications, the labeling of datasets is done by human experts, which is usually time-consuming in cases of large data sets. This raises the need for methods to make optimal use of the human expert by selecting model instances for which the expert opinion is of most added value. This paper introduces the problem of active pairwise distance learning (APDL), where the goal is to actively learn the pairwise distances between all instances. Any distance function can be used, which means that APDL techniques can e.g., be used to determine likeness between faces or similarities between users for recommender systems. Starting with an unlabeled dataset, each round an expert determines the distance between one pair of instances. Thus, there is an important choice to make each round: ‘Which combination of instances is presented to the expert?’ The objective is to accurately predict all pairwise distances, while minimizing the usage of the expert. In this research, we establish upper and lower bound approximations (including an update rule) for the pairwise distances and evaluate many domain-independent query strategies. The observations from the experiments are therefore general, and the selection strategies are ideal candidates to function as baseline in future research. We show that using the criterion max degree consistently ranks amongst the best strategies. By using this criterion, the pairwise distances of a new dataset can be labeled much more efficiently.},
  archive      = {J_APIN},
  author       = {Pries, Joris and Bhulai, Sandjai and van der Mei, Rob},
  doi          = {10.1007/s10489-023-04516-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24689-24708},
  shortjournal = {Appl. Intell.},
  title        = {Active pairwise distance learning for efficient labeling of large datasets by human experts},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent fault diagnosis and health stage division of
bearing based on tensor clustering and feature space denoising.
<em>APIN</em>, <em>53</em>(21), 24671–24688. (<a
href="https://doi.org/10.1007/s10489-023-04843-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional and unlabeled data collected from multi-sensor is a common scenario in practical production. The fault diagnosis and health stage (HS) division of bearing under different degradation processes is easily limited due to unlabeled and high-dimensional data. This work designs an intelligent fault diagnosis and HS division strategy for unlabeled and high-dimensional data. An adaptive tensor density peaks search (ATDPS) clustering algorithm is proposed for the HS division of rolling bearing. Moreover, to enhance the clustering performance, a novel neighborhood least square (NLS) technique is developed for feature space denoising, whose effectiveness and superiority are verified compared with the other feature space denoising techniques. The proposed strategies are subsequently applied to three benchmark cases and compared with other clustering methods. The experiment results demonstrate that the proposed strategy can reliably and accurately divide the different degradation stages depending on less prior knowledge. Furthermore, the presented HS division approach successfully monitors degradation with compound failure, showing potential for practical application.},
  archive      = {J_APIN},
  author       = {Wei, Zexian and He, Deqiang and Jin, Zhenzhen and Shan, Sheng and Zou, Xueyan and Miao, Jian and Liu, Chang},
  doi          = {10.1007/s10489-023-04843-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24671-24688},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent fault diagnosis and health stage division of bearing based on tensor clustering and feature space denoising},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology augmented dynamic spatial-temporal network for
passenger flow forecasting in urban rail transit. <em>APIN</em>,
<em>53</em>(21), 24655–24670. (<a
href="https://doi.org/10.1007/s10489-023-04651-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of residents travel by urban rail transit (URT) every day, and it is meaningful to accurately predict passenger flows for the purpose of intelligent system management. Considering the high traffic volume and complex spatial-temporal traffic patterns, accurate forecasting is quite challenging. Recently, advanced spatial-temporal graph neural networks that combine graph convolution and recurrent units have achieved superior traffic forecasting performance. However, existing methods either simply use a predefined physical network or directly learn latent static graphs by assigning a large number of trainable parameters. Moreover, they only consider capturing local temporal dependencies. However, the passenger flows of URT exhibit both physical adjacent and virtual distant spatial correlations and both local and global temporal fluctuations. In this paper, we propose a Topology Augmented Dynamic Spatial-Temporal Network (TADSTN) to fully exploit the complex spatial-temporal dependencies of passenger flows and make accurate forecasting in URT. Specifically, we first propose a virtual graph generation algorithm with no training parameters to obtain a topology-augmented URT graph, and we also sample multipattern temporal features from raw passenger flow data. Then, we propose a parallel spatial-temporal learning architecture with a time-aware global-level attention mechanism to simultaneously capture both distant and dynamic spatial dependencies and both local and global temporal dependencies of passenger flows. Finally, we evaluate the effectiveness and efficiency of our TADSTN on two real-world datasets.},
  archive      = {J_APIN},
  author       = {Yi, Peiyu and Huang, Feihu and Wang, Jince and Peng, Jian},
  doi          = {10.1007/s10489-023-04651-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {21},
  pages        = {24655-24670},
  shortjournal = {Appl. Intell.},
  title        = {Topology augmented dynamic spatial-temporal network for passenger flow forecasting in urban rail transit},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing computational algorithms for team formation in the
classroom: A classroom experience. <em>APIN</em>, <em>53</em>(20),
23883–23904. (<a
href="https://doi.org/10.1007/s10489-023-04748-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Throughout recent years, several researchers have proposed computational tools and algorithms to support team formation in the classroom. The result is that team formation algorithms have been widely applied in classroom environments to create well-balanced teams. One of the challenges in designing algorithms for automatic team formation is designing an appropriate function to estimate team performance, which is used as part of the optimization algorithm that divides students into teams. This function (referred to as a team evaluation heuristic) serves as an approximation to team performance, which is a complex phenomenon that is difficult to quantitatively assess in many settings and that cannot be accurately calculated prior to the task at hand. Despite showing their relative success compared to traditional and manual team formation strategies (manually employed by lecturers and teachers), there is a lack of research comparing team evaluation heuristics in a real classroom setting. Such a comparison would help teachers, practitioners, and system designers to appropriately select the most suitable team formation algorithms. In this article, we present an experimental evaluation that was carried out in a Bachelor’s Degree Program in Tourism that compares two team evaluation heuristics based on Belbin and Myer-Briggs. The experimental evaluation was carried out by means of an intelligent, extensible team formation tool whose optimization is based on an integer linear model that can be extended to support different team evaluation heuristics.},
  archive      = {J_APIN},
  author       = {Sanchez-Anguix, Victor and Alberola, Juan M. and Del Val, Elena and Palomares, Alberto and Teruel, Maria Dolores},
  doi          = {10.1007/s10489-023-04748-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23883-23904},
  shortjournal = {Appl. Intell.},
  title        = {Comparing computational algorithms for team formation in the classroom: A classroom experience},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph regularized discriminative nonnegative tucker
decomposition for tensor data representation. <em>APIN</em>,
<em>53</em>(20), 23864–23882. (<a
href="https://doi.org/10.1007/s10489-023-04738-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorization has been widely applied in computer vision and machine learning area. Nonnegative Tucker decomposition (NTD) is a popular tensor factorization technique. However, it neglects the geometrical structure of the data space and the available label information of sample data, lowering the data representation performance. To overcome this defect, in this paper, we propose a novel semi-supervised NTD method, named graph regularized discriminative nonnegative Tucker decomposition (GDNTD), which incorporates the graph construction of the data space and the available label information of the sample data into NTD. Specifically, a graph construction is utilized to preserve the geometric structure between data by a graph regularization term. Then, a label matrix is presented for guiding the data representation by a label constraint regularization term. Finally, based on the NTD property of maintaining the internal structure of data, the graph regularizer and the label regularizer are integrated into NTD to generate the proposed method. Thus, GDNTD can extract the part-based representation, preserve the local geometrical structure of the data space, and improve the discriminative ability of the learned model simultaneously, greatly boosting the model’s data representation performance. We test the proposed method through a set of evaluations on four image datasets. Experimental results show that the GDNTD method outperforms state-of-the-art approaches, demonstrating its strong potential for data representation.},
  archive      = {J_APIN},
  author       = {Jing, Wenjing and Lu, Linzhang and Liu, Qilong},
  doi          = {10.1007/s10489-023-04738-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23864-23882},
  shortjournal = {Appl. Intell.},
  title        = {Graph regularized discriminative nonnegative tucker decomposition for tensor data representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensionality reduction by t-distribution adaptive manifold
embedding. <em>APIN</em>, <em>53</em>(20), 23853–23863. (<a
href="https://doi.org/10.1007/s10489-023-04838-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data are difficult to explore and analyze due to they are highly correlative and redundant. Although previous dimensionality reduction methods have achieved promising performance, there are still some limitations. For example, the constructed distribution of data in the embedding space could not be approximated adaptively, and the parameters in these model lack of interpretation. To handle these problems, in this paper, a novel dimensionality reduction method named t-Distribution Adaptive Manifold Embedding (t-AME) is proposed. Firstly, t-AME constructs the pairwise distance similarity probability in the embedding space by Student-t distribution, and distributions generated by different degrees of freedom are learned according to the data itself to better match high-dimensional data distributions. Afterwards, to pull similar points together and push apart dissimilar points, an objective function with the corresponding optimization strategy is designed. Therefore, both the local and global structure of the original data could be well preserved in the embedding space. Finally, numerical experiments on synthetic and real datasets illustrate that the proposed method achieves a significant improvement over some representative and state-of-the-art dimensionality reduction methods.},
  archive      = {J_APIN},
  author       = {Wang, Changpeng and Feng, Linlin and Yang, Lijuan and Wu, Tianjun and Zhang, Jiangshe},
  doi          = {10.1007/s10489-023-04838-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23853-23863},
  shortjournal = {Appl. Intell.},
  title        = {Dimensionality reduction by t-distribution adaptive manifold embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image quality enhancement of 4D light field microscopy via
reference impge propagation-based one-shot learning. <em>APIN</em>,
<em>53</em>(20), 23834–23852. (<a
href="https://doi.org/10.1007/s10489-023-04684-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Four-dimensional (4D) light-field (LF) microscopes can acquire 3D information about target objects using a microlens array (MLA). However, the resolution and quality of sub-images in the LF images are reduced because of the spatial multiplexing of rays by the element lenses of the MLA. To overcome these limitations, this study proposes an LF one-shot learning technique that can convert LF sub-images into high-quality images similar to the 2D images of conventional optical microscopes obtained without any external training datasets for image enhancement. The proposed convolutional neural network model was trained using only one training dataset comprising a high-resolution reference image captured without an MLA as the ground truth. Further, its input was the central view of the LF image. After LF one-shot learning, the trained model should be able to convert well the other LF sub-images of various directional views that were not used in the main training process. Therefore, novel learning techniques were designed for LF one-shot learning. These novel techniques include an autoencoder-based model initialization method, a feature map-based learning algorithm to prevent the overfitting of the model, and cut loss to prevent saturation. The experimental results verified that the proposed technique effectively enhances the LF image quality and resolution using a reference image. Moreover, this method enhances the resolution by up to 13 times, decreases the noise amplification effect, and restores the lost details of microscopic objects. The proposed technique is stable and yields superior experimental results compared with those of the existing resolution-enhancing methods.},
  archive      = {J_APIN},
  author       = {Kwon, Ki Hoon and Erdenebat, Munkh-Uchral and Kim, Nam and Kwon, Ki-Chul and Kim, Min Young},
  doi          = {10.1007/s10489-023-04684-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23834-23852},
  shortjournal = {Appl. Intell.},
  title        = {Image quality enhancement of 4D light field microscopy via reference impge propagation-based one-shot learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modification of hybrid RNN-HMM model in asset pricing:
Univariate and multivariate cases. <em>APIN</em>, <em>53</em>(20),
23812–23833. (<a
href="https://doi.org/10.1007/s10489-023-04762-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov Model (HMM) which is frequently used in time series modeling with satisfactory results is commonly used for predicting stock prices in many studies. Due to its more transparent structure than most of the current Neural Network (NN) models and its sensitivity to the initial parameter settings, we propose a hybrid model that combines Recurrent NN (RNN) and HMM in modeling stock prices to eliminate initial parameter influence. Despite its common application to speech recognition data with categorical variables, we reconstruct RNN and HMM for financial data. RNN is used as a solution to the probability of not reaching the global maximum based on the HMM’s initial parameters selection. To do so, we improve the classification power of HMM to achieve the hidden states that do not get stuck at a local maximum but provide the global maximum. In addition, unlike the literature, the loss function is not chosen as the maximum likelihood but is defined directly over the prices. Thus, the model does not only detect the states appropriately, yet the predictions can get closer to the actual prices. Besides, a multivariate comparison is performed to determine the effect of different numbers and types of variables through bivariate and trivariate models. The application is made on S &amp;P 500, Nasdaq daily closing prices and daily EUR/USD exchange rates data from 2000 to 2021. It is shown that the accuracy is increased significantly compared to the implementations of HMM and RNN methods separately.},
  archive      = {J_APIN},
  author       = {Aydogan-Kilic, Dilek and Selcuk-Kestel, A. Sevtap},
  doi          = {10.1007/s10489-023-04762-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23812-23833},
  shortjournal = {Appl. Intell.},
  title        = {Modification of hybrid RNN-HMM model in asset pricing: Univariate and multivariate cases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LapRamp: A noise resistant classification algorithm based on
manifold regularization. <em>APIN</em>, <em>53</em>(20), 23797–23811.
(<a href="https://doi.org/10.1007/s10489-023-04825-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract: In many applications, data samples contain incorrect labels due to data quality issues and the high cost of labeling. Although current noise-resistant classification algorithms can handle specific types of label noise, identifying the type of noise present in the given data is challenging. To address this issue, we propose a robust classification method called LapRamp, which works with multiple types of label noise. LapRamp utilizes the ramp loss function to minimize the impact of mislabeled samples far from the discriminant surface. Additionally, we incorporate manifold regularization to capture the inherent geometric structure of the data. We analyze the generalization error bound of the model in terms of Rademacher complexity, and the preliminary experimental results indicate that LapRamp has good generalization performance despite the presence of mixed label noise. Furthermore, it demonstrates stable classification accuracy when dealing with noisy labels in various scenarios.},
  archive      = {J_APIN},
  author       = {Liang, Xijun and Yu, Qi and Zhang, Kaili and Zeng, Pan and Jian, Ling},
  doi          = {10.1007/s10489-023-04825-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23797-23811},
  shortjournal = {Appl. Intell.},
  title        = {LapRamp: A noise resistant classification algorithm based on manifold regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional causal discovery based on heuristic causal
partitioning. <em>APIN</em>, <em>53</em>(20), 23768–23796. (<a
href="https://doi.org/10.1007/s10489-023-04530-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery is one of the most important research directions in the field of machine learning, aiming to discover the underlying causal relationships in the observed data. In practice, the time complexity of causal discovery will grow exponentially with increasing variables. To alleviate this problem, many methods based on divide-and-conquer strategies have been proposed. Existing methods usually partition the variables heuristically using scattered variables to achieve the dividing process, which makes it difficult to minimize vertex cut-set C and then leads to diminished causal discovery performance. In this work, we design an elaborated causal partition strategy called Causal Partition Base Graph (CPBG) to solve this problem. CPBG uses a set of low-order conditional independence (CI) tests to construct a rough skeleton S corresponding to the observed data and takes a heuristic method to search S for the optimal vertex cut-set C. Then the observed data can be partitioned into multiple variable subsets. We therefore can run a causal discovery method on each part and finally obtain the complete causal structure by merging the partial results. The proposed method is evaluated by various real-world causal datasets. Experimental results show that the CPBG method outperforms its existing counterparts, which proves that the method can support more effective and efficient causal discovery. The source code of the proposed method and all experimental results are available at https://github.com/DreamEdm/Causal .},
  archive      = {J_APIN},
  author       = {Hong, Yinghan and Guo, Junping and Mai, Guizhen and Lin, Yingqing and Zhang, Hao and Hao, Zhifeng and Zheng, Gengzhong},
  doi          = {10.1007/s10489-023-04530-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23768-23796},
  shortjournal = {Appl. Intell.},
  title        = {High-dimensional causal discovery based on heuristic causal partitioning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A transformer framework for generating context-aware
knowledge graph paths. <em>APIN</em>, <em>53</em>(20), 23740–23767. (<a
href="https://doi.org/10.1007/s10489-023-04588-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual Path Generation (CPG) refers to the task of generating knowledge path(s) between a pair of entities mentioned in an input textual context to determine the semantic connection between them. Such knowledge paths, also called contextual paths, can be very useful in many advanced information retrieval applications. Nevertheless, CPG involves several technical challenges, namely, sparse and noisy input context, missing relations in knowledge graphs, and generation of ill-formed and irrelevant knowledge paths. In this paper, we propose a transformer-based model architecture. In this approach, we leverage a mixture of pre-trained word and knowledge graph embeddings to encode the semantics of input context, a transformer decoder to perform path generation controlled by encoded input context and head entity to stay relevant to the context, and scaling methods to sample a well-formed path. We evaluate our proposed CPG models derived using the above architecture on two real datasets, both consisting of Wikinews articles as input context documents and ground truth contextual paths, as well as a large synthetic dataset to conduct larger-scale experiments. Our experiments show that our proposed models outperform the baseline models, and the scaling methods contribute to better quality contextual paths. We further analyze how CPG accuracy can be affected by different amount of context data, and missing relations in the knowledge graph. Finally, we demonstrate that an answer model for knowledge graph questions adapted for CPG could not perform well due to the lack of an effective path generation module.},
  archive      = {J_APIN},
  author       = {Lo, Pei-Chi and Lim, Ee-Peng},
  doi          = {10.1007/s10489-023-04588-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23740-23767},
  shortjournal = {Appl. Intell.},
  title        = {A transformer framework for generating context-aware knowledge graph paths},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A DNN framework for learning lagrangian drift with
uncertainty. <em>APIN</em>, <em>53</em>(20), 23729–23739. (<a
href="https://doi.org/10.1007/s10489-023-04625-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructions of Lagrangian drift, for example for objects lost at sea, are often uncertain due to unresolved physical phenomena within the data. Uncertainty is usually overcome by introducing stochasticity into the drift, but this approach requires specific assumptions for modelling uncertainty. We remove this constraint by presenting a purely data-driven framework for modelling probabilistic drift in flexible environments. Using ocean circulation model simulations, we generate probabilistic trajectories of object location by simulating uncertainty in the initial object position. We train an emulator of probabilistic drift over one day given perfectly known velocities and observe good agreement with numerical simulations. Several loss functions are tested. Then, we strain our framework by training models where the input information is imperfect. On these harder scenarios, we observe reasonable predictions although the effects of data drift become noticeable when evaluating the models against unseen flow scenarios.},
  archive      = {J_APIN},
  author       = {Jenkins, Joseph and Paiement, Adeline and Ourmières, Yann and Le Sommer, Julien and Verron, Jacques and Ubelmann, Clément and Glotin, Hervé},
  doi          = {10.1007/s10489-023-04625-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23729-23739},
  shortjournal = {Appl. Intell.},
  title        = {A DNN framework for learning lagrangian drift with uncertainty},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine segmentation and difference-aware shape adjustment for
category-level 6DoF object pose estimation. <em>APIN</em>,
<em>53</em>(20), 23711–23728. (<a
href="https://doi.org/10.1007/s10489-023-04688-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Six-degree-of-freedom (6DoF) object pose estimation is a critical task for robot manipulation, autonomous vehicles, and augmented reality. Category-level 6DoF object pose estimation is trending because it can generalize to same-category unknown objects. However, existing mean shape based methods do not consider that predicting adjustment must model shape differences, which makes these methods still suffer from shape variations among same-category objects, limiting their accuracy. Also, existing methods overlook the importance of object segmentation to 6DoF pose estimation and use an RGB-based object segmentation method with low accuracy. To address these problems, we propose difference-aware shape adjustment and RGB-D feature fusion-based object segmentation for category-level 6DoF object pose estimation. The proposed method encodes shape differences, improving mean shape adjustment and alleviating same-category shape variations. Specifically, a difference-aware shape adjustment network (DASAN) is proposed to model shape differences between the object instance and mean shape by feature subtraction with an attention mechanism. We also propose an RGB-D feature fusion-based object segmentation method that uses a coarse-to-fine framework: a 2D detector and a novel RGB-D feature fusion-based binary classification network for coarse and fine segmentation. Experiments on two well-known datasets demonstrate the proposed method’s state-of-the-art (SOTA) pose estimation accuracy. In addition, we construct comparative experiments on the latest dataset (Wild6D) and a self-collected dataset (OBJECTS) and achieve high accuracies, demonstrating the strong generalizability of the proposed method. Also, we apply the proposed method to unknown object grasping, thus demonstrating the practicability of the proposed method.},
  archive      = {J_APIN},
  author       = {Liu, Chongpei and Sun, Wei and Liu, Jian and Zhang, Xing and Fan, Shimeng and Fu, Qiang},
  doi          = {10.1007/s10489-023-04688-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23711-23728},
  shortjournal = {Appl. Intell.},
  title        = {Fine segmentation and difference-aware shape adjustment for category-level 6DoF object pose estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting deep cross-modal retrieval hashing with
adversarially robust training. <em>APIN</em>, <em>53</em>(20),
23698–23710. (<a
href="https://doi.org/10.1007/s10489-023-04715-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing methods effectively enhance the performance of conventional machine learning retrieval models, particularly in visual medium evolving cross-modal retrieval tasks, by relying on the outstanding feature extraction ability of deep neural networks (DNNs). The state-of-the-art deep hashing research focuses on designing prominent models by employing DNNs to discover semantic information from different modalities of data and execute relevant information retrieval tasks. However, the robustness attribute considered essential for reliable DNN model design has limited concerns on deep hashing models. In this article, we present an end-to-end adversarial training framework for cross-modal retrieval. Our framework leverages a projected gradient descent(PGD)-based method to generate adversarial samples, which are then combined with normal samples to achieve robust training. Our approach addresses the vulnerability issues of existing cross-modal retrieval models and fills the gap in retrieval task design. We conduct extensive experiments and compare our model with state-of-the-art cross-modal retrieval models on three benchmark datasets to verify that our model can effectively boost the performance of deep hashing retrieval models on cross-modal retrieval . This work highlights the effectiveness of adversarial training in efficient deep hashing model design.},
  archive      = {J_APIN},
  author       = {Zhang, Xingwei and Zheng, Xiaolong and Mao, Wenji and Zeng, Daniel Dajun},
  doi          = {10.1007/s10489-023-04715-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23698-23710},
  shortjournal = {Appl. Intell.},
  title        = {Boosting deep cross-modal retrieval hashing with adversarially robust training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent decision support system for optimizing inventory
management under stochastic events. <em>APIN</em>, <em>53</em>(20),
23675–23697. (<a
href="https://doi.org/10.1007/s10489-023-04801-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines an optimal pricing policy for managing deteriorating products in a stochastic inventory system. Customer arrival is formulated as a Poisson process with a fixed rate, and customer demand is probabilistically expressed based on the valuation of the product. Typically, customer interest in items decreases over time because of the declining quality of deteriorating products. A dynamic pricing policy is needed to increase retailer profitability under stochastic scenarios. This paper combines the Hamilton–Jacobi–Bellman (HJB) equation with the Particle Swarm Optimization (PSO) approach to realize an optimal solution for the dynamic pricing strategy with specific boundaries using an analytical approach. Furthermore, an artificial intelligence-based High-Order Neural Network (HONN) model is implemented as an intelligent strategy to approximate the complex dynamic behaviors of the stochastic inventory system. The present study aims to fill the research gap by comparing the analytical solution obtained from the hybrid optimization using HJB–PSO and the intelligent strategy obtained from the HONN model. The proposed approaches bring greater accuracy and efficiency to inventory management tools. The performance difference between the proposed algorithms indicates that the error does not exceed the desired limit ( $$\le$$ 1%), indicating that the analytical and numerical solutions found can be used interchangeably in practice. Furthermore, all validation tests ensure that the proposed decision support system can aid policymakers in managing stochastic inventory systems with minimal effort and adopting strategies to ensure resilience and customer satisfaction.},
  archive      = {J_APIN},
  author       = {Long, Le Ngoc Bao and Kim, Hwan-Seong and Cuong, Truong Ngoc and You, Sam-Sang},
  doi          = {10.1007/s10489-023-04801-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23675-23697},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent decision support system for optimizing inventory management under stochastic events},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust multi-view clustering in latent low-rank space with
discrepancy induction. <em>APIN</em>, <em>53</em>(20), 23655–23674. (<a
href="https://doi.org/10.1007/s10489-023-04699-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the fantastic ability to capture consistent and complementary information between views, multi-view graph clustering has attracted extensive research attention. However, multi-view data are mostly high-dimensional, which may contain many redundant and irrelevant features. At the same time, the original data are usually contaminated by noise and outliers, which may destroy the intrinsic structural information of data and reduce the reliability of the affinity matrix learned. Moreover, most models assign different weights to each view to fully consider the relation between views. The intrinsic information of some views cannot be fully utilized due to the small view weights assigned to them. To deal with these problems, in this study, we propose a robust multi-view clustering model by combining low-dimensional and low-rank latent space learning, self-representation learning, and multi-view discrepancy induction fusion into a unified framework. Specifically, the original high-dimensional data is first reconstructed in a low-dimensional and low-rank space. A self-representation learning method is used to learn the reliable affinity matrix for each view. Furthermore, the Hilbert-Schmidt independence criterion is used as a discrepancy induction module for the complementary fusion of views. Finally, to preserve the data’s local geometric structure, a simple adaptive graph regularization term is applied to the affinity matrix for each view. The comprehensive experiments on six benchmark datasets validate that the proposed model outperforms the six state-of-the-art comparison models in robustness and clustering performance.},
  archive      = {J_APIN},
  author       = {Xiong, Bo and Chen, Hongmei and Li, Tianrui and Yang, Xiaoling},
  doi          = {10.1007/s10489-023-04699-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23655-23674},
  shortjournal = {Appl. Intell.},
  title        = {Robust multi-view clustering in latent low-rank space with discrepancy induction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved ID3 algorithm based on variable precision
neighborhood rough sets. <em>APIN</em>, <em>53</em>(20), 23641–23654.
(<a href="https://doi.org/10.1007/s10489-023-04779-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical ID3 decision tree algorithm cannot directly handle continuous data and has a poor classification effect. Moreover, most of the existing approaches use a single mechanism for node measurement, which is unfavorable for the construction of decision trees. In order to solve the above problems, we propose an improved ID3 algorithm (called DIGGI) based on variable precision neighborhood rough sets. First, we introduce the notions of variable precision neighborhood (VPN) equivalence relation and VPN equivalence granule, and probe some basic properties of these notions. Second, we give the model of VPN rough sets and propose two extended measures: the VPN information gain and the VPN Gini index. Finally, we construct a hybrid measure by using the VPN dependence to combine the above two extended measures, and adopt the VPN equivalence granule as the splitting rule of DIGGI. Experimental results show that DIGGI is effective and its accuracy is greatly improved compared with three traditional decision tree algorithms, the neighborhood decision tree (NDT) and variable precision neighborhood decision tree (VPNDT) proposed in the latest literature.},
  archive      = {J_APIN},
  author       = {Liu, Caihui and Lai, Jianying and Lin, Bowen and Miao, Duoqian},
  doi          = {10.1007/s10489-023-04779-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23641-23654},
  shortjournal = {Appl. Intell.},
  title        = {An improved ID3 algorithm based on variable precision neighborhood rough sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent feature reconstruction for unsupervised anomaly
detection. <em>APIN</em>, <em>53</em>(20), 23628–23640. (<a
href="https://doi.org/10.1007/s10489-023-04767-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies (or outliers) indicate a minority of data items that are quite different from the majority (inliers) of a dataset in a certain aspect. Unsupervised anomaly detection (UAD) is an important but not yet extensively studied research topic. Recent deep learning based methods exploit the reconstruction gap between inliers and outliers to discriminate them. However, it is observed that the reconstruction gap often decreases rapidly as the training process goes. And there is no reasonable way to set the training stop point. To support effective UAD, we propose a new UAD framework by introducing a Latent Feature Reconstruction (LFR) layer that can be applied to recent UAD methods. The LFR layer acts as a regularizer to constrain the latent features in a low-rank subspace from which inliers can be reconstructed well while outliers cannot. We develop two new UAD methods by implementing the proposed framework with autoencoder architecture and geometric transformation scheme. Experiments on five benchmarks show that our proposed methods can achieve state-of-the-art performance in most cases.},
  archive      = {J_APIN},
  author       = {Lin, Jinghuang and He, Yifan and Xu, Weixia and Guan, Jihong and Zhang, Ji and Zhou, Shuigeng},
  doi          = {10.1007/s10489-023-04767-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23628-23640},
  shortjournal = {Appl. Intell.},
  title        = {Latent feature reconstruction for unsupervised anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusion of energy sensors with missing values. <em>APIN</em>,
<em>53</em>(20), 23613–23627. (<a
href="https://doi.org/10.1007/s10489-023-04752-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Smart Energy Grids, the information flow used to make decisions is the result of fusion of different sources. Communication latency, possible sensor faults and inaccuracies, may negatively impact the data quality and hence the taken decisions. For these reasons, the construction of a robust representation of the input signals that replaces and/or corrects the inaccurate data is crucial for effective classification, anomaly detection and planning. Recent works on Data Fusion and data imputation suggest that the usage of other signals in the same context can empower the representation and can be a useful preprocessing task. In this work we describe an Autoencoder-based data fusion architecture with convolutional layers, skip connections and ad-hoc augmented training sets for data imputation applied to the power consumption measurements obtained by different sub-meters. Among the investigated architectures, the approach with the shared convolutional layers and an augmented dataset that consider missing data in the random positions and located in the central part (AE-A-ALL-CNN), is the most promising one. In presence of one half of the input signal, in the central part, completely erased, it improves the imputation capability, respect to two most employed approaches (denoising autoencoder and MICE) in the average of 12 %.},
  archive      = {J_APIN},
  author       = {Buonanno, Amedeo and Di Gennaro, Giovanni and Graditi, Giorgio and Nogarotto, Antonio and Palmieri, Francesco A N and Valenti, Maria},
  doi          = {10.1007/s10489-023-04752-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23613-23627},
  shortjournal = {Appl. Intell.},
  title        = {Fusion of energy sensors with missing values},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated machine learning with dynamic ensemble selection.
<em>APIN</em>, <em>53</em>(20), 23596–23612. (<a
href="https://doi.org/10.1007/s10489-023-04770-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) has been developed for automatically building effective machine learning pipelines. However, existing AutoML frameworks use a single individual pipeline or a weighted ensemble of several pipelines to create the final predictive model, which ignores the difference between the unseen instances and leads to undesirable performance. To construct customized models for different unseen instances, we propose a novel AutoML method based on dynamic ensemble selection of machine learning pipelines, where the most competent combination of base pipelines is selected and aggregated to predict a specific unseen instance. First, an effective base pipeline pool is generated by filtering out the underperforming pipelines. Second, when an unseen instance appears, we deploy a new dynamic balanced accuracy criterion to select the most competent ensemble of base pipelines according to its local region. Finally, the outputs of the selected pipelines are integrated to give the final prediction. Comprehensive experiments on 39 publicly available datasets demonstrate the superiority of the proposed method compared to some state-of-the-art AutoML frameworks.},
  archive      = {J_APIN},
  author       = {Zhu, Xiaoyan and Ren, Jingtao and Wang, Jiayin and Li, Jiaxuan},
  doi          = {10.1007/s10489-023-04770-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23596-23612},
  shortjournal = {Appl. Intell.},
  title        = {Automated machine learning with dynamic ensemble selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SD2Net: Surface-mounted device detection network with
convolution-free attention mechanism for printed circuit board integrity
assurance. <em>APIN</em>, <em>53</em>(20), 23582–23595. (<a
href="https://doi.org/10.1007/s10489-023-04800-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of detecting and classifying surface-mounted devices (SMDs) on printed circuit boards (PCBs) is influenced by the complexity of the background and the variations in scale among the SMDs. Several object detection networks, such as the you only look once (YOLO) series-based networks and two-stage models like Mask R-CNN, have been developed to handle the detection of surface-mounted devices (SMDs) at various scales. However, the majority of these networks exhibit a complex structure with an abundance of parameters, which presents difficulties in deploying SMD detection networks in real-world application scenarios. To overcome this challenge, we introduce SD2Net, a convolution-free attention-based network specifically designed for accurate SMD detection. The front-end of SD2Net employs the proposed pyramid vision transformer (PVT) block-based backbone to extract SMD features in five stages, incorporating an attention mechanism. Subsequently, the extracted multi-scale features are fed into the feature pyramid network (FPN) for efficient feature fusion. In the back-end of SD2Net, the final stage integrates the VarifocalNet head module (VFH). By utilizing the Varifocal loss and generalized intersection over union (GIoU), it performs precise regression and computation of the localization and class mapping vector for SMDs. We conduct extensive experiments using two well-established publicly available datasets: FICS-PCB and PCB-WACV. The experimental results indicate the superior performance of our model in comparison to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Zhihao and Shen, Xizhong},
  doi          = {10.1007/s10489-023-04800-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23582-23595},
  shortjournal = {Appl. Intell.},
  title        = {SD2Net: Surface-mounted device detection network with convolution-free attention mechanism for printed circuit board integrity assurance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SwinEFT: A robust and powerful swin transformer based event
frame tracker. <em>APIN</em>, <em>53</em>(20), 23564–23581. (<a
href="https://doi.org/10.1007/s10489-023-04763-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, event cameras, as a new generation of bionic cameras with characteristics of high dynamic range and high temporal resolution, provide a brand new competitive modal for multi-modal tracking. However, recent works on RGBE tracking pay too much attention to utilizing the complementary information while ignoring to enhance the modality-shared information and the global relations inside and across modalities. In this paper, we propose an end-to-end full attention tracker named Swin Transformer Event Frame Tracker (SwinEFT) to fully explore both modality-specific and modality-shared information. To be specific, we firstly adopt a simple but effective event representation to narrow the domain gap as well as obtain a clearer tracking target. With the deployment of shifted window based attention mechanism, our tracker is better able to leverage the global relations, resulting in locating a more accurate bounding box. Besides, in order to enhance the modality-shared information, we design Swin Decoder by introducing cross-attention based on shifted windows for information interaction. Extended experiments on two realistic RGBE tracking datasets demonstrate the outstanding performance and robustness of SwinEFT against the state-of-the-art methods under various challenging scenarios.},
  archive      = {J_APIN},
  author       = {Zeng, Zhaoyuan and Li, Xiaopeng and Fan, Cien and Zou, Lian and Chi, Ruan},
  doi          = {10.1007/s10489-023-04763-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23564-23581},
  shortjournal = {Appl. Intell.},
  title        = {SwinEFT: A robust and powerful swin transformer based event frame tracker},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAST: A convolutional attention spatiotemporal network for
predictive learning. <em>APIN</em>, <em>53</em>(20), 23553–23563. (<a
href="https://doi.org/10.1007/s10489-023-04750-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive learning is receiving growing interests with wide applications. Combining RNN and CNN, recent works attempt to capture temporal dependencies and spatial correlations simultaneously. However, these methods tend to be deeper networks without feature compression in scale and easily suffer from high computational costs, especially for high-resolution frames. To reduce resource burden, we introduce a novel and efficient network named CASTnet for spatiotemporal predictive learning. In this paper, we present a concise architecture cascading multiple stages, which can directly accept the raw frames without information loss in the first stage and compress features in the following stages leading to low computation relatively. Then, we adopt a special prediction head to aggregate multi-level features into predictions, which helps our model capture multi-scale targets. As for the spatiotemporal block, we adopt bidirectional convolutional attention (BCA) operations to capture the local and long-range information simultaneously without quadratic calculation complexity. In order to further improve the model performance while not causing much additional computation burden, we propose frame-wise knowledge distillation, which enables each frame at low-level to learn from each one at high-level. To evaluate our model, we conduct quantitative qualitative and ablation experiments on the MovingMNIST and Radar Echo datasets. The results show that our CASTnet achieves competitive results with lower computational costs and fewer network parameters than the state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Sun, Fengzhen and Jin, Weidong},
  doi          = {10.1007/s10489-023-04750-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23553-23563},
  shortjournal = {Appl. Intell.},
  title        = {CAST: A convolutional attention spatiotemporal network for predictive learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological enhanced graph neural networks for
semi-supervised node classification. <em>APIN</em>, <em>53</em>(20),
23538–23552. (<a
href="https://doi.org/10.1007/s10489-023-04739-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and non-Euclidean structure of graph data hinders the development of data augmentation methods similar to those in computer vision. In this paper, we propose a feature augmentation method for graph nodes based on topological regularization, in which topological structure information is introduced into an end-to-end model to promote better learning of node representation. Specifically, we first obtain topology embedding of nodes through Node2vec, an unsupervised graph feature learning method based on random walk. Then, the topological embedding as additional features and the original node features are input into a Symmetric Graph Neural Network framework for propagation, and two different high-order neighborhood representations of the nodes are obtained. On this basis, we propose a regularization technique to bridge the differences between the two different node representations, eliminate the adverse effects caused by the topological features of graphs directly used, and greatly improve the performance. Our framework can be effectively combined with other graph neural network models, and can effectively prevent over-smoothing of deep graph network. Experimental results on five datasets confirm the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Song, Rui and Giunchiglia, Fausto and Zhao, Ke and Xu, Hao},
  doi          = {10.1007/s10489-023-04739-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23538-23552},
  shortjournal = {Appl. Intell.},
  title        = {Topological enhanced graph neural networks for semi-supervised node classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed sparse learning for stochastic configuration
networks via alternating direction method of multipliers. <em>APIN</em>,
<em>53</em>(20), 23522–23537. (<a
href="https://doi.org/10.1007/s10489-023-04765-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a class of randomized learning algorithms, stochastic configuration networks (SCNs) have demonstrated excellent capabilities in various intelligent scenarios. Regarding these algorithms, a growing interest in recent years has been shown in sparse representation, which is a powerful tool in many large-scale applications with high-dimensional data. However, few methods currently exist to fit sparse models in distributed environments. In this paper, we focus on devising two fully distributed learning algorithms for SCNs with sparsity regularization. The first algorithm is based on sample-wise division which is appropriate for the problem of large samples. The second algorithm is fit for high-dimensional data, which is based on feature-wise division. The sparse solutions for the decentralized optimization problems are obtained by introducing the alternating direction method of multipliers (ADMM) and the distributed average consensus (DAC) algorithm. Experimental results are provided to demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_APIN},
  author       = {Zhou, Yujun and Ai, Wu and Tang, Guoqiang and Chen, Huazhou},
  doi          = {10.1007/s10489-023-04765-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23522-23537},
  shortjournal = {Appl. Intell.},
  title        = {Distributed sparse learning for stochastic configuration networks via alternating direction method of multipliers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The consistency and consensus analysis for group
decision-making with incomplete linguistic interval-valued
intuitionistic fuzzy preference relations. <em>APIN</em>,
<em>53</em>(20), 23500–23521. (<a
href="https://doi.org/10.1007/s10489-023-04605-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly provides a group decision-making (GDM) method based on linguistic interval-valued intuitionistic fuzzy preference relations (LIVIFPRs), where consistency and consensus analysis is conducted. The multiplicative consistency of LIVIFPRs is first introduced, and a consistency-based model is built to ascertain the missing values of an incomplete LIVIFPR. Considering the smallest distance, an optimization model is established to repair the unacceptably multiplicatively consistent LIVIFPR to have acceptable consistency. Meanwhile, the linguistic interval-valued intuitionistic fuzzy priority weights of LIVIFPR are constructed via the optimal solutions of a programming model. Then, Algorithm I for decision-making with one incomplete LIVIFPR is presented. For the GDM problem, the weights of experts are determined before aggregating the individual LIVIFPRs. Moreover, when the consensus of an individual LIVIFPR is unacceptable, a mathematical model is utilized to reach the consensus requirement. Subsequently, Algorithm II for GDM with incomplete LIVIFPRs is proposed step-by-step. Finally, the new GDM method is used to evaluate four Chinese express companies, and the advantages of this approach are demonstrated by performing a comparison analysis.},
  archive      = {J_APIN},
  author       = {Li, Tao and Zhang, Liyuan and Zhang, Zhenglong},
  doi          = {10.1007/s10489-023-04605-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23500-23521},
  shortjournal = {Appl. Intell.},
  title        = {The consistency and consensus analysis for group decision-making with incomplete linguistic interval-valued intuitionistic fuzzy preference relations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automatic segmentation framework of quasi-periodic time
series through graph structure. <em>APIN</em>, <em>53</em>(20),
23482–23499. (<a
href="https://doi.org/10.1007/s10489-023-04814-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of quasi-periodic time series (QTS) is crucial for modeling analysis in industrial and medical fields. However, it is challenging to automatically and effectively split different types of QTSs under the same settings. To address this issue, we propose an enhanced graph-based QTS automatic segmentation (EGQAS) framework that integrates an enhanced graph structure and hybrid clustering. The enhanced graph structure improves the stability of QTS segmentation, especially for a large number of QTS, by using edge weight filtering and aggregation. Hybrid clustering, which consists of hierarchical clustering and a modified k-means algorithm, removes clusters with outliers and incomplete divisions to improve the integrity of the final QTS split points. For four different types of public datasets, EGQAS outperforms the current state-of-the-art baselines, demonstrating its better adaptability. In tests with the MIT-BIH arrhythmia database (MITDB), EGQAS proves itself to be effective and stable with a large number of data.},
  archive      = {J_APIN},
  author       = {Tang, Xiaolan and Zheng, Desheng and Kebede, Gebre S. and Li, Zhengyu and Li, Xiaoyu and Lu, Chao and Li, Lintao and Zhou, Yong and Yang, Shan},
  doi          = {10.1007/s10489-023-04814-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23482-23499},
  shortjournal = {Appl. Intell.},
  title        = {An automatic segmentation framework of quasi-periodic time series through graph structure},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curvilinear object segmentation in medical images based on
ODoS filter and deep learning network. <em>APIN</em>, <em>53</em>(20),
23470–23481. (<a
href="https://doi.org/10.1007/s10489-023-04773-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of curvilinear objects in medical images plays an important role in the diagnosis and evaluation of human diseases, yet it is a challenging uncertainty in the complex segmentation tasks due to different issues such as various image appearances, low contrast between curvilinear objects and their surrounding backgrounds, thin and uneven curvilinear structures, and improper background illumination conditions. To overcome these challenges, we present a unique curvilinear structure segmentation framework based on an oriented derivative of stick (ODoS) filter and a deep learning network for curvilinear object segmentation in medical images. Currently, a large number of deep learning models emphasize developing deep architectures and ignore capturing the structural features of curvilinear objects, which may lead to unsatisfactory results.In consequence, a new approach that incorporates an ODoS filter as part of a deep learning network is presented to improve the spatial attention of curvilinear objects.Specifically, the input image is transferred into four-channel image constructed by the ODoS filter. In which, the original image is considered the principal part to describe various image appearance and complex background illumination conditions, a multi-step strategy is used to enhance the contrast between curvilinear objects and their surrounding backgrounds, and a vector field is applied to discriminate thin and uneven curvilinear structures. Subsequently, a deep learning framework is employed to extract various structural features for curvilinear object segmentation in medical images.Which can effectively capture the spatial attention of curvature objects. The performance of the computational model is validated in experiments conducted on the publicly available DRIVE, STARE and CHASEDB1 datasets. Compared with ground truths, the presented model acquired high F1 values of 0.826, 0.819 and 0.808, and high accuracy of 0.969, 0.973, and 0.973 on three different datasets, respectively. The experimental results indicate that the presented model yields surprising results compared with those of some state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Peng, Yuanyuan and Pan, Lin and Luan, Pengpeng and Tu, Hongbin and Li, Xiong},
  doi          = {10.1007/s10489-023-04773-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23470-23481},
  shortjournal = {Appl. Intell.},
  title        = {Curvilinear object segmentation in medical images based on ODoS filter and deep learning network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective credit assignment deep policy gradient multi-agent
reinforcement learning for vehicle dispatch. <em>APIN</em>,
<em>53</em>(20), 23457–23469. (<a
href="https://doi.org/10.1007/s10489-023-04689-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of online car-hailing platforms, more travel options and convenience have been provided to people. However, the ’tidal phenomenon’ of travel often leads to an imbalance between the supply and demand of vehicles, especially during peak hours. In this paper, we propose a reinforcement learning algorithm for fleet dispatch using effective Credit Assignment Deep Policy Gradient (CADPG). The CADPG model first learns an action for each agent (i.e., vehicle) with the local states of the vehicle through the policy network. Secondly, a set of parameters for credit assignment to compute the total Q value is learned by a hyper-network with the input of the global state. Finally, we feed the joint action vectors and the hyperparameters produced by the hyper-network into the critic network to obtain the total Q value of the joint actions. Experimental results conducted on real datasets show that our proposed method outperforms the compared algorithms.},
  archive      = {J_APIN},
  author       = {Huang, Xiaohui and Zhang, Xiong and Ling, Jiahao and Cheng, Xuebo},
  doi          = {10.1007/s10489-023-04689-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23457-23469},
  shortjournal = {Appl. Intell.},
  title        = {Effective credit assignment deep policy gradient multi-agent reinforcement learning for vehicle dispatch},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Collaborative representation induced broad learning model
for classification. <em>APIN</em>, <em>53</em>(20), 23442–23456. (<a
href="https://doi.org/10.1007/s10489-023-04709-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broad learning system (BLS) is a novel flat neural network that is fast and effective in various pattern recognition and classification applications. Many researchers have investigated this learning approach due to its remarkable performance. However, the feature nodes used in BLS are mapped with random weights for the input data, which is inefficient and can lead to inferior results since the random mapping contains redundant and unpredictable information for constructing the feature nodes. To resolve this issue and improve BLS, in this study, we aim to present one representation induced method, i.e., the collaborative representation induced broad learning model (CRI_BLM), to replace the random mapping for producing the feature nodes. This proposed method introduces the collaborative representation technique to code the input training sample as a collaborative linear combination (coding coefficient) of all dictionary samples, before further generating the enhancement nodes under the broad learning framework for classification. Compared to the original feature nodes with random mapping, this approach can capture more effective features for pattern recognition and classification. Extensive experiments with several datasets and comparisons with various classifiers were investigated to confirm that our proposed CRI_BLM is remarkable and effective (e.g., obtaining the best result: 96.80% in the Fifteen Scene Categories database).},
  archive      = {J_APIN},
  author       = {Zhang, Qi and Zhou, Jianhang and Xu, Yong and Zhang, Bob},
  doi          = {10.1007/s10489-023-04709-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23442-23456},
  shortjournal = {Appl. Intell.},
  title        = {Collaborative representation induced broad learning model for classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STMT: Spatio-temporal memory transformer for multi-object
tracking. <em>APIN</em>, <em>53</em>(20), 23426–23441. (<a
href="https://doi.org/10.1007/s10489-023-04617-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typically, modern online Multi-Object Tracking (MOT) methods first obtain the detected objects in each frame and then establish associations between them in successive frames. However, it is difficult to obtain high-quality trajectories when camera motion, fast motion, and occlusion challenges occur. To address these problems, this paper proposes a transformer-based MOT system named Spatio-Temporal Memory Transformer (STMT), which focuses on time and history information. The proposed STMT consists of a Spatio-Temporal Enhancement Module (STEM) that uses 3D convolution to model the spatial and temporal interactions of objects and obtains rich features in spatio-temporal information. Moreover, a Dynamic Spatio-Temporal Memory (DSTM) is presented to associate detections with tracklets and contains three units: an Identity Aggregation Module (IAM), a Linear Dynamic Encoder (LD-Encoder) and a memory Decoder (Decoder). The IAM utilizes the geometric changes of objects to reduce the impact of deformation on tracking performance, the LD-Encoder is used to obtain the dependency between objects, and the Decoder generates appearance similarity scores. Furthermore, a Score Fusion Equilibrium Strategy (SFES) is employed to balance the similarity and position distance fusion scores. Extensive experiments demonstrate that the proposed STMT approach is generally superior to the state-of-the-art trackers on the MOT16 and MOT17 benchmarks.},
  archive      = {J_APIN},
  author       = {Gu, Songbo and Ma, Jianxin and Hui, Guancheng and Xiao, Qiyang and Shi, Wentao},
  doi          = {10.1007/s10489-023-04617-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23426-23441},
  shortjournal = {Appl. Intell.},
  title        = {STMT: Spatio-temporal memory transformer for multi-object tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The 3D bin packing problem for multiple boxes and irregular
items based on deep q-network. <em>APIN</em>, <em>53</em>(20),
23398–23425. (<a
href="https://doi.org/10.1007/s10489-023-04604-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irregular packing in e-commerce warehouses is a special case of a three-dimensional box packing problem (3DBPP). It is necessary to select the type and quantity of boxes and determine the location and orientation of the items to maximize the use of the loading space. In this paper, a spatial particle model of the 3DBPP for multiple boxes and irregular items is constructed using the three-dimensional (3D) point cloud and granulation method. In the model, the 3D point cloud is used to describe the shapes of irregular items, and the granulation method is used for the transformation from sparse and uneven point clouds to spatial particle convex hulls. In addition, we designed an empirical simulation algorithm (ESA) based on the combination of expert rules extracted in practical packing activities and empirical simulation, and an intelligent algorithm for 3DBPPs with irregular items combined with the framework of the deep Q network (DQN) algorithm in deep reinforcement learning. An instance generator is proposed based on industry data to generate realistic projects with representative attributes for the above two algorithms, such as types of boxes, irregular items, 3D spatial plane convex hulls, and spatially granular data. The numerical results show that the ESA can quickly obtain a high-quality packing scheme, and the intelligent DQN packing algorithm in deep reinforcement learning can avoid the limitation of expert rules and achieve a better scheme with a certain time for the training process.},
  archive      = {J_APIN},
  author       = {Liu, Huwei and Zhou, Li and Yang, Jianglong and Zhao, Junhui},
  doi          = {10.1007/s10489-023-04604-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23398-23425},
  shortjournal = {Appl. Intell.},
  title        = {The 3D bin packing problem for multiple boxes and irregular items based on deep Q-network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced face aging using dual-learning and muti-attention
mechanism. <em>APIN</em>, <em>53</em>(20), 23383–23397. (<a
href="https://doi.org/10.1007/s10489-023-04713-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face aging is an active area of research in multimedia applications that involves modifying a person’s facial photo to resemble their appearance at a different age. While conditional Generative Adversarial Networks (cGANs) have made significant progress in this field, most current approaches still face challenges in generating convincing age progression while preserving the subject’s identity. These limitations arise due to three main factors: i) a scarcity of long-range sequential labelled faces of the same person in existing datasets, which are required for training; ii) a focus on texture changes such as wrinkles, which neglects structural variations that are important in aging and limit the effectiveness of these models for large age spans; and iii) the tendency to preserve personal identity by minimizing the differences between inputs and synthesized results, which can result in blurry artifacts and insufficient variations. In this paper, we propose a novel approach to address these limitations called Landmark-guided Dual-learning cGAN (LDcGAN), which includes a multi-attention mechanism. Our approach uses an external landmark attention to adjust variations in facial structure and a built-in attention to emphasize the most discriminative regions relevant to aging. The primal cGAN is conditioned with age vectors and converts input faces to target ages, while the dual cGAN inverts the process by feeding the synthesized results back to the original input age range. This enables LDcGAN to improve age consistency and minimize changes that affect personal identity and background. Our approach demonstrates appealing results in terms of image quality, personal identity, and age accuracy, as confirmed by both qualitative and quantitative experiments.},
  archive      = {J_APIN},
  author       = {Huang, Xin and Gong, Minglun},
  doi          = {10.1007/s10489-023-04713-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23383-23397},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced face aging using dual-learning and muti-attention mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Converting hyperparameter gamma in distance-based loss
functions to normal parameter for knowledge graph completion.
<em>APIN</em>, <em>53</em>(20), 23369–23382. (<a
href="https://doi.org/10.1007/s10489-023-04790-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameter gamma, which is used in distance-based knowledge graph embedding to distinguish positive and negative samples, plays an important role in model performance. Usually, gamma is considered a hyperparameter and taken from a discrete set. However, as the boundary of positive and negative samples, gamma may have a continuous property and be complementary to other parameters. The high consistency between gamma and the learnable parameter with respect to their attributes means that choosing gamma as a hyperparameter is probably not a good choice. To better explore the characteristics of gamma, we propose a multi-iterated parameterized scheme to convert gamma from a hyperparameter to a normal parameter. Two concrete implementation methods, the macro parametrized method and the micro parameterized method, are provided to achieve this conversion. The conversion for gamma can reduce dependency on expert knowledge in the gamma setting and can allow successors to become involved in the recent framework quickly. Experimental results on TransE, RotatE and LineaRE show that the proposed scheme and methods can achieve approximate or better results when compared with those of the original method. A series of experimental analyses further explore the characteristics and illustrate the availability of the proposed parameterized scheme and methods.},
  archive      = {J_APIN},
  author       = {Zhang, Jinglin and Shen, Bo and Wang, Tao and Zhong, Yu},
  doi          = {10.1007/s10489-023-04790-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23369-23382},
  shortjournal = {Appl. Intell.},
  title        = {Converting hyperparameter gamma in distance-based loss functions to normal parameter for knowledge graph completion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal attention-based transformer for video captioning.
<em>APIN</em>, <em>53</em>(20), 23349–23368. (<a
href="https://doi.org/10.1007/s10489-023-04597-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning is a computer vision task that generates a natural language description for a video. In this paper, we propose a multimodal attention-based transformer using the keyframe features, object features, and semantic keyword embedding features of a video. The Structural Similarity Index Measure (SSIM) is used to extract keyframes from a video. We also detect the unique objects from the extracted keyframes. The features from the keyframes and the objects detected in the keyframes are extracted using a pretrained Convolutional Neural Network (CNN). In the encoder, we use a bimodal attention block to apply two-way cross-attention between the keyframe features and the object features. In the decoder, we combine the features of the words generated up to the previous time step, the semantic keyword embedding features, and the encoder features using a tri-modal attention block. This allows the decoder to choose the multimodal features dynamically to generate the next word in the description. We evaluated the proposed approach using the MSVD, MSR-VTT, and Charades datasets and observed that the proposed model provides better performance than other state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Munusamy, Hemalatha and C, Chandra Sekhar},
  doi          = {10.1007/s10489-023-04597-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23349-23368},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal attention-based transformer for video captioning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bunet: An effective and efficient segmentation method based
on bilateral encoder-decoder structure for rapid detection of apple tree
branches. <em>APIN</em>, <em>53</em>(20), 23336–23348. (<a
href="https://doi.org/10.1007/s10489-023-04742-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic apple harvesting robots have received much research attention in recent years to lower harvesting costs. A fundamental problem for harvesting robots is how to quickly and accurately detect branches to avoid collisions with limited hardware resources. In this paper, we propose a lightweight, high-accurate and real-time semantic segmentation network, Bilateral U-shape Network (BUNet), to segment apple tree branches. The BUNet consists mainly of a U-shaped detail branch and a U-shaped semantic branch, the former for capturing spatial details and the latter for supplementing semantic information. These two U-shape branches complement each other, keeping the high accuracy of the Encoder-decoder Backbone while maintaining the efficiency and effectiveness of the Two-pathway Backbone. In addition, a Simplified Attention Fusion Module (SAFM) is proposed to effectively fuse different levels of information from two branches for pixel-by-pixel prediction. Experimental results show (on our own constructed dataset) that BUNet achieves the highest Intersection over Union (IoU) and F1-score of 75.96% and 86.34%, respectively, with minimum parameters of 0.93M and 11.94G Floating-point of Operations (FLOPs) in branch segmentation. Meanwhile, BUNet achieves a speed of 110.32 Frames Per Second (FPS) with input image size of 1280 $$\times $$ 720 pixels. These results confirm that the proposed method can effectively detect the branches and can, therefore, be used to plan an obstacle avoidance path for harvesting robots.},
  archive      = {J_APIN},
  author       = {Zhang, Shanshan and Wan, Hao and Fan, Zeming and Zeng, Xilei and Zhang, Ke},
  doi          = {10.1007/s10489-023-04742-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23336-23348},
  shortjournal = {Appl. Intell.},
  title        = {Bunet: An effective and efficient segmentation method based on bilateral encoder-decoder structure for rapid detection of apple tree branches},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating a new perspective of z-number into ELECTRE II
with group consensus involving reliance degree and prospect theory.
<em>APIN</em>, <em>53</em>(20), 23316–23335. (<a
href="https://doi.org/10.1007/s10489-023-04757-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Z-number expresses the membership degrees of an element to a set by several possibilities and their reliability. Therefore, it has a great advantage in dealing with fuzziness and uncertainty especially for multi-criteria group decision-making (MCGDM). In this paper, a novel approach, named HFLT-Z-h-ELECTRE II which incorporates Z-number into ELECTRE II is proposed to handle divergent views of group members and vague information. Specifically, hesitant fuzzy linguistic term set (HFLTS) helps experts to express evaluations about membership degrees. And a developed group consensus model enriched by reliance degree and cumulative prospect theory for further consideration is equipped for expert weights which replace the second part of Z-number in a brand-new perspective. Additionally, a corresponding average support degree function which is beneficial to comparing each pair of criteria is proposed. An application example about sustainable construction material suppliers selection is operated for experiment to show application and practicality of the proposal. Afterwards, further comparative analysis and sensitivity analysis are conducted to explore the impacts of parameters and discuss the merits of our proposal.},
  archive      = {J_APIN},
  author       = {Tu, Yan and Zhou, Renpeng and Zhou, Xiaoyang and Lev, Benjamin},
  doi          = {10.1007/s10489-023-04757-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23316-23335},
  shortjournal = {Appl. Intell.},
  title        = {Incorporating a new perspective of Z-number into ELECTRE II with group consensus involving reliance degree and prospect theory},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weakly supervised spatial group attention network for
fine-grained visual recognition. <em>APIN</em>, <em>53</em>(20),
23301–23315. (<a
href="https://doi.org/10.1007/s10489-023-04627-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fine-grained visual recognition is to classify several sub-categories affiliated to the same basic-level category, which is highly challenging because the same sub-category with large variance and different sub-categories with small variance. Previously approaches generally localize the targets or parts first, then determine which sub-category the image is attached to. They depend on target or part annotations, which are labor-intensive and a barrier to moving towards practical use. Other methods indirectly extract recognizable areas from the high-level feature maps, ignoring the spatial relationships between the target and its parts, which may cause inaccurate recognition. In this paper, we propose a weakly supervised spatial group attention network (WSSGA-Net) for fine-grained bird recognition. According to the spatial relationships between the target and its parts, we embed the spatial group attention (SGA) module into the WSSGA-Net to highlight the correct semantic feature regions by establishing a semantic feature space enhancement mechanism. In addition, we apply moment exchange (MoEx) to generate new feature maps by exchanging two input image feature moments for data augmentation. Comprehensive experiments indicate that our approach significantly has a better performance than the state-of-the-art approaches on the standard bird image datasets Bird-65, CUB200-2011 and fine-grained dataset Stanford Cars.},
  archive      = {J_APIN},
  author       = {Xie, Jiangjian and Zhong, Yujie and Zhang, Junguo and Zhang, Changchun and Schuller, Björn W},
  doi          = {10.1007/s10489-023-04627-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23301-23315},
  shortjournal = {Appl. Intell.},
  title        = {A weakly supervised spatial group attention network for fine-grained visual recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multispecies bird sound recognition using a fully
convolutional neural network. <em>APIN</em>, <em>53</em>(20),
23287–23300. (<a
href="https://doi.org/10.1007/s10489-023-04704-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a method based on fully convolutional neural networks (FCNs) to identify migratory birds from their songs, with the objective of recognizing which birds pass through certain areas and at what time. To determine the best FCN architecture, extensive experimentation was conducted through a grid search, exploring the optimal depth, width, and activation function of the network. The results showed that the optimal number of filters is 400 in the widest layer, with 4 convolutional blocks with maxpooling and an adaptive activation function. The proposed FCN offers a significant advantage over other techniques, as it can recognize the sound of a bird in audio of any length with an accuracy greater than 85%. Furthermore, due to its architecture, the network can detect more than one species from audio and can carry out near-real-time sound recognition. Additionally, the proposed method is lightweight, making it ideal for deployment and use in IoT devices. The study also presents a comparative analysis of the proposed method against other techniques, demonstrating an improvement of over 67% in the best-case scenario. These findings contribute to advancing the field of bird sound recognition and provide valuable insights into the practical application of FCNs in real-world scenarios.},
  archive      = {J_APIN},
  author       = {García-Ordás, María Teresa and Rubio-Martín, Sergio and Benítez-Andrades, José Alberto and Alaiz-Moretón, Hector and García-Rodríguez, Isaías},
  doi          = {10.1007/s10489-023-04704-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23287-23300},
  shortjournal = {Appl. Intell.},
  title        = {Multispecies bird sound recognition using a fully convolutional neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HIN-based rating prediction in recommender systems via GCN
and meta-learning. <em>APIN</em>, <em>53</em>(20), 23271–23286. (<a
href="https://doi.org/10.1007/s10489-023-04769-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rating prediction is a crucial task for recommender systems, but it has the problem of difficulty in quickly capturing user preference transfer and cold-start problem. Thus, this paper proposes the meta-learning-based rating prediction model for heterogeneous information networks (HIN) called Meta-HRP (HIN-based Rating Prediction) to solve these problems. The model first constructs meta-tasks through meta-paths on HIN and then constructs an embedding representation generator based on graph convolutional network (GCN) and attention mechanism to generate embeddings for users and items. Then the proposed rating prediction meta-learner leverages historical interaction data to learn prior knowledge and rapidly adapts to new items based on a few recent user rating records to timely capture user preference transfer and alleviate the cold-start problem. We validate Meta-HRP with extensive experiments, and the proposed model reduces root mean square error by at least 8.49 $$\%$$ on average over the baselines on two public benchmark datasets. Furthermore, Meta-HRP outperforms the state-of-the-arts in most cold-start cases.},
  archive      = {J_APIN},
  author       = {Zhou, Mingqiang and Li, Kunpeng and Dai, Kailang and Wu, Quanwang},
  doi          = {10.1007/s10489-023-04769-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23271-23286},
  shortjournal = {Appl. Intell.},
  title        = {HIN-based rating prediction in recommender systems via GCN and meta-learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RDDRL: A recurrent deduction deep reinforcement learning
model for multimodal vision-robot navigation. <em>APIN</em>,
<em>53</em>(20), 23244–23270. (<a
href="https://doi.org/10.1007/s10489-023-04754-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep reinforcement learning-based mobile robot navigation relies largely on single-modal visual perception to perform local-scale navigation. However, multimodal visual fusion-based global navigation is still under technical exploration. Visual navigation necessitates that agents drive safely in structured, changing, and even unpredictable environments; otherwise, inappropriate operations may result in mission failure and even irreversible damage to life and property. We propose a recurrent deduction deep learning model (RDDRL) for multimodal vision-robot navigation to address these issues. We incorporate a recurrent reasoning mechanism (RRM) into the reinforcement learning model, which allows the agent to store memory, predict the future, and aid in policy learning. Specifically, the RRM first stores current observations and states by learning a parameterized environment model and then predicts future transitions. The RRM then performs a self-assessment on the predicted behavior and perceives the consequences of the current policy, producing a more reliable decision-making process. Furthermore, to obtain global-scale behavioral decision-making, information from scene recognition, semantic segmentation, and pose estimation are fused and used as partial observations of the RDDRL. A large number of simulated experiments based on CARLA scenarios, as well as test results in real-world scenarios, show that RDDRL outperforms state-of-the-art RL methods in terms of driving stability and safety. The results show that by training the agent, the collision rate in the global decision-making of the unmanned vehicle decreases from 0.2 % in the training state to 0.0 % in the test state.},
  archive      = {J_APIN},
  author       = {Li, Zhenyu and Zhou, Aiguo},
  doi          = {10.1007/s10489-023-04754-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23244-23270},
  shortjournal = {Appl. Intell.},
  title        = {RDDRL: A recurrent deduction deep reinforcement learning model for multimodal vision-robot navigation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature engineering of EEG applied to mental disorders: A
systematic mapping study. <em>APIN</em>, <em>53</em>(20), 23203–23243.
(<a href="https://doi.org/10.1007/s10489-023-04702-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Around a third of the total population of Europe suffers from mental disorders. The use of electroencephalography (EEG) together with Machine Learning (ML) algorithms to diagnose mental disorders has recently been shown to be a prominent research area, as exposed by several reviews focused on the field. Nevertheless, previous to the application of ML algorithms, EEG data should be correctly preprocessed and prepared via Feature Engineering (FE). In fact, the choice of FE techniques can make the difference between an unusable ML model and a simple, effective model. In other words, it can be said that FE is crucial, especially when using complex, non-stationary data such as EEG. To this aim, in this paper we present a Systematic Mapping Study (SMS) focused on FE from EEG data used to identify mental disorders. Our SMS covers more than 900 papers, making it one of the most comprehensive to date, to the best of our knowledge. We gathered the mental disorder addressed, all the FE techniques used, and the Artificial Intelligence (AI) algorithm applied for classification from each paper. Our main contributions are: (i) we offer a starting point for new researchers on these topics, (ii) we extract the most used FE techniques to classify mental disorders, (iii) we show several graphical distributions of all used techniques, and (iv) we provide critical conclusions for detecting mental disorders. To provide a better overview of existing techniques, the FE process is divided into three parts: (i) signal transformation, (ii) feature extraction, and (iii) feature selection. Moreover, we classify and analyze the distribution of existing papers according to the mental disorder they treat, the FE processes used, and the ML techniques applied. As a result, we provide a valuable reference for the scientific community to identify which techniques have been proven and tested and where the gaps are located in the current state of the art.},
  archive      = {J_APIN},
  author       = {García-Ponsoda, Sandra and García-Carrasco, Jorge and Teruel, Miguel A. and Maté, Alejandro and Trujillo, Juan},
  doi          = {10.1007/s10489-023-04702-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23203-23243},
  shortjournal = {Appl. Intell.},
  title        = {Feature engineering of EEG applied to mental disorders: A systematic mapping study},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating short-term stochastic production planning
updating with mining fleet management in industrial mining complexes: An
actor-critic reinforcement learning approach. <em>APIN</em>,
<em>53</em>(20), 23179–23202. (<a
href="https://doi.org/10.1007/s10489-023-04774-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term production planning in industrial mining complexes involves defining daily, weekly or monthly decisions that aim to achieve production targets established by long-term planning. Operational requirements must be considered when defining fleet allocation and production scheduling decisions. Thus, this paper presents an actor-critic reinforcement learning (RL) method to make mining equipment allocation and production scheduling decisions that maximize the profitability of a mining operation. Two RL agents are proposed. The first agent allocates shovels to mining fronts by considering some operational requirements. The second agent defines the processing destination and the number of trucks required for transportation. A simulator of mining complex operations is proposed to forecast the material flow from the mining fronts to the destinations. This simulator provides new states and rewards to the RL agents, so shovel allocation and production scheduling decisions can be improved. Additionally, as the mining complex operates, sensors collect ore quality data, which are used to update the uncertainty associated with the orebody models. The improvement in material supply characterization allows the RL agents to make more informed decisions. A case study applied at a copper mining complex highlights the method’s ability to make informed decisions while collecting new data. The results show a 47% improvement in cash flow by adapting the shovel and truck allocation and material destination compared to a base case with predefined fleet assignments.},
  archive      = {J_APIN},
  author       = {de Carvalho, Joao Pedro and Dimitrakopoulos, Roussos},
  doi          = {10.1007/s10489-023-04774-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23179-23202},
  shortjournal = {Appl. Intell.},
  title        = {Integrating short-term stochastic production planning updating with mining fleet management in industrial mining complexes: An actor-critic reinforcement learning approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic temporal position observant graph neural network for
traffic forecasting. <em>APIN</em>, <em>53</em>(20), 23166–23178. (<a
href="https://doi.org/10.1007/s10489-023-04737-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal forecasting has several applications in neurology, climate, and transportation. One classic example of such a learning assignment is traffic forecasting. The task is complex because of traffic’s non-linearity and dynamic nature with shifting road conditions, complex geographic dependence, and the inherent challenges of long-term forecasting. We propose the Dynamic Temporal Position Observant Graph Neural Network (DTPO-GNN). DTPO-GNN considers positional awareness, spatial and temporal reliance on traffic flow for forecasting future traffic speeds. Specifically, DTPO-GNN employs positional awareness via reference nodes to collect global knowledge of graph structure and diffusion convolution employing random walks to gather local information about the structural properties of traffic networks. A controlled sample encoder-decoder architecture and two-way random walks across the graph are used to capture structural and temporal dependency. Combining three large-scale road network traffic statistics from the actual world, we observe a consistent improvement of 11-13 % above baselines from the most recent research.},
  archive      = {J_APIN},
  author       = {Waikhom, Lilapati and Patgiri, Ripon and Singh, Laiphrakpam Dolendro},
  doi          = {10.1007/s10489-023-04737-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23166-23178},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic temporal position observant graph neural network for traffic forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGU-GNN: Minimal gated unit based graph neural network for
session-based recommendation. <em>APIN</em>, <em>53</em>(20),
23147–23165. (<a
href="https://doi.org/10.1007/s10489-023-04679-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommender systems (SBRS) play a crucial role in predicting the next click of a user from anonymous session data on numerous online platforms such as e-commerce, music, etc. However, predicting the next click is a very challenging task within the session, as it contains a very little amount of contextual information. Most of the existing techniques consider a session like a sequence of items to make recommendations and ignore the complex transition between items. In order to get accurate item embeddings and capture the complex transitions of items, we have proposed a Minimal Gated Unit based Graph Neural Network (MGU-GNN) for the session-based recommendation (SBR) tasks. We have also integrated a soft-attention network and target-based interest-aware network module, called MGU-GNN-TAR. The target-based interest-aware network module adapts to varying users’ interests in terms of the items to be targeted. The soft-attention network module adapts long-term priorities and current session interest for better prediction of the user’s next item or action. This model provides precise item embedding by incorporating the complex item transitions. The proposed model uses a gated mechanism called the Minimal Gated Unit, which has a single gate, and due to this reason, the parameters have been reduced to $$67\%$$ as compared to the GRU cell. A GRU cell is the most basic of all gated hidden units. To demonstrate the efficacy of the proposed models, comprehensive experiments on four most commonly used publicly available real-world datasets have been performed, and they show that the proposed models routinely beat baseline methods and state-of-the-art SBR techniques on all four datasets.},
  archive      = {J_APIN},
  author       = {Kumar, Chhotelal and Abuzar, Md and Kumar, Mukesh},
  doi          = {10.1007/s10489-023-04679-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23147-23165},
  shortjournal = {Appl. Intell.},
  title        = {MGU-GNN: Minimal gated unit based graph neural network for session-based recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Generalized nonconvex regularization for tensor RPCA and
its applications in visual inpainting. <em>APIN</em>, <em>53</em>(20),
23124–23146. (<a
href="https://doi.org/10.1007/s10489-023-04744-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a demonstrated and foremost approach of extracting the key features from corrupted observations, tensor robust principal component analysis has been considered in various fields of data processing related to tensors. It is usually modeled as a low-rank and sparse tensor decomposition problem and can be solved by minimizing a simple convex program. However, convex optimization methods often fail to deeply explore the rank and sparsity of tensors, which leads to suboptimality. Based on tensor singular value decomposition, in this work, we introduce generalized nonconvex regularizers accommodating most popular nonconvex (and possibly nonsmooth) surrogate functions to be used as effective approximations of the tensor rank function and $$\ell _{0}$$ -norm. The established unified frame equips universality for a large group of nonconvex surrogate functions. Moreover, we consider tube-wise sparse noise in addition to entry-wise sparse noise, which provides a better way of handling structured corrupted observations arising from practical issues. We further develop an efficient algorithm with convergence guarantees to implement generalized nonconvex optimization based upon the alternating direction method of multipliers. The satisfactory performance results of the proposed method are verified by simulations and visual inpainting applications.},
  archive      = {J_APIN},
  author       = {Zhang, Feng and Wang, Hailin and Qin, Wenjin and Zhao, Xile and Wang, Jianjun},
  doi          = {10.1007/s10489-023-04744-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23124-23146},
  shortjournal = {Appl. Intell.},
  title        = {Generalized nonconvex regularization for tensor RPCA and its applications in visual inpainting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEPAKE: A structure-enhanced and position-aware knowledge
embedding framework for knowledge graph completion. <em>APIN</em>,
<em>53</em>(20), 23113–23123. (<a
href="https://doi.org/10.1007/s10489-023-04723-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) provide supportively structured knowledge and have been applied to various downstream applications. Given a large amount of incomplete knowledge in KGs, knowledge graph completion (KGC) is proposed to reason over known facts and infer the missing links. The previous graph embedding approaches learn graph structure (i.e., triple structure/neighborhood structure) but cannot handle unseen entities, which is addressed by textual encoding approaches that utilize the textual knowledge of graph elements (i.e., entities/relations). However, the previous textual encoding approaches only resort to triples and thus cannot exploit the knowledge of neighbors, which provides abundant evidence to facilitate prediction. Moreover, they are insensitive to changes in the position of elements in triples when performing text modeling, and thus cannot effectively distinguish triples with the same elements but completely different semantics, which is detrimental to the final result. To address the above challenges, we propose a novel Structure-Enhanced and Position-Aware Knowledge Embedding (SEPAKE) framework. Specifically, masked elements reconstruction is devised to predict missing elements by reasoning over the contexts of subgraphs. As such, we incorporate the graph structure while maintaining the feature that textual information can be encoded. Meanwhile, position-aware learning is conducted to capture the semantic knowledge implied by the relative positions of elements in textualization. In addition, we employ task-specific adapters to store knowledge in a unified way to facilitate the storage and transfer of knowledge. Extensive experiments demonstrate the effectiveness of our framework, and we achieve state-of-the-art performance on standard datasets compared with textual encoding approaches. Besides, our proposed framework can efficiently improve the previous approaches by optionally pluggable adapters, further verifying the advancement and applicability of our work.},
  archive      = {J_APIN},
  author       = {Yu, Mei and Jiang, Tingxu and Yu, Jian and Zhao, Mankun and Guo, Jiujiang and Yang, Ming and Yu, Ruiguo and Li, Xuewei},
  doi          = {10.1007/s10489-023-04723-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23113-23123},
  shortjournal = {Appl. Intell.},
  title        = {SEPAKE: A structure-enhanced and position-aware knowledge embedding framework for knowledge graph completion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intention-aware denoising graph neural network for
session-based recommendation. <em>APIN</em>, <em>53</em>(20),
23097–23112. (<a
href="https://doi.org/10.1007/s10489-023-04736-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation anticipates the next potential interest of users based on their previous anonymous interactions, which is a crucial and indispensable component of many online services. Recently, deep learning methods have attracted extensive attention for session-based recommendations due to their outstanding abilities in capturing user preferences. However, most existing methods fail to model the latent user intentions reflected by correlated items and ignore the noisy signals that exist in session sequences. To address these issues, we present a novel Intention-aware Denoising Graph Neural Network (ID-GNN) for session-based recommendation. Specifically, we propose an item graph construction module to explore the correlation of items in session sequences. Furthermore, we aggregate information on the constructed graph and employ an intention extraction matrix to capture the latent user intentions reflected by correlated items. Additionally, we introduce a relative sorting approach and a denoising threshold to adaptively filter out noisy user intentions. Experimental results on two e-commerce datasets demonstrate that ID-GNN outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Hua, Shanshan and Gan, Mingxin},
  doi          = {10.1007/s10489-023-04736-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23097-23112},
  shortjournal = {Appl. Intell.},
  title        = {Intention-aware denoising graph neural network for session-based recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel class-level weighted partial domain adaptation
network for defect detection. <em>APIN</em>, <em>53</em>(20),
23083–23096. (<a
href="https://doi.org/10.1007/s10489-023-04733-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised domain adaptation methods have been increasingly applied to address the domain shift problems in defect detection. However, the effectiveness of most existing methods is based on the identical category space in the source and target domains. A more practical scenario is when the target domain only contains a subset of source categories, i.e., partial domain adaptation, which has not been well-resolved. To this end, a novel class-level weighted partial domain adaptation network (CWPDAN) is proposed for defect detection. Specifically, a hybrid weighting mechanism is derived from a defect classifier and an auxiliary domain classifier. In this case, the weighting mechanism is injected into both the defect classifier and the fine-grained domain adaptation strategy. As such, the shared category space across domains can be aligned well and the outlier categories can be identified and filtered out to alleviate negative transfer. Comprehensive partial domain adaptation experiments verify that the proposed CWPDAN can achieve 95.07% and 98.27% average accuracy on a tire defect dataset and a benchmark dataset, respectively, outperforming other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Yulong and Wang, Yilin and Jiang, Zhiqiang and Zheng, Li and Chen, Jinshui and Lu, Jiangang},
  doi          = {10.1007/s10489-023-04733-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23083-23096},
  shortjournal = {Appl. Intell.},
  title        = {A novel class-level weighted partial domain adaptation network for defect detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph attention network-optimized dynamic monocular visual
odometry. <em>APIN</em>, <em>53</em>(20), 23067–23082. (<a
href="https://doi.org/10.1007/s10489-023-04687-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular Visual Odometry (VO) is often formulated as a sequential dynamics problem that relies on scene rigidity assumption. One of the main challenges is rejecting moving objects and estimating camera pose in dynamic environments. Existing methods either take the visual cues in the whole image equally or eliminate the fixed semantic categories by heuristics or attention mechanisms. However, they fail to tackle unknown dynamic objects which are not labeled in the training sets of the network. To solve these issues, we propose a novel framework, named graph attention network (GAT)-optimized dynamic monocular visual odometry (GDM-VO), to remove dynamic objects explicitly with semantic segmentation and multi-view geometry in this paper. Firstly, we employ a multi-task learning network to perform semantic segmentation and depth estimation. Then, we reject priori known and unknown objective moving objects through semantic information and multi-view geometry, respectively. Furthermore, to our best knowledge, we are the first to leverage GAT to capture long-range temporal dependencies from consecutive image sequences adaptively, while existing sequential modeling approaches need to select information manually. Extensive experiments on the KITTI and TUM datasets demonstrate the superior performance of GDM-VO overs existing state-of-the-art classical and learning-based monocular VO.},
  archive      = {J_APIN},
  author       = {Hongru, Zhao and Xiuquan, Qiao},
  doi          = {10.1007/s10489-023-04687-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23067-23082},
  shortjournal = {Appl. Intell.},
  title        = {Graph attention network-optimized dynamic monocular visual odometry},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multichannel location-aware interaction network for visual
classification. <em>APIN</em>, <em>53</em>(20), 23049–23066. (<a
href="https://doi.org/10.1007/s10489-023-04734-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification aims to identify images that belong to multiple subcategories within the same category. This is a challenging task as there are only subtle regional differences between subcategories. Most of the existing methods utilize neural networks to extract global image features and quickly lock local feature regions by adding various external attention mechanisms. This type of approach may ignore the details that are inherent in the feature map itself. This paper proposes an efficient global channel position-aware interaction method to solve this problem. Specifically, we first hierarchically group the original features and take advantage of the translation-invariant linearity and local weight sharing of convolutional networks to propose a hierarchical structure that enhances the receptive field of global features. Then, same-direction location attention interaction is performed based on the global feature with rich fields of view, thus encouraging the model to capture its common areas of interest according to the feature’s own learning ability. Finally, multiple attention feature map is obtained based on the relative position interactions of the global features. We again use convolutional networks to learn the discriminative features of the attention target regions and perform feature clustering optimization on the discriminative feature regions to guide the classification process. The proposed model performs well on three datasets, i.e. CUB-200-2011, Stanford Cars, and FGVC Aircraft.},
  archive      = {J_APIN},
  author       = {Zhu, Qiangxi and Li, Zhixin and Kuang, Wenlan and Ma, Huifang},
  doi          = {10.1007/s10489-023-04734-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23049-23066},
  shortjournal = {Appl. Intell.},
  title        = {A multichannel location-aware interaction network for visual classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Space or time for video classification transformers.
<em>APIN</em>, <em>53</em>(20), 23039–23048. (<a
href="https://doi.org/10.1007/s10489-023-04756-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial and temporal attention plays an important role in video classification tasks. However, there are few studies about the mechanism of spatial and temporal attention behind classification problems. Transformer owns excellent capabilities at training scalability and capturing long-range dependencies among sequences because of its self-attention mechanism, which has achieved great success in many fields, especially in video classifications. The spatio-temporal attention is separated into a temporal attention module and a spatial attention module through Divided-Space-Time Attention, which makes it more conveniently to configure the attention module and adjust the way of attention interaction. Then single-stream models and two-stream models are designed to study the laws of information interaction between spatial attention and temporal attention with a lot of carefully designed experiments. Experiments show that the spatial attention is more critical than the temporal attention, thus the balanced strategy that is commonly used is not always the best choice. Furthermore, there is a necessity to consider the classical two-stream structure models in some cases, which can get better results than the popular single-stream structure models.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Tao, Chenjie and Zhang, Jian and Sun, Qun and Wang, Jianjia and Li, Weimin and Liu, Yue and Guo, Yike},
  doi          = {10.1007/s10489-023-04756-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23039-23048},
  shortjournal = {Appl. Intell.},
  title        = {Space or time for video classification transformers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MuseFlow: Music accompaniment generation based on flow.
<em>APIN</em>, <em>53</em>(20), 23029–23038. (<a
href="https://doi.org/10.1007/s10489-023-04664-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arranging and orchestration are critical aspects of music composition and production. Traditional accompaniment arranging is time-consuming and requires expertise in music theory. In this work, we utilize a deep learning model, the flow model, to generate music accompaniment including drums, guitar, bass, and strings based on the input piano melody, which can assist musicians in creating popular music. The main contributions of this paper are as follows: 1) We propose a new pianoroll representation that solves the problem of recognizing the onset of a musical note and saves space. 2) We introduce the MuseFlow accompaniment generation model, which can generate multi-track, polyphonic music accompaniment. To the best of our knowledge, MuseFlow is the first flow-based music generation model. 3) We incorporate a sliding window into the model to enable long sequence music generation, breaking the length limitation. Comparisons on datasets such as LDP, FreeMidi, and GPMD verify the effectiveness of the model. MuseFlow produces better results in accompaniment quality and inter-track harmony. Additionally, the note pitch and duration distributions of the generated accompaniment are much closer to the real data.},
  archive      = {J_APIN},
  author       = {Ding, Fanyu and Cui, Yidong},
  doi          = {10.1007/s10489-023-04664-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {20},
  pages        = {23029-23038},
  shortjournal = {Appl. Intell.},
  title        = {MuseFlow: Music accompaniment generation based on flow},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust model estimation by using preference analysis and
information theory principles. <em>APIN</em>, <em>53</em>(19),
22363–22373. (<a
href="https://doi.org/10.1007/s10489-023-04697-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust model estimation aims to estimate the parameters of a given geometric model, and then separate the outliers and inliers belonging to different model instances into different groups based on the estimated parameters. Robust model estimation is a fundamental task in computer vision and artificial intelligence, and mainly contains two components: data sampling for generating hypotheses and model selection for segmenting data. Over the past decade, a number of guided data sampling algorithms and model selection algorithms have been proposed separately. This results in that the performance of the robust model estimation method is still unsatisfactory. In this paper, we first present a comprehensive study of the above algorithms, by analyzing and comparing them. Then, we propose an efficient and effective robust model estimation method by using preference analysis and information theory principles. Specifically, we first employ our previously proposed data sampling algorithm based on preference analysis to sample data subsets for generating promising hypotheses. Then, we build a discriminative sparse affinity matrix based on the generated hypotheses by using information theory principles. Finally, we segment data by conducting a spectral clustering on the discriminative affinity matrix. Experimental results on the AdelaideRMF and the Hopkins 155 datasets show that the proposed method achieves higher segmentation accuracies than several state-of-the-art model estimation methods.},
  archive      = {J_APIN},
  author       = {Lai, Taotao and Wang, Weice and Liu, Yizhang and Li, Zuoyong and Lin, Shuyuan},
  doi          = {10.1007/s10489-023-04697-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22363-22373},
  shortjournal = {Appl. Intell.},
  title        = {Robust model estimation by using preference analysis and information theory principles},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized label relaxation-based stacked autoencoder for
zero-shot learning. <em>APIN</em>, <em>53</em>(19), 22348–22362. (<a
href="https://doi.org/10.1007/s10489-023-04686-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Zero-Shot Learning (ZSL) has gained great attention due to its significant classification performance for novel unobserved classes. As seen and unseen classes are completely disjoint, the current ZSL methods inevitably suffer from the domain shift problem when transferring the knowledge between the observed and unseen classes. Additionally, most ZSL methods especially those targeting the semantic space may cause the hubness problem due to their use of nearest-neighbor classifiers in high-dimensional space. To tackle these issues, we propose a novel pathway termed Regularized Label Relaxation-based Stacked Autoencoder (RLRSA) to diminish the domain difference between seen and unseen classes by exploiting an effective label space, which has some notable advantages. First, the proposed method establishes the tight relations among the visual representation, semantic information and label space using via the stacked autoencoder, which is beneficial for avoiding the projection domain shift. Second, by incorporating a slack variable matrix into the label space, our RLRSA method has more freedom to fit the test samples whether they come from the observed or unseen classes, resulting in a very robust and discriminative projection. Third, we construct a manifold regularization based on a class compactness graph to further reduce the domain gap between the seen and unseen classes. Finally, the learned projection is utilized to predict the class label of the target sample, thus the hubness issue can be prevented. Extensive experiments conducted on benchmark datasets clearly show that our RLRSA method produces new state-of-the-art results under two standard ZSL settings. For example, the RLRSA obtains the highest average accuracy of 67.82% on five benchmark datasets under the pure ZSL setting. For the generalized ZSL task, the proposed RLRSA is still highly effective, e.g., it achieves the best H result of 58.9% on the AwA2 dataset.},
  archive      = {J_APIN},
  author       = {Song, Jianqiang and Zhao, Heng and Wei, Xing and Zhang, Xiutai and Yao, Haiyan},
  doi          = {10.1007/s10489-023-04686-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22348-22362},
  shortjournal = {Appl. Intell.},
  title        = {Regularized label relaxation-based stacked autoencoder for zero-shot learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusing heterogeneous information for multi-modal attributed
network embedding. <em>APIN</em>, <em>53</em>(19), 22328–22347. (<a
href="https://doi.org/10.1007/s10489-023-04675-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, networks with many types of nodes and edges are complex, forming a heterogeneous network. For instance, film networks contain different node types, such as directors, films and actors, as well as different types of edge and multimodal attributes. Most existing attribution network embedding algorithms cannot flexibly capture the impact of multimodal attributes on the topology. Premature fusion of multimodal features encodes different attribute information into the representation embedding, while the later fusion strategy ignores the interaction between different modes, both of which affect the modeling of graph embedding.To solve this problem, we propose a multimodal attribute network representation learning algorithm based on heterogeneity information fusion, named FHIANE. It extracts features from multimodal information sources through deep heterogeneous convolutional networks and projects them into a consistent semantic space while maintaining structural information. In addition, we design a modality fusion network based on an extended attention mechanism that takes full advantage of the consistency and complementarity of multimodal information. We evaluate the performance of the FHIANE algorithm on several real datasets through challenging tasks such as link prediction and node classification. The experimental results show that FHIANE outperforms other baselines.},
  archive      = {J_APIN},
  author       = {Jieyi, Yang and Feng, Zhu and Yihong, Dong and Jiangbo, Qian},
  doi          = {10.1007/s10489-023-04675-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22328-22347},
  shortjournal = {Appl. Intell.},
  title        = {Fusing heterogeneous information for multi-modal attributed network embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TBNF: A transformer-based noise filtering method for chinese
long-form text matching. <em>APIN</em>, <em>53</em>(19), 22313–22327.
(<a href="https://doi.org/10.1007/s10489-023-04607-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep matching, a large amount of noisy data in Chinese long texts affects the matching effect. Most long-form text matching models use all text data indiscriminately, which results in a large amount of noisy data, and thus the PageRank algorithm is combined with Transformer to filter noise. For sentence-level noise detection, after calculating the overlap rate of words to evaluate the similarity, a sentence-level relationship graph is constructed and filtered by using the PageRank algorithm; for word-level noise detection, based on the attention score in Transformer, a word graph is established, then the PageRank algorithm is executed on graph, combined with self-attention weights, to select keywords to highlight topic relevance, the noisy words are filtered sequentially at different layers in the module, layer by layer. In addition, during the model training, PolyLoss is applied to replace the traditional binary Cross-Entropy loss function, thus reducing the difficulty of hyperparameter tuning. Finally, a better filtering strategy is proposed and experiments are conducted to verify it on two Chinese long-form text matching datasets. The result shows that the matching model based on the noise filtering strategy of this paper can better filter the noise and capture the matching signal more accurately.},
  archive      = {J_APIN},
  author       = {Gan, Ling and Hu, Liuhui and Tan, Xiaodong and Du, Xinrui},
  doi          = {10.1007/s10489-023-04607-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22313-22327},
  shortjournal = {Appl. Intell.},
  title        = {TBNF: A transformer-based noise filtering method for chinese long-form text matching},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting log anomaly using subword attention encoder and
probabilistic feature selection. <em>APIN</em>, <em>53</em>(19),
22297–22312. (<a
href="https://doi.org/10.1007/s10489-023-04674-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log anomaly is a manifestation of a software system error or security threat. Detecting such unusual behaviours across logs in real-time is the driving force behind large-scale autonomous monitoring technology that can rapidly alert zero-day attacks. Increasingly, AI methods are being used to process voluminous log datasets and reveal patterns of correlated anomaly. In this paper, we propose an enhanced approach to learning semantic-aware embeddings for logs called the Subword Encoder Neural network (SEN). Solving upon a key limitation of previous semantic log parsing works, the proposed work introduces the concept of learning word vectors from subword-level granularity using an attention encoder strategy. The learnt embeddings reflect the contextual/lexical relationships at the word level. As a result, the learnt word representations precisely capture new log messages previously not seen by the model. Furthermore, we develop a novel feature distillation algorithm termed Naive Bayes Feature Selector (NBFS) to extract useful log events. This probabilistic technique examines the occurrence pattern of events to only select the salient ones that can aid anomaly detection. To our best knowledge, this is the first attempt to associate affinity to log events based on the target task. Since the predictions can be traced to the log messages, the AI is inherently explainable too. The model outperforms state-of-the-art methods by a fair margin. It achieves a 0.99 detection F1-score on the benchmarked BGL, HDFS and OpenStack log datasets.},
  archive      = {J_APIN},
  author       = {Hariharan, M. and Mishra, Abhinesh and Ravi, Sriram and Sharma, Ankita and Tanwar, Anshul and Sundaresan, Krishna and Ganesan, Prasanna and Karthik, R.},
  doi          = {10.1007/s10489-023-04674-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22297-22312},
  shortjournal = {Appl. Intell.},
  title        = {Detecting log anomaly using subword attention encoder and probabilistic feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Laplacian regularized deep low-rank subspace clustering
network. <em>APIN</em>, <em>53</em>(19), 22282–22296. (<a
href="https://doi.org/10.1007/s10489-023-04668-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-expression-based deep subspace clustering, integrating traditional subspace clustering methods into deep learning paradigm to enhance the representative capacity, has become an important branch in unsupervised learning methods. However, most existing methods investigate to impose only sparse constraint on the coefficient matrix to sparsely and independently represent all data, yet omitting another essential global low-rank prior. Meanwhile, some non-linear geometric structures within data has not been well utilized for deep subspace clustering. To conquer these challenges, this paper proposes a novel deep subspace clustering method, named Laplacian Regularized Deep Low-Rank Subspace Clustering Network (LDLRSC), in which the low-rank prior and non-linear geometric information in data are captured simultaneously. Specifically, LDLRSC utilizes the nonconvex surrogate instead of sparsity to describe the global low-rankness of the self-representation matrix. Moreover, two types of Laplacian constraints are exploited to mine the geometric structure of the data samples. Extensive experiments on the several widely-used datasets have demonstrated the effectiveness of the proposed LDLRSC over existing state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Chen, Yongyong and Cheng, Lei and Hua, Zhongyun and Yi, Shuang},
  doi          = {10.1007/s10489-023-04668-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22282-22296},
  shortjournal = {Appl. Intell.},
  title        = {Laplacian regularized deep low-rank subspace clustering network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic matrix equation solution method based on NCBC-ZNN
and its application on hyperspectral image multi-target detection.
<em>APIN</em>, <em>53</em>(19), 22267–22281. (<a
href="https://doi.org/10.1007/s10489-023-04724-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For effectively solving dynamic matrix equation (DME) problems, a non-convex and bound-constrained zeroing neural network (NCBC-ZNN) model is designed to relax the convex constraint on the demand of activation function and achieve robustness against various noises. Moreover, rigorous mathematical analyses and proofs are presented under the circumstances of noise-free and noise-perturbed to theoretically investigate the global convergence and robustness of the proposed NCBC-ZNN model. In addition, simulative experiments are further provided to substantiate the superior performance of the proposed model in solving the DME problem compared with the existing state-of-the-art models. Finally, the proposed model combines the constrained energy minimization (CEM) algorithm to solve the hyperspectral image multi-target detection (HIMTD) problem. Compared with existing schemes, our method is competitive in terms of accuracy.},
  archive      = {J_APIN},
  author       = {He, Huiting and Jiang, Chengze and Xiao, Xiuchun and Wang, Guancheng},
  doi          = {10.1007/s10489-023-04724-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22267-22281},
  shortjournal = {Appl. Intell.},
  title        = {A dynamic matrix equation solution method based on NCBC-ZNN and its application on hyperspectral image multi-target detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated market maker inventory management with deep
reinforcement learning. <em>APIN</em>, <em>53</em>(19), 22249–22266. (<a
href="https://doi.org/10.1007/s10489-023-04647-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock markets are the result of the interaction of multiple participants, and market makers are one of them. Their main goal is to provide liquidity and market depth to the stock market by streaming bids and offers at both sides of the order book, at different price levels. This activity allows the rest of the participants to have more available prices to buy or sell stocks. In the last years, reinforcement learning market maker agents have been able to be profitable. But profit is not the only measure to evaluate the quality of a market maker. Inventory management arises as a risk source that must be under control. In this paper, we focus on inventory risk management designing an adaptive reward function able to control inventory depending on designer preferences. To achieve this, we introduce two control coefficients, AIIF (Alpha Inventory Impact Factor) and DITF (Dynamic Inventory Threshold Factor), which modulate dynamically the behavior of the market maker agent according to its evolving liquidity with good results. In addition, we analyze the impact of these factors in the trading operative, detailing the underlying strategies performed by these intelligent agents in terms of operative, profitability and inventory management. Last, we present a comparison with other existing reward functions to illustrate the robustness of our approach.},
  archive      = {J_APIN},
  author       = {Vicente, Óscar Fernández and Fernández, Fernando and García, Javier},
  doi          = {10.1007/s10489-023-04647-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22249-22266},
  shortjournal = {Appl. Intell.},
  title        = {Automated market maker inventory management with deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IMSFNet: Integrated multi-source feature network for salient
object detection. <em>APIN</em>, <em>53</em>(19), 22228–22248. (<a
href="https://doi.org/10.1007/s10489-023-04636-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale context features are conducive to image understanding, so it plays an important role in salient object detection (SOD) tasks, and contextual information-based SOD methods have achieved fine performance. However, the context information obtained through parallel independent convolutions with large kernels or dilated convolutions with different dilation rates is lack relevance and dependence at different scales, which limits the expressive ability of context information. In this article, we propose a novel Integrated Multi-Source Feature Network (IMSFNet) for accurate SOD task, which mainly consists of three components. Specifically, we first develop a multi-scale feature aggregation module (MSFAM) to adequately capture and utilize multi-scale context features through a series of well-designed dilated convolutions and short hierarchical connections, and then aggregate these information to improve the performance of input initial features. Subsequently, based on the extracted high-level features, we introduce a global feature extractor (GFE) to further excavate higher-level global semantic information to help locate salient objects from cluttered real-world scenes. Finally, a correlation feature interaction module (CFIM) is designed to interact the diverse information from different level features, reducing the interference of complex backgrounds and highlighting salient objects. Extensive experimental results on six public SOD benchmark datasets convincingly demonstrate the effectiveness and superiority of the proposed IMSFNet method against the 18 state-of-the-art SOD methods under different evaluation metrics.},
  archive      = {J_APIN},
  author       = {Xia, Chenxing and Sun, Yanguang and Fang, Xianjin and Ge, Bin and Gao, Xiuju and Li, Kuan-Ching},
  doi          = {10.1007/s10489-023-04636-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22228-22248},
  shortjournal = {Appl. Intell.},
  title        = {IMSFNet: Integrated multi-source feature network for salient object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An autonomous lightweight model for aerial scene
classification under labeled sample scarcity. <em>APIN</em>,
<em>53</em>(19), 22216–22227. (<a
href="https://doi.org/10.1007/s10489-023-04694-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial scene classification using convolutional neural network (CNN) has gained substantial research interest during last few years. The performance of these deep models is found to improve more when the spatial relationships among the scene features are explicitly modelled using capsule network (CapsNet). However, the combined CNN and CapsNet-based scene classifiers are not only computationally intensive but also often suffer from over-parameterization, leading to remarkable performance deterioration under scarcity of labeled samples. In order to address this issue, we propose a lightweight as well as autonomous CapsNet model which auto-prunes the unnecessary weights/parameters during network learning/training phase, and eventually reduces the computational cost. The efficacy of our lightweight autonomous CapsNet is evaluated after embedding this into PReLim, a recently developed paradigm for remote sensing scene classification under observed sample limitation. Experiments on three benchmark datasets show that our proposed lightweight PReLim (LW-PReLim) is able to attain state-of-the-art accuracy even with 25% less CapsNet parameter count.},
  archive      = {J_APIN},
  author       = {Dutta, Suparna and Das, Monidipa},
  doi          = {10.1007/s10489-023-04694-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22216-22227},
  shortjournal = {Appl. Intell.},
  title        = {An autonomous lightweight model for aerial scene classification under labeled sample scarcity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). HGIM: Influence maximization in diffusion cascades from the
perspective of heterogeneous graph. <em>APIN</em>, <em>53</em>(19),
22200–22215. (<a
href="https://doi.org/10.1007/s10489-023-04711-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving the problem of influence maximization (IM) in social networks accompanied by diffusion cascades, existing related methods face some problems, such as relying only on diffusion cascades while ignoring network topology, inaccurate estimation of node influence, etc. To tackle these problems, a novel method named HGIM is proposed from the perspective of heterogeneous graph. HGIM is consisted of a learning node representation process and a selecting influential nodes process. A heterogeneous propagation graph is first constructed to integrate network topology and diffusion cascades, where topological edges come from the social network, and action edges come from diffusion cascades. Moreover, an influence learning framework is constructed to learn node representation and estimate node influence. To select influential nodes, two seed selection strategies are proposed based on the estimated node influence. To evaluate the proposed method, a series of experiments are carried out on four real datasets. Experimental results confirm that the proposed method outperforms other state-of-the-art methods in solving the IM problem accompanied by diffusion cascades.},
  archive      = {J_APIN},
  author       = {Wang, Ying and Zheng, Yunan and Liu, Yiguang},
  doi          = {10.1007/s10489-023-04711-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22200-22215},
  shortjournal = {Appl. Intell.},
  title        = {HGIM: Influence maximization in diffusion cascades from the perspective of heterogeneous graph},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MPF-FS: A multi-population framework based on
multi-objective optimization algorithms for feature selection.
<em>APIN</em>, <em>53</em>(19), 22179–22199. (<a
href="https://doi.org/10.1007/s10489-023-04696-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection algorithms based on evolutionary computation have continued to emerge, and most of them have achieved outstanding results. However, there are two drawbacks when facing high-dimensional datasets: firstly, it is difficult to reduce features effectively, and secondly, the “curse of dimensionality”. To alleviate those problems, we take the initial population generation as an entry point and propose a variant initial population generator, which can improve diversity and initialize populations randomly throughout the solution space. However, during the experimental process, it was found that the improved diversity would cause the algorithm to converge too fast and thus lead to premature. Therefore, we introduced multi-population techniques to balance diversity and convergence speed, and finally formed the MPF-FS framework. To prove the effectiveness of this framework, two feature selection algorithms, multi-population multi-objective artificial bee colony algorithm and multi-population non-dominated sorting genetic algorithm II, are implemented based on this framework. Nine well-known public datasets were used in this study, and the results reveal that the two proposed multi-population methods on high-dimensional datasets can reduce more features without reducing (or even improving) classification accuracy, which outperforms the corresponding single-population algorithms. Further compared to the state-of-the-art methods, our method still shows promising results.},
  archive      = {J_APIN},
  author       = {Yang, Jie and He, Junjiang and Li, Wenshan and Li, Tao and Lan, Xiaolong and Wang, Yunpeng},
  doi          = {10.1007/s10489-023-04696-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22179-22199},
  shortjournal = {Appl. Intell.},
  title        = {MPF-FS: A multi-population framework based on multi-objective optimization algorithms for feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic maintenance of updating rough approximations in
interval-valued ordered decision systems. <em>APIN</em>,
<em>53</em>(19), 22161–22178. (<a
href="https://doi.org/10.1007/s10489-023-04655-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the new era of information society, dynamic data is common and widely applied in many fields. To save the computing time of upper and lower approximations in rough methods, it is wise to study the incremental methods of calculating approximations and construct the incremental algorithms. In this study, we mainly focus on maintaining approximations dynamically in interval-valued ordered decision systems when the feature set and sample set increase or decrease, respectively. Firstly, the dominance relation on interval-valued ordered decision system are discussed. The two notions of interval dominance degree and interval overlap degree (denoted as IDD and IOD respectively) are introduced to describe the preference relation between interval values. Then, the incremental updating rules of approximations for four circumstances, namely adding attributes, removing attributes, adding objects, and removing objects, are obtained based on the matrix expression of approximations and dominated sets. Furthermore, the incremental algorithms are derived accordingly. By using six preprocessed data sets from UCI repository, a series of evaluations and comparisons are made on the calculation time of static algorithm and incremental algorithms. From these comparative experiments, the effectiveness and superiority of the proposed dynamic algorithms could be verified.},
  archive      = {J_APIN},
  author       = {Zhou, Haoxiang and Li, Wentao and Zhang, Chao and Zhan, Tao},
  doi          = {10.1007/s10489-023-04655-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22161-22178},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic maintenance of updating rough approximations in interval-valued ordered decision systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Improving greedy local search methods by switching the
search space. <em>APIN</em>, <em>53</em>(19), 22143–22160. (<a
href="https://doi.org/10.1007/s10489-023-04693-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks play a vital role in human understanding of the world. Finding a precise equivalence class of a Bayesian network is an effective way to represent causality. However, as one of the most widely used methods of searching for equivalence classes, greedy equivalence search (GES), can easily fall into a local optimum. To address this problem, we explore the reasons why GES becomes stuck in a local optimum by analyzing its operators and search strategies in detail. Moreover, we demonstrate that converting the search space into another space can address the drawbacks of local search in the space of the equivalence class. Accordingly, two novel frameworks based on switching spaces are proposed to improve GES. Finally, the effectiveness, scalability, and stability of the proposed methods are verified by extensive experiments through which our frameworks are compared with state-of-the-art methods on different benchmarks. The results show that our methods significantly strengthen the performance of GES.},
  archive      = {J_APIN},
  author       = {Liu, Xiaohan and Gao, Xiaoguang and Ru, Xinxin and Tan, Xiangyuan and Wang, Zidong},
  doi          = {10.1007/s10489-023-04693-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22143-22160},
  shortjournal = {Appl. Intell.},
  title        = {Improving greedy local search methods by switching the search space},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph embedding and completion based on entity
community and local importance. <em>APIN</em>, <em>53</em>(19),
22132–22142. (<a
href="https://doi.org/10.1007/s10489-023-04698-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion can solve the common problems of missing and incomplete knowledge in the process of building knowledge graphs by predicting the missing entity and relationship information in the knowledge base. To the best of our knowledge, existing knowledge graph completion algorithms seldom consider the influence of entity communities, and no algorithm further considers the influence of local importance based on entity communities. In this paper, we propose a knowledge graph embedding model and completion method based on entity feature information. First, we use the community detection method to divide the knowledge graph into different entity communities, and calculate the local importance of the entity in the community. Next, we apply community information to obtain entities and relationships with low similarities to construct more appropriate negative triples. A new hybrid objective function that can simultaneously reflect the importance of entities and the structure of the knowledge graph is proposed to obtain high-quality entity and relationship embedding vectors to complete the knowledge graph. On the FreeBase and WordNet datasets, through comparison with six well-known knowledge graph completion methods, the experimental results show that our proposed algorithm has good completion performance.},
  archive      = {J_APIN},
  author       = {Yang, Xu-Hua and Ma, Gang-Feng and Jin, Xin and Long, Hai-Xia and Xiao, Jie and Ye, Lei},
  doi          = {10.1007/s10489-023-04698-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22132-22142},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge graph embedding and completion based on entity community and local importance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A feature aggregation network for multispectral pedestrian
detection. <em>APIN</em>, <em>53</em>(19), 22117–22131. (<a
href="https://doi.org/10.1007/s10489-023-04628-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is an important task in many computer vision applications. Since multispectral pedestrian detection can alleviate the difficulties of insufficient illumination at night, it has been rapidly developed in recent years. However, the way for effective color-thermal image fusion still needs further research. In this paper, we propose a Feature Aggregation Module (FAM) that can adaptively capture the cross-channel and cross-dimension information interaction of the two modalities. In addition, we develop a Feature Aggregation Network (FANet) that embeds the proposed FAM module into a two-stream network adapted from the YOLOv5. FANet has the advantages that its size is small (15 MB) and it runs fast (8 ms per frame). Extensive experiments on the KAIST dataset show that the proposed method is effective for multispectral pedestrian detection, especially in the night-time condition, for which the Miss Rate is only 8.91%. Moreover, we show that the saliency map computed from the thermal image can be incorporated into FANet to further improve the detection accuracy. In order to verify the generalization ability of the FAM module, we have also conducted experiments on the person re-identification datasets, namely Market1501 and Duke. The performance of our FAM compares favorably against existing feature fusion mechanisms on the two datasets.},
  archive      = {J_APIN},
  author       = {Gong, Yan and Wang, Lu and Xu, Lisheng},
  doi          = {10.1007/s10489-023-04628-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22117-22131},
  shortjournal = {Appl. Intell.},
  title        = {A feature aggregation network for multispectral pedestrian detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Whether and how is a surveillance camera jittering? A ROR
perception based framework and method. <em>APIN</em>, <em>53</em>(19),
22105–22116. (<a
href="https://doi.org/10.1007/s10489-023-04631-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the video surveillance, cameras often jitter slightly because of some uncertain factors like wind, loose of bracket, and so on, which may add more difficulties to subsequent intelligent video analysis. Then, it is great significant to detect whether and how is a camera jittering. In this paper, a concept, i.e. region of reference (ROR), is defined, and a framework and method of surveillance camera jitter detection is proposed through perceiving the location changes of the ROR. First, an extraction model of motion information is established to extract and eliminate the moving information such as moving objects, swaying leaves, etc. Second, global contrast and octree color quantization are employed to filter out the significant regions in the background, then, the most representative regions are chosen as the RORs by tracking the boundary of significant regions. Third, the RORs and their surrounding image features are extracted and input into the filter to construct the response map, then, the corresponding region of peak value in the response map is perceived for location change. Average accuracy of the proposed method on video jitter data set is 95.36%. The experimental results show that the proposed method can solve the problem of video jitter detection in complex scenes.},
  archive      = {J_APIN},
  author       = {Gao, Fei and Mei, Kaitao and Weng, Libo and Zhuang, Yaozhong},
  doi          = {10.1007/s10489-023-04631-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22105-22116},
  shortjournal = {Appl. Intell.},
  title        = {Whether and how is a surveillance camera jittering? a ROR perception based framework and method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto CNN classifier based on knowledge transferred from
self-supervised model. <em>APIN</em>, <em>53</em>(19), 22086–22104. (<a
href="https://doi.org/10.1007/s10489-023-04598-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training with unlabeled datasets using self-supervised models has the edge over training with labeled datasets, reducing human effort, and no need for annotated data during training. The knowledge obtained from the pre-trained self-supervised model can be transferred to an image classifier as a target task. Transferring knowledge from source to target task requires tuning a large number of hyperparameters, such as number of CNN layers, number of filters in each CNN layer, kernel size, stride, number of fully connected (FC) layers, units in each FC layer, dropouts, learning rate, and many more. An efficient process is required to automatically tune these hyperparameters to reduce the manual efforts and enhance the performance of the self-supervised model. This paper uses a pre-trained self-supervised model to transfer knowledge for image classification. We propose an efficient Bayesian optimization-based method for automatically tuning (autotuning) the hyperparameters of the self-supervised model during the knowledge transfer. The proposed autotuned image classifier consists of a few CNN layers followed by an FC layer. Finally, we use a softmax layer to obtain the probability of classes for the images. To evaluate the performance of the proposed method, state-of-the-art self-supervised models such as SimClr, SWAV, and SWAV 2x are used to extract the learned features. According to our experiments on three benchmarked datasets (CIFAR-10, CIFAR-100, and Tiny Imagenet datasets), the proposed method not only enhances the performances of the self-supervised models, but also provides state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Kishore, Jaydeep and Mukherjee, Snehasis},
  doi          = {10.1007/s10489-023-04598-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22086-22104},
  shortjournal = {Appl. Intell.},
  title        = {Auto CNN classifier based on knowledge transferred from self-supervised model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dunhuang murals contour generation network based on
convolution and self-attention fusion. <em>APIN</em>, <em>53</em>(19),
22073–22085. (<a
href="https://doi.org/10.1007/s10489-023-04614-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dunhuang murals are a collection of Chinese style and national style, forming an autonomous Chinese-style Buddhist art. It has a very great historical and cultural value and an important research value. Among them, the lines of Dunhuang murals are extremely general and expressive. It reflects the character’s distinct character and intricate inner emotions. Hence, the outline drawing of the murals is of great importance in search of the Dunhuang culture. The generation of contours of Dunhuang murals belongs to the detection of image contours, which is an important branch of computer vision, aimed at extract salient contour information from the images. Although convolution-based deep learning networks have achieved good results in extracting image contours by exploring the contextual and semantic features of images. However, as the receptive field broadens, some detailed local information is lost. As a result, it is impossible for them to generate reasonable outline drawings of murals. In this paper, we propose a novel edge detector based on self-attention combined with convolution to generate line drawings of Dunhuang murals. Compared with existing edge detection methods, firstly, a new residual self-attention and convolution mixed module(Ramix) is proposed to merge local and global features into feature maps. Secondly, a novel densely connected backbone extraction network is designed to efficiently propagate information rich in edge features from shallow layers into deep layers. It is apparent from the experimental results that the method of this paper can generate richer and sharper edge maps on several standard edge detection datasets. In addition, tests on the Dunhuang mural dataset show that our method can achieve very competitive performance.},
  archive      = {J_APIN},
  author       = {Liu, Baokai and He, Fengjie and Du, Shiqiang and Zhang, Kaiwu and Wang, Jianhua},
  doi          = {10.1007/s10489-023-04614-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22073-22085},
  shortjournal = {Appl. Intell.},
  title        = {Dunhuang murals contour generation network based on convolution and self-attention fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introducing a framework and a decision protocol to
calibrated recommender systems. <em>APIN</em>, <em>53</em>(19),
22044–22072. (<a
href="https://doi.org/10.1007/s10489-023-04681-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems use the user’s profile to generate a recommendation list with unknown items to a target user. Although the primary goal of traditional recommendation systems is to deliver the most relevant items, such an effort unintentionally can cause collateral effects, including low diversity and unbalanced genres or categories, benefiting particular groups of categories. This paper proposes an approach to create recommendation lists with a calibrated balance of genres, avoiding disproportion between the user’s profile interests and the recommendation list. The calibrated recommendations consider the relevance and the divergence between the genre distributions extracted from the user’s preference and the recommendation list. The main claim is that calibration can contribute positively to generating fairer recommendations. In particular, we propose a new trade-off equation, which considers the users’ bias to provide a recommendation list that seeks the users’ tendencies. Moreover, we propose a conceptual framework and a decision protocol to generate more than one thousand combinations of calibrated systems to find the best combination. We compare our approach against state-of-the-art approaches using multiple domain datasets and evaluate using rank and calibration metrics. The results indicate that the trade-off, which considers the users’ bias, produces positive effects on precision and fairness, thus generating recommendation lists that respect the genre distribution. For the decision protocol, we also found the best system for each dataset.},
  archive      = {J_APIN},
  author       = {da Silva, Diego Corrêa and Durão, Frederico Araújo},
  doi          = {10.1007/s10489-023-04681-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22044-22072},
  shortjournal = {Appl. Intell.},
  title        = {Introducing a framework and a decision protocol to calibrated recommender systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced edge convolution-based spatial-temporal network for
network traffic prediction. <em>APIN</em>, <em>53</em>(19), 22031–22043.
(<a href="https://doi.org/10.1007/s10489-023-04626-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting network traffic is helpful for improving a variety of spatial-temporal data mining applications, such as intelligent traffic control, network planning and anomaly detection. The mainstream graph-based methods are limited by the node-level message passing mechanism and require transforming dimensions to generate edge representations. Accordingly, some researchers propose using edge convolution to directly learn edge representations for network traffic prediction. However, node-level and edge-level information aggregation are two different perspectives, and their combination of them can achieve better performance. This paper proposes a novel model for network traffic prediction named the Enhanced Edge Convolution-based Spatial-Temporal Network (EESTN). Armed with a Graph Neural Network and Hypergraph Neural Network, EESTN employs the edge convolutions defined on the graph and hypergraph to effectively extract spatial features. EESTN further combines node convolution to capture the complex correlations among nodes and utilizes an attention mechanism to generate the edge convolution kernel for the decoder. Moreover, a 3D convolution-based multihead self-attention mechanism and a hierarchical loss function are proposed to capture the long-term temporal dependence and make full use of the model’s represent ability. Finally, we conduct extensive experiments to validate the effectiveness of EESTN, and the related results demonstrate that EESTN outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Hu, Zehua and Ruan, Ke and Yu, Weihao and Chen, Siyuan},
  doi          = {10.1007/s10489-023-04626-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22031-22043},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced edge convolution-based spatial-temporal network for network traffic prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep discriminative dictionary pair learning for image
classification. <em>APIN</em>, <em>53</em>(19), 22017–22030. (<a
href="https://doi.org/10.1007/s10489-023-04708-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative dictionary learning has been extensively used for pattern classification tasks. By incorporating different kinds of label information into the dictionary learning framework, a dictionary can be attained that represents the original signal with discriminative reconstruction. The previous works learn the dictionary in the original space which limits the dictionary learning performance. In this paper, we propose an approach, namely Deep Discriminative Dictionary Pair Learning (D $$^3$$ PL) for image classification. The input of D $$^3$$ PL is not the matrix collected by original gray images or hand-crafted features but the relatively deeper features derived from autoencoders. Then, a structured dictionary is designed based on the discriminative contributions across different classes to reconstruct the deep feature. In addition, the associated structured projective dictionary is learned as well to guarantee the decoders updating towards the minimal error of deconvolution operator. By leveraging the discriminative-dictionary-learning-based loss function and the autoencoder loss function, D $$^3$$ PL can simultaneously learn the deep potential feature and the corresponding dictionary pair. In the testing phase of D $$^3$$ PL, the minimum error between the deep feature and the structured projective component with regard to different classes can directly indicate the label by a basic matrix multiplication operation. Experimental results on challenging Extended Yale B, AR, UMIST, COIL20, Scene 15, and Caltech101 datasets demonstrate that the proposed D $$^3$$ PL outperforms the prominent dictionary learning methods.},
  archive      = {J_APIN},
  author       = {Zhu, Wenjie and Peng, Bo and Chen, Chunchun and Chen, Hao},
  doi          = {10.1007/s10489-023-04708-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22017-22030},
  shortjournal = {Appl. Intell.},
  title        = {Deep discriminative dictionary pair learning for image classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph convolutional dynamic recurrent network with attention
for traffic forecasting. <em>APIN</em>, <em>53</em>(19), 22002–22016.
(<a href="https://doi.org/10.1007/s10489-023-04621-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting is a typical spatio-temporal graph modeling problem, which has become one of the key technical issues in modern intelligent transportation systems. However, existing methods cannot capture the long-range spatial and temporal characteristics very well because of the complexity and heterogeneity of the traffic flows. In this paper, a new deep learning framework called Graph Convolutional Dynamic Recurrent Network with Attention (GCDRNA) is proposed to predict the traffic state in the traffic network. GCDRNA mainly consists of two components, which are Graph Convolutional with Attention (GCA) block and Dynamic GRU with Attention (DGRUA) block. GCA block can capture both global and local spatial correlations of the traffic flows by k-hop GC, similarity GC and spatial attention modules. DGRUA block captures the long-term temporal correlation of the traffic flows by Dynamic GRU (DGRU) and Node Attention Unit (NAU) modules. Experimental results show that GCDRNA achieves the best prediction performance compared with other baseline models on two public real-world traffic datasets.},
  archive      = {J_APIN},
  author       = {Wu, Jiagao and Fu, Junxia and Ji, Hongyan and Liu, Linfeng},
  doi          = {10.1007/s10489-023-04621-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {22002-22016},
  shortjournal = {Appl. Intell.},
  title        = {Graph convolutional dynamic recurrent network with attention for traffic forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boost two-view learning-based method for label proportions
problem. <em>APIN</em>, <em>53</em>(19), 21984–22001. (<a
href="https://doi.org/10.1007/s10489-023-04643-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we mainly research the problem of learning from label proportions (LLP), in which the training data is divided into several bags, and only the label proportions information of each class can be accessible. As it has drawn increasing attention recently, there are a great many approaches have been proposed to solve the problem of LLP, while most existing LLP methods only consider a single-view feature of the training data, in which the classifiers are more likely to obtain deficient performance and be interfered by noise. Thus, we proposed a novel approach to solve the problem of LLP based on two-view learning, term two-view learning from label proportions (TV-LLP). The proposed method can build a more satisfying classifier by combining two-view knowledge than traditional LLP methods with a single view. In the first place, we formulate the TV-LLP model to deal with two-view learning in the label proportions setting and solve it iteratively, in which we assign weight distribution to each classier for acquiring basic classifiers. Meanwhile, we introduce boosting method into the model to construct a strong classifier based on the basic classifiers, which can improve the accuracy of the model further. We then apply the proposed method to the datasets of text categorization and image categorization respectively, in which extensive experiments have shown that the proposed TV-LLP method outperforms existing approaches.},
  archive      = {J_APIN},
  author       = {Lai, Jiantao and Xiao, Yanshan and Liu, Bo},
  doi          = {10.1007/s10489-023-04643-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21984-22001},
  shortjournal = {Appl. Intell.},
  title        = {Boost two-view learning-based method for label proportions problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A double-weighted outlier detection algorithm considering
the neighborhood orientation distribution of data objects.
<em>APIN</em>, <em>53</em>(19), 21961–21983. (<a
href="https://doi.org/10.1007/s10489-023-04593-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is a hot research topic in data mining, and its requirements for algorithms to engage with various complex-shaped datasets more effectively are also increasing. This paper conducts in-depth research on the existing problems, which focuses on the low-density pattern and the local outliers detection of the outlier detection algorithms. In order to resolve these problems, We present a double-weighted outlier detection (DDW) algorithm considering the dense direction, which simultaneously considers the distance and orientation relationship of the neighborhood distribution. In DDW, we first propose a concept of dense direction, which moves the research object of the algorithm from a point to a region to explore the relationship between the data points and the distribution of their neighbors more comprehensively. Then, we design a new point weighting strategy by exploring the point distribution of the neighborhood indicated by the dense directions of different data points and design a new edge weighting strategy where we give the edge weights to the edges between data points and their neighbors to better represent the closeness of data points. After that, we design a new double-weighted method that further actualizes the complementary advantages of the point weighting strategy and the edge weighting strategy to solve the problem that the existing outlier detection algorithms cannot fully characterize the potential structural information inside the data. The final comprehensive experiment shows that our proposed method not only eliminates the defect that traditional outlier detection algorithms are sensitive to neighboring parameters but our proposal also has higher detection accuracy of local outlier detection than many current methods on both synthetic and UCI datasets.},
  archive      = {J_APIN},
  author       = {Gao, Qiang and Gao, Qin-Qin and Xiong, Zhong-Yang and Zhang, Yu-Fang and Wang, Yu-Qin and Zhang, Min},
  doi          = {10.1007/s10489-023-04593-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21961-21983},
  shortjournal = {Appl. Intell.},
  title        = {A double-weighted outlier detection algorithm considering the neighborhood orientation distribution of data objects},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). User-station attention inference using smart card data: A
knowledge graph assisted matrix decomposition model. <em>APIN</em>,
<em>53</em>(19), 21944–21960. (<a
href="https://doi.org/10.1007/s10489-023-04678-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human mobility in urban areas is important for transportation, from planning to operations and online control. This paper proposes the concept of user-station attention, which describes the user’s (or user group’s) interest in or dependency on specific stations. The concept contributes to a better understanding of human mobility (e.g., travel purposes) and facilitates downstream applications, such as individual mobility prediction and location recommendation. However, intrinsic unsupervised learning characteristics and untrustworthy observation data make it challenging to estimate the real user-station attention. We introduce the user-station attention inference problem using station visit counts data in public transport and develop a matrix decomposition method capturing simultaneously user similarity and station-station relationships using knowledge graphs. Specifically, it captures the user similarity information from the user-station visit counts matrix. It extracts the stations’ latent representation and hidden relations (activities) between stations to construct the mobility knowledge graph (MKG) from smart card data. We develop a neural network (NN)-based nonlinear decomposition approach to extract the MKG relations capturing the latent spatiotemporal travel dependencies. The case study uses both synthetic and real-world data to validate the proposed approach by comparing it with benchmark models. The results illustrate the significant value of the knowledge graph in contributing to the user-station attention inference. The model with MKG improves the estimation accuracy by 35% in MAE and 16% in RMSE. Also, the model is not sensitive to sparse data provided only positive observations are used.},
  archive      = {J_APIN},
  author       = {Zhang, Qi and Ma, Zhenliang and Zhang, Pengfei and Jenelius, Erik and Ma, Xiaolei and Wen, Yuanqiao},
  doi          = {10.1007/s10489-023-04678-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21944-21960},
  shortjournal = {Appl. Intell.},
  title        = {User-station attention inference using smart card data: A knowledge graph assisted matrix decomposition model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using alignment-free and pattern mining methods for
SARS-CoV-2 genome analysis. <em>APIN</em>, <em>53</em>(19), 21920–21943.
(<a href="https://doi.org/10.1007/s10489-023-04618-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Examining the genome sequences of the SARS-CoV-2 virus, that causes the respiratory disease known as coronavirus disease 2019 (COVID-19), play important role in the proper understanding of this virus, its main characteristics and functionalities. This paper investigates the use of alignment-free (AF) sequence analysis and sequential pattern mining (SPM) to analyze SARS-CoV-2 genome sequences and learn interesting information about them respectively. AF methods are used to find (dis)similarity in the genome sequences of SARS-CoV-2 by using various distance measures, to compare the performance of these measures and to construct the phylogenetic trees. SPM algorithms are used to discover frequent amino acid patterns and their relationship with each other and to predict the amino acid(s) by using various sequence-based prediction models. In last, an algorithm is proposed to analyze mutation in genome sequences. The algorithm finds the locations for changed amino acid(s) in the genome sequences and computes the mutation rate. From obtained results, it is found that that both AF and SPM methods can be used to discover interesting information/patterns in SARS-CoV-2 genome sequences for examining the variations and evolution among strains.},
  archive      = {J_APIN},
  author       = {Nawaz, M. Saqib and Fournier-Viger, Philippe and Aslam, Memoona and Li, Wenjin and He, Yulin and Niu, Xinzheng},
  doi          = {10.1007/s10489-023-04618-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21920-21943},
  shortjournal = {Appl. Intell.},
  title        = {Using alignment-free and pattern mining methods for SARS-CoV-2 genome analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). HetGNN-SF: Self-supervised learning on heterogeneous graph
neural network via semantic strength and feature similarity.
<em>APIN</em>, <em>53</em>(19), 21902–21919. (<a
href="https://doi.org/10.1007/s10489-023-04612-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks (HGNNs) can effectively model multiple node types and complex interactions in real networks and solve problems in various practical applications. Self-supervised Learning-based HGNNs (SL-HGNNs) have become the current research focus in this field because they can solve the problem of difficult label acquisition in practical scenarios. These methods usually split heterogeneous graphs into multiple subgraphs based on meta-paths for separate study. However, they ignore the complex interactions between the different semantics of the graphs. In addition, they use node features as auxiliary information for heterogeneous graph representation learning and ignore the importance of features. To solve the above problems, we propose a Self-supervised Learning on Heterogeneous Graph Neural Network via Semantic strength and Feature similarity (HetGNN-SF) model. This model innovatively implements a Feature- and Topology-based Comparative optimization (FTC) method to generate weights for different meta-paths and then splits the original heterogeneous graph into different semantic subgraphs based on meta-paths. Thereafter, the different subgraphs are fused by the FTC to generate semantic fusion graphs that capture the interactions between different semantics. Semantic strength and feature similarity perspectives generate node embeddings from the semantic fusion graphs. Finally, the FTC positive and negative samples are used for contrastive learning from the two perspectives to yield the final node embeddings. Extensive experiments are conducted on three real datasets using the proposed HetGNN-SF model; the results reveal that HetGNN-SF outperforms state-of-the-art models. Our data and code are available on GitHub ( https://github.com/LiuXMaa/HetGNN-SF-.git ).},
  archive      = {J_APIN},
  author       = {Li, Chao and Liu, Xinming and Yan, Yeyu and Zhao, Zhongying and Zeng, Qingtian},
  doi          = {10.1007/s10489-023-04612-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21902-21919},
  shortjournal = {Appl. Intell.},
  title        = {HetGNN-SF: Self-supervised learning on heterogeneous graph neural network via semantic strength and feature similarity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowd counting based on multi-level multi-scale feature.
<em>APIN</em>, <em>53</em>(19), 21891–21901. (<a
href="https://doi.org/10.1007/s10489-023-04641-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has drawn more and more attention for its significance in reality application. However, it’s still a challenging task because of scale variation in images. In this paper, we propose a model to extract and refine features with abundant scale-relevant information, which consists of Multi-layer Multi-scale Feature Extraction Network (MLMS) and Dependency-based Feature Fusion Network (DFF). MLMS plays a role as feature extractor. Three multi-scale feature extraction modules (MSFE) are designed with dilated convolution layers and inserted in different levels of MLMS, which improve the ability for multi-scale feature extraction. DFF plays a role as feature refiner. DFF explores the dependency between hierarchical features. It’s the first time in crowd counting to use Long-short term memory (LSTM) to filter information and fuse the features with the assistance of the dependency. Our model provides new ideas for solving scale-relevant problems from two angels: scale feature extraction and fusion. In this way, our model extracts scale-relevant features and refines the features further. Experiments on four challenging datasets ShanghaiTech Part A/B, UCF_QNRF and UCF_CC_50, getting Mean Absolute Error (MAE) 65.3/8.3/113.2/216.3, demonstrate the effectiveness of the proposed model.},
  archive      = {J_APIN},
  author       = {Wu, Di and Fan, Zheyi and Yi, Shuhan},
  doi          = {10.1007/s10489-023-04641-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21891-21901},
  shortjournal = {Appl. Intell.},
  title        = {Crowd counting based on multi-level multi-scale feature},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seismic image super-resolution reconstruction through deep
feature mining network. <em>APIN</em>, <em>53</em>(19), 21875–21890. (<a
href="https://doi.org/10.1007/s10489-023-04660-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to complex geological conditions and instrument inaccuracy, raw seismic data are often characterized by low resolution. Deep learning is an emerging technique for seismic resolution improvement; however, its performance is often limited by the small amount of labeled data available. In this paper, we design a deep feature mining network (DFMN) to deal with this issue. DFMN has three components: shallow feature extraction block (SFB), deep feature mining block (DFB), and enhanced reconstruction block (ERB). First, the SFB component uses multi-scale kernels to learn rich information from low-resolution data. The convolutions incorporate the benefits of different kernel sizes, which are effective for shallow feature extraction. Second, the DFB component employs a dual-branch network architecture for deep feature mining. The dual-branch network learns more complementary features than a single-branch network, thus alleviating the requirement for large amounts of training data. Third, the ERB component combines the shuffled image and the interpolated image during reconstruction. Interpolated images, incorporating prior knowledge, can provide more contextual information in our model. The results show that DFMN is superior to a traditional upscaling algorithm and other deep learning methods in terms of (1) perceptual effects: more complete structural information, such as texture details; (2) quantitative evaluation indices: higher PSNR and SSIM; and (3) generalization ability: better performance on other data.},
  archive      = {J_APIN},
  author       = {Zeng, Dou and Xu, Qiong and Pan, Shulin and Song, Guojie and Min, Fan},
  doi          = {10.1007/s10489-023-04660-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21875-21890},
  shortjournal = {Appl. Intell.},
  title        = {Seismic image super-resolution reconstruction through deep feature mining network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual deep q-learning network guiding a multiagent path
planning approach for virtual fire emergency scenarios. <em>APIN</em>,
<em>53</em>(19), 21858–21874. (<a
href="https://doi.org/10.1007/s10489-023-04601-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With continuous deterioration of the natural environment and the corresponding significant increase in the occurrence of disasters, forest fire accidents have frequently occurred in recent decades. Therefore, it is important to perform extensive effective fire drills to increase evacuation experience and emergency reaction capacity. In comparison to traditional fire drills, which are subject to many latent uncertainties and incur high costs, fire exercises based on virtual scenarios offer many advantages, such as low cost and high safety. Accordingly, the planning and design of effective evacuation paths that sufficiently match real conditions have become an imperative focus of related research. In this paper, we propose a novel framework for path planning in virtual emergency scenarios, which consists of three parts. (a) Configuration of the virtual environment: for convenience in handling, the virtual emergency scenario is discretized into many individual grid cells. (b) Policy generation: a dual deep Q-learning network approach is employed to obtain an effective policy that can allow agents to intelligently find effective paths. (c) Grouping strategy: a strategy is proposed to support multiple agents in achieving collective evacuation based on a given policy. Finally, extensive experiments are presented to validate the superiority of the proposed framework. The results show that by comparison with the existing related state-of-the-art methods, our proposed framework is superior and feasible.},
  archive      = {J_APIN},
  author       = {Zhou, Wen and Zhang, Chen and Chen, Siyuan},
  doi          = {10.1007/s10489-023-04601-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21858-21874},
  shortjournal = {Appl. Intell.},
  title        = {Dual deep Q-learning network guiding a multiagent path planning approach for virtual fire emergency scenarios},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate use of label dependency in multi-label text
classification through the lens of causality. <em>APIN</em>,
<em>53</em>(19), 21841–21857. (<a
href="https://doi.org/10.1007/s10489-023-04623-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Label Text Classifiction (MLTC) aims to assign the most relevant labels to each given text. Existing methods demonstrate that label dependency can help to improve the model’s performance. However, the introduction of label dependency may cause the model to suffer from unwanted prediction bias. In this study, we attribute the bias to the model’s misuse of label dependency, i.e., the model tends to utilize the correlation shortcut in label dependency rather than fusing text information and label dependency for prediction. Motivated by causal inference, we propose a CounterFactual Text Classifier (CFTC) to eliminate the correlation bias, and make causality-based predictions. Specifically, our CFTC first adopts the predict-then-modify backbone to extract precise label information embedded in label dependency, then blocks the correlation shortcut through the counterfactual de-bias technique with the help of the human causal graph. Experimental results on three datasets demonstrate that our CFTC significantly outperforms the baselines and effectively eliminates the correlation bias in datasets.},
  archive      = {J_APIN},
  author       = {Fan, Caoyun and Chen, Wenqing and Tian, Jidong and Li, Yitian and He, Hao and Jin, Yaohui},
  doi          = {10.1007/s10489-023-04623-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21841-21857},
  shortjournal = {Appl. Intell.},
  title        = {Accurate use of label dependency in multi-label text classification through the lens of causality},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DaCo: Domain-agnostic contrastive learning for visual place
recognition. <em>APIN</em>, <em>53</em>(19), 21827–21840. (<a
href="https://doi.org/10.1007/s10489-023-04629-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition is a core component of visual information analysis, which serves for the position and orientation perception of autonomous driving and robotics. The current place recognition methods usually rely on image retrieval techniques to identify the visual similarity between query and gallery images. However, state-of-the-art image retrieval methods are often based on extensive labels, such as matched pairs (e.g., the image correspondences). Besides, image retrieval methods heavily suffer from environmental condition changes (i.e., a large range of illumination and weather changes). To alleviate the annotation cost, we introduce contrastive learning to perform feature extraction and feature similarity measurement in a self-supervised manner. Considering the heavy data augmentations of the existing contrastive learning approaches cannot effectively simulate domain disparities, we design the generative adversarial model to promote the extraction of domain-agnostic features. To tightly integrate the domain-agnostic representations and self-supervision, we design a self-generated soft constraint to achieve domain-agnostic contrastive learning (termed “DaCo”). Extensive experiments and analysis on cross-illumination and cross-weather settings are conducted on three challenging datasets. The proposed “DaCo” outperforms current contrastive learning based image retrieval methods by a large margin.},
  archive      = {J_APIN},
  author       = {Ren, Hao and Zheng, Ziqiang and Wu, Yang and Lu, Hong},
  doi          = {10.1007/s10489-023-04629-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21827-21840},
  shortjournal = {Appl. Intell.},
  title        = {DaCo: Domain-agnostic contrastive learning for visual place recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capsule networks for computer vision applications: A
comprehensive review. <em>APIN</em>, <em>53</em>(19), 21799–21826. (<a
href="https://doi.org/10.1007/s10489-023-04620-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved human-level performance in various computer vision tasks, such as image classification, object detection &amp; segmentation, etc. However, efficient CNN training requires a large amount of annotated data. Also, the CNNs, without explicit data augmentation, are bad at handling rotation and scale invariance. Besides, these neural networks do not learn the important spatial correlations between simple and complex objects in images. Recently, researchers introduced Capsule Network (CapsNet) to overcome the limitations of CNNs. CapsNet uses vector activation functions where the vectors’ length and orientation represent the entities’ existence and properties. Recent advances in the routing algorithms of CapsNets have increased their usefulness in solving complex computer vision problems. One can gauge their importance from numerous recently published articles in top-rank conferences and journals. Also, researchers have published a few review articles that discuss the structural and implementation details of CapsNets. This review focuses on the applications of CapsNet in computer vision. We first present a brief note on CNNs and their limitations, followed by basic structural and implementation details of CapsNet, including routing algorithms. Subsequently, the study investigates details of CapsNet variants that have evolved in recent years and their applications in different computer vision tasks. Finally, the paper presents a short commentary on the advantages, disadvantages, and limitations of CapsNet and outlines future research directions in the area of CapsNet.},
  archive      = {J_APIN},
  author       = {Choudhary, Seema and Saurav, Sumeet and Saini, Ravi and Singh, Sanjay},
  doi          = {10.1007/s10489-023-04620-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21799-21826},
  shortjournal = {Appl. Intell.},
  title        = {Capsule networks for computer vision applications: A comprehensive review},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel privacy protection approach with better human
imperceptibility. <em>APIN</em>, <em>53</em>(19), 21788–21798. (<a
href="https://doi.org/10.1007/s10489-023-04592-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our generation is quite obsessed with technology and we like to share our personal information such as photos and videos on the internet via different social networking websites i.e. Facebook, Snapchat, Instagram, etc. Therefore, it becomes easier for others to breach our privacy and harm us in a direct or indirect way. Now, computerized systems have advanced due to the improvements in Machine Learning (ML) algorithms and Artificial Intelligence (AI). These algorithms can extract sensitive information such as face attributes, text information, etc. from images or videos and can be used for privacy breaching. In this paper, we propose a novel privacy protection method by adding intelligent noise to the image while preserving image aesthetics and attributes. We determine multiple attributes for an image such as baldness, smiling, gender, etc. and we intelligently add noise to particular regions of the image that define a particular attribute using the visual explanation technique i.e. GradCam++, thereby preserving the other attributes. The addition of noise is based on the idea of Fast Gradient Sign Method (FGSM) that maximizes the gradients of the loss of an input image to create a new adversarial image. We integrate FGSM adversarial image and GradCam++ output to affect particular attributes only and hence keeping the image human imperceptible. The experiment results show that our attack outperforms the existing attacks including naive FGSM, Projected Gradient Descent (PGD), Momentum Iterative Method (MIM), Shadow Attack (SA), and Fast Minimum Norm (FMN) in terms of preserving attributes and image visual quality, when evaluated on CelebA dataset.},
  archive      = {J_APIN},
  author       = {Rana, Kapil and Pandey, Aman and Goyal, Parth and Singh, Gurinder and Goyal, Puneet},
  doi          = {10.1007/s10489-023-04592-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21788-21798},
  shortjournal = {Appl. Intell.},
  title        = {A novel privacy protection approach with better human imperceptibility},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully supervised contrastive learning in latent space for
face presentation attack detection. <em>APIN</em>, <em>53</em>(19),
21770–21787. (<a
href="https://doi.org/10.1007/s10489-023-04619-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of conventional face recognition systems to face presentation or face spoofing attacks has attracted a great deal of attention from information security, forensic, and biometric communities during the past few years. With the recent advancement and availability of cutting-edge computing technologies, sophisticated and computationally expensive solutions to many problems have been made possible. Accordingly, deep learning-based face presentation attack detection (PAD) methods have gained increasing popularity. In this research, we propose a supervised contrastive learning approach to tackle the face anti-spoofing problem. Essentially, the latent space encoding is achieved through an encoder network using the contrastive loss function infused with the class label information. The proposed robust encoding is followed by a simple classifier to distinguish between a real and a spoof face. To the best of our knowledge, this is the first work that uses fully supervised contrastive learning for the two-dimensional (2D) face PAD task. The performance of the proposed method is evaluated on several face anti-spoofing datasets and the results clearly show the efficacy of the proposed approach compared to other contemporary methods.},
  archive      = {J_APIN},
  author       = {Alassafi, Madini O. and Ibrahim, Muhammad Sohail and Naseem, Imran and AlGhamdi, Rayed and Alotaibi, Reem and Kateb, Faris A. and Oqaibi, Hadi Mohsen and Alshdadi, Abdulrahman A. and Yusuf, Syed Adnan},
  doi          = {10.1007/s10489-023-04619-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21770-21787},
  shortjournal = {Appl. Intell.},
  title        = {Fully supervised contrastive learning in latent space for face presentation attack detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear programming-based solution methods for constrained
partially observable markov decision processes. <em>APIN</em>,
<em>53</em>(19), 21743–21769. (<a
href="https://doi.org/10.1007/s10489-023-04603-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained partially observable Markov decision processes (CPOMDPs) have been used to model various real-world phenomena. However, they are notoriously difficult to solve to optimality, and there exist only a few approximation methods for obtaining high-quality solutions. In this study, grid-based approximations are used in combination with linear programming (LP) models to generate approximate policies for CPOMDPs. A detailed numerical study is conducted with six CPOMDP problem instances considering both their finite and infinite horizon formulations. The quality of approximation algorithms for solving unconstrained POMDP problems is established through a comparative analysis with exact solution methods. Then, the performance of the LP-based CPOMDP solution approaches for varying budget levels is evaluated. Finally, the flexibility of LP-based approaches is demonstrated by applying deterministic policy constraints, and a detailed investigation into their impact on rewards and CPU run time is provided. For most of the finite horizon problems, deterministic policy constraints are found to have little impact on expected reward, but they introduce a significant increase to CPU run time. For infinite horizon problems, the reverse is observed: deterministic policies tend to yield lower expected total rewards than their stochastic counterparts, but the impact of deterministic constraints on CPU run time is negligible in this case. Overall, these results demonstrate that LP models can effectively generate approximate policies for both finite and infinite horizon problems while providing the flexibility to incorporate various additional constraints into the underlying model.},
  archive      = {J_APIN},
  author       = {Helmeczi, Robert K. and Kavaklioglu, Can and Cevik, Mucahit},
  doi          = {10.1007/s10489-023-04603-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21743-21769},
  shortjournal = {Appl. Intell.},
  title        = {Linear programming-based solution methods for constrained partially observable markov decision processes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metaheuristic-based time series clustering for anomaly
detection in manufacturing industry. <em>APIN</em>, <em>53</em>(19),
21723–21742. (<a
href="https://doi.org/10.1007/s10489-023-04594-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays time series clustering is of great importance in manufacturing industries. Meanwhile, it is considerably challenging to achieve explainable solution as well as significant performance due to computation complexity and variable diversity. To efficaciously handle the difficulty, this paper presents a novel metaheuristic-based time series clustering method which can improve the effectiveness and logicality of existing clustering approaches. The proposed method collects candidate cluster references from hierarchical and partitional clustering through shape-based distance measure as well as dynamic time warping (DTW) on manufacturing time series data. By applying metaheuristics highlighting estimation of distribution algorithms (EDA), such as extended compact genetic algorithm (ECGA), on the collected candidate clusters, advanced cluster centroid combinations with minimal distances can be achieved. ECGA employs the least complicated and the most closely related probabilistic model structure regarding population space during generation cycle. This feature strengthens the comprehension of clustering results in how such optimal solutions were achieved. The proposed method was tested on real-world time series data, open to the public, from manufacturing industry, and showed noticeable performances compared to well-established methods. Accordingly, this paper demonstrates that obtaining both comprehensible result as well as prominent performance is feasible by employing metaheuristic techniques to time series data clustering methods.},
  archive      = {J_APIN},
  author       = {Suh, Woong Hyun and Oh, Sanghoun and Ahn, Chang Wook},
  doi          = {10.1007/s10489-023-04594-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21723-21742},
  shortjournal = {Appl. Intell.},
  title        = {Metaheuristic-based time series clustering for anomaly detection in manufacturing industry},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning prediction intervals based on selective joint
supervision. <em>APIN</em>, <em>53</em>(19), 21706–21722. (<a
href="https://doi.org/10.1007/s10489-023-04610-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a new methodology for the construction of deep learning-based prediction intervals (PIs) based on a selective jointly supervised cost function. The proposed method aims to preserve the advantages of the traditional joint supervision method, such as its interval robustness and compatibility with gradient-based optimizers, while improving the general convergence of the utilized training algorithms and reducing the computational costs incurred when using deep learning models. To test the capability of the proposed method to estimate uncertainties in complex, nonlinear dynamic systems, two prediction interval construction experiments were tested: one with an artificially generated dataset consisting of a modified Chen series and another using real electrical load data measured in the Huatacondo Microgrid in northern Chile. In these experiments, the proposed model was required to generate future predictions with accompanying prediction intervals, while its performance was measured according to its interval coverage, average width, average prediction error and computational cost of training. The proposal’s performance was compared with that of two other state-of-the-art interval models: the quality-driven method (Pearce et al. 2018), and the traditional joint supervision method (Cruz et al. 2018). The experimental results showed that the proposed selective joint supervision method incurred lower computational costs than the traditional joint supervision approach, with training times reduction magnitudes ranging from $$15\%$$ to $$85\%$$ . Additionally, the results showed that the proposed selective joint supervision method achieved better interval performance when using deep learning-based network architectures, showing up to a $$6\%$$ prediction error decrease and up to a $$15\%$$ overall interval width decrease.},
  archive      = {J_APIN},
  author       = {Parra, Sebastián and Sáez, Doris},
  doi          = {10.1007/s10489-023-04610-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21706-21722},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning prediction intervals based on selective joint supervision},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-skeleton structures graph convolutional network for
action quality assessment in long videos. <em>APIN</em>,
<em>53</em>(19), 21692–21705. (<a
href="https://doi.org/10.1007/s10489-023-04613-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most existing action quality assessment (AQA) methods, how to score simple actions in short-term sport videos has been widely explored. Recently, a few studies have attempted to solve the AQA problem of long-duration activity by extracting dynamic or static information directly from RGB video. However, these methods may ignore specific postures defined by dynamic changes in human body joints, which makes the results inaccurate and unexplainable. In this work, we propose a novel graph convolution network based on multiple skeleton structure modelling to address the problem of effective pose feature learning to improve the performance of AQA in complex activity. Specifically, three kinds of skeleton structures, including the joints’ self-connection, the intra-part connection, and the inter-part connection, are defined to model the motion patterns of joints and body parts. Moreover, a temporal attention learning module is designed to extract temporal relations between skeleton subsequences. We evaluate the proposed method on two benchmark datasets, the MIT-skate dataset and the Rhythmic Gymnastics dataset. Extensive experiments are conducted to verify the effectiveness of the proposed method. The experimental results show that our method achieves state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Lei, Qing and Li, Huiying and Zhang, Hongbo and Du, Jixiang and Gao, Shangce},
  doi          = {10.1007/s10489-023-04613-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21692-21705},
  shortjournal = {Appl. Intell.},
  title        = {Multi-skeleton structures graph convolutional network for action quality assessment in long videos},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of abnormal pedestrian behaviors at grade crossings
based on semi-supervised generative adversarial networks. <em>APIN</em>,
<em>53</em>(19), 21676–21691. (<a
href="https://doi.org/10.1007/s10489-023-04639-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexpected intrusions by pedestrians at grade crossings pose significant risks to the safety of railroad operations. Currently, there is no information processing system available for monitoring anomalies at grade crossings. Therefore, this paper presents a video processing pipeline and a generative adversarial network (GAN)-based deep learning framework to detect, localize, and analyze abnormal behaviors of pedestrians at grade crossings. First, the motions of pedestrians are represented by temporally-varying trajectories of key points identified by skeleton detection and tracking algorithms. A GAN model is developed to learn both global and local motion features of normal pedestrians only. The abnormal behaviors can then be detected as outliers by a discriminator during the testing phase. In contrast to existing efforts, several measures are taken to further boost model performance, including purposely exploiting overfitting in training to magnify the difference between the normal and the abnormal motion patterns and adding an appropriate amount of noise to enhance model generalization. The experiments conducted on a custom video dataset demonstrate the remarkable performance of our model, which successfully distinguishes motion patterns of squatting and lingering from normal walking. The model achieves a value of 0.89 in the AUC (Area Under the Curve) and notably outperforms the other seven benchmark models. The present method is also able to analyze multiple pedestrians in one video frame with a single run of the GAN model and requires no location-specific information, enabling salient robustness and field-deployability of the model at different locations without retraining.},
  archive      = {J_APIN},
  author       = {Song, Ge and Qian, Yu and Wang, Yi},
  doi          = {10.1007/s10489-023-04639-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21676-21691},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of abnormal pedestrian behaviors at grade crossings based on semi-supervised generative adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mathematical foundation, discussion and suggestion on
penalty parameter setting of penalty-based boundary intersection method
for many-objective optimization problems. <em>APIN</em>,
<em>53</em>(19), 21660–21675. (<a
href="https://doi.org/10.1007/s10489-023-04717-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition based method is a promising algorithmic framework for solving multi-objective optimization problems (MOPs). The target MOP is decomposed into a set of single-objective problems by using a scalarizing function with evenly specified weight vectors. Among the available scalarizing functions, penalty-based boundary intersection (PBI) with a proper penalty parameter is known to perform well. However, how to specify a penalty parameter θ that applies to an arbitrary Pareto front (PF) shape is not clear, let alone the theoretical basis behind the parameter setting. To the best of our knowledge, the theoretical basis has never been researched. The θ specification is basically a contour-setting problem. For a MOP with an unknown PF shape, one reliable practice is setting the contour on the border of the control area. To this end, a computational formulation of θ, which is determined by the angle between a given weight vector and the contour starting from this vector is deduced firstly. It provides a baseline (lower bound) of θ value and is a unified contour setting method applicable to various problems. Then, some fine-tuning methods are introduced for MOPs with a PF area that is hard to approach. Finally, a simple and classic minimal-PBI-function-first selection based genetic algorithm is used to illustrate the effectiveness of the proposed θ calculation. In the same algorithm frame, the performance of several representative θ setting methods are checked too. Their performances are compared on several test problems with different representative difficulties. The results show that the proposed θ setting can simultaneously ensure diversity and convergence.},
  archive      = {J_APIN},
  author       = {Yang, Chenglin and Tian, Shulin},
  doi          = {10.1007/s10489-023-04717-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21660-21675},
  shortjournal = {Appl. Intell.},
  title        = {Mathematical foundation, discussion and suggestion on penalty parameter setting of penalty-based boundary intersection method for many-objective optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatiotemporal fusion method based on interpretable deep
networks. <em>APIN</em>, <em>53</em>(19), 21641–21659. (<a
href="https://doi.org/10.1007/s10489-023-04608-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing spatiotemporal fusion is currently a popular research field in remote sensing that can cost-effectively generate remote sensing images with high spatiotemporal resolution. The deep neural network-based approach has strong feature extraction capability and has achieved great success in the fields of signal and image processing, but the deep network lacks interpretability. In this paper, a new interpretable deep network-based spatiotemporal fusion (UNSTF) method is proposed. The method proposes a new network model by first establishing a concise a priori formulation using sparse representation, constructing the proposed network by unfolding the proximal gradient algorithm for solving the model, and carefully designing each basic network module in the model to have a reasonable physical meaning, making the whole network interpretable. In addition, the UNSTF network method proposes a new network iteration loss function, where the predicted images of each iteration stage of the network are constrained by the real images, and the final stage and the intermediate network stages conform well to their inherent prior structures, effectively improving the accuracy and reliability of model prediction. Through extensive experiments, it is shown that the proposed method outperforms existing fusion methods in terms of both subjective and objective metrics.},
  archive      = {J_APIN},
  author       = {Lei, Dajiang and Tan, Jiayang and Wu, Yue and Liu, Qun and Li, Weisheng},
  doi          = {10.1007/s10489-023-04608-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21641-21659},
  shortjournal = {Appl. Intell.},
  title        = {A spatiotemporal fusion method based on interpretable deep networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short-term load forecasting system based on sliding fuzzy
granulation and equilibrium optimizer. <em>APIN</em>, <em>53</em>(19),
21606–21640. (<a
href="https://doi.org/10.1007/s10489-023-04599-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term electricity load forecasting is critical and challenging for scheduling operations and production planning in modern power management systems due to stochastic characteristics of electricity load data. Current forecasting models mainly focus on adapting to various load data to improve the accuracy of the forecasting. However, these models ignore the noise and nonstationarity of the load data, resulting in forecasting uncertainty. To address this issue, a short-term load forecasting system is proposed by combining a modified information processing technique, an advanced meta-heuristics algorithm and deep neural networks. The information processing technique utilizes a sliding fuzzy granulation method to remove noise and obtain uncertainty information from load data. Deep neural networks can capture the nonlinear characteristics of load data to obtain forecasting performance gains due to the powerful mapping capability. A novel meta-heuristics algorithm is used to optimize the weighting coefficients to reduce the contingency and improve the stability of the forecasting. Both point forecasting and interval forecasting are utilized for comprehensive forecasting evaluation of future electricity load. Several experiments demonstrate the superiority, effectiveness and stability of the proposed system by comprehensively considering multiple evaluation metrics.},
  archive      = {J_APIN},
  author       = {Li, Shoujiang and Wang, Jianzhou and Zhang, Hui and Liang, Yong},
  doi          = {10.1007/s10489-023-04599-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21606-21640},
  shortjournal = {Appl. Intell.},
  title        = {Short-term load forecasting system based on sliding fuzzy granulation and equilibrium optimizer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A large-scale multiobjective evolutionary algorithm with
overlapping decomposition and adaptive reference point selection.
<em>APIN</em>, <em>53</em>(19), 21576–21605. (<a
href="https://doi.org/10.1007/s10489-023-04596-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many large-scale multiobjective optimization problems with large decision space hinder the convergence search of evolutionary algorithms in various practical applications. Using the divide-and-conquer strategy to decompose the large-scale multiobjective problem into some subproblems and collaborative optimization is an effective strategy. However, the interactions between decision variables may cause many indirect interactions, which make complex high-dimensional problems impossible to decompose successfully using existing decomposition techniques. This paper proposes a multiobjective evolutionary algorithm with overlapping decomposition and adaptive reference point selection (MOEA-ODAR) for solving large-scale multiobjective problems. First, a decision variable overlap decomposition approach is suggested to group decision variables into several exclusive subcomponents. An adaptive resource allocation ensemble optimization method is then proposed to allocate corresponding resources to subcomponents with different structures. Finally, an adaptive reference point selection method based on Pareto shape estimation is designed to optimize the specific subcomponents. The theoretical analysis of the correctness of overlapping decomposition decision variables and collaborative optimization is presented. It is compared with the newly proposed excellent large-scale multiobjective optimization algorithm on many test problems. The experimental results show that the proposed algorithm performs better in terms of convergence, population distributivity, and computational efficiency. In addition, the superiority of the algorithm on large-scale many-objective problems is verified.},
  archive      = {J_APIN},
  author       = {Gao, Mengqi and Feng, Xiang and Yu, Huiqun and Li, Xiuquan},
  doi          = {10.1007/s10489-023-04596-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21576-21605},
  shortjournal = {Appl. Intell.},
  title        = {A large-scale multiobjective evolutionary algorithm with overlapping decomposition and adaptive reference point selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D reconstruction system and multiobject local tracking
algorithm designed for billiards. <em>APIN</em>, <em>53</em>(19),
21543–21575. (<a
href="https://doi.org/10.1007/s10489-023-04542-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of virtual reality or augmented reality systems in billiards sports are useful tools for pure entertainment or improving the player’s skills. Depending on the purpose of these systems, tracking algorithms based on computer vision must be used. These algorithms are especially useful in systems aiming to reconstruct the trajectories followed by the balls after a strike. However, depending on the billiard modality, the problem of tracking multiple small identical objects, such as balls, is a complex task. In addition, when an amateur or nontop professional player uses low-frame-rate and low-resolution devices, problems such as blurred balls, blurred contours, or fuzzy edges, among others, arise. These effects have a negative impact on ball-tracking accuracy and reconstruction quality. Thus, this work proposes two contributions. The first contribution is a new tracking algorithm called “multiobject local tracking (MOLT)”. This algorithm can track balls with high precision and accuracy even with motion blur caused by low-resolution and low-frame-rate devices. Moreover, the proposed MOLT algorithm is compared with nine tracking methods and four different metrics, outperforming the rest of the methods in the majority of the cases and providing a robust solution. The second contribution is a whole system to track (using the MOLT algorithm) and reconstruct the movements of the balls on a billiard table in a 3D virtual world using computer vision. The proposed system covers all steps from image capture to 3D reconstruction. The 3D reconstruction results have been qualitatively evaluated by different users through a series of questionnaires, obtaining an overall score of 7.6 (out of 10), which indicates that the system is a promising and useful tool for training. Finally, both the MOLT algorithm and the reconstruction system are tested in three billiard modalities: blackball, carom billiards, and snooker.},
  archive      = {J_APIN},
  author       = {Rodriguez-Lozano, Francisco J. and Gámez-Granados, Juan C. and Martínez, Héctor and Palomares, Jose M. and Olivares, Joaquín},
  doi          = {10.1007/s10489-023-04542-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21543-21575},
  shortjournal = {Appl. Intell.},
  title        = {3D reconstruction system and multiobject local tracking algorithm designed for billiards},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable global error weighted on feature importance: The
xGEWFI metric to evaluate the error of data imputation and data
augmentation. <em>APIN</em>, <em>53</em>(19), 21532–21542. (<a
href="https://doi.org/10.1007/s10489-023-04661-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating data imputation and augmentation performance is a critical issue in data science. In statistics, methods like Kolmogorov-Smirnov K-S test, Cramér-von Mises $$W^2$$ , Anderson-Darling $$A^2$$ , Pearson’s $$\chi ^2$$ and Watson’s $$U^2$$ exists for decades to compare the distribution of two datasets. In the context of data generation, typical evaluation metrics have the same flaw: They calculate the feature’s error and the global error on the generated data without weighting the error with the feature’s importance. In most cases, the importance of the features is imbalanced, and it can induce a bias on the features and global errors. This paper proposes a novel metric named “Explainable Global Error Weighted on Feature Importance” (xGEWFI). This new metric is tested in a whole preprocessing method that 1. Process the outliers, 2. impute the missing data, and 3. augments the data. At the end of the process, the xGEWFI error is calculated. The distribution error between the original and generated data is calculated using a Kolmogorov-Smirnov test (K-S test) for each feature. Those results are multiplied by the importance of the respective features and calculated using a Random Forest (RF) algorithm. The metric result is expressed in an explainable format, aiming for an ethical AI. This novel method provides a more precise evaluation of a data generation process than if only a K-S test were used.},
  archive      = {J_APIN},
  author       = {Dessureault, Jean-Sébastien and Massicotte, Daniel},
  doi          = {10.1007/s10489-023-04661-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21532-21542},
  shortjournal = {Appl. Intell.},
  title        = {Explainable global error weighted on feature importance: The xGEWFI metric to evaluate the error of data imputation and data augmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frame importance and temporal memory effect-based fast video
quality assessment for user-generated content. <em>APIN</em>,
<em>53</em>(19), 21517–21531. (<a
href="https://doi.org/10.1007/s10489-023-04624-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-generated content (UGC) has become increasingly popular, promoted by the widespread use of social media and mobile devices. Therefore, instant and immersive UGC video quality assessment is urgently needed to provide appropriate recommendations for video reviewers prior to distribution. However, existing methods are neither efficient at assessing UGC videos due to the expensive frame-by-frame process nor suitable for deployment on devices with limited computational capabilities because they require sophisticated GPU-dependent computation. In this paper, we propose a fast UGC video quality assessment method, named FastVQA, by considering both keyframe importance and human temporal memory effects. First, a novel key frame selection strategy based on feature entropy is developed to achieve efficient and accurate feature extraction. Inspired by human short-term and long-term memory effects, we design a temporal feature aggregation module by taking both local content details and global semantic information into consideration. Experimental results show that FastVQA can outperform the state-of-the-art (SOTA) methods on many datasets with significantly reduced CPU time, which implies that FastVQA can achieve a better balance between complexity and accuracy.},
  archive      = {J_APIN},
  author       = {Zhang, Yuan and Yang, Mingchuan and Huang, Zhiwei and He, Lijun and Wu, Zijun},
  doi          = {10.1007/s10489-023-04624-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21517-21531},
  shortjournal = {Appl. Intell.},
  title        = {Frame importance and temporal memory effect-based fast video quality assessment for user-generated content},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-attention network for bi-directional salient object
detection. <em>APIN</em>, <em>53</em>(19), 21500–21516. (<a
href="https://doi.org/10.1007/s10489-023-04648-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection models based on neural networks have achieved outstanding results, but there are still problems such as low accuracy of object boundaries and redundant parameters. To alleviate these problems, we make full use of position and contour information from the down-sampling layers, and optimize the detection result layer by layer. First, this paper designs an attention-based adaptive fusion module (AAF), which can suppress the background and highlight the foreground that is more relevant to the detection task. It automatically learns the fusion weights of different features to filter out conflict information. Second, this paper proposes a bi-attention block module which combines reverse attention and positive attention. Third, this paper introduces bi-directional task learning by decomposing the image into high-frequency and low-frequency components. This approach fully exploits the complementary and independent nature of different frequency information. Finally, the proposed method is compared with 14 state-of-the-art methods on 6 datasets, and achieves very competitive results. Additionally, the model size is only 114.19MB, and the inference speed can reach nearly 40 FPS.},
  archive      = {J_APIN},
  author       = {Xu, Cheng and Wang, Hui and Liu, Xianhui and Zhao, Weidong},
  doi          = {10.1007/s10489-023-04648-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21500-21516},
  shortjournal = {Appl. Intell.},
  title        = {Bi-attention network for bi-directional salient object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying bird species by their calls in soundscapes.
<em>APIN</em>, <em>53</em>(19), 21485–21499. (<a
href="https://doi.org/10.1007/s10489-023-04486-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real data science problems, it is common to encounter a domain mismatch between the training and testing datasets, which means that solutions designed for one may not transfer well to the other due to their differences. An example of such was in the BirdCLEF2021 Kaggle competition, where participants had to identify all bird species that could be heard in audio recordings. Thus, multi-label classifiers, capable of coping with domain mismatch, were required. In addition, classifiers needed to be resilient to a long-tailed (imbalanced) class distribution and weak labels. Throughout the competition, a diverse range of solutions based on convolutional neural networks were proposed. However, it is unclear how different solution components contribute to overall performance. In this work, we contextualise the problem with respect to the previously existing literature, analysing and discussing the choices made by the different participants. We also propose a modular solution architecture to empirically quantify the effects of different architectures. The results of this study provide insights into which components worked well for this challenge.},
  archive      = {J_APIN},
  author       = {Maclean, Kyle and Triguero, Isaac},
  doi          = {10.1007/s10489-023-04486-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {19},
  pages        = {21485-21499},
  shortjournal = {Appl. Intell.},
  title        = {Identifying bird species by their calls in soundscapes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: An enhanced SSD with feature
cross-reinforcement for small-object detection. <em>APIN</em>,
<em>53</em>(18), 21483–21484. (<a
href="https://doi.org/10.1007/s10489-023-04640-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Gong, Lixiong and Huang, Xiao and Chao, Yinkang and Chen, Jialin and Lei, Binwen},
  doi          = {10.1007/s10489-023-04640-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21483-21484},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: An enhanced SSD with feature cross-reinforcement for small-object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid training algorithm based on gradient descent and
evolutionary computation. <em>APIN</em>, <em>53</em>(18), 21465–21482.
(<a href="https://doi.org/10.1007/s10489-023-04595-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Back propagation (BP) is widely used for parameter search of fully-connected layers in many neural networks. Although BP has the potential of quickly converging to a solution, due to its gradient-based nature, it tends to fall into a local optimum. Metaheuristics such as evolutionary computation (EC) techniques, as gradient-free methods, may have excellent global search capability due to their stochastic nature. However, these techniques tend to perform worse than BP in terms of convergence speed. In this paper, a hybrid gradient descent search algorithm (HGDSA) is proposed for training the parameters in fully-connected neural networks. HGDSA initially searches the space extensively by means of an ensemble of gradient descent strategies in the early stage and then uses BP as an exploitative local search operator. Moreover, a self-adaptive method which selects strategies and updates the learning rates of strategies has been designed and embedded in the global search operators to prevent stagnation in local optima. To verify the effectiveness of HGDSA, experiments were performed on eleven classification datasets. Experimental results demonstrate that the proposed HGDSA possesses both powerful global and local search abilities. Furthermore, the proposed approach appears to be promising also on high-dimensional datasets.},
  archive      = {J_APIN},
  author       = {Xue, Yu and Tong, Yiling and Neri, Ferrante},
  doi          = {10.1007/s10489-023-04595-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21465-21482},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid training algorithm based on gradient descent and evolutionary computation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot relation classification based on the BERT model,
hybrid attention and fusion networks. <em>APIN</em>, <em>53</em>(18),
21448–21464. (<a
href="https://doi.org/10.1007/s10489-023-04634-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation classification (RC) is an essential task in information extraction. The distance supervision (DS) method can use many unlabeled data and solve the lack of training data on the RC task. However, the DS method has the problems of long tails and noise. Intuitively, people can solve these problems using few-shot learning (FSL). Our work aims to improve the accuracy and rapidity of convergence on the few-shot RC task. We believe that entity pairs have an essential role in the few-shot RC task. We propose a new context encoder, which is improved based on the bidirectional encoder representations from transformers (BERT) model to fuse entity pairs and their dependence information in instances. At the same time, we design hybrid attention, which includes support instance-level and query instance-level attention. The support instance level dynamically assigns the weight of each instance in the support set. It makes up for the insufficiency of prototypical networks, which distribute weights to sentences equally. Query instance-level attention is dynamically assigned weights to query instances by similarity with the prototype. The ablation study shows the effectiveness of our proposed method. In addition, a fusion network is designed to replace the Euclidean distance method of previous works when class matching is performed, improving the convergence’s rapidity. This makes our model more suitable for industrial applications. The experimental results show that the proposed model’s accuracy is better than that of several other models.},
  archive      = {J_APIN},
  author       = {Li, Yibing and Ding, Zenghui and Ma, Zuchang and Wu, Yichen and Wang, Yu and Zhang, Ruiqi and Xie, Fei and Ren, Xiaoye},
  doi          = {10.1007/s10489-023-04634-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21448-21464},
  shortjournal = {Appl. Intell.},
  title        = {Few-shot relation classification based on the BERT model, hybrid attention and fusion networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-agent reinforcement learning method with curriculum
transfer for large-scale dynamic traffic signal control. <em>APIN</em>,
<em>53</em>(18), 21433–21447. (<a
href="https://doi.org/10.1007/s10489-023-04652-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using reinforcement learning to control traffic signal systems has been discussed in recent years, but most works focused on simple scenarios such as a single crossroads, and the methods aiming at large-scale traffic scenarios face long-time training and suboptimal results. In this work, we develop a new multi-agent reinforcement model for large-scale traffic signal control tasks, and a curriculum transfer learning method is developed to optimize the joint policy step by step. The policies for different intersections are trained in a partially observable Markov decision process with centralized training and decentralized execution mechanism, and we design transformer modules for both the policy and evaluation networks by attention mechanism. We first train policies in a simple traffic scenario, and then these policies are transferred to the next curriculum by policy reloading, while the experiences of the source task are reused selectively. With the number of agents increasing, our method can achieve satisfactory performances quickly by reusing the knowledge from previous curriculums. We conduct several experiments on the Cityflow testbed. In the case of more than 10 crossroads, our model improve the mean reward from 3.0 to 5.0.},
  archive      = {J_APIN},
  author       = {Li, Xuesi and Li, Jingchen and Shi, Haobin},
  doi          = {10.1007/s10489-023-04652-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21433-21447},
  shortjournal = {Appl. Intell.},
  title        = {A multi-agent reinforcement learning method with curriculum transfer for large-scale dynamic traffic signal control},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal representation for few-shot text classification.
<em>APIN</em>, <em>53</em>(18), 21422–21432. (<a
href="https://doi.org/10.1007/s10489-023-04667-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Text Classification (FSTC) is a fundamental natural language processing problem that aims to classify small amounts of text with high accuracy. Mainstream methods model the superficial statistical relationships between text and labels. However, distributional imbalance problems are encountered during few-shot learning; therefore, questions remain regarding its robustness and generalization. The above problems can be addressed by intrinsic causal mechanisms. We introduce a general structural causal model to formalize the FSTC problem. To extract causal associations from text and reconstruct information to achieve a better classification effect, we propose a causal representation for few-shot learning (CRFL) framework to force representations to be causally related. Our framework performs well when the number of training examples is small or when it generalizes to the data transfer situation. CRFL is orthogonal to the existing fine-tuning and few-shot meta-learning methods and can be applied to any task. Extensive experimental results obtained on several widely used datasets validate the effectiveness of our approach, which can be attributed to our model’s stability and logical reasoning.},
  archive      = {J_APIN},
  author       = {Yang, Maoqin and Zhang, Xuejie and Wang, Jin and Zhou, Xiaobing},
  doi          = {10.1007/s10489-023-04667-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21422-21432},
  shortjournal = {Appl. Intell.},
  title        = {Causal representation for few-shot text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fast newton method to solve KLR based on multilevel
circulant matrix with log-linear complexity. <em>APIN</em>,
<em>53</em>(18), 21407–21421. (<a
href="https://doi.org/10.1007/s10489-023-04606-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel logistic regression (KLR) is a conventional nonlinear classifier in machine learning. With the explosive growth of data size, the storage and computation of large dense kernel matrices is a major challenge in scaling KLR. Even when the nyström approximation is applied to solve KLR, the corresponding method faces time complexity of $$\varvec{O}(\varvec{nc}^{\varvec{2}})$$ and space complexity of $$\varvec{O(nc)}$$ , where n is the number of training instances and c is the sample size. We propose a fast Newton method to efficiently solve large-scale KLR problems by exploiting the storage and computing advantages of a multilevel circulant matrix (MCM). By approximating the kernel matrix with an MCM, the storage space is reduced to $$\varvec{O(n)}$$ , and further approximating the coefficient matrix of the Newton equation as an MCM, the computational complexity of Newton iteration is reduced to $$\varvec{O}(\varvec{n \log n})$$ . The proposed method can run in log-linear time complexity per iteration, because the multiplication of an MCM (or its inverse) and a vector can be implemented by the multidimensional fast Fourier transform (mFFT). Experimental results on some large-scale binary- and multi-classification problems show that the proposed method enables KLR to scale to large scale problems with less memory consumption and less training time without sacrificing test accuracy.},
  archive      = {J_APIN},
  author       = {Zhang, Junna and Zhou, Shuisheng and Fu, Cui and Ye, Feng},
  doi          = {10.1007/s10489-023-04606-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21407-21421},
  shortjournal = {Appl. Intell.},
  title        = {Fast newton method to solve KLR based on multilevel circulant matrix with log-linear complexity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic coarse-to-refinement-based ultrasound prostate
segmentation using optimal polyline segment tracking method and deep
learning. <em>APIN</em>, <em>53</em>(18), 21390–21406. (<a
href="https://doi.org/10.1007/s10489-023-04676-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of the prostate in transrectal ultrasound (TRUS) images provides useful information for prostate cancer diagnosis and treatment. However, boundaries between the prostate and other tissues are often absent or ill defined in TRUS images, which means that automatic segmentation of the prostate in TRUS images is highly challenging. In this study, we attempted to overcome these challenges by developing a novel method we termed “automatic prostate segmentation” (Auto-ProSeg) that is capable of effectively segmenting the prostate in TRUS images. Auto-ProSeg comprises two steps: the first step is a preprocessing step that uses attention U-Net to extract approximate prostate contours automatically; then, in the second step, the approximate prostate contours are optimized via a modified principal curve-based method linked to an evolutionary neural network, whereby a mathematical mapping formula based on the parameters of an enhanced evolutionary neural network is used to generate smooth prostate contours. Our results illustrate that Auto-ProSeg exhibits better prostate segmentation performance than other recently developed methods: the average Dice similarity coefficient and Jaccard similarity coefficient (Ω) of Auto-ProSeg-generated prostate contours against ground truths were 94.2% ± 3.2% and 93% ± 3.7%, whereas those for other state-of-the-art fully automatic segmentation methods were approximately 90% ± 5% and 89% ± 6%, respectively.},
  archive      = {J_APIN},
  author       = {Peng, Tao and Xu, Daqiang and Tang, Caiyin and Zhao, Jing and Shen, Yuntian and Yang, Cong and Cai, Jing},
  doi          = {10.1007/s10489-023-04676-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21390-21406},
  shortjournal = {Appl. Intell.},
  title        = {Automatic coarse-to-refinement-based ultrasound prostate segmentation using optimal polyline segment tracking method and deep learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved teaching–learning-based optimization algorithm with
cauchy mutation and chaotic operators. <em>APIN</em>, <em>53</em>(18),
21362–21389. (<a
href="https://doi.org/10.1007/s10489-023-04705-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teaching–Learning-Based Optimization (TLBO) is a population-based intelligent optimization algorithm, which simulates the &quot;teaching&quot; process of teachers to students and the &quot;learning&quot; process of students in the class. In order to solve the problems of slow optimization speed, low optimization accuracy and easy to fall into local optimization, an improved TLBO algorithm based on Cauchy mutation and chaos operators are proposed. Firstly, the dynamic selection of teachers in the &quot;teaching&quot; stage leads to higher class average grades. Learning from the best students in the class during the &quot;learning&quot; phase makes class results more focused. Secondly, after a teaching is completed, Cauchy mutation is carried out to make the algorithm population more diverse so as to get rid of the local optimal solution. Finally, on the basis of Cauchy mutation, chaos theory is introduced into the optimization process of TLBO algorithm, and 10 chaos are embedded in the process of generating random numbers by Cauchy mutation, which enhances its ergo city and irreconcilability to further improve its convergence speed and accuracy. The performance of the proposed improved TLBO algorithm was tested by using 30 benchmark functions in CEC-BC-2017, and finally two engineering design problems (cantilever arm design and pressure vessel design) were optimized. The experimental results show that the proposed TLBO algorithm has significantly improved its convergence speed and optimization accuracy.},
  archive      = {J_APIN},
  author       = {Bao, Yin-Yin and Xing, Cheng and Wang, Jie-Sheng and Zhao, Xiao-Rui and Zhang, Xing-Yue and Zheng, Yue},
  doi          = {10.1007/s10489-023-04705-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21362-21389},
  shortjournal = {Appl. Intell.},
  title        = {Improved teaching–learning-based optimization algorithm with cauchy mutation and chaotic operators},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reversible data hiding of color image based on channel unity
embedding. <em>APIN</em>, <em>53</em>(18), 21347–21361. (<a
href="https://doi.org/10.1007/s10489-023-04707-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current reversible data hiding (RDH) methods are based on grayscale images; however, color images are widely used in real life. In this paper, we propose a color image RDH scheme based on channels united embedded, which utilizes pixel value ordering (PVO). First, the correlation between color image channels is utilized to re-collect the pixels in the current channel block and the pixels are guided by the reference channel. Large pixel values are preferentially collected to increase the number of expanded prediction errors. The traditional color image RDH capacity allocation method needs to allocate capacity to the three R, G, and B channels beforehand, and the three channels are independent for data embedding. This paper proposes a channel unity embedding (CUE) method to uniformly sort the pixel blocks of the three channels according to the complexity value, and perform data embedding on the blocks according to the order of the complexity values. In CUE, after data embedding is completed, capacity allocation can be achieved and smooth blocks in the channels can be fully utilized, thereby reducing invalid shifts. Finally, the recollection and CUE strategies are combined with pairwise prediction Error expansion (PEE) to further improve the performance. The experimental results show that the proposed scheme achieves significant superiority in fidelity over a series of state-of-the-art schemes. For example, the PSNR on the Lena image reaches 63.08 dB, which is a 0.81 dB gain compared to the best results in the literature with a 20,000 bits embedding capacity.},
  archive      = {J_APIN},
  author       = {Mao, Ningxiong and He, Hongjie and Chen, Fan and Zhu, Keke},
  doi          = {10.1007/s10489-023-04707-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21347-21361},
  shortjournal = {Appl. Intell.},
  title        = {Reversible data hiding of color image based on channel unity embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogenous image fusion model with SR-dual-channel PCNN
significance region for NSST in an apple orchard. <em>APIN</em>,
<em>53</em>(18), 21325–21346. (<a
href="https://doi.org/10.1007/s10489-023-04690-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visible light image and Time of Flight image fusion technology can effectively remedy the limitation of single orchard scene data source. The Dual-channel Pulse Coupled Neural Network is often used for multi-scale fusion rules in transform domain. However, in the ignition process of Dual-channel Pulse Coupled Neural Network, it ignores the impact of image changes and fluctuations on the results, resulting in pixel artifacts, region blurring and unclear edges. In order to solve this problem, a Heterogenous Image Fusion Model with Dual Channel Pulse Coupled Neural Network Significance Region for Nonsubsampled Shearlet Transform in an Apple Orchard is proposed. Three aspects of adaptive parameters improvement of Dual Channel Pulse Coupled Neural Network are proposed, including using root mean square error to improve the dynamic link domain, image gradient to define the link strength and the average gray value of pixels in the ignition region as the dynamic threshold respectively. Moreover, a Significant Region Extraction Method is proposed to calculate the low-frequency significant regions. The model improves the segmentation effect of significant regions, and the significance ratio of the three groups of samples under front light and back light reaches 100.00%, with the fastest segmentation reaching 2.13 s. The six evaluation index values of the three groups of samples in different periods of front light and back light are superior to other fusion models. The fusion rate of model recognition reaches 100.00%, and the fusion speed reaches 8.02 s at the fastest. The model has good effect in precision, time consumption, model size and so on, which can supplement and improve the fusion theory of multi-source image transform domain. • First, a Dual Channel Pulse Coupled Neural Network with Significant Region (SR-dual- channel PCNN) is proposed. • Second, three aspects of adaptive parameters improvement of Dual PCNN are proposed, including using root mean square error to improve the dynamic link domain, image gradient to define the link strength and the average gray value of pixels in the ignition region as the dynamic threshold respectively. • Third, a Significant Region Extraction Method (SRE-M) is proposed to calculate the low-frequency significant regions. A low frequency fusion rule is built to use the salient region as the low frequency subband fusion coefficient to suppress the background of the salient region.},
  archive      = {J_APIN},
  author       = {Liu, Liqun and Zhou, Yubo and Huo, Jiuyuan and Wu, Ye and Gu, Renyuan},
  doi          = {10.1007/s10489-023-04690-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21325-21346},
  shortjournal = {Appl. Intell.},
  title        = {Heterogenous image fusion model with SR-dual-channel PCNN significance region for NSST in an apple orchard},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new framework for intelligent fault diagnosis of spiral
bevel gears with unbalanced data. <em>APIN</em>, <em>53</em>(18),
21312–21324. (<a
href="https://doi.org/10.1007/s10489-023-04701-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The special meshing motion and complex internal structure of spiral bevel gears make modeling the fault mechanism and extracting manual features challenging. Intelligent technology is expected to significantly improve the automation level of spiral bevel gear fault diagnosis. However, the lack of spiral bevel gear fault samples can result in degraded diagnostic performance. Existing solutions have often been developed at either the algorithm level or the data level, making it difficult to achieve high-quality feature extraction and high-precision pattern classification simultaneously. To overcome this deficiency, a novel framework is proposed for the intelligent fault diagnosis under imbalanced dataset. Firstly, an unsupervised learning neural network, deep convolutional auto-encoder (DCAE), is constructed to encode the data features, which modeled global and local information by combing global pooling and max pooling. Then during the fine-tuning stage of the classifier, we used an improved cross-entropy loss function to impose a greater misclassification cost on difficult and small samples, which can avoid overfitting of the features to healthy data features. By analyzing the experimental datasets of spiral bevel gears, it is demonstrated that the proposed framework has better performance in handing intelligent fault diagnosis tasks under the condition of unbalanced data than the reported frameworks.},
  archive      = {J_APIN},
  author       = {Wei, Angang and Han, Songyu and Li, Wei and Shao, Haidong and Yang, Xingkai},
  doi          = {10.1007/s10489-023-04701-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21312-21324},
  shortjournal = {Appl. Intell.},
  title        = {A new framework for intelligent fault diagnosis of spiral bevel gears with unbalanced data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel action decision method of deep reinforcement
learning based on a neural network and confidence bound. <em>APIN</em>,
<em>53</em>(18), 21299–21311. (<a
href="https://doi.org/10.1007/s10489-023-04695-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the perspective of the deep reinforcement learning algorithm, the training effect of the agent will be affected because of the excessive randomness of the ε-greedy method. This paper proposes a novel action decision method to replace the ε-greedy method and avoid excessive randomness. First, a confidence bound span fitting model based on a deep neural network is proposed to fundamentally solve the problem that UCB cannot estimate the confidence bound span of each action in high-dimensional state space. Then, a confidence bound span balance model based on target value in reverse order is proposed. The parameters of the U network are updated after each action decision using the backpropagation of the neural network to balance the confidence bound span. Finally, an exploration-exploitation dynamic balance factor $$\alpha$$ is introduced to balance exploration and exploitation in the training process. Experiments are conducted using the Nature DQN and Double DQN algorithms, and the results demonstrate that the proposed method achieves higher performance than the ε-greedy method under the basic algorithm and experimental environment of this paper. The method presented in this paper has significance for applying a confidence bound to solve complex reinforcement problems.},
  archive      = {J_APIN},
  author       = {Zhang, Wenhao and Song, Yaqing and Liu, Xiangpeng and Shangguan, Qianqian and An, Kang},
  doi          = {10.1007/s10489-023-04695-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21299-21311},
  shortjournal = {Appl. Intell.},
  title        = {A novel action decision method of deep reinforcement learning based on a neural network and confidence bound},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reference ideal model with evidential reasoning for
probabilistic-based expressions. <em>APIN</em>, <em>53</em>(18),
21283–21298. (<a
href="https://doi.org/10.1007/s10489-023-04653-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to experts&#39; different cognitions, experiences, and knowledge backgrounds, their evaluations may be different, and none of them can be ignored, which leads to the development of the probabilistic linguistic term set (PLTS) and the probabilistic hesitant fuzzy set (PHFS). In practical situations, sometimes the optimal alternative exists in a reference ideal interval instead of the maximum or the minimum. This paper constructs a reference ideal model with evidential reasoning for the PLTS and the PHFS. At first, a maximum deviation method based on two hierarchical attributes is proposed, aiming at determining the attribute weights in a multi-attribute decision-making problem. Then, since the evaluations are provided with different forms and principles, a normalisation process can help to make the evaluations unified. Moreover, the evidential reasoning process is introduced to aggregate evaluation grades based on the probabilities in the probabilistic-based expressions. And the final decision results are obtained by applying the distance between the aggregated evaluation grades and the extreme values. Then, we use the proposed model for the potential chronic obstructive pulmonary disease patient evaluation to verify the operability. Besides, a comparative analysis is also conducted to prove the rationality of the model.},
  archive      = {J_APIN},
  author       = {He, Yue and Xu, Dongling and Yang, Jianbo and Xu, Zeshui and Liu, Nana},
  doi          = {10.1007/s10489-023-04653-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21283-21298},
  shortjournal = {Appl. Intell.},
  title        = {A reference ideal model with evidential reasoning for probabilistic-based expressions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multiple graph reasoning network for joint optic disc and
cup segmentation. <em>APIN</em>, <em>53</em>(18), 21268–21282. (<a
href="https://doi.org/10.1007/s10489-023-04560-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is one of the most irreversible eye diseases that causes visual damage worldwide. For glaucoma screening, it is essential to accurately segment the optic disc (OD) from the optic cup (OC) in fundus images. However, most of the known segmentation methods are two-stage processes and cannot obtain satisfactory segmentations without performing OD localization in advance. To overcome this limitation, in this paper, we propose a novel one-stage framework named the multiple graph reasoning network (MGRNet) to segment ODs and OCs from different fundus image datasets. The MGRNet performs graph inference in the Euclidean space, channel space, and hyperbolic space. Euclidean space-based graph reasoning captures the spatial dependencies between pixels from the coordinate dimension. Channel space-based graph reasoning constructs the channel relationship between any pair of channel maps. Hyperbolic space-based graph reasoning provides pixel-level hierarchical embeddings in the hyperbolic space. The MGRNet exhibits obvious advantages over a strong baseline without preprocessing operations, including OD localization, region of interest (ROI) cropping, and data augmentation advantages, and achieves satisfactory results on both the REFUGE (0.9442 mean intersection over union (IoU)) and DRISHTI-GS (0.8792 mean IoU) datasets. Additionally, the MGRNet can also provide reliable glaucoma screening results by measuring the cup-to-disc ratio (CDR) based on segmentation results.},
  archive      = {J_APIN},
  author       = {Zhang, Baoliang and Guo, Xiaoxin and Li, Guangyu and Shen, Zhengran and Hu, Xiaoying and Che, Songtian},
  doi          = {10.1007/s10489-023-04560-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21268-21282},
  shortjournal = {Appl. Intell.},
  title        = {Multiple graph reasoning network for joint optic disc and cup segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Giza pyramids construction algorithm with gradient contour
approach for multilevel thresholding color image segmentation.
<em>APIN</em>, <em>53</em>(18), 21248–21267. (<a
href="https://doi.org/10.1007/s10489-023-04512-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation is an essential processing step in computer vision. As a popular method in image segmentation, the pivotal of multilevel thresholding lies in determining specific thresholds of the image. In this paper, we introduce an enhanced Giza pyramids construction algorithm (GPC) with a gradient contour approach, termed GGPC, for solving color image segmentation problems. In the proposed GGPC algorithm, the gradient contour approach explores the fitness landscape based on population distributions to speculate promising summits, and provides an efficient guidance for further evolution. A composite suit of benchmark functions is adopted to evaluate the proposed GGPC algorithm which is shown via extensive comparisons to outperform eight state-of-the-art algorithms. To further develop the application potential, we propose an excellent GGPC-based thresholding segmentation method by multilevel Kapur entropy. The method is successfully exploited in color image segmentation with respect to different categories of benchmark images. Experiment results demonstrate that the GGPC-based segmentation method also exhibits better performance over peers, providing a more effective technique for color image segmentation.},
  archive      = {J_APIN},
  author       = {Wu, Bowen and Zhu, Liangkuan and Li, Xin},
  doi          = {10.1007/s10489-023-04512-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21248-21267},
  shortjournal = {Appl. Intell.},
  title        = {Giza pyramids construction algorithm with gradient contour approach for multilevel thresholding color image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-graph with non-convex sparse regularization for
multi-label feature selection. <em>APIN</em>, <em>53</em>(18),
21227–21247. (<a
href="https://doi.org/10.1007/s10489-023-04515-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As for single-label learning, feature selection has become a crucial data pre-processing tool for multi-label learning due to its ability to reduce the feature dimensionality to mitigate the influence of “the curse of dimensionality”. In recent years, the multi-label feature selection (MFS) methods based on manifold learning and sparse regression have confirmed their superiority and gained more and more attention than other methods. However, most of these methods only consider exploiting the geometric structure distributed in the data manifold but ignore the geometric structure distributed in the feature manifold, resulting in incomplete information about the learned manifold. Aiming to simultaneously exploit the geometric information of data and feature spaces for feature selection, this paper proposes a novel MFS method named dual-graph based multi-label feature selection (DGMFS). In the framework of DGMFS, two nearest neighbor graphs are constructed to form a dual-graph regularization to explore the geometric structures of both the data manifold and the feature manifold simultaneously. Then, we combined the dual-graph regularization with a non-convex sparse regularization l2,1 − 2-norm to obtain a more sparse solution for the weight matrix, which can better deal with the redundancy features. Finally, we adopt the EM strategy to design an iterative updating optimization algorithm for DGMFS and provide the convergence analysis of the optimization algorithm. Extensive experimental results on ten benchmark multi-label data sets have shown that DGMFS is more effective than several state-of-the-art MFS methods.},
  archive      = {J_APIN},
  author       = {Sun, Zhenzhen and Xie, Hao and Liu, Jinghua and Gou, Jin and Yu, Yuanlong},
  doi          = {10.1007/s10489-023-04515-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21227-21247},
  shortjournal = {Appl. Intell.},
  title        = {Dual-graph with non-convex sparse regularization for multi-label feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rotating machinery fault diagnosis based on feature
extraction via an unsupervised graph neural network. <em>APIN</em>,
<em>53</em>(18), 21211–21226. (<a
href="https://doi.org/10.1007/s10489-023-04665-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis is an essential process for the health maintenance of rotating machinery. With the development of AI technology, many deep learning-based methods have been applied to fault diagnosis to enhance the intelligence level of equipment maintenance. Such methods normally need a large amount of labeled data for model training. However, label acquisition is a difficult task that requires extensive human labor. To address these issues, a fault diagnosis method based on feature extraction via an unsupervised graph neural network is proposed in this paper. In the proposed method, the K-nearest neighbor approach is adopted to construct a fault graph from the collected signals, thereby providing extra relationship information for fine feature mining. Then, the GraphSAGE model is trained on the constructed graph in an unsupervised way, that is, it does not need labeled data, to extract features of each signal sample. Based on the extracted features, some traditional classifiers are adopted to identify the fault types. The proposed model is evaluated on a rolling bearing dataset provided by the University of Paderborn and a motor rotor dataset collected by a constructed motor rotor system. Compared with some traditional deep learning-based fault diagnosis methods, the proposed model can achieve more accurate diagnoses even when there are only a few labeled samples.},
  archive      = {J_APIN},
  author       = {Feng, Jing and Bao, Shouyang and Xu, Xiaobin and Zhang, Zhenjie and Hou, Pingzhi and Steyskal, Felix and Dustdar, Schahram},
  doi          = {10.1007/s10489-023-04665-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21211-21226},
  shortjournal = {Appl. Intell.},
  title        = {Rotating machinery fault diagnosis based on feature extraction via an unsupervised graph neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex-valued deng entropy. <em>APIN</em>, <em>53</em>(18),
21201–21210. (<a
href="https://doi.org/10.1007/s10489-023-04573-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-valued models have been applied to several fields as a result of their advantages in modeling and processing uncertain information, but few studies have addressed the uncertainty associated with them. It is therefore the main contribution of this paper to propose a complex-valued Deng entropy in the complex-valued evidence theory (One of the complex-valued models). Complex-valued Deng entropy effectively measures the uncertainty of the mass function in the complex-valued framework. It involves uncertainty related to phase angle information, inconsistency and non-specificity. Additionally, complex-valued Deng entropy is a generalization of the Deng entropy and Shannon entropy. In other words, complex-valued Deng entropy may collapse to classical Deng entropy if the complex-valued mass function collapses to a mass function in real space. If the complex-valued mass function collapses into the probability distribution in real space, the complex-valued Deng entropy also collapses into Shannon entropy. There are also a number of numerical examples illustrating the compatibility and effectiveness of the complex-valued Deng entropy. Finally, this article presents a classification experiment involving entropy. Results of classification experiments indicate that results associated with the core parameter of complex-valued Deng entropy are more accurate than results associated with the core parameter of Deng entropy in some data sets.},
  archive      = {J_APIN},
  author       = {Pan, Lipeng and Deng, Yong},
  doi          = {10.1007/s10489-023-04573-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21201-21210},
  shortjournal = {Appl. Intell.},
  title        = {Complex-valued deng entropy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple tangent space projection for motor imagery EEG
classification. <em>APIN</em>, <em>53</em>(18), 21192–21200. (<a
href="https://doi.org/10.1007/s10489-023-04551-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its non-invasiveness and easiness to implement, EEG signals decoding are in base of most based brain computer interfaces (BCI) studies. Given the non-stationary nature of these signals, a preprocessing phase is needed. An interesting idea to perform the preprocessing is the use of spatial covariance matrices. In the last years, spatial covariance matrices based preprocessing was extensively used in electroencephalography (EEG) signal processing and spatial filtering for Motor imagery (MI) BCI. Spatial covariance matrices lie in the Riemannian manifold of Symmetric Positive-Definite (SPD) matrices, therefore, the use of Riemannian geometry is attracting a lot of attention and showing to be simple, robust, and providing good performance. This paper explores the idea of enhancing the information provided to the classifier by the combination of different covariance matrices projections from their native Riemannian space to multiple class-depending tangent spaces. We demonstrate that this new approach provides a significant improvement in model accuracy.},
  archive      = {J_APIN},
  author       = {Omari, Sara and Omari, Adil and Abderrahim, Mohamed},
  doi          = {10.1007/s10489-023-04551-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21192-21200},
  shortjournal = {Appl. Intell.},
  title        = {Multiple tangent space projection for motor imagery EEG classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An imbalanced ensemble learning method based on dual
clustering and stage-wise hybrid sampling. <em>APIN</em>,
<em>53</em>(18), 21167–21191. (<a
href="https://doi.org/10.1007/s10489-023-04650-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification remains a research hotspot and a challenging problem in the field of machine learning. The challenge of imbalanced learning lies not only in class imbalance problem, but also in the class overlapping problem which is complex. However, most of the existing algorithms mainly focus on the former. The limitation prevents the existing methods from breaking through. To address this limitation, this paper proposes an ensemble algorithm based on dual clustering and stage-wise hybrid sampling (DCSHS) to address both class imbalance and class overlapping problems. The DCSHS has three main parts: projection clustering combination framework (PCC), stage-wise hybrid sampling (SHS) and envelope clustering transfer mapping mechanism (CTM). PCC is to create multiple subsets through projective clustering. SHS is to identify the overlapping region of each subset and conduct hybrid sampling. CTM is to explore more information of samples in each subset by combining the clustering and transfer learning. At first, we design a PCC framework guided by Davies-Bouldin clustering effectiveness index (DBI), which is used to obtain high-quality clusters and combine them to obtain a set of cross-complete subsets (CCS) with low overlapping. Secondly, according to the characteristics of subset classes, a SHS algorithm is designed to realize the de-overlapping and balancing of subsets. Finally, an envelope clustering transfer mapping mechanism (CTM) is constructed for all processed subsets by means of transfer learning, thereby reducing class overlapping and explore structural information of samples. Weak classifiers are trained on the balanced subsets, and fused as all the imbalanced ensemble algorithms did. The major advantage of our algorithm is that it can exploit the intersectionality of the CCS to realize the soft elimination of overlapping majority samples, and learn as much information of overlapping samples as possible, thereby enhancing the class overlapping while class balancing. In the experimental section, more than 30 public datasets and over ten representative algorithms are chosen for verification. The experimental results show that the DCSHS is significantly best in terms of anti-overlapping, Recall, F1-M, G-M, AUC, and diversity.},
  archive      = {J_APIN},
  author       = {Li, Fan and Wang, Bo and Wang, Pin and Jiang, Mingfeng and Li, Yongming},
  doi          = {10.1007/s10489-023-04650-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21167-21191},
  shortjournal = {Appl. Intell.},
  title        = {An imbalanced ensemble learning method based on dual clustering and stage-wise hybrid sampling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CNN-RNN architecture to calculate BPM from underwater ECG
samples. <em>APIN</em>, <em>53</em>(18), 21156–21166. (<a
href="https://doi.org/10.1007/s10489-023-04522-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for the generation of heart Beats Per Minute (BPM) from noisy/distorted underwater Electrocardiogram (ECG) samples. We solve this problem using a software based approach via a Convolutional - Recurrent (CNN-RNN) regression model and demonstrate good performance: Mean Absolute Error (MAE): 0.400, Root Mean Square Error (RMSE): 0.653 - for counted underwater heart beats. The neural network is trained on land based ECG samples that have been modified by replicating the signal noise/artefacts seen on under water ECG signals; this process has not yet been reported in literature. This allows the prediction of complex samples without the need for underwater sampling and improves neural network performance. To verify this approach, the trained neural network is tested on underwater ECG samples. This solution requires minimal signal pre-processing and does not require any specific ECG electrode/amplifier design. We have done this to minimise cost and ensure easy deployment. In addition, the techniques discussed here can be applied to any sampled ECG signals and are not hardware specific. This will lead to improved performance where underwater BPM data is required, for example: performance sport; rehabilitation and/or divers in hazardous environments.},
  archive      = {J_APIN},
  author       = {Beckingham, Thomas and Spencer, Joseph and McKay, Kirsty},
  doi          = {10.1007/s10489-023-04522-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21156-21166},
  shortjournal = {Appl. Intell.},
  title        = {CNN-RNN architecture to calculate BPM from underwater ECG samples},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust generalized canonical correlation analysis.
<em>APIN</em>, <em>53</em>(18), 21140–21155. (<a
href="https://doi.org/10.1007/s10489-023-04666-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized canonical correlation analysis (GCCA) has been widely used for classification and regression problems. The key idea of GCCA is to map the data from different views into a common space with the minimum reconstruction error. However, GCCA employs the squared Frobenius norm as a distance metric to find a latent correlated space without a specific strategy to cope with outliers, thus misguiding the GCCA’s training task in real-world applications and leading to suboptimal performance. This inspires us to propose a novel robust formulation for GCCA, namely, GCCA with the p-order ( $$0&lt;p\le 2$$ ) of Frobenius norm minimization (called RGCCA). It is difficult to solve the RGCCA involving the nonsmooth and nonconvex p-order of F-norm terms. Therefore, an efficient iterative algorithm is developed to solve RGCCA, theoretically analyzing its convergence property. In addition, the parameters of RGCCA nicely trade-off between accuracy and training time, a property especially useful for larger samples. Empirical experiments and theoretical analysis prove the effectiveness and robustness of RGCCA on both noiseless and noisy datasets.},
  archive      = {J_APIN},
  author       = {Yan, He and Cheng, Li and Ye, Qiaolin and Yu, Dong-Jun and Qi, Yong},
  doi          = {10.1007/s10489-023-04666-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21140-21155},
  shortjournal = {Appl. Intell.},
  title        = {Robust generalized canonical correlation analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). An efficient evolutionary algorithm based on deep
reinforcement learning for large-scale sparse multiobjective
optimization. <em>APIN</em>, <em>53</em>(18), 21116–21139. (<a
href="https://doi.org/10.1007/s10489-023-04574-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sparse multiobjective optimization problems (SMOPs) widely exist in academic research and engineering applications. The curse of dimensionality and the fact that most decision variables take zero values make optimization very difficult. Sparse features are common to many practical complex problems currently, and using sparse features as a breakthrough point can enable many large-scale complex problems to be solved. We propose an efficient evolutionary algorithm based on deep reinforcement learning to solve large-scale SMOPs. Deep reinforcement learning networks are used for mining sparse variables to reduce the problem dimensionality, which is a challenge for large-scale multiobjective optimization. Then the three-way decision concept is used to optimize decision variables. The emphasis is on optimizing deterministic nonzero variables and continuously mining uncertain decision variables. Experimental results on sparse benchmark problems and real-world application problems show that the proposed algorithm performs well on SMOPs while being highly efficient.},
  archive      = {J_APIN},
  author       = {Gao, Mengqi and Feng, Xiang and Yu, Huiqun and Li, Xiuquan},
  doi          = {10.1007/s10489-023-04574-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21116-21139},
  shortjournal = {Appl. Intell.},
  title        = {An efficient evolutionary algorithm based on deep reinforcement learning for large-scale sparse multiobjective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating relation-specific weights for ConvKB using a
HyperNetwork architecture. <em>APIN</em>, <em>53</em>(18), 21092–21115.
(<a href="https://doi.org/10.1007/s10489-023-04670-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction on knowledge graphs has become increasingly important in various fields, including recommender systems, question answering, and social networks. To address this challenge, ConvKB, a state-of-the-art model in deep learning approaches, has been proposed. However, ConvKB is limited in its ability to exploit general information about relations because it uses the same filters for all relations. Additionally, ConvKB&#39;s design, consisting of a convolution layer and a linear layer, may not provide enough parameters to store necessary information during feature learning. To address these limitations, this paper proposes the ConvHyper model, which combines the ConvKB model with a HyperNetwork architecture to create relation-specific weights for convolutional neural network layers. Specifically, the embedding of relations is linearly combined through a HyperNetwork to generate weights for the base neural network. This HyperNetwork architecture helps reduce the complexity of the search space to identify optimal weights and create a relation-specific weight structure for neural network layers. Experimental results show that ConvHyper significantly improves on all metrics in four well-known datasets. ConvHyper improves on H@1 up to 5.5% and achieves better results on other metrics, ranging from 0.8% to 1%. We also compared ConvHyper with other CNN-based models and found that many metrics have better values, particularly from 1.1% to 4.7% on the H@10 metric. Furthermore, we analyzed the influence of hyperparameters and found that the learning rate and negative samples significantly affect the model&#39;s results.},
  archive      = {J_APIN},
  author       = {Le, Thanh and Nguyen, Duy and Le, Bac},
  doi          = {10.1007/s10489-023-04670-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21092-21115},
  shortjournal = {Appl. Intell.},
  title        = {Generating relation-specific weights for ConvKB using a HyperNetwork architecture},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LSTMAE-DWSSLM: A unified approach for imbalanced time
series data classification. <em>APIN</em>, <em>53</em>(18), 21077–21091.
(<a href="https://doi.org/10.1007/s10489-023-04642-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced class distribution of time series data often results in bias in the classification of surfaces, the classifier cannot usually achieve the best classification performance. When the classifier fails to detect a minority class of data, it indicates that the model has failed. In the real world, there are serious consequences when models fail to classify the minority classes of data (rare events). In this paper, we propose a new model that integrates a long and short-term memory (LSTM)-based autoencoder (AE) network with dense weighted small spheres and large margins (LSTMAE-DWSSLM) for the classification of imbalanced time series. The long short-term memory autoencoder network can learn temporally dependent feature representations from unlabeled data. The optimal feature representation can improve the classification performance. The density weight function is applied to the small spheres and large margins (SSLM) classifier to effectively handle the imbalanced classification task. Finally, two metrics are chosen to evaluate the performance of the model. Experimental results on multiple UCR datasets and EEG datasets validate the significant advantages of our proposed model. Accordingly, our model is applied to the actual datasets, good results are obtained in the plant electrical signal datasets, which further validated the practicality of the method. The comprehensive experimental results show that for imbalanced time series classification, the proposed method outperforms similar methods.},
  archive      = {J_APIN},
  author       = {Liu, Jingjing and Yao, Jiepeng and Zhou, Qiao and Wang, Zhongyi and Huang, Lan},
  doi          = {10.1007/s10489-023-04642-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21077-21091},
  shortjournal = {Appl. Intell.},
  title        = {LSTMAE-DWSSLM: A unified approach for imbalanced time series data classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved α-GAN architecture for generating 3D connected
volumes with an application to radiosurgery treatment planning.
<em>APIN</em>, <em>53</em>(18), 21050–21076. (<a
href="https://doi.org/10.1007/s10489-023-04567-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D α-GAN by incorporating various architectural enhancements. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D α-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D α-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.},
  archive      = {J_APIN},
  author       = {Jafari, Sanaz Mohammad and Cevik, Mucahit and Basar, Ayse},
  doi          = {10.1007/s10489-023-04567-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21050-21076},
  shortjournal = {Appl. Intell.},
  title        = {Improved α-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient constrained large-scale multi-objective
optimization based on reference vector-guided evolutionary algorithm.
<em>APIN</em>, <em>53</em>(18), 21027–21049. (<a
href="https://doi.org/10.1007/s10489-023-04663-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale multi-objective optimization problem exist widely in reality while they have complex constraints. The simultaneous effect of the large-scale decision variables and the complexity of constraints makes the traditional multi-objective evolutionary algorithm face great challenges. For the large-scale of decision variables, some reference vector-guided, competitive group optimization-based and pairwise child generation-based algorithms have improved the search efficiency of constrained LSMOPs. However, these algorithms encounter difficulties in handling large-scale decision variables and complex constraints at the same time. In this paper, a reference vector-guided with dominance co-evolutionary multi-objective algorithm is proposed to solve constrained large-scale multi-objective problems. First, a reference vector is employed to guide several sub-populations with a fixed number of neighborhood solutions. Then, a new environmental selection is constructed using the angle penalty distance with dominance relationship. This new environmental selection strategy greatly enhances selection pressure. At the same time, a co-evolutionary constraint handling technology is applied to efficiently span the infeasible region. The proposed algorithm is evaluated on constrained large-scale multi-objective problems with 100, 500 and 1000 decision variables. In addition, the impact of each component of the proposed algorithm is examined for the overall performance of the algorithm and tested in a practical application in microgrids. The experimental results demonstrate the effectiveness of the algorithm in constrained large-scale multi-objective optimization.},
  archive      = {J_APIN},
  author       = {Fan, Chaodong and Wang, Jiawei and Yang, Laurence T. and Xiao, Leyi and Ai, Zhaoyang},
  doi          = {10.1007/s10489-023-04663-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21027-21049},
  shortjournal = {Appl. Intell.},
  title        = {Efficient constrained large-scale multi-objective optimization based on reference vector-guided evolutionary algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure guided network for human pose estimation.
<em>APIN</em>, <em>53</em>(18), 21012–21026. (<a
href="https://doi.org/10.1007/s10489-023-04521-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have an impressive ability to reliably perceive pose with semantic descriptions (e.g. both arm up or left leg bent). To leverage the transitive structure characteristics for human pose estimation, we explore the part descriptor that qualitatively describe the structure consistency on various appearance. Meantime, we utilize the fixed bone constraint to fully exploit structure knowledge. In this paper, we propose an effective network of jointly modeling part descriptor and bone heatmap as structure information to dynamically learn from compositional features. Specially, this part descriptor distill the structure consistency as external guidance via feature injection, and the introduced bone detection as internal guidance through multi-level feature fusion. Hence the proposed method enables the network effectively incorporating higher level structure into lower level keypoint detection models, which leads to extract more robust features for the optimal pose estimation. The effectiveness of proposed method has been evaluated on LSP, MPII, LIP, COCO and CrowdPose dataset. The experimental results demonstrate that it can outperform most of the state-of-the-art methods on the widely used benchmarks with less complexities.},
  archive      = {J_APIN},
  author       = {Chen, Yilei and Xie, Xuemei and Yin, Wenjie and Li, Bo’ao and Li, Fu},
  doi          = {10.1007/s10489-023-04521-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {21012-21026},
  shortjournal = {Appl. Intell.},
  title        = {Structure guided network for human pose estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EAdderSR: Enhanced AdderSR for single image super
resolution. <em>APIN</em>, <em>53</em>(18), 20998–21011. (<a
href="https://doi.org/10.1007/s10489-023-04536-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replacing multiplication with addition can effectively reduce the computational complexity. Based on this idea, adder neural networks (AdderNets) are proposed. Thereafter, AdderNets are applied to super-resolution (SR) task to obtain AdderSR, which significantly reduces the energy consumption caused by SR models. However, the weak fitting ability of AdderNets makes AdderSR only applicable to the low-complexity pixel-wise loss, and the performance of the model drops sharply when the high-complexity perceptual loss is used. Enhanced AdderSR (EAdderSR) is proposed to overcome the limitations of AdderSR in SR tasks. Specifically, current adder networks have serious gradient precision loss problem, which affects the training stability. The normalization layer is adjusted to normalize the output of the adder layer to a reasonably narrow range, which can reduce the amount of precision loss. Then, a coarse-grained knowledge distillation (CGKD) method is developed to give adder networks an efficient guidance to reduce the fitting burden. The experimental results show that the proposed method not only further improves the performance of adder networks, but also ensures the quality of the output results when the complexity of the loss function increases.},
  archive      = {J_APIN},
  author       = {Song, Jie and Yi, Huawei and Xu, Wenqian and Li, Xiaohui and Li, Bo and Liu, Yuanyuan},
  doi          = {10.1007/s10489-023-04536-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20998-21011},
  shortjournal = {Appl. Intell.},
  title        = {EAdderSR: Enhanced AdderSR for single image super resolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). AMMGAN: Adaptive multi-scale modulation generative
adversarial network for few-shot image generation. <em>APIN</em>,
<em>53</em>(18), 20979–20997. (<a
href="https://doi.org/10.1007/s10489-023-04559-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have recently advanced image generation by exploiting the valuable information within immense training data, but they struggle to synthesize new images for rare categories that are scarce enough to cover some visual concepts. Image generation based on few-shot learning is developed to capture the generalizable generative feature from limited data to produce new images for unseen categories. Existing few-shot methods focus on the high-level semantic difference between conditional images and fuse a generative feature based on the semantic metric. However, it ignores the impact of semantic information underlying different level feature subspaces throughout the generation process, leading to the degradation of visual quality when the diversity of synthetic images improves. In this work, we propose a novel Adaptive Multi-scale Modulation Generative Adversarial Network (AMMGAN) for few-shot image generation, leveraging the U-Net with skip connections to exploit multi-scale semantic metric information. Specifically, an adaptive self-metric fusion module is introduced at the junction between the encoder and decoder of generator, which measures the pixel-wise semantic information of conditional images as a self-metric attention map in each level of deep features to fuse a general feature of interest and adaptively adjust the mean and variance of the fused feature based on the high-level semantic feature of decoder. Meanwhile, the part network of discriminator is developed to co-learn with the generator to formulate a reference-metric modulation code embedding channel-wise metric information, which is integrated into the decoder of generator by learnable residual transformation to refine the unsampled fused feature from the channel dimension. Extensive experiments on several benchmark datasets demonstrate the effectiveness of AMMGAN on few-shot image generation and downstream visual classification tasks.},
  archive      = {J_APIN},
  author       = {Li, Wenkuan and Xu, Wenyi and Wu, Xubin and Wang, Qianshan and Lu, Qiang and Song, Tianxia and Li, Haifang},
  doi          = {10.1007/s10489-023-04559-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20979-20997},
  shortjournal = {Appl. Intell.},
  title        = {AMMGAN: Adaptive multi-scale modulation generative adversarial network for few-shot image generation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ST-VQA: Shrinkage transformer with accurate alignment for
visual question answering. <em>APIN</em>, <em>53</em>(18), 20967–20978.
(<a href="https://doi.org/10.1007/s10489-023-04564-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While transformer-based models have been remarkably successful in the field of visual question answering (VQA), their approaches to achieve vision and language feature alignment are simple and coarse. In recent years, this shortcoming has been further amplified with the popularity of vision-language pretraining, resulting in the slow development of an effective architecture for multimodal alignment. Based on this, we propose the shrinkage transformer-visual question answering (ST-VQA) framework. It aims to achieve more accurate multimodal alignment than the standard transformer. First, the ST-VQA framework uses the region feature of an image as a visual representation. Secondly, between the different Transformer layers, the ST-VQA framework reduces the number of visual regions in the transformer by feature fusion and ensures the difference between new regions by contrast loss. Finally, visual and textual features are fused and used for decision making answers. Many experiments demonstrate that without pretraining, our proposed method achieves better performance than the standard transformer and outperforms partial state-of-the-art methods on the VQA-v2 dataset.},
  archive      = {J_APIN},
  author       = {Xia, Haiying and Lan, Richeng and Li, Haisheng and Song, Shuxiang},
  doi          = {10.1007/s10489-023-04564-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20967-20978},
  shortjournal = {Appl. Intell.},
  title        = {ST-VQA: Shrinkage transformer with accurate alignment for visual question answering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Financial time series forecasting based on momentum-driven
graph signal processing. <em>APIN</em>, <em>53</em>(18), 20950–20966.
(<a href="https://doi.org/10.1007/s10489-023-04563-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting is important for social development and industrial production in today’s complex and fluctuating economic environment. The nonlinearity and non-stationarity of financial time series (FTS) data make it difficult to achieve accurate prediction. This work proposes a forecasting method for the return of FTS based on the emerging field of graph signal processing (GSP). The proposed method makes forecasting decisions based on the similarity between the current tendency and historical tendencies of FTS data. First, a topological graph is created based on the underlying structural relationship of different historical FTS datasets. Subsequently, spectral clustering is used to select historical datasets that are similar to the current dataset, and then the future values are predicted by weighted averaging the selected historical samples. In addition, a momentum-driven method is introduced to improve the robustness of the forecasting results. Finally, the proposed method is extended to a collaborative forecasting framework, where auxiliary macroeconomic data is introduced to improve the forecasting accuracy. The superiority of the proposed methods is verified by a set of numerical experiments with different stock indices.},
  archive      = {J_APIN},
  author       = {Zhang, Shengen and Ma, Xu and Fang, Zhen and Pan, Huifeng and Yang, Guangbing and Arce, Gonzalo R.},
  doi          = {10.1007/s10489-023-04563-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20950-20966},
  shortjournal = {Appl. Intell.},
  title        = {Financial time series forecasting based on momentum-driven graph signal processing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searching sharing relationship for instance segmentation
decoder. <em>APIN</em>, <em>53</em>(18), 20938–20949. (<a
href="https://doi.org/10.1007/s10489-022-04434-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is a typical visual task that requires per-pixel mask prediction with a category label for each instance. For the decoder in instance segmentation network, parallel branches or towers are commonly adopted to deal with instance- and dense-level predictions. However, this parallelism ignores inter-branch and inner-branch relationships. Besides, how the different branches are connected is unclear, which is difficult to explore manually in practice. To address the above issues, we introduce Neural Architecture Search (NAS) to automatically search for hardware and memory-friendly feature sharing branch. Concretely, applying to instance segmentation, we design a search space considering both operations and sharing connections of parallel branches. Through a tailored reinforcement learning(RL) paradigm, we can efficiently search multiple architectures with different shared patterns and tap more feature selection possibilities. Our method is generically useful and can be transferred to analogous multi-task networks. The searched architecture shares features in the middle of the head branches and utilizes instance-level head features to generate pixel-level predictions. Extensive experiments demonstrate the effectiveness and surpass classical parallel decoder networks, exceeding BlendMask by 1.2% on bounding box mAP and 0.9% on segmentation mAP.},
  archive      = {J_APIN},
  author       = {Xi, Yuling and Wang, Ning and Wan, Shaohua and Wang, Xiaoming and Wang, Peng and Zhang, Yanning},
  doi          = {10.1007/s10489-022-04434-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20938-20949},
  shortjournal = {Appl. Intell.},
  title        = {Searching sharing relationship for instance segmentation decoder},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal-difference emphasis learning with regularized
correction for off-policy evaluation and control. <em>APIN</em>,
<em>53</em>(18), 20917–20937. (<a
href="https://doi.org/10.1007/s10489-023-04579-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-policy learning, where the goal is to learn about a policy of interest while following a different behavior policy, constitutes an important class of reinforcement learning problems. It is well-known that emphatic temporal-difference (TD) learning is a pioneering off-policy reinforcement learning method involving the use of the followon trace. Although the gradient emphasis learning (GEM) algorithm has recently been proposed to fix the problems of unbounded variance and large emphasis approximation error introduced by the followon trace from the perspective of stochastic approximation. This approach, however, is limited to a single gradient-TD2-style update instead of considering the update rules of other GTD algorithms. Overall, it remains an open question on how to better learn the emphasis for off-policy learning. In this paper, we rethink GEM and investigate introducing a novel two-time-scale algorithm called TD emphasis learning with gradient correction (TDEC) to learn the true emphasis. Further, we regularize the update to the secondary learning process of TDEC and obtain our final TD emphasis learning with regularized correction (TDERC) algorithm. We then apply the emphasis estimated by the proposed emphasis learning algorithms to the value estimation gradient and the policy gradient, respectively, yielding the corresponding emphatic TD variants for off-policy evaluation and actor-critic algorithms for off-policy control. Finally, we empirically demonstrate the advantage of the proposed algorithms on a small domain as well as challenging Mujoco robot simulation tasks. Taken together, we hope that our work can provide new insights into the development of a better alternative in the family of off-policy emphatic algorithms.},
  archive      = {J_APIN},
  author       = {Cao, Jiaqing and Liu, Quan and Wu, Lan and Fu, Qiming and Zhong, Shan},
  doi          = {10.1007/s10489-023-04579-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20917-20937},
  shortjournal = {Appl. Intell.},
  title        = {Temporal-difference emphasis learning with regularized correction for off-policy evaluation and control},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Deep learning in medical image super resolution: A review.
<em>APIN</em>, <em>53</em>(18), 20891–20916. (<a
href="https://doi.org/10.1007/s10489-023-04566-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution (SR) reconstruction is a hot topic in medical image processing. SR implies reconstructing corresponding high-resolution (HR) images from observed low-resolution (LR) images or image sequences. In recent years, significant breakthroughs in SR based on deep learning have been made, and many advanced results have been achieved. However, there is a lack of review literature that summarizes the field’s current state and provides an outlook on future developments. Therefore, we provide a comprehensive summary of the literature on medical image SR (MedSR) based on deep learning since 2018 in five aspects: (1) The SR problem of medical images is described, and the methods of image degradation are summarized. (2) We divide the existing studies into three categories: two-dimensional image SR (2DISR), three-dimensional image SR (3DISR), and video SR (VSR). Each category is subdivided. We analyze the network structure and method characteristics of typical methods. (3) Existing SR reconstruction quality evaluation metrics are presented in detail. (4) The application of MedSR methods based on deep learning is discussed. (5) We discuss the challenges of this phase and point out valuable research directions.},
  archive      = {J_APIN},
  author       = {Yang, Hujun and Wang, Zhongyang and Liu, Xinyao and Li, Chuangang and Xin, Junchang and Wang, Zhiqiong},
  doi          = {10.1007/s10489-023-04566-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20891-20916},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning in medical image super resolution: A review},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-template global re-detection based on gumbel-softmax
in long-term visual tracking. <em>APIN</em>, <em>53</em>(18),
20874–20890. (<a
href="https://doi.org/10.1007/s10489-023-04584-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-term visual tracking, target occlusion and out-of-view are common problems that lead to target loss. Adding a re-detection module to the short-term tracking algorithm is a general solution. However, the existing re-detection methods have limited accuracy, a large amount of calculation, and serious error accumulation, which seriously affect the algorithm’s long-term tracking ability. This paper proposes a flexible and accurate global re-detection module that enhances long-term tracking performance of the algorithm while improving re-detection speed. The proposed method innovatively uses three templates for global sampling to improve the re-detection accuracy. Then, Gumbel-Softmax is introduced into the re-detection module for accurate sampling, and a less number of target candidate boxes are output, which reduces the amount of computation. Finally, color feature is added to assist cosine similarity to locate the final target position more accurately. Four tracking algorithms are selected as benchmark algorithms (STMTrack, KeepTrack, SuperDiMP, and DiMP). The experimental results on five datasets (UAV123, UAV20L, LaSOT, VOT2018-LT, and VOT2020-LT) show that the long-term tracking ability of these algorithms can be effectively improved after adding the re-detection module. Especially on UAV20L, the accuracy and success rate of the improved STMTrack can be increased by 15.6% and 11.6% respectively.},
  archive      = {J_APIN},
  author       = {Hou, Zhiqiang and Ma, Jingyuan and Yu, Wangsheng and Yang, Zhilong and Ma, Sugang and Fan, Jiulun},
  doi          = {10.1007/s10489-023-04584-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20874-20890},
  shortjournal = {Appl. Intell.},
  title        = {Multi-template global re-detection based on gumbel-softmax in long-term visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Amplitude transformed quantum convolutional neural network.
<em>APIN</em>, <em>53</em>(18), 20863–20873. (<a
href="https://doi.org/10.1007/s10489-023-04581-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of quantum neural networks (QNN), several quantum simulations of convolutional neural networks (CNN) have been proposed. Among them, Google has proposed three quantum convolutional neural network (QCNN) models, but its purely QCNN model suffers from slow convergence and low training efficiency. In this work, we design low-depth parameterized quantum circuits with only two quantum bits interacting and construct a QCNN framework with lower depths, fewer parameters and global correlation. Based on this, we propose an Amplitude Transformed Quantum Convolutional Neural Network (ATQCNN). Experiments show that our model achieves 100% and 97.92% accuracy and faster convergence on the quantum cluster state and CICMalDroid2020 datasets compared to the purely QCNN proposed by Google. In particular, the required parameters and depth of ATQCNN are in reduced by about 27% for the same scale of qubits. It will be more suitable for current noisy intermediate-scale quantum (NISQ) devices.},
  archive      = {J_APIN},
  author       = {Di, Shiqin and Xu, Jinchen and Shu, Guoqiang and Feng, Congcong and Ding, Xiaodong and Shan, Zheng},
  doi          = {10.1007/s10489-023-04581-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20863-20873},
  shortjournal = {Appl. Intell.},
  title        = {Amplitude transformed quantum convolutional neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image classification based on weighted nonconvex low-rank
and discriminant least squares regression. <em>APIN</em>,
<em>53</em>(18), 20844–20862. (<a
href="https://doi.org/10.1007/s10489-023-04541-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifiers based on least squares regression (LSR) are effective in multi-classification tasks. However, there are two main problems that greatly limit its performance. First of all, most of the existing methods use limited projections and cause a lot of loss of discriminative information, but excessive use of relaxed labels may lead to overfitting. Additionally, the traditional nuclear norm treats the weights of each singular value equally, and cannot fully discuss the influence of different weights on the rank. In order to solve these problems and improve the classification performance, this paper proposes a multi-class image classification method based on weighted nonconvex low-rank and discriminative least squares regression (WNLRDLSR). Specially, using relaxed labels to replace zero-one labels, which allows the margins from different classes of samples to be widened while enhancing the intra-class compactness and similarity, thus making the resulting projections more discriminative; Furthermore, introducing the weighted nonconvex low-rank constraint in the least squares regression model, applying the weighted nonconvex low-rank norm to fully explore the effect of different rank components on the label matrix while being close to the original low-rank hypothesis. Experiments show that it helps to learn more distinguished regression projections to achieve better classification performance. The classification accuracy on different face, object and handwriting datasets are higher than that of the contrastive methods, and experiments show that the proposed WNLRDLSR is superior to many state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhong, Kunyan and Liu, Jinglei},
  doi          = {10.1007/s10489-023-04541-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20844-20862},
  shortjournal = {Appl. Intell.},
  title        = {Image classification based on weighted nonconvex low-rank and discriminant least squares regression},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An augmented attention-based lightweight CNN model for plant
water stress detection. <em>APIN</em>, <em>53</em>(18), 20828–20843. (<a
href="https://doi.org/10.1007/s10489-023-04583-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning techniques specifically the Convolutional Neural Networks (CNNs) have reported outstanding results from the application for plant water stress detection based on computer vision system compared to other machine learning methods. However, the size of the conventional CNN models is generally too large for its deployment on resource-limited devices such as mobile smartphone or embedded devices. In this study, a lightweight CNN is proposed by incorporating attention mechanism as an augmentation module into the model. The model was trained, validated, and tested using plant images of Setaria grass undergone three water stress treatments. Experimental results show that the proposed method improved the interclass precision, recall, F1-score, and the overall accuracy by more than 9%. Compared to the established lightweight CNN models, the proposed lightweight CNN achieved faster computational time with comparable parameters. In addition, the proposed lightweight model is also efficient when trained on small plant dataset with limited overfitting.},
  archive      = {J_APIN},
  author       = {Kamarudin, Mohd Hider and Ismail, Zool Hilmi and Saidi, Noor Baity and Hanada, Kousuke},
  doi          = {10.1007/s10489-023-04583-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20828-20843},
  shortjournal = {Appl. Intell.},
  title        = {An augmented attention-based lightweight CNN model for plant water stress detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QCBA: Improving rule classifiers learned from quantitative
data by recovering information lost by discretisation. <em>APIN</em>,
<em>53</em>(18), 20797–20827. (<a
href="https://doi.org/10.1007/s10489-022-04370-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prediscretisation of numerical attributes which is required by some rule learning algorithms is a source of inefficiencies. This paper describes new rule tuning steps that aim to recover lost information in the discretisation and new pruning techniques that may further reduce the size of rule models and improve their accuracy. The proposed QCBA method was initially developed to postprocess quantitative attributes in models generated by Classification based on associations (CBA) algorithm, but it can also be applied to the results of other rule learning approaches. We demonstrate the effectiveness on the postprocessing of models generated by five association rule classification algorithms (CBA, CMAR, CPAR, IDS, SBRL) and two first-order logic rule learners (FOIL2 and PRM). Benchmarks on 22 datasets from the UCI repository show smaller size and the overall best predictive performance for FOIL2+QCBA compared to all seven baselines. Postoptimised CBA models have a better predictive performance compared to the state-of-the-art rule learner CORELS in this benchmark. The article contains an ablation study for the individual postprocessing steps and a scalability analysis on the KDD’99 Anomaly detection dataset.},
  archive      = {J_APIN},
  author       = {Kliegr, Tomáš and Izquierdo, Ebroul},
  doi          = {10.1007/s10489-022-04370-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20797-20827},
  shortjournal = {Appl. Intell.},
  title        = {QCBA: Improving rule classifiers learned from quantitative data by recovering information lost by discretisation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPL-LDP: A label distribution propagation method for
semi-supervised partial label learning. <em>APIN</em>, <em>53</em>(18),
20785–20796. (<a
href="https://doi.org/10.1007/s10489-023-04548-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning learns from examples represented by a single instance while associated with multiple candidate labels, among which only one valid label resides. However, in real-world applications, collecting candidate label sets for all training examples is costly. As unlabeled data are being considered as a indispensable ingredient for low-cost computing, the semi-supervised partial label learning underlying propagating labels between partially labeled and unlabeled instances has grown progressively momentous. Nevertheless, the noisy information carried by false-positive instances hides in the candidate label sets and is propagated as well. In this work, we propose a label distribution propagation based approach, namely Spl-ldp, which can jointly learn from partially labeled and unlabeled instances. Specifically, the label distribution of partially labeled instances is mined based on the topological information. Instead of directly logic label propagation, an iterative label distribution propagation procedure between partially labeled and unlabeled instances is subsequently employed to leverage the data distribution of unlabeled instances. Unseen instances are classified with the minimum reconstruction error on the whole data sets. Extensive experiments on five real-world data sets show that the proposed Spl-ldp method performs favorably against baselines.},
  archive      = {J_APIN},
  author       = {Song, Moxian and Sun, Chenxi and Cai, Derun and Hong, Shenda and Li, Hongyan},
  doi          = {10.1007/s10489-023-04548-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20785-20796},
  shortjournal = {Appl. Intell.},
  title        = {SPL-LDP: A label distribution propagation method for semi-supervised partial label learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised feature learning based on autoencoder for
epileptic seizures prediction. <em>APIN</em>, <em>53</em>(18),
20766–20784. (<a
href="https://doi.org/10.1007/s10489-023-04582-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is one of the most common neurological diseases in the world. It’s essential to predict epileptic seizures since it can provide patients with enough time for timely treatment. Currently, electroencephalogram (EEG) analysis has been adopted as the most popular method of epileptic seizures prediction, of which one key element is extracting important EEG features. Conventional technologies of EEG analysis mostly utilize supervised learning methods with a mass of labeled data, which bring leakage risks to healthcare data. In addition, it’s difficult to achieve high accuracy of epileptic seizure prediction based on unsupervised learning methods with huge network parameters. Furthermore, the insufficiency of preictal data leads to overfitting challenges for deep learning algorithms. To deal with this problem, a data augmentation method based on randomly translation strategy is proposed to address the insufficient datasets without additional noise. In this paper, we propose an improved unsupervised feature learning method, residual convolution variational autoencoder with randomly translation strategy (RTS-RCVAE). Residual learning is embedded in the VAE model, which improves the model’s ability to converge in the unsupervised learning stage and reduces the loss of useful information. The proposed model is trained and verified via simulation using the public dataset CHB-MIT. The results indicate that the proposed model achieves a high accuracy rate of 98.43% and a false alarm rate of 0.009.},
  archive      = {J_APIN},
  author       = {He, Peng and Wang, Linhai and Cui, Yaping and Wang, Ruyan and Wu, Dapeng},
  doi          = {10.1007/s10489-023-04582-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20766-20784},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised feature learning based on autoencoder for epileptic seizures prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised medical imaging segmentation with soft
pseudo-label fusion. <em>APIN</em>, <em>53</em>(18), 20753–20765. (<a
href="https://doi.org/10.1007/s10489-023-04569-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation is an essential task in modern medical imaging analysis. Since the scarcity of labeled pixel-level annotations often limits its wide applications, recent studies have proposed Semi-supervised Learning (SSL) frameworks to tackle this issue. Among them, the paradigm of pseudo-labeling, derived from SSL of natural images, has been popularly transferred on various medical datasets. Despite its promising results, we observe that many medical images’ regions are ambiguous, where pixels are challenging to be categorized as a specific class compared to natural images. Constructing hard pseudo-labels for these regions is consequently unintuitive and prone to be of low quality. To this end, we develop a novel SSL framework with the proposed Soft Pseudo-label Fusion strategy (called ”SPFSeg”). It can produce refined soft pseudo-labels, harboring the association knowledge between regions of interest (ROIs) and backgrounds while preserving the ”low-density” assumption of vanilla pseudo-labeling. These soft pseudo-labels can further establish potent supervision signals for unlabeled images, helping the segmentation model learn better feature representations. Through extensive experiments conducted on various datasets to evaluate the effectiveness of SPFSeg, our results manifest that its performance can surpass previous state-of-the-art semi-supervised frameworks on CXR-2014, ISIC-2017, and BUL-2020.},
  archive      = {J_APIN},
  author       = {Li, Xiaoqiang and Wu, Yuanchen and Dai, Songmin},
  doi          = {10.1007/s10489-023-04569-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20753-20765},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised medical imaging segmentation with soft pseudo-label fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classifier subset selection based on classifier
representation and clustering ensemble. <em>APIN</em>, <em>53</em>(18),
20730–20752. (<a
href="https://doi.org/10.1007/s10489-023-04572-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble pruning can improve the performance and reduce the storage requirements of an integration system. Most ensemble pruning approaches remove low-quality or redundant classifiers by evaluating the classifiers’ competence and relationships via their predictions. However, finding the best way to represent classifiers and create ensemble diversity is still a worthy research problem in the ensemble pruning field. To confront this issue, we discuss whether properties other than predictions can represent classifiers and propose a new classifier selection method, classifier-representation- and clustering-ensemble-based ensemble pruning (CRCEEP). In the proposed method, two new classifier-representation-learning methods, local-space- and relative-transformation-based representation, are proposed to obtain more information about classifiers. CRCEEP incorporates the clustering ensemble method to group classifiers and prune redundant learners. Finally, accurate and diverse classifiers are integrated to improve classification performance. Extensive experiments were carried out on UCI datasets, and the experimental results verify CRCEEP’s effectiveness and the necessity of classifier representation.},
  archive      = {J_APIN},
  author       = {Li, Danyang and Zhang, Zhuhong and Wen, Guihua},
  doi          = {10.1007/s10489-023-04572-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20730-20752},
  shortjournal = {Appl. Intell.},
  title        = {Classifier subset selection based on classifier representation and clustering ensemble},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIVLC: Improving the performance of co-training by
sufficient-irrelevant views and label consistency. <em>APIN</em>,
<em>53</em>(18), 20710–20729. (<a
href="https://doi.org/10.1007/s10489-023-04611-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most successful paradigms for semi-supervised learning, co-training trains two classifiers through two views, and it selects unlabeled samples and adds them to the labeled sample set iteratively. The traditional co-training method requires two natural self-sufficient and independent views, which is too strict and limits the applicability of co-training. Although several methods have been proposed to relax this requirement via manual view partition, they only consider difference of generated views and neglect relationship among them. Moreover, sample labels predicted by different classifiers in co-training may be not consistent but this consistency has not been fully exploited by existing methods. To solve these issues, we design a method that uses sufficient-irrelevant views and label consistency (SIVLC) to improve the performance of co-training. SIVLC can manually generate any number of views through the concept of sufficient-irrelevant views to expand the application scope of co-training. Moreover, it takes into account performance difference of classifiers and hence the label consistency of labeled data is used to measure the weight of classifiers while that of unlabeled data is applied to select unlabeled samples with high confidence. Since the performance of classifiers during training may fluctuate, a decision strategy of verification set is set in SIVLC. Four groups of experiments with 18 data sets are conducted and the results reveal that the proposed method is more effective than some state-of-the-art ones.},
  archive      = {J_APIN},
  author       = {Gong, Yanlu and Wu, Quanwang},
  doi          = {10.1007/s10489-023-04611-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20710-20729},
  shortjournal = {Appl. Intell.},
  title        = {SIVLC: Improving the performance of co-training by sufficient-irrelevant views and label consistency},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From gram to attention matrices: A monotonicity constrained
method for eeg-based emotion classification. <em>APIN</em>,
<em>53</em>(18), 20690–20709. (<a
href="https://doi.org/10.1007/s10489-023-04561-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a parameter efficient attention module is developed for the task of emotion classification as well as improved model interpretability based on EEG source data. Inspired by the self-attention mechanism used in transformers, we propose a Monotonicity Constrained Attention Module (MCAM) that can help incorporate different priors easily on the monotonicity when converting Gram matrices from deep features into attention matrices for better feature refinement. In the subject-dependent classification task, MCAM achieves 95.0% mean prediction accuracy on four classification task with DEAP and 91.1% mean prediction accuracy on three classification task with SEED. On both datasets, MCAM is shown comparable to state-of-the-art attention modules in terms of boosting the backbone network’s predictive performance while requiring significantly fewer parameters. A thorough analysis is also performed on tracking the different effects inserted modules have on the backbone model’s behavior. For example, visualization and analysis techniques are presented to examine changes in spatial attention patterns reflected via kernel weights, change in prediction performance when different frequency information is filtered out, or changes that occur when different amplitude information is suppressed; as well as how different models change their predictions along linear morphisms between two samples belonging to different emotion categories. The results help to reveal what different modules learn and use during prediction, and can also provide guidance when applying them to specific applications.},
  archive      = {J_APIN},
  author       = {Kuang, Dongyang and Michoski, Craig and Li, Wenting and Guo, Rui},
  doi          = {10.1007/s10489-023-04561-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20690-20709},
  shortjournal = {Appl. Intell.},
  title        = {From gram to attention matrices: A monotonicity constrained method for eeg-based emotion classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised generative learning for sequential data
prediction. <em>APIN</em>, <em>53</em>(18), 20675–20689. (<a
href="https://doi.org/10.1007/s10489-023-04578-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many real world applications, such as stock price prediction and video frame synthesis, sequential data prediction is a fundamental and challenging task. Considering the temporal features of sequential data, existing approaches generally adopt recurrent neural network and its variants for the prediction. However, for sequences with complex structure, these approaches cannot guarantee to obtain promising results. In this paper, to address the above issue, we formulate sequential data prediction as a self-supervised generative learning problem. Concretely, we design a generator to learn the distribution of the sequential data and generate the predicted values, as well as a discriminator to judge whether or not the input sequential data are real or fake. Based on this proposed framework and the adversarial learning scheme, we develop the corresponding networks for vector inputs and high-order tensor inputs, respectively, which are respectively named vector generative network (VGN) and high-order tensor generative network (HTGN). Extensive experiments on five stock price prediction datasets and two video frame prediction datasets demonstrate the effectiveness of our framework, and its advantages over the state-of-the-art approaches. Our main code and the used data have been shared at https://github.com/xsavagek/SSGL .},
  archive      = {J_APIN},
  author       = {Xu, Ke and Zhong, Guoqiang and Deng, Zhaoyang and Zhang, Kang and Huang, Kaizhu},
  doi          = {10.1007/s10489-023-04578-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20675-20689},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised generative learning for sequential data prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised pairwise-sample resistance model for
few-shot classification. <em>APIN</em>, <em>53</em>(18), 20661–20674.
(<a href="https://doi.org/10.1007/s10489-023-04525-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional supervised learning models rely on high-quality labeled samples heavily. In many fields, training the model on limited labeled samples will result in a weak generalization ability of the model. To address this problem, we propose a novel few-shot image classification method by self-supervised and metric learning, which contains two training steps: (1) Training the feature extractor and projection head with strong representational ability by self-supervised technology; (2) taking the trained feature extractor and projection head as the initialization meta-learning model, and fine-tuning the meta-learning model by the proposed loss functions. Specifically, we construct the pairwise-sample meta loss (ML) to consider the influence of each sample on the target sample in the feature space, and propose a novel regularization technique named resistance regularization based on pairwise-samples which is utilized as an auxiliary loss in the meta-learning model. The model performance is evaluated on the 5-way 1-shot and 5-way 5-shot classification tasks of mini-ImageNet and tired-ImageNet. The results demonstrate that the proposed method achieves the state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Li, Weigang and Xie, Lu and Gan, Ping and Zhao, Yuntao},
  doi          = {10.1007/s10489-023-04525-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {18},
  pages        = {20661-20674},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised pairwise-sample resistance model for few-shot classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Medical image security and authenticity via dual encryption.
<em>APIN</em>, <em>53</em>(17), 20647–20659. (<a
href="https://doi.org/10.1007/s10489-023-04550-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since medical images include sensitive patient information, security is the top priority during transmission. In addition to protecting patient data from potential criminals, security helps to confirm the field staff’s identity. However, many medical institutions still need to adopt advanced security measures. In this paper, a new dual encryption method is proposed that implements blowfish and signcryption in a certificateless generalized form. The proposed method has an advantage over other methods due to its computational cost-effectiveness and speed. The performance measurements used to assess a proposed strategy’s effectiveness are PSNR, entropy, MSE, correlation coefficient (CC), and time taken. We obtain a high PSNR value of 57.72 and a low time requirement of 42 seconds on average. Combining blowfish and certificateless signcryption iTqnto one double encryption scheme that is computationally secure, fast, and easy to implement. It would help push hospitals toward a cost-effective image security environment.},
  archive      = {J_APIN},
  author       = {Nampalle, Kishore Babu and Manhas, Shriansh and Raman, Balasubramanian},
  doi          = {10.1007/s10489-023-04550-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20647-20659},
  shortjournal = {Appl. Intell.},
  title        = {Medical image security and authenticity via dual encryption},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual learning in an easy-to-hard manner. <em>APIN</em>,
<em>53</em>(17), 20626–20646. (<a
href="https://doi.org/10.1007/s10489-023-04454-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks often suffer from catastrophic forgetting. Although the memory replay (MR) method is an effective continual learning method for mitigating catastrophic forgetting, it typically involves high training costs. This paper proposes a novel strategy for improving the training efficacy of MR. Inspired by the learning style of humans, who first learn easier concepts and then gradually learn harder ones, we hypothesize that the learning order based on the data difficulty may be beneficial for MR. Accordingly, this paper introduces a novel continual learning paradigm, namely continual learning easy to hard (CLeToh), to optimize the training of MR. CLeToh first measures the data difficulty and arranges the data from easy to hard. Then, CLeToh trains the model with easier samples and integrates harder samples until all the data are trained gradually. Our approach increases the convergence speed, stabilizes the training process, and overcomes the problem of catastrophic forgetting of MR.},
  archive      = {J_APIN},
  author       = {Yifan, Chang and Yulu, Chen and Yadan, Zhang and Wenbo, Li},
  doi          = {10.1007/s10489-023-04454-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20626-20646},
  shortjournal = {Appl. Intell.},
  title        = {Continual learning in an easy-to-hard manner},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated approach implementing sliding window and DTW
distance for time series forecasting tasks. <em>APIN</em>,
<em>53</em>(17), 20614–20625. (<a
href="https://doi.org/10.1007/s10489-023-04590-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To handle the scenario of time delay in single predicted results, a novel time-variant weighting method by integrating dynamic time warping (DTW) distance and sliding window model is introduced in combination forecasting. The usage of sliding window can model micro-level statistical characteristics contained in the observed time series and the predicted time series. While the introduction of DTW algorithm can not only measure the distance between the observed time series and the predicted time series, but also the similarity between the two sequences. The integration of sliding window and DTW distance could provide a novel mechanism that can comprehensively reflect the total and partial statistical characteristics of time series in combination forecasting. Two numerical studies show the feasibility and validity of the developed combination forecasting model.},
  archive      = {J_APIN},
  author       = {Tao, Zhifu and Xu, Qinghua and Liu, Xi and Liu, Jinpei},
  doi          = {10.1007/s10489-023-04590-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20614-20625},
  shortjournal = {Appl. Intell.},
  title        = {An integrated approach implementing sliding window and DTW distance for time series forecasting tasks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-robot deep q-learning framework for priority-based
sanitization of railway stations. <em>APIN</em>, <em>53</em>(17),
20595–20613. (<a
href="https://doi.org/10.1007/s10489-023-04529-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sanitizing railway stations is a relevant issue, primarily due to the recent evolution of the Covid-19 pandemic. In this work, we propose a multi-robot approach to sanitize railway stations based on a distributed Deep Q-Learning technique. The proposed framework relies on anonymous data from existing WiFi networks to dynamically estimate crowded areas within the station and to develop a heatmap of prioritized areas to be sanitized. Such heatmap is then provided to a team of cleaning robots - each endowed with a robot-specific convolutional neural network - that learn how to effectively cooperate and sanitize the station’s areas according to the associated priorities. The proposed approach is evaluated in a realistic simulation scenario provided by the Italian largest railways station: Roma Termini. In this setting, we consider different case studies to assess how the approach scales with the number of robots and how the trained system performs with a real dataset retrieved from a one-day data recording of the station’s WiFi network.},
  archive      = {J_APIN},
  author       = {Caccavale, Riccardo and Ermini, Mirko and Fedeli, Eugenio and Finzi, Alberto and Lippiello, Vincenzo and Tavano, Fabrizio},
  doi          = {10.1007/s10489-023-04529-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20595-20613},
  shortjournal = {Appl. Intell.},
  title        = {A multi-robot deep Q-learning framework for priority-based sanitization of railway stations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GACaps-HTC: Graph attention capsule network for hierarchical
text classification. <em>APIN</em>, <em>53</em>(17), 20577–20594. (<a
href="https://doi.org/10.1007/s10489-023-04585-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical text classification has been receiving increasing attention due to its vast range of applications in real-world natural language processing tasks. While previous approaches have focused on effectively exploiting the label hierarchy for classification or capturing latent label relationships, few studies have integrated these concepts. In this work, we propose a graph attention capsule network for hierarchical text classification (GACaps-HTC), designed to capture both the explicit hierarchy and implicit relationships of labels. A graph attention network is employed to incorporate the information on the label hierarchy into a textual representation, whereas a capsule network infers classification probabilities by understanding the latent label relationships via iterative updates. The proposed approach is optimized using a loss term designed to address the innate label imbalance issue of the task. Experiments were conducted on two widely used text classification datasets, the WOS-46985 dataset and the RCV1 dataset. The results reveal that the proposed approach achieved a 0.6% gain and a 2.0% gain in micro-F1 and macro-F1 scores, respectively, on the WOS-46985 dataset and a 0.3% gain and a 2.2% gain in micro-F1 and macro-F1 scores, respectively, on the RCV1 dataset compared to the previous state-of-the-art approaches. Further ablation studies show that each component in GACaps-HTC played a part in enhancing the classification performance.},
  archive      = {J_APIN},
  author       = {Bang, Jinhyun and Park, Jonghun and Park, Jonghyuk},
  doi          = {10.1007/s10489-023-04585-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20577-20594},
  shortjournal = {Appl. Intell.},
  title        = {GACaps-HTC: Graph attention capsule network for hierarchical text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full single-type deep learning models with multihead
attention for speech enhancement. <em>APIN</em>, <em>53</em>(17),
20561–20576. (<a
href="https://doi.org/10.1007/s10489-023-04571-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural network (ANN) models with attention mechanisms for eliminating noise in audio signals, called speech enhancement models, have proven effective. However, their architectures become complex, deep, and demanding in terms of computational resources when trying to achieve higher levels of efficiency. Given this situation, we selected and evaluated simple and less resource-demanding models and utilized the same training parameters and performance metrics to conduct a fair comparison among the four selected models. Our purpose was to demonstrate that simple neural network models with multihead attention are efficient when implemented on computational devices with conventional resources since they provide results that are competitive with those of hybrid, complex and resource-demanding models. We experimentally evaluated the efficiency of multilayer perceptron (MLP), one-dimensional and two-dimensional convolutional neural network (CNN), and gated recurrent unit (GRU) deep learning models with and without multiheaded attention. We also analyzed the generalization capability of each model. The results showed that although these architectures were composed of only one type of ANN, multihead attention increased the efficiency of the speech enhancement process, yielding results that were competitive with those of complex models. Therefore, this study is helpful as a reference for building simple and efficient single-type ANN models with attention.},
  archive      = {J_APIN},
  author       = {Zacarias-Morales, Noel and Hernández-Nolasco, José Adán and Pancardo, Pablo},
  doi          = {10.1007/s10489-023-04571-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20561-20576},
  shortjournal = {Appl. Intell.},
  title        = {Full single-type deep learning models with multihead attention for speech enhancement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A diversity enhanced hybrid particle swarm optimization and
crow search algorithm for feature selection. <em>APIN</em>,
<em>53</em>(17), 20535–20560. (<a
href="https://doi.org/10.1007/s10489-023-04519-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature Selection (FS) is choosing a subcategory of features purposed to construct a machine learning model. Among the copious existing FS algorithms, Binary Particle Swarm Optimization Algorithm (BPSO) is prevalent with applications in several domains. However, BPSO suffers from premature convergence that affects exploration, resulting in dilapidation. In this current work, we boost the exploration of BPSO, incorporating the intelligence of crows to hide their food sources from other crows and predators and maintain diversity by implementing a clustering strategy. The clustering technique guarantees that the starting population is evenly distributed over the feature space while including more promising features. Additionally, suppose a crow realizes another crow or a predator is tracking it. In that case, the crow moves randomly to evict the stalker, leading to a better exploration of unexplored regions within the search space. We named the proposed method Hybrid Particle Swarm Optimization and Crow Search Algorithm with clustering initialization strategy (HPSOCSA-CIS). To evaluate the performance of HPSOCSA-CIS, 15 standard UCI datasets are utilized, and the outcomes are compared with recently proposed hybrid and standard optimization algorithms. From observation, HPSOCSA-CIS outperforms the comparing approaches for feature selection challenges on representative datasets that fall in the three-category based on dimensions. The HPSOCSA-CIS improves performance in terms of mean classification accuracy by 8.87%, 17.5%, and 21.90% on Low, medium, and high dimensional datasets, respectively.},
  archive      = {J_APIN},
  author       = {Osei-kwakye, Jeremiah and Han, Fei and Amponsah, Alfred Adutwum and Ling, Qing-Hua and Abeo, Timothy Apasiba},
  doi          = {10.1007/s10489-023-04519-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20535-20560},
  shortjournal = {Appl. Intell.},
  title        = {A diversity enhanced hybrid particle swarm optimization and crow search algorithm for feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theories, algorithms and applications in tensor learning.
<em>APIN</em>, <em>53</em>(17), 20514–20534. (<a
href="https://doi.org/10.1007/s10489-023-04538-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the accelerated development and popularization of Internet, mobile Internet, and Internet of Things and the breakthrough of storage and communication technologies, the amount of data obtained in the fields of health care, social media, and climate science is increasing, showing complex high-dimensional, multimodal, and heterogeneous characteristics. As the expansion of a vector and matrix, a tensor is the natural and essential mode of representation for this kind of data. The theory of tensor algebra provides a powerful mathematical tool and an extensible framework for learning algorithms for processing data with high-dimensional heterogeneity and complex dependence. In recent years, tensor theory and its applications have become a research hotspot, from new tensor models and scalable algorithms in academia to industry solutions. The article shows its advances in tensor theories, algorithms, and applications. Firstly, tensor operation, classical tensor decomposition theory, and t-product tensor theory are introduced. Secondly, tensor supervised learning, tensor unsupervised learning, and tensor deep learning are discussed from the perspective of tensor decomposition and t-product, and then their application research is summarized. Finally, the opportunities and challenges of tensor learning are briefly discussed.},
  archive      = {J_APIN},
  author       = {Deng, Xiaowu and Shi, Yuanquan and Yao, Dunhong},
  doi          = {10.1007/s10489-023-04538-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20514-20534},
  shortjournal = {Appl. Intell.},
  title        = {Theories, algorithms and applications in tensor learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to use extra training data for better edge detection?
<em>APIN</em>, <em>53</em>(17), 20499–20513. (<a
href="https://doi.org/10.1007/s10489-023-04587-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevalent paradigms for edge detection tend to use extra data in a mixed training manner, which can increase the data diversity of training samples; however, a part of extra data may improve their performances, while the other will degrade their performances. This paper first proposes a selective training method to select positive data for improving the corresponding performance. Secondly, to properly use the negative data which may degrade the performance in previous schemes, an adaptive training method is also proposed by introducing a similarity-preserving self-distillation mechanism based on a teacher-student network, which can maintain equivalent performance to the selective one, and simultaneously achieves better generalization ability. Compared with the state-of-the-art method in CVPR 2022, our schemes achieve 0.5% and 7.6% improvement of the ODS F-scores for the data sets BSDS and Pascal, respectively. Experimental results on the benchmarks verify the effectiveness of the proposed schemes. Our source code and model parameters will be released at https://github.com/wenya1994/Boosting-ED .},
  archive      = {J_APIN},
  author       = {Yang, Wenya and Wu, Wen and Chen, Xiao-Diao and Tao, Xiuting and Mao, Xiaoyang},
  doi          = {10.1007/s10489-023-04587-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20499-20513},
  shortjournal = {Appl. Intell.},
  title        = {How to use extra training data for better edge detection?},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BARS: A benchmark for airport runway segmentation.
<em>APIN</em>, <em>53</em>(17), 20485–20498. (<a
href="https://doi.org/10.1007/s10489-023-04586-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airport runway segmentation can effectively reduce the accident rate during the landing phase, which has the largest risk of flight accidents. With the rapid development of deep learning (DL), related methods achieve good performance on segmentation tasks and can be well adapted to complex scenes. However, the lack of large-scale, publicly available datasets in this field makes the development of methods based on DL difficult. Therefore, we propose a benchmark for airport runway segmentation, named BARS. Additionally, a semiautomatic annotation pipeline is designed to reduce the annotation workload. BARS has the largest dataset with the richest categories and the only instance annotation in the field. The dataset, which was collected using the X-Plane simulation platform, contains 10,256 images and 30,201 instances with three categories. We evaluate eleven representative instance segmentation methods on BARS and analyze their performance. Based on the characteristic of an airport runway with a regular shape, we propose a plug-and-play smoothing postprocessing module (SPM) and a contour point constraint loss (CPCL) function to smooth segmentation results for mask-based and contour-based methods, respectively. Furthermore, a novel evaluation metric named average smoothness (AS) is developed to measure smoothness. The experiments show that existing instance segmentation methods can achieve prediction results with good performance on BARS. SPM and CPCL can effectively enhance the AS metric while modestly improving accuracy. Our work will be available at https://github.com/c-wenhui/BARS .},
  archive      = {J_APIN},
  author       = {Chen, Wenhui and Zhang, Zhijiang and Yu, Liang and Tai, Yichun},
  doi          = {10.1007/s10489-023-04586-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20485-20498},
  shortjournal = {Appl. Intell.},
  title        = {BARS: A benchmark for airport runway segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reverse survival model (RSM): A pipeline for explaining
predictions of deep survival models. <em>APIN</em>, <em>53</em>(17),
20469–20484. (<a
href="https://doi.org/10.1007/s10489-023-04577-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of survival analysis in healthcare is to estimate the probability of occurrence of an event, such as a patient’s death in an intensive care unit (ICU). Recent developments in deep neural networks (DNNs) for survival analysis show the superiority of these models in comparison with other well-known models in survival analysis applications. Ensuring the reliability and explainability of deep survival models deployed in healthcare is a necessity. Since DNN models often behave like a black box, their predictions might not be easily trusted by clinicians, especially when predictions are contrary to a physician’s opinion. A deep survival model that explains and justifies its decision-making process could potentially gain the trust of clinicians. In this research, we propose the reverse survival model (RSM) framework that provides detailed insights into the decision-making process of survival models. For each patient of interest, RSM can extract similar patients from a dataset and rank them based on the most relevant features that deep survival models rely on for their predictions. RSM acts as an add-on to a deep survival model and offers three functionalities: 1) Finding the most relevant clinical measurements for the probability density functions (PDFs) of events. 2) Categorizing patients into disjoint clusters based on the similarity of their survival PDFs. 3) Ranking similar patients based on the similarity of survival outcomes and relevant clinical measurements. The explainability of deep survival models is rarely addressed in the literature. Therefore, the RSM pipeline is a unique approach to explaining the predictions of deep survival models. We validated the RSM pipeline by testing it on a synthetic dataset and MIMIC-IV, a dataset of intensive care unit (ICU) clinical observations. Our experiments showed that given a deep survival model and a patient of interest, RSM can successfully detect similar patient records from historical data and rank them based on the similarities between their survival PDFs and the most relevant patient observations.},
  archive      = {J_APIN},
  author       = {Rezaei, Mohammad R. and Fard, Reza Saadati and Pourjafari, Ebrahim and Ziaei, Navid and Sameizadeh, Amir and Shafiee, Mohammad and Alavinia, Mohammad and Abolghasemian, Mansour and Sajadi, Nick},
  doi          = {10.1007/s10489-023-04577-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20469-20484},
  shortjournal = {Appl. Intell.},
  title        = {Reverse survival model (RSM): A pipeline for explaining predictions of deep survival models},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). AEP-GAN: Aesthetic enhanced perception generative
adversarial network for asian facial beauty synthesis. <em>APIN</em>,
<em>53</em>(17), 20441–20468. (<a
href="https://doi.org/10.1007/s10489-023-04576-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the task of facial aesthetics enhancement (FAE). Existing methods have made great progress, however, beautified images generated by existing methods are extremely prone to over-beautification, which limits the application of existing aesthetic enhancement methods in real scenes. To solve this problem, we propose a new method called aesthetic enhanced perception generative adversarial network (AEP-GAN). We builds three blocks to complete facial beautification guided by facial aesthetic landmarks: an aesthetic deformation perception block (ADP), an aesthetic synthesis and removal block (ASR), and a dual-agent aesthetic identification block (DAI). The ADP learns the implicit aesthetic transformation between the landmarks of the source image and enhanced image. ASR ensures the consistency of image identity before and after beautification. The DAI distinguishes between the source images and generated images. At the same time, to prevent over-beautification, we constructed a real-world facial wedding photography dataset to enable the model to learn human aesthetics. To evaluate the effectiveness of the AEP-GAN, this paper adopted the wedding photography dataset for training, the SCUT-FBP5500 dataset, and the high-resolution Asian face dataset for testing. Experiments showed that the AEP-GAN addresses the over-beautification problem and achieves excellent results.},
  archive      = {J_APIN},
  author       = {Chen, Huanyu and Li, Weisheng and Gao, Xinbo and Xiao, Bin},
  doi          = {10.1007/s10489-023-04576-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20441-20468},
  shortjournal = {Appl. Intell.},
  title        = {AEP-GAN: Aesthetic enhanced perception generative adversarial network for asian facial beauty synthesis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-term electrocardiogram signal quality assessment
pipeline based on a frequency-adaptive mean absolute deviation curve.
<em>APIN</em>, <em>53</em>(17), 20418–20440. (<a
href="https://doi.org/10.1007/s10489-023-04549-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health monitoring is hindered by various types of noise (especially motion artifacts) in electrocardiogram (ECG) collected via wearable devices. The main way to solve this problem is through denoising techniques or signal quality assessment(SQA). When denoising techniques cannot completely suppress motion artifacts, SQA is the most promising approach to address the problem. However, the performance of SQA based on morphological and RR interval features for expressing ECG quality features contaminated by motion artifacts remains unsatisfactory. Here, a frequency-adaptive pipeline based on the mean absolute deviation curve is proposed to achieve a simple and efficient SQA. The greatest advantage of the proposed pipeline is to implement SQA from a new perspective, without considering the influence of motion artifacts or paying attention to the morphological and RR interval features of ECG data. Specifically, the discrete wavelet transform (DWT) is used to capture the abrupt local changes in ECG and noise in a local time window and to form a curve that can reflect the ECG quality distribution. By setting thresholds appropriately (we select two threshold ranges: [0.107,0.143] and [0.324,0.390]), the curve can accurately reflect the ECG quality distribution and label it as three quality levels according to the noise-contamination degrees. On an artificial dataset based on the QT dataset, the F1 value of our pipeline reaches 97.01%, outperforming the SQA method based on morphology and RR interval features. More importantly, the proposed pipeline achieves SQA without corrupting pathological information, and has great potential for deployment in wearable devices.},
  archive      = {J_APIN},
  author       = {Yuan, Shuaiying and He, Ziyang and Zhao, Jianhui and Yang, Zheng and Yuan, Zhiyong},
  doi          = {10.1007/s10489-023-04549-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20418-20440},
  shortjournal = {Appl. Intell.},
  title        = {Long-term electrocardiogram signal quality assessment pipeline based on a frequency-adaptive mean absolute deviation curve},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compression-resistant backdoor attack against deep neural
networks. <em>APIN</em>, <em>53</em>(17), 20402–20417. (<a
href="https://doi.org/10.1007/s10489-023-04575-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a number of backdoor attacks against deep neural networks (DNN) have been proposed. In this paper, we reveal that backdoor attacks are vulnerable to image compressions, as backdoor instances used to trigger backdoor attacks are usually compressed by image compression methods during data transmission. When backdoor instances are compressed, the feature of backdoor trigger will be destroyed, which could result in significant performance degradation for backdoor attacks. As a countermeasure, we propose the first compression-resistant backdoor attack method based on feature consistency training. Specifically, both backdoor images and their compressed versions are used for training, and the feature difference between backdoor images and their compressed versions are minimized through feature consistency training. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack will be robust to image compressions. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) during the feature consistency training, so that the backdoor attack can be robust to multiple image compression algorithms. Experimental results demonstrate that when the backdoor instances are compressed, the attack success rate of common backdoor attack is 6.63% (JPEG), 6.20% (JPEG2000) and 3.97% (WEBP) respectively, while the attack success rate of the proposed compression-resistant backdoor attack is 98.77% (JPEG), 97.69% (JPEG2000), and 98.93% (WEBP) respectively. The compression-resistant attack is robust under various parameters settings. In addition, extensive experiments have demonstrated that even if only one image compression method is used in the feature consistency training process, the proposed compression-resistant backdoor attack has the generalization ability to resist multiple unseen image compression methods.},
  archive      = {J_APIN},
  author       = {Xue, Mingfu and Wang, Xin and Sun, Shichang and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
  doi          = {10.1007/s10489-023-04575-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20402-20417},
  shortjournal = {Appl. Intell.},
  title        = {Compression-resistant backdoor attack against deep neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methods of managing the evolution of ontologies and their
alignments. <em>APIN</em>, <em>53</em>(17), 20382–20401. (<a
href="https://doi.org/10.1007/s10489-023-04545-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, none can expect that knowledge about some part of reality will not change. Consequently, a representation of such evolving knowledge (for example, ontologies) also changes. Such changes entail that applications incorporating such knowledge may become compromised and yield wrong results. An example of such an application is ontology alignment which can be informally described as a set of connections between two ontologies. Those connections mark elements from two ontologies that relate to the same parts of reality. In changing one of the corresponding ontologies, such connections may become invalid. One may designate the ontology alignment once again from scratch for altered ontologies. However, such an approach is time and resource-consuming. The paper comprehensively presents our ontology evolution and alignment maintenance framework. It can be used to preserve the validity of ontology alignment using only the analysis of changes introduced to maintained ontologies. The precise definition of ontologies is provided, along with a definition of the ontology change log. A set of algorithms that allow revalidating ontology alignments have been built based on such elements.},
  archive      = {J_APIN},
  author       = {Pietranik, Marcin and Kozierkiewicz, Adrianna},
  doi          = {10.1007/s10489-023-04545-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20382-20401},
  shortjournal = {Appl. Intell.},
  title        = {Methods of managing the evolution of ontologies and their alignments},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An autoencoder considering multi-order and structural-role
similarity for community detection in attributed networks.
<em>APIN</em>, <em>53</em>(17), 20365–20381. (<a
href="https://doi.org/10.1007/s10489-023-04450-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A community is composed of closely related nodes. Detecting communities in a network has many practical applications, such as online product recommendation, biological molecule discovery and criminal group tracking. In recent years, network representation learning (NRL) has attracted much attention in the field of community detection because it can effectively extract complex relations between nodes which improves the quality of detected communities. In many real-world networks, the rich attribute information contained in nodes and the similarity between nodes and their multi-order neighbors has significant contributions to the generation of node embedding vectors in NRL. However, existing NRL algorithms treat a node’s different order neighbors equally as high-order contexts, leading to the ignorance of their different impacts on the generation of the node’s embedding vector. In addition, these algorithms do not focus on the coupling and interaction relations between nodes playing similar structural roles, which may ignore some nodes in a community with similar structural roles. In this paper, we propose a novel autoencoder considering the multi-order similarity and structural role similarity (AMOSOS) to solve the above problems. First, we design a strategy to obtain a multi-order weight matrix which preserves the differential influence of neighbors of different orders by sequentially decreasing the weight of each order. Second, we design a role similarity indicator to capture the complex coupling and interaction relations of nodes in the network. Experimental results on synthetic networks and real-world networks show that our proposed algorithm is more accurate than existing network representation learning algorithms for the task of community detection.},
  archive      = {J_APIN},
  author       = {Guo, Kun and Lin, Gaosheng and Wu, Ling},
  doi          = {10.1007/s10489-023-04450-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20365-20381},
  shortjournal = {Appl. Intell.},
  title        = {An autoencoder considering multi-order and structural-role similarity for community detection in attributed networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-start local search algorithm based on a novel
objective function for clustering analysis. <em>APIN</em>,
<em>53</em>(17), 20346–20364. (<a
href="https://doi.org/10.1007/s10489-023-04580-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is the process of partitioning data into different clusters with the goal of minimizing the difference of objects within each cluster, where the commonly used evaluation function is defined as the sum of the squared distance from each point to the cluster center to which it belongs. Nevertheless, this general evaluation function is extremely vulnerable to outliers and noisy data, and it is sensitive to initial cluster centers. More seriously, this evaluation function cannot effectively represent the core of clustering results; even if the partition achieves the global optimum value according to the evaluation function, the clustering results may not be good. In this study, we propose a multi-start local search algorithm (MLS) with several techniques to tackle this problem. First, the center of each cluster is no longer its centroid, which reduces the dependence of the cluster algorithm on difference in size and shape of ideal clusters. Second, the number of adjacent points shared between clusters is defined as the new objective function. Third, two basic meta-operations, merge and split, are used to optimize the objective function and make the iterative process insensitive to the initial solution. The novelty of our approach is the selection criterion of the initial centers and the new objective function, which enables MLS to explore more promising search area. Experimental results demonstrate that MLS outperforms traditional centroid-based clustering algorithms in terms of both solution quality and computational efficiency, and it is quite competitive to other reference algorithms such as spectral, density, and geometric based clustering algorithms.},
  archive      = {J_APIN},
  author       = {Liu, Xiaolu and Shao, Wenhan and Chen, Jiaming and Lü, Zhipeng and Glover, Fred and Ding, Junwen},
  doi          = {10.1007/s10489-023-04580-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20346-20364},
  shortjournal = {Appl. Intell.},
  title        = {Multi-start local search algorithm based on a novel objective function for clustering analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFSRNet: Spatial-angular correlation retaining for light
field super-resolution. <em>APIN</em>, <em>53</em>(17), 20327–20345. (<a
href="https://doi.org/10.1007/s10489-023-04558-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) images acquired by hand-held devices suffer from a trade-off between spatial and angular resolutions. To solve this problem, super-resolution (SR) in the spatial and angular domains is studied separately in previous works. However, spatial-angular correlation can not be reconstructed effectively by the separate SR methods. In this paper, a multi-scale feature-assisted synchronous SR network (MFSRNet) is presented to retain spatial-angular correlation for spatial and angular super-resolution, which consists of four modules: multi-scale feature extraction (MFE), view relation reconstruction (VRR), SR information acquisition (SIA) and up-sampling. The MFE module is used to acquire multi-scale angular SR features from low-resolution LF. In the VRR module, these multi-scale features are concatenated with two original adjacent low-resolution view images to reconstruct the angular relation among original and new views. Then, a continuous fusion mechanism is proposed in the SIA module to obtain spatial SR information from four surrounding views and reconstruct the spatial-angular correlation in LF. Finally, super-resolved LF is generated by allocating the sub-pixel information in the up-sampling module. Furthermore, a combined loss is proposed to provide constraints on both angular feature extraction and spatial and angular synchronous SR, and train MFSRNet in an end-to-end fashion. On synthetic and real-world datasets, experimental results show that our algorithm outperforms other state-of-the-art methods in both visual and numerical evaluations. Especially, our method brings significant improvements for sparse LFs from the dataset STFgantry using MFSRNet. Our method improves PSNR/SSIM while preserving the inherent epipolar property in LF.},
  archive      = {J_APIN},
  author       = {Wang, Sizhe and Sheng, Hao and Yang, Da and Cui, Zhenglong and Cong, Ruixuan and Ke, Wei},
  doi          = {10.1007/s10489-023-04558-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20327-20345},
  shortjournal = {Appl. Intell.},
  title        = {MFSRNet: Spatial-angular correlation retaining for light field super-resolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triple-kernel gated attention-based multiple instance
learning with contrastive learning for medical image analysis.
<em>APIN</em>, <em>53</em>(17), 20311–20326. (<a
href="https://doi.org/10.1007/s10489-023-04458-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, multiple instance learning is a method evolved from supervised learning algorithms, which defines a “bag” as a collection of multiple examples with a wide range of applications. In this paper, we propose a novel deep multiple instance learning model for medical image analysis, called triple-kernel gated attention-based multiple instance learning with contrastive learning. It can be used to overcome the limitations of the existing multiple instance learning approaches to medical image analysis. Our model consists of four steps. i) Extracting the representations by a simple convolutional neural network using contrastive learning for training. ii) Using three different kernel functions to obtain the importance of each instance from the entire image and forming an attention map. iii) Based on the attention map, aggregating the entire image together by attention-based MIL pooling. iv) Feeding the results into the classifier for prediction. The results on different datasets demonstrate that the proposed model outperforms state-of-the-art methods on binary and weakly supervised classification tasks. It can provide more efficient classification results for various disease models and additional explanatory information.},
  archive      = {J_APIN},
  author       = {Hu, Huafeng and Ye, Ruijie and Thiyagalingam, Jeyan and Coenen, Frans and Su, Jionglong},
  doi          = {10.1007/s10489-023-04458-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20311-20326},
  shortjournal = {Appl. Intell.},
  title        = {Triple-kernel gated attention-based multiple instance learning with contrastive learning for medical image analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning-based approach for rumor
influence minimization in social networks. <em>APIN</em>,
<em>53</em>(17), 20293–20310. (<a
href="https://doi.org/10.1007/s10489-023-04555-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spreading malicious rumors on social networks such as Facebook, Twitter, and WeChat can trigger political conflicts, sway public opinion, and cause social disruption. A rumor can spread rapidly across a network and can be difficult to control once it has gained traction.Rumor influence minimization (RIM) is a central problem in information diffusion and network theory that involves finding ways to minimize rumor spread within a social network. Existing research on the RIM problem has focused on blocking the actions of influential users who can drive rumor propagation. These traditional static solutions do not adequately capture the dynamics and characteristics of rumor evolution from a global perspective. A deep reinforcement learning strategy that takes into account a wide range of factors may be an effective way of addressing the RIM challenge. This study introduces the dynamic rumor influence minimization (DRIM) problem, a step-by-step discrete time optimization method for controlling rumors. In addition, we provide a dynamic rumor-blocking approach, namely RLDB, based on deep reinforcement learning. First, a static rumor propagation model (SRPM) and a dynamic rumor propagation model (DRPM) based on of independent cascade patterns are presented. The primary benefit of the DPRM is that it can dynamically adjust the probability matrix according to the number of individuals affected by rumors in a social network, thereby improving the accuracy of rumor propagation simulation. Second, the RLDB strategy identifies the users to block in order to minimize rumor influence by observing the dynamics of user states and social network architectures. Finally, we assess the blocking model using four real-world datasets with different sizes. The experimental results demonstrate the superiority of the proposed approach on heuristics such as out-degree(OD), betweenness centrality(BC), and PageRank(PR).},
  archive      = {J_APIN},
  author       = {Jiang, Jiajian and Chen, Xiaoliang and Huang, Zexia and Li, Xianyong and Du, Yajun},
  doi          = {10.1007/s10489-023-04555-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20293-20310},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning-based approach for rumor influence minimization in social networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new clustering algorithm based on connectivity.
<em>APIN</em>, <em>53</em>(17), 20272–20292. (<a
href="https://doi.org/10.1007/s10489-023-04543-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-median problem is the theoretical foundation of partitioning-based clustering algorithms. It was first proposed in 1964 and later it was demonstrated that k-median problem is an NP-hard problem in a network. To be exact, the k-median problem under Euclidean distance is NP hard. Fortunately, the k-median problem under connectivity measure is proved to be a deterministic polynomial problem in this study, and the optimal solution is solved. According to the work above, a connectivity-based clustering theory and algorithm is proposed, obtaining the theoretically optimal partition within polynomial time, and an outstanding performance in real data sets.},
  archive      = {J_APIN},
  author       = {Wan, Jiaqiang and Zhang, Kesheng and Guo, Zhenpeng and Miao, Duoqian},
  doi          = {10.1007/s10489-023-04543-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20272-20292},
  shortjournal = {Appl. Intell.},
  title        = {A new clustering algorithm based on connectivity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel gated dual convolutional neural network model with
autoregressive method and attention mechanism for probabilistic load
forecasting. <em>APIN</em>, <em>53</em>(17), 20256–20271. (<a
href="https://doi.org/10.1007/s10489-023-04589-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate load forecasting is prime in the electric power industry, while the complexity and variability of the load data make it a challenging problem. Therefore, the probabilistic load forecasting is used to provide a prediction interval for the power system. However, the input scale insensitive problem has rarely been addressed in existing works when exploring the multi-load zones data, resulting in a lack of robustness. In addition, the coverage and width of the prediction interval have seldom been simultaneously considered, affecting the forecasting accuracy. Therefore, to overcome the above drawbacks and achieve reliable forecasting results, a novel gated dual convolutional neural network model with autoregressive method and attention mechanism is proposed. Firstly, the autoregressive method is adopted as a linear component of the probabilistic load forecasting model to handle the input scale insensitive problem and enhance the modeling robustness. Then, a dual convolutional neural network is extended with the gating mechanism and is employed to capture the vertical correlation of information among multi-load zones and enhance the coverage of the prediction interval. Next, the attention mechanism is extended with pooling operation to extract the significant horizontal correlation of information among multi-load zones and obtain a narrower prediction interval. Finally, the superiority of the proposed model was validated through both ablation and comparison experiments on two real-world datasets.},
  archive      = {J_APIN},
  author       = {Qiu, Yilei and Wang, Shunzhen and Zhang, Shuai and Xu, Jiyuan},
  doi          = {10.1007/s10489-023-04589-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20256-20271},
  shortjournal = {Appl. Intell.},
  title        = {A novel gated dual convolutional neural network model with autoregressive method and attention mechanism for probabilistic load forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel prediction approach based on three-way decision for
cloud datacenters. <em>APIN</em>, <em>53</em>(17), 20239–20255. (<a
href="https://doi.org/10.1007/s10489-023-04505-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of the cloud workload is essential in cloud computing, especially for cloud resource demand with sudden changes. The existing research on workload prediction approaches for cloud datacentres has one disadvantage: the results of the prediction approach lack generalization and universality. Hence, to improve the accuracy and generalization of cloud workload prediction approaches, we propose a novel prediction approach, called the re-prediction for error correction model based on the change-based three-way decision model, for workload prediction problems in cloud datacentres. The core idea of our approach is to design a re-prediction system for the predicted error sequence, where we explore the re-prediction criteria of the error interval based on the trisecting-acting-outcome (TAO) model of C-3WD. First, we use a long short-term memory (LSTM) model to predict the workload, and then the error series generated by the LSTM model is represented as the error intervals by the interval set and divided into three regions by the C-3WD model. Finally, the error value of every interval region is corrected by the corresponding prediction model. The experiment results show that the RPEM-3WC model improves the prediction accuracy by up to 15.6%, 11.35%, 10.33%, 16.37%, 12.83%, 12.93%, and 8.68% over the EMA, HWES, ARIMA, SES, LSTM-RNN, NN, and TWD-RCPM models, respectively.},
  archive      = {J_APIN},
  author       = {Liu, Shuaishuai and Jiang, Chunmao},
  doi          = {10.1007/s10489-023-04505-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20239-20255},
  shortjournal = {Appl. Intell.},
  title        = {A novel prediction approach based on three-way decision for cloud datacenters},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alleviating over-smoothing via graph sparsification based on
vertex feature similarity. <em>APIN</em>, <em>53</em>(17), 20223–20238.
(<a href="https://doi.org/10.1007/s10489-023-04537-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks (GNNs) have developed rapidly. However, GNNs are difficult to deepen because of over-smoothing. This limits their applications. Starting from the relationship between graph sparsification and over-smoothing, for the problems existing in current graph sparsification methods, we propose a novel graph sparsification method SimSparse based on vertex feature similarity and theoretically prove that it can help to relieve the over-smoothing. Furthermore, we also propose its derivatives SimSparse-G (SimSparse with G umbel-Softmax) and SimSparse-GC (SimSparse-G with memory occupancy reduC tion), which can achieve end-to-end learning. Moreover, SimSparse-GC can apply to larger graphs. Based on the three graph sparsification methods, we further propose a general sparse-convolution block SparseConvBlock with a sparsification layer and a graph convolutional layer to construct deep GNNs in which over-smoothing can be better alleviated. Extensive experiments show that the GNNs constructed by SparseConvBlock have better performance when the numbers of the network layers increase (i.e., in deep GNNs). Furthermore, we verify that our graph sparsification methods help to relieve over-smoothing in deep GNNs.},
  archive      = {J_APIN},
  author       = {Wu, Gongce and Lin, Shukuan and Zhuang, Yilin and Qiao, Jianzhong},
  doi          = {10.1007/s10489-023-04537-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20223-20238},
  shortjournal = {Appl. Intell.},
  title        = {Alleviating over-smoothing via graph sparsification based on vertex feature similarity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-tailed graph neural networks via graph structure
learning for node classification. <em>APIN</em>, <em>53</em>(17),
20206–20222. (<a
href="https://doi.org/10.1007/s10489-023-04534-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed methods have gained increasing attention and achieved excellent performance due to the long-tailed distribution in graphs, i.e., many small-degree tail nodes have limited structural connectivity. However, real-world graphs are inevitably noisy or incomplete due to error-prone data acquisition or perturbations, which may violate the assumption that the raw graph structure is ideal for long-tailed methods. To address this issue, we study the impact of graph perturbation on the performance of long-tailed methods, and propose a novel GNN-based framework called LTSL-GNN for graph structure learning and tail node embedding enhancement. LTSL-GNN iteratively learns the graph structure and tail node embedding enhancement parameters, allowing information-rich head nodes to optimize the graph structure through multi-metric learning and further enhancing the embeddings of the tail nodes with the learned graph structure. Experimental results on six real-world datasets demonstrate that LTSL-GNN outperforms other state-of-the-art baselines, especially when the graph structure is disturbed.},
  archive      = {J_APIN},
  author       = {Lin, Junchao and Wan, Yuan and Xu, Jingwen and Qi, Xingchen},
  doi          = {10.1007/s10489-023-04534-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20206-20222},
  shortjournal = {Appl. Intell.},
  title        = {Long-tailed graph neural networks via graph structure learning for node classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). CNformer: A convolutional transformer with decomposition
for long-term multivariate time series forecasting. <em>APIN</em>,
<em>53</em>(17), 20191–20205. (<a
href="https://doi.org/10.1007/s10489-023-04496-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving long-term time series forecasting accuracy and efficiency is of great value for real-world applications. The main challenge in the long-term forecasting of multivariate time series is to accurately capture the local dynamics and long-term dependencies of time series. Currently, most approaches capture temporal dependencies and inter-variable dependencies in intertwined temporal patterns, which are unreliable. Moreover, models based on time series decomposition methods are still unable to capture both short- and long-term dependencies well. In this paper, we propose an efficient multivariate time series forecasting model CNformer with three distinctive features. (1) The CNformer is a fully CNN-based time series forecasting model. (2) In the encoder, the stacked dilated convolution as a built-in block is combined with the time series decomposition to extract the seasonal component of the time series. (3) The convolution-based encoder-decoder attention mechanism refines seasonal patterns in the decoder and captures complex combinations between different related time series. Owing to these features, our CNformer has lower memory and time overhead than models based on self-attention and the Auto-Correlation mechanism. Experimental results show that our model achieves state-of-the-art performance on four real-world datasets, with a relative performance improvement of 20.29%.},
  archive      = {J_APIN},
  author       = {Wang, Xingyu and Liu, Hui and Yang, Zhihan and Du, Junzhao and Dong, Xiyao},
  doi          = {10.1007/s10489-023-04496-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20191-20205},
  shortjournal = {Appl. Intell.},
  title        = {CNformer: A convolutional transformer with decomposition for long-term multivariate time series forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-semantic passing framework for semi-supervised long
text classification. <em>APIN</em>, <em>53</em>(17), 20174–20190. (<a
href="https://doi.org/10.1007/s10489-023-04556-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important task of natural language processing (NLP), text classification has flourished with the rise of deep learning techniques. However, existing deep learning methods face challenges as the length of input text increases. Many long text classification works are classified by text truncation or simply extracting keywords, which leads to the loss of rich semantic and structural information. Furthermore, there are great demands for studying semi-supervised long text classification due to the lack of labeled training data and continuously generated long texts in different stylistic. To alleviate these problems, we propose a heterogeneous attention network method based on a multi-semantic passing framework. In particular, we develop a flexible heterogeneous information graph to model the long texts by extracting information, including keywords, entities, titles, and their multi-interrelation. It can effectively integrate the semantic relationship and condense the global information to preserve the significant semantic and structural information well. Furthermore, we design a multi-semantic passing framework capable of extracting the semantic and structural information in the constructed heterogeneous information graph by the semantic degree of specific structures. Experimental works on four real-world datasets are studied, such as ThuCNews, SougouNews, 20NG, and Ohsumed, yielded outstanding results. It is shown an accuracy rate of 98.13%, 98.69%, 87.62%, and 71.46%, respectively, which performs better than the existing methods.},
  archive      = {J_APIN},
  author       = {Ai, Wei and Wang, Ze and Shao, Hongen and Meng, Tao and Li, Keqin},
  doi          = {10.1007/s10489-023-04556-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20174-20190},
  shortjournal = {Appl. Intell.},
  title        = {A multi-semantic passing framework for semi-supervised long text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Game theory and MCDM-based unsupervised sentiment analysis
of restaurant reviews. <em>APIN</em>, <em>53</em>(17), 20152–20173. (<a
href="https://doi.org/10.1007/s10489-023-04471-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Analysis is a method to identify, extract, and quantify people’s feelings, opinions, or attitudes. The wealth of online data motivates organizations to keep tabs on customers’ opinions and feelings by turning to sentiment analysis tasks. Along with the sentiment analysis, the emotion analysis of written reviews is also essential to improve customer satisfaction with restaurant service. Due to the availability of massive online data, various computerized methods are proposed in the literature to decipher text sentiments. The majority of current methods rely on machine learning, which necessitates the pre-training of large datasets and incurs substantial space and time complexity. To address this issue, we propose a novel unsupervised sentiment classification model. This study presents an unsupervised mathematical optimization framework to perform sentiment and emotion analysis of reviews. The proposed model performs two tasks. First, it identifies a review’s positive and negative sentiment polarities, and second, it determines customer satisfaction as either satisfactory or unsatisfactory based on a review. The framework consists of two stages. In the first stage, each review’s context, rating, and emotion scores are combined to generate performance scores. In the second stage, we apply a non-cooperative game on performance scores and achieve Nash Equilibrium. The output from this step is the deduced sentiment of the review and the customer’s satisfaction feedback. The experiments were performed on two restaurant review datasets and achieved state-of-the-art results. We validated and established the significance of the results through statistical analysis. The proposed model is domain and language-independent. The proposed model ensures rational and consistent results.},
  archive      = {J_APIN},
  author       = {Punetha, Neha and Jain, Goonjan},
  doi          = {10.1007/s10489-023-04471-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20152-20173},
  shortjournal = {Appl. Intell.},
  title        = {Game theory and MCDM-based unsupervised sentiment analysis of restaurant reviews},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dual-view co-contrastive learning for multi-behavior
recommendation. <em>APIN</em>, <em>53</em>(17), 20134–20151. (<a
href="https://doi.org/10.1007/s10489-023-04495-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-behavior recommender systems (MBR) typically utilize multi-typed user interactive behaviors (e.g., purchase, click, view and add-to-cart) in learning user preference on target behavior (i.e., purchase). Existing MBR models have a natural deficiency, namely the sparse supervision signal problem, which may degrade their actual recommendation performance to some extent. Inspired by the recent success of contrastive learning in mining additional supervision signals from raw data itself, in this work, we seek to exploit the co-contrastive learning to enhance multi-behavior recommendation. However, the following two key challenges remain to be addressed: (1) How to select proper views in multi-behavior modeling; (2) How to design a difficult but effective contrastive task. To tackle the above challenges, we devise a novel co-contrastive learning framework without sampling named D ual-view C o-contrastive L earning (DCL). Unlike traditional contrastive learning methods that generate two augmented views by corruption, we construct two views from different aspects of user preference. Technically, we first leverage the multi-behavior interaction graph to enhance two views that capture both local collaborative signals and high-order semantic information simultaneously. And then two asymmetric graph encoders are performed on both views, which recursively exploit the different structural information to generate ground-truth samples to collaboratively supervise each other by co-contrastive learning and finally high-level node embeddings are learned. Moreover, the view complementary mechanism and divergence constraint further make the two view encoders different but also complementary, so as to improve co-contrastive learning performance. Extensive experiments on two real-world datasets indicate that MBR can be significantly augmented under the regime of co-contrastive learning and then achieves the state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Li, Qingfeng and Ma, Huifang and Zhang, Ruoyi and Jin, Wangyu and Li, Zhixin},
  doi          = {10.1007/s10489-023-04495-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20134-20151},
  shortjournal = {Appl. Intell.},
  title        = {Dual-view co-contrastive learning for multi-behavior recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label classification with weak labels by learning
label correlation and label regularization. <em>APIN</em>,
<em>53</em>(17), 20110–20133. (<a
href="https://doi.org/10.1007/s10489-023-04562-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In conventional multi-label learning, each training instance is associated with multiple available labels. Nevertheless, real-world objects usually exhibit more sophisticated properties such as abundant irrelevant features, incomplete labels, noisy labels, as well as class imbalance. Unfortunately, most existing multi-label learning algorithms only discussed one of them and failed to consider the confounding effects of these factors, which will degrade the accuracy of multi-label classification. In this paper, we propose an integrated multi-label learning framework ML-INC that trains the multi-label model while addressing the aforementioned issues simultaneously. Specifically, we first decompose the observed label matrix into an incomplete ground-truth label matrix and a noisy label matrix by employing the low-rank and sparse decomposition scheme. Secondly, a label confidence matrix is learned to supplement the incomplete label matrix by utilizing the high-order label correlation and the label consistency. Additionally, the low-rank structure is adopted to capture the label correlation. Thirdly, a label regularization matrix is introduced to alleviate the effects of class imbalance in the label matrix, and a sparse constraint is imposed on the feature mapping matrix to select relevant discriminative features. Finally, the Alternating Direction Multiplier Method (ADMM) is employed to handle the optimization problem and comprehensive experiments are conducted to certify the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Ji, Xiaowan and Tan, Anhui and Wu, Wei-Zhi and Gu, Shenming},
  doi          = {10.1007/s10489-023-04562-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20110-20133},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label classification with weak labels by learning label correlation and label regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An autonomous navigation approach for unmanned vehicle in
off-road environment with self-supervised traversal cost prediction.
<em>APIN</em>, <em>53</em>(17), 20091–20109. (<a
href="https://doi.org/10.1007/s10489-023-04518-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a self-supervised learning-based terrain traversal cost prediction method that addresses different orientations and velocities to aid autonomous navigation in off-road environments. First, a cost prediction network is proposed to implement a mapping of the local terrain information around a vehicle to the traversal cost. Second, we propose an automatic data collection and self-labelling algorithm to achieve self-supervised learning for this network. Third, we proposed a map-free navigation strategy aimed at the terrain obstacles. This strategy incorporates the traversal cost prediction into a sampling-based trajectory planner, enabling the consideration of traversal orientation and velocity when estimating the traversal cost. Finally, both the proposed prediction method and the navigation strategy are extensively compared. The results show that our proposed traversability estimation method outperforms existing methods using convolutional neural networks (CNNs). Simultaneously, in both simulation and real-world experiments, our approach exhibits effective and safe autonomous navigation capabilities in off-road environments.},
  archive      = {J_APIN},
  author       = {Zhou, Bo and Yi, Jianjun and Zhang, Xinke and Wang, LianSheng and Zhang, Sizhe and Wu, Bin},
  doi          = {10.1007/s10489-023-04518-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20091-20109},
  shortjournal = {Appl. Intell.},
  title        = {An autonomous navigation approach for unmanned vehicle in off-road environment with self-supervised traversal cost prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LASSO and attention-TCN: A concurrent method for indoor
particulate matter prediction. <em>APIN</em>, <em>53</em>(17),
20076–20090. (<a
href="https://doi.org/10.1007/s10489-023-04507-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long time exposure to indoor air pollution environments can increase the risk of cardiovascular and respiratory system damage. Most previous studies focus on outdoor air quality, while few studies on indoor air quality. Current neural network-based methods for indoor air quality prediction ignore the optimization of input variables, process input features serially, and still suffer from loss of information during model training, which may lead to the problems of memory-intensive, time-consuming and low-precision. We present a novel concurrent indoor PM prediction model based on the fusion model of Least Absolute Shrinkage and Selection Operator (LASSO) and an Attention Temporal Convolutional Network (ATCN), together called LATCN. First, a LASSO regression algorithm is used to select features from PM1, PM2.5, PM10 and PM (&gt;10) datasets and environmental factors to optimize the inputs for indoor PM prediction model. Then an Attention Mechanism (AM) is applied to reduce the redundant temporal information to extract key features in inputs. Finally, a TCN is used to forecast indoor particulate concentration in parallel with inputting the extracted features, and it reduces information loss by residual connections. The results show that the main environmental factors affecting indoor PM concentration are the indoor heat index, indoor wind chill, wet bulb temperature and relative humidity. Comparing with Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) approaches, LATCN systematically reduced the prediction error rate (19.7% ~ 28.1% for the NAE, and 16.4% ~ 21.5% for the RMSE) and improved the model running speed (30.4% ~ 81.2%) over these classical sequence prediction models. Our study can inform the active prevention of indoor air pollution, and provides a theoretical basis for indoor environmental standards, while laying the foundations for developing novel air pollution prevention equipment in the future.},
  archive      = {J_APIN},
  author       = {Shi, Ting and Yang, Wu and Qi, Ailin and Li, Pengyu and Qiao, Junfei},
  doi          = {10.1007/s10489-023-04507-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20076-20090},
  shortjournal = {Appl. Intell.},
  title        = {LASSO and attention-TCN: A concurrent method for indoor particulate matter prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe sample screening for robust twin support vector
machine. <em>APIN</em>, <em>53</em>(17), 20059–20075. (<a
href="https://doi.org/10.1007/s10489-023-04547-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machine (TSVM) definitely improves computational speed compared with the classical SVM, and has been widely used in classification and regression problems. However, two problems should be aroused. First, since the convex hinge loss function of TSVM is unbounded, the generalization performance of TSVM declines under the noisy environment. Second, TSVM is challenging to deal with large-scale data. To handle these problem, in this paper, we propose a new method named Safe Sample Screening for robust TSVM (SSS-RTSVM). As the ramp loss is bounded, robust TSVM clips the hinge loss in the traditional soft margin twin support vector machine to the ramp loss, and provides a pair of nonparallel proximal hyperplanes to achieve good anti-noise ability to noisy data and outlier data. However, the non-convex problem of robust TSVM can be considered as a DC programming problem which is computationally inefficient. Then we integrate safe sample screening rules for RTSVM based on the framework of concave-convex procedure (CCCP) to delete the most training samples, i.e., a subset of the samples called support vectors (SVs) is selected to reduce the computational cost without sacrificing the optimal accuracy. Notably, for the proposed SSS-RTSVM, the security guarantee is provided to the sample screening rule. Extensive experiments are conducted on several benchmark datasets to fully demonstrate the robustness and acceleration of the proposed method.},
  archive      = {J_APIN},
  author       = {Li, Yanmeng and Sun, Huaijiang},
  doi          = {10.1007/s10489-023-04547-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20059-20075},
  shortjournal = {Appl. Intell.},
  title        = {Safe sample screening for robust twin support vector machine},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-label learning prediction model for heart failure in
patients with atrial fibrillation based on expert knowledge of disease
duration. <em>APIN</em>, <em>53</em>(17), 20047–20058. (<a
href="https://doi.org/10.1007/s10489-023-04487-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with atrial fibrillation (AF) are prone to complications such as heart failure with preserved ejection fraction (HFpEF), which accelerates the progress of the disease and can lead to death. It is of great practical importance to predict the onset timing and probability of HFpEF, which can guide the personalized treatment of patients with AF and improve their survival rate. However, there are no models for predicting HFpEF in patients with AF at present. A multi-label learning prediction model based on expert knowledge of disease duration is proposed in this paper to achieve accurate prediction of HFpEF in patients with AF. First, the expert knowledge of the relationship between the duration of disease progression and outcome is adopted to divided the duration of AF disease into different periods. Next, multi-label decision tree models were used to build different multi-label prediction models for each stage. With the multi-label design, the model can learn the intrinsic correlation between different labels and achieve dual-task prediction of HFpEF attack time and probability. Finally, the results from different periods were fused and output. The proposed method considers the time dependence of medical data and the development pattern of the disease, which realizes 0.0352 hloss, 0.8571 micro-F1 score, and 0.90 average micro-AUC. The experimental results show that the proposed model achieved better prediction of onset time and outcomes in HFpEF patients, which will help the prognosis and personalize the treatment of patients.},
  archive      = {J_APIN},
  author       = {Huang, Youhe and Zhang, Rongfeng and Li, Hongru and Xia, Yunlong and Yu, Xia and Liu, Songbai and Yang, Yiheng},
  doi          = {10.1007/s10489-023-04487-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20047-20058},
  shortjournal = {Appl. Intell.},
  title        = {A multi-label learning prediction model for heart failure in patients with atrial fibrillation based on expert knowledge of disease duration},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NDDNet: A deep learning model for predicting
neurodegenerative diseases from gait pattern. <em>APIN</em>,
<em>53</em>(17), 20034–20046. (<a
href="https://doi.org/10.1007/s10489-023-04557-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases damage neuromuscular tissues and deteriorate motor neurons which affects the motor capacity of the patient. Particularly the walking gait is greatly influenced by the deterioration process. Early detection of anomalous gait patterns caused by neurodegenerative diseases can help the patient to prevent associated risks. Previous studies in this domain relied on either features extracted from gait parameters or the Ground Reaction Force (GRF) signal. In this work, we aim to combine both GRF signals and extracted features to provide a better analysis of walking gait patterns. For this, we designed NDDNet, a novel neural network architecture to process both of these data simultaneously to detect 3 different Neurodegenerative Diseases (NDDs). We have done several experiments on the data collected from 64 participants and got 96.75% accuracy on average in detecting 3 types of NDDs. The proposed method might provide a way to get the most out of the data in hand while working with GRF signals and help diagnose patients with an anomalous gait more effectively.},
  archive      = {J_APIN},
  author       = {Faisal, Md. Ahasan Atick and Chowdhury, Muhammad E. H. and Mahbub, Zaid Bin and Pedersen, Shona and Ahmed, Mosabber Uddin and Khandakar, Amith and Alhatou, Mohammed and Nabil, Mohammad and Ara, Iffat and Bhuiyan, Enamul Haque and Mahmud, Sakib and AbdulMoniem, Mohammed},
  doi          = {10.1007/s10489-023-04557-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20034-20046},
  shortjournal = {Appl. Intell.},
  title        = {NDDNet: A deep learning model for predicting neurodegenerative diseases from gait pattern},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-aware deep reinforcement learning with multi-temporal
abstraction. <em>APIN</em>, <em>53</em>(17), 20007–20033. (<a
href="https://doi.org/10.1007/s10489-022-04392-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) is advantageous, but it rarely performs well when tested on real-world decision-making tasks, particularly those involving irregular time series with sparse actions. Although irregular time series with sparse actions can be handled using temporal abstractions for the agent to grasp high-level states, they aggravate temporal irregularities by increasing the range of time intervals essential to represent a state and estimate expected returns. In this work, we propose a general Time-aware DRL framework with Multi-Temporal Abstraction (T-MTA) that incorporates the awareness of time intervals from two aspects: temporal discounting and temporal abstraction. For the former, we propose a Time-aware DRL method, whereas for the latter we propose a Multi-Temporal Abstraction mechanism. T-MTA was tested in three standard RL testbeds and two real-life tasks (control of nuclear reactors and prevention of septic shock), which represent four common contexts of learning environments, online and offline, as well as fully and partially observable. As T-MTA is a general framework, it can be combined with any model-free DRL method. In this work, we examined two in particular: the Deep Q-Network approach and its variants, and Truly Proximal Policy Optimization. Our results show that T-MTA significantly outperforms competing baseline frameworks, including a standalone Time-aware DRL framework, MTAs, and the original DRL methods without considering either type of temporal aspect, especially when partially observable environments are involved and the range of time intervals is large.},
  archive      = {J_APIN},
  author       = {Kim, Yeo Jin and Chi, Min},
  doi          = {10.1007/s10489-022-04392-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {20007-20033},
  shortjournal = {Appl. Intell.},
  title        = {Time-aware deep reinforcement learning with multi-temporal abstraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HCT-net: Hybrid CNN-transformer model based on a neural
architecture search network for medical image segmentation.
<em>APIN</em>, <em>53</em>(17), 19990–20006. (<a
href="https://doi.org/10.1007/s10489-023-04570-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that many manually designed convolutional neural networks (CNNs) for different tasks that require considerable time, labor, and domain knowledge have been designed in the medical image segmentation domain and that most CNN networks only consider local feature information while ignoring the global receptive field due to the convolution limitation, there is still much room for performance improvement. Therefore, designing a new method that can fully capture feature information and save considerable time and human energy with less GPU memory consumption and complexity is necessary. In this paper, we propose a novel hybrid CNN-transformer model based on a neural architecture search network (HCT-Net), which designs a hybrid U-shaped CNN with a key-sampling Transformer backbone that considers contextual and long-range pixel information in the search space and uses a single-path neural architecture search that contains a flexible search space and an efficient search strategy to simultaneously find the optimal subnetwork including three types of cells during SuperNet. Compared with various types of medical image segmentation methods, our framework can achieve competitive precision and efficiency on various datasets, and we also validate the generalization on unseen datasets in extended experiments. In this way, we can verify that our method is competitive and robust. The code for the method is available at https://github.com/yuzh2022/HCT-Net .},
  archive      = {J_APIN},
  author       = {Yu, Zhihong and Lee, Feifei and Chen, Qiu},
  doi          = {10.1007/s10489-023-04570-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19990-20006},
  shortjournal = {Appl. Intell.},
  title        = {HCT-net: Hybrid CNN-transformer model based on a neural architecture search network for medical image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DISCONA: Distributed sample compression for nearest neighbor
algorithm. <em>APIN</em>, <em>53</em>(17), 19976–19989. (<a
href="https://doi.org/10.1007/s10489-023-04482-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample compression using 𝜖-net effectively reduces the number of labeled instances required for accurate classification with nearest neighbor algorithms. However, one-shot construction of an 𝜖-net can be extremely challenging in large-scale distributed data sets. We explore two approaches for distributed sample compression: one where local 𝜖-net is constructed for each data partition and then merged during an aggregation phase, and one where a single backbone of an 𝜖-net is constructed from one partition and aggregates target label distributions from other partitions. Both approaches are applied to the problem of malware detection in a complex, real-world data set of Android apps using the nearest neighbor algorithm. Examination of the compression rate, computational efficiency, and predictive power shows that a single backbone of an 𝜖-net attains favorable performance while achieving a compression rate of 99%.},
  archive      = {J_APIN},
  author       = {Rybicki, Jedrzej and Frenklach, Tatiana and Puzis, Rami},
  doi          = {10.1007/s10489-023-04482-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19976-19989},
  shortjournal = {Appl. Intell.},
  title        = {DISCONA: Distributed sample compression for nearest neighbor algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gated graph attention network based on dual graph
convolution for node embedding. <em>APIN</em>, <em>53</em>(17),
19962–19975. (<a
href="https://doi.org/10.1007/s10489-023-04568-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on node classification is based on node embeddings. Node classification accuracy can be improved if the embeddings of different nodes are well discriminated. With the rapid development of deep learning, researchers have proposed many graph neural network models (GNNs), such as GCN and GAT, which generally obtain node embeddings by aggregating neighborhood information. However, such methods only emphasize feature aggregation in neighborhoods and do not consider the class labels of nodes, which leads to the oversmoothing problem and weak differences in inter-class nodes. In this paper, we propose a gated graph attention network based on dual graph convolution for node embedding (GGAN-DGC). To strengthen the embedding difference of inter-class nodes, GGAN-DGC introduces a gated attention mechanism. This mechanism utilizes a supervised gated attention (GA) matrix to separate the GNN aggregation process according to the node class, so as to heterogenize the homogenous graphs. The GA matrix is obtained by the dual graph convolutional network (DGC), which can improve the receptive field of the original graph. In addition, GGAN-DGC adopts triplet loss as the global supervision function of node embedding, which can streng-then the class correlation of node embedding at the global level. Finally, based on the obtained node embedding, nodes can be classified correctly. The experimental results on five datasets confirm that our GGAN-DGC model performs better than other representative methods in node classification, especially for datasets with strong heterophily. In addition, we verify that GGAN-DGC can also perform better than other methods in graph classification experiments.},
  archive      = {J_APIN},
  author       = {Yu, Ruowang and Wang, Lanting and Xin, Yu and Qian, Jiangbo and Dong, Yihong},
  doi          = {10.1007/s10489-023-04568-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19962-19975},
  shortjournal = {Appl. Intell.},
  title        = {A gated graph attention network based on dual graph convolution for node embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry-based anisotropy representation learning of
concepts for knowledge graph embedding. <em>APIN</em>, <em>53</em>(17),
19940–19961. (<a
href="https://doi.org/10.1007/s10489-023-04528-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entities in the knowledge graphs are generally categorized into concepts and instances, where each concept is used to represent the abstraction of a set of instances with common properties. Most previous Knowledge Graph Embedding methods tend to treat them in the same way by projecting them into low-dimension space as vector points without explicitly distinguishing them, therefore ignoring the potential specification of concept. Some recent studies address this problem by modeling each concept as a sphere rather than a vector point. However, the isotropy of the sphere is less capable of modeling the semantic abstraction of concepts, as well as the complex relations between concepts and instances. To solve this problem, we propose to model concepts using geometric shapes with anisotropy to enrich the representation power of concepts. Two algorithms, named as TransEllipsoid and TransCuboid, are presented to project each concept as an ellipsoid and a cuboid in embedding space respectively. The anisotropy of concept embedding is learned by allowing the length of the axes of the ellipsoid or the edges of the cuboid to vary across different dimensions. Experimental results on three real-world datasets show that modeling the anisotropy of concept embedding would significantly benefit not only the learned representations of concepts but also the corresponding instances. The visualization of embedding results reveals human-intuitive relative positions between concepts and instances and provides potential interpretability for the transitivity of isA relations.},
  archive      = {J_APIN},
  author       = {Yu, Jibin and Zhang, Chunhong and Hu, Zheng and Ji, Yang and Fu, Dongjun and Wang, Xueyu},
  doi          = {10.1007/s10489-023-04528-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19940-19961},
  shortjournal = {Appl. Intell.},
  title        = {Geometry-based anisotropy representation learning of concepts for knowledge graph embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybridizing genetic algorithm with grey prediction evolution
algorithm for solving unit commitment problem. <em>APIN</em>,
<em>53</em>(17), 19922–19939. (<a
href="https://doi.org/10.1007/s10489-023-04527-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unit commitment problem (UCP), which includes the unit schedule and power dispatch, is a nonlinear high-dimensional and highly constrained mixed-integer combinatorial optimization problem. One challenge herein is to obtain high-quality solutions considering various constraints. Developing a competitive hybrid method is a mainstream study goal in this field, which has focused on the unit schedule optimization but less on power dispatch. Inspired by the advantage of genetic algorithms (GAs) in solving combinational optimization problems and the characteristic of grey prediction evolution algorithm (GPE) with strong exploration ability, this paper proposes a novel hybrid GA and GPE method, termed hGAGPE, to solve the UCP. In hGAGPE, GPE, as a novel real parameter stochastic search algorithm based on the grey prediction theory for data mining, is first employed to solve the power dispatch of the UCP. Meanwhile, the unit schedule is performed by the popular GA. Additionally, some heuristic repair mechanisms based on the priority list and an elite selection mechanism are incorporated to enhance the performance of hGAGPE. The proposed hGAGPE is evaluated on six test systems with generating units in the range of 10 to 100 during a 24-h scheduling period. The numerical results demonstrate the feasibility and effectiveness of hGAGPE in comparison with other existing approaches.},
  archive      = {J_APIN},
  author       = {Tong, Wangyu and Liu, Di and Hu, Zhongbo and Su, Qinghua},
  doi          = {10.1007/s10489-023-04527-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19922-19939},
  shortjournal = {Appl. Intell.},
  title        = {Hybridizing genetic algorithm with grey prediction evolution algorithm for solving unit commitment problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary many-objective algorithm with improved growing
neural gas and angle-penalized distance for irregular problems.
<em>APIN</em>, <em>53</em>(17), 19892–19921. (<a
href="https://doi.org/10.1007/s10489-023-04526-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For solving complex many-objective optimization problems, the many-objective evolutionary algorithms may require new distance measures and more enormous selection pressures to guide solutions to the Pareto front. In this case, solving irregular Pareto front problems with growing neural gas networks as reference vectors is a great challenge. This paper proposes an algorithm with improved growing neural gas and angle-penalized distance to overcome the difficulty: First, a new parameter, the proportion of the solutions relatively close to the input data, is introduced to the angle-penalty distance, boosting the influence on the entire environment selection and enhancing the effect for Pareto front learning. Second, the edge-acceleration strategy enhances the ability of growing neural gas to match Pareto fronts and adapt to rapidly changing input. Finally, by adjusting the nodes deletion strategy in the search procedure, more promising solutions may be maintained to optimize the solutions’ distribution. In experiments tackling problems with irregular Pareto fronts, the proposed algorithm performs competitively in contrast to the other six excellent algorithms.},
  archive      = {J_APIN},
  author       = {Gu, Qinghua and Pang, Dejun and Wang, Qian},
  doi          = {10.1007/s10489-023-04526-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19892-19921},
  shortjournal = {Appl. Intell.},
  title        = {Evolutionary many-objective algorithm with improved growing neural gas and angle-penalized distance for irregular problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SKGCR: Self-supervision enhanced knowledge-aware graph
collaborative recommendation. <em>APIN</em>, <em>53</em>(17),
19872–19891. (<a
href="https://doi.org/10.1007/s10489-023-04539-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph (KG) can be used as supplementary information for improving the performance of recommendations and it plays an increasingly important role in the recommendation area. As graph neural networks (GNNs) have advanced, propagation-based models that propagate user-item interactive data in the KG as a means of enhancing the representations of users and items have achieved state-of-the-art (SOTA) performance. The existing propagation-based models adopt the only supervised learning paradigm, which is reliant on the label information of the data. In addition, the features of the data itself, which are also known as self-supervised signals, can be employed for supporting supervised learning in improving model performance. This paper proposes the introduction of self-supervised learning (SSL) as a means of exploring the self-supervised signals in the data and a new recommendation model, Self-Supervision Enhanced Knowledge-aware Graph Collaborative Recommendation (SKGCR) is proposed. A unique feature of SKGCR is that it models each node in the user-item interaction data from multiple views for exploring the self-supervised signals. Multiple views are constructed through the random deletion of a certain percentage of interactions from the user-item interaction data. In addition, adopting the information noise contrastive estimation (InfoNCE) is proposed for maximizing the mutual information of the same node in different views. The results from this step serve as the self-supervised signals for training SKGCR, together with the supervised learning signals in the form of multi-task training. The experimental results demonstrate that the proposed SKGCR outperforms SOTA baselines.},
  archive      = {J_APIN},
  author       = {Liu, Xiangkun and Yang, Bo and Xu, Jingyu},
  doi          = {10.1007/s10489-023-04539-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19872-19891},
  shortjournal = {Appl. Intell.},
  title        = {SKGCR: Self-supervision enhanced knowledge-aware graph collaborative recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identity-invariant representation and transformer-style
relation for micro-expression recognition. <em>APIN</em>,
<em>53</em>(17), 19860–19871. (<a
href="https://doi.org/10.1007/s10489-023-04533-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is a challenging task due to the subtle changes of facial muscle movements in a short duration. These muscle movements, which are generalized as action units (AUs), have close correlations with micro-expressions (MEs). On the other hand, due to the limited and imbalanced training data, most existing MER works are apt to learn facial identities instead of MEs as the intrinsic representations. In this paper, we propose a novel MER method by identity-invariant representation learning and transformer-style relational modeling. Specifically, we propose to disentangle the identity information from the input via an adversarial training strategy. Considering the coherent relationships between AUs and MEs, we further employ AU recognition as an auxiliary task to learn AU representations with ME information captured. Moreover, we introduce a transformer to achieve MER by modeling the correlations among AUs. MER and AU recognition are jointly trained, in which the two correlated tasks can contribute to each other. Extensive experiments show that our method (i) obtains competitive performance over the state-of-the-art MER methods on the CASME II and SAMM benchmarks, and (ii) also works well for macro-expression recognition.},
  archive      = {J_APIN},
  author       = {Shao, Zhiwen and Li, Feiran and Zhou, Yong and Chen, Hao and Zhu, Hancheng and Yao, Rui},
  doi          = {10.1007/s10489-023-04533-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19860-19871},
  shortjournal = {Appl. Intell.},
  title        = {Identity-invariant representation and transformer-style relation for micro-expression recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring misclassifications of robust neural networks to
enhance adversarial attacks. <em>APIN</em>, <em>53</em>(17),
19843–19859. (<a
href="https://doi.org/10.1007/s10489-023-04532-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progress in making neural networks more robust against adversarial attacks is mostly marginal, despite the great efforts of the research community. Moreover, the robustness evaluation is often imprecise, making it challenging to identify promising approaches. We do an observational study on the classification decisions of 19 different state-of-the-art neural networks trained to be robust against adversarial attacks. This analysis gives a new indication of the limits of the robustness of current models on a common benchmark. In addition, our findings suggest that current untargeted adversarial attacks induce misclassification toward only a limited amount of different classes. Similarly, we find that previous attacks under-explore the perturbation space during optimization. This leads to unsuccessful attacks for samples where the initial gradient direction is not a good approximation of the final adversarial perturbation direction. Additionally, we observe that both over- and under-confidence in model predictions result in an inaccurate assessment of model robustness. Based on these observations, we propose a novel loss function for adversarial attacks that consistently improves their efficiency and success rate compared to prior attacks for all 30 analyzed models.},
  archive      = {J_APIN},
  author       = {Schwinn, Leo and Raab, René and Nguyen, An and Zanca, Dario and Eskofier, Bjoern},
  doi          = {10.1007/s10489-023-04532-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19843-19859},
  shortjournal = {Appl. Intell.},
  title        = {Exploring misclassifications of robust neural networks to enhance adversarial attacks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining inter-sequence patterns with itemset constraints.
<em>APIN</em>, <em>53</em>(17), 19827–19842. (<a
href="https://doi.org/10.1007/s10489-023-04514-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, raw data is rarely used directly. In real world applications, data is often processed, and the necessary knowledge extracted, depending on the purpose of the user. Applying constraints in pattern mining is a major factor in reducing the resulting patterns to help decision support systems work efficiently. In 2018, a constraint-based approach was developed to discover inter-sequence patterns. However, this method only focused on the constraints with single items. The task of discovering constraint-based inter-sequential patterns is our target in this work. We propose the DBV-ISPMIC algorithm, a DBV-PatternList based structure, for mining inter-sequential patterns with itemset constraints. The proposed algorithm utilizes an organized search tree structure stored as dynamic bit vectors to quickly compute the support of patterns. In addition, we also develop a property and, based on it, an improved algorithm is proposed to reduce checking candidates. Finally, we develop the pDBV-ISPMIC algorithm as a parallel method of the DBV-ISPMIC algorithm. Empirical evaluations show that DBV-ISPMIC has better performance than the post-processing algorithms in experimental databases and pDBV-ISPMIC is better than DBV-ISPMIC with regard to the runtime.},
  archive      = {J_APIN},
  author       = {Nguyen, Anh and Nguyen, Ngoc-Thanh and Nguyen, Loan T.T. and Vo, Bay},
  doi          = {10.1007/s10489-023-04514-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19827-19842},
  shortjournal = {Appl. Intell.},
  title        = {Mining inter-sequence patterns with itemset constraints},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep deterministic policy gradient and graph attention
network for geometry optimization of latticed shells. <em>APIN</em>,
<em>53</em>(17), 19809–19826. (<a
href="https://doi.org/10.1007/s10489-023-04565-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a combined approach of deep deterministic policy gradient (DDPG) and graph attention network (GAT) to the geometry optimization of latticed shells with surface shapes defined by a Bézier control net. The optimization problem is formulated to minimize the strain energy of the latticed structures with heights of the Bézier control points as design variables. The information of the latticed shells, including nodal configurations, element properties and internal forces, and the Bézier control net, consisting of control points and control net, are represented as graphs using node feature matrices, adjacency matrices, and weighted adjacency matrices. A specifically designed DDPG agent utilizes GAT and matrix manipulations to observe the state of the structure through the graphs, and decides which and how Bézier control points to move. The agent is trained to excel in the task through a reward signal computed from changes in the strain energy in each optimization step. As shown in numerical examples, the trained agent can effectively optimize structures of different sizes, control nets, configurations, and initial geometries from those used during the training. The performance of the trained agent is competitive compared to particle swarm optimization and simulated annealing despite using a lower computational cost. - A method using a reinforcement learning agent is proposed to optimize the geometry of latticed structures. - The agent is designed to observe the structure and Bézier control net and modify the Bézier control net. - The method yields good results using fewer computations when compared to other conventional methods.},
  archive      = {J_APIN},
  author       = {Kupwiwat, Chi-tathon and Hayashi, Kazuki and Ohsaki, Makoto},
  doi          = {10.1007/s10489-023-04565-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {17},
  pages        = {19809-19826},
  shortjournal = {Appl. Intell.},
  title        = {Deep deterministic policy gradient and graph attention network for geometry optimization of latticed shells},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retraction note: ICDN: Integrating consistency and
difference networks by transformer for multimodal sentiment analysis.
<em>APIN</em>, <em>53</em>(16), 19808. (<a
href="https://doi.org/10.1007/s10489-023-04869-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Zhang, Qiongan and Shi, Lei and Liu, Peiyu and Zhu, Zhenfang and Xu, Liancheng},
  doi          = {10.1007/s10489-023-04869-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19808},
  shortjournal = {Appl. Intell.},
  title        = {Retraction note: ICDN: integrating consistency and difference networks by transformer for multimodal sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Detection and localization of splicing on
remote sensing images using image-to-image transformation.
<em>APIN</em>, <em>53</em>(16), 19807. (<a
href="https://doi.org/10.1007/s10489-023-04476-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Alsughayer, Rawan and Hussain, Muhammad and Saeed, Fahman and AboalSamh, Hatim},
  doi          = {10.1007/s10489-023-04476-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19807},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Detection and localization of splicing on remote sensing images using image-to-image transformation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning spatial-temporal dynamics and interactivity for
short-term passenger flow prediction in urban rail transit.
<em>APIN</em>, <em>53</em>(16), 19785–19806. (<a
href="https://doi.org/10.1007/s10489-023-04508-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term passenger flow prediction in urban rail transit is critical in ensuring the stable operation of urban rail systems. However, accurate passenger flow prediction still faces challenges, including modeling the dynamics of passenger flow data in spatial and temporal dimensions and capturing the interactions between the inflows and outflows. To solve these problems, a novel model called the multi-feature fusion graph convolutional network (MFGCN) is proposed. Firstly, parallel graph branch networks are established to describe inflow and outflow information from geographic and semantic perspectives. Then, in the spatial dimension, the graph convolutional networks with spatial attention are designed to learn the dynamic spatial correlations of nodes in the two graphs. In the temporal dimension, the long short-term memory networks with temporal attention are developed to learn the dynamic temporal dependencies of passenger flow data. Finally, a three-dimensional residual network is established to capture the spatial-temporal interactive dependencies between inflows and outflows. Experiments on Nanning Metro Line 1 passenger flow datasets demonstrated that MFGCN outperformed the existing baseline models, which could provide technical support for URT network operation management.},
  archive      = {J_APIN},
  author       = {Wu, Jinxin and Li, Xianwang and He, Deqiang and Li, Qin and Xiang, Weibin},
  doi          = {10.1007/s10489-023-04508-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19785-19806},
  shortjournal = {Appl. Intell.},
  title        = {Learning spatial-temporal dynamics and interactivity for short-term passenger flow prediction in urban rail transit},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). See, move and hear: A local-to-global multi-modal
interaction network for video action recognition. <em>APIN</em>,
<em>53</em>(16), 19765–19784. (<a
href="https://doi.org/10.1007/s10489-023-04497-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual and audio signals are concurrent and complementary types of modality in some video actions. A single visual modality limits the performance of video action recognition due to the similar appearance with subtle movements, such as tapping guitar and playing guitar. However, it is challenging to fuse audio-visual modalities since the heterogeneity gap caused by inconsistent distribution and representation of multi-modal data. In this paper, we propose a local-to-global multi-modal interaction network (LGMI-Net) that integrates RGB, optical flow with sound information. First, for the local multi-modal interaction, we propose a novel inter-modal channel recalibration (IMCR) block to learn a joint representation from different input modalities by recalibrating the channel information distribution of one modality according to another modality. Besides, we also propose a novel RGB modality aggregation (RMA) block to obtain the more robust appearance features by mixing optical flow and sound information. Second, for the global multi-modal interaction, we propose three distinct encoder modes: unitary, parallel and triplet encoders, to capture the global multi-modal representation. The unitary encoder has the lowest computational complexity. The parallel encoder utilizes RGB-guided attention to improve accuracy while maintaining the lightweight. The triplet encoder aggregates the self-attentions of different modalities and achieves the best recognition performance. We implement extensive experiments on three public datasets: UCF101 subset, Kinetics-Sounds and EPIC-Kitchens-55. The results demonstrate the effectiveness of audio-visual complementation. Compared with the state-of-the-art multi-modal methods with sound (i.e. MM-ViT, AdaMML and G-Blend), the proposed LGMI-Net achieves superior accuracies of 96.05%, 88.37% and 50.3% with the 4.49×, 1.40× and 6.02× lower giga floating point operations (GFLOPs) respectively.},
  archive      = {J_APIN},
  author       = {Feng, Fan and Ming, Yue and Hu, Nannan and Zhou, Jiangwan},
  doi          = {10.1007/s10489-023-04497-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19765-19784},
  shortjournal = {Appl. Intell.},
  title        = {See, move and hear: A local-to-global multi-modal interaction network for video action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A monte carlo manifold spectral clustering algorithm based
on emotional preference and migratory behavior. <em>APIN</em>,
<em>53</em>(16), 19742–19764. (<a
href="https://doi.org/10.1007/s10489-023-04484-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by various behaviors of creatures in nature, numerous efficient bionic algorithms are designed for dealing with complex clustering problems. As a population-based intelligence bionic optimization model mixes individual information interaction and physical mechanism, the emotional preference and migration behavior clustering (EPMC) algorithm had proposed for dealing with clustering tasks. It is superior to multiple classic clustering algorithms and obtains epoch-making clustering effects. However, EPMC still shows premature convergence and ineffectiveness in balancing exploration and exploitation. To further enhance the searchability and the performance during clustering, we proposed a Monte Carlo spectral clustering algorithm for emotional preference and migratory behavior optimization named MCSC-EPMC. Specifically, we first incorporate the spectral clustering strategy based on Laplacian eigenmaps to assist in updating the individual. Second, a Monte Carlo statistical data theory is utilized to simulate the cluster center point and help to approach the optimal. In addition, the theoretical analysis and convergence property of MCSC-EPMC are discussed. Numerous experiments were performed to compare the proposed MCSC-EPMC with the other seven clustering algorithms on several standard datasets. And the experimental results on several standards show the effectiveness and feasibility of MCSC-EPMC. It also enhanced the clustering performance by about 6.036% compared to the EPMC.},
  archive      = {J_APIN},
  author       = {Dai, Mingzhi and Feng, Xiang and Yu, Huiqun and Guo, Weibin and Li, Xiuquan},
  doi          = {10.1007/s10489-023-04484-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19742-19764},
  shortjournal = {Appl. Intell.},
  title        = {A monte carlo manifold spectral clustering algorithm based on emotional preference and migratory behavior},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lung cancer classification and identification framework with
automatic nodule segmentation screening using machine learning.
<em>APIN</em>, <em>53</em>(16), 19724–19741. (<a
href="https://doi.org/10.1007/s10489-023-04552-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is often a fatal disease. To minimize patient mortality, the ability to identify the nodule malignancy stage from computed tomography (CT) lung scans is critical. Most existing methods rely exclusively on deep learning (DL) networks. However, duplicated structures and insufficient training data make DL-based malignancy diagnosis from CT images time-consuming and imprecise. Additionally, the features that DL networks use to make decisions cannot be recognized. In response to these challenges in previous work, we built an end-to-end conventional machine learning (ML) model called ‘NoduleDiag’ to classify cancerous CT images and their malignancy stage based on the nodule characteristics observable in CT images that radiologists typically prefer for diagnosis. We also prepared a fully automatic pulmonary nodule segmentation model for implementation during the final stage of diagnosis (after disease-stage identification) and added it to our classification framework as immense practical assistance to radiologists and medical personnel. To the best of our knowledge, this work will be the first all in one computer-assisted diagnosis (CAD) framework with lung cancer classification, automatic malignancy-stage identification and nodule segmentation screening together using conventional ML approaches. Our proposed model classifies nodule malignancy on a scale of 1–5. It achieves 99.65% accuracy, 99.64% sensitivity, 99.76% precision, 99.9% specificity, and 99.91% negative predictive value (NPV) with XGBoost. Our segmentation model uses DeepLabv3+ (weights initialized by ResNet-18), achieving a 99.86% global accuracy, mean boundary F1 (BF) score of 0.976, 0.715 mean IoU and 0.997 weighted IoU.},
  archive      = {J_APIN},
  author       = {Alshayeji, Mohammad H. and Abed, Sa’ed},
  doi          = {10.1007/s10489-023-04552-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19724-19741},
  shortjournal = {Appl. Intell.},
  title        = {Lung cancer classification and identification framework with automatic nodule segmentation screening using machine learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic segmentation of thyroid with the assistance of the
devised boundary improvement based on multicomponent small dataset.
<em>APIN</em>, <em>53</em>(16), 19708–19723. (<a
href="https://doi.org/10.1007/s10489-023-04540-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely considered in medical image segmentation. However, the difficulty of acquiring medical images and labels can affect the accuracy of the segmentation results for deep learning methods. In this paper, an automatic segmentation method is proposed by devising a multicomponent neighborhood extreme learning machine to improve the boundary attention region of the preliminary segmentation results. The neighborhood features are acquired by training U-Nets with the multicomponent small dataset, which consists of original thyroid ultrasound images, Sobel edge images and superpixel images. Afterward, the neighborhood features are selected by min-redundancy and max-relevance filter in the designed extreme learning machine, and the selected features are used to train the extreme learning machine to obtain supplementary segmentation results. Finally, the accuracy of the segmentation results is improved by adjusting the boundary attention region of the preliminary segmentation results with the supplementary segmentation results. This method combines the advantages of deep learning and traditional machine learning, boosting the accuracy of thyroid segmentation accuracy with a small dataset in a multigroup test.},
  archive      = {J_APIN},
  author       = {Chen, Yifei and Zhang, Xin and Li, Dandan and Park, HyunWook and Li, Xinran and Liu, Peng and Jin, Jing and Shen, Yi},
  doi          = {10.1007/s10489-023-04540-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19708-19723},
  shortjournal = {Appl. Intell.},
  title        = {Automatic segmentation of thyroid with the assistance of the devised boundary improvement based on multicomponent small dataset},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Granularity-driven trisecting-and-learning models for
interval-valued rule induction. <em>APIN</em>, <em>53</em>(16),
19685–19707. (<a
href="https://doi.org/10.1007/s10489-023-04468-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision (3WD) theory and granular computing have recently received much attention as methods for thinking and problem solving. Based on the trisecting-and-acting model in the 3WD, this paper proposes two trisecting-and-learning models for rule induction, which begin with a constructed concept space and a search for the most suitable level of granularity in high-to-low and low-to-high manners, respectively. The learning results are sets of granules that can induce raw rules. By combining the advantages of granular computing and the trisecting-and-acting method, the models only need to process one or two parts of the trisecting result, simplifying the calculation. An interval-valued rule-inducing algorithm is proposed to obtain more general rules from raw rules. It coarsens the granularity further by merging the granules through the maximal adjacent relation of the learned granules. The interval-valued rules are obtained at the same time. The models in the paper are compatible with incomplete mixed data containing numeric, symbolic, do not care, and lost types. Symbolic data are converted into numeric types by considering their semantic order. This type of conversion enables intervals to be generated from symbolic data, producing more general rules. The rationality for allowing rules with missing data is also considered in this paper. Comparative analysis with existing works and experiments reveals the improvements of our models. Although there are a small fraction of incomplete rules and fewer intervalized rules on numeric data, the rules we induce have a higher coverage ratio and a much larger intervalized ratio for symbolic data. Moreover, our model induces simpler rules for all types of datasets.},
  archive      = {J_APIN},
  author       = {Chen, Yingxiao and Zhu, Ping and Li, Qiaoyi and Yao, Yiyu},
  doi          = {10.1007/s10489-023-04468-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19685-19707},
  shortjournal = {Appl. Intell.},
  title        = {Granularity-driven trisecting-and-learning models for interval-valued rule induction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Olfactory perception prediction model inspired by olfactory
lateral inhibition and deep feature combination. <em>APIN</em>,
<em>53</em>(16), 19672–19684. (<a
href="https://doi.org/10.1007/s10489-023-04517-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the relationship between the chemical structure and physicochemical properties of odor molecules and olfactory perception prediction, i.e. quantitative structure-odor relationship (QSOR), remains a challenging, decades-old task. With the development of deep learning, data-driven methods such as convolutional neural networks or deep neural networks have gradually been used to predict QSOR. However, the differences between the molecular structure of different molecules are subtle and complex, the molecular feature descriptors are numerous and their interactions are complex. In this paper, we propose the Lateral Inhibition-inspired feature pyramid dynamic Convolutional Network, using the feature pyramid network as the backbone network to extract the odor molecular structure features, which can deal with multi-scale changes well. Imitating the lateral inhibition mechanism of animal olfactory, we add the lateral inhibition-inspired attention maps to the dynamic convolution, to improve the prediction accuracy of olfactory perception prediction. Besides, due to a large number of molecular feature descriptors and their complex interactions, we propose to add Attentional Factorization Mechanism to a deep neural network to obtain molecular descriptive features through weighted deep feature combination based on the attention mechanism. Our proposed olfactory perception prediction model noted as LIFMCN has achieved a state-of-the-art result and will help the product design and quality assessment in food, beverage, and fragrance industries.},
  archive      = {J_APIN},
  author       = {Wang, Yu and Zhao, Qilong and Ma, Mingyuan and Xu, Jin},
  doi          = {10.1007/s10489-023-04517-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19672-19684},
  shortjournal = {Appl. Intell.},
  title        = {Olfactory perception prediction model inspired by olfactory lateral inhibition and deep feature combination},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse and robust SVM classifier for large scale
classification. <em>APIN</em>, <em>53</em>(16), 19647–19671. (<a
href="https://doi.org/10.1007/s10489-023-04511-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) has drawn wide attention in various fields, such as image classification, pattern recognition and disease diagnosis and so on. Nevertheless, it requires much memory and runs very slow in large-scale datasets setting. To reduce computational cost and the required storage, we first design a new sparse and robust SVM model according to our construct truncated smoothly clipped absolute deviation (SCAD) loss and then establish its first-order necessary and sufficient optimality conditions though newly defined proximal stationary point. Following the idea of the proximal stationary point, we define the Lts support vectors of Lts-SVM and then show that the Lts support vectors are a small portion of the whole training set, which is convenient for us to introduce a novel working set in each step. We next present a new alternating direction method of multipliers with working set (Lts-ADMM) to deal with Lts-SVM, which proves that the proposed algorithm not only converges to a local minimizer of Lts-SVM but also possesses a relatively low computational complexity. Finally, the extensive numerical experiments demonstrate that the proposed algorithm enjoys better performance than nine leading state-of-the-art methods with regard to best classification accuracy, smallest number of support vectors and super fast computational speed in large-scale datasets setting.},
  archive      = {J_APIN},
  author       = {Wang, Huajun and Shao, Yuanhai},
  doi          = {10.1007/s10489-023-04511-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19647-19671},
  shortjournal = {Appl. Intell.},
  title        = {Sparse and robust SVM classifier for large scale classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Mining frequent weighted utility patterns with dynamic
weighted items from quantitative databases. <em>APIN</em>,
<em>53</em>(16), 19629–19646. (<a
href="https://doi.org/10.1007/s10489-023-04554-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mining of frequent weighted utility patterns (FWUPs) is an important task in the field of data mining that aims to discover frequent patterns from quantitative databases while taking into account the importance or weight of each item. Although there are many approaches that have been proposed to solve this problem, all of these methods focus on databases in which the weight of each item is fixed. In real-life situations, the weight of each item may change over time; for example, the weights of the products in a store may change every month, every quarter, or every year. This is an important aspect that previous studies have not considered. In this paper, we first introduce a new problem that involves mining FWUPs with dynamic weighted items from quantitative databases (called dynamic quantitative databases, dQDBs). Following this, we propose an algorithm called dFWUT that uses a tidset data structure to solve this problem. Next, an algorithm called dFWUNL is developed that uses a new data structure called a WUNList to mine FWUPs from dQDBs. Finally, experiments on multiple databases are carried out to show that the proposed method is more efficient than another state-of-the-art algorithm in terms of running time and memory usage, especially for dense datasets or sparse datasets with a small mining threshold.},
  archive      = {J_APIN},
  author       = {Nguyen, Ham and Le, Nguyen and Bui, Huong and Le, Tuong},
  doi          = {10.1007/s10489-023-04554-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19629-19646},
  shortjournal = {Appl. Intell.},
  title        = {Mining frequent weighted utility patterns with dynamic weighted items from quantitative databases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic detection of relevant information, predictions and
forecasts in financial news through topic modelling with latent
dirichlet allocation. <em>APIN</em>, <em>53</em>(16), 19610–19628. (<a
href="https://doi.org/10.1007/s10489-023-04452-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. They are typically written by market experts who describe stock market events within the context of social, economic and political change. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (nlp) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (lda) to separate relevant from less relevant text and then analyse the relevant text using a Machine Learning-oriented temporal approach to identify predictions and speculative statements. Our solution outperformed a rule-based baseline system. We created an experimental data set composed of 2,158 financial news items that were manually labelled by nlp researchers to evaluate our solution. Inter-agreement Alpha-reliability and accuracy values, and rouge-l results endorse its potential as a valuable tool for busy investors. The rouge-l values for the identification of relevant text and predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph topic segmentation and co-reference resolution to separate author expression patterns, topic modelling with lda to detect relevant text, and discursive temporality analysis to identify forecasts and predictions within this text. Our solution may have compelling applications in the financial field, including the possibility of extracting relevant statements on investment strategies to analyse authors’ reputations.},
  archive      = {J_APIN},
  author       = {García-Méndez, Silvia and de Arriba-Pérez, Francisco and Barros-Vila, Ana and González-Castaño, Francisco J. and Costa-Montenegro, Enrique},
  doi          = {10.1007/s10489-023-04452-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19610-19628},
  shortjournal = {Appl. Intell.},
  title        = {Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with latent dirichlet allocation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rolling horizon wind-thermal unit commitment optimization
based on deep reinforcement learning. <em>APIN</em>, <em>53</em>(16),
19591–19609. (<a
href="https://doi.org/10.1007/s10489-023-04489-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing penetration of renewable energy has brought significant challenges for modern power system operation. Academic research and industrial practice show that adjusting unit commitment (UC) scheduling periodically according to new forecasts of renewable power provides a promising way to improve system stability and economy; however, this greatly increases the computational burden for solution methods. In this paper, a deep reinforcement learning (DRL) method is proposed to obtain timely and reliable solutions for rolling-horizon UC (RHUC). First, based on historical data and day-ahead point forecasting, a data-driven method is designed to construct typical wind power scenarios that are regarded as components of the state space of DRL. Second, a rolling mechanism is proposed to dynamically update the state space based on real-time wind power data. Third, unlike existing reinforcement learning-based UC solution methods that segment the continuous outputs of generators as discrete variables, all the variables in RHUC are regarded as continuous. Additionally, a series of updating regulations are defined to ensure that the model is realistic. Thus, a DRL algorithm, the twin delayed deep deterministic policy gradient (TD3), can be utilized to effectively solve the problem. Finally, several case studies are conducted based on different test systems to demonstrate the efficiency of the proposed method. According to the experimental results, the proposed algorithm can obtain high-quality solutions in a considerably shorter time than traditional methods, which leads to a reduction of at least 1.1% in the power system operation cost.},
  archive      = {J_APIN},
  author       = {Shi, Jinhao and Wang, Bo and Yuan, Ran and Wang, Zhi and Chen, Chunlin and Watada, Junzo},
  doi          = {10.1007/s10489-023-04489-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19591-19609},
  shortjournal = {Appl. Intell.},
  title        = {Rolling horizon wind-thermal unit commitment optimization based on deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proposal and comparative analysis of a voting-based election
algorithm for managing service replication in MANETs. <em>APIN</em>,
<em>53</em>(16), 19563–19590. (<a
href="https://doi.org/10.1007/s10489-023-04506-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel approaches are needed to better facilitate dynamic service replication management in mobile ad-hoc networks (MANETs) and to use and apply them within current and emerging autonomous intelligent systems and the Internet of Things (IoT) paradigm. Such approaches should address the context-awareness and self-adaptation of service replication, while paying special attention to quality attributes (e.g. availability, reliability, etc.) under specific runtime changes and adverse conditions with unstable communications and network partitions. The dynamic election for a node to host a service replica in MANETs can be based on the use of leader election (LE) algorithms. In this research work, a new voting-based election algorithm for managing dynamic service replication in MANETs (namely, VOELA) is proposed. This algorithm is based upon a utility function to score node resources and features (i.e., battery level and topology position) to decide where the service replica will be activated. VOELA is compared to a previously proposed consensus-based algorithm and three other well-known leader election algorithms in terms of service availability, election algorithm reliability, coordination message usage, and network lifetime. For this comparative analysis, the ns-3 network simulator is used together with three different mobility models, namely Manhattan Grid Mobility (MGM), Random Walk Mobility (RWM) and Reference Point Group Mobility (RPGM). The VOELA algorithm demonstrates in balance the most promising results.},
  archive      = {J_APIN},
  author       = {Guerrero-Contreras, Gabriel and Balderas-Díaz, Sara and Garrido, José Luis and Rodríguez-Fórtiz, María José and O’Hare, Gregory M. P.},
  doi          = {10.1007/s10489-023-04506-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19563-19590},
  shortjournal = {Appl. Intell.},
  title        = {Proposal and comparative analysis of a voting-based election algorithm for managing service replication in MANETs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of a production system of green products
considering single-level trade credit financing via a parametric
approach of intervals and meta-heuristic algorithms. <em>APIN</em>,
<em>53</em>(16), 19532–19562. (<a
href="https://doi.org/10.1007/s10489-023-04493-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of chemicals and preservatives during the production of most daily-used products has increased rapidly over the last few decades. In turn, manufacturers, to improve the sustainability of a manufacturing unit in the current highly competitive market, are obliged to provide various facilities to their customers. However, estimating the best optimal decision for manufacturing companies seems like a daunting task amid highly uncertain market conditions. By shedding more light on these facts and considering the gaps in the existing literature on production inventory, this work aims to incorporate a production model which is concerned with the factors mentioned above, so that it can be beneficial for a manufacturer. This work involves the demonstration of an imperfect green production system under single-layer full-trade credit financing and partial backlog shortages using interval flexibility. Based on the credit period lengths provided by the manufacturer to the consumers, three distinct cases have arisen and for each case, the average profit of the system appears to be an interval valued optimal control problem, and several theorems are evaluated to obtain analytical forms of respective objective functions of each case. Additionally, to test the effect of different credit lengths on optimal policy and to examine the feasibility of each case, a numerical example is solved using the Equilibrium Optimizer (EO) algorithm. Finally, a sensitivity experiment is performed and the results obtained from the simulation are shown graphically to gain some managerial insights.},
  archive      = {J_APIN},
  author       = {Das, Subhajit and Manna, Amalesh Kumar and Shaikh, Ali Akbar and Konstantaras, Ioannis},
  doi          = {10.1007/s10489-023-04493-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19532-19562},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of a production system of green products considering single-level trade credit financing via a parametric approach of intervals and meta-heuristic algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning with text augmentation for text
classification. <em>APIN</em>, <em>53</em>(16), 19522–19531. (<a
href="https://doi.org/10.1007/s10489-023-04453-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various contrastive learning models have been successfully applied to representation learning for downstream tasks. The positive samples used in contrastive learning are often derived from augmented data, which improve the performance of many computer vision tasks while still not being fully utilized for natural language processing tasks, such as text classification. The existing data augmentation methods have been rarely applied to contrastive learning in the field of NLP. In this paper, we propose a Text Augmentation Contrastive Learning Representation model, TACLR, that combines the easy text augmentation techniques (i.e., synonym replacement, random insertion, random swap and random deletion) and textMixup augmentation method with contrastive learning for text classification task. Furthermore, we propose a unified method that allows flexibly adapting supervised, semi-supervised and unsupervised learning. Experimental results on five text classification datasets show that our TACLR can significantly improve text classification accuracies. We also provide extensive ablation studies for exploring the validity of each component of our model. The source code of our work is publicly available from https://gitlab.com/models-for-paper/taclr .},
  archive      = {J_APIN},
  author       = {Jia, Ouyang and Huang, Huimin and Ren, Jiaxin and Xie, Luodi and Xiao, Yinyin},
  doi          = {10.1007/s10489-023-04453-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19522-19531},
  shortjournal = {Appl. Intell.},
  title        = {Contrastive learning with text augmentation for text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic connection pruning for densely connected
convolutional neural networks. <em>APIN</em>, <em>53</em>(16),
19505–19521. (<a
href="https://doi.org/10.1007/s10489-023-04513-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Densely connected convolutional neural networks dominate in a variety of downstream tasks due to their extraordinary performance. However, such networks typically require excessive computing resources, which hinders their deployment on mobile devices. In this paper, we propose a dynamic connection pruning algorithm, which is a cost-effective method to eliminate a large amount of redundancy in densely connected networks. First, we propose a Sample-Evaluation process to assess the contributions of connections. Specifically, sub-networks are sampled from the unpruned network in each epoch, while the parameters of the unpruned network are subsequently updated and the contributions of the connections are evaluated based on the performance of the sub-networks. Connections with low contribution will be pruned first. Then, we search for the distribution of pruning ratios by the Markov process. Finally, we prune the network based on the connection contribution and pruning ratios learned in the above two stages and obtain a lightweight network. The effectiveness of our method is verified on both high-level and low-level tasks. On the CIFAR-10 dataset, the top-1 accuracy barely drops (-0.03%) when FLOPs are reduced by 46.8%. In the super-resolution task, our model remarkably outperforms other lightweight networks in both visual and quantitative experiments. These results verify the effectiveness and generality of our proposed method.},
  archive      = {J_APIN},
  author       = {Hu, Xinyi and Fang, Hangxiang and Zhang, Ling and Zhang, Xue and Yang, Howard H. and Yang, Dongxiao and Peng, Bo and Li, Zheyang and Hu, Haoji},
  doi          = {10.1007/s10489-023-04513-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19505-19521},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic connection pruning for densely connected convolutional neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to discover frequent weighted subgraphs
using the average measure. <em>APIN</em>, <em>53</em>(16), 19491–19504.
(<a href="https://doi.org/10.1007/s10489-023-04501-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining a weighted single large graph has recently attracted many researchers. The WeGraMi algorithm is considered the state-of-the-art among current approaches. It uses a MaxMin measure to calculate weights for all mined subgraphs. However, if all values in the domain have the same role and the user needs an average value of that domain, then we have to use another measure. In this paper, we introduce a novel algorithm called AWeGraMi (Average Weighted Graph Mining) to solve the above problem, and our method calculates the weight based on the average of all values in the domain. We also apply the MaxMin measure as an upper-bound to prune the search space. The new algorithm can mine all frequent weighted subgraphs effectively. Our experiments on the directed and undirected datasets have shown that AWeGraMi has better performance in comparison to post-processing GraMi for all three criteria: search space (the number of candidate subgraphs), running time, and memory consumption.},
  archive      = {J_APIN},
  author       = {Le, Ngoc-Thao and Vo, Bay and Yun, Unil and Le, Bac},
  doi          = {10.1007/s10489-023-04501-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19491-19504},
  shortjournal = {Appl. Intell.},
  title        = {A novel approach to discover frequent weighted subgraphs using the average measure},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research progress of spiking neural network in image
classification: A review. <em>APIN</em>, <em>53</em>(16), 19466–19490.
(<a href="https://doi.org/10.1007/s10489-023-04553-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural network (SNN) is a new generation of artificial neural networks (ANNs), which is more analogous with the brain. It has been widely considered with neural computing and brain-like intelligence. SNN is a sparse trigger event-driven model, and it has the characteristics of hardware friendliness and energy saving. SNN is more suitable for hardware implementation and rapid information processing. SNN is also a powerful method for deep learning (DL) to study brain-like computing. In this paper, the common SNN learning and training methods in the field of image classification are reviewed. In detail, we examine the SNN algorithms based on synaptic plasticity, approximate backpropagation (BP), and ANN to SNN. This paper comprehensively introduces and tracks the latest progress of SNN. On this basis, we also analyze and discuss the challenges and opportunities it faces. Finally, this paper prospects for the future development of SNN in the aspects of the biological mechanism, network training and design, computing platform, and interdisciplinary communication. This review can provide a reference for the research of SNN to promote its application in complex tasks.},
  archive      = {J_APIN},
  author       = {Niu, Li-Ye and Wei, Ying and Liu, Wen-Bo and Long, Jun-Yu and Xue, Tian-hao},
  doi          = {10.1007/s10489-023-04553-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19466-19490},
  shortjournal = {Appl. Intell.},
  title        = {Research progress of spiking neural network in image classification: A review},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An enhanced SSD with feature cross-reinforcement for
small-object detection. <em>APIN</em>, <em>53</em>(16), 19449–19465. (<a
href="https://doi.org/10.1007/s10489-023-04544-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited feature information possessed by small objects in images, it is difficult for a single-shot multibox detector (SSD) to quickly notice the important regions of these small image objects. We propose an enhanced SSD based on feature cross-reinforcement (FCR-SSD). For shallow sampling, an improved group shuffling-efficient channel attention (GS-ECA) mechanism is used to make the model focus on the object areas rather than the background. Then, an FCR module allows the multiscale information from the shallow layer to be passed to the subsequent layer and fused to generate an enhanced feature map, which improves the utilization of the context information associated with small objects. We develop an adaptive algorithm for calculating positive and negative candidate box selection thresholds to select positive and negative samples, determine the intersection over union (IOU) thresholds of candidate boxes and ground-truth boxes, and adaptively determine the threshold for each ground-truth box. The proposed FCR-SSD algorithm achieves 79.6% mean average precision (mAP) for the PASCAL VOC 2007 dataset and 30.1% mAP for the MS COCO dataset at 34.2 frames per second (FPS) when run on an RTX 3080Ti GPU. The experimental results show that the FCR-SSD model yields high accuracy and a good detection speed in small-target detection tasks.},
  archive      = {J_APIN},
  author       = {Gong, Lixiong and Huang, Xiao and Chao, Yinkang and Chen, Jialin and Lei, Binwen},
  doi          = {10.1007/s10489-023-04544-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19449-19465},
  shortjournal = {Appl. Intell.},
  title        = {An enhanced SSD with feature cross-reinforcement for small-object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage single image reflection removal with
reflection-aware guidance. <em>APIN</em>, <em>53</em>(16), 19433–19448.
(<a href="https://doi.org/10.1007/s10489-022-04391-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing undesired reflection from an image captured through a glass surface is a very challenging problem with many practical applications. For improving reflection removal, cascaded deep models have been usually adopted to estimate the transmission in a progressive manner. However, most existing methods are still limited in exploiting the result in prior stage for guiding transmission estimation. In this paper, we present a novel two-stage network with reflection-aware guidance (RAGNet) for single image reflection removal (SIRR). To be specific, the reflection layer is firstly estimated due to that it generally is much simpler and is relatively easier to estimate. Reflection-aware guidance (RAG) module is then elaborated for better exploiting the estimated reflection in predicting transmission layer. By incorporating feature maps from the estimated reflection and observation, RAG can be used (i) to mitigate the effect of reflection from the observation, and (ii) to generate mask in soft partial convolution for mitigating the effect of deviating from linear combination hypothesis. A dedicated mask loss is further presented for reconciling the contributions of encoder and decoder features. Experiments on five commonly used datasets demonstrate the quantitative and qualitative superiority of our RAGNet in comparison to the state-of-the-art SIRR methods. The source code and pre-trained model are available at https://github.com/liyucs/RAGNet .},
  archive      = {J_APIN},
  author       = {Li, Yu and Liu, Ming and Yi, Yaling and Li, Qince and Ren, Dongwei and Zuo, Wangmeng},
  doi          = {10.1007/s10489-022-04391-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19433-19448},
  shortjournal = {Appl. Intell.},
  title        = {Two-stage single image reflection removal with reflection-aware guidance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic visual simultaneous localization and mapping based
on semantic segmentation module. <em>APIN</em>, <em>53</em>(16),
19418–19432. (<a
href="https://doi.org/10.1007/s10489-023-04531-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) is a key technique for mobile robotics. Moving objects can vastly impair the performance of a visual SLAM system. To deal with the problem, a new semantic visual SLAM system for indoor environments is proposed. Our system adds a semantic segmentation network and geometric model to detect and remove dynamic feature points on moving objects. Moreover, a 3D point cloud map with semantic information is created using semantic labels and depth images. We evaluate our method on the TUM RGB-D dataset and real-world environments. The evaluation metrics used are absolute trajectory error and relative position error. Experimental results show our method improves the accuracy in dynamic scenes compared to ORB-SLAM3 and other advanced methods.},
  archive      = {J_APIN},
  author       = {Jin, Jing and Jiang, Xufeng and Yu, Chenhui and Zhao, Lingna and Tang, Zhen},
  doi          = {10.1007/s10489-023-04531-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19418-19432},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic visual simultaneous localization and mapping based on semantic segmentation module},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inspection-l: Self-supervised GNN node embeddings for money
laundering detection in bitcoin. <em>APIN</em>, <em>53</em>(16),
19406–19417. (<a
href="https://doi.org/10.1007/s10489-023-04504-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Criminals have become increasingly experienced in using cryptocurrencies, such as Bitcoin, for money laundering. The use of cryptocurrencies can hide criminal identities and transfer hundreds of millions of dollars of dirty funds through their criminal digital wallets. However, this is considered a paradox because cryptocurrencies are goldmines for open-source intelligence, giving law enforcement agencies more power when conducting forensic analyses. This paper proposes Inspection-L, a graph neural network (GNN) framework based on a self-supervised Deep Graph Infomax (DGI) and Graph Isomorphism Network (GIN), with supervised learning algorithms, namely Random Forest (RF), to detect illicit transactions for anti-money laundering (AML). To the best of our knowledge, our proposal is the first to apply self-supervised GNNs to the problem of AML in Bitcoin. The proposed method was evaluated on the Elliptic dataset and shows that our approach outperforms the state-of-the-art in terms of key classification metrics, which demonstrates the potential of self-supervised GNN in the detection of illicit cryptocurrency transactions.},
  archive      = {J_APIN},
  author       = {Lo, Wai Weng and Kulatilleke, Gayan K. and Sarhan, Mohanad and Layeghy, Siamak and Portmann, Marius},
  doi          = {10.1007/s10489-023-04504-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19406-19417},
  shortjournal = {Appl. Intell.},
  title        = {Inspection-L: Self-supervised GNN node embeddings for money laundering detection in bitcoin},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quadratic association vector and dynamic guided operator
search algorithm for large-scale sparse multi-objective optimization
problem. <em>APIN</em>, <em>53</em>(16), 19384–19405. (<a
href="https://doi.org/10.1007/s10489-023-04500-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many large-scale sparse multi-objective optimization problems in real life, but it is difficult to solve such problems due to the high dimension of decision variables and the excessive sparseness of Pareto optimal solutions. To address these issues, this paper proposes a quadratic association vector and dynamic guided operator search algorithm for large-scale sparse multi-objective optimization problems. Firstly, according to the individual distribution in the decision space, the reference vector is used to quadratically associate the individual, aiming to find better solutions during the initialization process. Secondly, the dynamic guided operator is introduced into the crossover mutation. The population is guided to evolve toward the sparse optimal solution by flipping the real and binary variables according to the dynamic guided operator. Thirdly, individuals are selected into the mating pool for crossover mutation based on the decision variables of the population individuals, with the aim of further enhancing the convergence of the algorithm. The experimental results on eight benchmark problems show that the algorithm obtains the best comprehensive performance on 66.7% of the test problems. The proposed algorithm is superior to the existing algorithm in terms of effectiveness.},
  archive      = {J_APIN},
  author       = {Gu, Qinghua and Sun, Yixiao and Wang, Qian and Chen, Lu},
  doi          = {10.1007/s10489-023-04500-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19384-19405},
  shortjournal = {Appl. Intell.},
  title        = {A quadratic association vector and dynamic guided operator search algorithm for large-scale sparse multi-objective optimization problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAPredRNN: Multi-attention predictive RNN for traffic flow
prediction by dynamic spatio-temporal data fusion. <em>APIN</em>,
<em>53</em>(16), 19372–19383. (<a
href="https://doi.org/10.1007/s10489-023-04494-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a key component of intelligent transportation system, especially for increasingly complex urban traffic networks. An accurate flow prediction can help to relieve traffic congestion and reduce traffic accidents. However, the patterns of traffic flow are very complex and volatile, which will be affected by many factors, such as traffic accident, weather, point-of-interests, etc. It is still a challenging issue due to the high nonlinearity and dynamicity of traffic flow. In this paper, we propose a multi-attention predictive recurrent neural networks (MAPredRNN) for traffic flow prediction by dynamic spatio-temporal data fusion. First, convolutional neural network and predictive recurrent neural network are used to obtain the spatio-temporal information of the closeness, periodicity and trend features. And then, multi-attention mechanism is employed to further extract feature fusing information of closeness, periodicity and trend. Experimental results conducted on two real datasets show that our proposed method outperforms the compared algorithms.},
  archive      = {J_APIN},
  author       = {Huang, Xiaohui and Jiang, Yuan and Tang, Jie},
  doi          = {10.1007/s10489-023-04494-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19372-19383},
  shortjournal = {Appl. Intell.},
  title        = {MAPredRNN: Multi-attention predictive RNN for traffic flow prediction by dynamic spatio-temporal data fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Environment-sensitive crowd behavior modeling method based
on reinforcement learning. <em>APIN</em>, <em>53</em>(16), 19356–19371.
(<a href="https://doi.org/10.1007/s10489-023-04509-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing crowd evacuation methods focus on internal factors and do not consider the influence of the external environment factors, producing unrealistic global behavior occurs when individuals are moving through the crowded space. As an essential part of a building, safety indication signs (SISs) are a form of environmental information and perceptual access that play an important role in promoting wayfinding by virtue of their guiding role in movement direction and route selection by providing guidance, warning and mandatory message to people. In this paper, we propose an innovative crowd simulation method guided by SISs with reinforcement learning strategy for use in emergencies. To illustratethe guiding function of the SISs, we establish a guidance field for each SIS and add the attractive force to the guidance field. Besides, we formulate the multi-agent (crowd) navigation problem as an action-selection problem and design a novel reinforcement learning strategy for driving individuals to accomplish collision-free movement more efficiently. Particularly, we define a state transition strategy between the SIS area and the non-SIS area to achieve continuity of guidance. We use extensive simulations to highlight the potential of our method in different scenarios and evaluate the results in terms of evacuation efficiency and the reasonableness of SIS placement. In practice, our system runs at interactive rates and can solve complex planning problems involving one or more groups.},
  archive      = {J_APIN},
  author       = {Pang, Chen and Lyu, Lei and Zhou, Qinglin and Zhou, Limei},
  doi          = {10.1007/s10489-023-04509-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19356-19371},
  shortjournal = {Appl. Intell.},
  title        = {Environment-sensitive crowd behavior modeling method based on reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Numerical computation of ocean HABs image enhancement based
on empirical mode decomposition and wavelet fusion. <em>APIN</em>,
<em>53</em>(16), 19338–19355. (<a
href="https://doi.org/10.1007/s10489-023-04502-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the microscopic images of Harmful Algae Blooms (HABs) acquired in marine engineering are subject to blurred cell textures and poor overall clarity due to the effects of seawater impurities, unknown suspended particle deposits and high-speed cell motions. To solve the above problems, a microscopic image enhancement method based on the recursive-overlapped contrast limited adaptive histogram specification (CLAHS) and dual-image wavelet fusion (RO-CLAHS and DIWF) is proposed in this paper. It combines three main steps: homomorphic filtering, empirical modal feature map extraction, and dual-image wavelet fusion. This method firstly adopts homomorphic filtering to strengthen the illumination uniformity of the entire HABs image, and then obtains an empirical modal feature map of algal cells by an improved empirical modal decomposition method, which highlights the detailed features of algal cells due to histogram stretching, clip limit and grey-level mapping. Finally, this paper proposes a dual-image wavelet fusion method adapted for HAB microscopic images, which integrates the images processed by contrast limited adaptive histogram equalization (CLAHE) with an empirical modal feature map and then achieves the image enhancement effect. The comparison experimental results show that the enhanced microscopic images of HABs by this method presents the best texture clarity and global contrast and improves the accuracy of algae image edge detection significantly.},
  archive      = {J_APIN},
  author       = {Wu, Geng-Kun and Zhang, Bei-Ping and Xu, Jie},
  doi          = {10.1007/s10489-023-04502-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19338-19355},
  shortjournal = {Appl. Intell.},
  title        = {Numerical computation of ocean HABs image enhancement based on empirical mode decomposition and wavelet fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general multi-factor norm based low-rank tensor completion
framework. <em>APIN</em>, <em>53</em>(16), 19317–19337. (<a
href="https://doi.org/10.1007/s10489-023-04477-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank tensor completion aims to recover the missing entries of the tensor from its partially observed data by using the low-rank property of the tensor. Since rank minimization is an NP-hard problem, the convex surrogate nuclear norm is usually used to replace the rank norm and has obtained promising results. However, the nuclear norm is not a tight envelope of the rank norm and usually over-penalizes large singular values. In this paper, inspired by the effectiveness of the matrix Schatten-q norm, which is a tighter approximation of rank norm when 0 &lt; q &lt;  1, we generalize the matrix Schatten-q norm to tensor case and propose a Unitary Transformed Tensor Schatten-q Norm (UTT-Sq) with an arbitrary unitary transform matrix. More importantly, the factor tensor norm surrogate theorem is derived. We prove large-scale UTT-Sq norm (which is nonconvex and not tractable when 0 &lt; q &lt; 1) is equivalent to minimizing the weighted sum formulation of multiple small-scale UTT- $S_{q_{i}}$ (with different qi and qi ≥ 1). Based on this equivalence, we propose a low-rank tensor completion framework using Unitary Transformed Tensor Multi-Factor Norm (UTTMFN) penalty. The optimization problem is solved using the Alternating Direction Method of Multipliers (ADMM) with the proof of convergence. Experimental results on synthetic data, images and videos show that the proposed UTTMFN can achieve competitive results with the state-of-the-art methods for tensor completion.},
  archive      = {J_APIN},
  author       = {Tian, Jialue and Zhu, Yulian and Liu, Jiahui},
  doi          = {10.1007/s10489-023-04477-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19317-19337},
  shortjournal = {Appl. Intell.},
  title        = {A general multi-factor norm based low-rank tensor completion framework},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A similar structural and semantic integrated method for RDF
entity embedding. <em>APIN</em>, <em>53</em>(16), 19302–19316. (<a
href="https://doi.org/10.1007/s10489-023-04520-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource Description Framework (RDF) graphs have become an important data source for many knowledge discovery algorithms and data mining tasks. However, most complex analyses that use knowledge discovery algorithms require data in a vector representation format. As a result, several RDF entity embedding techniques have emerged in which entities in the RDF graph are represented as low-dimensional vectors. These techniques generate sequences of entities using graph walk and use language modeling techniques to extract the feature from the sequences used to learn the embedding. However, sequences produced by graph walks only capture structural context; they are unable to capture latent context, such as semantically related information, which is an important property of RDF data. In this paper, we present a novel method that consists of a series of steps that generate sequences. These sequences not only capture structural context but also semantic property. The method for structural context includes (1) a new concept of similar entities in which tradeoffs are made between similar outgoing edges and outgoing nodes and (2) a new structural similarity, which calculates the similarity between two entities in each sequence. We can generate sequences based on structural similarity so that similar entities contain sequences with similar structures. The method for the semantic property combines sequences with the same semantic to generate latent sequences that cannot be generated by traversing the graph. This paper presents experimental results and a case study using real graphs to show that the proposed method outperforms existing methods in terms of quality and efficiency.},
  archive      = {J_APIN},
  author       = {Van, Duong Thi Thu and Lee, Young-Koo},
  doi          = {10.1007/s10489-023-04520-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19302-19316},
  shortjournal = {Appl. Intell.},
  title        = {A similar structural and semantic integrated method for RDF entity embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). GLANet: Temporal knowledge graph completion based on global
and local information-aware network. <em>APIN</em>, <em>53</em>(16),
19285–19301. (<a
href="https://doi.org/10.1007/s10489-023-04481-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion (KGC) has been widely explored, but the task of temporal knowledge graph completion (TKGC) for predicting future events is far from perfection. Some embedding-based approaches have achieved significant results on the TKGC task by modeling the structural information of each temporal snapshot and the evolution between temporal snapshots. However, due to the uneven distribution of data in knowledge graphs (KGs), models that only utilize local structure and time series information suffer from information sparsity, resulting in some entities failing to obtain a better embedding representation due to less available information. Moreover, existing methods usually do not distinguish between the time span and frequency of historical information, which reduces the performance of link prediction. For this reason, we propose the G lobal and L ocal Information-A ware Net work (GL-ANet) to capture both global and local information. In particular, to model global information, we capture global structural information of entities across time using a global neighborhood aggregator to enrich the representation of entities; global historical information is obtained based on the frequency and time span of historical facts, focusing on recent and frequent events rather than all historical events to suggest the performance of link prediction; to model local information, we propose a two-layer attention network to capture local structural information at each timestamp, using a gating mechanism and GRU to capture local evolution information. Extensive experiments demonstrate the effectiveness of our model, achieving significant improvements and outperforming state-of-the-art models on five benchmark datasets.},
  archive      = {J_APIN},
  author       = {Wang, Jingbin and Lin, Xinyu and Huang, Hao and Ke, Xifan and Wu, Renfei and You, Changkai and Guo, Kun},
  doi          = {10.1007/s10489-023-04481-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19285-19301},
  shortjournal = {Appl. Intell.},
  title        = {GLANet: Temporal knowledge graph completion based on global and local information-aware network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The granulation attribute reduction of multi-label data.
<em>APIN</em>, <em>53</em>(16), 19266–19284. (<a
href="https://doi.org/10.1007/s10489-023-04510-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning, the classification obtained by attribute set is usually inconsistent with that based on label set. This makes multi-label learning to possess fuzziness and uncertainty, which leads to the poor performance of learning algorithms. To address this fuzziness and uncertainty, this paper first combines multiple labels into a granulation multi-label decision function by using the classification obtained by attribute set. Then we give three kinds of formulations to construct the granulation multi-label decision functions, which are divided three levels, that is, macroscopic level, mesoscopic level and microscopic level. Moreover, by using the granulation multi-label decision function, this paper provides the method that transforms a multi-label decision table into a granulation multi-label decision table. Then three-level granulation attribute reductions of a multi-label decision table are defined and investigated. Furthermore, this paper shows that the existing multi-label attribute reduction, complementary decision reduction, can be viewed as the coarsest granularity reduction proposed by this paper, that is, a macroscopic-level granulation attribute reduction. In summary, this paper establishes a relatively systematic theoretical framework for attribute reduction of multi-label data based on rough set theory. Finally, by several comparative analysis, the reasonability, feasibility and effectiveness of these granulation attribute reductions are demonstrated.},
  archive      = {J_APIN},
  author       = {Wang, Zhaohao and Zhang, Xiaoping},
  doi          = {10.1007/s10489-023-04510-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19266-19284},
  shortjournal = {Appl. Intell.},
  title        = {The granulation attribute reduction of multi-label data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A joint attention enhancement network for text
classification applied to citizen complaint reporting. <em>APIN</em>,
<em>53</em>(16), 19255–19265. (<a
href="https://doi.org/10.1007/s10489-023-04490-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citizen complaint classification plays an important role in the construction of the smart city. For text data, the most expressive semantic information is reflected in the keyword of the text. With the proposed Transformer structure and further expansion of the model structure, natural language processing has embarked on a path of fine-tuning the pre-trained model based on the multi-headed attention mechanism. Although the above method works well, it further deepens the black box model of the network. To verify whether the multi-headed attention mechanism adds enough attention to the keyword information, this paper proposes a joint attention enhancement network that places the attention mechanism outside the main network model. This paper uses the idea of lexical frequency statistics to obtain keyword information through the macroscopic use of corpus contents and improves the attention through knowledge incorporation based on soft attention. In this paper, a comparison experiment is performed by the current hot open-source network models on Hugging Face. Experiments show that the proposed model improves about 10%-20% in accuracy compared with the different original models, while the network training time only increases about 5%. The joint enhancement network can identify the key region of input data more accurately and converge quickly.},
  archive      = {J_APIN},
  author       = {Wang, Yuanhang and Zhou, Yonghua and Mei, Yiduo},
  doi          = {10.1007/s10489-023-04490-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19255-19265},
  shortjournal = {Appl. Intell.},
  title        = {A joint attention enhancement network for text classification applied to citizen complaint reporting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSDCLA: Driving style detection via hybrid CNN-LSTM with
multi-level attention fusion. <em>APIN</em>, <em>53</em>(16),
19237–19254. (<a
href="https://doi.org/10.1007/s10489-023-04451-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving style detection is an essential real-world requirement in diverse contexts, such as traffic safety, car insurance and fuel consumption optimization. However, the existing methods either rely on handcrafted features or fail to explore deep spatial-temporal features from multi-modal sensing signals. In this paper, we propose a novel attention-based hybrid convolutional neural network (CNN) and long short-term memory (LSTM) framework named DSDCLA to address these problems. Specifically, DSDCLA first introduces CNN and self-attention for extracting local spatial features from multi-modal driving sequences. Then, we utilize LSTM and multi-head attention to explore the long-term temporal relationships between timesteps. Therefore, DSDCLA can identify driving style by short- and long-term spatial-temporal features. Furthermore, we design three variants with different levels of fusion, which shows the advantage of selecting components and improves the interpretability. We extensively evaluated the proposed DSDCLA on two public real-world datasets, and the experimental results show that DSDCLA outperforms the current state-of-the-art methods, achieving the F1-scores of 97.03% and 97.65%. Numerous ablation studies and visualizations indicate the effectiveness of the model and the importance of multi-level attention fusion for identifying driving style between timesteps.},
  archive      = {J_APIN},
  author       = {Liu, Jing and Liu, Yang and Li, Di and Wang, Hanqi and Huang, Xiaohong and Song, Liang},
  doi          = {10.1007/s10489-023-04451-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19237-19254},
  shortjournal = {Appl. Intell.},
  title        = {DSDCLA: Driving style detection via hybrid CNN-LSTM with multi-level attention fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge discovery assistants for crash simulations with
graph algorithms and energy absorption features. <em>APIN</em>,
<em>53</em>(16), 19217–19236. (<a
href="https://doi.org/10.1007/s10489-022-04371-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the representation of data from finite element car crash simulations in a graph database to empower analysis approaches. The industrial perspective of this work is to narrow the gap between the uptake of modern machine learning methods and the current computer-aided engineering-based vehicle development workflow. The main goals for the graph representation are to achieve searchability and to enable pattern and trend investigations in the product development history. In this context, we introduce features for car crash simulations to enrich the graph and to provide a summary overview of the development stages. These features are based on the energy output of the finite element solver and, for example, enable filtering of the input data by identifying essential components of the vehicle. Additionally, based on these features, we propose fingerprints for simulation studies that assist in summarizing the exploration of the design space and facilitate cross-platform as well as load-case comparisons. Furthermore, we combine the graph representation with energy features and use a weighted heterogeneous graph visualization to identify outliers and cluster simulations according to their similarities. We present results on data from the real-life development stages of an automotive company.},
  archive      = {J_APIN},
  author       = {Pakiman, Anahita and Garcke, Jochen and Schumacher, Axel},
  doi          = {10.1007/s10489-022-04371-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19217-19236},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge discovery assistants for crash simulations with graph algorithms and energy absorption features},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPANet: Feature pyramid attention network for crowd
counting. <em>APIN</em>, <em>53</em>(16), 19199–19216. (<a
href="https://doi.org/10.1007/s10489-023-04499-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting in congested scenarios is an essential yet challenging task in detecting abnormal crowd for contemporary urban planning. The counting accuracy has been significantly improved with the rapid development of deep learning over the last decades. However, current models are fragile in the real-world application mainly due to two inherent weaknesses: (1) Scale variations always exert negative influences on counting accuracy. (2) Overwhelming amount of parameters in the deep neural network will lead to low efficiency. To address these two limitations, in this paper, we propose a Feature Pyramid Attention Network (FPANet). Specifically, the FPANet consists of three modules, namely the feature pyramid module, attention module, and multiscale aggregation module. The feature pyramid module is built in a lightweight architecture to extract multiscale features. The attention module focuses on the crowd region and suppresses misleading information. The multiscale aggregation module is derived to adaptively fuse the discriminative knowledge extracted in different granularities. Additionaly, the efficiency of FPANet is boosted by the multi-group structure. Experimental results on five crowd benchmark datasets, i.e., ShanghaiTech, UCF_CC_50, UCF-QNRF, WorldExpo’10, and NWPU-Crowd, and two cross-domain datasets, i.e., CARPK, and PUCPR+, demonstrate that the FPANet achieves superior performances in terms of accuracy, efficiency and generalization.},
  archive      = {J_APIN},
  author       = {Zhai, Wenzhe and Gao, Mingliang and Li, Qilei and Jeon, Gwanggil and Anisetti, Marco},
  doi          = {10.1007/s10489-023-04499-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19199-19216},
  shortjournal = {Appl. Intell.},
  title        = {FPANet: Feature pyramid attention network for crowd counting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning vision based autonomous lateral vehicle control
without supervision. <em>APIN</em>, <em>53</em>(16), 19186–19198. (<a
href="https://doi.org/10.1007/s10489-023-04478-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning methods using image data as input have shown promising results in the context of vehicle control. However, these supervised methods have two main disadvantages: 1) They require a copious amount of labeled training data, which is difficult and expensive to collect. 2) Such models do not perform well, when situations that are not in the distribution of the training set are encountered. This includes deviations from the designated driving behavior. We therefore provide a framework to mitigate these problems from merely an unlabeled sequence of images. Visual Odometry is first used to determine the vehicle trajectory. Model Predictive Control (MPC) then uses this trajectory to implicitly infer the steering labels. Meanwhile, synthesized images at deviated trajectories are included in the training distribution for enhanced robustness of the neural network model. Experimental results demonstrate that the performance of our network is at par with methods requiring additional data collection or supervision. Code and supplementary information is available here: https://github.com/idilsulo/nn-driving},
  archive      = {J_APIN},
  author       = {Khan, Qadeer and Sülö, Idil and Öcal, Melis and Cremers, Daniel},
  doi          = {10.1007/s10489-023-04478-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19186-19198},
  shortjournal = {Appl. Intell.},
  title        = {Learning vision based autonomous lateral vehicle control without supervision},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting textual adversarial examples through text
modification on text classification systems. <em>APIN</em>,
<em>53</em>(16), 19161–19185. (<a
href="https://doi.org/10.1007/s10489-022-03313-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a method for detecting adversarial examples using a text modification module. The proposed method detects adversarial examples based on the change in classification result that occurs when a sample is modified by arbitrarily changing a specific word to a similar word. The method exploits the fact that the adversarial example’s sensitivity to changes to specific words is greater than that of the original sample. Experiments were conducted with three datasets (AG’s News, a movie review dataset, and the IMDB Large Movie Review Dataset), and TensorFlow was used as a machine learning library. In the experiment using these datasets, the proposed method detected an average of 71.7% of the adversarial sentences while minimizing the change in the results given by the model for the original sentences to an average of 2.9%.},
  archive      = {J_APIN},
  author       = {Kwon, Hyun and Lee, Sanghyun},
  doi          = {10.1007/s10489-022-03313-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19161-19185},
  shortjournal = {Appl. Intell.},
  title        = {Detecting textual adversarial examples through text modification on text classification systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast coarse-to-fine point cloud registration based on
optical flow for autonomous vehicles. <em>APIN</em>, <em>53</em>(16),
19143–19160. (<a
href="https://doi.org/10.1007/s10489-022-04308-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a vital prerequisite for many autonomous vehicle tasks. However, balancing the accuracy and computational complexity is very challenging for existing point cloud registration algorithms. This paper proposes a fast coarse-to-fine point cloud registration approach for autonomous vehicles. Our method uses nearest neighbor sample consensus optical flow pairwise matching resulting from a 2D bird’s eye view to initialize the coarse registration. It provides an initial 2D guess matrix for the fine registration and effectively reduces the computational complexity. In two-stage registration, our approach eliminates outliers by utilizing our self-correction module, which improves the robustness without using global positioning system (GPS) information. Point cloud registration experiments show that only our approach can process in real-time (71 ms, on average) while achieving state-of-the-art accuracy on the KITTI Odometry dataset, achieving a mean relative rotation error of 0.125∘ and a mean relative translation error of 0.038 m. In addition, real-road vehicle-to-vehicle point cloud registration experiments verify that the proposed algorithm can effectively align two vehicles’ point cloud when the GPS is not synchronized. A demonstration video is available at https://youtu.be/BJTSDChQchw .},
  archive      = {J_APIN},
  author       = {Wang, Hanqi and Liang, Huawei and Li, Zhiyuan and Zhou, Pengfei and Chen, Liangji},
  doi          = {10.1007/s10489-022-04308-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19143-19160},
  shortjournal = {Appl. Intell.},
  title        = {A fast coarse-to-fine point cloud registration based on optical flow for autonomous vehicles},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VIGCN: An isotropic natural image stitching network based on
graph convolution. <em>APIN</em>, <em>53</em>(16), 19128–19142. (<a
href="https://doi.org/10.1007/s10489-023-04472-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural scene images contain a large number of repetitive structures and irregular contours, which increases the difficulty of stitching. Traditional image stitching methods are highly dependent on feature extraction and matching results, resulting in a large number of matching errors when stitching natural images. In recent years, some scholars have proposed several deep learning-based image stitching networks. However, the structure of the network has a great influence on the stitching results. Due to the limitation of the receptive field of the convolution kernel, the deep feature extraction process often requires the stacking of multiple convolutional layers, resulting in redundant network structures. To address this issue, we propose an isotropic network architecture (VIGCN), which segments the image into labels and transforms the graph structure through patch embedding, and then utilizes the GCN structure to capture global correlations instead of deep convolutional layers. We combine supervised and unsupervised loss functions to train the network on real natural image datasets, and the results show that the model exhibits good performance in both visible and infrared image domains.},
  archive      = {J_APIN},
  author       = {Li, Yuheng and Guo, Fan and Wu, Zhihu and Tang, Jin},
  doi          = {10.1007/s10489-023-04472-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19128-19142},
  shortjournal = {Appl. Intell.},
  title        = {VIGCN: An isotropic natural image stitching network based on graph convolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape-aware fine-grained classification of erythroid cells.
<em>APIN</em>, <em>53</em>(16), 19115–19127. (<a
href="https://doi.org/10.1007/s10489-023-04465-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained classification and counting of bone marrow erythroid cells are vital for evaluating the health status and formulating therapeutic schedules for leukemia or hematopathy. Due to the subtle visual differences between different types of erythroid cells, it is challenging to apply existing image-based deep learning models for fine-grained erythroid cell classification. Moreover, there is no large open-source datasets on erythroid cells to support the model training. In this paper, we introduce BMEC (Bone Morrow Erythroid Cells), the first large fine-grained image dataset of erythroid cells, to facilitate more deep learning research on erythroid cells. BMEC contains 5,666 images of individual erythroid cells, each of which is extracted from the bone marrow erythroid cell smears and professionally annotated to one of the four types of erythroid cells. To distinguish the erythroid cells, one key indicator is the cell shape which is closely related to the cell growth and maturation. Therefore, we design a novel shape-aware image classification network for fine-grained erythroid cell classification. The shape feature is extracted from the shape mask image and aggregated to the raw image feature with a shape attention module. With the shape-attended image feature, our network achieved superior classification performance (81.12% top-1 accuracy) on the BMEC dataset comparing to the baseline methods. Ablation studies also demonstrate the effectiveness of incorporating the shape information for the fine-grained cell classification. To further verify the generalizability of our method, we tested our network on two additional public white blood cells (WBC) datasets and the results show our shape-aware method can generally outperform recent state-of-the-art works on classifying the WBC. The code and BMEC dataset can be found on https://github.com/wangye8899/BMEC .},
  archive      = {J_APIN},
  author       = {Wang, Ye and Ma, Rui and Ma, Xiaoqing and Cui, Honghua and Xiao, Yubin and Wu, Xuan and Zhou, You},
  doi          = {10.1007/s10489-023-04465-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19115-19127},
  shortjournal = {Appl. Intell.},
  title        = {Shape-aware fine-grained classification of erythroid cells},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sentiment analysis driven method based on public and
personal preferences with correlated attributes to select online
doctors. <em>APIN</em>, <em>53</em>(16), 19093–19114. (<a
href="https://doi.org/10.1007/s10489-023-04485-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method to assist patients in finding the most appropriate doctor for online medical consultation. To do that, it constructs an online doctor selection decision-making method that considers the correlation attributes, in which the measure of attribute correlation is derived from the history real decision data. To combine public and personal preference with correlated attributes, it proposes a Choquet integral based comprehensive online doctor ranking method. In detail, a two stage classification model based on BERT (Bidirectional Encoder Representations from Transformers) is used to extract service features from unstructured text reviews. Then, 2-additive fuzzy measure is adopted to represent the patient public group aggregated attribute preference. Next, a novel optimization model is proposed to combine the public preference and personal preference. Finally, a case study of dxy.com is carried out to illustrate the procedure of the method. The comparison result between proposed method and other traditional MADM (multi-attribute decision-making) methods prove its rationality.},
  archive      = {J_APIN},
  author       = {Wu, Jian and Zhang, Guangyin and Xing, Yumei and Liu, Yujia and Zhang, Zhen and Dong, Yucheng and Herrera-Viedma, Enrique},
  doi          = {10.1007/s10489-023-04485-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19093-19114},
  shortjournal = {Appl. Intell.},
  title        = {A sentiment analysis driven method based on public and personal preferences with correlated attributes to select online doctors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning what to memorize: Using intrinsic motivation to
form useful memory in partially observable reinforcement learning.
<em>APIN</em>, <em>53</em>(16), 19074–19092. (<a
href="https://doi.org/10.1007/s10489-022-04328-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning faces an important challenge in partially observable environments with long-term dependencies. In order to learn in an ambiguous environment, an agent has to keep previous perceptions in a memory. Earlier memory-based approaches use a fixed method to determine what to keep in the memory, which limits them to certain problems. In this study, we follow the idea of giving the control of the memory to the agent by allowing it to take memory-changing actions. Thus, the agent becomes more adaptive to the dynamics of an environment. Further, we formalize an intrinsic motivation to support this learning mechanism, which guides the agent to memorize distinctive events and enable it to disambiguate its state in the environment. Our overall approach is tested and analyzed on several partial observable tasks with long-term dependencies. The experiments show a clear improvement in terms of learning performance compared to other memory based methods.},
  archive      = {J_APIN},
  author       = {Demir, Alper},
  doi          = {10.1007/s10489-022-04328-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19074-19092},
  shortjournal = {Appl. Intell.},
  title        = {Learning what to memorize: Using intrinsic motivation to form useful memory in partially observable reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). VGPCNet: Viewport group point clouds network for 3D shape
recognition. <em>APIN</em>, <em>53</em>(16), 19060–19073. (<a
href="https://doi.org/10.1007/s10489-023-04498-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud recognition is fundamental and popular in vision perceptual systems such as autonomous driving, robotics, and virtual reality. Due to the sparse distribution and irregularity of point clouds, previous 3D point networks perform convolution on nearby points, ignoring the long-range dependence on the global structure. To solve this problem, we propose a Viewport Group Point Cloud Network for 3D Shape Recognition (VGPCNet) in which features are grouped according to viewports instead of local neighbor points to model the long-range global context. First, we propose to use viewport as proxy to capture both local and global features from an outside view of the object. The related points are grouped by visibility attribute effectively and efficiently which can not only capture the inside local geometry details but also obtain the global structure from the outside viewport. Second, we use a graph-based feature consolidation module to enhance the viewport features by modeling interactions between different viewports. Finally, to aggregate a global representation from multiple viewport features, we propose a novel attention-based feature aggregation module. We evaluate our VGPCNet on three widely used benchmarks including ModelNet40/10, ScanObjectNN, and ShapeCore55 for shape classification and retrieval tasks. Extensive experiments have demonstrated the effectiveness and superior performance (94.1% on ModelNet40) of our method over state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Ziyu and Yu, Yi and Da, Feipeng},
  doi          = {10.1007/s10489-023-04498-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19060-19073},
  shortjournal = {Appl. Intell.},
  title        = {VGPCNet: Viewport group point clouds network for 3D shape recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BRGR: Multi-agent cooperative reinforcement learning with
bidirectional real-time gain representation. <em>APIN</em>,
<em>53</em>(16), 19044–19059. (<a
href="https://doi.org/10.1007/s10489-022-04426-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-agent cooperative decision-making process, an agent needs to learn cooperatively with its neighbors to obtain the optimal strategy. The actions of agents can be classified into independent actions and interactive actions. The overall information and neighbor information obtained by the agent are different in guiding the selection of the two types of actions. Generally, real-time interaction between the representations of overall information and neighborhood information can facilitate the cooperative decision-making of agents. Therefore, this paper proposes a bidirectional real-time gain representation (BRGR) mechanism, which explicitly enables such real-time interactions. On the one hand, real-time effective neighborhood information representation is incorporated into the overall information representation via an attention module to achieve the gain of the overall information. The gains provide a better understanding and utilization of neighborhood information and guide the agents to make independent action selections. On the other hand, real-time overall information representation is integrated into the neighborhood information representation to achieve the gain of neighborhood information, which guarantees that the interactive actions are based on the current state of agent. The gains make the agents select proper interactive actions. Thus, the proposed BRGR mechanism enables the agents to effectively learn the optimal cooperative strategies. The BRGR is applied to state-of-the-art multi-agent reinforcement learning algorithms. The experimental results show that the BRGR significantly improves the performance of the base algorithms, and have more advantage in complex environments.},
  archive      = {J_APIN},
  author       = {He, Xin and Ge, Hongwei and Sun, Liang and Li, Qifeng and Hou, Yaqing},
  doi          = {10.1007/s10489-022-04426-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19044-19059},
  shortjournal = {Appl. Intell.},
  title        = {BRGR: Multi-agent cooperative reinforcement learning with bidirectional real-time gain representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-task learning with helpful word selection for
lexicon-enhanced chinese NER. <em>APIN</em>, <em>53</em>(16),
19028–19043. (<a
href="https://doi.org/10.1007/s10489-023-04464-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is a common task in the field of natural language processing, but it remains more challenging in Chinese due to the lack of natural delimiters. Recently, lots of works incorporate external lexicon into character-level Chinese NER, which focus on how to integrate the matched words in the lexicon into a specific model like LSTM or Transformer. However, in this case, the performance strongly depends on the quality of lexicon and the matching between lexicon and corpora. In reality, there are definitely some noises in the words provided by lexicon, being unhelpful for Chinese NER. To address this issue, in this paper, we propose a simple but effective multi-task learning method with helpful word selection for lexicon-enhanced Chinese NER. One task is to score the matched words and select top-K more helpful ones of them. The other task is to integrate the selected words by multi-head attention network and further implement Chinese NER by character-level sequence labeling. The two tasks are jointly learned with the same encoder. A series of experiments are conducted on three public datasets, demonstrating that the proposed method outperforms the recent advanced baselines.},
  archive      = {J_APIN},
  author       = {Tian, Xuetao and Bu, Xiaoxuan and He, Lu},
  doi          = {10.1007/s10489-023-04464-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19028-19043},
  shortjournal = {Appl. Intell.},
  title        = {Multi-task learning with helpful word selection for lexicon-enhanced chinese NER},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TAN-GFD: Generalizing face forgery detection based on
texture information and adaptive noise mining. <em>APIN</em>,
<em>53</em>(16), 19007–19027. (<a
href="https://doi.org/10.1007/s10489-023-04462-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery detection has become a research hotspot due to security concerns about spreading ultrarealisitc fake faces over social platforms. However, most existing deep learning-based approaches fail to generalize in cross-dataset scenarios since the learning-based methods tend to overfit manipulation-specific artifacts and advanced manipulations tamper with the target face locally or globally. In this work, we find that multiscale texture differences and regional noise inconsistencies are two intrinsic but complementary forged clues in the face manipulation pipeline. To comprehensively dig into generalized forgery clues, we propose a novel framework named TAN-GFD, based on texture information and adaptive noise mining. Specifically, we design a texture difference representation block that combines pixel intensity and gradient information of feature maps to extract multiscale texture difference features from different shallow feature maps. Moreover, since face tampering in real scenes swaps the whole face or partial facial expressions, we thus design the multilevel adaptive noise mining module, which consists of data preprocessing with learnable SRM filters and a cross-modality feature pyramid block, to capture the abundant features of regional noise inconsistencies. In addition, we introduce the cross-entropy loss with supervised contrastive loss collaboration strategy to guide the framework in learning more generalized representations. Extensive experiments on several benchmark datasets demonstrate the effectiveness and superior generalization performance of our framework.},
  archive      = {J_APIN},
  author       = {Zhao, Yi and Jin, Xin and Gao, Song and Wu, Liwen and Yao, Shaowen and Jiang, Qian},
  doi          = {10.1007/s10489-023-04462-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {19007-19027},
  shortjournal = {Appl. Intell.},
  title        = {TAN-GFD: Generalizing face forgery detection based on texture information and adaptive noise mining},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methodology for the projection of population pyramids based
on monte carlo simulation and genetic algorithms. <em>APIN</em>,
<em>53</em>(16), 18989–19006. (<a
href="https://doi.org/10.1007/s10489-023-04492-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of the evolution of population pyramids is crucial to study and tackle the growing issue associated to the depopulation of different regions around the world. This task has been based on the application of Monte Carlo simulation for the projection of the new population from the original one using birth, mortality, immigration and emigration data to model it. It has been applied both with current trends and with modified ones according to the user selections in order to obtain and analyse possible alternative scenarios. An inverse process is also presented, where a desired population pyramid is provided, and the system, based on genetic algorithms, determines the modifications of the previously stated rates to get the most similar output. Both tools are applied to a particular situation, the municipalities of a province in the north of Spain, so the functionality of the whole methodology is shown in an empiric way. These results show that projections are accurate with respect to the real data available to contrast, as well as that the inverse system generates close pyramids to the desired ones under reasonable circumstances. Additionally, these approaches have novel functionalities and make use of techniques not widely applied in the field, thus making them innovative with respect to other noteworthy examples in the field.},
  archive      = {J_APIN},
  author       = {Quirós, Pelayo and Lasheras, Fernando Sánchez},
  doi          = {10.1007/s10489-023-04492-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {18989-19006},
  shortjournal = {Appl. Intell.},
  title        = {Methodology for the projection of population pyramids based on monte carlo simulation and genetic algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intra-graph and inter-graph joint information propagation
network with third-order text graph tensor for fake news detection.
<em>APIN</em>, <em>53</em>(16), 18971–18988. (<a
href="https://doi.org/10.1007/s10489-023-04455-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the Internet and social media provide people with a range of opportunities and benefits in a variety of ways, the proliferation of fake news has negatively affected society and individuals. Many efforts have been invested to detect the fake news. However, to learn the representation of fake news by context information, it has brought many challenges for fake news detection due to the feature sparsity and ineffectively capturing the non-consecutive and long-range context. In this paper, we have proposed Intra-graph and Inter-graph Joint Information Propagation Network (abbreviated as IIJIPN) with Third-order Text Graph Tensor for fake news detection. Specifically, data augmentation is firstly utilized to solve the data imbalance and strengthen the small corpus. In the stage of feature extraction, Third-order Text Graph Tensor with sequential, syntactic, and semantic features is proposed to describe contextual information at different language properties. After constructing the text graphs for each text feature, Intra-graph and Inter-graph Joint Information Propagation is used for encoding the text: intra-graph information propagation is performed in each graph to realize homogeneous information interaction, and high-order homogeneous information interaction in each graph can be achieved by stacking propagation layer; inter-graph information propagation is performed among text graphs to realize heterogeneous information interaction by connecting the nodes across the graphs. Finally, news representations are generated by attention mechanism consisting of graph-level attention and node-level attention mechanism, and then news representations are fed into a fake news classifier. The experimental results on four public datasets indicate that our model has outperformed state-of-the-art methods. Our source code is available at https://github.com/cuibenkuan/IIJIPN .},
  archive      = {J_APIN},
  author       = {Cui, Benkuan and Ma, Kun and Li, Leping and Zhang, Weijuan and Ji, Ke and Chen, Zhenxiang and Abraham, Ajith},
  doi          = {10.1007/s10489-023-04455-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {18971-18988},
  shortjournal = {Appl. Intell.},
  title        = {Intra-graph and inter-graph joint information propagation network with third-order text graph tensor for fake news detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminatively embedded fuzzy k-means clustering with
feature selection strategy. <em>APIN</em>, <em>53</em>(16), 18959–18970.
(<a href="https://doi.org/10.1007/s10489-022-04376-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy K-Means clustering (FKM) is one of the most popular methods to partition data into clusters. Traditional FKM and its extensions perform fuzzy clustering based on original high-dimensional features. However, the presence of noisy and redundant features would cause the degradation of clustering performance. To avoid this problem, we integrate fuzzy clustering and feature selection into a unified model where the structured sparsity-inducing norm is imposed on the transformation matrix to determine the valuable feature subse adaptively. The clustering task and feature selection process are promoted mutually. To solve this model, an iterative algorithm is developed. Extensive experiments conducted on benchmark data sets demonstrate the effectiveness of our proposed method.},
  archive      = {J_APIN},
  author       = {Zhao, Peng and Zhang, Yongxin and Ma, Youzhong and Zhao, Xiaowei and Fan, Xunli},
  doi          = {10.1007/s10489-022-04376-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {16},
  pages        = {18959-18970},
  shortjournal = {Appl. Intell.},
  title        = {Discriminatively embedded fuzzy K-means clustering with feature selection strategy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-learning discrete salp swarm algorithm based on deep
reinforcement learning for dynamic job shop scheduling problem.
<em>APIN</em>, <em>53</em>(15), 18925–18958. (<a
href="https://doi.org/10.1007/s10489-023-04479-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the dynamic job shop scheduling problem (DJSP), an improved variant of salp swarm algorithm (SSA) named self-learning discrete salp swarm algorithm (SLDSSA) is proposed to minimize makespan. The primary intentions are to enhance SSA’s exploration, exploitation, diversity, and dynamic balance of exploration and exploitation. SLDSSA benefits from three new improvement strategies: hybrid initialization, discrete position update strategy, and self-learning population partitioning mechanism. The hybrid initialization significantly improves the overall quality of the initial population. The proposed discrete update strategy enhances the exploration and exploitation capability of the algorithm. The self-learning population partitioning mechanism achieves a dynamic balance of exploration/exploitation rate according to the population state. The SlDSSA algorithm is tested on 25 test functions, 27 job shop benchmark instances , and composite DJSP instances to evaluate the performance of SLDSSA. Furthermore, the results of SLDSSA are compared with 13 existing algorithms. The results show that the SLDSSA algorithm can provide competitive results to the comparative algorithms, effectively solve job shop scheduling problems and deal with the interference caused by dynamic events.},
  archive      = {J_APIN},
  author       = {Gu, Yiming and Chen, Ming and Wang, Liang},
  doi          = {10.1007/s10489-023-04479-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18925-18958},
  shortjournal = {Appl. Intell.},
  title        = {A self-learning discrete salp swarm algorithm based on deep reinforcement learning for dynamic job shop scheduling problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured validation of AI-based systems by virtual testing
in simulated test scenarios. <em>APIN</em>, <em>53</em>(15),
18910–18924. (<a
href="https://doi.org/10.1007/s10489-023-04475-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing relevance of artificial intelligence (AI) for technical systems offers significant potential for the realization and operation of autonomous systems in complex and potentially unknown environments. However, unlike classical solution approaches, the functionality of an AI system cannot be verified analytically, which is why data-driven approaches such as scenario-based testing are used. With the increasing complexity of the required functionality of the AI-based system, the quantity, and quality of the data needed for development and validation also increase. To meet this demand, data generated synthetically using simulation is increasingly being used. Compared to the acquisition of real-world reference data, simulation offers the major advantage that it can be configured to test specific scenarios of interest. This paper presents an architecture for the systematic generation of virtual test scenarios to establish synthetically generated test data as an integral part of the development and validation process for AI systems. Key aspects of this architecture are the consistent use of digital twins as virtual 1-to-1 replicas and a simulation infrastructure that enables the generation of training and validation data for AI-based systems in appropriate quantity, quality, and time. In particular, this paper focuses on the application of the architecture in the context of two use cases from different application domains.},
  archive      = {J_APIN},
  author       = {Dahmen, Ulrich and Osterloh, Tobias and Roßmann, Jürgen},
  doi          = {10.1007/s10489-023-04475-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18910-18924},
  shortjournal = {Appl. Intell.},
  title        = {Structured validation of AI-based systems by virtual testing in simulated test scenarios},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic segmentation based on double pyramid network with
improved global attention mechanism. <em>APIN</em>, <em>53</em>(15),
18898–18909. (<a
href="https://doi.org/10.1007/s10489-023-04463-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene semantic segmentation is an important and challenging task, which requires labeling the category of each pixel in the image accurately. The encoder-decoder framework represented by fully convolutional network(FCN) has unique advantages in semantic segmentation. However, it is still hard to segment the small target and object boundary in the FCN framework. So, this paper proposes a global attention double pyramid network(GADPNet) based on an improved global attention mechanism to improve the performance of semantic segmentation. It is composed of deep convolutional neural networks Resnet-101, atrous spatial pyramid pooling(ASPP) module, proposed pyramid decoder structure and improved global attention module. Resnet-101 is the backbone which is used to extract different stages’ features. ASPP module is used to capture multi-scale features from a high-level feature branch. Pyramid decoder structure can take advantage of multi-scale features from ASPP module and different stages’ low-level multi-scale feature maps guided by improved global attention module. The proposed decoder enhances the ability to capture multi-scale features. GADPNet is an end-to-end network. The experimental results of the value of mIoU on Pascal VOC 2012 test dataset and cityscapes val dataset are 80.5% and 72.9%, which indicate that the proposed GADPNet obtains higher semantic segmentation accuracy compared with the current methods.},
  archive      = {J_APIN},
  author       = {Ou, Xianfeng and Wang, Hanpu and Zhang, Guoyun and Li, Wujing and Yu, Shuixiang},
  doi          = {10.1007/s10489-023-04463-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18898-18909},
  shortjournal = {Appl. Intell.},
  title        = {Semantic segmentation based on double pyramid network with improved global attention mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised variational autoencoder towards
recommendation by nested contrastive learning. <em>APIN</em>,
<em>53</em>(15), 18887–18897. (<a
href="https://doi.org/10.1007/s10489-023-04488-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation methods predicting potential items for user has evolved from linear factor models to non-linear factor deep learning models. Deep generative model, especially variational autoencoder(VAE), has been used in a wide range of recommendation systems such as MultVAE and RecVAE. Despite effectiveness, we argue that they suffer from two limitations: (1)sparse and noisy user-item interactions will affect the performance of VAE-based recommendation models; and (2)incorporating simple priors,e.g.,isotropic Gaussian, in VAE couldn’t extract personalized user preference,as user’s preference may be highly complex. In this paper, we propose a Nested Self-supervised Variational Autoencoder (NSVAE) model for recommendation to enhance generalization and accuracy of VAE-based recommendation models. Besides using VAE for predicting user interests, NSVAE supplements supervised task of recommendation with nested self-supervised task, which consider both partial and entire preference. Nested self-supervised task is composed of inside and outside pretext tasks. Inside pretext task aligns the representations learned from different views, where views contain user partial preference, and outside pretext task discriminates entire preference from other user. Recommendation task and pretext tasks can be seamlessly integrated and enhance each other. Extensive experiment results on three real-world benchmarks validate the superiority of our NSVAE model to state-of-the-art VAE-based recommendation models.},
  archive      = {J_APIN},
  author       = {Wang, Jing and Wu, Jun and Jia, Caiyan and Zhang, Zhifei},
  doi          = {10.1007/s10489-023-04488-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18887-18897},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised variational autoencoder towards recommendation by nested contrastive learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight deep neural network from scratch. <em>APIN</em>,
<em>53</em>(15), 18868–18886. (<a
href="https://doi.org/10.1007/s10489-022-04394-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, deep neural networks (DNNs) are seriously overparameterized with enormous hardware resources demanded, which creates a heavy burden for inference applications especially for resource-constrained edge devices. To overcome this difficulty, there are two principal solutions: optimizing the overparameterized DNNs and designing high-efficiency DNN algorithms with lightweight architectures. In terms of the optimization methods, pruning is the most effective technique because the solution fundamentally optimizes the bloated DNNs by removing redundant structures from the network and can be seamlessly incorporated into all other optimization solutions as well as all kinds of DNN architectures. Nevertheless, the study reported in this paper reveals that the various excellent but also complicated pruning algorithms may not be as effective as proposals demonstrate and do not yield optimal solutions for all cases. In addition, the current lightweight DNN architectures are also overparameterized to a large extent. In this research, we propose a mechanism for determining lightweight DNN networks From Scratch (FS-DNN). First, we conduct a thorough study on the theoretical basis of evaluating the hardware resources demanded by DNNs, and establish the objective function for determining a lightweight DNN network. Based on the study, the theoretical FS-DNN for determining lightweight DNNs from scratch with high efficiency is proposed. Then, we perform a series of experiments with FS-DNN based lightweight DNNs on the public dataset CIFAR10/100 and a private dataset Kuzushiji, which prove the feasibility and efficiency of FS-DNN. According to the research, instead of adopting bloated DNN networks that demand complicated pruning algorithms to optimize the networks after the fact or the current so-called lightweight DNNs, the experimental results demonstrate that lightweight networks based on FS-DNN achieve superior performance in computing consumption with competitive or even better accuracy.},
  archive      = {J_APIN},
  author       = {Li, Hengyi and Yue, Xuebin and Zhao, Chengyan and Meng, Lin},
  doi          = {10.1007/s10489-022-04394-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18868-18886},
  shortjournal = {Appl. Intell.},
  title        = {Lightweight deep neural network from scratch},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Porn streamer audio recognition based on deep learning and
random forest. <em>APIN</em>, <em>53</em>(15), 18857–18867. (<a
href="https://doi.org/10.1007/s10489-023-04491-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing porn streamers audio recognition algorithms show poor performance in increasingly complex network environment. To resolve this problem, a porn streamer audio recognition algorithm based on deep learning and random forest is proposed. In this algorithm, a more stable complementary feature is first proposed, which consists of Log Mel Spectrum (LMS), Mel Frequency Cepstrum Coefficient (MFCC) and Gammatone Frequency Cepstrum Coefficient (GFCC), and the Dual-Path Fused Transformer Net (DPFTNet) network structure is then proposed for sound classification, which parallelizes the two main modules of the Swin Transformer, so that more feature details can be retained. Finally, the random forest is utilized to identify porn streamer. The experimental results show that this algorithm has higher recognition accuracy than the comparison algorithm.},
  archive      = {J_APIN},
  author       = {Liu, Shangfeng and Li, Ruwei and Li, Qiuyan and Zhao, Jingyu},
  doi          = {10.1007/s10489-023-04491-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18857-18867},
  shortjournal = {Appl. Intell.},
  title        = {Porn streamer audio recognition based on deep learning and random forest},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dictionary learning for unsupervised feature selection via
dual sparse regression. <em>APIN</em>, <em>53</em>(15), 18840–18856. (<a
href="https://doi.org/10.1007/s10489-023-04480-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With unlabeled and high-dimensional data explosion, unsupervised feature selection has become an essential step in many machine learning and data mining tasks. Many dictionary learning based models have been successfully developed for unsupervised feature selection in recent years. These models learn an over-complete dictionary to investigate more data distribution information. However, over-complete dictionary learning will generate redundancy in the latent representations for data. Moreover, if data contain noise, dictionary learning will also yield noise in the latent representations. In this paper, we propose a novel unsupervised feature selection framework, named dictionary learning for unsupervised feature selection via dual sparse regression. In this model, dictionary learning is first embedded into a sparse regression to learn an over-complete dictionary with sparse representations for data, in which the redundancy and noise are eliminated. The data are then projected to the representations to evaluate the significance of features using the other sparse regression. We also offer an efficient algorithm to solve this problem and theoretically analyze its convergence and computational complexity, which is proportional to the data dimensionality. Finally, the evaluation results with the k-means task utilizing the selected features on 9 benchmark datasets demonstrate the superiority of our approach in terms of effectiveness and efficiency.},
  archive      = {J_APIN},
  author       = {Wu, Jian-Sheng and Liu, Jing-Xin and Wu, Jun-Yun and Huang, Wei},
  doi          = {10.1007/s10489-023-04480-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18840-18856},
  shortjournal = {Appl. Intell.},
  title        = {Dictionary learning for unsupervised feature selection via dual sparse regression},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive model training strategy for continuous
classification of time series. <em>APIN</em>, <em>53</em>(15),
18821–18839. (<a
href="https://doi.org/10.1007/s10489-022-04433-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of time series is essential in many real-world applications like healthcare. The class of a time series is usually labeled at the final time, but more and more time-sensitive applications require classifying time series continuously. For example, the outcome of a critical patient is only determined at the end, but he should be diagnosed at all times for timely treatment. For this demand, we propose a new concept, Continuous Classification of Time Series (CCTS). Different from the existing single-shot classification, the key of CCTS is to model multiple distributions simultaneously due to the dynamic evolution of time series. But the deep learning model will encounter intertwined problems of catastrophic forgetting and over-fitting when learning multi-distribution. In this work, we found that the well-designed distribution division and replay strategies in the model training process can help to solve the problems. We propose a novel Adaptive model training strategy for CCTS (ACCTS). Its adaptability represents two aspects: (1) Adaptive multi-distribution extraction policy. Instead of the fixed rules and the prior knowledge, ACCTS extracts data distributions adaptive to the time series evolution and the model change; (2) Adaptive importance-based replay policy. Instead of reviewing all old distributions, ACCTS only replays important samples adaptive to their contribution to the model. Experiments on four real-world datasets show that our method outperforms all baselines.},
  archive      = {J_APIN},
  author       = {Sun, Chenxi and Li, Hongyan and Song, Moxian and Cai, Derun and Zhang, Baofeng and Hong, Shenda},
  doi          = {10.1007/s10489-022-04433-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18821-18839},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive model training strategy for continuous classification of time series},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). De-MISTED: Image-based classification of erroneous multiple
sequence alignments using convolutional neural networks. <em>APIN</em>,
<em>53</em>(15), 18806–18820. (<a
href="https://doi.org/10.1007/s10489-022-04390-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of high throughput genome sequencing technologies has resulted in a significant increase in the number of available sequences, creating new challenges for genome annotation and prediction of protein-coding genes in terms of error detection and quality control. Multiple Sequence Alignments (MSAs) of the predicted protein sequences provide important contextual information that can be used to distinguish errors (caused by artifacts in the raw genome data, badly predicted gene sequences, or the alignment methods themselves) from true biological events. This can be achieved either by human expertise or by statistical analysis of the sequence data. Here, we propose a new approach that uses visual representations of MSAs as inputs for Convolutional Neural Networks (CNN) to classify MSAs into erroneous and non-erroneous categories. The MSAs are extracted from a unique in-house dataset, in which errors are carefully identified. Our model, called De-MISTED (Deep learning for MultIple Sequence alignmenTs Error Detection) identifies MSAs containing erroneous sequences with high accuracy (87%) and sensitivity (92%). Visual explanation techniques show that our model correctly identifies the position of multiple errors of different types (insertions, deletions and mismatches). Close examination of the data showed that our model can also identify errors that were not previously annotated in the data. The De-MISTED method thus contributes to a more robust exploitation of the genome data.},
  archive      = {J_APIN},
  author       = {Khodji, Hiba and Collet, Pierre and Thompson, Julie D. and Jeannin-Girardon, Anne},
  doi          = {10.1007/s10489-022-04390-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18806-18820},
  shortjournal = {Appl. Intell.},
  title        = {De-MISTED: Image-based classification of erroneous multiple sequence alignments using convolutional neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample separation and domain alignment complementary
learning mechanism for open set domain adaptation. <em>APIN</em>,
<em>53</em>(15), 18790–18805. (<a
href="https://doi.org/10.1007/s10489-022-04262-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open Set Domain Adaptation (OSDA) reduces domain shift and semantic shift by dividing the known/unknown target samples and aligning the known target samples with the source samples. Unfortunately, either separating or aligning first will cause the negative shift to the other side. Moreover, numerous methods do not utilize the sample knowledge of the target domain. In this study, a new method is put forward to address the issue called Sample Separation and Domain Alignment Complementary Learning Mechanism (CLM) for Open Set Domain Adaptation. Specifically, this work proposes a complementary learning mechanism that jointly trains two complementary learning structures including Sample Separation Module (SSMod) and Domain Alignment Module (DAMod). SSMod and DAMod are performed simultaneously, exchanging training experiences during the learning process using the self-supervised pseudo-labeling method. In addition, we introduce a novel sample separation method, which not only facilitates the distinction between known and unknown classes of target samples but also enriches the semantic knowledge of the model by employing the unlabeled data in an unsupervised manner. Extensive experiments demonstrate that our method realizes significant performance on four standard Digits, Office-31, Office-Home and VisDA-2017 benchmarks. For example, CML achieves 89.2% accuracy of HOS on Office-31 and increases by 1.8% than the second best method.},
  archive      = {J_APIN},
  author       = {Sifan, Long and Shengsheng, Wang and Xin, Zhao and Zihao, Fu and Bilin, Wang},
  doi          = {10.1007/s10489-022-04262-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18790-18805},
  shortjournal = {Appl. Intell.},
  title        = {Sample separation and domain alignment complementary learning mechanism for open set domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). T-norms driven loss functions for machine learning.
<em>APIN</em>, <em>53</em>(15), 18775–18789. (<a
href="https://doi.org/10.1007/s10489-022-04383-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Injecting prior knowledge into the learning process of a neural architecture is one of the main challenges currently faced by the artificial intelligence community, which also motivated the emergence of neural-symbolic models. One of the main advantages of these approaches is their capacity to learn competitive solutions with a significant reduction of the amount of supervised data. In this regard, a commonly adopted solution consists of representing the prior knowledge via first-order logic formulas, then relaxing the formulas into a set of differentiable constraints by using a t-norm fuzzy logic. This paper shows that this relaxation, together with the choice of the penalty terms enforcing the constraint satisfaction, can be unambiguously determined by the selection of a t-norm generator, providing numerical simplification properties and a tighter integration between the logic knowledge and the learning objective. When restricted to supervised learning, the presented theoretical framework provides a straight derivation of the popular cross-entropy loss, which has been shown to provide faster convergence and to reduce the vanishing gradient problem in very deep structures. However, the proposed learning formulation extends the advantages of the cross-entropy loss to the general knowledge that can be represented by neural-symbolic methods. In addition, the presented methodology allows the development of novel classes of loss functions, which are shown in the experimental results to lead to faster convergence rates than the approaches previously proposed in the literature.},
  archive      = {J_APIN},
  author       = {Giannini, Francesco and Diligenti, Michelangelo and Maggini, Marco and Gori, Marco and Marra, Giuseppe},
  doi          = {10.1007/s10489-022-04383-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18775-18789},
  shortjournal = {Appl. Intell.},
  title        = {T-norms driven loss functions for machine learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Enhancing the generalization ability of deep learning model
for radio signal modulation recognition. <em>APIN</em>, <em>53</em>(15),
18758–18774. (<a
href="https://doi.org/10.1007/s10489-022-04374-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic modulation recognition is a major project in the field of radio cognition; however, the generalization ability of conventional models cannot satisfy practical applications. In order to improve the generalization performance of the deep learning model and increase its recognition efficiency, we propose a novel model: ElsNet (elastic convolutional neural network). This network designs a channel optimization module, by inputting the average pooling information of the feature map and the intrinsic parameters of the batch normalization layer, to dynamically optimize the connection relations between network neurons and enhance the generalization ability of the model. ElsNet achieves an accuracy of about 94% at signal-to-noise ratios of 0-20 dB. Subsequent experiments have also demonstrated that, the ElsNet has a satisfying performance in transferred data sets and a peak accuracy of 82% through transfer learning, which to a certain extent alleviates the problem that the current signal modulation recognition can only be applied to signals with the same modulation parameters as the training dataset and has poor performance in recognizing real signals with different modulation parameters.},
  archive      = {J_APIN},
  author       = {Wang, Faquan and Zhou, Yucheng and Yan, Hanzhi and Luo, Ruisen},
  doi          = {10.1007/s10489-022-04374-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18758-18774},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing the generalization ability of deep learning model for radio signal modulation recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification framework for faulty-software using enhanced
exploratory whale optimizer-based feature selection scheme and random
forest ensemble learning. <em>APIN</em>, <em>53</em>(15), 18715–18757.
(<a href="https://doi.org/10.1007/s10489-022-04427-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Fault Prediction (SFP) is an important process to detect the faulty components of the software to detect faulty classes or faulty modules early in the software development life cycle. In this paper, a machine learning framework is proposed for SFP. Initially, pre-processing and re-sampling techniques are applied to make the SFP datasets ready to be used by ML techniques. Thereafter seven classifiers are compared, namely K-Nearest Neighbors (KNN), Naive Bayes (NB), Linear Discriminant Analysis (LDA), Linear Regression (LR), Decision Tree (DT), Support Vector Machine (SVM), and Random Forest (RF). The RF classifier outperforms all other classifiers in terms of eliminating irrelevant/redundant features. The performance of RF is improved further using a dimensionality reduction method called binary whale optimization algorithm (BWOA) to eliminate the irrelevant/redundant features. Finally, the performance of BWOA is enhanced by hybridizing the exploration strategies of the grey wolf optimizer (GWO) and harris hawks optimization (HHO) algorithms. The proposed method is called SBEWOA. The SFP datasets utilized are selected from the PROMISE repository using sixteen datasets for software projects with different sizes and complexity. The comparative evaluation against nine well-established feature selection methods proves that the proposed SBEWOA is able to significantly produce competitively superior results for several instances of the evaluated dataset. The algorithms’ performance is compared in terms of accuracy, the number of features, and fitness function. This is also proved by the 2-tailed P-values of the Wilcoxon signed ranks statistical test used. In conclusion, the proposed method is an efficient alternative ML method for SFP that can be used for similar problems in the software engineering domain.},
  archive      = {J_APIN},
  author       = {Mafarja, Majdi and Thaher, Thaer and Al-Betar, Mohammed Azmi and Too, Jingwei and Awadallah, Mohammed A. and Abu Doush, Iyad and Turabieh, Hamza},
  doi          = {10.1007/s10489-022-04427-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18715-18757},
  shortjournal = {Appl. Intell.},
  title        = {Classification framework for faulty-software using enhanced exploratory whale optimizer-based feature selection scheme and random forest ensemble learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved dummy generation approach for infeasible
regions. <em>APIN</em>, <em>53</em>(15), 18700–18714. (<a
href="https://doi.org/10.1007/s10489-022-04379-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based services (LBS), which provide personalized and timely information, entail privacy concerns such as unwanted leaks of current user locations to potential stalkers. In this regard, existing works have proposed dummy generation techniques by creating a cloaking region (CR) such that the user’s location is at a fixed distance from the centre of CR. Hence, if the adversary somehow knows the location of the centre of CR, the user’s location would be vulnerable to attacks. Moreover, in case of the existing approaches, infeasible regions are assumed to have no relationship with time. However, this assumption is typically not valid in real-world scenarios. For example, a supermarket can be considered to be an infeasible region from 9 pm to 9 am since it would be closed at that time. Thus, if a dummy is placed at this location at that particular time, the attacker would know that it is a dummy, thereby reducing the user’s location privacy. In this regard, our key contributions are three-fold. First, we propose an improved dummy generation approach, which we designate as Annulus-based Gaussian Dummy Generation (AGDG), for facilitating improved location privacy for mobile users. Second, we introduce the notion of time-dependent infeasible regions to further improve the dummy generation approach by considering infeasible regions that change with time. Third, we conducted experiments to demonstrate that the AGDG effectively provides improved location privacy, including for regions with time-dependent infeasible regions w.r.t. existing approaches.},
  archive      = {J_APIN},
  author       = {Siddiqie, Shadaab and Mondal, Anirban and Reddy, P Krishna},
  doi          = {10.1007/s10489-022-04379-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18700-18714},
  shortjournal = {Appl. Intell.},
  title        = {An improved dummy generation approach for infeasible regions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel automated robust dual-channel EEG-based sleep
scoring system using optimal half-band pair linear-phase biorthogonal
wavelet filter bank. <em>APIN</em>, <em>53</em>(15), 18681–18699. (<a
href="https://doi.org/10.1007/s10489-022-04432-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the hectic work life of people has led to sleep deprivation. This may further result in sleep-related disorders and adverse physiological conditions. Therefore, sleep study has become an active research area. Sleep scoring is crucial for detecting sleep-related disorders like sleep apnea, insomnia, narcolepsy, periodic leg movement (PLM), and restless leg syndrome (RLS). Sleep is conventionally monitored in a sleep laboratory using polysomnography (PSG) which is the recording of various physiological signals. The traditional sleep stage scoring (SSG) done by professional sleep scorers is a tedious, strenuous, and time-consuming process as it is manual. Hence, developing a machine-learning model for automatic SSG is essential. In this study, we propose an automated SSG approach based on the biorthogonal wavelet filter bank’s (BWFB) novel least squares (LS) design. We have utilized a huge Wisconsin sleep cohort (WSC) database in this study. The proposed study is a pioneering work on automatic sleep stage classification using the WSC database, which includes good sleepers and patients suffering from various sleep-related disorders, including apnea, insomnia, hypertension, diabetes, and asthma. To investigate the generalization of the proposed system, we evaluated the proposed model with the following publicly available databases: cyclic alternating pattern (CAP), sleep EDF, ISRUC, MIT-BIH, and the sleep apnea database from St. Vincent’s University. This study uses only two unipolar EEG channels, namely O1-M2 and C3-M2, for the scoring. The Hjorth parameters (HP) are extracted from the wavelet subbands (SBS) that are obtained from the optimal BWFB. To classify sleep stages, the HP features are fed to several supervised machine learning classifiers. 12 different datasets have been created to develop a robust model. A total of 12 classification tasks (CT) have been conducted employing various classification algorithms. Our developed model achieved the best accuracy of 83.2% and Cohen’s Kappa of 0.7345 to reliably distinguish five sleep stages, using an ensemble bagged tree classifier with 10-fold cross-validation using WSC data. We also observed that our system is either better or competitive with existing state-of-art systems when we tested with the above-mentioned five databases other than WSC. This method yielded promising results using only two EEG channels using a huge WSC database. Our approach is simple and hence, the developed model can be installed in home-based clinical systems and wearable devices for sleep scoring.},
  archive      = {J_APIN},
  author       = {Sharma, Manish and Makwana, Paresh and Chad, Rajesh Singh and Acharya, U Rajendra},
  doi          = {10.1007/s10489-022-04432-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18681-18699},
  shortjournal = {Appl. Intell.},
  title        = {A novel automated robust dual-channel EEG-based sleep scoring system using optimal half-band pair linear-phase biorthogonal wavelet filter bank},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-language font style transfer. <em>APIN</em>,
<em>53</em>(15), 18666–18680. (<a
href="https://doi.org/10.1007/s10489-022-04375-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a cross-language font style transfer system that can synthesize a new font by observing only a few samples from another language. Automatic font synthesis is a challenging task and has attracted much research interest. Most previous works addressed this problem by transferring the style of the given subset to the content of unseen ones. Nevertheless, they only focused on the font style transfer in the same language. In many cases, we need to learn font style from one language and then apply it to other languages. Existing methods make this difficult to accomplish because of the abstraction of style and language differences. To address this problem, we specifically designed the network into a multi-level attention form to capture both local and global features of the font style. To validate the generative ability of our model, we constructed an experimental font dataset of 847 fonts, each containing English and Chinese characters with the same style. Results show that our model generates 80.3% of users’ preferred images compared with state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Li, Chenhao and Taniguchi, Yuta and Lu, Min and Konomi, Shin’ichi and Nagahara, Hajime},
  doi          = {10.1007/s10489-022-04375-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18666-18680},
  shortjournal = {Appl. Intell.},
  title        = {Cross-language font style transfer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain-inspired STA for parameter estimation of
fractional-order memristor-based chaotic systems. <em>APIN</em>,
<em>53</em>(15), 18653–18665. (<a
href="https://doi.org/10.1007/s10489-022-04435-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very important to estimate the unknown parameters of the fractional-order memristor-based chaotic systems (FOMCSs). In this study, a brain-inspired state transition algorithm (BISTA) is proposed to estimate the parameters of the FOMCSs. In order to generate a better initial population, a novel initialization approach based on opposition-based learning is presented. To balance the global search and local search, and accelerate the convergence speed, the mutual learning and selective learning are proposed in the optimization process. The performance of the proposed algorithm is comprehensively evaluated on two typical FOMCSs. The simulation results and statistical analysis have demonstrated the effectiveness of the proposed algorithm. For the fractional-order memristor-based Lorenz system, the proposed method can increase the estimated value of parameters by at least one order of magnitude compared with the other methods.},
  archive      = {J_APIN},
  author       = {Huang, Zhaoke and Yang, Chunhua and Zhou, Xiaojun and Gui, Weihua and Huang, Tingwen},
  doi          = {10.1007/s10489-022-04435-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18653-18665},
  shortjournal = {Appl. Intell.},
  title        = {Brain-inspired STA for parameter estimation of fractional-order memristor-based chaotic systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection of pre-trained shallow CNN using the
QLESCA optimizer: COVID-19 detection as a case study. <em>APIN</em>,
<em>53</em>(15), 18630–18652. (<a
href="https://doi.org/10.1007/s10489-022-04446-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the World Health Organization, millions of infections and a lot of deaths have been recorded worldwide since the emergence of the coronavirus disease (COVID-19). Since 2020, a lot of computer science researchers have used convolutional neural networks (CNNs) to develop interesting frameworks to detect this disease. However, poor feature extraction from the chest X-ray images and the high computational cost of the available models introduce difficulties for an accurate and fast COVID-19 detection framework. Moreover, poor feature extraction has caused the issue of ‘the curse of dimensionality’, which will negatively affect the performance of the model. Feature selection is typically considered as a preprocessing mechanism to find an optimal subset of features from a given set of all features in the data mining process. Thus, the major purpose of this study is to offer an accurate and efficient approach for extracting COVID-19 features from chest X-rays that is also less computationally expensive than earlier approaches. To achieve the specified goal, we design a mechanism for feature extraction based on shallow conventional neural network (SCNN) and used an effective method for selecting features by utilizing the newly developed optimization algorithm, Q-Learning Embedded Sine Cosine Algorithm (QLESCA). Support vector machines (SVMs) are used as a classifier. Five publicly available chest X-ray image datasets, consisting of 4848 COVID-19 images and 8669 non-COVID-19 images, are used to train and evaluate the proposed model. The performance of the QLESCA is evaluated against nine recent optimization algorithms. The proposed method is able to achieve the highest accuracy of 97.8086% while reducing the number of features from 100 to 38. Experiments prove that the accuracy of the model improves with the usage of the QLESCA as the dimensionality reduction technique by selecting relevant features.},
  archive      = {J_APIN},
  author       = {Hamad, Qusay Shihab and Samma, Hussein and Suandi, Shahrel Azmin},
  doi          = {10.1007/s10489-022-04446-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18630-18652},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection of pre-trained shallow CNN using the QLESCA optimizer: COVID-19 detection as a case study},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing storage assignment, order picking, and their
interaction in mezzanine warehouses. <em>APIN</em>, <em>53</em>(15),
18605–18629. (<a
href="https://doi.org/10.1007/s10489-022-04443-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In warehouses, order picking is known to be the most labor-intensive and costly task in which the employees account for a large part of the warehouse performance. Hence, many approaches exist, that optimize the order picking process based on diverse economic criteria. However, most of these approaches focus on a single economic objective at once and disregard ergonomic criteria in their optimization. Further, the influence of the placement of the items to be picked is underestimated and accordingly, too little attention is paid to the interdependence of these two problems. In this work, we aim at optimizing the storage assignment and the order picking problem within mezzanine warehouse with regards to their reciprocal influence. We propose a customized version of the Non-dominated Sorting Genetic Algorithm II (NSGA-II) for optimizing the storage assignment problem as well as an Ant Colony Optimization (ACO) algorithm for optimizing the order picking problem. Both algorithms incorporate multiple economic and ergonomic constraints simultaneously. Furthermore, the algorithms incorporate knowledge about the interdependence between both problems, aiming to improve the overall warehouse performance. Our evaluation results show that our proposed algorithms return better storage assignments and order pick routes compared to commonly used techniques for the following quality indicators for comparing Pareto fronts: Coverage, Generational Distance, Euclidian Distance, Pareto Front Size, and Inverted Generational Distance. Additionally, the evaluation regarding the interaction of both algorithms shows a better performance when combining both proposed algorithms.},
  archive      = {J_APIN},
  author       = {Lesch, Veronika and Müller, Patrick B.M. and Krämer, Moritz and Hadry, Marius and Kounev, Samuel and Krupitzer, Christian},
  doi          = {10.1007/s10489-022-04443-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18605-18629},
  shortjournal = {Appl. Intell.},
  title        = {Optimizing storage assignment, order picking, and their interaction in mezzanine warehouses},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved sine cosine algorithm with heterogeneous
subpopulations for global optimization and fractional order PID
controller design. <em>APIN</em>, <em>53</em>(15), 18581–18604. (<a
href="https://doi.org/10.1007/s10489-023-04473-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of recently proposed optimization algorithm, sine cosine algorithm (SCA) suffers from the problems of skipping over true solutions, stagnating in local optima and premature convergence. Therefore, an improved sine cosine algorithm with heterogeneous subpopulations (HSISCA) is developed in this paper. The population of HSISCA is divided into two subpopulations which comply with different search mechanisms and information sharing rules. One subpopulation is specified to enhance exploration by incorporating Levy flight and random search guidance into SCA, while the other is specified to enhance exploitation by adaptive differential mutation operator. The exploration subpopulation is denied access to the information of exploitation subpopulation, but the exploitation subpopulation is allowed access to the information of exploration subpopulation. Moreover, the greedy selection is applied to each individual to preserve useful information. HSISCA is tested on the CEC2014 and CEC2017 benchmark functions, and is used for designing fractional order PID (FOPID) controller. The results confirm the better performance of HSISCA compared to other competitive algorithms, and demonstrate the effectiveness of HSISCA in designing FOPID controller for complex systems.},
  archive      = {J_APIN},
  author       = {Li, Qijun and Ning, Huifeng and Gong, Jun},
  doi          = {10.1007/s10489-023-04473-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18581-18604},
  shortjournal = {Appl. Intell.},
  title        = {An improved sine cosine algorithm with heterogeneous subpopulations for global optimization and fractional order PID controller design},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general robust low–rank multinomial logistic regression
for corrupted matrix data classification. <em>APIN</em>,
<em>53</em>(15), 18564–18580. (<a
href="https://doi.org/10.1007/s10489-022-04424-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-classification of corrupted matrix data is a significant problem in machine learning and pattern recognition. However, most of the existing methods can only handle the clean data or the corrupted data with the know statistical information of noises. Besides, they usually reshape the matrix data into a vector as the input, very likely to destroy the structure of the raw data, thereby reducing the model performance. In order to address the above issues, a general robust low–rank multinomial logistic regression is proposed for corrupted matrix data. The proposed approach has three outstanding merits as follows: (1) by using the multinomial logistic regression combined with three regularization terms corresponding to matrix structure, low–rank and sparsity, the clean data recovery and the classification are simultaneously fulfilled; (2) the proposed method can adapt to more general corrupted matrix data since it does not require strong statistical assumptions about noise, and (3) the theoretical analysis is provided to show the convergence of the proposed multi-block ADMM algorithm, and such a convergence can be rigidly guaranteed by introducing two auxiliary variables such that the coefficients of the equality constraints are orthogonal. Finally, extensive experimental results have demonstrated the effectiveness and robustness of the proposed method.},
  archive      = {J_APIN},
  author       = {Hu, Yuyu and Fan, Yali and Song, Yan and Li, Ming},
  doi          = {10.1007/s10489-022-04424-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18564-18580},
  shortjournal = {Appl. Intell.},
  title        = {A general robust low–rank multinomial logistic regression for corrupted matrix data classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). FedDKD: Federated learning with decentralized knowledge
distillation. <em>APIN</em>, <em>53</em>(15), 18547–18563. (<a
href="https://doi.org/10.1007/s10489-022-04431-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneity of the data distribution generally influences federated learning performance in neural networks. For a well-performing global model, taking a weighted average of the local models, as in most existing federated learning algorithms, may not guarantee consistency with local models in the space of neural network maps. In this paper, we highlight the significance of the space of neural network maps to relieve the performance decay produced by data heterogeneity and propose a novel federated learning framework equipped with the decentralized knowledge distillation process (FedDKD). In FedDKD, we introduce a decentralized knowledge distillation (DKD) module to distill the knowledge of local models to teach the global model approaching the neural network map average by optimizing the divergence defined in the loss function, other than only averaging parameters as in the literature. Numerical experiments on various heterogeneous datasets reveal that FedDKD outperforms the state-of-the-art methods, especially on some extremely heterogeneous datasets.},
  archive      = {J_APIN},
  author       = {Li, Xinjia and Chen, Boyu and Lu, Wenlian},
  doi          = {10.1007/s10489-022-04431-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18547-18563},
  shortjournal = {Appl. Intell.},
  title        = {FedDKD: Federated learning with decentralized knowledge distillation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting metro rail transit passenger flow with
multiple-attention deep neural networks and surrounding vehicle
detection devices. <em>APIN</em>, <em>53</em>(15), 18531–18546. (<a
href="https://doi.org/10.1007/s10489-023-04483-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapid development of public transportation led, the traffic flow prediction has become one of the most crucial issues, especially estimating the number of passengers using the Mass Rapid Transit (MRT) system. In general, predicting the passenger flow of traffic is a time-series problem that requires external information to improve accuracy. Because many MRT passengers take cars or buses to MRT stations, this study used external information from vehicle detection (VD) devices to improve the prediction of passenger flow. This study proposed a deep learning architecture, called a multiple-attention deep neural network (MADNN) model, based on historical MRT passenger flow and the flow from surrounding VD devices that estimates the weights of the vehicle detection devices. The model consists of (1) an MRT attention layer (MRT-AL) that generate hidden features for MRT stations, (2) a surrounding VD (SVD) attention layer (SVD-AL) that generate hidden features for SVD devices, and (3) an MRT-SVD attention layer (MRT-SVD-AL) that generate attention weights for each VD device in an MRT station. The results of the investigation indicated that the MADNN model outperformed the models without multiple-attention mechanisms in predicting the passenger flow of MRT traffic.},
  archive      = {J_APIN},
  author       = {Wu, Jheng-Long and Lu, Mingying and Wang, Chia-Yun},
  doi          = {10.1007/s10489-023-04483-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18531-18546},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting metro rail transit passenger flow with multiple-attention deep neural networks and surrounding vehicle detection devices},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label learning with relief-based label-specific
feature selection. <em>APIN</em>, <em>53</em>(15), 18517–18530. (<a
href="https://doi.org/10.1007/s10489-022-04350-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning is an emerging paradigm exploiting samples with rich semantics. As an effective solution to multi-label learning, the strategy of label-specific features (LIFT) has been widely applied. Technically, such strategy feeds the tailored features to learning model instead of the original ones. However, tailoring features for each label may cause redundancy or irrelevance in feature space, thereby deteriorating the learning performance. To alleviate such a problem, a novel multi-label classification method named Relief-LIFT is proposed in this study. Relief-LIFT firstly leverages LIFT to generate the toiled features, and then adjusts Relief to select informative features from those toiled ones for the classification model. Experimental results on 12 real-world multi-label data sets demonstrate that, our proposed Relief-LIFT can achieve better performance as compared with other well-established multi-label classification methods.},
  archive      = {J_APIN},
  author       = {Zhang, Jiadong and Liu, Keyu and Yang, Xibei and Ju, Hengrong and Xu, Suping},
  doi          = {10.1007/s10489-022-04350-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18517-18530},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label learning with relief-based label-specific feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multilevel object pose estimation algorithm based on point
cloud keypoints. <em>APIN</em>, <em>53</em>(15), 18508–18516. (<a
href="https://doi.org/10.1007/s10489-022-04411-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main task of object pose estimation is to predict the 3D rotation and 3D translation of an object in the current scene relative to a fixed object in the world coordinates. The most commonly used algorithm in pose estimation is based on the object characteristics or keypoint information for matching. The accuracy of these algorithms in pose estimation depends on whether the object surface characteristics are apparent. To solve the problem mentioned above, we propose a pose estimation algorithm using multilevel keypoint aggregation in the point cloud. First, we use a deep learning convolutional neural network to predict the keypoint positions in the point cloud. Then we estimate multiple poses at different levels according to the keypoints predicted above. Finally, we aggregate multiple poses into the final pose according to the weight of each pose. Our experiments show that our method outperforms other approaches in two datasets, YCB-Video and LineMOD.},
  archive      = {J_APIN},
  author       = {Yang, Haibo and Jia, Junying and Lu, Xin},
  doi          = {10.1007/s10489-022-04411-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18508-18516},
  shortjournal = {Appl. Intell.},
  title        = {A multilevel object pose estimation algorithm based on point cloud keypoints},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capped norm linear discriminant analysis and its
applications. <em>APIN</em>, <em>53</em>(15), 18488–18507. (<a
href="https://doi.org/10.1007/s10489-022-04395-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical linear discriminant analysis (LDA) is based on squared Frobenious norm and hence is sensitive to outliers and noise. To improve the robustness of LDA, this paper introduces a capped l2,1-norm of a matrix, which employs non-squared l2-norm and “capped” operation, and further proposes a novel capped l2,1-norm linear discriminant analysis, called CLDA. Due to the use of capped l2,1-norm, CLDA can effectively remove extreme outliers and suppress the effect of noise data. In fact, CLDA can also be viewed as a weighted LDA and is solved through a series of generalized eigenvalue problems. The experimental results on an artificial data set, some UCI data sets and two image data sets demonstrate the effectiveness of CLDA.},
  archive      = {J_APIN},
  author       = {Liu, Jiakou and Xiong, Xiong and Ren, Peiwei and Li, Chun-Na and Shao, Yuan-Hai},
  doi          = {10.1007/s10489-022-04395-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18488-18507},
  shortjournal = {Appl. Intell.},
  title        = {Capped norm linear discriminant analysis and its applications},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Mixed structure low-rank representation for multi-view
subspace clustering. <em>APIN</em>, <em>53</em>(15), 18470–18487. (<a
href="https://doi.org/10.1007/s10489-023-04474-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering method utilizes the diversity of multi-view information to access better clustering results than a single view. Most existing multi-view clustering methods do not take full advantage of the diversity of information views, which makes the affinity matrix insufficiently clear and accurate to precisely describe the potential structure of multi-view data, resulting in poor clustering results. To solve the above problems, mixed structure low-rank representation (MSLRR) for multi-view subspace clustering and its kernel version (ker-MSLRR) are proposed in this paper. The mixed low-rank structure algorithm takes the multi-view data after the feature concatenation as input and then uses the nested mixed structure of least squares regression (LSR) and low-rank representation (LRR) as the unified model to effectively reduce the noise of the affinity matrix. In addition, to effectively deal with nonlinear data, the kernel method ker-MSLRR based on MSLRR is proposed, which improves the processing ability of processing nonlinear data. The experimental results of five real datasets demonstrate that the proposed methods have better clustering performance than other existing methods.},
  archive      = {J_APIN},
  author       = {Wang, Shouhang and Wang, Yong and Lu, Guifu and Le, Wenge},
  doi          = {10.1007/s10489-023-04474-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18470-18487},
  shortjournal = {Appl. Intell.},
  title        = {Mixed structure low-rank representation for multi-view subspace clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mechanics model based on information entropy for
identifying influencers in complex networks. <em>APIN</em>,
<em>53</em>(15), 18450–18469. (<a
href="https://doi.org/10.1007/s10489-023-04457-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network, with some or all characteristics of scale-free, self-similarity, self-organization, attractor and small world, is defined as a complex network. The identification of significant spreaders is an indispensable research direction in complex networks, which aims to discover nodes that play a crucial role in the structure and function of the network. Since influencers are essential for studying the security of the network and controlling the propagation process of the network, their assessment methods are of great significance and practical value to solve many problems. However, how to effectively combine global information with local information is still an open problem. To solve this problem, the generalized mechanics model is further improved in this paper. A generalized mechanics model based on information entropy is proposed to discover crucial spreaders in complex networks. The influence of each neighbor node on local information is quantified by information entropy, and the interaction between each node on global information is considered by calculating the shortest distance. Extensive tests on eleven real networks indicate the proposed approach is much faster and more precise than traditional ways and state-of-the-art benchmarks. At the same time, it is effective to use our approach to identify influencers in complex networks.},
  archive      = {J_APIN},
  author       = {Li, Shuyu and Xiao, Fuyuan},
  doi          = {10.1007/s10489-023-04457-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18450-18469},
  shortjournal = {Appl. Intell.},
  title        = {A mechanics model based on information entropy for identifying influencers in complex networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building discriminative features of scene recognition using
multi-stages of inception-ResNet-v2. <em>APIN</em>, <em>53</em>(15),
18431–18449. (<a
href="https://doi.org/10.1007/s10489-023-04460-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is a challenging problem due to intra-class variations and inter-class similarities. Traditional methods and convolutional neural networks (CNN) represent the global spatial structure, which is suitable for general scene classification and object recognition, but show poor presentation for particular indoor or outdoor medium–scale scene datasets. In this manuscript, we study the local and global structures of image scene, and then combine both types of information for indoor and outdoor scenes to improve the scene recognition accuracy. Local region structure indicates sub-part of the scene, such as sky or ground, etc., and global structure indicates whole scene structure, such as sky-background-ground outdoor scene type. For this purpose, the multi-layer convolutional features of inception and residual-based architecture are used at intermediate and higher layers to preserve both local and global structures of image scene. Each layer used for feature extraction, is connected with the global average pooling to obtain a discriminative representation of the image scenes. In this way, local structure is explored at the intermediate convolutional layers, and global spatial structure is obtained from the higher layers. The proposed method is evaluated on 8-scene, 15-scene, UMC-21, MIT67, and 12-scene challenging datasets achieving 98.51%, 96.49%, 99.05%, 80.31%, and 84.88%, respectively, significantly outperforming state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Khan, Altaf and Chefranov, Alexander and Demirel, Hasan},
  doi          = {10.1007/s10489-023-04460-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18431-18449},
  shortjournal = {Appl. Intell.},
  title        = {Building discriminative features of scene recognition using multi-stages of inception-ResNet-v2},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RRQA: Reconfirmed reader for open-domain question answering.
<em>APIN</em>, <em>53</em>(15), 18420–18430. (<a
href="https://doi.org/10.1007/s10489-023-04461-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-domain question answering (QA), the system needs to answer questions from various fields and forms according to given passages. Machine reading comprehension (MRC) can assist the system in comprehending passages and questions, hence often used in improving the performance of the QA system. More passages will bring more features to the result of comprehension, making the reading result more accurate. However, there is still a lack of effective ways when facing multiple passages because it is difficult to use the effective information brought by multiple passages while processing noisy information. This study introduces a new MRC model called RRQA (Reconfirmed Reader for Open-Domain Question Answering) to integrate the QA scenario into traditional MRC, which focuses on the interaction between questions and passages, considering the noisy information. The proposed model consists of two parts: answer extraction and answer verification. In the part of answer extraction, a multi-layer neural network model extracts candidate answers from each passage according to the question, taking into account that the question may have no answer. In the answer verification part, the final answer is verified by combining the features of all candidate answers, which can reduce the incompleteness of the independent answer extraction. Experiments show that the model RRQA outperforms the state-of-the-art models on the WebQA dataset.},
  archive      = {J_APIN},
  author       = {Li, Shi and Zhang, Wenqian},
  doi          = {10.1007/s10489-023-04461-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18420-18430},
  shortjournal = {Appl. Intell.},
  title        = {RRQA: Reconfirmed reader for open-domain question answering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic multiobjective evolutionary algorithm based on
fine prediction strategy and nondominated solutions-guided evolution.
<em>APIN</em>, <em>53</em>(15), 18398–18419. (<a
href="https://doi.org/10.1007/s10489-022-04429-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic multiobjective evolutionary algorithm (DMOEA) is an efficient solver for dynamic multiobjective optimization problems (DMOPs). It is challenging for algorithms to converge quickly and maintain diversity in new environments. However, existing DMOEAs incorporate strategies only in the environment response stage, which may limit the further improvement of the algorithm performance. To address this problem, different strategies have been proposed in the environment response stage and static optimization stage to balance convergence and diversity throughout the optimization process. In the static optimization stage, nondominated solutions-guided evolution leaves the individuals that are closer to the nondominated individuals among the original individuals and the opposition individuals generated by opposition-based learning, which can accelerate the convergence of each generation. In the environment response stage, based on the individual dominance relationship before environmental change, the fine prediction strategy performs difference prediction and opposition-based learning prediction for nondominated and dominated individuals, respectively, which results in an initial population with good convergence and diversity in the new environment. The performance of proposed algorithm was evaluated on 22 instances and compared to eight state-of-the-art algorithms. The results show that proposed algorithm outperforms its competitors on most problems. Additionally, proposed algorithm performs better in early environmental changes and is relatively insensitive to different severity changes.},
  archive      = {J_APIN},
  author       = {Wang, Peidi and Ma, Yongjie},
  doi          = {10.1007/s10489-022-04429-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18398-18419},
  shortjournal = {Appl. Intell.},
  title        = {A dynamic multiobjective evolutionary algorithm based on fine prediction strategy and nondominated solutions-guided evolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally automatic fuzzy clustering for probability density
functions and its application for image data. <em>APIN</em>,
<em>53</em>(15), 18381–18397. (<a
href="https://doi.org/10.1007/s10489-023-04470-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering for probability density functions (CDF) can be categorized as non-fuzzy and fuzzy approaches. Regarding the second approach, the iterative refinement technique has been used for searching the optimal partition. This method could be easily trapped at a local optimum. In order to find the global optimum, a meta-heuristic optimization (MO) algorithm must be incorporated into the fuzzy CDF problem. However, no research utilizing MO to solve the fuzzy CDF problem has been proposed so far due to the lack of a reasonable encoding for converting a fuzzy clustering solution to a chromosome. To address this shortcoming, a new definition called Gaussian prototype is defined first. This type of prototype is capable of accurately representing the cluster without being overly complex. As a result, prototypes’ information can be easily integrated into the chromosome via a novel prototype-based encoding method. Second, a new objective function is introduced to evaluate a fuzzy CDF solution. Finally, Differential Evolution (DE) is used to determine the optimal solution for fuzzy clustering. The proposed method, namely DE-AFCF, is the first to propose a globally automatic fuzzy CDF algorithm, which not only can automatically determine the number of clusters k but also can search for the optimal fuzzy partition matrix by taking into account both clustering compactness and separation. The DE-AFCF is also applied in some image clustering problems, such as processed image detection, and traffic image recognition.},
  archive      = {J_APIN},
  author       = {Nguyen-Trang, Thao and Nguyen-Thoi, Trung and Vo-Van, Tai},
  doi          = {10.1007/s10489-023-04470-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18381-18397},
  shortjournal = {Appl. Intell.},
  title        = {Globally automatic fuzzy clustering for probability density functions and its application for image data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-and category-aware double self-attention model for
next POI recommendation. <em>APIN</em>, <em>53</em>(15), 18355–18380.
(<a href="https://doi.org/10.1007/s10489-022-04396-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-Interest (POI) recommender systems can effectively assist users to find their preferred POIs. Recent studies mainly focus on extracting users’ dynamic context from their check-in behaviors and using attention mechanism to capture different influence of context information for predicting their real-time requirements. However, the existing methods mainly focus on learning the weights of different POI as well as their correlations in the check-in sequences. In addition, these methods still suffer from limited performance, especially when the interaction data are sparse. In this paper, we propose a C ontext- and C ategory-aware D ouble S elf-A ttention (CCDSA) model for POI recommendation to explore and capture users’ contextual preferences in two different aspects collaboratively, including the fine-grained preference for POI in check-in behaviors and the coarse-grained preference for category. Specifically, we first design a double self-attention mechanism module to learn the users’ preferences for both POI and category in specific context. Then we combine users’ check-in behaviors with POIs’ category information to alleviate data sparsity problem in context-aware recommendation. Finally, we leverage context and category information to perform personalized POI recommendation. In particular, we devise an improved version of CCDSA, i.e., CCDSA+, which further replaces the self-attention mechanism with the sparse self-attention mechanism for improving training efficiency. The experimental results on four real-world datasets, Foursquare-NY, Foursquare-TKY, Weeplaces-NY and Weeplaces-SF show that the proposed models, CCDSA and CCDSA+, outperform the baselines.},
  archive      = {J_APIN},
  author       = {Wang, Dongjing and Wan, Feng and Yu, Dongjin and Shen, Yi and Xiang, Zhengzhe and Xu, Yueshen},
  doi          = {10.1007/s10489-022-04396-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18355-18380},
  shortjournal = {Appl. Intell.},
  title        = {Context-and category-aware double self-attention model for next POI recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic signal optimization control method based on adaptive
weighted averaged double deep q network. <em>APIN</em>, <em>53</em>(15),
18333–18354. (<a
href="https://doi.org/10.1007/s10489-023-04469-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a critical node and major bottleneck of the urban traffic networks, the control of traffic signals at road intersections has an essential impact on road traffic flow and congestion. Deep reinforcement learning algorithms have shown excellent control effects on traffic signal timing optimization. Still, the diversity of actual road control scenarios and real-time control requirements have put forward higher requirements on the adaptiveness of the algorithms. This paper proposes an Adaptive Weighted Averaged Double Deep Q Network (AWA-DDQN) based traffic signal optimal control method. Firstly, the formula is used to calculate the double estimator weight for updating the network model. Then, the mean value of the action evaluation is calculated by the network history parameters as the target value. Based on this, a certain number of adjacent action evaluation values are used to generate hyperparameters for weight calculation through the fully connected layer, and the number of action values for mean calculation is gradually reduced to enhance the stability of model training. Finally, simulation experiments were conducted using the traffic simulation software Vissim. The results show that the AWA-DDQN-based signal control method effectively reduces the average delay time, the average queue length and the average number of stops of vehicles compared with existing methods, and significantly improves traffic flow efficiency at intersections.},
  archive      = {J_APIN},
  author       = {Chen, Youqing and Zhang, Huizhen and Liu, Minglei and Ye, Ming and Xie, Hui and Pan, Yubiao},
  doi          = {10.1007/s10489-023-04469-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18333-18354},
  shortjournal = {Appl. Intell.},
  title        = {Traffic signal optimization control method based on adaptive weighted averaged double deep q network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel spatiotemporal multigraph convolutional network for
air pollution prediction. <em>APIN</em>, <em>53</em>(15), 18319–18332.
(<a href="https://doi.org/10.1007/s10489-022-04418-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the industrialization of society, air pollution has become a critical environmental issue, leading to excessive morbidity and mortality from cardiovascular and respiratory diseases in humans. Accurate air pollution prediction has strongly promoted air quality control, which is important for human health. However, previous studies have failed to model spatiotemporal dependencies simultaneously with non-Euclidean distributions considering meteorological factors. In this study, a novel multigraph convolutional neural network for air pollution prediction is proposed. First, a spatial graph, an air pollution pattern graph and a meteorological pattern graph are constructed to model different relationships among non-Euclidean areas. Second, the graph convolutional network is applied to learn and incorporate the information of neighbour nodes of the corresponding graph, and then the graphs after convolution are fused. Finally, the fused matrix of GCNs is input into the gate recurrent units to capture temporal dependencies. Experimental results on the real dataset collected at air quality monitoring stations in Beijing validate the effectiveness of our proposed model.},
  archive      = {J_APIN},
  author       = {Chen, Jing and Yuan, Changwei and Dong, Shi and Feng, Jian and Wang, Hujun},
  doi          = {10.1007/s10489-022-04418-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18319-18332},
  shortjournal = {Appl. Intell.},
  title        = {A novel spatiotemporal multigraph convolutional network for air pollution prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local community detection based on influence maximization in
dynamic networks. <em>APIN</em>, <em>53</em>(15), 18294–18318. (<a
href="https://doi.org/10.1007/s10489-022-04403-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network analysis (SNA) has opened up different research areas to researchers, such as Community Detection and Influence Maximization. By modeling social networks as graphs, one can detect one’s communities or find the most Influential nodes for different applications. Despite extensive research in this area, existing methods have not yet fully met analysts’ needs and are still being improved. Researchers have recently begun to apply certain concepts of a research area in social network analysis to improve social network analysis methods in other areas. In this article, we claimed that applying Two-phase Influence Maximization can improve some community detection methods. To prove the claim, we made some changes in one of the current and efficient local community detection methods to improve the way of finding the initial nodes with the new approach to finding the most influential nodes. The results showed a significant improvement. Another problem was applying this method to dynamic networks, which could be time consuming. To solve this problem, proposed a new technique that allows us to find the initial nodes in each snapshot in a new way without carrying time consuming calculations. The experimental results showed that the novel approach and the new method outperformed the previous ones in both static and dynamic social networks.},
  archive      = {J_APIN},
  author       = {Samie, Mohammad Ebrahim and Behbood, Eileen and Hamzeh, Ali},
  doi          = {10.1007/s10489-022-04403-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18294-18318},
  shortjournal = {Appl. Intell.},
  title        = {Local community detection based on influence maximization in dynamic networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Center transfer for supervised domain adaptation.
<em>APIN</em>, <em>53</em>(15), 18277–18293. (<a
href="https://doi.org/10.1007/s10489-022-04414-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) is a popular strategy for pattern recognition and classification tasks. It leverages a large amount of data from the source domain to help train the model applied in the target domain. Supervised domain adaptation (SDA) approaches are desirable when only few labeled samples from the target domain are available. They can be easily adopted in many real-world applications where data collection is expensive. In this study, we propose a new supervision signal, namely center transfer loss (CTL), to efficiently align features under the SDA setting in the deep learning (DL) field. Unlike most previous SDA methods that rely on pairing up training samples, the proposed loss is trainable only using one-stream input based on the mini-batch strategy. The CTL exhibits two main functionalities in training to increase the performance of DL models, i.e., domain alignment and increasing the feature’s discriminative power. The hyper-parameter to balance these two functionalities is waived in CTL, which is the second improvement from the previous approaches. Extensive experiments completed on well-known public datasets show that the proposed method performs better than recent state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Huang, Xiuyu and Zhou, Nan and Huang, Jian and Zhang, Huaidong and Pedrycz, Witold and Choi, Kup-Sze},
  doi          = {10.1007/s10489-022-04414-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18277-18293},
  shortjournal = {Appl. Intell.},
  title        = {Center transfer for supervised domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-comparison network for visual tracking.
<em>APIN</em>, <em>53</em>(15), 18263–18276. (<a
href="https://doi.org/10.1007/s10489-023-04466-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese networks for visual tracking have been widely applied due to their good performance. However, the performance of Siamese networks relies on the selection of several hyperparameters, including the cosine window weight and target scale penalty. Inappropriate parameter selection will lead to biased target localization and unsteady tracking. The parameter selection is dataset-specific and time-consuming. The necessity of these parameters is caused by the diffused and background-interfered target response map. In addition, the comparison between the target template and candidates in Siamese networks is performed by a simple inner product, which is linear, unbounded, covariate shifted, and cannot benefit the learning of target-background discriminant features. To address the above issues, a novel feature-comparison network (FCNet) has been developed, which combines a feature extraction network and a feature comparison network. First, an RoIAlign layer is incorporated for efficient target proposal generation. Then, the Siamese structure is borrowed to form the feature extraction network but with a different network architecture. Instead of the simple inner product in Siamese networks, a feature concatenation and comparison structure have been adopted for sample feature similarity evaluation, which has combined several convolutional and fully-connected layers for similarity computation. The comparison network, which is nonlinear, bounded and covariate unshifted, performs more efficient correlation computation and provides similarity feedback for target-background discriminant feature learning with stronger representation and generalization. A more compact and target-dominant response map has been obtained by FCNet, which assures robust and steady tracking. Experiments on benchmarks OTB2013, OTB2015, VOT2016 and UAV123 show that FCNet has obtained state-of-the-art real-time tracking performance with 30 FPS. The code and models will be available on GitHub.},
  archive      = {J_APIN},
  author       = {Cui, Zhiyan and Lu, Na},
  doi          = {10.1007/s10489-023-04466-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18263-18276},
  shortjournal = {Appl. Intell.},
  title        = {Feature-comparison network for visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using relative dependency complement
mutual information in fitting fuzzy rough set model. <em>APIN</em>,
<em>53</em>(15), 18239–18262. (<a
href="https://doi.org/10.1007/s10489-022-04445-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a reliable and valid tool for analyzing uncertain information, fuzzy rough set theory has attracted widespread concern in feature selection. However, the performance of fuzzy rough set model is generally affected by various factors, for instance, large data distribution differences, unreasonable settings for fuzzy information granules and feature evaluation functions with single perspective. Considering these problems, a fitting fuzzy rough set model with relative dependency complement mutual information is proposed in this paper. First, the relative distance is introduced to eliminate the influence of data distribution on the fuzzy similarity relation. Then, by analyzing the similarity distributions of samples with regard to decisions, a fitting fuzzy neighborhood radius is proposed to improve the fuzzy information granules, and a fitting fuzzy rough set model is proposed based on the relative distance and the fitting fuzzy neighborhood radius. Moreover, considering the complementary characteristics between fuzzy information granularity and fuzzy information entropy, the related definitions of relative complement information entropy in the fitting fuzzy rough set model are offered, and a multiview uncertainty measure based on relative dependency complement mutual information is constructed to comprehensively analyze the uncertainty of information. Finally, a heuristic feature selection algorithm is designed. A series of experiments designed in this paper prove the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Xu, Jiucheng and Meng, Xiangru and Qu, Kanglin and Sun, Yuanhao and Hou, Qinchen},
  doi          = {10.1007/s10489-022-04445-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18239-18262},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection using relative dependency complement mutual information in fitting fuzzy rough set model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale attention context-aware network for detection
and localization of image splicing. <em>APIN</em>, <em>53</em>(15),
18219–18238. (<a
href="https://doi.org/10.1007/s10489-022-04421-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, advanced image editing tools and techniques produce more realistic tampered images, especially the addition of intelligent retouching technology makes the threshold of tampered operations lower and lower, which can easily evade image forensic systems and make it more difficult to verify the authenticity of images. The field of forensics has failed to achieve effective development due to the lack of high-quality splicing datasets. In this paper, we present the SMI20K-the first benchmark dataset for image splicing operations under intelligent tampered techniques, which contains a total of 20,000 splicing tampered images. By combining Seamless Cloning and image similarity search techniques, the tampered images have more hidden manipulation traces, making it difficult for the naked eye to distinguish the tampered targets. SMI20K brings a new challenge to the field of image forensics. Furthermore, we propose the novel Multi-scale Attention Context-aware Network (MAC-Net) to address the novel challenge of image tampered presently. Specifically, we propose a Multi-scale Multi-level Attention Module (MMAM) that not only effectively resolves feature inconsistencies at different scales, but also automatically adjusts the coefficients of the original input features to maintain detailed features. The fused features are then fed to the proposed Multi-Branch Global Context Module (MGCM), it has three different branches that not only enriches the contextual information but also maintains the detailed features of the target through automatic coefficient adjustment. Extensive experimental results on three public datasets and the proposed dataset show that the proposed model outperforms other state-of-the-art (SOTA) models in image forgery localization.},
  archive      = {J_APIN},
  author       = {Ren, Ruyong and Niu, Shaozhang and Jin, Junfeng and Zhang, Jiwei and Ren, Hua and Zhao, Xiaojie},
  doi          = {10.1007/s10489-022-04421-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18219-18238},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale attention context-aware network for detection and localization of image splicing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained semantic textual similarity measurement via a
feature separation network. <em>APIN</em>, <em>53</em>(15), 18205–18218.
(<a href="https://doi.org/10.1007/s10489-022-04448-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic text similarity (STS), which measures the semantic similarity of sentences, is an important task in the field of NLP. It has a wide range of applications, such as machine translation (MT), semantic search, and summarization. In recent years, with the development of deep neural networks, the existing semantic similarity measurement has made great progress. In particular, pretraining models, such as BERT-based models, which have been good representations of sentence features, have set a new state-of-the-art on STS tasks. Although a large amount of corpus data are used in the pretraining stage, there is no fine-grained semantic analysis. We observe that many sentences, such as user reviews and the QA corpus, can be abstractly regarded as including two core parts: a) this sentence states a certain attribute; and b) this attribute is described by descriptive words. This feature is particularly prominent in the corpus of reviews. Motivated by the above observations, in this paper, we propose a feature separation network (FSN) model, which can further separate and extract attribute features and description features and then measure the semantic similarity according to the separated features. To better verify the effectiveness of our model, we propose an unsupervised approach to construct the semantic similarity dataset in the review domain. Experimental results demonstrate that our method outperforms the general semantic similarity measurement method.},
  archive      = {J_APIN},
  author       = {Chen, Qiang and Zhao, Guoshuai and Wu, Yuxia and Qian, Xueming},
  doi          = {10.1007/s10489-022-04448-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18205-18218},
  shortjournal = {Appl. Intell.},
  title        = {Fine-grained semantic textual similarity measurement via a feature separation network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-view graph neural network with gating mechanism for
entity alignment. <em>APIN</em>, <em>53</em>(15), 18189–18204. (<a
href="https://doi.org/10.1007/s10489-022-04393-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment aims to connect equivalent entities between different knowledge graphs (KGs), which is an important step in knowledge fusion. The structural heterogeneity between KGs severely hinders the development of entity alignment. The existing researches mainly focus on alleviating the structural heterogeneity from the view of entity neighborhood heterogeneity, ignoring the important effect of the relation heterogeneity on it. To this end, we propose a Dual-view graph neural network (GNN) based on a gating mechanism named DvGNet, which comprehensively alleviates the structural heterogeneity of KG from the perspective of entity interaction and relation interaction. From the perspective of entity interaction, DvGNet gives important neighbors high weights to alleviate the heterogeneity of entity neighborhood. From the perspective of relation interaction, DvGNet obtains the relation matching degree between KGs according to the relation embeddings, so as to alleviate the relation heterogeneity. Furthermore, to learn the precise representation for the entity, we propose a concise and effective gating mechanism to aggregate the embeddings among the network layers. We conduct extensive experiments on three entity alignment datasets, as well as detailed ablation studies and analyses, demonstrating the effectiveness of DvGNet.},
  archive      = {J_APIN},
  author       = {Li, Lishuang and Dong, Jiangyuan and Qin, Xueyang},
  doi          = {10.1007/s10489-022-04393-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18189-18204},
  shortjournal = {Appl. Intell.},
  title        = {Dual-view graph neural network with gating mechanism for entity alignment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SE-YOLOv4: Shuffle expansion YOLOv4 for pedestrian detection
based on PixelShuffle. <em>APIN</em>, <em>53</em>(15), 18171–18188. (<a
href="https://doi.org/10.1007/s10489-023-04456-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pedestrian detection, the upsampling operation of YOLOv4 during feature aggregation affects the integrity of feature information for small-scale and occluded targets. To address this issue, we propose a pedestrian detection model named Shuffle Expansion YOLOv4 (SE-YOLOv4) composed of a path aggregation network based on PixelShuffle (Shuffle-PANet) and an efficient pyramid atrous convolutional block attention module (EPA-CBAM), to improve the detection performance of small-scale and occluded pedestrian targets. First, we propose a feature aggregation network Shuffle-PANet based on PixelShuffle to maintain the feature information integrity of small-scale and occluded targets by expanding high-resolution feature maps through convolutions and interchannel periodic shuffling instead of linear interpolation-based upsampling. Then, we propose EPA-CBAM, whose channel attention module (EPA-CAM) can build a pyramid structure and obtain fine-grained multiscale spatial information in different channels by dilated convolutions of corresponding sizes. The results show that the miss rate of SE-YOLOv4 decreased by 3.54% compared with that of the vanilla YOLOv4 on the CityPersons dataset. Comparison experiment results on four challenging pedestrian detection datasets show that our method achieves very competitive performance and maintains a reasonable balance between accuracy and speed.},
  archive      = {J_APIN},
  author       = {Liu, Mingsheng and Wan, Liang and Wang, Bo and Wang, Tingting},
  doi          = {10.1007/s10489-023-04456-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18171-18188},
  shortjournal = {Appl. Intell.},
  title        = {SE-YOLOv4: Shuffle expansion YOLOv4 for pedestrian detection based on PixelShuffle},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label learning with missing labels using sparse global
structure for label-specific features. <em>APIN</em>, <em>53</em>(15),
18155–18170. (<a
href="https://doi.org/10.1007/s10489-022-04439-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning associates a given data instance with one or several class labels. A frequent problem with real life multi-label datasets is the lack of complete label information. Incomplete labels increase model complexity as the label correlation information is not reliable, resulting in a suboptimal multi-label classifier. Further, high dimensionality of multi-label datasets often introduces spurious feature-label dependencies. Thus, discovering label-specific features is imperative for efficient handling of high-dimensional data for multi-label learning with missing labels. To deal with the issues emerging from incomplete labels and high-dimensional input space, we propose a multi-label learning approach based on identifying the label-specific features and constraining them with a sparse global structure. The sparse structural constraint helps maintain the typical characteristics of the multi-label learning data. Instances are expressed as linear combination of label-specific features and the inter-relation guides the construction of model coefficients. The model also constructs supplementary label correlations to assist missing label recovery as part of the optimization problem. Empirical results on benchmark multi-label datasets highlight the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Kumar, Sanjay and Ahmadi, Nadira and Rastogi, Reshma},
  doi          = {10.1007/s10489-022-04439-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18155-18170},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label learning with missing labels using sparse global structure for label-specific features},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scale-aware UNet++ model combined with attentional context
supervision and adaptive tversky loss for accurate airway segmentation.
<em>APIN</em>, <em>53</em>(15), 18138–18154. (<a
href="https://doi.org/10.1007/s10489-022-04380-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated and accurate airway segmentation from chest computed tomography (CT) images is essential to enable quantitative assessment of airway diseases and aid intra-operative navigation for pulmonary intervention surgery. Although deep learning-based methods have achieved massive success in medical image segmentation, it is still challenging to segment the airways accurately and entirely from CT images, especially the small airways. The feature vanishing of small airways, the local discontinuities of small airway branches, and the varying degrees of class imbalance between foreground and background have seriously affected airway segmentation performance. This paper presents an improved UNet++-based model that introduces a novel supervision manner and a new adaptive loss function to address these problems. Specifically, we put forward an attentional context supervision (ACS) manner, where different supervision branches and attention mechanisms are presented to capture more discriminative multi-scale features. In addition, we present an adaptive Tversky loss (ATL) function by integrating radial distance information and segmentation-wise focal loss into the Tversky loss, enabling adaptive focus on learning the target airways under the particular class imbalance condition. The experimental results on the public dataset showed that the proposed ACS and ATL brought considerable performance gains. Moreover, our method obtained the best sensitivity and comparable accuracy on the complete airway segmentation compared with the state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Ke, Zunyun and Xu, Xiuyuan and Zhou, Kai and Guo, Jixiang},
  doi          = {10.1007/s10489-022-04380-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18138-18154},
  shortjournal = {Appl. Intell.},
  title        = {A scale-aware UNet++ model combined with attentional context supervision and adaptive tversky loss for accurate airway segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HaarAE: An unsupervised anomaly detection model for IOT
devices based on haar wavelet transform. <em>APIN</em>, <em>53</em>(15),
18125–18137. (<a
href="https://doi.org/10.1007/s10489-023-04449-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the shortcomings of the existing anomaly detection methods based on IoT devices, including insufficient feature extraction, poor model fitting effect and low accuracy, this paper proposes an unsupervised IoT device traffic anomaly detection model called HaarAE, which introduces Haar wavelet transform to enhance the feature expression of original data and improve the model’s ability to identify anomalies. The convolutional autoencoder was used to construct the network structure, the memory module is introduced to increase the reconstruction error, and the ConvLSTM layer was added to the encoder to extract the temporal characteristics of the data. The output of each layer of decoder is cascaded with the output of the corresponding ConvLSTM layer, so that the decoder can obtain more coding information of each layer to reconstruct the original data and enhance the fitting ability of the model. Experiments on public datasets and real traffic datasets indicate that compared to the mainstream unsupervised models, HaarAE improves the anomaly detection effect.},
  archive      = {J_APIN},
  author       = {Xie, Xin and Li, Xinlei and Xu, Lei and Ning, Weiye and Huang, Yuhui},
  doi          = {10.1007/s10489-023-04449-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18125-18137},
  shortjournal = {Appl. Intell.},
  title        = {HaarAE: An unsupervised anomaly detection model for IOT devices based on haar wavelet transform},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fast instance selection method for SVM training based on
fuzzy distance metric. <em>APIN</em>, <em>53</em>(15), 18109–18124. (<a
href="https://doi.org/10.1007/s10489-022-04447-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a well-known classification technique which has achieved excellent performance in many nonlinear and high dimensional pattern recognition fields. However, due to the high time complexity of training SVM model, it’s difficult to implement it for large-scale data sets. One of the most promising solutions is to reduce the training data used for establishing the optimal classification hyperplane by means of selecting relevant support vectors which are the only factors affecting the classification rule. Thus, instance selection method is an efficient pre-processing technique to reduce the computational complexity and storage requirements of the learning process. In this manuscript, considering the geometry-distribution of data sets, we propose a Half Shell Extraction (HSE) algorithm which falls into the condensation category of instance selection methods. Moreover, fuzzy distance metric based on locality sensitive hash is employed to accelerate the instance selection process. Empirically, an experimental study involving various of data sets is carried out to compare the proposed algorithm with five competitive algorithms, and the results obtained show that the proposed algorithm consistently outperforms the other algorithms in terms of accuracy, reduction capability and runtime.},
  archive      = {J_APIN},
  author       = {Zhang, Junyuan and Liu, Chuan},
  doi          = {10.1007/s10489-022-04447-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {15},
  pages        = {18109-18124},
  shortjournal = {Appl. Intell.},
  title        = {Fast instance selection method for SVM training based on fuzzy distance metric},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correction to: 3D solid model generation method based on a
generative adversarial network. <em>APIN</em>, <em>53</em>(14), 18107.
(<a href="https://doi.org/10.1007/s10489-023-04459-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Du, Wenfeng and Xia, Zhuang and Han, Leyu and Gao, Boqing},
  doi          = {10.1007/s10489-023-04459-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18107},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: 3D solid model generation method based on a generative adversarial network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to: Fusion of overexposed and underexposed images
using caputo differential operator for resolution and texture based
enhancement. <em>APIN</em>, <em>53</em>(14), 18106. (<a
href="https://doi.org/10.1007/s10489-022-04404-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Zhou, Liang and Alenezi, Fayadh S. and Nandal, Amita and Dhaka, Arvind and Wu, Tao and Koundal, Deepika and Alhudhaif, Adi and Polat, Kemal},
  doi          = {10.1007/s10489-022-04404-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18106},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Fusion of overexposed and underexposed images using caputo differential operator for resolution and texture based enhancement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Multibranch multilevel federated learning
for a better feature extraction and a plug-and-play dynamic-adjusting
double flow personalization approach. <em>APIN</em>, <em>53</em>(14),
18105. (<a href="https://doi.org/10.1007/s10489-022-04438-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Ren, Maoye and Yu, Xinhai},
  doi          = {10.1007/s10489-022-04438-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18105},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Multibranch multilevel federated learning for a better feature extraction and a plug-and-play dynamic-adjusting double flow personalization approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning based evolutionary algorithm framework for
multi-objective optimization problems. <em>APIN</em>, <em>53</em>(14),
18085–18104. (<a
href="https://doi.org/10.1007/s10489-022-04444-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a transfer learning based evolutionary algorithm (TLEA) framework for multi-objective optimization problems (MOPs) is proposed. In the TLEA framework, a complex multi-objective optimization task is decomposed into a set of relatively simple multi-objective optimization subtasks and then optimized collaboratively by parallel subpopulation searches with the proposed transfer learning method. More specifically, neighboring subtasks may have some similar features during parallel searches of corresponding subpopulations, and those similarities can be exploited through the proposed transfer learning strategy to improve the collaboration among these search subpopulations and achieve greater efficiency. To show the generality of the proposed algorithm framework, two implementations of the proposed TLEA framework based on differential evolution (DE) and particle swarm optimization (PSO), i.e., TLPSO and TLDE, are presented and studied in detail. In TLPSO and TLDE, the subproblem features are reflected by the search subpopulations, which are generated by a pair of specific parameters. Therefore, subpopulations can adaptively adjust parameter settings by learning useful information from neighboring subproblems with more appropriate parameters during the search. The experimental results show that TLPSO performs better than other algorithms on at least five out of 12 test problems in terms of the IGD indicator and on at least seven out of 12 test problems in terms of the HV indicator. TLDE has an advantage over the other algorithms on five out of 12 test problems in terms of the IGD indicator and on seven out of 12 test problems in terms of the HV indicator.},
  archive      = {J_APIN},
  author       = {Huang, Jiaheng and Wen, Jiechang and Chen, Lei and Liu, Hai-Lin},
  doi          = {10.1007/s10489-022-04444-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18085-18104},
  shortjournal = {Appl. Intell.},
  title        = {Transfer learning based evolutionary algorithm framework for multi-objective optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obsolete personal information update system: Towards the
prevention of falls in the elderly. <em>APIN</em>, <em>53</em>(14),
18061–18084. (<a
href="https://doi.org/10.1007/s10489-022-04289-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falls stand for a prevalent problem among the elderly and a significant public health concern. In recent years, a growing number of apps have been developed to assist in terms of the delivery of more effective and efficient falls prevention programs. All of these apps rely on a massive elderly personal database gathered from hospitals, mutual health groups, and other organizations that help the elderly. Information on an older adult is constantly changing, and it may become obsolete at any time, contradicting what we currently know about the same person. As a result, it needs to be checked and updated on a regular basis in order to maintain database consistency and hence provide a better service. This research work describes an Obsolete Personal Information Update System (OIUS) developed as part of the elderly-fall prevention project. Our OIUS intends to control and update the information gathered about each older adult in real-time, to provide consistent information on demand, and to provide tailored interventions to carers and fall-risk patients. The method discussed here is based upon a polynomial-time algorithm built on top of a causal Bayesian network that models the older adults data. The outcome is presented as an AND-OR recommendation Tree with a certain level of accuracy. On an aged personal information base, we perform an empirical study for such a model. Experiments corroborate our OIUS’s viability and effectiveness.},
  archive      = {J_APIN},
  author       = {Chaieb, Salma and Mrad, Ali Ben and Hnich, Brahim},
  doi          = {10.1007/s10489-022-04289-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18061-18084},
  shortjournal = {Appl. Intell.},
  title        = {Obsolete personal information update system: Towards the prevention of falls in the elderly},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). The charging station and swapping station site selection
with many-objective evolutionary algorithm. <em>APIN</em>,
<em>53</em>(14), 18041–18060. (<a
href="https://doi.org/10.1007/s10489-022-04292-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The battery swap mode is a novel way of energy supplement for electric vehicles. Inevitably, there are some business transactions between battery swapping station (BSS) and battery centralized charging station (BCCS) in the mode. Therefore, it is essential to plan the construction of BSS and BCCS uniformly. Moreover, the needs of enterprises and users are not taken into account simultaneously in the existing site selection model. To resolve this problem, a many-objective joint site selection (MOJSS) model of BSS and BCCS is proposed in this paper. It mainly includes four objective functions: construction cost, coverage rate, investment income and satisfaction, which consider distance constraint between user demand points and the BSS, distance constraint between BBS and BSS, and the service ability constraint of BSS and the BCCS. To better solve the proposed model, a Grid-based evolutionary algorithm based on hybrid environment selection strategy is proposed. Furthermore, the segmented integer coding strategy and the specific genetic operation are designed based on the characteristic of model. It is compared with the existing many-objective evolutionary algorithms on standard test problems. Then the algorithm is applied to solve the established model. The experimental result demonstrated the reasonableness and effectiveness of proposed model. Finally, the site selection results are illustrated by a set of solutions.},
  archive      = {J_APIN},
  author       = {He, Yongqiang and Zhang, Yanjun and Fan, Tian and Cai, Xingjuan and Xu, Yubin},
  doi          = {10.1007/s10489-022-04292-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18041-18060},
  shortjournal = {Appl. Intell.},
  title        = {The charging station and swapping station site selection with many-objective evolutionary algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit regularization of a deep augmented neural network
model for human motion prediction. <em>APIN</em>, <em>53</em>(14),
18027–18040. (<a
href="https://doi.org/10.1007/s10489-022-04419-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting human motion based on past observed motion is one of the challenging issues in computer vision and graphics. Existing research works are dealing with this issue by using discriminative models and showing the results for cases that follow a homogeneous distribution (in distribution) and not discussing the issues of the domain shift problem, where training and testing data follow a heterogeneous (out of distribution) problem, which is the reality when such models are used in practice. However, recent research proposed addressing domain shift issues by augmenting the discriminative model with a generative model and obtained better results. In the present investigation, we propose regularizing the extended network by inserting linear layers to minimize the rank of the latent space and train the entire end-to-end network. We regularize the network to strengthen the model to deal effectively with domain shift scenarios. Both training and testing data come from different distribution sets; to deal with this, we toughen our network by adding the extra linear layers to the network encoder. We tested our model with the benchmark datasets, CMU Motion Capture and Human3.6M, and proved that our model outperforms 14 OoD actions of H3.6M and 7 OoD actions of CMU MoCap in terms of the Euclidean distance calculated between predicted and ground truth joint angle values. Our average results of 14 OoD actions for short-term (80, 160, 320, 400) are 0.34, 0.6, 0.96, 1.07, and for CMU MoCap of 7 OoD actions for short-term and long term (80, 160, 320, 400, 1000) are 0.28, 0.45, 0.77, 0.89, 1.46. All these results are much better than the other state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Yadav, Gaurav Kumar and Abdel-Nasser, Mohamed and Rashwan, Hatem A. and Puig, Domenec and Nandi, G. C.},
  doi          = {10.1007/s10489-022-04419-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18027-18040},
  shortjournal = {Appl. Intell.},
  title        = {Implicit regularization of a deep augmented neural network model for human motion prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view multi-objective clustering-based framework for
scientific document summarization using citation context. <em>APIN</em>,
<em>53</em>(14), 18002–18026. (<a
href="https://doi.org/10.1007/s10489-022-04166-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the expanding rate of scientific publications, it has become a necessity to summarize scientific documents to allow researchers to keep track of recent developments. In this paper, we formulate the scientific document summarization problem in a multi-view clustering (MVC) framework. Two views of the scientific documents, semantic and syntactic, are considered jointly in MVC framework. To obtain an improved partitioning corresponding to different views, a differential evolution algorithm is utilized as the underlying optimization strategy. After obtaining the final optimal partitioning, various sentence-level features like length, position of the sentences, among others, are investigated to extract high scoring sentences and these extracted sentences are used to form the summary. We have also investigated the use of (a) two embedding spaces to represent the sentences of the documents in semantic space; and (b) use of citation contexts to incorporate their importance in summarizing a given scientific document. Our proposed multi-view based clustering approach is purely unsupervised in nature and, does not utilize any labelled data for the generation of the summary. To show the potentiality of multi-view clustering, a single-view based clustering framework is also developed for the purpose of comparison. The performance of the multi-view based system in comparison to a single-view is first tested on some random articles selected from the recently released SciSummNet 2019 dataset using ROUGE measure. Then, two more datasets namely, CL-SciSumm 2016 and CL-SciSumm 2017 are considered for evaluation. Our investigations prove that the proposed approach yields better results than the existing supervised and unsupervised methods including transformer-based deep learning model, with statistically significant improvements.},
  archive      = {J_APIN},
  author       = {Saini, Naveen and Reddy, Saichethan Miriyala and Saha, Sriparna and Moreno, Jose G. and Doucet, Antoine},
  doi          = {10.1007/s10489-022-04166-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {18002-18026},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view multi-objective clustering-based framework for scientific document summarization using citation context},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Localized shapelets selection for interpretable time series
classification. <em>APIN</em>, <em>53</em>(14), 17985–18001. (<a
href="https://doi.org/10.1007/s10489-022-04422-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapelet-based methods have attracted increasing attention in time series research due to their good classification performance. Most existing approaches distinguish time series from different classes by evaluating the discriminative ability of all subsequeces according to the distance information, and the extracted shapelets might not always be real subsequeces of the original time series, which lead to expensive computation and poor interpretability. However, the information about where the shapelet is located in the time series is of importance to discriminate classes. In this paper, we propose a lo calized shapelets selection approach for interpretable time series classification. Specifically, a location measure and distance measure are defined to evaluate the discriminative ability of each shapelet candidate, and then the shapelet transformation process also integrates the location information of shapelets to provide a more interpretable insight in the classification result. Extensive experiments show that our proposed method is competitive on accuracy and efficiency compared with 18 baselines on UCR repository, and the location information effectively contributes to the improvement of shapelet interpretability.},
  archive      = {J_APIN},
  author       = {Chen, Jiahui and Wan, Yuan},
  doi          = {10.1007/s10489-022-04422-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17985-18001},
  shortjournal = {Appl. Intell.},
  title        = {Localized shapelets selection for interpretable time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved belief hellinger divergence for dempster-shafer
theory and its application in multi-source information fusion.
<em>APIN</em>, <em>53</em>(14), 17965–17984. (<a
href="https://doi.org/10.1007/s10489-022-04428-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster-Shafer theory (DST), as a generalization of Bayesian probability theory, is a useful technique for achieving multi-source information fusion under uncertain environments. Nevertheless, when a high degree of conflict exists between pieces of evidence, unreasonable results are often generated using Dempster’s combination rule. How to fuse highly conflicting information is still an open problem. In this study, we first propose an improved belief Hellinger divergence measure, which can fully consider the uncertainty in basic probability assignments, to quantify the conflict level between evidence. Second, some properties (i.e., nonnegativity, nondegeneracy, symmetry, and trigonometric inequality) of the proposed divergence measure are discussed. Then, we present a novel multi-source information fusion strategy, in which the credibility of the evidence is determined based on external discrepancy and internal ambiguity. Additionally, we consider the decay of credibility when fusing evidence across different times. Finally, applications in fault diagnosis and Iris dataset classification are presented to demonstrate the effectiveness of our method. The results indicate that our approach is more reasonable and can identify the target with a higher belief degree.},
  archive      = {J_APIN},
  author       = {Hua, Zhen and Jing, Xiaochuan},
  doi          = {10.1007/s10489-022-04428-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17965-17984},
  shortjournal = {Appl. Intell.},
  title        = {An improved belief hellinger divergence for dempster-shafer theory and its application in multi-source information fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese few-shot network: A novel and efficient network for
medical image segmentation. <em>APIN</em>, <em>53</em>(14), 17952–17964.
(<a href="https://doi.org/10.1007/s10489-022-04417-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is attracting more researchers due to its outstanding ability to find unseen classes with less data. Meanwhile, we noticed that medical data is difficult to collect and label, but there is a major need for higher accuracy in either organ segmentation or disease classification. Therefore, we propose a few-shot learning model with a Siamese core, the Siamese few-shot network (SFN) to improve medical image segmentation. To the beset of our knowledge, SFN is the first model to introduce few-shot learning combined with the Siamese idea to medical image segmentation. Furthermore, we also design a grid attention(GA) module to locally focus semantic information, especially in medical images. The results prove that our method outperforms the state-of-the-art model on abdominal organ segmentation for CT and MRI.},
  archive      = {J_APIN},
  author       = {Xiao, Guangli and Tian, Shengwei and Yu, Long and Zhou, Zhicheng and Zeng, Xuanli},
  doi          = {10.1007/s10489-022-04417-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17952-17964},
  shortjournal = {Appl. Intell.},
  title        = {Siamese few-shot network: A novel and efficient network for medical image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved label propagation algorithm based on community
core node and label importance for community detection in sparse
network. <em>APIN</em>, <em>53</em>(14), 17935–17951. (<a
href="https://doi.org/10.1007/s10489-022-04397-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structure can be used to analyze and understand the structural functions in a network, reveal its implicit information, and predict its dynamic development pattern. Existing community detection algorithms are very sensitive to the sparsity of network, and they have difficulty in obtaining stable community detection results. To address these shortcomings, an improved label propagation algorithm combining community core nodes and label importance is proposed (CCLI-LPA). Firstly, the core nodes in a network are selected by fusing the first-order and second-order structures of the nodes, and the network is initialized by them. Then, a new label selection mechanism is defined by combining the importance of both neighboring nodes and their labels, and the label of a node is updated based on it. Validation experiments are conducted on six real networks and eight synthetic networks, and the results show that CCLI-LPA can not only obtain stable results in real networks but also obtain stable and accurate results in sparse networks.},
  archive      = {J_APIN},
  author       = {Yue, Yubin and Wang, Guoyin and Hu, Jun and Li, Yuan},
  doi          = {10.1007/s10489-022-04397-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17935-17951},
  shortjournal = {Appl. Intell.},
  title        = {An improved label propagation algorithm based on community core node and label importance for community detection in sparse network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OCR-RTPS: An OCR-based real-time positioning system for the
valet parking. <em>APIN</em>, <em>53</em>(14), 17920–17934. (<a
href="https://doi.org/10.1007/s10489-022-04362-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining the position of ego-vehicle is a crucial prerequisite for automatic control and path planning in the field of autonomous driving. Most existing positioning systems rely on GPS, RTK, or wireless signals, which are arduous to provide effective localization under weak signal conditions. This paper proposes a real-time positioning system based on the detection of the parking numbers as they are unique positioning marks in the parking lot scene. It does not only can help with the positioning with open area, but also run independently under isolation environment. The result tested on both public datasets and self-collected dataset show that the system outperforms others in both performances and applies in practice. In addition, the code and dataset will release later.},
  archive      = {J_APIN},
  author       = {Wu, Zizhang and Chen, Xinyuan and Wang, Jizheng and Wang, Xiaoquan and Gan, Yuanzhu and Fang, Muqing and Xu, Tianhao},
  doi          = {10.1007/s10489-022-04362-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17920-17934},
  shortjournal = {Appl. Intell.},
  title        = {OCR-RTPS: An OCR-based real-time positioning system for the valet parking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early prediction of sepsis using double fusion of deep
features and handcrafted features. <em>APIN</em>, <em>53</em>(14),
17903–17919. (<a
href="https://doi.org/10.1007/s10489-022-04425-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening medical condition that is characterized by the dysregulated immune system response to infections, having both high morbidity and mortality rates. Early prediction of sepsis is critical to the decrease of mortality. This paper presents a novel early warning model called Double Fusion Sepsis Predictor (DFSP) for sepsis onset. DFSP is a double fusion framework that combines the benefits of early and late fusion strategies. First, a hybrid deep learning model that combines both the convolutional and recurrent neural networks to extract deep features is proposed. Second, deep features and handcrafted features, such as clinical scores, are concatenated to build the joint feature representation (early fusion). Third, several tree-based models based on joint feature representation are developed to generate the risk scores of sepsis onset that are combined with an End-to-End neural network for final sepsis detection (late fusion). To evaluate DFSP, a retrospective study was conducted, which included patients admitted to the ICUs of a hospital in Shanghai China. The results demonstrate that the DFSP outperforms state-of-the-art approaches in early sepsis prediction.},
  archive      = {J_APIN},
  author       = {Duan, Yongrui and Huo, Jiazhen and Chen, Mingzhou and Hou, Fenggang and Yan, Guoliang and Li, Shufang and Wang, Haihui},
  doi          = {10.1007/s10489-022-04425-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17903-17919},
  shortjournal = {Appl. Intell.},
  title        = {Early prediction of sepsis using double fusion of deep features and handcrafted features},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCF-PPI: Improved prediction of protein-protein
interactions by combining learned and handcrafted features based on
attention mechanisms. <em>APIN</em>, <em>53</em>(14), 17887–17902. (<a
href="https://doi.org/10.1007/s10489-022-04387-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein pairs can interact with each other by physical connections through electrostatic forces or hydrophobic effects. Prediction of protein-protein interactions (PPIs) is critical in many biological applications, including protein function identification, drug design, and disease detection. In this paper, we propose a method of combining features for protein representation and PPI prediction directly from protein sequences. First, we utilized 5 protein sequence extractors including amino acid composition, pseudo-amino acid composition, amphiphilic pseudo-amino acid composition, quasi-sequence-order, and dipeptide composition to extract handcrafted features. Next, we applied a natural language processing technique, Word2vec, to generate learned features by embedding protein sequences into a feature space. Finally, a deep neural network architecture was employed for combining two types of features and identifying PPIs. The proposed method was evaluated on the Yeast core, Human, and eight independent datasets. The experimental results show that our method achieved 95.6% of accuracy on Yeast core, 99.2% of accuracy on Human, and 100% of accuracy on eight independent datasets, respectively. In addition, we also made extensive comparisons with other existing prediction methods, and the experimental results demonstrated the superior ability of our proposed method. The datasets and source code of our method can be downloaded at https://github.com/thnhan/DeepCF-PPI.git .},
  archive      = {J_APIN},
  author       = {Tran, Hoai-Nhan and Xuan, Quynh Nguyen Phuc and Nguyen, Tuong-Tri},
  doi          = {10.1007/s10489-022-04387-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17887-17902},
  shortjournal = {Appl. Intell.},
  title        = {DeepCF-PPI: Improved prediction of protein-protein interactions by combining learned and handcrafted features based on attention mechanisms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Robust variable structure discovery based on tilted
empirical risk minimization. <em>APIN</em>, <em>53</em>(14),
17865–17886. (<a
href="https://doi.org/10.1007/s10489-022-04409-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust group lasso regression plays an important role in high-dimensional regression modeling such as biological data analysis for disease diagnosis and gene expression. However, most existing methods are optimized with prior variable structure knowledge under the traditional empirical risk minimization (ERM) framework, in which the estimators are excessively dependent on prior structure information and sensitive to outliers. To address this issue, we propose a new robust variable structure discovery method for group lasso based on a convergent bilevel optimization framework. In this paper, we adopt tilted empirical risk minimization (TERM) as the target function to improve the robustness of the estimator by assigning less weight to the outliers. Moreover, we modify the TERM objective function to calculate its Fenchel conjugate while maintaining its robust property, which is proven theoretically and empirically. Experimental results on both synthetic and real-world datasets show that the proposed method can improve the robust performance on prediction and variable structure discovery compared to the existing techniques.},
  archive      = {J_APIN},
  author       = {Zhang, Xuelin and Wang, Yingjie and Zhu, Liangxuan and Chen, Hong and Li, Han and Wu, Lingjuan},
  doi          = {10.1007/s10489-022-04409-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17865-17886},
  shortjournal = {Appl. Intell.},
  title        = {Robust variable structure discovery based on tilted empirical risk minimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual prune-and-select: Class-incremental learning with
specialized subnetworks. <em>APIN</em>, <em>53</em>(14), 17849–17864.
(<a href="https://doi.org/10.1007/s10489-022-04441-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is capable of learning tasks sequentially mostly without forgetting. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning one task after another. We address this challenge considering a class-incremental learning scenario where the DNN sees test data without knowing the task from which this data originates. During training, Continual Prune-and-Select (CP&amp;S) finds a subnetwork within the DNN that is responsible for solving a given task. Then, during inference, CP&amp;S selects the correct subnetwork to make predictions for that task. A new task is learned by training available neuronal connections of the DNN (previously untrained) to create a new subnetwork by pruning, which can include previously trained connections belonging to other subnetwork(s) because it does not update shared connections. This enables to eliminate catastrophic forgetting by creating specialized regions in the DNN that do not conflict with each other while still allowing knowledge transfer across them. The CP&amp;S strategy is implemented with different subnetwork selection strategies, revealing superior performance to state-of-the-art continual learning methods tested on various datasets (CIFAR-100, CUB-200-2011, ImageNet-100 and ImageNet-1000). In particular, CP&amp;S is capable of sequentially learning 10 tasks from ImageNet-1000 keeping an accuracy around 94% with negligible forgetting, a first-of-its-kind result in class-incremental learning. To the best of the authors’ knowledge, this represents an improvement in accuracy above 10% when compared to the best alternative method.},
  archive      = {J_APIN},
  author       = {Dekhovich, Aleksandr and Tax, David M.J. and Sluiter, Marcel H.F and Bessa, Miguel A.},
  doi          = {10.1007/s10489-022-04441-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17849-17864},
  shortjournal = {Appl. Intell.},
  title        = {Continual prune-and-select: Class-incremental learning with specialized subnetworks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-collection latent beta-liouville allocation model
training with privacy protection and applications. <em>APIN</em>,
<em>53</em>(14), 17824–17848. (<a
href="https://doi.org/10.1007/s10489-022-04378-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-collection topic models extend previous single-collection topic models, such as Latent Dirichlet Allocation (LDA), to multiple collections. The purpose of cross-collection topic modeling is to model document-topic representations and reveal similarities between each topic and differences among groups. However, the restriction of Dirichlet prior and the significant privacy risk have hampered those models’ performance and utility. Training those cross-collection topic models may, in particular, leak sensitive information from the training dataset. To address the two issues mentioned above, we propose a novel model, cross-collection latent Beta-Liouville allocation (ccLBLA), which operates a more powerful prior, Beta-Liouville distribution with a more general covariance structure that enhances topic correlation analysis. To provide privacy protection for the ccLBLA model, we leverage the inherent differential privacy guarantee of the Collapsed Gibbs Sampling (CGS) inference scheme and then propose a hybrid privacy protection algorithm for the ccLBLA model (HPP-ccLBLA) that prevents inferring data from intermediate statistics during the CGS training process without sacrificing its utility. More crucially, our technique is the first attempt to use the cross-collection topic model in image classification applications and investigate the cross-collection topic model’s capabilities beyond text analysis. The experimental results for comparative text mining and image classification will show the merits of our proposed approach.},
  archive      = {J_APIN},
  author       = {Luo, Zhiwen and Amayri, Manar and Fan, Wentao and Bouguila, Nizar},
  doi          = {10.1007/s10489-022-04378-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17824-17848},
  shortjournal = {Appl. Intell.},
  title        = {Cross-collection latent beta-liouville allocation model training with privacy protection and applications},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principal graph embedding convolutional recurrent network
for traffic flow prediction. <em>APIN</em>, <em>53</em>(14),
17809–17823. (<a
href="https://doi.org/10.1007/s10489-022-04211-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential part of traffic management, traffic flow prediction attracts worldwide attention to intelligent traffic systems (ITSs). Complicated spatial dependencies due to the well-connected road networks and time-varying traffic dynamics make this problem extremely challenging. Recent works have focused on modeling this complicated spatial-temporal dependence through graph neural networks with a fixed weighted graph or an adaptive adjacency matrix. However, fixed graph methods cannot address data drift due to changes in the road network structure, and adaptive methods are time consuming and prone to be overfitting because the learning algorithm thoroughly optimizes the adaptive matrix. To address this issue, we propose a principal graph embedding convolutional recurrent network (PGECRN) for accurate traffic flow prediction. First, we propose the adjacency matrix graph embedding (AMGE) generation algorithm to solve the data drift problem. AMGE can model the distribution of spatiotemporal series after data drift by extracting the principal components of the original adjacency matrix and performing an adaptive transformation. At the same time, it has fewer parameters, alleviating overfitting. Then, except for the essential spatial correlations, traffic flow data are also temporally dynamic. We utilize temporal variation by integrating gated recurrent units (GRU) and AMGE to comprise the proposed model. Finally, PGECRN is evaluated on two real-world highway datasets, PeMSD4 and PeMSD8. Compared with the existing baselines, the better prediction accuracy of our model shows that it can accurately and efficiently model traffic flow.},
  archive      = {J_APIN},
  author       = {Han, Yang and Zhao, Shengjie and Deng, Hao and Jia, Wenzhen},
  doi          = {10.1007/s10489-022-04211-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17809-17823},
  shortjournal = {Appl. Intell.},
  title        = {Principal graph embedding convolutional recurrent network for traffic flow prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial adaptive graph convolutional network for
skeleton-based action recognition. <em>APIN</em>, <em>53</em>(14),
17796–17808. (<a
href="https://doi.org/10.1007/s10489-022-04442-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, great achievements have been made in graph convolutional network (GCN) for non-Euclidean spatial data feature extraction, especially the skeleton-based feature extraction. However, the fixed graph structure determined by the fixed adjacency matrix usually causes the problems such as the weak spatial modeling ability, the unsatisfactory generalization performance, the excessively large number of model parameters, and so on. In this paper, a spatially adaptive residual graph convolutional network (SARGCN) is proposed for action recognition based on skeleton feature extraction. Firstly, the uniform and fixed topology is not required in our graph. Secondly, a learnable parameter matrix is added to the GCN operation, which can enhance the model’s capabilities of feature extraction and generalization, while reducing the number of parameters. Therefore, compared with the several existing models mentioned in this paper, the least number of parameters are used in our model while ensuring the comparable recognition accuracy. Finally, inspired by the ResNet architecture, a residual connection is introduced in GCN to obtain higher accuracy at lower computational costs and learning difficulties. Extensive experimental on two large-scale datasets results validate the effectiveness of our proposed approach, namely NTU RGB+D 60 and NTU RGB+D 120.},
  archive      = {J_APIN},
  author       = {Zhu, Qilin and Deng, Hongmin},
  doi          = {10.1007/s10489-022-04442-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17796-17808},
  shortjournal = {Appl. Intell.},
  title        = {Spatial adaptive graph convolutional network for skeleton-based action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FT-FVC: Fast transformation-based feature vector
concatenation for time series classification. <em>APIN</em>,
<em>53</em>(14), 17778–17795. (<a
href="https://doi.org/10.1007/s10489-022-04386-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few decades, a large number of time series classification (TSC) algorithms have been published based on different class pattern hypotheses, among which a vital component is the feature extraction of time series. Currently, the TSC algorithm with the highest classification accuracy is the heterogeneous ensemble, which significantly improves the classification accuracy but greatly increases the algorithm complexity. Therefore, a novel feature extraction algorithm called pipeline transform is proposed to achieve feature diversity enhancement by three fast time series transformations and feature vector concatenation. The time series is transformed to Hilbert, first-order differential, and second-order differential spaces to generate corresponding transformed series. A random convolutional kernel transform is performed on each series to generate the corresponding feature vector. The raw feature vector is concatenated with the three transformed feature vectors respectively to obtain three feature vectors, namely the three-view. In the proposed TSC algorithm FT-FVC, pipeline transform and hard voting are combined, which has excellent classification accuracy and speed. Furthermore, the semi-supervised TSC algorithm, semi-FT-FVC, combines pipeline transform with Tri-Training, improving classification accuracy and reducing classification volatility. The proposed algorithms are compared with other state-of-the-art algorithms on 85 datasets in the UCR archive, whose excellent classification performance is demonstrated by statistical tests.},
  archive      = {J_APIN},
  author       = {He, Changchun and Huo, Xin and Gao, Hewei},
  doi          = {10.1007/s10489-022-04386-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17778-17795},
  shortjournal = {Appl. Intell.},
  title        = {FT-FVC: Fast transformation-based feature vector concatenation for time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale feature aggregation network for aspect sentiment
triplet extraction. <em>APIN</em>, <em>53</em>(14), 17762–17777. (<a
href="https://doi.org/10.1007/s10489-022-04402-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect sentiment triplet extraction (ASTE) aims to extract all aspect terms with their corresponding opinion terms and sentiment polarity simultaneously from reviews. Recent work processed the ASTE task in an end-to-end manner, which fully utilized the interactive relations among tasks and modeled the interactive relations between words. However, span-level features have not been fully explored. To this end, we propose a novel multiscale feature aggregation network (MSFAN) for end-to-end aspect sentiment triplet extraction (E2E-ASTE), which extracts multiscale local feature representations and explores the deeper interactions between aspect terms and opinion terms. We also design a simple span-awareness representation selection mechanism (SRSM) to further obtain span-level word representations. Extensive experimental results indicate that our model significantly outperforms strong baselines and achieves state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Zhu, Linan and Xu, Minhao and Zhu, Zhechao and Xu, Yifei and Kong, Xiangjie},
  doi          = {10.1007/s10489-022-04402-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17762-17777},
  shortjournal = {Appl. Intell.},
  title        = {Multiscale feature aggregation network for aspect sentiment triplet extraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving autoencoder by mutual information maximization and
shuffle attention for novelty detection. <em>APIN</em>, <em>53</em>(14),
17747–17761. (<a
href="https://doi.org/10.1007/s10489-022-04196-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under an open dynamic environment, a challenging task in object detection is to determine whether samples belong to a known class. Novelty detection can be exploited to identify classes that have not appeared in training process, that is, unknown classes. Current methods mainly adopt autoencoder (AE) to model inlier samples to generate reconstructions of specified categories and distinguish them from outlier samples by the reconstruction error. However, the AE generalizes well to construct images outside of the distribution of the training data, and it makes the model challenging to differentiate inlier samples from outlier samples. To this end, we propose a novelty detection model based on shuffle attention mechanism and mutual information maximization (MIM) to modify the effect of traditional AE on the reconstruction of inlier and outlier samples. Firstly, the rotated inlier samples are reconstructed and classified to enhance the mutual information between latent codes and inlier samples, thus constraining the representation of the latent space. Subsequently, the efficient shuffle attention mechanism is introduced to enable the model to focus more on inlier representation with negligible computation. Experimental results on four public datasets verify the potential performance of the proposed method for novelty detection.},
  archive      = {J_APIN},
  author       = {Sun, Liu and He, Ming and Wang, Nianbin and Wang, Hongbin},
  doi          = {10.1007/s10489-022-04196-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17747-17761},
  shortjournal = {Appl. Intell.},
  title        = {Improving autoencoder by mutual information maximization and shuffle attention for novelty detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum relevance minimum redundancy-based feature selection
using rough mutual information in adaptive neighborhood rough sets.
<em>APIN</em>, <em>53</em>(14), 17727–17746. (<a
href="https://doi.org/10.1007/s10489-022-04398-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection based on neighborhood rough sets (NRSs) has become a popular area of research in data mining. However, the limitation that NRSs inherently ignore the differences between different classes hinders their performance and development, and feature evaluation functions for NRSs cannot reflect the relevance between features and decisions effectively. The above two points restrict the effect of NRSs on feature selection. Consequently, a feature selection algorithm using the rough mutual information in adaptive neighborhood rough sets (ANRSs) is presented in this paper. First, we propose the boundaries of samples to granulate all samples and build ANRSs model by combining the boundaries of all samples in the same class, and the model discovers the characteristics of each class from the given data and overcomes the inherent limitation of NRSs. Second, inspired by combining information and algebraic views, we naturally extend the mutual information that represents the information view to ANRSs and combine it with the roughness that represents the algebraic view to propose the rough mutual information to quantitatively compute the relevance between features and decisions. Third, a maximum relevance minimum redundancy-based feature selection algorithm using the rough mutual information is studied. Additionally, to decrease the time consumption of the algorithm in processing high-dimensional datasets and improve the classification accuracy of the algorithm, the Fisher score dimensionality reduction method is introduced into the designed algorithm. Finally, the experimental results of the designed algorithm based on ANRSs and various algorithms based on NRSs are compared on eleven datasets to demonstrate the efficiency of our algorithm.},
  archive      = {J_APIN},
  author       = {Qu, Kanglin and Xu, Jiucheng and Han, Ziqin and Xu, Shihui},
  doi          = {10.1007/s10489-022-04398-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17727-17746},
  shortjournal = {Appl. Intell.},
  title        = {Maximum relevance minimum redundancy-based feature selection using rough mutual information in adaptive neighborhood rough sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rough sets-based tri-trade for partially labeled data.
<em>APIN</em>, <em>53</em>(14), 17708–17726. (<a
href="https://doi.org/10.1007/s10489-022-04405-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theory of rough sets is one of the most representative models for handling supervised data entangled with vagueness, impreciseness, or uncertainty. However, little work has been devoted to learning from partially labeled data using rough sets. In this study, a rough sets-based tri-trade model is proposed for partially labeled data. More specifically, a new discernibility matrix that considers both labeled and unlabeled data is first proposed, based on which a beam search-based heuristic algorithm is provided to generate multiple semi-supervised reducts. Then, a tri-trade model using three diverse semi-supervised reducts is developed, in which a data editing technique is embedded to generate reliable pseudo-labels for unlabeled data to improve the tri-trade model. Both theoretical analysis and comparative experiments on the UCI datasets show that the proposed model can effectively utilize unlabeled data to improve generalization performance and compare favorably to other representative methods.},
  archive      = {J_APIN},
  author       = {Luo, Ziming and Gao, Can and Zhou, Jie},
  doi          = {10.1007/s10489-022-04405-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17708-17726},
  shortjournal = {Appl. Intell.},
  title        = {Rough sets-based tri-trade for partially labeled data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous image anomaly detection based on contrastive
lifelong learning. <em>APIN</em>, <em>53</em>(14), 17693–17707. (<a
href="https://doi.org/10.1007/s10489-022-04401-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning techniques, an increasing number of anomaly detection methods based on deep neural networks have been proposed during the last decade. Nevertheless, these methods often suffer from catastrophic forgetting when trained on continuously arriving data samples, as deep neural networks quickly forget the knowledge obtained from previous training while adjusting to learning new information. In this work, we propose a contrastive lifelong learning model for image anomaly detection. Rather than adopting CNN-based neural networks as in other anomaly detection approaches to learn representations from training samples, we propose a contrastive learning framework for anomaly detection in which Vision Transformer (VIT) is adopted for extracting promising representations. Two nonlinear structures (projector and predictor) are integrated into our model, which is helpful in improving the performance of anomaly detection. Moreover, a lifelong learning framework that contains teacher and student networks is deployed in our model, which is able to mitigate the problem of catastrophic forgetting in image anomaly detection. By leveraging both lifelong learning and contrastive learning frameworks, our model is able to progressively perform image anomaly detection where the problem of catastrophic forgetting can be greatly mitigated. We demonstrate the effectiveness of the proposed anomaly detection method by conducting experiments on multiple image data sets.},
  archive      = {J_APIN},
  author       = {Fan, Wentao and Shangguan, Weimin and Bouguila, Nizar},
  doi          = {10.1007/s10489-022-04401-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17693-17707},
  shortjournal = {Appl. Intell.},
  title        = {Continuous image anomaly detection based on contrastive lifelong learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CN-MgMP: A multi-granularity module partition approach for
complex mechanical products based on complex network. <em>APIN</em>,
<em>53</em>(14), 17679–17692. (<a
href="https://doi.org/10.1007/s10489-022-04430-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Module partitioning is beneficial for engineers to gain a better understanding of the structure and function of complex mechanical products (CMPs). It plays an important role in the entire life cycle of the CMPs. However, owing to the large number of mechanical parts and the complex relationships, existing module partition approaches cannot obtain multi-level information. Consequently, the in-depth analysis of CMPs is hindered. Therefore, a novel multi-granularity module partition approach for CMPs based on a complex network (CN-MgMP) is proposed in this paper. Using this approach, a weighted complex network for mechanical parts (MP_WCN) was constructed. The multi-granularity module partition algorithm is presented based on MP_WCN. It consists of three phases. First, based on the concept of modularity increment, the MP_WCN is decomposed into several sub-networks, and fine-grained modules are thus obtained. Next, the topic semantic vectors of these fine-grained modules are extracted using the termfrequency–inversedocumentfrequency (TF-IDF) text analysis method. Finally, taking the global module distance (Glo-MD) as the optimization goal, the fine-grained modules are merged hierarchically to obtain optimal coarse-grained modules. The experimental results for the vertical elevator demonstrate the effectiveness and superiority of the proposed approach.},
  archive      = {J_APIN},
  author       = {Zhang, Zhenjie and Lu, Botao and Xu, Xiaobin and Shen, Xufeng and Feng, Jing and Brunauer, Georg},
  doi          = {10.1007/s10489-022-04430-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17679-17692},
  shortjournal = {Appl. Intell.},
  title        = {CN-MgMP: A multi-granularity module partition approach for complex mechanical products based on complex network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online local fisher risk minimization: A new online kernel
method for online classification. <em>APIN</em>, <em>53</em>(14),
17662–17678. (<a
href="https://doi.org/10.1007/s10489-022-04400-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new online kernel algorithm for online classification, called the online local Fisher rick minimization (OLFRM). Motivated by the Fisher criterion, OLFRM stresses the connection between data by introducing a local Fisher criterion, which is represented by two parts: the local Fisher loss function and the local Fisher regularization term. The local Fisher loss function generates a loss when the distance between heterogeneous nearest neighbors is not large enough, whereas the local Fisher regularization term works by minimizing the distance between homogeneous nearest neighbors. To reduce computational complexity and save memory resources, OLFRM is extended to two budgeted OLFRM (BOLFRM) algorithms. One uses a removal method as the budget maintenance strategy, called BOLFRM-R, and the other adopts an approximate projection method as the budget maintenance strategy, named BOLFRM-AP. For both BOLFRM-R and BOLFRM-AP, this study further designs an outlier scheme based on the unique mechanism of the proposed algorithms for selecting pending support vectors. Comprehensive experiments were conducted to compare the performance of the related algorithms on public datasets, and the results demonstrate that BOLFRMs improve in terms of robustness.},
  archive      = {J_APIN},
  author       = {Su, Changzhi and Zhang, Li and Zhao, Lei},
  doi          = {10.1007/s10489-022-04400-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17662-17678},
  shortjournal = {Appl. Intell.},
  title        = {Online local fisher risk minimization: A new online kernel method for online classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to multi-attribute predictive analysis
based on rough fuzzy sets. <em>APIN</em>, <em>53</em>(14), 17644–17661.
(<a href="https://doi.org/10.1007/s10489-022-04360-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive analysis is vital for decision management especially when involving in multiple attributes information systems. The correlation between attributes reflects that there exists a certain association between attributes and objects. How to effectively use the correlation between attributes to guide the prediction is a hot research topic in predictive analysis. In this paper, based on the designed attribute-oriented rough fuzzy set (RFS) model, a new multi-attribute predictive analysis approach is put forward for dealing with multi-attribute fuzzy information systems. This approach uses δ-fuzzy similarity classes to gather attributes with strong correlation to construct a new RFS model. Moreover, two prediction directions are constructed on the basis of the lower and upper approximations of the new RFS model. By analyzing the cosine distances between alternative and pessimistic and optimistic prediction directions, a trend predictive function is designed to forecast the development trend of the alternative. In the process of model building, trend prediction is carried out based on certain semantic information, which shows that the prediction model has certain interpretability. To evaluate the performance of the proposed multi-attribute predictive analysis model, experiments with UCI datasets are conducted for analysis and discussion. The obtained experimental results indicate that the established predictive analysis model is feasible and satisfactory.},
  archive      = {J_APIN},
  author       = {Kang, Yun and Yu, Bin and Xu, Zeshui},
  doi          = {10.1007/s10489-022-04360-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17644-17661},
  shortjournal = {Appl. Intell.},
  title        = {A novel approach to multi-attribute predictive analysis based on rough fuzzy sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Skeleton-based action recognition with multi-stream,
multi-scale dilated spatial-temporal graph convolution network.
<em>APIN</em>, <em>53</em>(14), 17629–17643. (<a
href="https://doi.org/10.1007/s10489-022-04365-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition techniques based on skeleton data are receiving more and more attention in the field of computer vision due to their ability to adapt to dynamic environments and complex backgrounds. Topologizing human skeleton data as spatial-temporal graphs and processing them using graph convolutional networks (GCNs) has been shown to produce good recognition results. However, with existing GCN methods, a fixed-size convolution kernel is often used to extract time-domain features, which may not be very suitable for multi-level model structures. Equal proportion fusion of different streams in a multi-stream network may ignore the difference in recognition ability of different streams, and these will affect the final recognition result. In this paper, we are proposing (1) a multi-scale dilated temporal graph convolution layer (MDTGCL) and (2) a multi-branch feature fusion (MFF) structure. The MDTGCL utilizes multiple convolution kernels and dilated convolution to better adapt to the multi-layer structure of the GCN model and to obtain longer periods of contextual spatial-temporal information, resulting in richer behavioural features. MFF entails weighted fusion based on the results of multi-stream outputs, and this is used to obtain the final recognition results. As higher-order skeleton data are highly discriminative and more conducive to human action recognition, we used spatial information on joints and bones and their multiple motion, as well as angle information pertaining to bones, to model together in this study. By combining the above, we designed a multi-stream, multi-scale dilated spatial-temporal graph convolutional network (2M-STGCN) model and conducted extensive experiments with two large datasets (NTU RGB+D 60 and Kinetics Skeleton 400), which showed that our model performs at SOTA level.},
  archive      = {J_APIN},
  author       = {Zhang, Haiping and Liu, Xu and Yu, Dongjin and Guan, Liming and Wang, Dongjing and Ma, Conghao and Hu, Zepeng},
  doi          = {10.1007/s10489-022-04365-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17629-17643},
  shortjournal = {Appl. Intell.},
  title        = {Skeleton-based action recognition with multi-stream, multi-scale dilated spatial-temporal graph convolution network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weighted-link graph neural network for lung cancer
knowledge classification. <em>APIN</em>, <em>53</em>(14), 17610–17628.
(<a href="https://doi.org/10.1007/s10489-022-04437-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualized knowledge representation can more effectively help the public gain knowledge about lung cancer prevention, diagnosis, treatment, and subsequent life. Therefore, this study collected articles on lung cancer from the well-known Web of Science database to analyze lung cancer literature, and the text data were published between 2016 and 2021. First, we used natural language processing to handle the collected text data, and then we used the latent Dirichlet allocation method to perform topic modeling and obtain the optimal topic numbers based on two coherence metrics for assigning the class of every article. Next, a PMI_2 weighted was proposed to build an initial weighted knowledge graph, and four graph neural network algorithms were used to train the initial weighted knowledge graph. In addition, we proposed a PMI_2 + link to improve the classification performance, and the additional links were obtained from the graph auto-encoder and graph convolutional network training. When the best classification performance has been obtained, these edge weights have a representative. For visualized knowledge representation, we used the Neo4j tool to display the nodes and edge weights for the final literature knowledge. The results show that the use of the proposed PMI_2 + link to build a weighted graph has a better classification performance. Further, the proposed PMI_2 + link can effectively reduce the number of edges on the knowledge graphs and avoid insufficient GPU memory.},
  archive      = {J_APIN},
  author       = {Cheng, Ching-Hsue and Ji, Zheng-Ting},
  doi          = {10.1007/s10489-022-04437-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17610-17628},
  shortjournal = {Appl. Intell.},
  title        = {A weighted-link graph neural network for lung cancer knowledge classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locality-constrained continuous place recognition for SLAM
in extreme conditions. <em>APIN</em>, <em>53</em>(14), 17593–17609. (<a
href="https://doi.org/10.1007/s10489-022-04415-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) in extreme lighting conditions and weather conditions is a challenging problem due to a lack of features that actively cause the vehicle to drift. The localization is not possible on maps produced by the ideal continuous as features in different domains rarely match. Place recognition techniques have achieved improved results by using known poses from ideal domains; however, they are generally effective only in regions of high distinctiveness. In this paper we first solve the place recognition problem by comparing a pair of sequences of images taken by the robot in ideal and extreme domains with the constraint that every place predicted can only be from the neighboring places of the previous prediction. A neural network is used to get cross-domain similarity measures between images. A binning technique is used to discretize the continuous poses into discrete places, where bins are large enough to maintain distinctiveness and small enough to have an acceptable loss by discretization. Furthermore, we employ a global landmark search in case the confidence score of the model gets under a threshold i.e. the robot loses its way. The outputs are passed through a Particle filter to give a continuous trajectory and the final trajectories are observed to be more stable. The results show that the proposed technique beats the state-of-the-art Visual Odometry (VO) libraries and place recognition libraries.},
  archive      = {J_APIN},
  author       = {Yadav, Rohit and Pani, Vishal and Mishra, Arpit and Tiwari, Naman and Kala, Rahul},
  doi          = {10.1007/s10489-022-04415-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17593-17609},
  shortjournal = {Appl. Intell.},
  title        = {Locality-constrained continuous place recognition for SLAM in extreme conditions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approach for combining MULTIMOORA method and muirhead
mean operators based on the complex pythagorean fuzzy uncertain
linguistic representation model. <em>APIN</em>, <em>53</em>(14),
17561–17592. (<a
href="https://doi.org/10.1007/s10489-022-04408-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MULTIMOORA is a relatively new multi-attribute decision-making (MADM) technique containing three major parts. It has partial and complete aggregation information. The MULTIMOORA procedure contains three major parts: the full multiplicative shape, ratio system, and reference point. Furthermore, the Muirhead mean (MM) operator can easily capture the interaction between any number of preferences. The main feature of this model is to evaluate the novel theory of complex Pythagorean fuzzy uncertain linguistic (CPFUL) settings and utilize their useful properties (“algebraic laws, score value, and accuracy value”). Additionally, we combined the main theory of MM operators with information taken from the theory of CPFUL to diagnose the major idea of the CPFUL Muirhead mean (CPFULMM), CPFUL weighted Muirhead mean (CPFULWMM), CPFUL dual Muirhead mean (CPFULDMM), and CPFUL dual weighted Muirhead mean (CPFULDWMM) operators. We then evaluated their valuable properties (“idempotency, Boundedness, and monotonicity”). Moreover, to enhance the worth and feasibility of the diagnosed approach, we elaborated the extension of the MULTIMOORA method under CPFUL information. We then diagnosed a procedure of the MADM scenario whilst considering the evaluated operators using the CPFUL set theory. Lastly, to compare the diagnosed information with some old operators, we used numerical examples to illustrate the flexibility and feasibility of the presented information.},
  archive      = {J_APIN},
  author       = {Qi, Xiaoming and Ali, Zeeshan and Mahmood, Tahir and Liu, Peide},
  doi          = {10.1007/s10489-022-04408-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17561-17592},
  shortjournal = {Appl. Intell.},
  title        = {An approach for combining MULTIMOORA method and muirhead mean operators based on the complex pythagorean fuzzy uncertain linguistic representation model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swin transformer-based supervised hashing. <em>APIN</em>,
<em>53</em>(14), 17548–17560. (<a
href="https://doi.org/10.1007/s10489-022-04410-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the modern internet, image data are growing explosively. How to retrieve specific images from such big data has become an urgent problem. The common solution is the hash-based approximate nearest neighbor retrieval method, which uses compact binary hash codes to represent the original image data. When calculating the image similarity, it can quickly retrieve similar images by bit operation and requires only a small memory space to store hash codes. In recent years, the combination of deep learning and hash learning has led to breakthroughs in hash-based image retrieval methods. In particular, convolutional neural networks (CNNs) are widely used in various deep hashing methods. However, CNNs cannot capture global image information well when extracting image features, which affects the quality of the hash codes. Therefore, we first introduce the Swin Transformer network into hash learning and propose Swin Transformer-based supervised hashing (SWTH). Using the Swin Transformer as the feature extraction backbone network, we can capture the global context information of an image as much as possible by establishing the relations among different blocks of the image. Furthermore, the Swin Transformer adopts a hierarchical structure of layer-by-layer downsampling, which can obtain rich multiscale feature information while extracting global information. After the feature extraction network, we add a hash layer for hash learning. The image feature representation and hash function can be learned by optimizing the combination of hash loss, classification loss and quantization loss. Extensive experimental results show that the SWTH method outperforms many state-of-the-art methods and achieves excellent retrieval performance.},
  archive      = {J_APIN},
  author       = {Peng, Liangkang and Qian, Jiangbo and Wang, Chong and Liu, Baisong and Dong, Yihong},
  doi          = {10.1007/s10489-022-04410-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17548-17560},
  shortjournal = {Appl. Intell.},
  title        = {Swin transformer-based supervised hashing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A non-convex economic load dispatch problem using chameleon
swarm algorithm with roulette wheel and levy flight methods.
<em>APIN</em>, <em>53</em>(14), 17508–17547. (<a
href="https://doi.org/10.1007/s10489-022-04363-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Enhanced Chameleon Swarm Algorithm (ECSA) by integrating roulette wheel selection and Lé vy flight methods is presented to solve non-convex Economic Load Dispatch (ELD) problems. CSA has diverse strategies to move towards the optimal solution. Even so, this algorithm’s performance faces some hurdles, such as early convergence and slumping into local optimum. In this paper, several enhancements were made to this algorithm. First, it’s position updating process was slightly tweaked and took advantage of the chameleons’ randomization as well as adopting several time-varying functions. Second, the Lévy flight operator is integrated with roulette wheel selection method and both are combined with ECSA to augment the exploration behavior and lessen its bias towards exploitation. Finally, an add-on position updating strategy is proposed to develop a further balance between exploration and exploitation conducts. The optimization performance of ECSA is shown by testing it on five various real ELD cases with a generator having 3, 13, 40, 80 and 140 units, each with different constraints. The results of the ELD systems’ analysis depict that ECSA is better than the parent CSA and other state-of-the art methods. Further, the efficacy of ECSA was experimented on several benchmark test functions, and its performance was compared to other well-known optimization methods. Experimental results show that ECSA surpasses other methods on complex benchmark functions with modest computational burdens. The superiority and practicality of ECSA is demonstrated by getting new best solutions for large-scale ELD cases such as 40-unit and 140-unit test systems.},
  archive      = {J_APIN},
  author       = {Braik, Malik Sh. and Awadallah, Mohammed A. and Al-Betar, Mohammed Azmi and Hammouri, Abdelaziz I. and Zitar, Raed Abu},
  doi          = {10.1007/s10489-022-04363-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17508-17547},
  shortjournal = {Appl. Intell.},
  title        = {A non-convex economic load dispatch problem using chameleon swarm algorithm with roulette wheel and levy flight methods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese conditional generative adversarial network for
multi-focus image fusion. <em>APIN</em>, <em>53</em>(14), 17492–17507.
(<a href="https://doi.org/10.1007/s10489-022-04406-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) combines information by utilizing various image sequences of the same scenes at different of focus depths. The available MFIF method based on generative adversarial networks (GAN) lacks the feature complementarity of multi-focus images, resulting in the loss of details and noises in the generated decision maps. To resolve this problem, the learning framework of a joint distribution was developed via Siamese conditional generative adversarial network (SCGAN). This framework utilizes Siamese conditional generator that produces two-probabilistic feature maps from multi-focus images with complementary information. Additionally, the proposed framework also considers both the diversity of datasets and network convergence. The structural sparse objective function is designed to penalize the prediction of low confidence by sparse calculation of the rows and columns of the matrix. So, it endows a better Dice coefficient with higher values and improves the generalization capability of the GAN. Also, Wasserstein Divergence (DIV) is utilized to optimize the discrimination performance with stable training. In both quantitative and qualitative experiments, SCGAN has better scores on MFIF than other methods.},
  archive      = {J_APIN},
  author       = {Li, Huaguang and Qian, Wenhua and Nie, Rencan and Cao, Jinde and Xu, Dan},
  doi          = {10.1007/s10489-022-04406-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17492-17507},
  shortjournal = {Appl. Intell.},
  title        = {Siamese conditional generative adversarial network for multi-focus image fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Belief entropy rate: A method to measure the uncertainty of
interval-valued stochastic processes. <em>APIN</em>, <em>53</em>(14),
17476–17491. (<a
href="https://doi.org/10.1007/s10489-022-04407-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy rate, as an effective tool in information theory, can measure the uncertainty of stochastic processes modeled by probability mass function. However, when the stochastic process to be measured cannot be accurately modeled, i.e., it is more sense to describe the phenomenon with an interval, the stochastic process needs a more general method to represent. In this paper, the interval-valued stochastic process is modeled with the basic belief assignment in an ordered frame of discernment and the corresponding belief entropy rate is proposed to measure its uncertainty. Two common stochastic processes are discussed. The first is the case of independent identically distributed stochastic processes, where the belief entropy rate is formally the same as the Shannon entropy rate. The second is Markov processes. We construct the evidential Markov chain and calculate its belief entropy rate. Compared with the Shannon entropy rate, the belief entropy rate is easier to implement the Markov chains. By validating in real dataset, the proposed method can better deal with interval information with stronger practicability. When encountering tiny disturbances, the variance of the Shannon entropy rate is more than 50 times the variance of the belief entropy rate, which reflects the stronger robustness of the belief entropy rate.},
  archive      = {J_APIN},
  author       = {Wang, Zhiyuan and Zhou, Qianli and Deng, Yong},
  doi          = {10.1007/s10489-022-04407-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17476-17491},
  shortjournal = {Appl. Intell.},
  title        = {Belief entropy rate: A method to measure the uncertainty of interval-valued stochastic processes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive optimal safety tracking control for multiplayer
mixed zero-sum games of continuous-time systems. <em>APIN</em>,
<em>53</em>(14), 17460–17475. (<a
href="https://doi.org/10.1007/s10489-022-04348-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the equipment is working, it is very important to avoid the occurrence of malignant accidents by providing a highly reliable safety protection means. In this paper, for multiplayer mixed zero-sum games, an optimal safety tracking control scheme based on adaptive dynamic programming (ADP) is proposed, and a control barrier function (CBF) is introduced into the value function of the system to ensure that the system operates within its safe region. Firstly, through system transformation, the original tracking problem is transformed into a state tracking error problem. Secondly, an augmented Hamilton-Jacobi-Bellman (HJB) equation is derived from the improved augmented error system and the value function. Different from traditional methods, this paper uses a single critic neural network (NN) instead of the actor-critic NN to approximate the Nash equilibrium solution of the system, and introduces a concurrent learning technique that can relax the traditional continuous excitation condition into a simplified condition of recording data. Then, according to the Lyapunov theory, the stability of the system is analyzed in detail. Finally, two simulation examples are used to verify the effectiveness of the proposed scheme.},
  archive      = {J_APIN},
  author       = {Qin, Chunbin and Zhang, Zhongwei and Shang, Ziyang and Zhang, Jishi and Zhang, Dehua},
  doi          = {10.1007/s10489-022-04348-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17460-17475},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive optimal safety tracking control for multiplayer mixed zero-sum games of continuous-time systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminant space metric network for few-shot image
classification. <em>APIN</em>, <em>53</em>(14), 17444–17459. (<a
href="https://doi.org/10.1007/s10489-022-04413-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based few-shot learning has gained considerable attention for simply and effectively addressing the few-shot classification problem. However, a huge number of the existing approaches focus only on the similarity or distance between features of the instances in the embedding space, neglecting the geometric structure of the samples. To remedy this, we propose a novel approach referred to as the discriminant space metric network (DSMNet) for few-shot image classification problem. DSMNet exploits the geometric structure of the samples within each episode to enhance the discriminative ability of the embedding space. Specifically, DSMNet aims to increase the distance between features belonging to different classes while making those from the same class more compact by maximizing the between-class scatter and minimizing the within-class scatter in the embedding space. Moreover, we developed a novel adaptation strategy for improving the model’s generalizing capability. Extensive experiments are conducted on four few-shot classification benchmark datasets to demonstrate the proposed DSMNet. We also performed several ablation studies to analyze its performance. The superiority of DSMNet over existing networks is indicated by the experimental results.},
  archive      = {J_APIN},
  author       = {Yan, Leilei and Li, Fanzhang and Zhang, Li and Zheng, Xiaohan},
  doi          = {10.1007/s10489-022-04413-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17444-17459},
  shortjournal = {Appl. Intell.},
  title        = {Discriminant space metric network for few-shot image classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularization-based pruning of irrelevant weights in deep
neural architectures. <em>APIN</em>, <em>53</em>(14), 17429–17443. (<a
href="https://doi.org/10.1007/s10489-022-04353-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks exploiting million parameters are currently the norm. This is a potential issue because of the great number of computations needed for training, and the possible loss of generalization performance of overparameterized networks. We propose in this paper a method for learning sparse neural topologies via a regularization approach that identifies nonrelevant weights in any type of layer (i.e., convolutional, fully connected, attention and embedding ones) and selectively shrinks their norm while performing a standard back-propagation update for relevant layers. This technique, which is an improvement of classical weight decay, is based on the definition of a regularization term that can be added to any loss function regardless of its form, resulting in a unified general framework exploitable in many different contexts. The actual elimination of parameters identified as irrelevant is handled by an iterative pruning algorithm. To explore the possibility of an interdisciplinary use of our proposed technique, we test it on six different image classification and natural language generation tasks, among which four are based on real datasets. We reach state-of-the-art performance in one out of four imaging tasks while obtaining results better than competitors for the others and one out of two of the considered language generation tasks, both in terms of compression and metrics.},
  archive      = {J_APIN},
  author       = {Bonetta, Giovanni and Ribero, Matteo and Cancelliere, Rossella},
  doi          = {10.1007/s10489-022-04353-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17429-17443},
  shortjournal = {Appl. Intell.},
  title        = {Regularization-based pruning of irrelevant weights in deep neural architectures},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting drug-drug adverse reactions via multi-view graph
contrastive representation model. <em>APIN</em>, <em>53</em>(14),
17411–17428. (<a
href="https://doi.org/10.1007/s10489-022-04372-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug-drug adverse reactions (DDADRs) is an important task because many patients inevitably take multiple medicines to pursue sound therapeutic results. However, predicting DDADRs is an extremely challenging task. Graph representation is a popular learning method which can simultaneously learn the attribute information of nodes and graph structural information. In this paper, we propose DMVDGI, a novel self-supervised multi-view graph learning framework for predicting DDADRs. Specifically, we first describe drug features with multi-view data, which makes the drug feature representation more comprehensive due to containing multiple biomedical information. Then, we depict the drug-drug interactions (DDIs) with the signed network, which can clearly express the positive and negative relationships between drugs. The DDI signed network makes the drug feature representations imply rich semantic information. Finally, we train our model using contrastive learning, utilizing the mutual information between local-global representations to optimize model parameters. Besides, we conduct extensive experiments to verify the effectiveness of our model. The results show that our model is superior to baseline methods, and the best performance of AUROC outperforms the baseline model by 8%. More encouragingly, our DMVDGI model is superior to some supervised baseline methods on several benchmarks. To the best of our knowledge, the DMVDGI model is the first self-supervised multi-view graph model for predicting DDADRs.},
  archive      = {J_APIN},
  author       = {Zhuang, Luhe and Wang, Hong and Hua, Meifang and Li, Wei and Zhang, Hui},
  doi          = {10.1007/s10489-022-04372-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17411-17428},
  shortjournal = {Appl. Intell.},
  title        = {Predicting drug-drug adverse reactions via multi-view graph contrastive representation model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pairwise open-sourced dataSet protection based on adaptive
blind watermarking. <em>APIN</em>, <em>53</em>(14), 17391–17410. (<a
href="https://doi.org/10.1007/s10489-022-04416-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of collecting and labeling open-sourced datasets which promote the development of deep learning is expensive. Thus, it is important to design an efficient open-sourced data set protection algorithm to detect the open-sourced data sets used in illegal ways. In this paper, a protection algorithm based on adaptive robust blind-watermark is proposed suitable for multiple paired open-sourced datasets, and the evaluation criteria of the algorithm are defined. Specifically, in the embed stage, the highly concealed of the watermark is realized by combining UNet and double GAN to take into account the local and global features of the carrier and the watermark image. A preprocess network is used in the Embedding network to adapt different watermark size. In the extraction part, a modified feature sharing UNet with GAN is used to ensure robustness of the extraction network. Paired datasets are used for training to ensure accurate extraction of watermarks. After the target model is trained using the watermarked dataset, its inference output will contain watermark information. When it is believed that a suspicious model is illegally trained with the dataset, it can be verified by the watermark extracted from inference output of the suspicious model. We evaluate our method on three target models including nine datasets. The results show that our framework successfully verifies the dataset used illegally and without a noticeable impact on the target model task when training with the watermark dataset.},
  archive      = {J_APIN},
  author       = {Pang, Zilong and Wang, Mingxu and Cao, Lvchen and Chai, Xiuli and Gan, Zhihua},
  doi          = {10.1007/s10489-022-04416-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17391-17410},
  shortjournal = {Appl. Intell.},
  title        = {Pairwise open-sourced dataSet protection based on adaptive blind watermarking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced context-aware citation recommendation with
auxiliary textual information based on an auto-encoding mechanism.
<em>APIN</em>, <em>53</em>(14), 17381–17390. (<a
href="https://doi.org/10.1007/s10489-022-04423-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of retrieving suitable papers which are related to an interesting topic while doing research normally takes a lot of time and effort. Citation recommendation is frequently used to solve this problem by automatically suggesting a list of candidate papers that should match with the user’s references or topics of interest. Applying the recent research results of deep learning in multiple disciplines, the performance of citation recommendation systems has been significantly improved with the facilitation of powerful deep neural data analysis and representation learning techniques. However, most of the recent Neural Citation Network (NCN) model-based techniques still encounter limitations related to the capability of integrating auxiliary information to assist the citation contextual learning process. Thus, in this paper, we propose a novel context-aware NCN-based model with the extra textual data integration and Bidirectional Encoder Representations from Transformers (BERT) model to improve the performance of the citation recommendation task. To do this, an extensive deep neural auto-encoding mechanism with a self-attention-based mechanism is utilized in our proposed model to flexibly learn both associated textual and citation contextual data in the given dataset. These enriched citation contextual information representations are then utilized to improve the performance of the citation recommendation task as the end-to-end neural learning process. Extensive experiments in the standard arXiv dataset show the effectiveness and good performance of the proposed model.},
  archive      = {J_APIN},
  author       = {Dinh, Thi N. and Pham, Phu and Nguyen, Giang L. and Vo, Bay},
  doi          = {10.1007/s10489-022-04423-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17381-17390},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced context-aware citation recommendation with auxiliary textual information based on an auto-encoding mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual transformer sequence-based recognition network
for medical examination reports. <em>APIN</em>, <em>53</em>(14),
17363–17380. (<a
href="https://doi.org/10.1007/s10489-022-04420-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic recognition of the medical examination report table (MERT) is receiving increasing attention in recent years as it is an essential step for intelligent healthcare and medical treatment. However, there are still some challenges in the table prediction when it is applied practically. In this paper, a recognition network (CoT_SRN) for medical examination reports is proposed to improve the recognition accuracy of MERT structure and reconstruct the image into a spreadsheet. The network is based on contextual transformer sequence and consists of CoT encoder and SRN decoder. In the encoder, the CNN backbone is constructed to extract the MERT image structure features based on the Contextual Transformer (CoT) proposed in this paper. In the decoder, an attention head with gated recurrent unit (GRU) was used for feature sequence recognition to obtain the cell location and table structure represented by a structured language. In addition, MERT structure labels are defined as character-level HTML formats, which are added in the training of the table structure recognition. The proposed method can achieve competitive tree-edit-distance-based similarity (TEDS) scores on the English datasets, such as PubTabNet and SciTSR, and Chinese datasets, such as the Chinese medical document dataset (CMDD). It demonstrates that the Cot_SRN is helpful to preserve the good performance across multi-language MERT structure recognition. Additionally, the performance of the proposed method is verified on the practical examples with folds and small angle deflection. The experimental results show that the proposed method is promising in practical application.},
  archive      = {J_APIN},
  author       = {Wan, Honglin and Zhong, Zongfeng and Li, Tianping and Zhang, Huaxiang and Sun, Jiande},
  doi          = {10.1007/s10489-022-04420-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17363-17380},
  shortjournal = {Appl. Intell.},
  title        = {Contextual transformer sequence-based recognition network for medical examination reports},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint network of non-linear graph attention and temporal
attraction force for geo-sensory time series prediction. <em>APIN</em>,
<em>53</em>(14), 17346–17362. (<a
href="https://doi.org/10.1007/s10489-022-04412-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-sensory time series, such as the air quality and water distribution, are collected from numerous sensors at different geospatial locations in the same time interval. Each sensor monitors multiple parameters and generates multivariate time series. These time series change over time and vary geographically; hence, geo-sensory time series contain multi-scale spatial-temporal correlations, namely inter-sensor spatial-temporal correlations and intra-sensor spatial-temporal correlations. To capture spatial-temporal correlations, although various deep learning models have been developed, few of the models focus on capturing both correlations. To solve this problem, we propose simultaneously capture the inter- and intra-sensor spatial-temporal correlations by designing a joint network of non-linear graph attention and temporal attraction force(J-NGT) consisting two graph attention mechanisms. The non-linear graph attention mechanism can characterize node affinities for adaptively selecting the relevant exogenous series and relevant sensor series. The temporal attraction force mechanism can weigh the effect of past values on current values to represent the temporal correlation. To prove the superiority and effectiveness of our model, we evaluate our model in three real-world datasets from different fields. Experimental results show that our model can achieve better prediction performance than eight state-of-the-art models, including statistical models, machine learning models, and deep learning models. Furthermore, we conducted experiments to capture inter- and intra-sensor spatial-temporal correlations. Experimental results indicate that our model significantly improves performance by capturing both inter- and intra-sensor spatial-temporal correlations. This fully shows that our model has a greater advantage in geo-sensory time series prediction.},
  archive      = {J_APIN},
  author       = {Dong, Hongbin and Han, Shuang and Pang, Jinwei and Yu, Xiaodong},
  doi          = {10.1007/s10489-022-04412-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17346-17362},
  shortjournal = {Appl. Intell.},
  title        = {A joint network of non-linear graph attention and temporal attraction force for geo-sensory time series prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG emotion recognition based on PLV-rich-club dynamic brain
function network. <em>APIN</em>, <em>53</em>(14), 17327–17345. (<a
href="https://doi.org/10.1007/s10489-022-04366-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During emotional changes, the brain generates many highly connected and highly concentrated hub regions. Thus, barely studying the whole-brain network architecture while ignoring the connection information between small-scale structures, will not be able to accurately differentiate the information interaction patterns of the brain under different emotions. The connections between high-degree nodes in the brain are stronger than those between low-degree nodes. The rich-club structure composed by those high-degree nodes is a hot topic in the current research of emotion recognition. Therefore, this paper investigates the dynamics of the rich-club structure generated by high-degree nodes in different time periods during different emotional changes. Firstly, the dynamic PLV brain network is constructed using non-overlapping time windows. Afterwards, the rich-club coefficients in the brain network are calculated and the nodes with higher degrees are selected as rich-club nodes within the coefficient range. Furthermore, the ReliefF algorithm is used to filter the frequency domain features of rich-club nodes and PLV-rich-club graph-theoretical features to derive the most emotionally relevant features for emotion recognition. The experimental results show that the composition of the rich-club is roughly the same but there are subtle differences over time. The smaller the valence, the larger the K-degree range, in which the structure of the rich-club is more stable, and the information interaction is more complex. The larger the valence, in which the information contained by the rich-club structure is more modular, and its ability to transmit information is stronger. It is revealed that the key to distinguishing different emotions is the brain information represented by the connections between small-scale structures in complex brain networks. The LOSO validation method was used for verification on the DEAP and SEED datasets, and the features based on the rich-club structure achieved 86.11% and 87.92% accuracy in the valence dimension, respectively.},
  archive      = {J_APIN},
  author       = {Wang, Zhong-Min and Chen, Zhe-Yu and Zhang, Jie},
  doi          = {10.1007/s10489-022-04366-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17327-17345},
  shortjournal = {Appl. Intell.},
  title        = {EEG emotion recognition based on PLV-rich-club dynamic brain function network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bio-inspired robot swarm path formation with local sensor
scope. <em>APIN</em>, <em>53</em>(14), 17310–17326. (<a
href="https://doi.org/10.1007/s10489-022-04356-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating a robot network to connect the targets while exploring an unknown map is the goal of the path formation issue. It is difficult to maximize the efficiency of exploring a multi-target maze by a robot swarm with limited sensor capabilities. In this study, a novel behavior-based path formation approach (BPFM) that incorporates an artificial potential field and a bio-model inspired by slime mold is proposed for this problem. The robot’s controller can operate more sensibly while using the heuristic term from particle swarm. In order to maintain a dynamic multi-source network, a series of mechanisms and transition rules have been designed for the multi-target maze. Grid maps with obstacle density from sparse to dense are utilized in simulations to compare the proposed method with other algorithms. The results indicate that the performance of collective exploration, which is examined in diverse circumstances, is unexpectedly efficient and robust.},
  archive      = {J_APIN},
  author       = {Zhao, Yuhang and Qu, Zhenshen and Liu, Haichao and Zhu, Runwen},
  doi          = {10.1007/s10489-022-04356-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17310-17326},
  shortjournal = {Appl. Intell.},
  title        = {Bio-inspired robot swarm path formation with local sensor scope},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAS-GNN: Denoising autoencoder integrated with
self-supervised learning in graph neural network-based recommendations.
<em>APIN</em>, <em>53</em>(14), 17292–17309. (<a
href="https://doi.org/10.1007/s10489-022-04399-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the recommendation performance, session-based recommendations typically model based on graph neural networks (GNN). These models use the most recently clicked item as the user’s short-term interest, as well as the query vector in the attention mechanism. Based on it, the attention score is calculated with the remaining items to obtain the user’s long-term interest. However, the obtained representation of long-term interest is one-sided. Furthermore, unlike other recommendation technology, such as collaborative filtering that includes the user’s entire history information, the session-based recommendation is more vulnerable to data sparsity. Existing models primarily make predictions based on observable user-item interactions and ignore items not interacted with by users. To address the aforementioned issues, we propose the denoising autoencoder integrated with self-supervised learning (SSL) in graph neural networks (DAS-GNN). In DAS-GNN, the query extraction module based on denoising autoencoder can mine multiple user interests and assist long-term interest to express user needs more comprehensively. We propose an effective way of dividing positive and negative samples in the SSL module and use adaptive thresholds to mine negative hard samples, thereby improving training efficiency and alleviating data sparsity. Extensive experiments demonstrate that the proposed DAS-GNN outperforms state-of-the-art models on four benchmarks. The source code is available at: https://github.com/daijiuqian/DAS-GNN .},
  archive      = {J_APIN},
  author       = {Dai, Jiuqian and Yuan, Weihua and Bao, Chen and Zhang, Zhijun},
  doi          = {10.1007/s10489-022-04399-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17292-17309},
  shortjournal = {Appl. Intell.},
  title        = {DAS-GNN: Denoising autoencoder integrated with self-supervised learning in graph neural network-based recommendations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A many-objective evolutionary algorithm with adaptive
convergence calculation. <em>APIN</em>, <em>53</em>(14), 17260–17291.
(<a href="https://doi.org/10.1007/s10489-022-04296-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since different reference points are crucial for calculating convergence, we design a many-objective evolutionary algorithm with an adaptive convergence calculation method (ACC-MaOEA). This algorithm uses the adaptive convergence calculation method to estimate the shape of the Pareto front (PF) and adaptively determines a reference point to calculate convergence based on the shape. It estimates the PF shape by comparing the distances from the ideal and key points to two parallel planes. If the PF is concave, the ideal point is used as the reference point, and the distance from the solution to a plane through the ideal point is calculated to approximate convergence; if the PF is convex, the nadir point is used as the reference point, and the distance from the solution to a plane through the nadir point is calculated to approximate convergence. To avoid the overestimation of the nadir point, we first adopt a ratio-based infinite norm indicator to determine a potential region in which the optimal solution exists and then estimate the PF shape in this region and adaptively calculate convergence. Additionally, we use a determinantal point process to sample solutions with good convergence and diversity. We compare ACC-MaOEA with state-of-the-art algorithms on 21 test problems and up to 15 objectives. The experimental results show that ACC-MaOEA significantly outperforms its competitors, especially on regular PF problems.},
  archive      = {J_APIN},
  author       = {Wang, Mengzhen and Ge, Fangzhen and Chen, Debao and Liu, Huaiyu},
  doi          = {10.1007/s10489-022-04296-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17260-17291},
  shortjournal = {Appl. Intell.},
  title        = {A many-objective evolutionary algorithm with adaptive convergence calculation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised image-to-image translation via long-short
cycle-consistent adversarial networks. <em>APIN</em>, <em>53</em>(14),
17243–17259. (<a
href="https://doi.org/10.1007/s10489-022-04389-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cycle consistency conducts generative adversarial networks from aligned image pairs to unpaired training sets and can be applied to various image-to-image translations. However, the accumulation of errors that may occur during image reconstruction can affect the realism and quality of the generated images. To address this, we exploit a novel long and short cycle-consistent loss. This new loss is simple and easy to implement. Our dual-cycle constrained cross-domain image-to-image translation method can handle error accumulation and enforce adversarial learning. When image information is migrated from one domain to another, the cycle consistency-based image reconstruction constraint should be constrained in both short and long cycles to eliminate error accumulation. We adopt the cascading manner with dual-cycle consistency, where the reconstructed image in the first cycle can be cast as the new input to the next cycle. We show a distinct improvement over baseline approaches in most translation scenarios. With extensive experiments on several datasets, the proposed method is superior to several tested approaches.},
  archive      = {J_APIN},
  author       = {Wang, Gang and Shi, Haibo and Chen, Yufei and Wu, Bin},
  doi          = {10.1007/s10489-022-04389-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {14},
  pages        = {17243-17259},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised image-to-image translation via long-short cycle-consistent adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Personality-based and trust-aware products
recommendation in social networks. <em>APIN</em>, <em>53</em>(13),
17241. (<a href="https://doi.org/10.1007/s10489-022-04204-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Vatani, Nasim and Rahmani, Amir Masoud and Javadi, Hamid Haj Seyyed},
  doi          = {10.1007/s10489-022-04204-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17241},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Personality-based and trust-aware products recommendation in social networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correction to: Embedded mutual learning: A novel online
distillation method integrating diverse knowledge sources.
<em>APIN</em>, <em>53</em>(13), 17240. (<a
href="https://doi.org/10.1007/s10489-022-04236-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Li, Chuanxiu and Li, Guangli and Zhang, Hongbin and Ji, Donghong},
  doi          = {10.1007/s10489-022-04236-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17240},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: embedded mutual learning: a novel online distillation method integrating diverse knowledge sources},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HSJ-solver: A new method based on GHD for answering
conjunctive queries and solving constraint satisfaction problems.
<em>APIN</em>, <em>53</em>(13), 17226–17239. (<a
href="https://doi.org/10.1007/s10489-022-04361-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating conjunctive queries (CQs) is NP-hard in general; however, acyclic CQs or nearest acyclic CQs can be evaluated in polynomial time. Many structural methods for characterising such classes are proposed in the literature. However, exploiting these methods to answer CQs is not efficient in practice when the relations have a realistic size. In this work, we propose a new method called HSJ-Solver for evaluating CQs and for solving constraint satisfaction problems (CSPs) represented by a generalised hypertree decomposition (GHD). Experiments carried out on CSP benchmarks show that using HSJ-Solver significantly improves the achieved performance.},
  archive      = {J_APIN},
  author       = {Younsi, Zineb and Amroun, Kamal and Bouarab-Dahmani, Farida and Bennai, Sofia},
  doi          = {10.1007/s10489-022-04361-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17226-17239},
  shortjournal = {Appl. Intell.},
  title        = {HSJ-solver: A new method based on GHD for answering conjunctive queries and solving constraint satisfaction problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TSF-transformer: A time series forecasting model for exhaust
gas emission using transformer. <em>APIN</em>, <em>53</em>(13),
17211–17225. (<a
href="https://doi.org/10.1007/s10489-022-04326-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring and prediction of exhaust gas emissions for heavy trucks is a promising way to solve environmental problems. However, the emission data acquisition is time delayed and the pattern of emission is usually irregular, which makes it very difficult to accurately predict the emission state. To deal with these problems, in this paper, we interpret emission prediction as a time series prediction problem and explore a deep learning model, a time-series forecasting Transformer (TSF-Transformer) for exhaust gas emission prediction. The exhaust emission of the heavy truck is not directly predicted, but indirectly predicted by predicting the temperature and pressure changes of the exhaust pipe under the working state of the truck. The basis of our research is based on real-time data feeds from temperature and pressure sensors installed on the exhaust pipe of approximately 12,000 heavy trucks. Therefore, the task of time series forecasting consists of two key stages: monitoring and prediction. The former utilizes the server to receive the data sent by the sensors in real-time, and the latter uses these data as samples for network training and testing. The training of the network throughout the prediction process is done in an unsupervised manner. Also, to visualize the forecast results, we weight the forecast data with the truck trajectories and present them as heatmaps. To the best of our knowledge, this is the first case of using the Transformer as the core component of the prediction model to complete the task of exhaust emissions prediction from heavy trucks. Experiments show that the prediction model outperforms other state-of-the-art methods in prediction accuracy.},
  archive      = {J_APIN},
  author       = {Li, Zhenyu and Zhang, Xikun and Dong, Zhenbiao},
  doi          = {10.1007/s10489-022-04326-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17211-17225},
  shortjournal = {Appl. Intell.},
  title        = {TSF-transformer: A time series forecasting model for exhaust gas emission using transformer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling opponent learning in multiagent repeated games.
<em>APIN</em>, <em>53</em>(13), 17194–17210. (<a
href="https://doi.org/10.1007/s10489-022-04249-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (MARL) has been used extensively in the game environment. One of the main challenges in MARL is that the environment of the agent system is dynamic, and the other agents are also updating their strategies. Therefore, modeling the opponents’ learning process and adopting specific strategies to shape learning is an effective way to obtain better training results. Previous studies such as DRON, LOLA and SOS approximated the opponent’s learning process and gave effective applications. However, these studies modeled only transient changes in opponent strategies and lacked stability in the improvement of equilibrium efficiency. In this article, we design the MOL (modeling opponent learning) method based on the Stackelberg game. We use best response theory to approximate the opponents’ preferences for different actions and explore stable equilibrium with higher rewards. We find that MOL achieves better results in several games with classical structures (the Prisoner’s Dilemma, Stackelberg Leader game and Stag Hunt with 3 players), and in randomly generated bimatrix games. MOL performs well in competitive games played against different opponents and converges to stable points that score above the Nash equilibrium in repeated game environments. The results may provide a reference for the definition of equilibrium in multiagent reinforcement learning systems, and contribute to the design of learning objectives in MARL to avoid local disadvantageous equilibrium and improve general efficiency.},
  archive      = {J_APIN},
  author       = {Hu, Yudong and Han, Congying and Li, Haoran and Guo, Tiande},
  doi          = {10.1007/s10489-022-04249-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17194-17210},
  shortjournal = {Appl. Intell.},
  title        = {Modeling opponent learning in multiagent repeated games},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight 2-d CNN model with dual attention mechanism
for heartbeat classification. <em>APIN</em>, <em>53</em>(13),
17178–17193. (<a
href="https://doi.org/10.1007/s10489-022-04303-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of heartbeats is important for the diagnosis of arrhythmia, and more and more attention has been paid to deep learning methods to avoid manual feature design and extraction. Many methods are proposed based on the two-dimensional image representation of heartbeat and achieve good results, but these methods usually have a large number of parameters and high computational cost, which are not conducive to the real-world applications (e.g. wearable health monitoring). To address the problem, we propose a lightweight and high-performance inter-patient heartbeat classification method with dual attention mechanism. Specifically, we design a lightweight residual block, which is mainly composed of residual and depthwise separable convolution, for reducing the number of parameters. The dual attention mechanism is implemented by combining the “Squeeze-and-Excitation” (SE) module and a new “Convolutional Squeeze-and-Excitation” (Conv-SE) module. Two different kinds of channel attention are fused to improve the performance of the model. In addition, an effective image-based data augmentation procedure for 2-D representation of heartbeats is developed to overcome the data imbalance problem. We evaluated the proposed method on the MIT-BIH Arrhythmia Database and the Supraventricular Arrhythmia Database. Our method achieved an average classification accuracy of 98.68% and an average F1-score of 0.9202 on the MIT-BIH Arrhythmia database, and achieved an average classification accuracy of 97.46% and an average F1-score of 0.8984 on the Supraventricular Arrhythmia Database. Compared with other methods, the proposed method achieved best classification performance, and the number of parameters and the computational complexity were reduced by more than 70%.},
  archive      = {J_APIN},
  author       = {Xie, Hongfu and Liu, Hui and Zhou, Shuwang and Gao, Tianlei and Shu, Minglei},
  doi          = {10.1007/s10489-022-04303-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17178-17193},
  shortjournal = {Appl. Intell.},
  title        = {A lightweight 2-D CNN model with dual attention mechanism for heartbeat classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated gender classification from handwriting: A
systematic survey. <em>APIN</em>, <em>53</em>(13), 17154–17177. (<a
href="https://doi.org/10.1007/s10489-022-04347-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically identifying the gender of a writer from a handwritten sample is an essential task in various domains, including historical document analysis, handwriting biometrics, and psychology. Technological advances in computer vision and image analysis have yielded various techniques suitable for this task, each with its own merits and limitations. However, a systematic survey of these techniques, which would provide researchers guidance on selecting the appropriate approach, is currently lacking. To address this gap, we used a predefined query to select and then analyze a selection of peer-reviewed studies published between 2012 and 2021 that presented automatic methods for the classification of gender from handwriting. Next, we describe and categorize the feature extraction methods applied and classifiers used in each study, overview the existing datasets, and compare the results across studies. Finally, based on these data, we specify yet-unresolved issues in the field and provide recommendations for the development of new and improved classification methods.},
  archive      = {J_APIN},
  author       = {Rabaev, Irina and Litvak, Marina},
  doi          = {10.1007/s10489-022-04347-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17154-17177},
  shortjournal = {Appl. Intell.},
  title        = {Automated gender classification from handwriting: A systematic survey},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical history based information selection for
document grounded dialogue generation. <em>APIN</em>, <em>53</em>(13),
17139–17153. (<a
href="https://doi.org/10.1007/s10489-022-04373-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting appropriate information from the dialogue history and the document is a prerequisite for a high-quality response in document grounded dialogue generation task. Most of the existing works take dialogue history information as a sequence to interact with documents. In fact, dialogue history has an internal hierarchical structure, which can provide constraints on the selection of information and the interaction with document information. Therefore, this paper proposes a model that uses the hierarchical structure of dialogue history for key information selection. The main idea is to locate important information in history and document by merging both word-level and utterance-level attention of history, and then to generate a better response. The experimental results on two public data sets show that our method significantly outperforms the baseline models.},
  archive      = {J_APIN},
  author       = {Wang, Meiqi and Tian, Shiyu and Bai, Ziwei and Yuan, Caixia and Wang, Xiaojie},
  doi          = {10.1007/s10489-022-04373-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17139-17153},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical history based information selection for document grounded dialogue generation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary equilibrium SR: Effective loss functions for single
image super-resolution. <em>APIN</em>, <em>53</em>(13), 17128–17138. (<a
href="https://doi.org/10.1007/s10489-022-04162-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, single image super-resolution (SISR) has made great progress due to the rapid development of deep convolutional neural networks (CNN), and the application of Generative Adversarial Networks (GAN ) has made super-resolution networks even more effective. However, GAN-based methods have many problems such as lengthy and unstable convergence. To solve these problems, this paper presents a mechanism that employs boundary equilibrium in the image super-resolution network to balance the convergence of the generator and the discriminator and improve the visual quality of the generated synthetic images. Furthermore, current methods often use perceptual loss based on the VGG network. However, experiments show that the visual quality improvement brought by this perceptual loss is very limited, so we propose an improved perceptual loss based on Learned Perceptual Image Patch Similarity (LPIPS) to acquire better human visual effects rather than adopting the traditional perceptual loss based on VGG. The experimental results clearly show that using our proposed method can considerably improve the performance of image super-resolution and obtain clearer details than state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Zhang, Zhechen and Lu, Weigang and Chen, Shuo and Yang, Fei and Jingchang, Pan},
  doi          = {10.1007/s10489-022-04162-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17128-17138},
  shortjournal = {Appl. Intell.},
  title        = {Boundary equilibrium SR: Effective loss functions for single image super-resolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OTDE: Optimal transport distribution enhancement for
few-shot video recognition. <em>APIN</em>, <em>53</em>(13), 17115–17127.
(<a href="https://doi.org/10.1007/s10489-022-04369-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, action recognition has become a subject of focus in the field of computer vision. Interest has emerged regarding the recognition of previously unseen classes given a few labeled examples; this is known as few-shot video recognition (F-SVR). However, it is particularly challenging to learn the class representation in this kind of setting. In response to this difficulty, we present an optimal transport distribution enhancement (OTDE) mechanism that enables networks to adaptively enhance the given support videos. Our main idea is to design an optimal transport method by using the base classes of data to calibrate the biased distribution of the support set in F-SVR and generate enhanced samples to better model the distribution of intra-class features and estimate the similarity between them in an accurate and robust manner. In addition, the proposed OTDE component is a simple yet flexible approach and is adaptable to multiple existing F-SVR frameworks. By adopting OTDE, our design brings substantial performance improvements to a variety of current works, achieving competitive results on the Kinetics, UCF101 and HMDB51 datasets under various evaluation settings.},
  archive      = {J_APIN},
  author       = {Qin, Yanfei and Liu, Baolin},
  doi          = {10.1007/s10489-022-04369-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17115-17127},
  shortjournal = {Appl. Intell.},
  title        = {OTDE: Optimal transport distribution enhancement for few-shot video recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor completion via hybrid shallow-and-deep priors.
<em>APIN</em>, <em>53</em>(13), 17093–17114. (<a
href="https://doi.org/10.1007/s10489-022-04331-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the ill-posed tensor completion (TC) problem, how to appropriately explore the hidden information is a pivotal yet challenging issue. Two well-known priors have been intensively studied, including the shallow priors such as low-rankness and local piecewise smoothness, and the deep priors using data-driven learning. However, their singleton usage or the simple combination hardly touches an acceptable performance. In this paper, we propose a hybrid shallow-and-deep priors model (HPM) to simultaneously exploit the respective strengths for a better performance. Specifically, based on the frontiers, i.e., tensor nuclear norm of discrete cosine transform (DCTNN) and enhanced 3D total variation (E3DTV), we propose a weighting scheme to characterize both the globally low-rank correlation and the locally smoothness. On that basis, a data-driven denoiser following the Plug-and-Play (PnP) framework is incorporated to provide the implicit information that cannot be captured by the shallow priors. Finally, the overall tensor completion model is optimized by the well-known alternating direction method of multiplier (ADMM). Numerical experiments show that the hybrid priors benefit from both types of visual properties and enable state-of-the-art quantitative and qualitative performance.},
  archive      = {J_APIN},
  author       = {Xu, Honghui and Jiang, Jiawei and Feng, Yuchao and Jin, Yiting and Zheng, Jianwei},
  doi          = {10.1007/s10489-022-04331-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17093-17114},
  shortjournal = {Appl. Intell.},
  title        = {Tensor completion via hybrid shallow-and-deep priors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust graph representation clustering based on adaptive
data correction. <em>APIN</em>, <em>53</em>(13), 17074–17092. (<a
href="https://doi.org/10.1007/s10489-022-04268-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Impressive performance has been achieved when learning graphs from data in clustering tasks. However, real data often contain considerable noise, which leads to unreliable or inaccurate constructed graphs. In this paper, we propose adaptive data correction-based graph clustering (ADCGC), which can be used to adaptively remove errors and noise from raw data and improve the performance of clustering. The ADCGC method mainly contains three advantages. First, we design the weighted truncated Schatten p-norm (WTSpN) instead of the nuclear norm to recover the low-rank clean data. Second, we choose clean data samples that represent the essential properties of the data as the vertices of the undirected graph, rather than using all the data feature points. Third, we adopt the block-diagonal regularizer to define the edge weights of the graph, which helps to learn an ideal affinity matrix and improve the performance of clustering. In addition, an efficient iterative scheme based on the generalized soft-thresholding operator and alternating minimization is developed to directly solve the nonconvex optimization model. Experimental results show that ADCGC both quantitatively and visually outperforms existing advanced methods.},
  archive      = {J_APIN},
  author       = {Guo, Li and Zhang, Xiaoqian and Zhang, Rui and Wang, Qian and Xue, Xuqian and Liu, Zhigui},
  doi          = {10.1007/s10489-022-04268-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17074-17092},
  shortjournal = {Appl. Intell.},
  title        = {Robust graph representation clustering based on adaptive data correction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SRENet: A spatiotemporal relationship-enhanced 2D-CNN-based
framework for staging and segmentation of kidney cancer using CT images.
<em>APIN</em>, <em>53</em>(13), 17061–17073. (<a
href="https://doi.org/10.1007/s10489-022-04384-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney cancer (KC) is among the 10 most common cancers posing health threats to humans, with an average lifetime risk of 1.53%. Computed tomography (CT) is regarded as the golden standard for the characterization of KC and is widely used for KC prognosis. However, it is challenging to segment KC in CT images and perform cancer staging simultaneously due to the variable positions and shapes of kidney tumors and similar textural features in the background and target areas. We propose a novel spatiotemporal relationship-enhanced convolutional neural network (CNN)-based framework called SRENet. It consists of a spatial transformer framework and a residual U-Net with a temporal relationship extraction module for the staging and segmentation of KC. The SRENet achieves excellent performance on six evaluation metrics (Kappa, Sensitivity, Specificity, Precision, Accuracy, F1-score) for KC staging with an F1-score of 98.46%. The framework demonstrates a strong and reliable capacity for kidney and KC segmentation with Dice coefficients (DCs) of 97.89% and 92.54%, respectively, outperforming the state-of-the-art models (ResNeXt-101, ViT and Swin-Transformer for staging, and VNet, UNet 3+ and nnUNet for segmentation). The proposed SRENet helps accelerate the development of reliable kidney and KC segmentation methodologies and shows significant potential for KC diagnosis in clinical practice.},
  archive      = {J_APIN},
  author       = {Liang, Shuang and Gu, Yu},
  doi          = {10.1007/s10489-022-04384-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17061-17073},
  shortjournal = {Appl. Intell.},
  title        = {SRENet: A spatiotemporal relationship-enhanced 2D-CNN-based framework for staging and segmentation of kidney cancer using CT images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). 3D solid model generation method based on a generative
adversarial network. <em>APIN</em>, <em>53</em>(13), 17035–17060. (<a
href="https://doi.org/10.1007/s10489-022-04381-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) solid model generation technology is the foundation to realize intelligently generated structural design, but this problem has not yet been effectively solved. This paper proposes a comprehensive generation method named 3D-JointGAN for 3D solid models by combining a 3D generative adversarial network (GAN) and reverse engineering (RE) technology. First, the basic idea, relevant theories and specific implementation process of 3D-JointGAN are introduced. Then, the approach is applied to the generation of a three-branch cast-steel joint in practical engineering, and the mechanical properties of representative joints selected after evaluation are synthetically calculated. Finally, reduced-scale models of the representative joints are manufactured using 3D printing technology to verify the manufacturability of the generated models. By comparison with three other types of joints common in engineering, the results show that the proposed method has outstanding generation and optimization abilities and can generate a variety of innovative and highly vivid 3D solid models. Furthermore, the representative joints chosen after assessment have better mechanical properties. The method proposed in this paper solves the bottleneck problem of intelligently generated structural design and has broad application prospects.},
  archive      = {J_APIN},
  author       = {Du, Wenfeng and Xia, Zhuang and Han, Leyu and Gao, Boqing},
  doi          = {10.1007/s10489-022-04381-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17035-17060},
  shortjournal = {Appl. Intell.},
  title        = {3D solid model generation method based on a generative adversarial network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TriView-ParNet: Parallel network for hybrid recognition of
touching printed and handwritten strings based on feature fusion and
three-view co-training. <em>APIN</em>, <em>53</em>(13), 17015–17034. (<a
href="https://doi.org/10.1007/s10489-022-04257-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been a mainstream solution for recognizing printed and isolated handwritten characters in Optical Characters Recognition (OCR). However, it is still a challenge to hybrid recognition of adjoining strings in printed and handwritten format, especially in the case that characters are touching and the data is imbalanced. In this paper, we propose a hybrid recognition scheme, termed TriView-ParNet, for adjoining printed-and-handwritten strings. First of all, we introduce a Parallel Network, which consists of Two-stream feature Extraction and Fusion Module (TEFM) and Context Extraction and Transcription Module (CETM). The TEFM is proposed to address the issue where characters are touched in printed and handwritten format. It can fuse the content and positional features extracted by two feature extraction networks to enrich the original feature representation. For another, the CETM is used to further extract the contextual information of the sequence. By using the contextual prompts of sequence, the recognition ability of long strings can be enhanced by CETM. Secondly, we propose a Three-view Co-training Module, in view of the poor performance of direct training based on a small amount of labeled data. Using the idea of semi-supervised learning, a classifier is trained from three different views, print, handwriting, and hybrid. Finally, we compare our method with state-of-the-art methods on the public dataset NIST SD19 and the newly collected dataset CPHS2020. The experimental results demonstrate that our method gets a higher accuracy of strings recognition. As a result, our TriView-ParNet extracts positional and contextual information to enhance the performance of recognition, which also provides a semi-supervised learning solution.},
  archive      = {J_APIN},
  author       = {Qiu, Junhao and Lai, Shangyu and Huang, Guoheng and Zhang, Weiwen and Mai, Junhui and Pun, Chi-Man and Ling, Wing-Kuen},
  doi          = {10.1007/s10489-022-04257-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {17015-17034},
  shortjournal = {Appl. Intell.},
  title        = {TriView-ParNet: Parallel network for hybrid recognition of touching printed and handwritten strings based on feature fusion and three-view co-training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UAV target following in complex occluded environments with
adaptive multi-modal fusion. <em>APIN</em>, <em>53</em>(13),
16998–17014. (<a
href="https://doi.org/10.1007/s10489-022-04317-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, deep reinforcement learning (DRL) has made remarkable achievements in unmanned aerial vehicle (UAV) target following. However, current DRL-based solutions only exploit appearance features to recognize and follow the target, and thus suffer from loss of the target in complex occluded environments. To that end, a novel target following solution based on adaptive appearance-motion feature fusion is proposed. First, we follow prior works to exploit convolutional neural network to extract appearance features of the target from the observation image captured from a down-looking camera. Meanwhile, we innovatively leverage action sequences of UAV to explicitly encode the motion features of the target. An attention module is subsequently introduced to adaptively select relevant useful features which serve as environment states and are fed into the decision module to produce the motion action of UAV. The whole network is trained using Deep Q-Network to learn the motion policy from observation in an end-to-end manner. We perform simulation experiments on Virtual Robot Experimentation Platform. Extensive experimental results demonstrate that: (1) Our proposed method achieves higher tracking accuracy and longer tracking time in various environments compared to state-of-the-art approaches; (2) The learned DRL policy could be well generalized to unseen environments.},
  archive      = {J_APIN},
  author       = {Xu, Lele and Wang, Teng and Cai, Wenzhe and Sun, Changyin},
  doi          = {10.1007/s10489-022-04317-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16998-17014},
  shortjournal = {Appl. Intell.},
  title        = {UAV target following in complex occluded environments with adaptive multi-modal fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust multi-label feature selection with shared coupled and
dynamic graph regularization. <em>APIN</em>, <em>53</em>(13),
16973–16997. (<a
href="https://doi.org/10.1007/s10489-022-04343-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph-based multi-label feature selection (MFS) method plays a pivotal role in big data era with the exponential growth of multi-label data. As the activity of the multi-label field increases, it also exposes some multi-label problems. Traditional graph-based methods adopt the original spaces to construct Laplacian graphs, which increases redundancy and noise. In addition, when exploring the deeper subtle connections between features and labels of multi-label samples, and extracting spatial correlations, there is a lack of consideration for shared connection information between feature space and label space. To track these tricky problems, this study adopts matrix factorization method to decompose the original matrix space and extract the low dimensional matrix. On the one hand, dynamic graph regularization preserves the spatial geometry and the interference of redundant features are reduced while extracting correlations. On the other hand, the decomposed low dimensional matrix obtains the shared connection information from the dual space of feature and label, which is beneficial for mining information from data. Furthermore, l2,1/2-norm is applied to the feature weight matrix to enforce row-sparsity and robustness. So this study proposes a robust MFS with Shared Coupled and Dynamic graph Regularization (SCDRMFS). An iterative method for solving the objective function is proposed, and its convergence is proved from two aspects, theoretically and experimentally. Moreover, experiments on nine real benchmark datasets are performed to verify the effectiveness of the proposed method. SCDRMFS is contrasted with six latest algorithms. It is concluded from the experimental results that the proposed algorithm SCDRMFS can improve the classification performance for multi-label datasets.},
  archive      = {J_APIN},
  author       = {Wang, Lingzhi and Chen, Hongmei and Peng, Bo and Li, Tianrui and Yin, Tengyu},
  doi          = {10.1007/s10489-022-04343-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16973-16997},
  shortjournal = {Appl. Intell.},
  title        = {Robust multi-label feature selection with shared coupled and dynamic graph regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view co-clustering with multi-similarity.
<em>APIN</em>, <em>53</em>(13), 16961–16972. (<a
href="https://doi.org/10.1007/s10489-022-04385-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view co-clustering, which clustering the two dimensions of samples and features of multi-view data at the same time, has attracted much attention in recent years. It aims to exploit the duality of multi-view data to get better clustering results. However, most of the existing multi-view co-clustering algorithms consider the sample-feature information of the data while ignoring the sample-sample, feature-feature information, and thus cannot fully mine the potential information contained in the data. Therefore, this paper proposes a multi-view co-clustering based on multi-similarity. In particular, based on spectral clustering, we propose a method of constructing graph to improve the performance of clustering, which is no longer limited to the relevance between samples and features. At the same time, inspired by the ensemble algorithm, we use multiple co-clustering algorithms to calculate the similarity information of each view data, which makes the algorithm more robust. Compared with the existing multi-view co-clustering methods, the proposed algorithm exploits the more comprehensive similarity information in each view data, including sample-sample, feature-feature, and sample-feature similarity information. We performed experiments on several benchmark datasets. Due to mining and using more similarity information, our experimental results are better than the comparison method in the three evaluation indicators. In particular, on some data with co-occurrence features such as (word-document), our algorithm achieves better results and can obtain higher accuracy.},
  archive      = {J_APIN},
  author       = {Zhao, Ling and Ma, Yunpeng and Chen, Shanxiong and Zhou, Jun},
  doi          = {10.1007/s10489-022-04385-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16961-16972},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view co-clustering with multi-similarity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global reliable data generation for imbalanced binary
classification with latent codes reconstruction and feature repulsion.
<em>APIN</em>, <em>53</em>(13), 16922–16960. (<a
href="https://doi.org/10.1007/s10489-022-04330-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common binary classification algorithms which learn directly from imbalanced data can lead to a bias towards the majority class. Although the over-sampling technology can solve the imbalance problem, the realness of the synthesized samples cannot be guaranteed. Generative Adversarial Networks can improve the authenticity of the generated samples. However, it may cause mode collapse, resulting in the data distribution space of the minority class changed after balance. A sample-level data generation method is proposed in this paper for imbalanced classification. Firstly, we present a reconstruction technique of latent codes with mutual information constraints for global data generation. The latent codes of the input sample are divided into latent vectors of key features and subordinate features respectively. We can obtain the mutated latent codes by retaining the key features’ latent vector and randomly replacing the subordinate features’ latent vector. Then the reliable similar mutation samples are generated through decoder restoration, mutual information constraint, and discriminant confrontation. In addition, the feature repulsion technique and the combination coding technique are proposed to solve the problem of feature extraction and classification for samples in overlapping areas. The former carries out supervised feature representation learning of the key features’ latent vector. The latter superimposes the reconstruction error of each dimension of the sample as a supplement for the latent vector of key features. Combined with a variety of typical base classifiers, a large number of experimental results on public datasets show that the proposed method outperforms other typical data balancing methods in F1-Measure and G-Mean.},
  archive      = {J_APIN},
  author       = {Jia, Xin and Gao, Xin and Chen, Wenli and Cheng, Yingying and Meng, Zhihang and Xue, Bing and Huang, Zijian and Fu, Shiyuan},
  doi          = {10.1007/s10489-022-04330-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16922-16960},
  shortjournal = {Appl. Intell.},
  title        = {Global reliable data generation for imbalanced binary classification with latent codes reconstruction and feature repulsion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal prediction in three-dimensional space by
separating information interactions. <em>APIN</em>, <em>53</em>(13),
16908–16921. (<a
href="https://doi.org/10.1007/s10489-022-04338-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal prediction in two-dimensional (2D) space has been widely studied in computer science and geosciences, such as video prediction and weather forecasting. However, many spatiotemporal evolutions take place in three-dimensional (3D) space, e.g., atmospheric temperature changes of multiple isobaric surfaces. For such a 3D multi-plane prediction task, existing methods usually fail to model the information interactions between different 2D planes. In this paper, we propose a novel neural network-based method to tackle 3D multi-plane spatiotemporal prediction tasks. Specifically, compared with prediction in 2D space, information interactions in 3D space are more complicated because of the emergence of a new dimension. To clarify these interactions, we first propose to separate them into three types, i.e., inter-level interaction, intra-level interaction, and unknown residual interaction. This separation is novel and comprehensive, which covers vertical, horizontal, and potentially unknown factors. Based on the separation, we design a new spatiotemporal module for prediction. It contains three units to capture the three separated interactions, respectively: (1) an inter-level interaction unit, which is the first one dedicated to modeling the information interaction between different levels. The inter-level interaction, as a new feature in 3D space, has not been effectively studied. Hence, in this unit, we propose to model it by leveraging the spatial associations between adjacent levels and developing a new dynamics simulation. (2) An intra-level interaction unit is utilized to model the information interaction within the same level. With the benefit of interaction separation, we can leverage the neural partial differential equations to formulate spatiotemporal dynamics and exploit prior physical knowledge in data. (3) Furthermore, a residual unit is employed to capture the remaining unknown and uncertain factors, which can help further improve the expressiveness of our model. As for a prediction, we employ the three units to capture various dynamics, and combine them to obtain a comprehensive one. The final dynamics will be decoded for generating prediction via a convolutional neural network. We conduct extensive experiments on a dataset of atmospheric temperature changes. The experimental results show that our method obtains a significant improvement and achieves a new state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Huang, Xu and Zhang, Bowen and Ye, Yunming and Feng, Shanshan and Li, Xutao},
  doi          = {10.1007/s10489-022-04338-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16908-16921},
  shortjournal = {Appl. Intell.},
  title        = {Spatiotemporal prediction in three-dimensional space by separating information interactions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dyna-PPO reinforcement learning with gaussian process for
the continuous action decision-making in autonomous driving.
<em>APIN</em>, <em>53</em>(13), 16893–16907. (<a
href="https://doi.org/10.1007/s10489-022-04354-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed rapid development of autonomous driving. Model-based and model-free reinforcement learning are two popular learning methods for autonomous driving. However, these two kinds of methods have their own advantages in achieving excellent driving experience. In order to improve their efficiency and performance, Dyna framework is an promising way to combine their advantages. Unfortunately, the classical Dyna framework can not deal with the continuous actions in reinforcement learning. In addition, the interaction between the world model and the model-free reinforcement learning agent remains at the unidirectional data level. To further improve the effectiveness and efficiency of driving policy learning, we propose a novel Gaussian Process based Dyna-PPO approach in this paper. The Gaussian Process model, which is analytically tractable and fits for small-sample problems, is introduced to build the world model. In addition, we design a mechanism to realize bidirectional interaction between the world model and the policy model. Extensive experiments validate the effectiveness and robustness of our proposed approach. According to our simulation result, the driving distance of the vehicle could be improved by approximately 0.2× times.},
  archive      = {J_APIN},
  author       = {Wu, Guanlin and Fang, Wenqi and Wang, Ji and Ge, Pin and Cao, Jiang and Ping, Yang and Gou, Peng},
  doi          = {10.1007/s10489-022-04354-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16893-16907},
  shortjournal = {Appl. Intell.},
  title        = {Dyna-PPO reinforcement learning with gaussian process for the continuous action decision-making in autonomous driving},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised actor-critic reinforcement learning with action
feedback for algorithmic trading. <em>APIN</em>, <em>53</em>(13),
16875–16892. (<a
href="https://doi.org/10.1007/s10489-022-04322-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning is one of the promising approaches for algorithmic trading in financial markets. However, in certain situations, buy or sell orders issued by an algorithmic trading program may not be fulfilled entirely. By considering the actual scenarios from the financial markets, in this paper, we propose a novel framework named Supervised Actor-Critic Reinforcement Learning with Action Feedback (SACRL-AF) for solving this problem. The action feedback mechanism of SACRL-AF notifies the actor about the dealt positions and corrects the transitions of the replay buffer. Meanwhile, the dealt positions are used as the labels for the supervised learning. Recent studies have shown that Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) are more stable and superior to other actor-critic algorithms. Against this background, based on the proposed SACRL-AF framework, two reinforcement learning algorithms henceforth referred to as Supervised Deep Deterministic Policy Gradient with Action Feedback (SDDPG-AF) and Supervised Twin Delayed Deep Deterministic Policy Gradient with Action Feedback (STD3-AF) are proposed in this paper. Experimental results show that SDDPG-AF and STD3-AF achieve the state-of-art performance in profitability.},
  archive      = {J_APIN},
  author       = {Sun, Qizhou and Si, Yain-Whar},
  doi          = {10.1007/s10489-022-04322-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16875-16892},
  shortjournal = {Appl. Intell.},
  title        = {Supervised actor-critic reinforcement learning with action feedback for algorithmic trading},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Winding pathway understanding based on angle projections in
a field environment. <em>APIN</em>, <em>53</em>(13), 16859–16874. (<a
href="https://doi.org/10.1007/s10489-022-04325-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene understanding is a core problem for autonomous navigation. However, its implementation is frustrated by a variety of unsettled issues, such as understanding winding pathways in unknown, dynamic, and field environments. Traditional three-dimensional (3D) estimation from 3D point clouds or fused data is memory intensive and energy-consuming, which makes these approaches less reliable in a resource-constrained field robot system with limited computation, memory, and energy resources. In this study, we present a methodology to understand winding field pathways and reconstruct them in a 3D environment, using a low-cost monocular camera without prior training. Winding angle projections are assigned to clusters. By composing subclusters, candidate surfaces are shaped. Based on geometric inferences of integrity and orientation, a field pathway can be approximately understood and reconstructed using straight and winding surfaces in a 3D scene. With the use of geometric inference, no prior training is needed, and the approach is robust to colour and illumination. The percentage of incorrectly classified pixels was compared to the ground truth. Experimental results demonstrated that the method can successfully understand winding pathways, meeting the requirements for robot navigation in an unstructured environment.},
  archive      = {J_APIN},
  author       = {Wang, Luping and Wei, Hui},
  doi          = {10.1007/s10489-022-04325-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16859-16874},
  shortjournal = {Appl. Intell.},
  title        = {Winding pathway understanding based on angle projections in a field environment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiffMoment: An adaptive optimization technique for
convolutional neural network. <em>APIN</em>, <em>53</em>(13),
16844–16858. (<a
href="https://doi.org/10.1007/s10489-022-04382-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Gradient Decent (SGD) is a very popular basic optimizer applied in the learning algorithms of deep neural networks. However, it has fixed-sized steps for every epoch without considering gradient behaviour to determine step size. The improved SGD optimizers like AdaGrad, Adam, AdaDelta, RAdam, and RMSProp make step sizes adaptive in every epoch. However, these optimizers depend on square roots of exponential moving averages (EMA) of squared previous gradients or momentums or both and cannot take the benefit of local change in gradients or momentums or both. To reduce these limitations, a novel optimizer has been presented in this paper where the adjustment of step size is done for each parameter based on changing information between the 1st and the 2nd moment estimate (i.e., diffMoment). The experimental results depict that diffMoment offers better performance than AdaGrad, Adam, AdaDelta, RAdam, and RMSProp optimizers. It is also noticed that diffMoment does uniformly better for training Convolutional Neural Networks (CNN) applying different activation functions.},
  archive      = {J_APIN},
  author       = {Bhakta, Shubhankar and Nandi, Utpal and Si, Tapas and Ghosal, Sudipta Kr and Changdar, Chiranjit and Pal, Rajat Kumar},
  doi          = {10.1007/s10489-022-04382-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16844-16858},
  shortjournal = {Appl. Intell.},
  title        = {DiffMoment: An adaptive optimization technique for convolutional neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SleepXAI: An explainable deep learning approach for
multi-class sleep stage identification. <em>APIN</em>, <em>53</em>(13),
16830–16843. (<a
href="https://doi.org/10.1007/s10489-022-04357-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research has been conducted on the automatic classification of sleep stages utilizing deep neural networks and other neurophysiological markers. However, for sleep specialists to employ models as an assistive solution, it is necessary to comprehend how the models arrive at a particular outcome, necessitating the explainability of these models. This work proposes an explainable unified CNN-CRF approach (SleepXAI) for multi-class sleep stage classification designed explicitly for univariate time-series signals using modified gradient-weighted class activation mapping (Grad-CAM). The proposed approach significantly increases the overall accuracy of sleep stage classification while demonstrating the explainability of the multi-class labeling of univariate EEG signals, highlighting the parts of the signals emphasized most in predicting sleep stages. We extensively evaluated our approach to the sleep-EDF dataset, and it demonstrates the highest overall accuracy of 86.8% in identifying five sleep stage classes. More importantly, we achieved the highest accuracy when classifying the crucial sleep stage N1 with the lowest number of instances, outperforming the state-of-the-art machine learning approaches by 16.3%. These results motivate us to adopt the proposed approach in clinical practice as an aid to sleep experts.},
  archive      = {J_APIN},
  author       = {Dutt, Micheal and Redhu, Surender and Goodwin, Morten and Omlin, Christian W.},
  doi          = {10.1007/s10489-022-04357-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16830-16843},
  shortjournal = {Appl. Intell.},
  title        = {SleepXAI: An explainable deep learning approach for multi-class sleep stage identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised anomaly detection via two-dimensional singular
value decomposition and subspace reconstruction for multivariate time
series. <em>APIN</em>, <em>53</em>(13), 16813–16829. (<a
href="https://doi.org/10.1007/s10489-022-04337-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increasing requirement of stability, security, and availability in practical applications, multivariate time series (MTS) anomaly detection has received widespread attention. Currently, potential variable correlation and multiple patterns in MTS are prevalent due to the modular deployment of large systems. However, existing anomaly detection methods are either weak in modeling the variable correlation, or poor in capturing the multiple patterns. In this paper, we propose GMSAE-2dSVD, an unsupervised anomaly detection model, to solve the challenging problems. The model employs two-dimensional singular value decomposition (2dSVD) as a representation method to capture temporal dependence and correlation between variables in MTS. In addition, a subspace reconstruction strategy based on the sparse autoencoder (SAE) and gaussian mixture model (GMM) is proposed to extract multiple patterns accurately. Experimental results on six public datasets show that the proposed method is superior to the state-of-the-art methods. The model is simple in structure, efficient in training and capable of capturing various patterns in complicated MTS, which has considerable application value.},
  archive      = {J_APIN},
  author       = {Ge, NingZhen and Weng, Xiaoqing and Yang, QiuYing},
  doi          = {10.1007/s10489-022-04337-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16813-16829},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised anomaly detection via two-dimensional singular value decomposition and subspace reconstruction for multivariate time series},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint grading of diabetic retinopathy and diabetic macular
edema using an adaptive attention block and semisupervised learning.
<em>APIN</em>, <em>53</em>(13), 16797–16812. (<a
href="https://doi.org/10.1007/s10489-022-04295-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early screening and treatment of diabetic retinopathy (DR) and diabetic macular edema (DME) can prevent the risk of blindness for most diabetic patients. Joint grading of DR and DME can reduce the screening cost and improve screening efficiency. Most grading methods only focus on grading one disease and lack generality. In this paper, we propose an adaptive attention block (AAB) to better extract features and improve model performance, which can be adaptively adjusted according to different grading tasks. We propose the AABNet to handle multiple grading tasks, consisting of a lightweight architecture MobileNetv2 as a backbone for feature extraction and two independent branches, the DR branch and the DME branch, for grading prediction. To alleviate the lack of labeled data, we propose an adaptive teacher-student model as a semisupervised learning method to train the AABNet with additional unlabeled data, dependent on fewer model parameters, whose foundation is consistency regularization, which is added to the loss to provide additional supervised signals for training to obtain a more accurate model without destroying the image details and strictly requiring the quality of unlabeled images. Extensive experiments are conducted, including performance comparisons with state-of-the-art methods and ablation studies of attention mechanisms and semisupervised learning mechanisms. Experiments verify that AABNet can bring a satisfactory improvement in the multiple DR grading, DME grading and joint DR&amp;DME grading tasks. AABNet outperforms other state-of-the-art methods and achieves better results in multiple grading tasks on the Messidor and DDR datasets.},
  archive      = {J_APIN},
  author       = {Guo, Xiaoxin and Li, Xiang and Lin, Qifeng and Li, Guangyu and Hu, Xiaoying and Che, Songtian},
  doi          = {10.1007/s10489-022-04295-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16797-16812},
  shortjournal = {Appl. Intell.},
  title        = {Joint grading of diabetic retinopathy and diabetic macular edema using an adaptive attention block and semisupervised learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept drift detection and localization framework based on
behavior replacement. <em>APIN</em>, <em>53</em>(13), 16776–16796. (<a
href="https://doi.org/10.1007/s10489-022-04341-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application of numerous services or software, process mining has attracted more and more attention. However, concept drift may occur during process mining due to the instability of the process. Sudden and gradual drifts are considered to be two basic modes of change, that may always appear in independent or nested forms. Although the existing methods have studied the detection of two basic modes, they do not consider the nesting of two change modes. We identify the change mode that sudden drifts and gradual drifts do not appear independently as nested drifts. The current drift detection methods can only detect the drift modes that occur independently, but not suitable for nested drift detection. To fill this gap, this paper proposes a business concept drift detection and localization framework called BRDDL (Behavior Replacement-based Drift Detection and Localization) which can not only detect independent drifts such as sudden drifts and gradual drifts, but also detect nested drifts. Firstly, we propose an integrated drift point detection and localization method which can report the location of change points and return the changed behaviors (activity relationship pairs). On this basis, we propose a behavior replacement method by updating the changed traces to restore an unchanged sub log. Then we compare the behaviors in the updated traces with those in the associated unchanged traces to judge the type of drifts. The effectiveness of the method is verified by simulation experiments on the synthetic log.},
  archive      = {J_APIN},
  author       = {Xu, Jiuyun and Zhang, Yue and Duan, Qiang},
  doi          = {10.1007/s10489-022-04341-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16776-16796},
  shortjournal = {Appl. Intell.},
  title        = {Concept drift detection and localization framework based on behavior replacement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Global spatio-temporal aware graph neural network for next
point-of-interest recommendation. <em>APIN</em>, <em>53</em>(13),
16762–16775. (<a
href="https://doi.org/10.1007/s10489-022-04377-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next Point-of-Interest (POI) recommendation is becoming increasingly popular with the rapidly growing of Location-based Social Networks (LBSNs). Most existing models only focus on exploring the local spatio-temporal relationships between POIs based on the trajectory sequence of current user. However, we argue that there exits not only local spatio-temporal relationship but also global spatio-temporal relationship, where two POIs are correlated if they appear in the trajectories of all users within certain geographical distance and time intervals. In this paper, we propose Global Spatio-Temporal Aware Graph Neural Network (GSTA-GNN), a model that captures and utilizes the global spatio-temporal relationships from the global view across the trajectories of all users. Specifically, we first break down the independence between trajectories and link all pairs of POIs based on the POI transitions to construct a global spatial graph and a global temporal graph. Then graph neural network is utilized to learn the global general representations of POIs. In addition, we introduce the spatio-temporal weight matrix, which converts the spatial and temporal intervals into suitable weight values and combines them in an adaptive manner. Then we propose to incorporate the spatio-temporal weight matrix into self-attention module of a multi-head self-attention layer to enrich the personalized representation of user trajectory. Experiments on three real datasets show that GSTA-GNN is superior to the state-of-the-art models in next POI recommendation task.},
  archive      = {J_APIN},
  author       = {Wang, Jingkuan and Yang, Bo and Liu, Haodong and Li, Dongsheng},
  doi          = {10.1007/s10489-022-04377-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16762-16775},
  shortjournal = {Appl. Intell.},
  title        = {Global spatio-temporal aware graph neural network for next point-of-interest recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MACFNet: Multi-attention complementary fusion network for
image denoising. <em>APIN</em>, <em>53</em>(13), 16747–16761. (<a
href="https://doi.org/10.1007/s10489-022-04313-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, thanks to the prosperous development of deep convolutional neural network, image denoising task has achieved unprecedented achievements. However, previous researches have difficulties in keeping the balance between noise removing and textual details preserving, even bringing the negative effect, such as local blurring. To overcome these weaknesses, in this paper, we propose an innovative multi-attention complementary fusion network (MACFNet) to restore delicate texture details while eliminating noise to the greatest extent. To be specific, our proposed MACFNet mainly composes of several multi-attention complementary fusion modules (MACFMs). Firstly, we use feature extraction block (FEB) to extract basic features.Then, we use spatial attention (SA), channel attention (CA) and patch attention (PA) three different kinds of attention mechanisms to extract spatial-dimensional, channel-dimensional and patch-dimensional attention aware features, respectively. In addition, we attempt to integrate three attention mechanisms in an effective way. Instead of directly concatenate, we design a subtle complementary fusion block (CFB), which is skilled in incorporating three sub-branches characteristics adaptively. Extensive experiments are carried out on gray-scale image denoising, color image denoising and real noisy image denoising. The quantitative results (PSNR) and visual effects all prove that our proposed network achieves great performance over some state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yu, Jiaolong and Zhang, Juan and Gao, Yongbin},
  doi          = {10.1007/s10489-022-04313-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16747-16761},
  shortjournal = {Appl. Intell.},
  title        = {MACFNet: Multi-attention complementary fusion network for image denoising},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of deep reinforcement learning to intelligent
distributed humidity control system. <em>APIN</em>, <em>53</em>(13),
16724–16746. (<a
href="https://doi.org/10.1007/s10489-022-04320-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The indoor environment of buildings is complex and changeable, and it is difficult to ensure that the indoor humidity is uniform and stable while employing a centralized humidity control system. To address this challenge, this paper proposes an intelligent distributed humidity control system based on model-free deep reinforcement learning. The proposed system consists of three parts: an intelligent controller, distributed facilities, and distributed sensors. The distributed sensors are used to monitor the environmental parameters. This study developed a reinforcement learning algorithm called RH-rainbow and deployed it in distributed facilities. In RH-rainbow, the reward consists of the mean absolute difference of humidity and the energy consumption of distributed facilities. The action is the humidity setpoints and fan settings of the constant humidity machines. The performance of RH-rainbow was evaluated and compared to that of other algorithms in two scenarios with different air outlet settings under different sensor numbers, reporting time intervals, and external interference modes. It was found that RH-rainbow is superior to manual strategies, the traditional analog control strategy, DQN, and PID in terms of uniformity, anti-interference ability, and energy consumption.},
  archive      = {J_APIN},
  author       = {Guo, Da and Luo, Danfeng and Zhang, Yong and Zhang, Xiuyong and Lai, Yuyang and Sun, Yunqi},
  doi          = {10.1007/s10489-022-04320-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16724-16746},
  shortjournal = {Appl. Intell.},
  title        = {Application of deep reinforcement learning to intelligent distributed humidity control system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local self-attention in transformer for visual question
answering. <em>APIN</em>, <em>53</em>(13), 16706–16723. (<a
href="https://doi.org/10.1007/s10489-022-04355-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a multimodal task that requires models to understand both textual and visual information. Various VQA models have applied the Transformer structure due to its excellent ability to model self-attention global dependencies. However, balancing global and local dependency modeling in traditional Transformer structures is an ongoing issue. A Transformer-based VQA model that only models global dependencies cannot effectively capture image context information. Thus, this paper proposes a novel Local Self-Attention in Transformer (LSAT) for a visual question answering model to address these issues. The LSAT model simultaneously models intra-window and inter-window attention by setting local windows for visual features. Therefore, the LSAT model can effectively avoid redundant information in global self-attention while capturing rich contextual information. This paper uses grid visual features to conduct extensive experiments and ablation studies on the VQA benchmark datasets VQA 2.0 and CLEVR. The experimental results show that the LSAT model outperforms the benchmark model in all indicators when the appropriate local window size is selected. Specifically, the best test results of LSAT using grid visual features on the VQA 2.0 and CLEVR datasets were 71.94% and 98.72%, respectively. Experimental results and ablation studies demonstrate that the proposed method has good performance. Source code is available at https://github.com/shenxiang-vqa/LSAT.},
  archive      = {J_APIN},
  author       = {Shen, Xiang and Han, Dezhi and Guo, Zihan and Chen, Chongqing and Hua, Jie and Luo, Gaofeng},
  doi          = {10.1007/s10489-022-04355-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16706-16723},
  shortjournal = {Appl. Intell.},
  title        = {Local self-attention in transformer for visual question answering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hybrid whale optimization algorithm based on symbiosis
strategy for global optimization. <em>APIN</em>, <em>53</em>(13),
16663–16705. (<a
href="https://doi.org/10.1007/s10489-022-04132-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whale optimization algorithm (WOA) is a simple structured and easily implemented swarm-based algorithm inspired by the unique bubble-net feeding method of humpback whales. Past studies have shown that WOA performs well in a number of optimization problems. However, it is difficult for WOA to completely free itself from the problems of insufficient convergence accuracy and premature convergence when solving global optimization problems. To address these issues, a hybrid whale optimization algorithm based on symbiotic strategy (HWOAMS) is proposed in this paper. The main idea of the proposed method is to combine the improved symbiotic organisms search algorithm (SOS) with the whale optimization algorithm thus enhancing the search ability of WOA. First, an improved symbiotic phase based on Lévy flight and chaos strategy is introduced into the exploration process to enhance the global search capability; Second, an improved mutualism phase based on Brownian motion is used instead of the original shrinking encircling phase to achieve better local exploitation. Third, an improved parasitic phase based on a modified global optimal spiral operator strategy is embedded in the spiral updating position phase to help the algorithm further improve the exploitation efficiency and convergence accuracy. Finally, a global search strategy is proposed to help the algorithm better balance exploration and exploitation. To establish the effectiveness of the new algorithm, extensive simulation experiments are conducted on HWOAMS using the classical function test set, the CEC 2019 function set and four classical engineering problems. Numerical evaluation results indicate that HWOAMS outperforms 18 other algorithms in terms of local optimum avoidance ability and convergence accuracy in a majority of cases, and has better search performance.},
  archive      = {J_APIN},
  author       = {Li, Maodong and Xu, Guang-hui and Zeng, Liang and Lai, Qiang},
  doi          = {10.1007/s10489-022-04132-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16663-16705},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid whale optimization algorithm based on symbiosis strategy for global optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-pose face reconstruction and gabor-based dictionary
learning for face recognition. <em>APIN</em>, <em>53</em>(13),
16648–16662. (<a
href="https://doi.org/10.1007/s10489-022-04336-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods based on data and model analysis have been proposed to overcome the adverse effect of poses, occlusion, and illumination on face recognition. Frontal face generation from multi-pose faces is still a widely studied and challenging problem due to its ill-posed. We focus on using a simple model to overcome the effect of pose changes on face recognition. This paper proposes a multi-pose face reconstruction model (MPFR) to generate available face information and combines the model with Gabor-based dictionary learning methods to learn discriminant features. The MPFR is adopted to generate the frontal face image. Given the distortion of the image only generated by Generative Adversarial Networks, the identity loss functions and the symmetry loss function are utilized in the multi-pose faces reconstruction model to reconstruct a more realistic reconstruction frontal image. Besides, we combine discriminative dictionary learning with Gabor features to better express face features for image classification. We report qualitative visualization results and quantitative recognition results of the MPFR model. Further, the experimental results demonstrate the application of the MPFR model in face recognition.},
  archive      = {J_APIN},
  author       = {He, Huanjie and Liang, Jiuzhen and Hou, Zhenjie and Di, Lan and Xia, Yunfei},
  doi          = {10.1007/s10489-022-04336-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16648-16662},
  shortjournal = {Appl. Intell.},
  title        = {Multi-pose face reconstruction and gabor-based dictionary learning for face recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Part-and-whole: A novel framework for deformable medical
image registration. <em>APIN</em>, <em>53</em>(13), 16630–16647. (<a
href="https://doi.org/10.1007/s10489-022-04329-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image registration is a vital initial step in medical image analysis that aims to establish accurate correspondence between image pairs. Various methods have achieved impressive results on this task over the last few years. However, the existing methods fail to achieve desirable registration accuracy because they do not pay sufficient attention on the integration between global and local information in medical images and ignore the geometric constraints in anatomical regions. The registration results still need to be refined to become sufficiently accurate for clinical application considering the current situation. In this paper, we propose a novel part-and-whole registration framework for 3D medical images. The proposed framework consists of two encoder-decoder architectures which take image pairs and patch pairs as inputs, respectively, and estimate the deformation field as the result. In particular, we propose a multi-view feature slicing attention module and embed it into the part-and-whole architecture to enhance the context relation between the local and global information. A voxel-shape-aware loss function is developed to refine the registration performance. We validate the proposed framework on several 3D human brain magnetic resonance image datasets. Results show that our method consistently outperforms recent state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {Zhang, Jinshuo and Liu, Zhaoyang and Ma, Yingjun and Zhao, Xiuyang and Yang, Bo},
  doi          = {10.1007/s10489-022-04329-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16630-16647},
  shortjournal = {Appl. Intell.},
  title        = {Part-and-whole: A novel framework for deformable medical image registration},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-based stability evaluation in time series data sets.
<em>APIN</em>, <em>53</em>(13), 16606–16629. (<a
href="https://doi.org/10.1007/s10489-022-04231-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data analysis, time is often considered just another feature. Yet time has a special role that is regularly overlooked. Procedures are usually only designed for time-independent data and are therefore often unsuitable for the temporal aspect of the data. This is especially the case for clustering algorithms. Although there are a few evolutionary approaches for time-dependent data, the evaluation of these and therefore the selection is difficult for the user. In this paper, we present a general evaluation measure that examines clusterings with respect to their temporal stability and thus provides information about the achieved quality. For this purpose, we examine the temporal stability of time series with respect to their cluster neighbors, the temporal stability of clusters with respect to their composition, and finally conclude on the temporal stability of the entire clustering. We summarise these components in a parameter-free toolkit that we call Cluster Over-Time Stability Evaluation (CLOSE). In addition to that we present a fuzzy variant which we call FCSETS (Fuzzy Clustering Stability Evaluation of Time Series). These toolkits enable a number of advanced applications. One of these is parameter selection for any type of clustering algorithm. We demonstrate parameter selection as an example and evaluate results of classical clustering algorithms against a well-known evolutionary clustering algorithm. We then introduce a method for outlier detection in time series data based on CLOSE. We demonstrate the practicality of our approaches on three real world data sets and one generated data set.},
  archive      = {J_APIN},
  author       = {Klassen, Gerhard and Tatusch, Martha and Conrad, Stefan},
  doi          = {10.1007/s10489-022-04231-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16606-16629},
  shortjournal = {Appl. Intell.},
  title        = {Cluster-based stability evaluation in time series data sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TPCaps: A framework for code clone detection and
localization based on improved CapsNet. <em>APIN</em>, <em>53</em>(13),
16594–16605. (<a
href="https://doi.org/10.1007/s10489-022-03158-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose TPCaps, a new code clone detection framework for addressing the inefficiency of semantic clone detection and the difficulty in locating code clones. Based on CapsNet with tokens and Program Dependence Graph (PDG), TPCaps can improve the processing rate and the capability for detecting. Firstly, TPCaps determines tokens by dataset partitioning and semantic signature, filtering out the valid code clone. Then, using the tokens mentioned above, it can detect Type-1 and Type-2 clones effectively. In addition, TPcaps generates PDG that is composed of data dependencies and control dependencies extracted from codes. Using PDG as input, the improved capsule network, we called RCapsNet, is able to detect Type-3 and Type-4 clones. Based on the CapsNet, RCapsNet introduces selective search algorithm combines with the Regional Proposal Network (RPN), where CapsNet handles the clone features to achieve detection and classification, and RPN processes the location information and updates trains the candidate frames to obtain a specific clone location. In the experimental section, we evaluate the recall and precision of the model. TPCaps shows its high accuracy compared to other models.},
  archive      = {J_APIN},
  author       = {Li, Yuancheng and Yu, Chaohang and Cui, Yaqi},
  doi          = {10.1007/s10489-022-03158-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16594-16605},
  shortjournal = {Appl. Intell.},
  title        = {TPCaps: A framework for code clone detection and localization based on improved CapsNet},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new population initialization approach based on
metropolis–hastings (MH) method. <em>APIN</em>, <em>53</em>(13),
16575–16593. (<a
href="https://doi.org/10.1007/s10489-022-04359-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new population initialization method for metaheuristic algorithms is proposed. In our approach, the initial set of candidate solutions is obtained through the sampling of the objective function with the use of the Metropolis–Hastings technique. Under this method, the set of initial solutions adopts a value close to the prominent values of the objective function to be optimized. Different from most of the initialization methods that consider only spatial distribution, in our algorithm, the initial points represent promising regions of the search space, which deserve to be exploited to identify the global optimum. These characteristics allow the proposed approach obtains a faster convergence and improves the quality of the produced solutions. With the objective to demonstrate the capacities of our initialization method, it has been embedded in the classical Differential Evolution algorithm. To evaluate its performance, the complete system has been tested in a set of representative benchmark functions extracted from several datasets. Experimental results show that the proposed technique presents a better convergence speed and an increment in the quality of the solutions than other similar approaches.},
  archive      = {J_APIN},
  author       = {Cuevas, Erik and Escobar, Héctor and Sarkar, Ram and Eid, Heba F.},
  doi          = {10.1007/s10489-022-04359-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16575-16593},
  shortjournal = {Appl. Intell.},
  title        = {A new population initialization approach based on Metropolis–Hastings (MH) method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-training transformer for source-free domain adaptation.
<em>APIN</em>, <em>53</em>(13), 16560–16574. (<a
href="https://doi.org/10.1007/s10489-022-04364-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the task of source-free domain adaptation (SFDA), where the source data are not available during target adaptation. Previous works on SFDA mainly focus on aligning the cross-domain distributions. However, they ignore the generalization ability of the pretrained source model, which largely influences the initial target outputs that are vital to the target adaptation stage. To address this, we make the interesting observation that the model accuracy is highly correlated with whether attention is focused on the objects in an image. To this end, we propose a generic and effective framework based on Transformer, named TransDA, for learning a generalized model for SFDA. First, we apply the Transformer blocks as the attention module and inject it into a convolutional network. By doing so, the model is encouraged to turn attention towards the object regions, which can effectively improve the model’s generalization ability on unseen target domains. Second, a novel self-supervised knowledge distillation approach is proposed to adapt the Transformer with target pseudo-labels, further encouraging the network to focus on the object regions. Extensive experiments conducted on three domain adaptation tasks, including closed-set, partial-set, and open-set adaption, demonstrate that TransDA can significantly improve the accuracy over the source model and can produce state-of-the-art results on all settings. The source code and pretrained models are publicly available at: https://github.com/ygjwd12345/TransDA .},
  archive      = {J_APIN},
  author       = {Yang, Guanglei and Zhong, Zhun and Ding, Mingli and Sebe, Nicu and Ricci, Elisa},
  doi          = {10.1007/s10489-022-04364-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16560-16574},
  shortjournal = {Appl. Intell.},
  title        = {Self-training transformer for source-free domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bibliometric analysis and basic model introduction of
opinion dynamics. <em>APIN</em>, <em>53</em>(13), 16540–16559. (<a
href="https://doi.org/10.1007/s10489-022-04368-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information exchange and dissemination happen among individuals in a group or social network, which is an inevitable process before the formation of the group and individual opinions. Opinion dynamics is a multidisciplinary research field that models information/opinion exchange processes to simulate the dynamic of opinions in networks with different scales and features. The research on the dynamic of opinions is complex because the involved interactions with individuals are complicated. Many pieces of research and reviews have been conducted on relative models focusing on diverse aspects. Unlike the existing studies, this paper develops a bibliometric analysis and brief introduction of opinion dynamics by science mapping from a relatively macroscopical perspective. Bibliometrics tools such as VOS Viewer and Cite Space are applied to figure out the collaborative relationship networks of countries/regions, organizations, and authors, the bibliographic coupling analysis of documents, and the detection and clusters of keywords. A basic and brief introduction of the Ising model, Voter model, Majority model, Sznajd model, and Bounded confidence model are also given. This paper is propaedeutic material for readers who want to grasp the fundamental knowledge of opinion dynamics on what it is, how it develops, what the hotspots are, which articles to read, and what the cooperative relationship looks like.},
  archive      = {J_APIN},
  author       = {Li, Yang and Xu, Zeshui},
  doi          = {10.1007/s10489-022-04368-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16540-16559},
  shortjournal = {Appl. Intell.},
  title        = {A bibliometric analysis and basic model introduction of opinion dynamics},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rapid extraction of skin physiological parameters from
hyperspectral images using machine learning. <em>APIN</em>,
<em>53</em>(13), 16519–16539. (<a
href="https://doi.org/10.1007/s10489-022-04327-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noninvasive assessment of skin structure using hyperspectral images has been intensively studied in recent years. Due to the high computational cost of the classical methods, such as the inverse Monte Carlo (IMC), much research has been done with the aim of using machine learning (ML) methods to reduce the time required for estimating parameters. This study aims to evaluate the accuracy and the estimation speed of the ML methods for this purpose and compare them to the traditionally used inverse adding-doubling (IAD) algorithm. We trained three models – an artificial neural network (ANN), a 1D convolutional neural network (CNN), and a random forests (RF) model – to predict seven skin parameters. The models were trained on simulated data computed using the adding-doubling algorithm. To improve predictive performance, we introduced a stacked dynamic weighting (SDW) model combining the predictions of all three individually trained models. SDW model was trained by using only a handful of real-world spectra on top of the ANN, CNN and RF models that were trained using simulated data. Models were evaluated based on the estimated parameters’ mean absolute error (MAE), considering the surface inclination angle and comparing skin spectra with spectra fitted by the IAD algorithm. On simulated data, the lowest MAE was achieved by the RF model (0.0030), while the SDW model achieved the lowest MAE on in vivo measured spectra (0.0113). The shortest time to estimate parameters for a single spectrum was 93.05 μs. Results suggest that ML algorithms can produce accurate estimates of human skin optical parameters in near real-time.},
  archive      = {J_APIN},
  author       = {Manojlović, Teo and Tomanič, Tadej and Štajduhar, Ivan and Milanič, Matija},
  doi          = {10.1007/s10489-022-04327-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16519-16539},
  shortjournal = {Appl. Intell.},
  title        = {Rapid extraction of skin physiological parameters from hyperspectral images using machine learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A noise robust kernel fuzzy clustering based on picture
fuzzy sets and KL divergence measure for MRI image segmentation.
<em>APIN</em>, <em>53</em>(13), 16487–16518. (<a
href="https://doi.org/10.1007/s10489-022-04315-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image segmentation task becomes complex due to spatially distributed noise and vague boundary structure between the regions. For handling the vagueness present in images, fuzzy set theory-based clustering is popular for segmentation. Recently, the picture fuzzy set theoretic clustering methods have been investigated for image segmentation in literature. Few works based on picture fuzzy clustering have been reported to handle the vagueness and noise present in the image during segmentation. In literature, most research work uses smoothing for handling noise in the segmentation, which results in the loss of fine structure of images. Moreover, the non-linearity present in the data is also not addressed well, resulting in the loss of crucial features. In this research work, we have presented a picture fuzzy set-based clustering method termed as kernel fuzzy clustering based on picture fuzzy sets and KL divergence measure (KFPKL) to address the problem of noise, vagueness, and non-linear structure present in images. The picture fuzzy set can handle the vagueness present in data. We have included a KL divergence measure-based term in the proposed optimization problem to dampen the effect of noise in the segmentation process. To capture the non-linear structure present in images, the kernel distance measure is used in the proposed optimization problem. The experiments have been carried out on several synthetic image datasets and two publicly available brain MRI datasets. The comparison with the state-of-the-art methods shows that the proposed approach provides better segmentation performance in terms of average segmentation accuracy and Dice score.},
  archive      = {J_APIN},
  author       = {Khatri, Inder and Kumar, Dhirendra and Gupta, Aaryan},
  doi          = {10.1007/s10489-022-04315-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16487-16518},
  shortjournal = {Appl. Intell.},
  title        = {A noise robust kernel fuzzy clustering based on picture fuzzy sets and KL divergence measure for MRI image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical framework integrating rapidly-exploring random
tree with deep reinforcement learning for autonomous vehicle.
<em>APIN</em>, <em>53</em>(13), 16473–16486. (<a
href="https://doi.org/10.1007/s10489-022-04358-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a systematic driving framework where the decision making module of reinforcement learning (RL) is integrated with rapidly-exploring random tree (RRT) as motion planning. RL is used to generate local goals and semantic speed commands to control the longitudinal speed of a vehicle while rewards are designed for the driving safety and the traffic efficiency. Guaranteeing the driving comfort, RRT returns a feasible path to be followed by the vehicle with the speed commands. The scene decomposition approach is implemented to scale the deep neural network (DNN) to environments with multiple traffic participants and double deep Q-networks (DDQN) with prioritized experience replay (PER) is utilized to accelerate the training process. To handle the disturbance of the perception of the agent, we use an ensemble of neural networks to evaluate the uncertainty of decisions. It has shown that the proposed framework can tackle unexpected actions of traffic participants at an intersection yielding safe, comfort and efficient driving behaviors. Also, the ensemble of DDQN with PER is proved to be superior over standard DDQN in terms of learning efficiency and disturbance vulnerability.},
  archive      = {J_APIN},
  author       = {Yu, Jiaxing and Arab, Aliasghar and Yi, Jingang and Pei, Xiaofei and Guo, Xuexun},
  doi          = {10.1007/s10489-022-04358-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16473-16486},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical framework integrating rapidly-exploring random tree with deep reinforcement learning for autonomous vehicle},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-step wind speed prediction based on an improved
multi-objective seagull optimization algorithm and a multi-kernel
extreme learning machine. <em>APIN</em>, <em>53</em>(13), 16445–16472.
(<a href="https://doi.org/10.1007/s10489-022-04312-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the large-scale integration of wind power into the power grid, improving the wind speed prediction accuracy is of great significance for promoting the consumption of renewable energy. In this paper, a hybrid prediction method for multi-step wind speed prediction based on the empirical wavelet transform (EWT), multi-objective modified seagull optimization algorithm (MOMSOA), and multi-kernel extreme learning machine (MKELM) is proposed. First, EWT is used to decompose the nonstationary wind speed data into a set of stationary subsequences. Then, each subsequence of wind speed is predicted by the MKELM, and the MKELM network is optimized by the MOMSOA newly proposed in this study. Finally, the inverse empirical wavelet transform (IEWT) is adopted to reconstruct the prediction results into the final wind speed prediction results. To assess the performance of the proposed combined model, four groups of experiments are carried out on four wind speed sequences, and a comparative analysis is made with 16 comparison models. The models involved in the investigation are discussed comprehensively in terms of significance and stability. The results demonstrate that the developed combined model outperforms the comparison models.},
  archive      = {J_APIN},
  author       = {Guo, Xiuting and Zhu, Changsheng and Hao, Jie and Zhang, Shengcai},
  doi          = {10.1007/s10489-022-04312-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16445-16472},
  shortjournal = {Appl. Intell.},
  title        = {Multi-step wind speed prediction based on an improved multi-objective seagull optimization algorithm and a multi-kernel extreme learning machine},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FIRE: Knowledge-enhanced recommendation with feature
interaction and intent-aware attention networks. <em>APIN</em>,
<em>53</em>(13), 16424–16444. (<a
href="https://doi.org/10.1007/s10489-022-04300-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the information overload issue and enhance the user experience of various web applications, recommender systems aim to better model user interests and preferences. Knowledge Graphs (KGs), consisting of real-world objective facts and fruitful entities, play a vital role in recommender systems. Recently, a technological trend has been to develop end-to-end Graph Neural Networks (GNNs)-based knowledge-aware recommendation (a.k.a., Knowledge Graph Recommendation, KGR) models. Unfortunately, current GNNs-based KGR approaches focus on how to capture high-order feature information on KGs while neglecting the following two crucial limitations: 1) The explicitly high-order feature interaction and fusion mechanism and 2) The valid user intent modelling mechanism. As such, these issues lead to insufficient user/item representation learning capability and unsatisfactory KGR performance. In this work, we present a novel Knowledge-enhanced Re commendation with F eature I nteraction and Intent-aware Attention Networks (FIRE) to address the latent intent modelling and high-order feature interaction deficiencies ignored by existing KGR methods. Based on the prototype user/item representation learning leveraging the GNNs-based approach, our model offers the following major improvements: One is the innovative use of Convolutional Neural Networks (CNNs) that perform vertical convolutional (a.k.a., bit-level convolutional) and horizontal convolutional (a.k.a., vector-level convolutional) processes to model multi-granular high-order feature interactions to enhance item-side representation learning. Another is to model users’ latent intent factors by utilizing a two-level attention mechanism (i.e., node- and intent-level attention mechanism) to enhance user-side representation learning. Extensive experiments on three KGs domain public datasets demonstrate that our method outperforms the existing state-of-the-art baseline. Last but not least, numerous ablation- and model studies demystify the working mechanism and elucidate the plausibility of the proposed model.},
  archive      = {J_APIN},
  author       = {Zhang, Ruoyi and Ma, Huifang and Li, Qingfeng and Wang, Yike and Li, Zhixin},
  doi          = {10.1007/s10489-022-04300-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16424-16444},
  shortjournal = {Appl. Intell.},
  title        = {FIRE: Knowledge-enhanced recommendation with feature interaction and intent-aware attention networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVID vaccine stigma: Detecting stigma across social media
platforms with computational model based on deep learning.
<em>APIN</em>, <em>53</em>(13), 16398–16423. (<a
href="https://doi.org/10.1007/s10489-022-04311-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study presents the first computational model of COVID vaccine stigma that can identify stigmatised sentiment with a high level of accuracy and generalises well across a number of social media platforms. The aim of the study is to understand the lexical features that are prevalent in COVID vaccine discourse and disputes between anti-vaccine and pro-vaccine groups. This should provide better insight for healthcare authorities, enabling them to better navigate those discussions. The study collected posts and their comments related to COVID vaccine sentiment in English, from Reddit, Twitter, and YouTube, for the period from April 2020 to March 2021. The labels used in the model, “stigma”, “not stigma”, and “undefined”, were collected from a smaller Facebook (Meta) dataset and successfully propagated into a larger dataset from Reddit, Twitter, and YouTube. The success of the propagation task and consequent classification is a result of state-of-the-art annotation scheme and annotated dataset. Deep learning and pre-trained word vector embedding significantly outperformed traditional algorithms, according to two-tailed P(T≤t) test and achieved F1 score of 0.794 on the classification task with three classes. Stigmatised text in COVID anti-vaccine discourse is characterised by high levels of subjectivity, negative sentiment, anxiety, anger, risk, and healthcare references. After the first half of 2020, anti-vaccination stigma sentiment appears often in comments to posts attempting to disprove COVID vaccine conspiracy theories. This is inconsonant with previous research findings, where anti-vaccine people stayed primarily within their own in-group discussions. This shift in the behaviour of the anti-vaccine movement from affirming climates to ones with opposing opinions will be discussed and elaborated further in the study.},
  archive      = {J_APIN},
  author       = {Straton, Nadiya},
  doi          = {10.1007/s10489-022-04311-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16398-16423},
  shortjournal = {Appl. Intell.},
  title        = {COVID vaccine stigma: Detecting stigma across social media platforms with computational model based on deep learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic linguistic evolutionary game with risk
perception in applications to carbon emission reduction decision making.
<em>APIN</em>, <em>53</em>(13), 16381–16397. (<a
href="https://doi.org/10.1007/s10489-022-04340-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon emission reduction, an effective way to facilitate carbon neutrality, has gained increasing attention in government policy and scientific research. However, the establishment of a sustainable carbon emission reduction market is a complex game between governments and enterprises. In addition, it is difficult to obtain precise evaluations of the political and environmental factors in most cases. Irrational enterprises with a profit-seeking nature bring challenges to the strategy selection. To bridge this gap, we propose a probabilistic linguistic evolutionary game to model strategic behavior in carbon emission reduction assistant decision making. First, we introduce a probabilistic linguistic payoff matrix to describe the uncertain payoffs of players. A new distance measure for the probabilistic variables is also proposed to construct the prospect payoff matrix in the prospect theory framework. Then, the evolutionary dynamics and the probabilistic linguistic evolutionary stability of the proposed methods are analyzed. A comprehensive case study for carbon emission reduction with comparisons is presented for validation.},
  archive      = {J_APIN},
  author       = {Hao, Zhinan and Wang, Xiang and Zhang, Yaojia and Zhang, Ren},
  doi          = {10.1007/s10489-022-04340-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16381-16397},
  shortjournal = {Appl. Intell.},
  title        = {Probabilistic linguistic evolutionary game with risk perception in applications to carbon emission reduction decision making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The efficient-CapsNet model for facial expression
recognition. <em>APIN</em>, <em>53</em>(13), 16367–16380. (<a
href="https://doi.org/10.1007/s10489-022-04349-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has attracted much attention lately. However, the current methods are concerned primarily with recognition accuracy, while ignoring efficiency. Efficient-CapsNet, which employs deep separable convolution operations based on CapsNet, has low network parameters and high network training efficiency while ensuring recognition accuracy. Using three public datasets, JAFFE, CK+, and FER2013, we comprehensively compared the recognition accuracy and training efficiency of Efficient-CapsNet and CapsNet. Results showed that the Efficient-CapsNet’s recognition accuracy reached 99.13%, 93.07%, and 72.94%, respectively, which is superior to most of the latest methods. In terms of training efficiency, the training time of a single image of Efficient-CapsNet under 64x64 size input and 48x48 size input is only 0.125ms and 0.033ms, respectively, which is 1454.28 times and 2730.03 times faster than CapsNet, respectively. Results also suggest that the training efficiency of Efficient-CapsNet is affected by the sample size. When the sample size grows, the training efficiency gradually slows down until it stabilizes.},
  archive      = {J_APIN},
  author       = {Wang, Kunxia and He, Ruixiang and Wang, Shu and Liu, Li and Yamauchi, Takashi},
  doi          = {10.1007/s10489-022-04349-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16367-16380},
  shortjournal = {Appl. Intell.},
  title        = {The efficient-CapsNet model for facial expression recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online reviews-oriented hotel selection: A large-scale group
decision-making method based on the expectations of decision makers.
<em>APIN</em>, <em>53</em>(13), 16347–16366. (<a
href="https://doi.org/10.1007/s10489-022-04273-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As there are an increasing number of people who prefer to travel as a group, hotel selection during group travel has become a problem worth investigating. From the large-scale group decision-making perspective, a method for hotel selection based on online reviews and the expectation of decision makers is proposed in this research. This method mainly includes two stages: constructing an evaluation matrix and large-scale group decision-making for hotel selection. In the first stage, the evaluation of alternative hotels is obtained from the sentiment analysis result of online reviews for hotels. In the second stage, according to the expectations of decision makers and prospect theory, the prospect value for alternative hotels can be obtained. Then, a large-scale group decision-making model based on the prospect value is proposed. In this model, a novel clustering method based on the similarity among decision makers is provided. Additionally, a feedback mechanism for decision makers to adjust expectations is presented in this research. Finally, the viability of the proposed method is demonstrated by an illustrative example in which hotel selection is conducted through TripAdvisor.com . The effectiveness of the proposed method has been demonstrated through extensive simulation experiments. The comparative analysis highlights the features of the method proposed in this paper.},
  archive      = {J_APIN},
  author       = {Guo, Jie and Liang, Xia and Wang, Lei},
  doi          = {10.1007/s10489-022-04273-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {13},
  pages        = {16347-16366},
  shortjournal = {Appl. Intell.},
  title        = {Online reviews-oriented hotel selection: A large-scale group decision-making method based on the expectations of decision makers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Differential evolution with variable leader-adjoint
populations. <em>APIN</em>, <em>53</em>(12), 15580–15602. (<a
href="https://doi.org/10.1007/s10489-022-04290-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of differential evolution (DE) is significantly affected by the selection of mutation strategies and control parameters. Inappropriate selection may lead to premature convergence and stagnation. Therefore, selecting appropriate mutation strategies and control parameters has always been a challenging task. In this paper, a differential evolution with variable leader-adjoint populations (LADE) is proposed. In LADE, a leader-adjoint model is used to divide the population in each generation into leader population and adjoint population. The leader population adopts a novel DE/current-best-rand/1 mutation strategy that can enhance the exploitation ability and avoid evolution stagnation. The adjoint population employs an improved DE/rand/1 mutation strategy that can not only strengthen the exploration ability, but also accelerate individual evolution by guiding the search process to the promising regions. Consequently, the leader-adjoint model can achieve a good balance between exploration and exploitation at different stages of evolution. Moreover, a parameter adaptation method is utilized to dynamically adjust the values of control parameters. To verify the performance of LADE, numerical experiments on the CEC2014 benchmark functions and Lennard-Jones potential real-world problem are executed. Experiment results show that the proposed LADE is significantly better than, or at least comparable to recent and advanced algorithms. In addition, experiments evaluate and analyze the effect of control parameters on the algorithm.},
  archive      = {J_APIN},
  author       = {Li, Yuzhen and Wang, Shihao and Yang, Hongyu and Chen, Hu},
  doi          = {10.1007/s10489-022-04290-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15580-15602},
  shortjournal = {Appl. Intell.},
  title        = {Differential evolution with variable leader-adjoint populations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3WC-d: A feature distribution-based adaptive three-way
clustering method. <em>APIN</em>, <em>53</em>(12), 15561–15579. (<a
href="https://doi.org/10.1007/s10489-022-04332-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a significant unsupervised learning method in the machine learning field, which can mine the distribution pattern and attribute of data. However, traditional clustering methods can not fully represent the attribution relationship between objects and classes. Therefore, a three-way clustering (3WC), which combines three-way decision (3WD) with clustering, has gradually received widespread attention from researchers in recent years. However, existing 3WC methods mostly use traditional clustering results or randomly assigned results as initial division results, which largely ignore the distribution relation of each object. Moreover, most of 3WC methods are soft clustering, i.e., there are some objects that will belong to more than one class, which makes clustering results more ambiguous. In light of this situation, we establish a feature distribution-based adaptive three-way clustering (3WC-D) method to address the above challenge. First, 3WC-D utilizes 3WD to characterize the distribution relation of objects for obtaining initial clustering results. Then, several representative classes are selected for further processing based on the interrelationship among classes in initial clustering results. Finally, the remaining objects are divided according to the relative relation between objects and classes, so as final clustering results can be obtained, and the effectiveness of the method is illustrated by comparing with several clustering methods on diverse datasets.},
  archive      = {J_APIN},
  author       = {Zhang, Rongtao and Ma, Xueling and Zhan, Jianming and Yao, Yiyu},
  doi          = {10.1007/s10489-022-04332-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15561-15579},
  shortjournal = {Appl. Intell.},
  title        = {3WC-D: A feature distribution-based adaptive three-way clustering method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KnAC: An approach for enhancing cluster analysis with
background knowledge and explanations. <em>APIN</em>, <em>53</em>(12),
15537–15560. (<a
href="https://doi.org/10.1007/s10489-022-04310-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern discovery in multidimensional data sets has been the subject of research for decades. There exists a wide spectrum of clustering algorithms that can be used for this purpose. However, their practical applications share a common post-clustering phase, which concerns expert-based interpretation and analysis of the obtained results. We argue that this can be the bottleneck in the process, especially in cases where domain knowledge exists prior to clustering. Such a situation requires not only a proper analysis of automatically discovered clusters but also conformance checking with existing knowledge. In this work, we present Knowledge Augmented Clustering (KnAC). Its main goal is to confront expert-based labelling with automated clustering for the sake of updating and refining the former. Our solution is not restricted to any existing clustering algorithm. Instead, KnAC can serve as an augmentation of an arbitrary clustering algorithm, making the approach robust and a model-agnostic improvement of any state-of-the-art clustering method. We demonstrate the feasibility of our method on artificially, reproducible examples and in a real life use case scenario. In both cases, we achieved better results than classic clustering algorithms without augmentation.},
  archive      = {J_APIN},
  author       = {Bobek, Szymon and Kuk, Michał and Brzegowski, Jakub and Brzychczy, Edyta and Nalepa, Grzegorz J.},
  doi          = {10.1007/s10489-022-04310-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15537-15560},
  shortjournal = {Appl. Intell.},
  title        = {KnAC: An approach for enhancing cluster analysis with background knowledge and explanations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Exploiting semantic-level affinities with a mask-guided
network for temporal action proposal in videos. <em>APIN</em>,
<em>53</em>(12), 15516–15536. (<a
href="https://doi.org/10.1007/s10489-022-04261-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal (TAP) aims to detect the action instances’ starting and ending times in untrimmed videos, which is fundamental and critical for large-scale video analysis and human action understanding. The main challenge of the temporal action proposal lies in modeling representative temporal relations in long untrimmed videos. Existing state-of-the-art methods achieve temporal modeling by building local-level, proposal-level, or global-level temporal dependencies. Local methods lack a wider receptive field, while proposal and global methods lack the focalization of learning action frames and contain background distractions. In this paper, we propose that learning semantic-level affinities can capture more practical information. Specifically, by modeling semantic associations between frames and action units, action segments (foregrounds) can aggregate supportive cues from other co-occurring actions, and nonaction clips (backgrounds) can learn the discriminations between them and action frames. To this end, we propose a novel framework named the Mask-Guided Network (MGNet) to build semantic-level temporal associations for the TAP task. Specifically, we first propose a Foreground Mask Generation (FMG) module to adaptively generate the foreground mask, representing the locations of the action units throughout the video. Second, we design a Mask-Guided Transformer (MGT) by exploiting the foreground mask to guide the self-attention mechanism to focus on and calculate semantic affinities with the foreground frames. Finally, these two modules are jointly explored in a unified framework. MGNet models the intra-semantic similarities for foregrounds, extracting supportive action cues for boundary refinement; it also builds the inter-semantic distances for backgrounds, providing the semantic gaps to suppress false positives and distractions. Extensive experiments are conducted on two challenging datasets, ActivityNet-1.3 and THUMOS14, and the results demonstrate that our method achieves superior performance.},
  archive      = {J_APIN},
  author       = {Yang, Yu and Wang, Mengmeng and Mei, Jianbiao and Liu, Yong},
  doi          = {10.1007/s10489-022-04261-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15516-15536},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting semantic-level affinities with a mask-guided network for temporal action proposal in videos},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Firefighting multi strategy marine predators algorithm for
the early-stage forest fire rescue problem. <em>APIN</em>,
<em>53</em>(12), 15496–15515. (<a
href="https://doi.org/10.1007/s10489-022-04265-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a firefighting multi strategy marine predators algorithm(FMMPA), which uses opposition-based learning (OBL) to make the initial population more uniform, introduces adaptive weight factors to balance exploration ability and exploitation ability, and combines DE/RAND/1 mutation mechanism to increase population diversity. Another purpose of our FMMPA is to add fire weighted selection, including flame edge suppression, combustion unit continuity and wind direction, as the rescue algorithm of forest fire rescue assemble to deal with the allocation of firefighting aircraft in forest fire rescue. We conduct this study based on real maps and the capabilities of the real firefighting aircraft. Compared with ten algorithms, the experimental results show that the proposed FMMPA has superior performance in reducing rescue time, controlling the spread speed of fire edge and minimizing loss cost.},
  archive      = {J_APIN},
  author       = {Chen, Jiaming and Luo, Qifang and Zhou, Yongquan and Huang, Huajuan},
  doi          = {10.1007/s10489-022-04265-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15496-15515},
  shortjournal = {Appl. Intell.},
  title        = {Firefighting multi strategy marine predators algorithm for the early-stage forest fire rescue problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Homogeneous ensemble extreme learning machine autoencoder
with mutual representation learning and manifold regularization for
medical datasets. <em>APIN</em>, <em>53</em>(12), 15476–15495. (<a
href="https://doi.org/10.1007/s10489-022-04284-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a single learner, extreme learning machine autoencoder (ELM-AE) and generalized extreme learning machine autoencoder (GELM-AE) have limited ability to learn high-dimensional complex data features because high-dimensional data contains more complex and rich discriminative information. GELM-AE only pays attention to the internal relationship of each data subset for dimensionality reduction, ignoring the relationship between different subsets. This paper proposes the homogeneous ensemble extreme learning machine autoencoder (HeELM-AE) to extract high-dimensional complex data diversity features. This method combines the ideas of ensemble feature learning and mutual representation matrix learning. Multiple data subsets are constructed from the original high-dimensional complex dataset with feature learning methods. Generalized extreme learning machine autoencoder(GELM-AE) is used as a base dimension reducer to learn rich discriminative information from highly redundant features. Mutual representation learning methods can characterize correlations between different data subsets and the local manifold structure inherent in different data subsets is maintained through manifold regularization at the same time. Extensive comparative experiments on medical datasets show that compared with other ensemble feature learning models, HeELM-AE is an efficient and accurate model. Finally, visual analysis is used to explain the working mechanism of each stage of HeELM-AE and explore feature learning model interpretability.},
  archive      = {J_APIN},
  author       = {Chen, Wenjian and Chen, Xiaoyun and Lin, Yanming},
  doi          = {10.1007/s10489-022-04284-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15476-15495},
  shortjournal = {Appl. Intell.},
  title        = {Homogeneous ensemble extreme learning machine autoencoder with mutual representation learning and manifold regularization for medical datasets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rule reductions of decision formal context based on mixed
information. <em>APIN</em>, <em>53</em>(12), 15459–15475. (<a
href="https://doi.org/10.1007/s10489-022-04194-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal concept analysis is an effective tool in knowledge discovery. Nonetheless, it ignores the negative information, especially in rule extraction. And some applications require to mix positive and negative information for management and representation explicitly. This paper discusses an emerging method for rule reduction in formal contexts with mixed information by presenting a mixed decision rule based on a mixed concept lattice. Based on the mixed concept model, we discuss the relationships between mixed concept, formal concept and three-way concept, respectively. In this paper, to fully consider the mixed information, we construct a mixed decision rule. Moreover, to reduce the redundance of rule, a novel approach for weak-basis from the viewpoint of granular computing is designed to selection the core mixed decision rules and necessary mixed decision rules. Finally, the comparison of mixed decision rules and three-way decision rules is perfectly discussed, and case study is presented for the difference.},
  archive      = {J_APIN},
  author       = {Huang, Ju and Lin, Yidong and Li, Jinjin},
  doi          = {10.1007/s10489-022-04194-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15459-15475},
  shortjournal = {Appl. Intell.},
  title        = {Rule reductions of decision formal context based on mixed information},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based image exposure reconstruction
for homography estimation. <em>APIN</em>, <em>53</em>(12), 15442–15458.
(<a href="https://doi.org/10.1007/s10489-022-04287-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The homography matrix plays a vital role in robotics and computer vision applications, but mainstream estimators are usually customized for specific problems and are sensitive to image quality. In response to this situation, a reinforced agent is proposed to improve image quality by sequentially reconstructing the exposure. First, the gamma correction theory is employed to design a nonlinear exposure adjustment function so that the agent’s action is not bound to additional hardware or software. Then, the agent is designed as consisting of a metric network and a Q network that are trained under the reinforcement learning framework. When a black-box nondifferentiable homography estimator is given, the metric network can map the image into its corresponding embedding space, and the Q network can further determine an exposure value to produce pleasing images for it. Comprehensive experiments are conducted on homography samples generated from the public aerial DOTASet. After reconstructing the exposure of the original input, all selected estimators can obtain more accurate results. It also reveals that visually satisfactory images may not always be the best choice for homography estimation.},
  archive      = {J_APIN},
  author       = {Lin, Yijun and Wu, Fengge and Zhao, Junsuo},
  doi          = {10.1007/s10489-022-04287-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15442-15458},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning-based image exposure reconstruction for homography estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SVM ensemble training for imbalanced data classification
using multi-objective optimization techniques. <em>APIN</em>,
<em>53</em>(12), 15424–15441. (<a
href="https://doi.org/10.1007/s10489-022-04291-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main problems with classifier training for imbalanced data is defining the correct learning criterion. On the one hand, we want the minority class to be correctly recognized, and on the other hand, we do not want to make too many mistakes in the majority class. Commonly used metrics focus either on the predictive quality of the distinguished class or propose an aggregation of simple metrics. The aggregate metrics, such as Gmean or AUC, are primarily ambiguous, i.e., they do not indicate the specific values of errors made on the minority or majority class. Additionally, improper use of aggregate metrics results in solutions selected with their help that may favor the majority class. The authors realize that a solution to this problem is using overall risk. However, this requires knowledge of the costs associated with errors made between classes, which is often unavailable. Hence, this paper will propose the semoos algorithm - an approach based on multi-objective optimization that optimizes criteria related to the prediction quality of both minority and majority classes. semoos returns a pool of non-dominated solutions from which the user can choose the model that best suits him. Automatic solution selection formulas with a so-called Pareto front have also been proposed to compare state-of-the-art methods. The proposed approach will train a svm classifier ensemble dedicated to the imbalanced data classification task. The experimental evaluations carried out on a large number of benchmark datasets confirm its usefulness.},
  archive      = {J_APIN},
  author       = {Grzyb, Joanna and Woźniak, Michał},
  doi          = {10.1007/s10489-022-04291-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15424-15441},
  shortjournal = {Appl. Intell.},
  title        = {SVM ensemble training for imbalanced data classification using multi-objective optimization techniques},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGC-ARANet: Scale-wise global contextual axile reverse
attention network for automatic brain tumor segmentation. <em>APIN</em>,
<em>53</em>(12), 15407–15423. (<a
href="https://doi.org/10.1007/s10489-022-04209-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated brain tumour segmentation using magnetic resonance imaging (MRI) is essential for clinical decision-making and surgical planning. Numerous studies have demonstrated the feasibility of segmenting brain tumours using deep learning models such as U-shaped architectures. Unfortunately, due to the diversity of tumors and complex boundaries, it is insufficient to obtain contextual data on tumor cells and their surroundings from a single stage. To overcome this limitation, we proposed a Scale-wise Global Contextual Axile Reverse Attention Network (SGC-ARANet) consisting of four modules that improve segmentation performance. We begin by creating three global multi-level guidance (GMLG) modules to provide various levels of global contextual data. Additionally, we develop a scale-wise multi-level blend module (SWMB) that dynamically blends multi-scale contextual data with high-level features. Following that, we demonstrated how a partial decoder (PD) connected in parallel to the encoder is utilized to aggregate high-level and SWMB feature maps to create a global map. The axile reverse attention (ARA) module is then presented to simulate multi-modality tumor regions and boundaries using global and GMLG feature maps. We evaluate our model using the publicly available BraTS 2019 and 2020 brain tumor segmentation datasets. The results indicate that our SGC-ARANet is competitive or outperforms numerous State-of-the-art (SOTA) algorithms for several segmentation measures.},
  archive      = {J_APIN},
  author       = {Karri, Meghana and Annvarapu, Chandra Sekhara Rao and Acharya, U Rajendra},
  doi          = {10.1007/s10489-022-04209-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15407-15423},
  shortjournal = {Appl. Intell.},
  title        = {SGC-ARANet: Scale-wise global contextual axile reverse attention network for automatic brain tumor segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Deformable graph convolutional transformer for
skeleton-based action recognition. <em>APIN</em>, <em>53</em>(12),
15390–15406. (<a
href="https://doi.org/10.1007/s10489-022-04302-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical problem in skeleton-based action recognition is to extract high-level semantics from dynamic changes between skeleton joints. Therefore, Graph Convolutional Networks (GCNs) are widely applied to capture the spatial-temporal information of dynamic joint coordinates by graph-based convolution. However, previous GCNS with fixed graph convolution kernel are limited to the static topology of graphs and the geometric variations of actions. Moreover, the local information of adjacent nodes of the graph is aggregated layer by layer, which increases the model complexity. In this work, a Deformable Graph Convolutional Transformer (DGT) for skeleton-based action recognition is proposed to extract adaptive features via a flexible receptive field that is learnable. In our DGT model, a multiple-input-branches (MIB) architecture is adopted to obtain multiple information, such as joints, bones, and motions. The multiple features are fused in the Transformer Classifier. Then, the Spatial-Temporal Graph Convolution units (STGC) are used to learn a preliminary feature representation indicating both spatial and temporal dependencies on the graph. Next, a Deformable spatial-temporal compound attention backbone is followed, which learns to represent a robust feature via adaptive deformable skeleton features. The adaptive representation is obtained by dynamically adjusting its receptive field owing to the offset-based convolution method. In addition, a self-attention-based transformer classifier (TC) is designed to encode the sequence of features flattened on the spatial and temporal dimensions. The fully-connected attention mechanism further helps the high-level semantic representation by focusing on essential nodes in the graph. We evaluated DGT on two challenging large-scale datasets, NTU-RGBD 60 and NTU-RGBD 120. Experiment results support the efficacy of DGT to optimize the attention for different joints adaptively. A comparable performance but much more efficient than the state-of-the-art demonstrates the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Chen, Shuo and Xu, Ke and Zhu, Bo and Jiang, Xinghao and Sun, Tanfeng},
  doi          = {10.1007/s10489-022-04302-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15390-15406},
  shortjournal = {Appl. Intell.},
  title        = {Deformable graph convolutional transformer for skeleton-based action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A q-based policy gradient optimization approach for
doudizhu. <em>APIN</em>, <em>53</em>(12), 15372–15389. (<a
href="https://doi.org/10.1007/s10489-022-04281-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has recently been employed in various games, with which superhuman intelligence has been achieved, including Atari, Go, no-limit, and Texas hold’em. However, this technique has not been fully considered for Doudizhu which is a popular poker game in Asia and involves confrontation and cooperation among multiple players with imperfect information. In this paper we present a new deep reinforcement learning approach NV-Dou for the game Doudizhu. It adopts a variant of neural fictitious self-play to approximate the Nash equilibria of the game. The loss functions of the neural network integrate Q-Based policy gradient (mean actor critic) with advantage learning and proximal policy optimization. In addition, parametric noises are adopted for the fully connected layers in the neural network. The experimental results show that it needs only a few hours of training and achieves almost state-of-the-art performance comparing with the well-known open implementations RHCP, CQL, MCTS and others for Doudizhu.},
  archive      = {J_APIN},
  author       = {Yu, Xiaomin and Wang, Yisong and Qin, Jin and Chen, Panfeng},
  doi          = {10.1007/s10489-022-04281-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15372-15389},
  shortjournal = {Appl. Intell.},
  title        = {A Q-based policy gradient optimization approach for doudizhu},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic preserving asymmetric discrete hashing for
cross-modal retrieval. <em>APIN</em>, <em>53</em>(12), 15352–15371. (<a
href="https://doi.org/10.1007/s10489-022-04282-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hashing technologies have garnered substantial attention and achieved notable results due to their low storage costs and excellent retrieval efficiency. However, the majority of existing approaches build a massive pairwise similarity matrix to maintain the similarity relationship in the original space, which can easily produce huge time-space overhead and lose the class information, making these approaches unscalable to large-scale multimedia datasets. Additionally, the majority of cross-modal techniques concurrently learn the hash function and binary representations, which makes optimization more difficult. To tackle these issues, we developed a hashing approach called Semantic preserving Asymmetric discrete Hashing for cross-modal retrieval (SEAH), which aims to preserve the similarity metric based on the global semantic information and the local similarity structure. Specifically, SEAH adopts an asymmetric learning scheme and embeds class attribute information to boost the discriminating strength of the learned binary codes. Then, SEAH employs a well-designed optimization algorithm to achieve efficient iterative optimization, thus avoiding the quantization error problem. In addition, the proposed SEAH is a two-stage approach; two algorithms, SEAH-t and SEAH-s, are developed in the second stage. The first one adopts linear classifiers as hash functions, while the second is a semantic-enhanced strategy utilizing distance-distance difference minimization to improve the ability of the to-be-learnedhash functions. Extensive experiments on three frequently used benchmark datasets highlight that the proposed SEAH-t and SEAH-s are not only superior to several state-of-the-art approaches but also retain their query and storage efficiency.},
  archive      = {J_APIN},
  author       = {Yang, Fan and Zhang, Qiao-xi and Ding, Xiao-jian and Ma, Fu-min and Cao, Jie and Tong, De-yu},
  doi          = {10.1007/s10489-022-04282-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15352-15371},
  shortjournal = {Appl. Intell.},
  title        = {Semantic preserving asymmetric discrete hashing for cross-modal retrieval},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Least squares structural twin bounded support vector machine
on class scatter. <em>APIN</em>, <em>53</em>(12), 15321–15351. (<a
href="https://doi.org/10.1007/s10489-022-04237-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several projects and application development teams are spending their precious time and energy in the field of classification and regression. So, the main target of the proposed model is to develop a computationally fast and efficient algorithm for binary classification problems, which also provides better generalization in various realistic applications. As we know that support vector machines (SVMs) obtain better generalization by considering the structural risk minimization (SRM) principle but suffers from computational cost and the twin version of SVM (TWSVM) finds faster learning time by following the empirical risk minimization (ERM) principle that compromises with the generalization. However, the impact of class distributions has not been discussed in either classical SVM or conventional TWSVM. To address this issue majorly, Peng and Xu, (2013) have proposed an approach named minimum class variance TWSVM (RMCV-TWSVM), which deals with the class information by considering a model of data uncertainty but missed out between class information. Here, we propose an efficient approach that improves the generalization performance and effectively handles the computational burden to follows the least-squares approach by incorporating the total within-class and between-class information for binary classification named least squares structural twin bounded support vector machine on class scatter (LS-STBSVM). All the computational results of approaches on several important benchmark UCI real-world datasets as well as KEEL artificial datasets by using a linear and non-linear kernel have been analyzed which shows that the proposed approach LS-STBSVM has a significant impact on both computational cost and generalization ability over other classification approaches.},
  archive      = {J_APIN},
  author       = {Gupta, Umesh and Gupta, Deepak},
  doi          = {10.1007/s10489-022-04237-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15321-15351},
  shortjournal = {Appl. Intell.},
  title        = {Least squares structural twin bounded support vector machine on class scatter},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mutual attention based multimodal fusion for fake news
detection on social network. <em>APIN</em>, <em>53</em>(12),
15311–15320. (<a
href="https://doi.org/10.1007/s10489-022-04266-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the advance of social networks, the emergency of fake news has been the major threat for information security, privacy, and trustworthiness. The fake news can leverage multimedia contents to fabricate evidences or mislead readers, which damages a lot in machine learning and network systems. In this work, we explored the task of multimodal fake news detection. The major challenge of fake news detection stems from the modality fusion by abundant information. Overcoming the limitations of the current models, we tackle the challenge of learning corrections between modalities in news, and substantially proposed a mutual attention neural network (MANN) that can learn the relationship between each different modality. Our model consists of four components: multimodal feature extractor, mutual attention fusion, fake news detector and irrelevant event discriminator. The performance of our proposed architecture is evaluated on Weibo dataset, which indicates the MANN model outperforms the state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Guo, Ying},
  doi          = {10.1007/s10489-022-04266-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15311-15320},
  shortjournal = {Appl. Intell.},
  title        = {A mutual attention based multimodal fusion for fake news detection on social network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A document image classification system fusing deep and
machine learning models. <em>APIN</em>, <em>53</em>(12), 15295–15310.
(<a href="https://doi.org/10.1007/s10489-022-04306-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) technologies are now widely employed to overcome human-induced faults in a variety of systems used in our daily lives, thanks to the digital transformation.One example of such systems is online document tracking systems (DTS). The DTS’s reliability and preferability are enhanced by automatic document classification and understanding features. Although automatic document classification systems can assist humans in document understanding tasks, most of of them are not designed to function with Portable Document Format (PDF), which contains text, tables or figures. In this study, we investigate separate ways to efficiently classify student documents that are uploaded in PDF format and are required for university education. We propose three possible techniques for this issue. The first approach is based on Optical Character Recognition (OCR) and traditional machine learning methods. The second is purely on deep learning. The third one is based on fusion of deep learning methods based on entropy. The proposed techniques can classify twelve distinct types of digital documents. The validity of the proposed methods has been verified by student affairs department of Kocaeli University in Turkey. The system has not only increased the efficiency of online document uploading steps for students, but also reduced the human cost for tracking the documents. The highest F-score (94.45%) is obtained by the ensemble of EfficientNetB3 and ExtraTree.},
  archive      = {J_APIN},
  author       = {Omurca, Sevinç İlhan and Ekinci, Ekin and Sevim, Semih and Edinç, Eren Berk and Eken, Süleyman and Sayar, Ahmet},
  doi          = {10.1007/s10489-022-04306-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15295-15310},
  shortjournal = {Appl. Intell.},
  title        = {A document image classification system fusing deep and machine learning models},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-hemisphere asymmetric attention network: Recognizing
emotion from EEG signals based on the transformer. <em>APIN</em>,
<em>53</em>(12), 15278–15294. (<a
href="https://doi.org/10.1007/s10489-022-04228-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based emotion recognition is not only an important branch in the field of affective computing, but is also an indispensable task for harmonious human–computer interaction. Recently, many deep learning emotion recognition algorithms have achieved good results, but most of them have been based on convolutional and recurrent neural networks, resulting in complex model design, poor modeling of long-distance dependency, and the inability to parallelize computations. Here, we proposed a novel bi-hemispheric asymmetric attention network (Bi-AAN) combining a transformer structure with the asymmetric property of the brain’s emotional response. In this way, we modeled the difference of bi-hemispheric attention, and mined the long-term dependency between EEG sequences, which exacts more discriminative emotional representations. First, the differential entropy (DE) features of each frequency band were calculated using the DE-embedding block, and the spatial information between the electrode positions was extracted using positional encoding. Then, a bi-headed attention mechanism was employed to capture the intra-attention of frequency bands in each hemisphere and the attentional differences between the bi-hemispheric frequency bands. After carring out experiments both in DEAP and DREAMER datasets, we found that the proposed Bi-AAN achieved superior recognition performance as compared to state-of-the-art EEG emotion recognition methods.},
  archive      = {J_APIN},
  author       = {Zhong, Xinyue and Gu, Yun and Luo, Yutong and Zeng, Xiaomei and Liu, Guangyuan},
  doi          = {10.1007/s10489-022-04228-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15278-15294},
  shortjournal = {Appl. Intell.},
  title        = {Bi-hemisphere asymmetric attention network: Recognizing emotion from EEG signals based on the transformer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel context inconsistency elimination algorithm based on
the optimized dempster-shafer evidence theory for context-awareness
systems. <em>APIN</em>, <em>53</em>(12), 15261–15277. (<a
href="https://doi.org/10.1007/s10489-022-04223-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast advancement of the Internet of things (IoT), context-awareness systems (CASs) have been widely used in many different fields, such as digital home and smart healthcare. However, low quality of context (QoC) usually causes the CASs to make inappropriate decisions, therefore, the context inconsistency has become an urgent problem that needs to be resolved. Although many researchers have adopted some QoC parameters to solve this problem, they do not take sufficient account of the relationships between context information sources and the impacts of the uncertainty of context information on the credibility of context information sources. In this paper, different distance measurement methods are utilized to divide context information sources into credible context information sources and incredible context information sources, on this basis, the Deng entropy is introduced to construct a new discounting factor in order to assign different discounting factors for different kinds of context information sources and a novel context inconsistency elimination algorithm based on the optimized Dempster-Shafer (D-S) evidence theory is proposed. The experimental results demonstrate that the proposed algorithm based on the Cosine distance can obtain 94.33% context-judge rate under high precision configuration of sensors. Besides, under low precision configuration of sensors, compared to the correlation coefficient based on generalized information quality (CIQ)-weighted algorithm which has the highest context-judge rate among other inconsistency elimination algorithms, the proposed algorithm based on the Cosine distance can solve more inconsistent context information.},
  archive      = {J_APIN},
  author       = {Liu, Qiang and Xu, Hongji and He, Bo and Yuan, Hui and Liu, Zhi and Fan, Shidi and Xu, Jie and Li, Tiankuo and Li, Juan and Wang, Mengmeng and Li, Shijie},
  doi          = {10.1007/s10489-022-04223-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15261-15277},
  shortjournal = {Appl. Intell.},
  title        = {A novel context inconsistency elimination algorithm based on the optimized dempster-shafer evidence theory for context-awareness systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TranGRU: Focusing on both the local and global information
of molecules for molecular property prediction. <em>APIN</em>,
<em>53</em>(12), 15246–15260. (<a
href="https://doi.org/10.1007/s10489-022-04280-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction is an essential but challenging task in drug discovery. The recurrent neural network (RNN) and Transformer are the mainstream methods for sequence modeling, and both have been successfully applied independently for molecular property prediction. As the local information and global information of molecules are very important for molecular properties, we aim to integrate the bi-directional gated recurrent unit (BiGRU) into the original Transformer encoder, together with self-attention to better capture local and global molecular information simultaneously. To this end, we propose the TranGRU approach, which encodes the local and global information of molecules by using the BiGRU and self-attention, respectively. Then, we use a gate mechanism to reasonably fuse the two molecular representations. In this way, we enhance the ability of the proposed model to encode both local and global molecular information. Compared to the baselines and state-of-the-art methods when treating each task as a single-task classification on Tox21, the proposed approach outperforms the baselines on 9 out of 12 tasks and state-of-the-art methods on 5 out of 12 tasks. TranGRU also obtains the best ROC-AUC scores on BBBP, FDA, LogP, and Tox21 (multitask classification) and has a comparable performance on ToxCast, BACE, and ecoli. On the whole, TranGRU achieves better performance for molecular property prediction. The source code is available in GitHub: https://github.com/Jiangjing0122/TranGRU .},
  archive      = {J_APIN},
  author       = {Jiang, Jing and Zhang, Ruisheng and Ma, Jun and Liu, Yunwu and Yang, Enjie and Du, Shikang and Zhao, Zhili and Yuan, Yongna},
  doi          = {10.1007/s10489-022-04280-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15246-15260},
  shortjournal = {Appl. Intell.},
  title        = {TranGRU: Focusing on both the local and global information of molecules for molecular property prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A priority-aware scheduling framework for heterogeneous
workloads in container-based cloud. <em>APIN</em>, <em>53</em>(12),
15222–15245. (<a
href="https://doi.org/10.1007/s10489-022-04164-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the uncertainty of a cloud environment and the diversity of workload requirements increasing the scheduling cost of container-based cloud, especially for load spikes of application access, optimizing the utilization efficiency of cloud resources and quality of service is the focus of container cluster technology in the future. Different from traditional virtual machine-based scheduling, containerized applications of heterogeneous workloads bring higher scheduling complexity with its elastic scaling and multi-replicas operation. To tackle this problem, we propose a priority-aware workloads scheduling algorithm PA-CCWS. Firstly, we implement workload characterization and behavior identification, quantify the analysis results with TOPSIS method, generate the workloads priority and build priority scheduling buffer queue. Meanwhile, the model learning is accelerated by the experience replay mechanism that inserts and updates the priority of historical experience through the real-time feedback of actual container scheduling from DDQN. Then, we describe containerized applications oriented deep reinforcement learning scheduling algorithm which combined with the two kinds of priorities, to optimize scheduling decision. Finally, we evaluate the effectiveness of our algorithm in terms of resource utilization, resource imbalance degree and SLA compliance rate, etc. Compared with meta-heuristic algorithm PSOS, mathematical model-based algorithm KCSS and other excellent deep reinforcement learning based scheduling algorithms such as DeepRM-Plus and RLSched applying in the container-based cloud, PA-CCWS shows better resource utilization efficiency and convergence stability in containerized applications scheduling.},
  archive      = {J_APIN},
  author       = {Zhu, Lilu and Huang, Kai and Fu, Kun and Hu, Yanfeng and Wang, Yang},
  doi          = {10.1007/s10489-022-04164-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15222-15245},
  shortjournal = {Appl. Intell.},
  title        = {A priority-aware scheduling framework for heterogeneous workloads in container-based cloud},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BovdGFE: Buffer overflow vulnerability detection based on
graph feature extraction. <em>APIN</em>, <em>53</em>(12), 15204–15221.
(<a href="https://doi.org/10.1007/s10489-022-04214-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically detecting buffer overflow vulnerabilities is an important research topic in software security. Recent studies have shown that vulnerability detection performance utilizing deep learning-based techniques can be significantly enhanced. However, due to information loss during code representation, existing approaches cannot learn the features associated with vulnerabilities, leading to a high false negative rate (FNR) and low precision. To resolve the existing problems, we propose a method for buffer overflow vulnerability detection based on graph feature extraction (BovdGFE) in C/C++ programs. BovdGFE constructs the buffer overflow function samples. Then, we present a new representation structure, code representation sequence (CoRS), which incorporates the control flow, data dependencies, and syntax structure of the vulnerable code for reducing information loss during code representation. After the function samples are transformed into CoRS, a deep learning model is used to learn vulnerable features and perform vulnerability classification. The results of the experiments show that BovdGFE improves the precision and FNR by 6.3% and 3.9% respectively compared with state-of-the-art methods, which can significantly improve the capability of vulnerability detection.},
  archive      = {J_APIN},
  author       = {Lv, Xinghang and Peng, Tao and Chen, Jia and Liu, Junping and Hu, Xinrong and He, Ruhan and Jiang, Minghua and Cao, Wenli},
  doi          = {10.1007/s10489-022-04214-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15204-15221},
  shortjournal = {Appl. Intell.},
  title        = {BovdGFE: Buffer overflow vulnerability detection based on graph feature extraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From deterministic to stochastic: An interpretable
stochastic model-free reinforcement learning framework for portfolio
optimization. <em>APIN</em>, <em>53</em>(12), 15188–15203. (<a
href="https://doi.org/10.1007/s10489-022-04217-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental problem in algorithmic trading, portfolio optimization aims to maximize the cumulative return by continuously investing in various financial derivatives within a given time period. Recent years have witnessed the transformation from traditional machine learning trading algorithms to reinforcement learning algorithms due to their superior nature of sequential decision making. However, the exponential growth of the imperfect and noisy financial data that is supposedly leveraged by the deterministic strategy in reinforcement learning, makes it increasingly challenging for one to continuously obtain a profitable portfolio. Thus, in this work, we first reconstruct several deterministic and stochastic reinforcement algorithms as benchmarks. On this basis, we introduce a risk-aware reward function to balance the risk and return. Importantly, we propose a novel interpretable stochastic reinforcement learning framework which tailors a stochastic policy parameterized by Gaussian Mixtures and a distributional critic realized by quantiles for the problem of portfolio optimization. In our experiment, the proposed algorithm demonstrates its superior performance on U.S. market stocks with a 63.1% annual rate of return while at the same time reducing the market value max drawdown by 10% when back-testing during the stock market crash around March 2020.},
  archive      = {J_APIN},
  author       = {Song, Zitao and Wang, Yining and Qian, Pin and Song, Sifan and Coenen, Frans and Jiang, Zhengyong and Su, Jionglong},
  doi          = {10.1007/s10489-022-04217-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15188-15203},
  shortjournal = {Appl. Intell.},
  title        = {From deterministic to stochastic: An interpretable stochastic model-free reinforcement learning framework for portfolio optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small-sample size problems solving based on incremental
learning: An adaptive bayesian quadrature approach. <em>APIN</em>,
<em>53</em>(12), 15174–15187. (<a
href="https://doi.org/10.1007/s10489-022-04305-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving programming problems with objectives, we are often faced with the challenge of insufficient samples. And when new samples are generated, re-modeling based on historical and incremental samples is costly. Many Bayesian based approaches have been proposed to update the original models using newly coming samples which are called incremental learning. In this paper, we derive a calculation method of information value based on Bayesian quadrature technology, and use multi-start Adaptive Momentum (ADAM) stochastic gradient algorithm to design independent adaptive learning rates by computing the first-order moment and the second-order moment of the gradient then we propose a small-sample size problems solving based on incremental learning called Adaptive Bayesian Quadrature Approach (ABQA) We use a numerical validation experiment to verify the effectiveness of our approach, and then use two programming problems, shared bike system and newsvendor problem, to verify the effectiveness of our approach on incremental learning and small-samples size problems. In all these experiments, ABQA outperforms the benchmarks and state-of-the-art Bayesian algorithms. Performance and speed are improved by 2.8% and 8.6%, respectively.},
  archive      = {J_APIN},
  author       = {Feng, Yiding and Feng, Xiang and Yu, Huiqun},
  doi          = {10.1007/s10489-022-04305-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15174-15187},
  shortjournal = {Appl. Intell.},
  title        = {Small-sample size problems solving based on incremental learning: An adaptive bayesian quadrature approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid static-sensory data modeling for prediction tasks in
basic oxygen furnace process. <em>APIN</em>, <em>53</em>(12),
15163–15173. (<a
href="https://doi.org/10.1007/s10489-022-04293-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel data-driven prediction system for Multivariate Time Series (MTS) in an industrial context, where classic relational data contain keyinformation in order to properly interpret the MTS. Particularly we focus on the accurate endpoint prediction of temperature and chemical composition at the basic oxygen furnace, which is a step in the steel production pipeline where liquid iron is refined to steel. The precise prediction of temperature is important for proper process control while reaching the target chemical composition is essential for quality control. Our deep learning methodology employs two modules followed by an aggregation block; a Convolutional Neural Network (CNN) handles the MTS, while in parallel, the static data is processed by a Fully Connected Network (FCN). We enhance the CNN performance by adding two Squeeze-and-excitation (SE) blocks, which act like an attention module over the different channels. By taking the MTS data into account we improve the prediction by up to 10% relative over the models which only consider the static data. The hybrid FCN-CNN-SE architecture slightly improves the state-of-the-art MTS approaches by 2%, with less outliers on the prediction of final temperature and phosphorus concentration, while being easier to implement and more scalable to larger datasets and input space than current solutions.},
  archive      = {J_APIN},
  author       = {Sala, Davi Alberto and Van Yperen-De Deyne, Andy and Mannens, Erik and Jalalvand, Azarakhsh},
  doi          = {10.1007/s10489-022-04293-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15163-15173},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid static-sensory data modeling for prediction tasks in basic oxygen furnace process},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power-efficient gesture sensing for edge devices: Mimicking
fourier transforms with spiking neural networks. <em>APIN</em>,
<em>53</em>(12), 15147–15162. (<a
href="https://doi.org/10.1007/s10489-022-04258-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key design requirements for any portable/mobile device is low power. To enable such a low powered device, we propose an embedded gesture detection system that uses spiking neural networks (SNNs) applied directly to raw ADC data of a 60GHz frequency modulated continuous wave radar. SNNs can facilitate low power systems because they are sparse in time and space and are event-driven. The proposed system, as opposed to earlier state-of-the-art methods, relies solely on the target’s raw ADC data, thus avoiding the overhead of performing slow-time and fast-time Fourier transforms (FFTs) processing. The proposed architecture mimics the discrete Fourier transformation within the SNN itself avoiding the need for FFT accelerators and makes the FFT processing tailored to the specific application, in this case gesture sensing. The experimental results demonstrate that the proposed system is capable of classifying 8 different gestures with an accuracy of 98.7%. This result is comparable to the conventional approaches, yet it offers lower complexity, lower power consumption and faster computations comparable to the conventional approaches.},
  archive      = {J_APIN},
  author       = {Arsalan, Muhammad and Santra, Avik and Issakov, Vadim},
  doi          = {10.1007/s10489-022-04258-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15147-15162},
  shortjournal = {Appl. Intell.},
  title        = {Power-efficient gesture sensing for edge devices: Mimicking fourier transforms with spiking neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel group decision-making approach in multi-scale
environments. <em>APIN</em>, <em>53</em>(12), 15127–15146. (<a
href="https://doi.org/10.1007/s10489-022-04279-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of real decision-making problems, some experts try to use multi-scale information to express expert opinions in group decision-making problems. Facing the problem of group decision-making with multi-scale information, this paper attempts to explore a multi-scale group decision-making method consisting of two stages to provide theoretical support and methodological basis for establishing a multi-scale decision analysis system. In the first stage, we introduce a newly ranking decision-making approach based on a reflexive fuzzy α-neighborhood operator to deal with single-scale ranking problems with a single expert, which greatly enriches the ranking decision analysis theory. We also use a numerical example and experimental analysis to detect the stability and validity of the method. In the second stage, considering that the decision-making opinions of multiple experts may appear at the same time in the decision-making process, we propose the score labeling approach and the decision fusion approach to obtain the comprehensive decision-making result, which provides a feasible research idea for the comprehensive analysis of group decision-making results. Combining these two stages, a complete multi-scale group decision-making method in a multi-scale environment is described in detail, which can effectively deal with multi-scale group decision-making problems. Moreover, a series of simulation calculations are conducted to test the validity and stability of the proposed group decision-making method.},
  archive      = {J_APIN},
  author       = {Zhan, Jianming and Zhang, Kai and Liu, Peide and Pedrycz, Witold},
  doi          = {10.1007/s10489-022-04279-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15127-15146},
  shortjournal = {Appl. Intell.},
  title        = {A novel group decision-making approach in multi-scale environments},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised person re-identification based on high-quality
pseudo labels. <em>APIN</em>, <em>53</em>(12), 15112–15126. (<a
href="https://doi.org/10.1007/s10489-022-04270-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsupervised domain adaptive (UDA) person re-identification (re-ID) method is of great significance to promote the practical application of person re-ID. However, the noisy pseudo labels in the target domain hinder its performance. In this paper, a novel high-quality pseudo labels (HQP) method for UDA person re-ID is proposed, which improves the performance from the perspectives of sample feature expression and similarity measurement in the clustering. In order to obtain better feature representation for target domain samples, a source domain generalization method based on contrastive learning (SCL) is designed. SCL learns the inherently consistent information within a sample, thereby improving the expression ability of the source domain pre-trained model. In order to provide a more reasonable similarity measurement for the clustering method, a soft label similarity based on neighborhood information integration (NII) is designed, which aids the clustering method to generate reliable pseudo labels. Market-1501, DukeMTMC-ReID and MSMT17 datasets are employed to evaluate the performance of the proposed HQP method. It achieves the results of 80.3%/92.3%, 68.0%/82.6% and 25.4/53.3 mAP/Rank-1 on DukeMTMC-ReID-to-Market-1501, Market-1501-to-DukeMTMC-ReID and DukeMTMC-ReID-to-MSMT17 tasks. Experimental results demonstrate that our HQP method performs favorably against the state-of-the-art UDA person re-ID methods.},
  archive      = {J_APIN},
  author       = {Li, Yanfeng and Zhu, Xiaodi and Sun, Jia and Chen, Houjin and Li, Zhiyuan},
  doi          = {10.1007/s10489-022-04270-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15112-15126},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised person re-identification based on high-quality pseudo labels},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSO-NRS: An online group feature selection algorithm based
on PSO multi-objective optimization. <em>APIN</em>, <em>53</em>(12),
15095–15111. (<a
href="https://doi.org/10.1007/s10489-022-04275-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online streaming feature selection plays an important role in dealing with multi-dimensional data problems. Many online streaming feature selection algorithms have been combined with evolutionary algorithms (EA) and play an important role, however, most of them use single-objective optimization which has some limitations. Meanwhile, they ignore the interaction between features. The combination of features with each other may generates higher relevance. Therefore, this paper proposes a new online group feature selection algorithm PSO-NRS by fusing particle swarm optimization (PSO) algorithm and neighborhood rough set theory (NRS). PSO-NRS is able to select the set of features that are highly correlated with labels by combining features randomly. Using NRS for online feature selection does not require any domain knowledge, which makes PSO-NRS generalize better and can handle different types of data. PSO-NRS applies two layers of filtering for online feature selection. In the first filtering layer, two objective functions are designed and multi-objective optimization by particle swarm is used to select the set of features with the highest relevance. In the second filtering layer, a search strategy is defined using a rough set-based evaluation method to complete the final feature selection. The interactions between features are considered and redundant features are removed during the two filtering layers. Finally, PSO-NRS is experimented on 14 different types of datasets and compared with six state-of-the-art online feature selection algorithms to strongly validate the effectiveness and generalization of this algorithm.},
  archive      = {J_APIN},
  author       = {Liang, Shunpan and Liu, Ze and You, Dianlong and Pan, Weiwei and Zhao, Junjie and Cao, Yefan},
  doi          = {10.1007/s10489-022-04275-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15095-15111},
  shortjournal = {Appl. Intell.},
  title        = {PSO-NRS: An online group feature selection algorithm based on PSO multi-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Self-paced uncertainty estimation for one-shot person
re-identification. <em>APIN</em>, <em>53</em>(12), 15080–15094. (<a
href="https://doi.org/10.1007/s10489-022-04245-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The one-shot person Re-ID scenario faces two kinds of uncertainties when constructing the prediction model from X to Y. The first is model uncertainty, which captures the noise of the parameters in DNNs due to a lack of training data. The second is data uncertainty, which can be divided into two subtypes: image noise, where severe occlusion and the complex background contain irrelevant information about the identity; and label noise, where mislabeling affects visual appearance learning. We find that the state-of-the-art one-shot person Re-ID addresses the first issue of model uncertainty via a dynamic sampling strategy, while the second issue of data uncertainty remains. In this paper, to simultaneously address both issues, we propose a novel SPUE-Net for one-shot person Re-ID. By introducing a self-paced sampling strategy, our method can estimate the pseudolabels of unlabeled samples iteratively to expand the labeled samples gradually and remove model uncertainty without extra supervision. We divide the pseudolabel samples into two subsets to make the use of training samples more reasonable and effective. In addition, we apply a co-operative learning method of local uncertainty estimation combined with determinacy estimation to achieve better-hidden space feature mining and to improve the precision of selected pseudolabeled samples, which reduces data uncertainty. Extensive comparative evaluation experiments on video-based and image-based datasets show that SPUE-Net has significant advantages over state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Yulin and Ma, Bo and Liu, Longyao and Yi, Xin and Li, Meng and Diao, Yunfeng},
  doi          = {10.1007/s10489-022-04245-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15080-15094},
  shortjournal = {Appl. Intell.},
  title        = {Self-paced uncertainty estimation for one-shot person re-identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C2ShadowGAN: Cycle-in-cycle generative adversarial network
for shadow removal using unpaired data. <em>APIN</em>, <em>53</em>(12),
15067–15079. (<a
href="https://doi.org/10.1007/s10489-022-04269-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning technology, and the availability of public shadow image datasets, have enabled significant performance improvements of shadow removal tasks in computer vision. However, most deep learning-based shadow removal methods are usually trained in a supervised manner, in which paired shadow and shadow-free data are required. We developed a weakly supervised generative adversarial network with a cycle-in-cycle structure for shadow removal using unpaired data. In addition, we introduced new loss functions to reduce unnecessary transformations for non-shadow areas and to enable smooth transformations for shadow boundary areas. We conducted extensive experiments using the ISTD and Video Shadow Removal datasets to assess the effectiveness of our methods. The experimental results show that our method is superior to other state-of-the-art methods trained on unpaired data.},
  archive      = {J_APIN},
  author       = {Kang, Sunwon and Kim, Juwan and Jang, In Sung and Lee, Byoung-Dai},
  doi          = {10.1007/s10489-022-04269-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15067-15079},
  shortjournal = {Appl. Intell.},
  title        = {C2ShadowGAN: Cycle-in-cycle generative adversarial network for shadow removal using unpaired data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection based on knowledge graph network.
<em>APIN</em>, <em>53</em>(12), 15045–15066. (<a
href="https://doi.org/10.1007/s10489-022-04116-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection using convolutional neural networks addresses the recognition problem solely in terms of feature extraction and disregards knowledge and experience to explore higher-level relationships between objects. This paper proposed a knowledge graph network based on a graph convolution network to improve the accuracy of baseline detectors. This network can be integrated into any object detection framework. First, this paper created an experience memory module to store information about categories in the database. When inputting the image to the database, an experience vector for it was obtained. The experience data graph was then constructed by counting the co-occurrences of labels in the dataset. Finally, a graph convolutional neural network was used to extract the relationship between the experience vector and the data graph matrix. This relational pattern can help the baseline detector perform better. Several classical object detectors were then evaluated using the COCO, VOC, and KITTI datasets. The results indicated a significant increase for the baseline detector in mAP using the knowledge graph network.},
  archive      = {J_APIN},
  author       = {Li, Jianping and Tan, Guozhen and Ke, Xiao and Si, Huaiwei and Peng, Yanfei},
  doi          = {10.1007/s10489-022-04116-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15045-15066},
  shortjournal = {Appl. Intell.},
  title        = {Object detection based on knowledge graph network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retinal vessel segmentation based on self-distillation and
implicit neural representation. <em>APIN</em>, <em>53</em>(12),
15027–15044. (<a
href="https://doi.org/10.1007/s10489-022-04252-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting retinal blood vessels from retinal images is a crucial step in ocular disease diagnosis. It is also one of the most important applications and research in ophthalmic image analysis. However, the contrast between the retinal vessels and background in fundus images is low. The size and shape of retinal vessels vary significantly, and the width of some small vessels is often below 10 pixels or even 1 pixel. Moreover, some blood vessels are discontinuous owing to illumination, which complicates the segmentation of retinal blood vessels. To address these problems, this paper innovatively proposes a novel retinal vessel segmentation network framework based on self-distillation and implicit neural representation, which predicts retinal vessels in two stages. First, the self-distillation method extracts the main features of retinal images using the properties of Vision Transformer (ViT) to obtain preliminary images for the blood vessel segmentation. Second, implicit neural representation improves the resolution of the original retinal image, and the details of blood vessels are enhanced through the texture enhancement module to obtain accurate results of the blood vessel segmentation. Furthermore, we adopted an improved centerline dice (clDice) loss function to constrain the topology of blood vessels. We experimented on two benchmark retinal datasets (i.e., Drive and Chase) to quantitatively and qualitatively analyze the proposed method. The results indicate that the proposed outperformed the mainstream baseline. The visual segmentation results also show that this method can segment thin blood vessels more accurately and ensure the continuity of blood vessels.},
  archive      = {J_APIN},
  author       = {Gu, Jia and Tian, Fangzheng and Oh, Il-Seok},
  doi          = {10.1007/s10489-022-04252-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15027-15044},
  shortjournal = {Appl. Intell.},
  title        = {Retinal vessel segmentation based on self-distillation and implicit neural representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic incremental dynamic analysis of structures
using temporal surrogate model. <em>APIN</em>, <em>53</em>(12),
15011–15026. (<a
href="https://doi.org/10.1007/s10489-022-04264-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a highly efficient framework, termed iDANS, for Incremental Dynamic Analysis (IDA) of civil structures subjected to earthquakes using a physical-induced data-driven surrogate model. IDA is a reliable tool for assessing the structure’s seismic performance; however, it requires extensive calculations to model the structures’ behavior from linear to non-linear ranges, leading to a high computational cost. Therefore, establishing a surrogate model that is able to provide the structure’s responses rapidly, is helpful. To this end, one leverages multiple advanced techniques from data analytic, artificial intelligence, and structural dynamic such as rolling window strategy, data fusion, a temporal neural network architecture, and a length- and magnitude-agnostic loss function. The surrogate model is trained on a dataset generated by finite element models carefully calibrated with experimentally measured data in advance. The proposed approach’s accuracy and efficiency are quantitatively demonstrated through a case study of a six-story steel building. The computed results show that iDANS can reduce the computational complexity by three orders compared to conventional IDA methods. Furthermore, iDANS is employed to perform probabilistic analysis for assessing the impact of input uncertainty on the structure’s fragility curves.},
  archive      = {J_APIN},
  author       = {Nguyen, Truong-Thang and Dang, Viet-Hung},
  doi          = {10.1007/s10489-022-04264-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {15011-15026},
  shortjournal = {Appl. Intell.},
  title        = {Probabilistic incremental dynamic analysis of structures using temporal surrogate model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning hash index based on a shallow autoencoder.
<em>APIN</em>, <em>53</em>(12), 14999–15010. (<a
href="https://doi.org/10.1007/s10489-022-04274-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hash index plays an important role in improving query efficiency in databases. Because traditional hash algorithms cannot use the original data distribution, there is often a high collision rate in large-scale datasets. Additionally, some traditional hashing algorithms are data-dependent and cannot be accelerated in parallel. The learned index provides a new method for index design in data management systems. The key idea of the learned index is to consider the index as a model that can be learned. In this paper, we propose a learning hash algorithm based on a shallow autoencoder that can make full use of the original data characteristics and take advantage of the parallelism of matrix operations. Therefore, compared with traditional hash functions, the proposed method has a lower collision rate and higher efficiency. Finally, we verify the effectiveness of the proposed method through a series of experiments on synthetic datasets and real datasets. Experimental results show that the proposed hash algorithm has considerable advantages in reducing the collision rate and computing time while improving the retrieval efficiency.},
  archive      = {J_APIN},
  author       = {Lin, Yuming and Huang, Zhengguo and Li, You},
  doi          = {10.1007/s10489-022-04274-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14999-15010},
  shortjournal = {Appl. Intell.},
  title        = {Learning hash index based on a shallow autoencoder},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial dynamic graph convolutional network for traffic flow
forecasting. <em>APIN</em>, <em>53</em>(12), 14986–14998. (<a
href="https://doi.org/10.1007/s10489-022-04271-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex traffic network spatial correlation and the characteristic of high nonlinear and dynamic traffic conditions in the time are the challenges to accurate traffic flow forecasting. Existing spatiotemporal models attempt to utilize the static graph to explore spatial dependency and employ RNN-based model to capture temporal dependency. However, the static graph fails to reflect the dynamic changeable correlation between each node. That is some nodes have a strong connection in a real traffic network, whereas a weak connection is in a static predefined graph. To overcome the above problems, we propose a spatial dynamic graph convolutional network (SDGCN) for traffic flow forecasting. With the support of an attention fusion network in graph learning, SDGCN generates the dynamic graph at each time step, which can model the changeable spatial correlation from traffic data. By embedding dynamic graph diffusion convolution into gated recurrent unit, our model can explore spatio-temporal dependency simultaneously. Moreover, to handle long sequence forecasting, ReZero transformer is utilized to detect the global temporal correlation capturing. The experiments are conducted on two public datasets. The experimental results demonstrate the superior performance of our network.},
  archive      = {J_APIN},
  author       = {Li, Huaying and Yang, Shumin and Song, Youyi and Luo, Yu and Li, Junchao and Zhou, Teng},
  doi          = {10.1007/s10489-022-04271-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14986-14998},
  shortjournal = {Appl. Intell.},
  title        = {Spatial dynamic graph convolutional network for traffic flow forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-space and detail-supplemented attention network for
point cloud completion. <em>APIN</em>, <em>53</em>(12), 14971–14985. (<a
href="https://doi.org/10.1007/s10489-022-04219-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparsity and incompleteness of point clouds generally result in challenges in point cloud analysis. Most existing point cloud completion methods use an individual Euclidean space feature to generate point clouds. Consequently, the generated point clouds are relatively rough. This paper proposes a multi-space and detail-supplemented attention point cloud completion network (MSDSA-Net). Here, the key is to utilize multi-space features to generate high-quality point clouds. First, we construct a dual-branch multi-space feature extractor (MSFE). A branch of the MSFE is a local-holistic geometric feature extractor based on Euclidean space and eigenvalue space. It can extract features with similar local geometric structures at points that are at a distance, to compensate for the missing part of the feature information of the point cloud. Another branch of the MSFE is a global feature extractor based on Euclidean space to extract the global features. Second, we continue to follow the coarse-to-fine decoding framework of general completion networks. However, in the fine generation stage, we propose a detail-supplemented (DS) module to supplement the features used to guide point cloud generation in detail. Extensive experiments demonstrate that our network has a good effect on the shape completion of point clouds.},
  archive      = {J_APIN},
  author       = {Xiang, Min and Ye, Hailiang and Yang, Bing and Cao, Feilong},
  doi          = {10.1007/s10489-022-04219-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14971-14985},
  shortjournal = {Appl. Intell.},
  title        = {Multi-space and detail-supplemented attention network for point cloud completion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-RCap: RNN-capsule model via label semantics for MLTC.
<em>APIN</em>, <em>53</em>(12), 14961–14970. (<a
href="https://doi.org/10.1007/s10489-022-04286-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification attempts to assign a label set to one specific document, which is more closely related to real life. Network models based on traditional deep learning achieve good prediction results. However, these models generally ignore the importance of label semantics and do not fit the connection between categories and text features well. Therefore, this paper proposes a novel L-RCap model. L-RCap uses the Bi-LSTM to extract global text features. With the global text features, we can use the label semantics to construct label-text features in the label semantic attention mechanism. Besides, we use the capsule network to extend the features information and use the dynamic routing algorithm to fit the association between features and categories. Compared with the baseline models, our model exhibits the best performance on two datasets.},
  archive      = {J_APIN},
  author       = {Zhang, Xiuling and Luo, Zhaoci and Du, Bingce and Wu, Ziyun},
  doi          = {10.1007/s10489-022-04286-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14961-14970},
  shortjournal = {Appl. Intell.},
  title        = {L-RCap: RNN-capsule model via label semantics for MLTC},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel high-level target navigation pigeon-inspired
optimization for global optimization problems. <em>APIN</em>,
<em>53</em>(12), 14918–14960. (<a
href="https://doi.org/10.1007/s10489-022-04224-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical Pigeon-inspired optimization (PIO) possesses excellent local exploitation capability and can provide a very fast convergence speed, but it is also easily trapped in local optima especially when facing complex problems. In order to take full advantage of the superior local exploitation capability, and at the same time enhance the global exploration capability of PIO, we present a modified PIO called high-level target navigation PIO (HTNPIO). The HTNPIO includes three strategies, selective mutation strategy (SMS), levy-based map-compass strategy (LMS), and enhanced landmark strategy (ELS). In the strategies, two kinds of mutation strategies and a simple levy-flight operator are performed. What’s more, an LMS-ELS probability is proposed to balance the exploration and exploitation. In order to test the performance of the proposed optimizer, HTNPIO is made comparisons with other 15 PIO and advanced heuristic algorithms on the IEEE CEC2017 benchmark problems and 5 real world optimization problems. Experimental results demonstrate that HTNPIO defeats all the competitors on the CEC2017 benchmark problems including the extraordinarily competitive LSHADE, and also exhibits extremely competitive performance in dealing with the real-world problems. Therefore, HTNPIO might be effective to provide promising solutions in various function and industrial optimization problems.},
  archive      = {J_APIN},
  author       = {Wang, Hanming and Zhao, Jinghong},
  doi          = {10.1007/s10489-022-04224-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14918-14960},
  shortjournal = {Appl. Intell.},
  title        = {A novel high-level target navigation pigeon-inspired optimization for global optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning more with the same effort: How randomization
improves the robustness of a robotic deep reinforcement learning agent.
<em>APIN</em>, <em>53</em>(12), 14903–14917. (<a
href="https://doi.org/10.1007/s10489-022-04227-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial application of Deep Reinforcement Learning (DRL) is frequently slowed down due to an inability to generate the experience required to train the models. Collecting data often involves considerable time and financial outlays that can make it unaffordable. Fortunately, devices like robots can be trained with synthetic experience through virtual environments. With this approach, the problems of sample efficiency with artificial agents are mitigated, but another issue arises: the need to efficiently transfer the synthetic experience into the real world (sim-to-real). This paper analyzes the robustness of a state-of-the-art sim-to-real technique known as Progressive Neural Networks (PNNs) and studies how adding diversity to the synthetic experience can complement it. To better understand the drivers that lead to a lack of robustness, the robotic agent is still tested in a virtual environment to ensure total control on the divergence between the simulated and real models. The results show that a PNN-like agent exhibits a substantial decrease in its robustness at the beginning of the real training phase. Randomizing specific variables during simulation-based training significantly mitigates this issue. The average increase in the model’s accuracy is around 25% when diversity is introduced in the training process. This improvement can translate into a decrease in the number of real experiences required for the same final robust performance. Notwithstanding, adding real experience to agents should still be beneficial, regardless of the quality of the virtual experience fed to the agent. The source code is available at: https://gitlab.com/comillas-cic/sim-to-real/pnn-dr.git .},
  archive      = {J_APIN},
  author       = {Güitta-López, Lucía and Boal, Jaime and López-López, Álvaro J.},
  doi          = {10.1007/s10489-022-04227-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14903-14917},
  shortjournal = {Appl. Intell.},
  title        = {Learning more with the same effort: How randomization improves the robustness of a robotic deep reinforcement learning agent},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive active learning through k-nearest neighbor
optimized local density clustering. <em>APIN</em>, <em>53</em>(12),
14892–14902. (<a
href="https://doi.org/10.1007/s10489-022-04169-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning iteratively constructs a refined training set to train an effective classifier with as few labeled instances as possible. In areas where labeling is expensive, active learning plays an important and irreplaceable role. The main challenge of active learning is to correctly identify critical samples. One of the current mainstream methods is to mine the potential data structure based on clustering and then identify key instances. However, the existing methods all adopt deterministic strategies, and the number of key samples is only related to the number of samples to be classified. The internal structure information of the sample clusters to be classified is not used. After analysis and verification, this deterministic key sample selection strategy has serious label waste. This is a serious problem that urgently needs to be solved in active learning. To this end, we propose an adaptive active learning algorithm based on density clustering (AAKC). Firstly, we introduce k-nearest neighbor information to redefine the local density of the instance. The new sample density can clearly express the local structural information of the sample. Secondly, we developed an adaptive key instance selection strategy based on the k-nearest neighbor sample density, which can adaptively select the necessary number of instance queries according to the structural information of the instance clusters to be classified, avoiding label waste. The experimental results of comparison with other algorithms show that our algorithm uses fewer labels to achieve better classification accuracy and has excellent stability.},
  archive      = {J_APIN},
  author       = {Ji, Xia and Ye, WanLi and Li, XueJun and Zhao, Peng and Yao, Sheng},
  doi          = {10.1007/s10489-022-04169-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14892-14902},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive active learning through k-nearest neighbor optimized local density clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible job-shop scheduling method based on interval grey
processing time. <em>APIN</em>, <em>53</em>(12), 14876–14891. (<a
href="https://doi.org/10.1007/s10489-022-04213-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the complexity of industrial products, the processing time of products is affected by many factors, and it is difficult to give a concrete time estimate. Therefore, it is significant to study the flexible job shop scheduling problem (FJSP) with uncertain processing time. This paper defines the uncertain processing time as the interval grey processing time (IGPT). Also, an FJSP model with IGPT (G-FJSP) is formulated to minimize the interval grey maximum completion time, and the mathematical operation rules of IGPT are improved. Based on this, a step-size adaptive discrete particle swarm algorithm with load balancing (LS-DPSO) is put forward to solve the G-FJSP model. The experimental analysis on six classical test cases indicates that LS-DPSO outperforms four algorithms proposed in recent literature in terms of speed and solution quality. Taking IMK05 as an example, the minimum and the average values of LS-DPSO IGPT are 1.8% and 2.2% smaller than the optimal results of other four algorithms. Also, the resulting grey Gantt chart has better processing time flexibility to guide practical production.},
  archive      = {J_APIN},
  author       = {Xu, Wenxing and Wu, Wentong and Wang, Yao and He, Yunliang and Lei, Zhimei},
  doi          = {10.1007/s10489-022-04213-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14876-14891},
  shortjournal = {Appl. Intell.},
  title        = {Flexible job-shop scheduling method based on interval grey processing time},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive sparse graph learning for multi-view spectral
clustering. <em>APIN</em>, <em>53</em>(12), 14855–14875. (<a
href="https://doi.org/10.1007/s10489-022-04267-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph-based multi-view spectral clustering methods have achieved remarkable progress. However, most of which are generally deficient in the following two aspects. First, ignoring the different importance of multiple views, low-quality views in the multi-view data may seriously affect the clustering performance. Second, for constructed graphs, noise and outliers are difficult to effectively filter out. For the purpose of overcoming the above two deficiencies, this paper proposes a novel adaptive sparse graph learning for multi-view spectral clustering (ASGL) method. Specifically, the adaptive neighbor graph learning method is adopted to construct the similarity matrices of all views, which improves the robustness to noise and outliers. By adaptively assigning the weight of each view, the complementary information between the views is combined to more accurately describe the essential category attributes between the sample data. An effective algorithm for solving the optimization problem of ASGL model is proposed. Compared to several state-of-the-art algorithms, extensive experiments on several benchmark datasets verify good clustering performance of ASGL.},
  archive      = {J_APIN},
  author       = {Xiao, Qingjiang and Du, Shiqiang and Zhang, Kaiwu and Song, Jinmei and Huang, Yixuan},
  doi          = {10.1007/s10489-022-04267-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14855-14875},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive sparse graph learning for multi-view spectral clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stream adaptive 3D attention graph convolution network
for skeleton-based action recognition. <em>APIN</em>, <em>53</em>(12),
14838–14854. (<a
href="https://doi.org/10.1007/s10489-022-04179-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition methods based on spatial-temporal skeleton graphs have been applied extensively. The spatial and temporal graphs are generally modeled individually in previous approaches. Recently, many researchers capture the correlation information of temporal and spatial dimensions in spatial-temporal graphs. However, the existing methods have several issues such as 1. The existing modal graphs are defined based on the human body structure which is not flexible enough; 2. The approach to extracting non-local neighborhood features is insufficiently powerful; 3. Attention modules are limited to a single scale; 4. The fusion of multiple data streams is not sufficiently effective. This work proposes a novel multi-stream adaptive 3D attention graph convolution network for skeleton-based action recognition that improves the aforementioned issues. The method utilizes an adaptive topology graph with an adaptive connection coefficient to adaptively optimize the topology of the graph during the training process according to the input data. An optimal high-order adjacency matrix is constructed in our work to balance the weight bias, which captures non-local neighborhood features precisely. Moreover, we design a multi-scale attention mechanism to aggregate information from multiple ranges, which makes the graph convolution focus on more efficient nodes, frames, and channels. To further improve the performance of the model, a novel multi-stream framework is proposed to aggregate the high-order information of the skeleton. The experiment results on the NTU-RGBD and Kinetics-Skeleton prove that our proposed method reveals better results than existing methods.},
  archive      = {J_APIN},
  author       = {Yu, Lubin and Tian, Lianfang and Du, Qiliang and Bhutto, Jameel Ahmed},
  doi          = {10.1007/s10489-022-04179-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14838-14854},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stream adaptive 3D attention graph convolution network for skeleton-based action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning controlled and targeted communication with the
centralized critic for the multi-agent system. <em>APIN</em>,
<em>53</em>(12), 14819–14837. (<a
href="https://doi.org/10.1007/s10489-022-04225-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent deep reinforcement learning (MDRL) has attracted attention for solving complex tasks. Two main challenges of MDRL are non-stationarity and partial observability from the perspective of agents, impacting the performance of agents’ learning cooperative policies. In this study, Controlled and Targeted Communication with the Centralized Critic (COTAC) is proposed, thereby constructing the paradigm of centralized learning and decentralized execution with partial communication. It is capable of decoupling how the MAS obtains environmental information during training and execution. Specifically, COTAC can make the environment faced by agents to be stationarity in the training phase and learn partial communication to overcome the limitation of partial observability in the execution phase. Based on this, decentralized actors learn controlled and targeted communication and policies optimized by centralized critics during training. As a result, agents comprehensively learn when to communicate during the sending and how to target information aggregation during the receiving. Apart from that, COTAC is evaluated on two multi-agent scenarios with continuous space. Experimental results demonstrated that partial agents with important information choose to send messages and targeted aggregate received information by identifying the relevant important information, which can still have better cooperation performance while reducing the communication traffic of the system.},
  archive      = {J_APIN},
  author       = {Sun, Qingshuang and Yao, Yuan and Yi, Peng and Hu, YuJiao and Yang, Zhao and Yang, Gang and Zhou, Xingshe},
  doi          = {10.1007/s10489-022-04225-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14819-14837},
  shortjournal = {Appl. Intell.},
  title        = {Learning controlled and targeted communication with the centralized critic for the multi-agent system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-device context-aware misuse detection framework for
heterogeneous IoT edge. <em>APIN</em>, <em>53</em>(12), 14792–14818. (<a
href="https://doi.org/10.1007/s10489-022-04039-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional AI techniques for offline misuse network intrusion detection have performed well, assuming that the traffic from the datasets is sufficiently large for generalization, balanced, independently and identically distributed—exhibiting homogeneous behavior with little to no context change. However, the rapidly expanding IoT network is an ensemble of proliferating internet-connected devices catering to the growing need for handling highly distributed, heterogeneous, and time-critical workloads that conform to none of the above assumptions. Moreover, the evolving Botnet-based attack vectors exploit the non-standardized and poorly scrutinized architectural vulnerabilities of such devices—leading to compounding threat intensity, rapidly rendering the network defenseless. Furthermore, the memory, processor, and energy resource constraints of the IoT devices necessitate lightweight device-specific intrusion detection policies for effective and updated rule learning in real-time through the edge infrastructures. However, the existing methods proposed to solve such issues are either centralized, data and resource-intensive, context-unaware, or inefficient for online rule learning with smaller and imbalanced data samples. Thus, this paper addresses such issues through a context-aware expert system-based feature subset framework with minimal processing overhead and a decentralized on-device misuse detection scheme for IoT—called HetIoT-NIDS, capable of efficiently inferring over smaller data samples, tolerant to class imbalance, and deployable on the low-memory and low-power edge of IoT devices. Furthermore, HetIoT-NIDS facilitates threat localization within the deployed device, preventing threat progression and intensity compounding. The experiments and analyses of the propounded algorithms and the resulting training times and model sizes prove that the proposed approach is efficient and adaptable to online and offline misuse intrusion detection, especially on smaller data sample sizes.},
  archive      = {J_APIN},
  author       = {A, Nitish and J, Hanumanthappa and P, Shiva Prakash S. and Krinkin, Kirill},
  doi          = {10.1007/s10489-022-04039-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14792-14818},
  shortjournal = {Appl. Intell.},
  title        = {On-device context-aware misuse detection framework for heterogeneous IoT edge},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial learning based intermediate feature refinement
for semantic segmentation. <em>APIN</em>, <em>53</em>(12), 14775–14791.
(<a href="https://doi.org/10.1007/s10489-022-04107-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image semantic segmentation is a meaningful task that requires both accuracy and efficiency in computer vision. At present, most current deep learning based semantic segmentation methods needs extensive computational resources, and knowledge distillation may reduce such a computational burden due to its model compression ability. In this paper, different from previous knowledge distillation methods that directly transfer the knowledge of the teacher network to the student network, we propose a novel intermediate feature refinement method for semantic segmentation based on adversarial learning, which reduces the error and redundant information contained in the teacher network in the process of knowledge distillation, enhances the correct information contained in the teacher network and transfers it to the student network. Then we improve the conventional discriminator in adversarial learning to help the student network align more correct intermediate features in the teacher network. Our method can make the feature distribution of the student network closer to that of the teacher network, and finally improve the segmentation performance of the student network. Finally, we conducted experiments on three popular benchmarks to verify the effectiveness of our proposed method, including Pascal VOC, Cityscapes and CamVid. Compared with the competitive baseline, our proposed method can improve the performance of the student network by up to 1.43% (the mIOU increases from 67.14% to 68.57% on the Cityscapes val set).},
  archive      = {J_APIN},
  author       = {Wang, Dongli and Yuan, Zhitian and Ouyang, Wanli and Li, Baopu and Zhou, Yan},
  doi          = {10.1007/s10489-022-04107-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14775-14791},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial learning based intermediate feature refinement for semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label-representative graph convolutional network for
multi-label text classification. <em>APIN</em>, <em>53</em>(12),
14759–14774. (<a
href="https://doi.org/10.1007/s10489-022-04106-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification (MLTC) is the task that assigns each document to the most relevant subset of class labels. Previous works usually ignored the correlation and semantics of labels resulting in information loss. To deal with this problem, we propose a new model that explores label dependencies and semantics by using graph convolutional networks (GCN). Particularly, we introduce an efficient correlation matrix to model label correlation based on occurrence and co-occurrence probabilities. To enrich the semantic information of labels, we design a method to use external information from Wikipedia for label embeddings. Correlated label information learned from GCN is combined with fine-grained document representation generated from another sub-net for classification. Experimental results on three benchmark datasets show that our model outweighs prior state-of-the-art methods. Ablation studies also show several aspects of the proposed model. Our code is available at https://github.com/chiennv2000/LR-GCN .},
  archive      = {J_APIN},
  author       = {Vu, Huy-The and Nguyen, Minh-Tien and Nguyen, Van-Chien and Pham, Minh-Hieu and Nguyen, Van-Quyet and Nguyen, Van-Hau},
  doi          = {10.1007/s10489-022-04106-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14759-14774},
  shortjournal = {Appl. Intell.},
  title        = {Label-representative graph convolutional network for multi-label text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal selection and feedback for solving math word problems.
<em>APIN</em>, <em>53</em>(12), 14744–14758. (<a
href="https://doi.org/10.1007/s10489-022-04253-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving Math Word Problems (MWPs) automatically is a challenging task for AI-tutoring in online education. Most of the existing State-Of-The-Art (SOTA) neural models for solving MWPs use Goal-driven Tree-structured Solver (GTS) as their decoders. However, owing to the defects of the tree-structured recurrent neural networks, GTS can not obtain the information of all generated nodes in each decoding time step. Therefore, the performance for long math expressions is not satisfactory enough. To address such limitations, we propose a Goal Selection and Feedback (GSF) decoding module. In each time step of GSF, we firstly feed the latest result back to all goal vectors through goal feedback operation, and then the goal selection operation based on attention mechanism is designed for generate the new goal vector. Not only can the decoder collect the historical information from all generated nodes through goal selection operation, but also these generated nodes are always updated timely by goal feedback operation. In addition, a Multilayer Fusion Network (MFN) is proposed to provide a better representation of each hidden state during decoding. Combining the ELECTRA language model with our novel decoder, experiments on the Math23k, Ape-clean, and MAWPS datasets show that our model outperforms the SOTA baselines, especially on the MWPs of complex samples with long math expressions. The ablation study and case study further verify that our model can better solve the samples with long expressions, and the proposed components are indeed able to help enhance the performance of the model.},
  archive      = {J_APIN},
  author       = {He, Daijun and Xiao, Jing},
  doi          = {10.1007/s10489-022-04253-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14744-14758},
  shortjournal = {Appl. Intell.},
  title        = {Goal selection and feedback for solving math word problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective genetic algorithm based on the fuzzy
MULTIMOORA method for solving the cardinality constrained portfolio
optimization. <em>APIN</em>, <em>53</em>(12), 14717–14743. (<a
href="https://doi.org/10.1007/s10489-022-04240-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the recent decades, Markowitz mean-variance models with additional constraints and objectives have still attracted the attention of many researchers. Stock evaluation and portfolio optimization have a decision-making process that contains tangible and intangible factors under vague parameters. This study presents an integrated approach including the fuzzy MULTIMOORA based on Correlation Coefficient and Standard Deviation (CCSD) method and the mean–variance-ranking cardinality constrained portfolio optimization (MVRCCPO), which is the extension of classical mean-variance cardinality constrained portfolio optimization model. The ranking obtained from the fuzzy MULTIMOORA based on the CCSD method is handled as one of the objective functions in the proposed model. The MVRCCPO problem is an NP-Complete problem when taking cardinality constraints into account. In this case, existing exact algorithms such as quadratic programming may not be efficient for solving the problem. Due to the complexity of the problem and the nature of the multiple objectives, we used the multi-objective genetic algorithm based on three different scalarization techniques, involving conic, tchebycheff, and weighted-sum. The performance of the multi-objective genetic algorithm with scalarization variants is investigated across forty-nine benchmark instances consisting of seven different objective weight combinations and seven different instances from ‘small’ to ‘large’ size derived from a large real-life problem. The computational results indicate that the multi-objective genetic algorithm utilizing weighted-sum in accordance with the weighted distance to the ideal point outperforms the rest in point of the obtained results in most cases. Moreover, the multi-objective genetic algorithm using conic in accordance with the weighted distance to the reference point is better performance than the multi-objective genetic algorithm using tchebycheff.},
  archive      = {J_APIN},
  author       = {Deliktaş, Derya and Ustun, Ozden},
  doi          = {10.1007/s10489-022-04240-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14717-14743},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective genetic algorithm based on the fuzzy MULTIMOORA method for solving the cardinality constrained portfolio optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view multi-label learning with double orders manifold
preserving. <em>APIN</em>, <em>53</em>(12), 14703–14716. (<a
href="https://doi.org/10.1007/s10489-022-04242-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view multi-label learning, each instance has multiple heterogeneous views and is marked with a collection of non-exclusive discrete labels. This type of data is usually subject to dimensional catastrophe. Previous multi-view multi-label works look for a low-dimensional shared subspace to tackle this problem. However, these methods ignore the global structural information of the original feature space during dimension reduction. In this paper, we propose Multi-view Multi-label learning with Double Orders Manifold Preserving (MMDOM). MMDOM utilizes manifold preserving constraint to guide the formation of low-dimensional shared subspace. To obtain exact manifold preserving, the first-order and the second-order similarity matrices are both introduced to explore the local and global structural information of the original feature space. Experiments on various benchmark datasets demonstrate the superior effectiveness of MMDOM against state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yin, Jun and Zhang, Wentao},
  doi          = {10.1007/s10489-022-04242-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {12},
  pages        = {14703-14716},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view multi-label learning with double orders manifold preserving},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Optimal electricity distribution of “three innovations”
manufacturing enterprises under china’s peak carbon strategy.
<em>APIN</em>, <em>53</em>(11), 14043–14057. (<a
href="https://doi.org/10.1007/s10489-022-04143-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {China’s carbon peak strategy constrains industry’s carbon footprint, limiting the total amount of electricity production from coal burning, rendering electricity a relatively scarce resource and affecting manufacturing enterprises’ production and operation activities. To address these circumstances, this paper applies a non-cooperative game model to describe the electricity resource distribution and product production process under enterprises’ competitive state, proposing an optimal electricity distribution analysis framework for manufacturing enterprises. The results of optimal electricity distribution for six subindustries in the ‘three innovations’ manufacturing industries (referring to a collection of economic activities to establish new industries, new formats and new business models) are analysed to examine the difference in electricity consumption efficiency among industries. The electricity quota of each industrial chain significantly differs before and after redistribution;therefore, the industrial differences of optimal electricity distribution are further explored from the perspective of industrial chains, revealing the supply–demand relationship of upstream, middle and downstream enterprises. Finally, corresponding policy suggestions and directions for further research are presented.},
  archive      = {J_APIN},
  author       = {Zhang, Chonghui and Guo, Zixu and Xu, Zeshui and Jin, Huanhuan},
  doi          = {10.1007/s10489-022-04143-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {14043-14057},
  shortjournal = {Appl. Intell.},
  title        = {Optimal electricity distribution of ‘three innovations’ manufacturing enterprises under china’s peak carbon strategy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dictionary based model for bengali document
classification. <em>APIN</em>, <em>53</em>(11), 14023–14042. (<a
href="https://doi.org/10.1007/s10489-022-03955-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided documented content analysis is a prominent research area in natural language processing. A realistic implementation of this task is related to the subjectivity of the quantifiable data. One of the most interesting specialisations of this problem is automated document classification, which is a system that can identify the category of a document without human intervention. The problem of document classification has to consider the evaluation of the heart-of-the-matter of the textual material. Being one of the most-spoken languages in the world, a huge number of Bengali documents are present in digital form, and it is increasing rapidly due to the age of the internet. A document classification method is required to organise and categorise these huge documents rapidly and efficiently. In this paper, a decisive dictionary based model has been presented for the classification of documents in the Bengali text. We have introduced the concepts of lexiconid, lexiconaffinity, lexiconunicity, and lexiconassociation to acquire the features. The feature set is integrated with different levels of threshold. The proposed model is supervised, and the entire dataset has been split into testing and training sets. The proposed model has been validated using the k-fold cross-validation strategy. A significant number of dictionary based parameter values have been estimated for each token present in the text. In this paper, the text has been classified using a new rule based classification algorithm, predictive lexicon inference (PLI) classifier. The proposed model has been evaluated on five datasets: Paradise Lost, Iliad, Odyssey, Ramayana, and Mahabharata. In addition to document classification, this algorithm enables name entity classification, and chronology or description classification.},
  archive      = {J_APIN},
  author       = {Das Dawn, Debapratim and Khan, Abhinandan and Shaikh, Soharab Hossain and Pal, Rajat Kumar},
  doi          = {10.1007/s10489-022-03955-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {14023-14042},
  shortjournal = {Appl. Intell.},
  title        = {A dictionary based model for bengali document classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mjolnir: A framework agnostic auto-tuning system with deep
reinforcement learning. <em>APIN</em>, <em>53</em>(11), 14008–14022. (<a
href="https://doi.org/10.1007/s10489-022-03956-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing the right setting for big data frameworks is an important yet difficult task. These frameworks come with a complex set of parameters that need to be tuned to achieve the best performance in terms of throughput and latency. Learning-based auto-tuning methods using traditional machine learning models might not be effective for the task because they require huge amounts of high-quality training data, which is time-consuming and very expensive. A good alternative would be to consider reinforcement learning methods to train an intelligent agent through trial and error. In this context, we propose a framework-agnostic auto-tuning system implementing an actor-critic algorithm namely TD3 (Twin Delayed Deep Deterministic Policy Gradient). We show that the agent can find an optimal configuration in a continuous high-dimensional search space with a limited number of steps. We conducted extensive experiments on Apache Spark, under different workloads from the HiBench, TPC-DS and TPC-H benchmarking tools. In this paper, we give a detailed representation of the reinforcement learning environment and show the best design through experiments. Results showed that our approach outperforms the state-of-the-art tuning methods and can improve the performance of spark workloads over the default configurations by up to $\sim 77\%$ with an average of $\sim 45\%$ . It also showed a promising adaptation behaviour to workload variation during evaluation.},
  archive      = {J_APIN},
  author       = {Ben Slimane, Nourchene and Sagaama, Houssem and Marwani, Maher and Skhiri, Sabri},
  doi          = {10.1007/s10489-022-03956-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {14008-14022},
  shortjournal = {Appl. Intell.},
  title        = {Mjolnir: A framework agnostic auto-tuning system with deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Fixed global memory for controllable long text generation.
<em>APIN</em>, <em>53</em>(11), 13993–14007. (<a
href="https://doi.org/10.1007/s10489-022-04197-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long text generation is a challenging yet unsolved task. To generate long, coherent, and consistent text, existing approaches need to increase the language model length accordingly. However, the cost of the computational and memory resources grows as the square of the length. Even trained with thousands of GPUs, the length of language models is still limited to a few thousand, which may cause the generation of longer texts to be inconsistent with the topics and ideas in their preceding texts. To address this, we propose a novel Transformer architecture called Transformer with Local and Global Memory (Transformer LGM). It is inspired by the way people write long articles, which generate a key idea first and then guide the writing of the entire article with the idea in mind. Such a “key idea” can be put into the fixed global memory of the Transformer LGM to guide the whole generation process. On the contrary, the local memory, which is responsible for local coherence, could shift and drop with the increasing length of the generated text. We implement the global memory by introducing a negative positional embedding, while the traditional positive positional embedding is still used for the local memory. Experiments show that by utilizing the global memory, our model could generate long, coherent, and consistent text without enlarging the length of the language model.},
  archive      = {J_APIN},
  author       = {Chen, Zheng and Liu, Zhejun},
  doi          = {10.1007/s10489-022-04197-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13993-14007},
  shortjournal = {Appl. Intell.},
  title        = {Fixed global memory for controllable long text generation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection optimized by the artificial immune
algorithm based on genome shuffling and conditional lethal mutation.
<em>APIN</em>, <em>53</em>(11), 13972–13992. (<a
href="https://doi.org/10.1007/s10489-022-03971-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving classification performance is an essential goal for various practical applications. Feature selection has become an important data preprocessing step in machine learning systems. However, many effective methods based on heuristic search strategies have the problem of high running costs. This paper proposes an efficient multiobjective feature selection method based on artificial immune algorithm optimization. It introduces a clone selection algorithm to explore the search space of optimal feature subsets. According to the target requirements of feature selection, combined with biological research results, this method introduces genome shuffling technology and a conditional lethal mutation mechanism to improve the search performance of the algorithm. Experimental comparisons are conducted on 21 benchmark datasets with 17 advanced feature selection methods in terms of classification accuracy, the number of feature subsets, and computational cost. The results show that the algorithm achieves the smallest number of selected features (only 3.26% compared to the lowest) and better average classification accuracy with a much lower average computational cost than others (only 3.62% compared to the lowest).},
  archive      = {J_APIN},
  author       = {Zhu, Yongbin and Li, Tao and Lan, Xiaolong},
  doi          = {10.1007/s10489-022-03971-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13972-13992},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection optimized by the artificial immune algorithm based on genome shuffling and conditional lethal mutation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multibranch multilevel federated learning for a better
feature extraction and a plug-and-play dynamic-adjusting double flow
personalization approach. <em>APIN</em>, <em>53</em>(11), 13956–13971.
(<a href="https://doi.org/10.1007/s10489-022-04193-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging technique used to preserve the privacy of users’ data in training by a distributed machine learning approach. Previously, a client-edge-cloud hierarchical federated learning (HierFL) system was proposed to reduce the long communication latency between the client and the cloud. However, HierFL performs very poorly at extracting better features from the data in the client, which is also a common drawback of traditional FL architectures. HierFL also constrains the its hierarchy to server-edge-client three levels which is very restrictive. This paper proposes that the specifically designed FL architecture can naturally have an excellent effect on feature extraction. Specifically, this paper proposes a multibranch multilevel federated learning (MBMLFL) framework to perform a better job at feature extraction, and each branch and level has its own self specific effect. The proposed framework is also friendlier as well as further enhances the privacy. By extending FedAvg, we design an M2DCFedAvg algorithm for the framework to optimize its objective function distributedly, and by conducting numerous experiments and analyses, we propose a general epoch selection principle for all the FL methods: the ${1^{T}_{d}}$ -n principle. Experiments with various data distributions are performed on MBMLFL to comprehensively research its characteristics. Moreover, to complete the personalization ability of our framework, we propose a plug-and-play dynamic-adjusting double flow personalization approach (DADFPA) for our MBMLFL method, which will further enhance the ability of the MBMLFL, such that it comprehensively and adaptively satisfies the generalization and personalization needs of clients. Experiments show that the MBMLFL and DADFPA can improve their baselines by an average of 5.26% and 2.24% points, which demonstrates their excellent performance.},
  archive      = {J_APIN},
  author       = {Ren, Maoye and Yu, Xinhai},
  doi          = {10.1007/s10489-022-04193-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13956-13971},
  shortjournal = {Appl. Intell.},
  title        = {Multibranch multilevel federated learning for a better feature extraction and a plug-and-play dynamic-adjusting double flow personalization approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LDoS attack traffic detection based on feature optimization
extraction and DPSA-WGAN. <em>APIN</em>, <em>53</em>(11), 13924–13955.
(<a href="https://doi.org/10.1007/s10489-022-04171-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rate Denial of Service (LDoS) attacks cause severe destructiveness to network security. Moreover, they are more difficult to detect because they are more hidden and lack distinguishing features. Consequently, packets belonging to legitimate users can be misplaced. The performance of a transport system can be degraded by frequently sending short bursts of packets. An attack program generates its own traffic. Additionally, a diverse array of attack strategies is available. Traditional detection methods cannot effectively extract attack features. This increases the difficulty of detecting LDoS attacks in streams of fluctuating normal traffic. In this study, we establish a method for detecting LDoS attacks. It is constructed using Laplacian eigenmap (LE) and a Wasserstein generative adversarial network (WGAN) with a denoising penalty and sample-augmented discriminator (DPSA-WGAN) based on deep learning. First, an unsupervised deep learning network (LENet) is generated using LE. Moreover, data are preprocessed by dimensionality reduction to extract low-dimensional LDoS attacks features. Subsequently, we build the DPSA-WGAN on top of the WGAN to detect LDoS attacks. Moreover, a denoising penalty term and a sample-augmented discriminator are added to improve the LDoS attacks feature generation. The denoising penalty is employed to improve the performance of the generator. Moreover, the sample-augmented discriminator learns a more accurate classification-decision hyperplane. We test our method on NS2 and test-bed platforms. Using our method, dimensionality reduction and sample generation are demonstrated to be more effective than with other techniques. When background noise traffic is added, the precision rate increases. Moreover, the proposed method shows high efficiency and has an outstanding noise-reduction ability. Furthermore, it is highly robust and can detect LDoS attacks in real time.},
  archive      = {J_APIN},
  author       = {Ma, Wengang and Liu, Ruiqi and Guo, Jin},
  doi          = {10.1007/s10489-022-04171-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13924-13955},
  shortjournal = {Appl. Intell.},
  title        = {LDoS attack traffic detection based on feature optimization extraction and DPSA-WGAN},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ensemble algorithm integrating consensus-clustering with
feature weighting based ranking and probabilistic fuzzy logic-multilayer
perceptron classifier for diagnosis and staging of breast cancer using
heterogeneous datasets. <em>APIN</em>, <em>53</em>(11), 13882–13923. (<a
href="https://doi.org/10.1007/s10489-022-04157-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a major threat, predominantly affecting the female population. Staging of cancer enables early detection and prognosis of patients, leading to determination of efficient and accurate treatment. Consequently, simplified models are required to integrate heterogeneous data for deriving knowledge about patients for further treatment. To achieve this goal, developing machine learning based diagnostic techniques is the predominant need. Prompted by these facts, a novel diagnostic model for staging of breast cancer infusing ensemble clustering, feature weighting based ranking of clusters and ensemble classification into benign or malignant class is developed. The proposed work constitutes of five different phases: data pre-processing, feature selection, ensemble clustering, ensemble classification, and staging of cancer. This work first employs Multiple Imputation Chained Equation for imputing missing values, followed by proposed feature selection technique employing Association Rules, Classification and Regression Tree, and Fuzzy Logic. Subsequently, a coupled clustering and classification algorithm based on consensus is developed to cluster features from different datasets using Self-Organizing Map and Decision Tree. A hierarchical clustering based ranking of these clusters using Multilinear Regression and Modified Fuzzy Analytical Hierarchical Process is proposed to prioritize features. Next, a staged classifier is developed integrating Probabilistic Fuzzy Logic and Multilayer Perceptron followed by feature extraction based staging of cancer. Finally, proposed work is validated on four datasets with various performance metrics using different combinations of train-test dataset. Moreover, k-fold cross-validation is implemented to eliminate biasedness. The detailed analysis of results of this work showcases superiority over other state-of-art methods in literature.},
  archive      = {J_APIN},
  author       = {Chatterjee, Subhashis and Das, Ananya},
  doi          = {10.1007/s10489-022-04157-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13882-13923},
  shortjournal = {Appl. Intell.},
  title        = {An ensemble algorithm integrating consensus-clustering with feature weighting based ranking and probabilistic fuzzy logic-multilayer perceptron classifier for diagnosis and staging of breast cancer using heterogeneous datasets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient lightweight CNN acceleration architecture for
edge computing based-on FPGA. <em>APIN</em>, <em>53</em>(11),
13867–13881. (<a
href="https://doi.org/10.1007/s10489-022-04251-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the system performance, volume and power restriction requirements in edge computing, single chip based on Field Programmable Gate Array (FPGA), with the characteristics of parallel execution, flexible configuration and power efficiency, is more desirable for realizing Convolutional Neural Network (CNN) acceleration. However, implementing a lightweight CNN with limited on-chip resources while maintaining high computing efficiency and utilization is still a challenging task. To achieve efficient acceleration with single chip, we implement Network-on-Chip (NoC) based on Processing Element (PE) that consists of multiple node arrays. Moreover, the computing and memory efficiencies of PE are optimized with a sharing function and hybrid memory. To maximize resource utilization, a theoretical model is constructed to explore the parallel parameters and running cycles of each PE. In the experimental results of LeNet and MobileNet, resource utilization values of 83.61% and 95.28% are achieved, where the throughput values are 53.3 Giga Operations Per Second (GOPS) and 41.9 GOPS, respectively. Power measurements show that the power efficiency is optimized to 77.25 GOPS/W and 85.51 GOPS/W on our platform, which is sufficient to realize efficient inference for edge computing.},
  archive      = {J_APIN},
  author       = {Wu, Ruidong and Liu, Bing and Fu, Ping and Chen, Haolin},
  doi          = {10.1007/s10489-022-04251-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13867-13881},
  shortjournal = {Appl. Intell.},
  title        = {An efficient lightweight CNN acceleration architecture for edge computing based-on FPGA},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved generalization performance of convolutional neural
networks with LossDA. <em>APIN</em>, <em>53</em>(11), 13852–13866. (<a
href="https://doi.org/10.1007/s10489-022-04208-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks (CNNs) have been used in many fields. Nowadays, CNNs have a high learning capability, and this learning capability is accompanied by a more complex model architecture. Complex model architectures allow CNNs to learn more data features, but such a learning process tends to reduce the training model’s ability to generalize to unknown data, and may be associated with problems of overfitting. Although many regularization methods have been proposed, such as data augmentation, batch normalization, and Dropout, research on improving generalization performance is still a common concern in the training process of robust CNNs. In this paper, we propose a dynamically controllable adjustment method, which we call LossDA, that embeds a disturbance variable in the fully-connected layer. The trend of this variable is kept consistent with the training loss, while the magnitude of the variable can be preset to adapt to the training process of different models. Through this dynamic adjustment, the training process of CNNs can be adaptively adjusted. The whole regularization process can improve the generalization performance of CNNs while helping to suppress overfitting. To evaluate this method, this paper conducts comparative experiments on MNIST, FashionMNIST, CIFAR-10, Cats_vs_Dogs, and miniImagenet datasets. The experimental results show that the method can improve the model performance of Light CNNs and Transfer CNNs (InceptionResNet, VGG19, ResNet50, and InceptionV3). The average maximum improvement in accuracy of Light CNNs is 4.62%, F1 is 3.99%, and Recall is 4.69%. The average maximum improvement accuracy of Transfer CNNs is 4.17%, F1 is 5.64%, and Recall is 4.05%.},
  archive      = {J_APIN},
  author       = {Liu, Juncheng and Zhao, Yili},
  doi          = {10.1007/s10489-022-04208-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13852-13866},
  shortjournal = {Appl. Intell.},
  title        = {Improved generalization performance of convolutional neural networks with LossDA},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to attribute reduction and rule acquisition
of formal decision context. <em>APIN</em>, <em>53</em>(11), 13834–13851.
(<a href="https://doi.org/10.1007/s10489-022-04139-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rule acquisition and attribute reduction are important research topics in formal concept analysis. Many existing rule-based attribute reduction algorithms are designed to computing all reductions by using discernibility functions and therefore these algorithms are NP-hard. To improve the applicability of rule-based attribute reduction algorithms, firstly, we propose a method to simplify the discernibility matrix such that fewer concepts need to be distinguished. Then a heuristics approach is presented to compute one reduction by using the ordered attributes method. In addition, a novel rule acquisition algorithm for OW-decision rules is presented. Some comparative analyses of the rule acquisition algorithm with the existing algorithms are examined which shows that the algorithms presented in this study behave well. And finally, we select some datasets from UCI datasets for taking experiments and illustrate the effectiveness and efficiency of our proposed reduction algorithms.},
  archive      = {J_APIN},
  author       = {Hu, Qian and Qin, Keyun and Yang, Han and Xue, Binbin},
  doi          = {10.1007/s10489-022-04139-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13834-13851},
  shortjournal = {Appl. Intell.},
  title        = {A novel approach to attribute reduction and rule acquisition of formal decision context},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-phase filtering of discriminative shapelets learning
for time series classification. <em>APIN</em>, <em>53</em>(11),
13815–13833. (<a
href="https://doi.org/10.1007/s10489-022-04043-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to the full-length methods for time series classification, shapelet-based methods acquire better interpretation, higher efficiency and precision since shapelets are discriminative features that well represent a time series. However, because of the large number of shapelets candidates, determining how to filter out shapelets with higher discriminability remains a challenge. In this paper, we propose a two-phase shapelets learning filtering framework for time series classification. Time series is first split into groups using the extreme key points, and local linear discriminant analysis with sparse group lasso regularizer is proposed to find projection vector. Then, a two-phase filtering framework is established to measure the sparsity of groups in order to quickly find the key group, where l2-norm is introduced in phase-1 and group sparsity degree is defined in phase-2 to filter sparse groups. Following that, only a few groups are used to extract shapelets and classify them, reducing the number of shapelets significantly. Finally, the group with the highest classification accuracy, i.e., the key group, is determined accurately. Extensive experiments on 28 time series datasets show that, when compared to other state-of-the-art shapelet-based classification methods, our proposed method achieves significant improvement and a competitive time cost.},
  archive      = {J_APIN},
  author       = {Li, Chen and Wan, Yuan and Zhang, Wenjing and Li, Huanhuan},
  doi          = {10.1007/s10489-022-04043-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13815-13833},
  shortjournal = {Appl. Intell.},
  title        = {A two-phase filtering of discriminative shapelets learning for time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Serial fuzzy system algorithm for predicting biological
activity of anti-breast cancer compounds. <em>APIN</em>,
<em>53</em>(11), 13801–13814. (<a
href="https://doi.org/10.1007/s10489-022-04134-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer has become one of the most common and deadly cancers in the world, and its treatment has been the focus of research. In the search for breast cancer drug candidate compounds, it is important to establish an effective quantitative structure-activity relationship for drug research and development. Neural networks have achieved high accuracy in this field, but with shortcomings of a large number of parameters, high model complexity, and poor interpretability. Therefore, a Serial Fuzzy System built by Subtractive clustering and ANFIS (SFSSA) layer by layer is proposed to explore a solution with better interpretability. Through the experiment in the bioactivity data set of candidate compounds with several models, the following conclusions are found: 1) The precision of SFSSA is better than that of classic linear regression; 2) SFSSA has fewer parameters and rules, and has better interpretability and generalization ability than classic neural network algorithms; 3) SFSSA has less training time and higher prediction accuracy than optimized TSK fuzzy system algorithm MBGD-RDA (Minibatch Gradient Descent with Regularization, DropRule, and AdaBound); 4) SFSSA’s subsystem with 15 inputs achieved best prediction effect. In short, SFSSA provides a new way to apply fuzzy systems for high-dimensional regression problems.},
  archive      = {J_APIN},
  author       = {Zhao, Wendi and Chen, Dewang and Zheng, Xiaoyu and Lu, Yuqi},
  doi          = {10.1007/s10489-022-04134-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13801-13814},
  shortjournal = {Appl. Intell.},
  title        = {Serial fuzzy system algorithm for predicting biological activity of anti-breast cancer compounds},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underdetermined blind source separation method based on
quantum archimedes optimization algorithm. <em>APIN</em>,
<em>53</em>(11), 13763–13800. (<a
href="https://doi.org/10.1007/s10489-022-03962-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of the existing underdetermined blind source separation methods is very sensitive to the initial parameters, meanwhile, the existing setting or selection methods of initial parameters need to be improved. Consequently, an effective underdetermined blind source separation method is proposed in this paper to solve the above engineering problems. Based on the Archimedes optimization algorithm and quantum computing theory, this paper proposes a novel intelligent algorithm named quantum Archimedes optimization algorithm, which solves the objective functions for engineering problems. Then the optimal solution obtained through the quantum Archimedes optimization algorithm is used as the initial clustering centers of the K-means clustering algorithm to achieve mixing matrix estimation. In addition, the original initial estimation signal setting of the source recovery based on radial basis function network is converted into an initial solution in population for quantum Archimedes optimization algorithm. The optimal solution obtained through the quantum Archimedes optimization algorithm is used as the new initial estimation signal setting to achieve source recovery. The simulation results show that the proposed underdetermined blind source separation method has higher accuracy than previous methods. The proposed method that is more robust and applicable makes the setting and selection of initial parameters more reasonable so that the performance is no longer limited to the initial parameters.},
  archive      = {J_APIN},
  author       = {Gao, Hongyuan and Zhang, Zhiwei and Wang, Shihao and Sun, Helin},
  doi          = {10.1007/s10489-022-03962-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13763-13800},
  shortjournal = {Appl. Intell.},
  title        = {Underdetermined blind source separation method based on quantum archimedes optimization algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient-only surrogate to resolve learning rates for robust
and consistent training of deep neural networks. <em>APIN</em>,
<em>53</em>(11), 13741–13762. (<a
href="https://doi.org/10.1007/s10489-022-04206-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mini-batch sub-sampling (MBSS) is favored in deep neural network training to reduce the computational cost. Still, it introduces an inherent sampling error, making the selection of appropriate learning rates challenging. The sampling errors can manifest either as a bias or variances in a line search. Dynamic MBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS results in point-wise discontinuous loss functions with smaller bias but larger variance than static sampled loss functions. However, dynamic MBSS has the advantage of having larger data throughput during training but requires resolving the complexity regarding discontinuities. This study extends the vanilla gradient-only surrogate line search (GOS-LS), a line search method using quadratic approximation models built with only directional derivative information for dynamic MBSS loss functions. We propose a conservative gradient-only surrogate line search (GOS-LSC) with strong convergence characteristics with a defined optimality criterion. For the first time, we investigate both GOS-LS’s and GOS-LSC’s performance on various optimizers, including SGD, RMSProp, and Adam on ResNet-18 and EfficientNet-B0. We also compare GOS-LS and GOS-LSC against the other existing learning rate methods. We quantify both the best-performing and most robust algorithms. For the latter, we introduce a relative robust criterion that allows us to quantify the difference between an algorithm and the best performing algorithm for a given problem. The results show that training a model with the recommended learning rate for a class of search directions helps to reduce the model errors in multimodal cases. The results also show that GOS-LS ranked first in training and test results, while GOS-LSC ranked third and second in training and test results among nine other learning rate strategies.},
  archive      = {J_APIN},
  author       = {Chae, Younghwan and Wilke, Daniel N. and Kafka, Dominic},
  doi          = {10.1007/s10489-022-04206-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13741-13762},
  shortjournal = {Appl. Intell.},
  title        = {Gradient-only surrogate to resolve learning rates for robust and consistent training of deep neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JointGraph: Joint pre-training framework for traffic
forecasting with spatial-temporal gating diffusion graph attention
network. <em>APIN</em>, <em>53</em>(11), 13723–13740. (<a
href="https://doi.org/10.1007/s10489-022-04218-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate highway traffic forecasting is a critical task in intelligent transportation systems (ITSs), which needs to capture complex spatiotemporal dependencies from traffic sensors data. Recently, spatial-temporal graph networks have become one famous technology for various traffic forecasting tasks. Nevertheless, most of these works have assumed that the correlations between the sensors are fixed, so that it is often unable to effectively deal with the more realistic and dynamic traffic environment. To tackle this issue, we propose a joint pretraining framework for traffic flow forecasting with a gating diffusion graph attention network (JointGraph). Specifically, our proposed joint training architecture consists of two parts: network reconstructor (reconstruct a discrete graph from input data) and spatiotemporal model (forecast traffic speed with the generated network). Owing to the capabilities of the network reconstructor in generating graph structure through input node features, it is possible to apply our spatiotemporal model among multiple datasets directly. In addition, our model can expediently use information from data-rich regions and improve traffic forecasting performance on data-lacking regions in highway networks. Experiments are conducted on traffic datasets: METR-LA, PEMS-BAY, and trajectory dataset: BikeNYC. The results show that our JointGraph achieves superior performance with the state-of-the-art baselines, which further indicate that our pre-training mechanism in JointGraph provides an effective solution for multi-dataset cooperative training.},
  archive      = {J_APIN},
  author       = {Kong, Xiangyuan and Wei, Xiang and Zhang, Jian and Xing, Weiwei and Lu, Wei},
  doi          = {10.1007/s10489-022-04218-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13723-13740},
  shortjournal = {Appl. Intell.},
  title        = {JointGraph: Joint pre-training framework for traffic forecasting with spatial-temporal gating diffusion graph attention network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of cooperative multi-agent deep reinforcement
learning. <em>APIN</em>, <em>53</em>(11), 13677–13722. (<a
href="https://doi.org/10.1007/s10489-022-04105-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.},
  archive      = {J_APIN},
  author       = {Oroojlooy, Afshin and Hajinezhad, Davood},
  doi          = {10.1007/s10489-022-04105-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13677-13722},
  shortjournal = {Appl. Intell.},
  title        = {A review of cooperative multi-agent deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source inverse-curriculum-based training for
low-resource dialogue generation. <em>APIN</em>, <em>53</em>(11),
13665–13676. (<a
href="https://doi.org/10.1007/s10489-022-04190-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective dialogue system needs amount of training data, but the existing training data is insufficient. Although the pre-trained model has made great progress in recent years, which can alleviate the problem of low resource dialogue to a certain extent, the pre-trained model is large and difficult to deploy. How to improve the performance of dialogue model without additional annotation data and decreasing the model volume has become a new challenge. We propose a multi-source data augmentation method for low-resource dialogue generation by utilizing inverse curriculum learning (inverse CL). Firstly, we adopt three data augmentation methods, including round-trip translation, paraphrasing and pre-trained model, to generate augmentation data. Next, we propose a new training strategy based on inverse CL to utilize different augmentation data. Comparing with the baselines, our method comprehensively outperform the baselines on all evaluation metrics, which shows the effectiveness of our proposed training strategy for dialogue generation. To the best of our knowledge, this is the first systematic investigation of data augmentation in the dialogue generation.},
  archive      = {J_APIN},
  author       = {Cui, Fuwei and Di, Hui and Huang, Hui and Ren, Hongjie and Ouchi, Kazushige and Liu, Ze and Xu, Jinan},
  doi          = {10.1007/s10489-022-04190-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13665-13676},
  shortjournal = {Appl. Intell.},
  title        = {Multi-source inverse-curriculum-based training for low-resource dialogue generation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User location-aware edge services selection based on
generative adversarial network and improved ant colony algorithm.
<em>APIN</em>, <em>53</em>(11), 13643–13664. (<a
href="https://doi.org/10.1007/s10489-022-04093-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a mobile edge environment, a mobile user’s location may change at any time. So the service coverages of current edge servers may exceed the mobile user’s location, which prevents the user from getting the service they need. For some low latency services, such as video optimized transmission and Internet of vehicles, it is necessary to maintain business continuity by timely selecting services provided by appropriate edge servers during the rapid movement of users. Therefore, this paper proposes a user location-aware edge services selection method based on generative adversarial network (GAN) and improved ant colony algorithm. Using this method to select edge services can provide users with uninterrupted services and improve service quality and user satisfaction. Firstly, a mobile user location prediction method based on the generative adversarial network and the attention mechanism is proposed. This method adopts graph attention network (GAT) and recurrent neural network (RNN) to model user in space and time dimensions, and uses GAN to predict the user’s next moment location, according to which, the edge services meeting user’s needs can be selected in advance. Then, an edge services selection method based on the predicted user location and improved ant colony algorithm is proposed. This method adopts ant colony algorithm with improved pheromone updating rules and single-cycle optimal list to select edge services, which enhances the global search ability of ants, accelerates the convergence speed of the algorithm and avoids the algorithm falling into a local optimal solution. Experimental results show that the proposed user location prediction method is more reasonable and accurate in predicting user location. The proposed edge service selection method has obvious advantages in aspects of algorithm convergence speed, accuracy and service response time.},
  archive      = {J_APIN},
  author       = {Zhang, Xiuguo and Tian, Shasha and Liu, Yufei and Cao, Zhiying},
  doi          = {10.1007/s10489-022-04093-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13643-13664},
  shortjournal = {Appl. Intell.},
  title        = {User location-aware edge services selection based on generative adversarial network and improved ant colony algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory prediction of flying vehicles based on deep
learning methods. <em>APIN</em>, <em>53</em>(11), 13621–13642. (<a
href="https://doi.org/10.1007/s10489-022-04098-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with the threat from flying vehicles, it is of great significance to accurately predict the flight trajectory of flying vehicles using the known historical position data. In this paper, we investigate eight typical types of maneuvers of flying vehicles. Then, according to the kinematics model of the flying vehicle, eight kinds of typical maneuver trajectory equations are introduced. Next, the data set for neural network training is created by varying the critical parameters of the trajectory equation. Accordingly, we train the offline Long-Short Term Memory (LSTM) using 1/12 of the dataset of trajectories, and results show that the trained network can classify types of trajectories accurately. Based on the results of classification using the offline LSTM, we propose two trajectory prediction methods, and both of them achieve excellent prediction with and without noise interference. Furthermore, we propose an online prediction method, and it can accurately predict the subsequent trajectory of flying vehicles when the type of trajectory is not clear. And we propose a filter to improve the accuracy of the online prediction method for trajectory prediction with noise. The simulation results verify that all three previously proposed methods can accurately predict the next move based on historical data.},
  archive      = {J_APIN},
  author       = {Tan, Minghu and Shen, Hong and Xi, Kang and Chai, Bin},
  doi          = {10.1007/s10489-022-04098-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13621-13642},
  shortjournal = {Appl. Intell.},
  title        = {Trajectory prediction of flying vehicles based on deep learning methods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-constraints in deep graph convolutional networks with
initial residual. <em>APIN</em>, <em>53</em>(11), 13608–13620. (<a
href="https://doi.org/10.1007/s10489-022-04222-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCN) can effectively extract rich information from non-structured data. However, in deep GCN models, the iterative propagation and updating of node information will lead to severe over-smoothing, which hampers the model’s performance. In our view, when the model suffers from over-smoothing, there will be little difference between the node features before and after updating. This paper proposes a more comprehensive smoothness metric according to nodes themselves, which considers both numerical and directional differences between nodes. Furthermore, (1) adding a similarity constraint between the initial features and the current layer features, which ensures the nodes’ representations avoid moving away from the initial features during the updating process; and (2) introducing a disparity constraint to the features of the nodes at each GCN layer to slow down the speed of node features becoming similar between before and after updating. We conduct extensive experiments on models with initial residual and achieve state-of-the-art results on several standard datasets.},
  archive      = {J_APIN},
  author       = {Chen, Hui and Li, Yuancheng},
  doi          = {10.1007/s10489-022-04222-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13608-13620},
  shortjournal = {Appl. Intell.},
  title        = {Multi-constraints in deep graph convolutional networks with initial residual},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View incremental decremental multi-view discriminant
analysis. <em>APIN</em>, <em>53</em>(11), 13593–13607. (<a
href="https://doi.org/10.1007/s10489-022-04168-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of multi-view methods in the literature are batch methods that need all data at the beginning of a training session. However, these methods are not scalable and need to retrain if new data are added or removed from the existing dataset. Incremental methods that support the addition of data samples have been developed, but these methods do not support the addition of views. To address this issue, we present view incremental decremental multi-view discriminant analysis (VIDMvDA) that updates a learned model without retraining when new views are added or existing views are deleted. VIDMvDA is presented in two forms: incremental learning and decremental unlearning. It provides closed-form solutions to update the within-class and the between-class scatter. We have measured the performance of our method against multi-view batch methods on criteria such as discriminability, order independence, classification accuracy, training time, and memory. We prove that using significantly less training time and memory, VIDMvDA constructs a similar discriminant subspace and has the same or better classification accuracy than the batch methods.},
  archive      = {J_APIN},
  author       = {Shivagunde, Saroj S. and Saradhi, V. Vijaya},
  doi          = {10.1007/s10489-022-04168-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13593-13607},
  shortjournal = {Appl. Intell.},
  title        = {View incremental decremental multi-view discriminant analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-organizing fuzzy neural network modeling approach
using an adaptive quantum particle swarm optimization. <em>APIN</em>,
<em>53</em>(11), 13569–13592. (<a
href="https://doi.org/10.1007/s10489-022-04133-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the model’s flexibility, this study proposes a self-organizing fuzzy neural network (SOFNN) modeling methodology based on an adaptive quantum particle swarm optimization algorithm (AQPSO). First, to address the shortcoming of premature convergence of the QPSO algorithm when dealing with complex problems and to acquire the best balance between the exploration and exploitation of the algorithm, a cooperative adaptive adjustment strategy for attractor, coefficient, and boundary is designed. Second, to obtain a suitable number of fuzzy rules and optimal premise parameters, the fitness function is constructed using system accuracy (RMSE) and network complexity (rule number) in the learning process. Simultaneously, an enhanced fuzzy recursive least square (FRLS) algorithm is designed to estimate the output weights of the FNN to identify the nonlinear dynamical system effectively. Furthermore, to ensure that the presented AQPSO-SOFNN can efficiently solve practical engineering problems, Lyapunov stability theory is adopted to prove its convergence in detail. Finally, four testing cases, including identification of Mackey-Glass time series, modeling of concrete compressive strength (CCS), prediction of carbon dioxide, and soft-sensing of effluent total phosphorus (TP), are utilized to verify the usefulness of the proposed AQPSO-SOFNN-based modeling approach. The simulation results of four testing cases demonstrate that the designed AQPSO-SOFNN has high prediction accuracy with a parsimonious network topology. The MATLAB source codes of AQPSO-SOFNN and other comparison algorithms can be downloaded from https://github.com/hyitzhb/AQPSO-SOFNN.git .},
  archive      = {J_APIN},
  author       = {Zhou, Hongbiao and Li, Yang and Xu, Haoyuan and Su, Yan and Chen, Lianghai},
  doi          = {10.1007/s10489-022-04133-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13569-13592},
  shortjournal = {Appl. Intell.},
  title        = {A self-organizing fuzzy neural network modeling approach using an adaptive quantum particle swarm optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). A hybrid framework for multivariate long-sequence time
series forecasting. <em>APIN</em>, <em>53</em>(11), 13549–13568. (<a
href="https://doi.org/10.1007/s10489-022-04110-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting provides insights into the far future by utilizing the available history observations. Recent studies have demonstrated the superiority of transformer-based models in dealing with multivariate long-sequence time series forecasting (MLTSF). However, the data complexity hinders the forecasting accuracy of current deep neural network models. In this article, a hybrid framework - Waveformer - is proposed, which decomposes fluctuated and complex data sequence into multiple stable and more predictable subsequences (components) through the entire forecasting process. Waveformer interactively learns temporal dependencies on each pair of decomposed components, which enhances its ability of learning their temporal dependencies. Moreover, Waveformer treats the implicit and dynamic dependencies among variables as a set of dynamic direct graphs. Based on which, an attention adaptive graph convolution net (AAGCN) is designed, which combines self-attention and adaptive direct graph convolution to capture multivariate dynamic dependencies in a flexible manner. The experimental results on six public datasets show that Waveformer considerably outperforms a varied range of state-of-the-art benchmarks, with at the most 54.3% relative improvement.},
  archive      = {J_APIN},
  author       = {Wang, Xiaohu and Wang, Yong and Peng, Jianjian and Zhang, Zhicheng and Tang, Xueliang},
  doi          = {10.1007/s10489-022-04110-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13549-13568},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid framework for multivariate long-sequence time series forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cascaded refined rgb-d salient object detection network
based on the attention mechanism. <em>APIN</em>, <em>53</em>(11),
13527–13548. (<a
href="https://doi.org/10.1007/s10489-022-04186-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The RGB-D salient object detection algorithm simulates human attention behavior and attempts to locate the most visually prominent object(s) from a set of RGB and depth images. Existing works often follow a deterministic decoding network, with few methods explicitly considering how to establish connections between features at various levels. To this end, we first propose a cascaded refined RGB-D salient object detection network based on the attention mechanism (CRNet), whose primary contribution is a cascaded refined upsampling network layout. Specifically, we have developed an adaptive channel transformation ratio α in the micro modification module of convolutional block attention (MM), adaptively adjusting the feature channel conversion ratio according to the original input depth feature level to maximize the integration of contextual information during the feature extraction phase. For the multi-modal feature interaction section, we propose a contextual feature aggregation module (ACF) consisting of separable convolution, dilated convolution, and adaptive averaging pooling. Extend multi-modal fused features’ receptive fields, reduce redundant information, and decrease background noise interference. Furthermore, we first propose a cascaded refined upsampling network, a precise refining process that includes personal refinement, team expansion, and sequential execution operations. Among them, most of the actions were performed in a new sequential refinement module based on attention mechanism (SRM-Wm). We put the training of CRNet under the supervision of a new hybrid loss function. The experiment results show that the structure of our model is simple but very effective and outperforms the 19 SOTAs on six public datasets using four metrics ( $\sim $ 1.6% improvement in F-measure vs. the top-ranked model: BBSNet-TIP2021). You can find the code and results of our method at https://github.com/guanyuzong/CR-Net .},
  archive      = {J_APIN},
  author       = {Zong, Guanyu and Wei, Longsheng and Guo, Siyuan and Wang, Yongtao},
  doi          = {10.1007/s10489-022-04186-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13527-13548},
  shortjournal = {Appl. Intell.},
  title        = {A cascaded refined rgb-d salient object detection network based on the attention mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering a cohesive football team through players’
attributed collaboration networks. <em>APIN</em>, <em>53</em>(11),
13506–13526. (<a
href="https://doi.org/10.1007/s10489-022-04199-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of team composition in multiplayer sports such as football has been a main area of interest within the field of the science of teamwork, which is important for improving competition results and game experience. Recent algorithms for the football team composition problem take into account the skill proficiency of players but not the interactions between players that contribute to winning the championship. To automate the composition of a cohesive team, we consider the internal collaborations among football players. Specifically, we propose a Team Composition based on the Football Players’ Attributed Collaboration Network (TC-FPACN) model, aiming to identify a cohesive football team by maximizing football players’ capabilities and their collaborations via three network metrics, namely, network ability, network density and network heterogeneity&amp;homogeneity. Solving the optimization problem is NP-hard; we develop an approximation method based on greedy algorithms and then improve the method through pruning strategies given a budget limit. We conduct experiments on two popular football simulation platforms. The experimental results show that our proposed approach can form effective teams that dominate others in the majority of simulated competitions.},
  archive      = {J_APIN},
  author       = {Yu, Shenbao and Zeng, Yifeng and Pan, Yinghui and Chen, Bilian},
  doi          = {10.1007/s10489-022-04199-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13506-13526},
  shortjournal = {Appl. Intell.},
  title        = {Discovering a cohesive football team through players’ attributed collaboration networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Know-UCP: Locally weighted linear regression based approach
for UCP estimation. <em>APIN</em>, <em>53</em>(11), 13488–13505. (<a
href="https://doi.org/10.1007/s10489-022-04160-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving accuracy in Software Effort Estimation (SEE) is probably the greatest difficulty in software project management. Since the Unified Modeling Language (UML) became increasingly noticeable in requirement analysis and software system design, researchers and practitioners became progressively intrigued to utilize the Use Case Point (UCP) metrics derived from UML diagrams for SEE. A lot of research has already been done in this area. Several researchers have used different regression and clustering-based models for UCP estimation. However, most of these models suffer from low accuracy. Out of different regression models, the linear regression (LR) model has been used in most studies. But, the LR model’s major problem is that it tries to fit a straight line on the data after model creation. Therefore, the LR model leads to underfitting for the non-linear data, which is generally the case with UCP estimation datasets. This study proposes a UCP Estimation method based on a Locally Weighted Linear Regression (LWLR) model, which we call Know-UCP, that tries to handle the abovementioned issues by assigning weights to the training projects. In addition, it has been found that all the UCP variables are not significant in UCP estimation, affecting the model’s performance. So, we performed a significance analysis in the proposed model to find the most significant predictors for UCP estimation. Further, we compare the proposed approach with the other models and found that the proposed Know-UCP approach performs better than the other models with reference to various performance measures.},
  archive      = {J_APIN},
  author       = {Shukla, Suyash and Kumar, Sandeep},
  doi          = {10.1007/s10489-022-04160-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13488-13505},
  shortjournal = {Appl. Intell.},
  title        = {Know-UCP: Locally weighted linear regression based approach for UCP estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SubTST: A consolidation of sub-word latent topics and
sentence transformer in semantic representation. <em>APIN</em>,
<em>53</em>(11), 13470–13487. (<a
href="https://doi.org/10.1007/s10489-022-04184-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most applications, text understanding and representation always play an important role, especially in automatic processing. Together with the surface features of words, topic information is highly meaningful and essential to provide the context meaning in the text representation. Recently, the integration of linguistic features and topic information has not received the close critical attention. With the aim to take advantage of topic information, we propose a novel approach to integrate the topic features into the most popular language models which are called the Sub-word Latent Topic and Sentence Transformer (SubTST). Inspired by Sentence-BERT and tBERT, our proposed architecture has a significant chance to learn and incorporate topic information with linguistic features. The most strength of our proposed approach comes from the delicate combination between latent topic information and linguistic features of language models instead of only utilizing topic information in the previous works. The comparison in experiments and ablation studies against competitive baselines proves the strength of our proposed approach in most benchmark datasets in both Semantic Textual Similarity and Semantic Similarity Detection.},
  archive      = {J_APIN},
  author       = {Dang, Binh and Le, Tung and Nguyen, Le-Minh},
  doi          = {10.1007/s10489-022-04184-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13470-13487},
  shortjournal = {Appl. Intell.},
  title        = {SubTST: A consolidation of sub-word latent topics and sentence transformer in semantic representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A task allocation algorithm based on reinforcement learning
in spatio-temporal crowdsourcing. <em>APIN</em>, <em>53</em>(11),
13452–13469. (<a
href="https://doi.org/10.1007/s10489-022-04151-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the pervasiveness of dynamic task allocation in sharing economy applications, online bipartite graph matching has attracted more and more research attention. In sharing economy applications, crowdsourcing platforms need to allocate tasks to workers dynamically. Previous studies have low allocation utility. To increase the allocation utility of the Spatio-temporal crowdsourcing system, this paper proposes a dynamic delay bipartite matching(DDBM) problem, and designs Value Based Task Allocation(VBTA) and Policy Gradient Based Task Allocation(PGTA) frameworks respectively. According to the current state, VBTA and PGTA could enhance the allocation utility by selecting appropriate thresholds. The convergence of the algorithm is proved. Extensive experimental results on two real datasets demonstrate that the proposed algorithms are superior to the existing algorithms in effectiveness and efficiency.},
  archive      = {J_APIN},
  author       = {Zhao, Bingxu and Dong, Hongbin and Wang, Yingjie and Pan, Tingwei},
  doi          = {10.1007/s10489-022-04151-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13452-13469},
  shortjournal = {Appl. Intell.},
  title        = {A task allocation algorithm based on reinforcement learning in spatio-temporal crowdsourcing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LMA: Lightweight mixed-domain attention for efficient
network design. <em>APIN</em>, <em>53</em>(11), 13432–13451. (<a
href="https://doi.org/10.1007/s10489-022-04170-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms, benefiting from the capability of modeling feature inter-dependencies among channels or spatial locations, have been demonstrated to have great potential in improving the performance of deep convolutional neural networks. However, most existing methods are dedicated to separately developing more intricate channel attention or spatial attention modules to achieve good performance, which inevitably results in losing important information and increasing model overhead. To alleviate this dilemma, in this paper, we propose a novel architecture unit called the lightweight mixed-domain attention (LMA) module. First, LMA aggregates spatial features by using two direction-aware 1D average pooling, which not only captures contextual long-range dependencies but also retains accurate positional information. Subsequently, it adaptively models inter-channel relationships by utilizing our proposed nonlinear local cross-channel interaction strategy, substantially decreasing model overhead while maintaining competitive performance. Our LMA is lightweight yet efficient and can be flexibly plugged into various classic backbones including lightweight MobileNetV2 and heavyweight ResNets as a plug-and-play module. Extensive experimental results of image classification on ImageNet-1K and object detection and instance segmentation on MS COCO demonstrate the superiority of our method against state-of-the-art (SOTA) counterparts. Furthermore, we verify our advanced philosophy through the Grad-CAM++ visualization results.},
  archive      = {J_APIN},
  author       = {Yu, Yang and Zhang, Yi and Song, Zhe and Tang, Cheng-Kai},
  doi          = {10.1007/s10489-022-04170-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13432-13451},
  shortjournal = {Appl. Intell.},
  title        = {LMA: Lightweight mixed-domain attention for efficient network design},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nash equilibrium inspired greedy search for solving flow
shop scheduling problems. <em>APIN</em>, <em>53</em>(11), 13415–13431.
(<a href="https://doi.org/10.1007/s10489-022-04090-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new methaheurstic to solve the flow shop scheduling problem which is considered as an $\mathcal {N}\mathcal {P}-$ hard problem for relatively high dimensions. The flow shop scheduling problems are commonly encountered in many industrial applications and manufacturing systems. For the purpose, a mixed integer linear programming model is presented to articulate the relationship between the objective function and the constraints of the problem. A proposed hybrid greedy algorithm based on the Nash equilibrium concept and the genetic operators is an attempt to outperform the classical algorithms frequently employed to approach the optimal solution of scheduling problems. In order to minimize the makespan criterion, various computational experiments were conducted for different size of the problem. Furthermore, a comparative study is performed to assess the developed methaheuristic against other algorithms. Simulations have shown that the proposed procedure is the most effective and robust.},
  archive      = {J_APIN},
  author       = {Belabid, Jabrane and Aqil, Said and Allali, Karam},
  doi          = {10.1007/s10489-022-04090-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13415-13431},
  shortjournal = {Appl. Intell.},
  title        = {Nash equilibrium inspired greedy search for solving flow shop scheduling problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-feature fusion enhanced transformer with multi-layer
fused decoding for image captioning. <em>APIN</em>, <em>53</em>(11),
13398–13414. (<a
href="https://doi.org/10.1007/s10489-022-04202-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objects’ semantic information of the image is vital for image captioning. Though some methods have used semantic information, the alignment between the specific semantic feature and the corresponding visual feature has not been explored. In this paper, we propose a novel Multi-Feature Fusion enhanced Transformer (MFF-Transformer) for image captioning, which can realize the multi-feature fusion by aligning the specific semantic features with their corresponding visual features for achieving a better visual feature representation. In the MFF-Transformer, a novel Interacted Multi-Feature REpresentation (IMFRE) module is proposed, which effectively fuses the visual features with the semantic features by adaptive pooling to obtain global and local multi-feature information for enhancing the visual features. In addition, a new Multi-Layer Features Fusion (MLFF) module is proposed to achieve complete and valid multi-layer decoding information for utilizing the hierarchical context of the MFF-Transformer model. Experiments on the MSCOCO dataset illustrate that our proposed MFF-Transformer can achieve good performance and outperform other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Jing and Fang, Zhongjun and Wang, Zhe},
  doi          = {10.1007/s10489-022-04202-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13398-13414},
  shortjournal = {Appl. Intell.},
  title        = {Multi-feature fusion enhanced transformer with multi-layer fused decoding for image captioning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). S-LMPNet: A super-lightweight multi-stage progressive
network for image super-resolution. <em>APIN</em>, <em>53</em>(11),
13378–13397. (<a
href="https://doi.org/10.1007/s10489-022-04185-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) has achieved great success in recent years due to the representation ability of large and deep models. However, these models usually have a large number of network parameters, which hinders their application to real-world scenarios. To reduce the number of parameters in the SISR models, we propose a super-lightweight model termed s-LMPNet with a multi-stage architecture. Specifically, s-LMPNet includes three sub-networks, which are organized in a cascaded way. Each sub-network is constructed with multiple lightweight cross-group skip-connecting blocks (CGSCBs). To enhance the model performance, a residual feature fusion attention module is adopted to integrate intermediate features from different CGSCBs in a self-adaptive weighted way. A cross-stage feature propagation module is used to propagate the information from the low stage to the high stage, thereby making the network optimization procedure more stable. Extensive experiments are performed on commonly-used super-resolution benchmarks. Experiment results have shown that s-LMPNet achieves promising performance compared to other state-of-the-art lightweight super-resolution methods.},
  archive      = {J_APIN},
  author       = {Li, Meng and Ma, Bo and Liu, Ying and Zhang, Yulin},
  doi          = {10.1007/s10489-022-04185-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13378-13397},
  shortjournal = {Appl. Intell.},
  title        = {S-LMPNet: A super-lightweight multi-stage progressive network for image super-resolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection and clustering based web service selection
using QoSs. <em>APIN</em>, <em>53</em>(11), 13352–13377. (<a
href="https://doi.org/10.1007/s10489-022-04042-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web Services act as a backbone to realize the smart city concept. Web service technology is useful to offer various services as part of the smart city. From the smart city perspective, the fundamental problem is selecting the web services offering desired functionality and meeting an end-user’s quality of Service (QoS) expectations. With the rapid increase in the number of web services with similar functionality, the performance of the selection mechanism degrades, and the complexity of the web service selection mechanism increases. A web service selection method is presented in this work, which combines feature selection and QoS-based clustering for an improved web service selection mechanism. The presented method aims to improve the performance and quality of the web service selection mechanism and reduce the complexity. An empirical analysis of the presented method using QoS parameters is performed on the real-world web services QWS dataset, available in the public repository. We compare the performance of the presented method with other state-of-the-art clustering techniques using different evaluation measures based on various performance parameters for the quality of clustering. The experimental results showed that integrating feature selection and QoS-based clustering in the selection mechanism improves the quality of clusters and ultimately improves the performance of the web service selection.},
  archive      = {J_APIN},
  author       = {Purohit, Lalit and Rathore, Santosh S. and Kumar, Sandeep},
  doi          = {10.1007/s10489-022-04042-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13352-13377},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection and clustering based web service selection using QoSs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying key grid cells for crowd flow predictions based
on CNN-based models with the grad-CAM kit. <em>APIN</em>,
<em>53</em>(11), 13323–13351. (<a
href="https://doi.org/10.1007/s10489-022-03988-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholars have long sought to identify key city locations that have a pronounced effect on the flow of people under various conditions. Identifying key locations makes it possible for government and/or enterprises to obtain accurate estimates as to the flow of people and thereby formulate reasonable management strategies. Note however that much of the previous research based on professional knowledge or machine learning fails to provide accurate results. Some researchers have employed CNN-based models to predict the flow of people, claiming that those models can extract freatures from multiple locations to improve prediction accuracy. Theoretically, the features extracted using CNN-based models could be used themselves as key locations for specific problems; however, the fact that the features are consolidated via multiple operations often renders interpretation difficult when applied to a real-world setting. In the current study, we developed a novel approach to the identification of key locations based on the results of a CNN-based model and the Grad-Cam kit. The structure of a CNN-based models can have a profound effect on the results of the Grad-Cam kit; therefore, we compared the results obtained using three state-of-art models as well as our CNN-LSTM. Actual flow patterns based on telecommunication data in Taipei were used to verify the efficacy of the proposed method.},
  archive      = {J_APIN},
  author       = {Chiu, Sheng-Min and Liou, Yow-Shin and Chen, Yi-Chung and Lee, Chiang and Shang, Rong-Kang and Chang, Tzu-Yin},
  doi          = {10.1007/s10489-022-03988-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13323-13351},
  shortjournal = {Appl. Intell.},
  title        = {Identifying key grid cells for crowd flow predictions based on CNN-based models with the grad-CAM kit},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal stroke learning with policy gradient approach for
robotic table tennis. <em>APIN</em>, <em>53</em>(11), 13309–13322. (<a
href="https://doi.org/10.1007/s10489-022-04131-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to play table tennis is a challenging task for robots, as a wide variety of strokes are required. Recent advances have shown that deep Reinforcement Learning (RL) is able to successfully learn the optimal actions in a simulated environment. However, the applicability of RL in real scenarios remains limited due to the high exploration effort. In this work, we propose a realistic simulation environment in which multiple models are built for the dynamics of the ball and the kinematics of the robot. Instead of training an end-to-end RL model, a novel policy gradient approach with TD3 backbone is proposed to learn the racket strokes based on the predicted state of the ball at the hitting time. In the experiments, we show that the proposed approach significantly outperforms the existing RL methods in simulations. Furthermore, to cross the domain from simulation to reality, we adopt an efficient retraining method and test it in three real scenarios. The resulting success rate is 98% and the distance error is around 24.9 cm. The total training time is about 1.5 hours.},
  archive      = {J_APIN},
  author       = {Gao, Yapeng and Tebbe, Jonas and Zell, Andreas},
  doi          = {10.1007/s10489-022-04131-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13309-13322},
  shortjournal = {Appl. Intell.},
  title        = {Optimal stroke learning with policy gradient approach for robotic table tennis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reason more like human: Incorporating meta information into
hierarchical reinforcement learning for knowledge graph reasoning.
<em>APIN</em>, <em>53</em>(11), 13293–13308. (<a
href="https://doi.org/10.1007/s10489-022-04147-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, reasoning over knowledge graphs (KGs) has been widely adapted to empower retrieval systems, recommender systems, and question answering systems, generating a surge in research interest. However, recently developed reasoning methods usually lack interpretability, and can hardly tackle the large-scale action space problem over KGs. Inspired by the ability of human hierarchical decision making, we propose a multi-hop reasoning framework with deep reinforcement learning (RL) to fill this gap, which incorporates meta information into hierarchical reasoning over KGs. We first use optimization-based meta learning method to initialize parameters for RL agents, allowing for efficient adaptation for tasks in a few gradient steps. Then, a hierarchical RL framework is designed to decompose reasoning tasks into several sub-tasks and solve them separately, performed more efficient and natural than directly solving the entire problem. We further evaluated our model through different tasks on five real world datasets. The experimental results indicate that our method outperforms state-of-the-art baseline models without losing interpretability.},
  archive      = {J_APIN},
  author       = {Xia, Yi and Luo, Junyong and Lan, Mingjing and Zhou, Gang and Li, Zhibo and Liu, Shuo},
  doi          = {10.1007/s10489-022-04147-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13293-13308},
  shortjournal = {Appl. Intell.},
  title        = {Reason more like human: Incorporating meta information into hierarchical reinforcement learning for knowledge graph reasoning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Detection and localization of splicing on remote sensing
images using image-to-image transformation. <em>APIN</em>,
<em>53</em>(11), 13275–13292. (<a
href="https://doi.org/10.1007/s10489-022-04126-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images can be easily tampered with user-friendly tools to hide important information. It necessitated the development of automatic splicing detection methods. The existing few methods concentrate on the semantic content in images for tamper detection and are not robust. On the contrary, we hypothesize that residual noise is independent of the semantic content and embeds the tampering traces; it is helpful for splice detection. In view of this, we focus on residual noise and formulate the problem as an image-to-image transformation, and model it using a U-net architecture. To suppress semantic content and extract the residual noise, we introduce a constrained convolutional layer in the U-net model. The model processes the input image and yields a map that localizes tampering in case of splicing. The model is trained using the conditional generative adversarial network (cGAN) framework. The loss function is composed of the cross-entropy, the Jaccard, and the end-point error (EPE) loss functions to enhance the detection and localization of tampered regions. To evaluate the proposed method, we develop a new dataset containing remote sensing images from different satellites and aerial sensors. The model detects splicing at pixel and image levels with high accuracy. It shows good robustness against well-known post-processing operations, including Gaussian blurring (GB) and white additive Gaussian noise (WAGN).},
  archive      = {J_APIN},
  author       = {Alsughayer, Rawan and Hussain, Muhammad and Saeed, Fahman and AboalSamh, Hatim},
  doi          = {10.1007/s10489-022-04126-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13275-13292},
  shortjournal = {Appl. Intell.},
  title        = {Detection and localization of splicing on remote sensing images using image-to-image transformation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolving graph convolutional network for dynamic
functional brain network. <em>APIN</em>, <em>53</em>(11), 13261–13274.
(<a href="https://doi.org/10.1007/s10489-022-04203-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain networks have received extensive attention because of its important significance in understanding human brain organization and analyzing neuropsychiatric diseases. Existing methods are mostly based on the static functional brain network. However, the static brain network only considers the correlation between global signals, and cannot reflect the changes of brain information over time. In reality, the brain activities are constantly changing. Therefore, the dynamic functional brain network is proposed in order to reflect the variations of the signal. In recent years, as an important and effective method, graph convolutional network has been widely used in the analysis of brain networks. In consequence, an evolving graph convolutional network based on dynamic functional brain network is proposed. The network not only considers the neighbor node information in the current snapshot, but also considers the neighbor node information in the precursor and the subsequent time in the process of convolution. Furthermore, four kinds of convolution rules are put forward based on the evolving graph convolutional network. Alzheimer’s disease diagnosis, as a representative neuropsychiatric diseases analysis method, is used to evaluate the model, and experiments are performed on the open dataset. The experimental results show that the proposed evolving graph convolutional network can improve the diagnostic accuracy to 99.16%, which proves the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Xinlei and Xin, Junchang and Wang, Zhongyang and Chen, Qi and Wang, Zhiqiong},
  doi          = {10.1007/s10489-022-04203-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13261-13274},
  shortjournal = {Appl. Intell.},
  title        = {An evolving graph convolutional network for dynamic functional brain network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opposition-based sine cosine optimizer utilizing refraction
learning and variable neighborhood search for feature selection.
<em>APIN</em>, <em>53</em>(11), 13224–13260. (<a
href="https://doi.org/10.1007/s10489-022-04201-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes new improved binary versions of the Sine Cosine Algorithm (SCA) for the Feature Selection (FS) problem. FS is an essential machine learning and data mining task of choosing a subset of highly discriminating features from noisy, irrelevant, high-dimensional, and redundant features to best represent a dataset. SCA is a recent metaheuristic algorithm established to emulate a model based on sine and cosine trigonometric functions. It was initially proposed to tackle problems in the continuous domain. The SCA has been modified to Binary SCA (BSCA) to deal with the binary domain of the FS problem. To improve the performance of BSCA, three accumulative improved variations are proposed (i.e., IBSCA1, IBSCA2, and IBSCA3) where the last version has the best performance. IBSCA1 employs Opposition Based Learning (OBL) to help ensure a diverse population of candidate solutions. IBSCA2 improves IBSCA1 by adding Variable Neighborhood Search (VNS) and Laplace distribution to support several mutation methods. IBSCA3 improves IBSCA2 by optimizing the best candidate solution using Refraction Learning (RL), a novel OBL approach based on light refraction. For performance evaluation, 19 real-wold datasets, including a COVID-19 dataset, were selected with different numbers of features, classes, and instances. Three performance measurements have been used to test the IBSCA versions: classification accuracy, number of features, and fitness values. Furthermore, the performance of the last variation of IBSCA3 is compared against 28 existing popular algorithms. Interestingly, IBCSA3 outperformed almost all comparative methods in terms of classification accuracy and fitness values. At the same time, it was ranked 15 out of 19 in terms of number of features. The overall simulation and statistical results indicate that IBSCA3 performs better than the other algorithms.},
  archive      = {J_APIN},
  author       = {Abed-alguni, Bilal H. and Alawad, Noor Aldeen and Al-Betar, Mohammed Azmi and Paul, David},
  doi          = {10.1007/s10489-022-04201-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13224-13260},
  shortjournal = {Appl. Intell.},
  title        = {Opposition-based sine cosine optimizer utilizing refraction learning and variable neighborhood search for feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A single-valued neutrosophic gaussian process regression
approach for stability prediction of open-pit mine slopes.
<em>APIN</em>, <em>53</em>(11), 13206–13223. (<a
href="https://doi.org/10.1007/s10489-022-04089-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of the stability of open-pit mine slopes is usually one of the difficult problems faced by mining engineers and researchers, which is caused by the indeterminacy and uncertainty contained in the influence factors of stability of open-pit mine slopes. The single-valued neutrosophic number (SVNN) is very suitable for representing the information of truth, falsity, and indeterminacy in inconsistent and indeterminate situations. This paper proposes a method combining SVNN with Gaussian process regression (SVNN-GPR) to evaluate the open-pit mine slopes stability in uncertain environment. At first, the proposed method expresses the uncertain influence factors/indices (dip of potential failure plane, cohesion, friction angle, etc.) and the stability state of open-pit mine slopes as SVNNs. Then, the SVNN-GPR is established to obtain the latent complicated relationship between SVNN influence factors and slope stability. Finally, we use the score values of the SVNN output of the proposed method to predict the stability of each slope. To test the performance of the proposed method, we collected 286 slope cases from Zhejiang Province, China. The practical application results reflect that the SVNN-GPR method based on the exponential covariance function and isotropic distance measure has best training and testing results, and the training accuracy reaches 98.0% and the testing accuracy reaches 96.5%. This shows the proposed SVNN-GPR method provides a new effective way for predicting open-pit mine slopes stability in inconsistent and indeterminate situations.},
  archive      = {J_APIN},
  author       = {Qin, Jibo and Ye, Jun and Sun, Xiaoming and Yong, Rui and Du, Shigui},
  doi          = {10.1007/s10489-022-04089-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13206-13223},
  shortjournal = {Appl. Intell.},
  title        = {A single-valued neutrosophic gaussian process regression approach for stability prediction of open-pit mine slopes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized lane change decision algorithm using deep
reinforcement learning approach. <em>APIN</em>, <em>53</em>(11),
13192–13205. (<a
href="https://doi.org/10.1007/s10489-022-04172-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop driving automation technologies for humans, a human-centered methodology should be adopted for safety and satisfactory user experience. Automated lane change decision in dense highway traffic is challenging, especially when considering different driver preferences. This paper proposes a personalized lane change decision algorithm based on deep reinforcement learning. Firstly, driving experiments are carried out on a moving-base simulator. Based on the analysis of the experiment data, three personalization indicators are selected to describe the driver preferences in lane-change decisions. Then, a deep reinforcement learning (RL) approach is applied to design human-like agents for automated lane change decisions to capture the driver preferences, with refined rewards using the three personalization indicators. Finally, the trained RL agents and benchmark agents are tested in a two-lane highway driving scenario. Results show that the proposed algorithm can achieve higher consistency of lane change decision preferences than the comparison algorithm.},
  archive      = {J_APIN},
  author       = {Li, Daofei and Liu, Ao},
  doi          = {10.1007/s10489-022-04172-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13192-13205},
  shortjournal = {Appl. Intell.},
  title        = {Personalized lane change decision algorithm using deep reinforcement learning approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Business process remaining time prediction using explainable
reachability graph from gated RNNs. <em>APIN</em>, <em>53</em>(11),
13178–13191. (<a
href="https://doi.org/10.1007/s10489-022-04192-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gated recurrent neural networks (RNNs) are successfully applied to predict the remaining time of business processes. Existing methods typically train multiple prediction models for prefixes bucketing. Furthermore, the gated RNNs are more like black boxes and lack interpretability. An explainable gated RNN using a reachability graph is proposed to improve the results of prediction. First, prefixes of the event log are generated to train a single prediction model, and hidden states of the gated RNN are saved. Second, a Petri net and its corresponding reachability graph are constructed by taking an event log as input. Next, the hidden states of the gated RNN are mapped to a reachability state of the reachability graph by the decoding mapping to explain the remaining time prediction model, i.e., gated RNN. Finally, our method is validated by six real-life event logs. Based on the experimental results, it is shown that a mapping from the hidden states of the gated RNN to a reachability state of the reachability graph is established, and the gated RNN that recognizes transition sequences is also explained for improving the performance of remaining time prediction of business processes.},
  archive      = {J_APIN},
  author       = {Cao, Rui and Zeng, Qingtian and Ni, Weijian and Duan, Hua and Liu, Cong and Lu, Faming and Zhao, Ziqi},
  doi          = {10.1007/s10489-022-04192-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13178-13191},
  shortjournal = {Appl. Intell.},
  title        = {Business process remaining time prediction using explainable reachability graph from gated RNNs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view improved sequence behavior with adaptive
multi-task learning in ranking. <em>APIN</em>, <em>53</em>(11),
13158–13177. (<a
href="https://doi.org/10.1007/s10489-022-04088-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Click through rate (CTR) and Conversion Rate (CVR) are core tasks in e-commerce recommender systems. Sequence behavior and multi-task learning have been widely used in CTR and CVR. Based on the concept of a transformer, we develop a technique of time and space feature representation for the prediction, which can capture high-level information better. In order to formulate user’s different interests from historical sequence behavior, we design multi-task learning to improve multiple objectives simultaneously. It is difficult to turn the super parameters as the tasks increasing. In this paper, we propose an adaptive learning mixture-of-experts approach, which tackles this challenge and can learn super parameters among tasks automatically. It not only saves resources but also improves the performance with cognitive of the model. Furthermore, to enhance the flexibility, we improve the loss function with a constrained joint strategy and introduce RESNET mechanism. We design feature-cross-unit module, augment-expert module, and topK-dispatch module, which assist multi-task learning to improve better. Experiments on public dataset and our library dataset demonstrate the superiority of our model over the state-of-art method. Our method achieves + 2.29% AUC gain in the CTR task and + 1.81% AUC gain in the CVR task, which is a significant improvement and demonstrates the effectiveness of proposed approach.},
  archive      = {J_APIN},
  author       = {Wang, Yingshuai and Zhang, Dezheng and Wulamu, Aziguli},
  doi          = {10.1007/s10489-022-04088-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13158-13177},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view improved sequence behavior with adaptive multi-task learning in ranking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dual-channel and multi-granularity gated graph attention
network for aspect-based sentiment analysis. <em>APIN</em>,
<em>53</em>(11), 13145–13157. (<a
href="https://doi.org/10.1007/s10489-022-04198-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Aspect-Based Sentiment Analysis(ABSA) aims to determine the sentiment polarity of a specific aspect. Existing approaches use graph attention networks(GAT) to model syntactic information with dependency trees. However, these methods do not consider the noise of the dependency tree and ignore the sentence-level feature. To this end, we propose the Dual-Channel and Multi-Granularity Gated Graph Attention Network(DMGGAT) to jointly consider semantics and syntactic information of multiple granularity features generated by GAT and BERT, in which BERT alleviates the instability of the dependency tree and enhance the semantic information lost in the graph calculation. First, We propose a two-channel structure composed of BERT and GAT, enabling syntactic and semantic information generated by BERT to assist GAT. Furthermore, an aspect-based attention mechanism is used to generate sentence-level features. Finally, a newly designed gated module is introduced to integrate the aspect(fine-Granularity) and sentence-level (coarse-Granularity) features from the two channels to classify jointly. The experimental results show that our model achieves advanced performance compared to the current model on three extensive datasets.},
  archive      = {J_APIN},
  author       = {Wang, Yong and Yang, Ningchuang and Miao, Duoqian and Chen, Qiuyi},
  doi          = {10.1007/s10489-022-04198-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13145-13157},
  shortjournal = {Appl. Intell.},
  title        = {Dual-channel and multi-granularity gated graph attention network for aspect-based sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using class-level regularized
self-representation. <em>APIN</em>, <em>53</em>(11), 13130–13144. (<a
href="https://doi.org/10.1007/s10489-022-04177-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims at selecting representative features from the original high-dimensional feature set, and it has drawn much attention in most real-world applications like data mining and pattern recognition. This paper studies feature selection problem from the viewpoint of feature self-representation. Traditionally, feature self-representation is only performed on the whole-level reconstruction, whereas the feature selection ability is insufficient owing to the intra-class variations. To address this problem, we propose a new feature selection method, i.e., class-level regularized self-representation (CLRSR). In the proposed method, a class-level reconstruction term is designed to reduce intra-class variations of the samples from different categories. By jointly optimizing the whole-level reconstruction and the class-level reconstruction, CLRSR is able to select more discriminative and informative features. Moreover, an iterative algorithm is proposed to minimize cost function of CLRSR, and its convergence is proven in theory. By comparing with several state-of-the-art feature selection methods, experimental evaluations on six benchmark datasets have verified effectiveness and superiority of CLRSR.},
  archive      = {J_APIN},
  author       = {Lu, Zhenghua and Chu, Qihuan},
  doi          = {10.1007/s10489-022-04177-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13130-13144},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection using class-level regularized self-representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Attentive recurrent adversarial domain adaptation with
top-k pseudo-labeling for time series classification. <em>APIN</em>,
<em>53</em>(11), 13110–13129. (<a
href="https://doi.org/10.1007/s10489-022-04176-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key challenge of Unsupervised Domain Adaptation (UDA) for analyzing time series data is to learn domain-invariant representations by capturing complex temporal dependencies. In addition, existing unsupervised domain adaptation methods for time series data are designed to align marginal distribution between source and target domains. However, existing UDA methods (e.g. R-DANN Purushotham et al. (2017), VRADA Purushotham et al. (2017), CoDATS Wilson et al. (2020)) neglect the conditional distribution discrepancy between two domains, leading to misclassification of the target domain. Therefore, to learn domain-invariant representations by capturing the temporal dependencies and to reduce the conditional distribution discrepancy between two domains, a novel Attentive Recurrent Adversarial Domain Adaptation with Top-k time series pseudo-labeling method called ARADA-TK is proposed in this paper. In the experiments, our proposed method was compared with the state-of-the-art UDA methods (R-DANN, VRADA and CoDATS). Experimental results on four benchmark datasets revealed that ARADA-TK achieves superior classification accuracy when it is compared to the competing methods.},
  archive      = {J_APIN},
  author       = {He, Qi-Qiao and Siu, Shirley Weng In and Si, Yain-Whar},
  doi          = {10.1007/s10489-022-04176-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13110-13129},
  shortjournal = {Appl. Intell.},
  title        = {Attentive recurrent adversarial domain adaptation with top-k pseudo-labeling for time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rough set approximations based on a matroidal structure over
three sets. <em>APIN</em>, <em>53</em>(11), 13082–13109. (<a
href="https://doi.org/10.1007/s10489-022-04144-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pawlak’s classical model of rough set approximations provides an efficient tool for extracting information exactly by employing available knowledge (i.e., known knowledge) in an information system, since many problems in rough set theory are NP-hard and their solution process is therefore greedy and approximate. Many extensions of Pawlak’s classical model have been proposed in recent years. Most of them are considered over one or two sets, that is, one- or two-dimensional space or one- or two-dimensional data. Aided by relation-based rough set models, a few of these extensions are considered over three sets. However, the real world is in three-dimensional space. Therefore, it is necessary to solve these problems with other models, such as covering rough set models. For this purpose, we propose the TP-matroid—a matroidal structure over three sets. Employing the family of feasible sets of a TP-matroid as the available knowledge, a pair of rough set approximations—lower and upper approximations—is provided. In addition, for an information system defined over three sets, assisted by formal concept analysis, we establish a pair of rough set approximations. Furthermore, two TP-matroids are established based on the above pair of rough set approximations. The integration between the two pairs of rough set approximations presented here is discussed. The results show that for an information system in three-dimensional space, the rough set approximations provided here can effectively explore unknown knowledge by using available knowledge based on the family of feasible sets of a TP-matroid.},
  archive      = {J_APIN},
  author       = {Wang, Gang and Mao, Hua and Liu, Chang and Zhang, Zhiming and Yang, Lanzhen},
  doi          = {10.1007/s10489-022-04144-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13082-13109},
  shortjournal = {Appl. Intell.},
  title        = {Rough set approximations based on a matroidal structure over three sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single image super-resolution via a ternary attention
network. <em>APIN</em>, <em>53</em>(11), 13067–13081. (<a
href="https://doi.org/10.1007/s10489-022-04129-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the deep convolutional neural network (CNN) has been widely explored in single image super-resolution (SISR) and achieves excellent performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, ignoring the internal dependencies between the features of different layers in the network and the intrinsic statistical properties of the feature maps. It hinders the maximization of the network representation power. We propose a ternary attention mechanism network (TAN) for effective feature extraction and feature correlation learning to address this issue. Specifically, we introduced a layer attention mechanism(LAM) to fully use the features generated by each layer of the network. Furthermore, we present a spatial attention mechanism(SAM) that uses the internal statistical characteristics of the features to enhance themself. Finally, we design a new channel attention mechanism(CAM) to ensure the feature diversity in channel dimensions. Extensive experiments show that our TAN achieves better both quantitative metrics and visual quality compared with state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yang, Lianping and Tang, Jian and Niu, Ben and Fu, Haoyue and Zhu, Hegui and Jiang, Wuming and Wang, Xin},
  doi          = {10.1007/s10489-022-04129-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13067-13081},
  shortjournal = {Appl. Intell.},
  title        = {Single image super-resolution via a ternary attention network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A combination of supervised dimensionality reduction and
learning methods to forecast solar radiation. <em>APIN</em>,
<em>53</em>(11), 13053–13066. (<a
href="https://doi.org/10.1007/s10489-022-04175-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is routinely used to forecast solar radiation from inputs, which are forecasts of meteorological variables provided by numerical weather prediction (NWP) models, on a spatially distributed grid. However, the number of features resulting from these grids is usually large, especially if several vertical levels are included. Principal Components Analysis (PCA) is one of the simplest and most widely-used methods to extract features and reduce dimensionality in renewable energy forecasting, although this method has some limitations. First, it performs a global linear analysis, and second it is an unsupervised method. Locality Preserving Projection (LPP) overcomes the locality problem, and recently the Linear Optimal Low-Rank (LOL) method has extended Linear Discriminant Analysis (LDA) to be applicable when the number of features is larger than the number of samples. Supervised Nonnegative Matrix Factorization (SNMF) also achieves this goal extending the Nonnegative Matrix Factorization (NMF) framework to integrate the logistic regression loss function. In this article we try to overcome all these issues together by proposing a Supervised Local Maximum Variance Preserving (SLMVP) method, a supervised non-linear method for feature extraction and dimensionality reduction. PCA, LPP, LOL, SNMF and SLMVP have been compared on Global Horizontal Irradiance (GHI) and Direct Normal Irradiance (DNI) radiation data at two different Iberian locations: Seville and Lisbon. Results show that for both kinds of radiation (GHI and DNI) and the two locations, SLMVP produces smaller MAE errors than PCA, LPP, LOL, and SNMF, around 4.92% better for Seville and 3.12% for Lisbon. It has also been shown that, although SLMVP, PCA, and LPP benefit from using a non-linear regression method (Gradient Boosting in this work), this benefit is larger for PCA and LPP because SMLVP is able to perform non-linear transformations of inputs.},
  archive      = {J_APIN},
  author       = {García-Cuesta, Esteban and Aler, Ricardo and Pózo-Vázquez, David del and Galván, Inés M.},
  doi          = {10.1007/s10489-022-04175-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {11},
  pages        = {13053-13066},
  shortjournal = {Appl. Intell.},
  title        = {A combination of supervised dimensionality reduction and learning methods to forecast solar radiation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RERG: Reinforced evidence reasoning with graph neural
network for table-based fact verification. <em>APIN</em>,
<em>53</em>(10), 12308–12323. (<a
href="https://doi.org/10.1007/s10489-022-04130-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table-based fact verification aims to check whether a statement is entailed by the content of relevant table. Existing works mainly either parse a statement with logical form or design a table-aware neural network to represent the statement-table pair. However, they fail to directly exploit guidance signals to capture enough evidence from the table, which lead to performance degradation. Thus, to investigate how to select potential key words from the table for fact verification, we propose a Reinforced Evidence Reasoning framework with Graph neural network (RERG), which simulates human inference process of focusing on some words at each step. Specifically, we employ a Transformer-based graph neural network to represent multi-granularity features. Then, we design a monitor node and connect it with some potential key nodes by reinforcement learning on each graph layer, according to the feedback of the reward. In this way, the monitor node can be used to predict the label, which has aggregated various key information through multiple graph layers. Besides, we add secondary updating after the attention mechanism to enhance information aggregation of each graph layer. Experimental results on two benchmark datasets TABFACT and INFOTABS show performance improvements over state-of-the-art baselines and the feasibility of selecting some meaningful evidences during graph reasoning.},
  archive      = {J_APIN},
  author       = {Zhao, Guangzhen and Yang, Peng and Yao, Yu},
  doi          = {10.1007/s10489-022-04130-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12308-12323},
  shortjournal = {Appl. Intell.},
  title        = {RERG: Reinforced evidence reasoning with graph neural network for table-based fact verification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised feature selection based on pairwise
constraint-guided dual space latent representation learning and double
sparse graphs discriminant. <em>APIN</em>, <em>53</em>(10), 12288–12307.
(<a href="https://doi.org/10.1007/s10489-022-04040-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised graph based sparse learning investigates both label related information and inherent characteristic in data when only a small number of labels are available. How to fully explore the structure information in data space and feature space and well utilize the label information in the partial labeled data is the crux to improve the performance of a learning model. In this study, the latent representation learning is introduced into data space and feature space (dual space) simultaneously in the framework of the semi-supervised graph lased sparse learning. A novel semi-supervised feature selection algorithm called semi-supervised feature selection based on pairwise constraint-guided dual space latent representation learning and double sparse graphs discriminant (PCDLRD) is proposed. Latent representation learning is firstly used in semi-supervised feature selection considering dual graph information, which aims to capture the complex internal correlation structure information in data space and feature space and well guide the process of feature selection. Furthermore, the latent representation learning with the pairwise constraints is defined, which embeds the pairwise constraints into the latent representation learning in data space so as to fully exploit the intrinsic structure of the data in the help of some supervised information. A double sparse graph discriminative model is also integrated into the framework to enhance the discriminative ability of the method, which uses both labeled and unlabeled data. In addition, a robust dual space latent representation learning model based on L2,1 − norm is established, which is more robust to outliers than the squared norm. A unified solution for the objective function is investigated and its convergence is proved theoretically and experimentally. Experimental results on twelve public datasets validate the viability of the proposed method and the effectiveness of the proposed method in classification tasks compared to the other six feature selection methods.},
  archive      = {J_APIN},
  author       = {Chen, Hao and Chen, Hongmei and Li, Weiyi and Li, Tianrui},
  doi          = {10.1007/s10489-022-04040-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12288-12307},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised feature selection based on pairwise constraint-guided dual space latent representation learning and double sparse graphs discriminant},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single document text summarization addressed with a cat
swarm optimization approach. <em>APIN</em>, <em>53</em>(10),
12268–12287. (<a
href="https://doi.org/10.1007/s10489-022-04149-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of a tremendous amount of online information bringing about a broad interest in extracting relevant information in a compact and meaningful way, prompted the need for automatic text summarization. Hence, in the proposed system, the automated text summarization has been considered as an extractive single-document summarization problem, and a Cat Swarm Optimization (CSO) algorithm-based approach is proposed to solve it, whose objective is to generate good summaries in terms of content coverage, informative, anti-redundancy, and readability. In this work, input documents are pre-processed first. Then the cat population is initialized, where each individual (cat) in a binary vector is randomly initialized in the search space, considering the constraint. The objective function is then formulated considering different sentence quality measures. The Best Cat Memory Pool (BCMP) is initialized based on the objective function score. After that, individuals are randomly distributed for position updating to perform seeking/tracing mode operations based on the mixture ratio in each iteration. BCMP is also updated accordingly. Finally, an optimal individual is chosen to generate the summary after the last iteration. DUC-2001 and DUC-2002 data sets and ROUGE measures are used for system evaluation, and the obtained results are compared with the various state-of-the-art methods. We have achieved approximately 25% and 5% improvement on ROUGE-1 and ROUGE-2 scores on the datasets over the best existing method mentioned in this paper, revealing the proposed method’s superiority. The proposed system is also evaluated considering the generational distance, CPU processing time, cohesion, and readability factor, reflecting that the system-generated summaries are readable, concise, relevant, and fast. We have also conducted a two-sample t-test, and one-way ANOVA test showing the proposed approach is statistically significant.},
  archive      = {J_APIN},
  author       = {Debnath, Dipanwita and Das, Ranjita and Pakray, Partha},
  doi          = {10.1007/s10489-022-04149-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12268-12287},
  shortjournal = {Appl. Intell.},
  title        = {Single document text summarization addressed with a cat swarm optimization approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spike-train level supervised learning algorithm based on
bidirectional modification for liquid state machines. <em>APIN</em>,
<em>53</em>(10), 12252–12267. (<a
href="https://doi.org/10.1007/s10489-022-04152-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liquid state machine (LSM) of spiking neurons is a biologically plausible computational model imitating the structure and functions of the nervous system for information processing. Current supervised learning algorithms for spiking LSMs, such as the remote supervised method (ReSuMe), generally only adjust the synaptic weights in the output layer, while the synaptic weights of input and liquid layers are no longer changed during supervised learning. In this paper, a spike-train level supervised learning algorithm for spiking LSMs based on a bidirectional modification mechanism is proposed, which is called the bidirectional modification method (BiMoMe). Unlike ReSuMe, the proposed BiMoMe algorithm can adjust all synaptic weights in LSMs, which can enhance the dynamics of the network and is a biologically plausible supervised learning algorithm. The learning performance of BiMoMe is evaluated through several spike train learning tasks and an image classification problem. Experimental results show that BiMoMe has stronger spike train learning ability and image classification performance compared with other related algorithms, indicating that BiMoMe is effective in solving spatio-temporal pattern learning problems.},
  archive      = {J_APIN},
  author       = {Lu, Han and Lin, Xianghong and Wang, Xiangwen and Du, Pangao},
  doi          = {10.1007/s10489-022-04152-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12252-12267},
  shortjournal = {Appl. Intell.},
  title        = {Spike-train level supervised learning algorithm based on bidirectional modification for liquid state machines},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diversity-induced consensus and structured graph learning
for multi-view clustering. <em>APIN</em>, <em>53</em>(10), 12237–12251.
(<a href="https://doi.org/10.1007/s10489-022-04074-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view clustering has recently attracted extensive attention due to its capacity of exploring the nonlinear structure of data points from multiple views. However, most existing methods focus on discovering the consistency of multiple views, while ignoring the inconsistencies that may be caused by noise, outliers or view-specific attributes. To this end, this paper proposes a Diversity-induced Consensus and Structured Graph Learning model for multi-view clustering (DCSGL), which simultaneously formulates the multi-view consistency and the multi-view diversity into a unified framework to guide the consensus and structured graph learning. Specifically, DCSGL decomposes each view initial graph into two latent factors, i.e., consistent factor and inconsistent factor, and then adopts the Hilbert Schmidt Independence Criterion (HSIC) as a diversity penalty term to force inconsistent factors to be sparse across views, thereby inducing consistent factors to be cleaner than initial graphs. Furthermore, the consistent factors of different views are fused into a consensus graph with an explicit connectivity constraint in a self-weighted manner, leading to the components in the consensus graph indicate clusters directly. Experimental results on seven benchmark datasets demonstrate the effectiveness of our proposed method, comparing to the state-of-the-art methods for multi-view clustering.},
  archive      = {J_APIN},
  author       = {Gu, Zhibin and Liu, Hongzhe and Feng, Songhe},
  doi          = {10.1007/s10489-022-04074-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12237-12251},
  shortjournal = {Appl. Intell.},
  title        = {Diversity-induced consensus and structured graph learning for multi-view clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying algorithm in program code based on structural
features using CNN classification model. <em>APIN</em>, <em>53</em>(10),
12210–12236. (<a
href="https://doi.org/10.1007/s10489-022-04078-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software, an algorithm is a well-organized sequence of actions that provides the optimal way to complete a task. Algorithmic thinking is also essential to break-down a problem and conceptualize solutions in some steps. The proper selection of an algorithm is pivotal to improve computational performance and software productivity as well as to programming learning. That is, determining a suitable algorithm from a given code is widely relevant in software engineering and programming education. However, both humans and machines find it difficult to identify algorithms from code without any meta-information. This study aims to propose a program code classification model that uses a convolutional neural network (CNN) to classify codes based on the algorithm. First, program codes are transformed into a sequence of structural features (SFs). Second, SFs are transformed into a one-hot binary matrix using several procedures. Third, different structures and hyperparameters of the CNN model are fine-tuned to identify the best model for the code classification task. To do so, 61,614 real-world program codes of different types of algorithms collected from an online judge system are used to train, validate, and evaluate the model. Finally, the experimental results show that the proposed model can identify algorithms and classify program codes with a high percentage of accuracy. The average precision, recall, and F-measure scores of the best CNN model are 95.65%, 95.85%, and 95.70%, respectively, indicating that it outperforms other baseline models.},
  archive      = {J_APIN},
  author       = {Watanobe, Yutaka and Rahman, Md. Mostafizer and Amin, Md. Faizul Ibne and Kabir, Raihan},
  doi          = {10.1007/s10489-022-04078-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12210-12236},
  shortjournal = {Appl. Intell.},
  title        = {Identifying algorithm in program code based on structural features using CNN classification model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEN-FCB: An unsupervised twinning neural network for image
registration. <em>APIN</em>, <em>53</em>(10), 12198–12209. (<a
href="https://doi.org/10.1007/s10489-022-04109-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image registration is a fundamental and vital task in medical image analysis. Deformable medical image registration generates a dense nonlinear transformation from the moving image to the fixed image. Current learning-based image registration methods utilize U-shaped networks, concatenate moving and fixed images as one input, and then impose a global regularization to ensure smooth deformation fields. However, existing deformable image registration approaches concatenate image pairs as one input to their model and may ignore independent anatomical relevance of the images. Moreover, the global regularization causes over/underconstraining, affecting their model registration accuracy and over/under enforcing the deformation field’s smoothness. To address these two problems, we propose a twinning network, consisting of two subnetworks. The first subnetwork is the proposed separate encoding neural network (SEN) for predicting high-accuracy deformation fields, and the second subnetwork is a folding correction block (FCB) to correct the deformation fields to achieve folding reduction. Comparing our experimental results to the state-of-the-art displacement and diffeomorphic methods, the proposed method provides superior registration accuracy and reduces the folding numbers. Moreover, we utilize the FCB to correct the baselines’ output deformation fields, proving that the FCB outperforms global regularization.},
  archive      = {J_APIN},
  author       = {Ma, Mingrui and Liu, Guixia and Song, Lei and Xu, Yuanbo},
  doi          = {10.1007/s10489-022-04109-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12198-12209},
  shortjournal = {Appl. Intell.},
  title        = {SEN-FCB: An unsupervised twinning neural network for image registration},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information block multi-head subspace based long short-term
memory networks for sentiment analysis. <em>APIN</em>, <em>53</em>(10),
12179–12197. (<a
href="https://doi.org/10.1007/s10489-022-03998-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a vital task in the domain of natural language processing for semantic handling. Numerous neural network systems are introduced into sentiment analysis by researchers, some of which deal with textual datasets. These models are frequently built using LSTM sequence models and attention mechanism. However, these models have some obvious flaws. For LSTM family networks, if the sequence is too long, the information about the long-distance sequence will be lost, and the gradient explosion will almost certainly occur. Additionally, these methods directly connect bidirectional hidden vectors, resulting in information redundancy. Multi-head attention is a variant of attention mechanism, and it is also widely used to process textual information in parallel. However, multi-head subspace information is also used in a mixed linear mapping, resulting in insufficient information usage. Further, multi-head attention calculates a long sequence of text, which has a large complexity overhead. To address these issues and boost performance, firstly, a complete sentence is divided into multiple information blocks. Then, we designed a unique model to input the information block into multi-head attention for parallel multi-space feature extraction; Furthermore, the output multi-subspace information is fed into the LSTM network that processes the subspace information to fully flow the message and to obtain the hidden states at each information block time step in each subspace. Blocking the sequence can not only reduce the number of cycles of LSTM, but also reduce the computational complexity of multi-head attention. Eventually, a dual fusion mechanism is presented that allows for the fusion and seizing of each subspace significant information. Experiments on real-world datasets demonstrate that our proposed model outperforms the majority of existing methods.},
  archive      = {J_APIN},
  author       = {Zhang, Xiao and Chen, Yumin and He, Linjie},
  doi          = {10.1007/s10489-022-03998-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12179-12197},
  shortjournal = {Appl. Intell.},
  title        = {Information block multi-head subspace based long short-term memory networks for sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep structural enhanced network for document clustering.
<em>APIN</em>, <em>53</em>(10), 12163–12178. (<a
href="https://doi.org/10.1007/s10489-022-04112-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep document clustering, which employs deep neural networks to learn semantic document representation for clustering purpose, has attracted increasing research interests. Traditional deep document clustering models rely only the document internal content features for learning the representation and suffer from the insufficient problem of representation learning. In this paper, we introduce a deep structural enhanced network for document clustering, namely DSEDC. The DSEDC model enhances the AE-based internal document representation with GCN-based external structural document semantics for achieving better clustering performance. An ensemble-reinforced enhancement strategy is designed, in which a complete document representation, captured by fusing document internal semantics and external semantics, and an enhanced document internal representation, captured with the help of complete document representation, are learned in a layer-by-layer reinforcement manner. Extensive experiments demonstrated that our proposed DSEDC model performs substantially better than state-of-the-art deep document clustering models.},
  archive      = {J_APIN},
  author       = {Ren, Lina and Qin, Yongbin and Chen, Yanping and Bai, Ruina and Xue, Jingjing and Huang, Ruizhang},
  doi          = {10.1007/s10489-022-04112-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12163-12178},
  shortjournal = {Appl. Intell.},
  title        = {Deep structural enhanced network for document clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining modified inverted generational distance indicator
with reference-vector-guided selection for many-objective optimization.
<em>APIN</em>, <em>53</em>(10), 12149–12162. (<a
href="https://doi.org/10.1007/s10489-022-04115-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modified inverted generational distance (IGD+) indicator has been widely used to handle optimization problems with two or three objectives due to its ability to obtain weak Pareto optimal solutions. However, only using the IGD+ indicator cannot effectively balance the convergence and diversity of candidate solutions in the high-dimensional objective space of many-objective optimization problems (MaOPs). To solve this issue, we propose a two-stage selection strategy based on the IGD+ indicator and the reference vector guidance method. This two-stage selection mechanism uses the IGD+ indicator and the reference vector guidance method to select two sub-populations, which form the parent population at the next generation. In this way, it can balance convergence and diversity well when solving MaOPs. Experiments were performed on 65 test problems. The proposed algorithm achieved the best HV value 39 times, showing competitive performance compared to five representative algorithms for many-objective optimization.},
  archive      = {J_APIN},
  author       = {Li, Fei and Shang, Zhengkun and Shen, Hao and Liu, Yuanqu and Huang, Pei-Qiu},
  doi          = {10.1007/s10489-022-04115-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12149-12162},
  shortjournal = {Appl. Intell.},
  title        = {Combining modified inverted generational distance indicator with reference-vector-guided selection for many-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FuCWO: A novel fuzzy-based approach of contention window
optimization for IEEE-802.15.6 WBANs. <em>APIN</em>, <em>53</em>(10),
12132–12148. (<a
href="https://doi.org/10.1007/s10489-022-04001-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its low-powered, low-cost implementation, 802.15.6 standard dedicated for Wireless Body Area Networks (WBANs) have become important, and the urge to increase their performance has become a necessity. With low / heavy traffic, the most notable persisting problems are packet delivery ratio (PDR), packet loss ratio (PLR), and end-to-end (E2D) delay; improvements in wireless technology emphasise surmounting these challenges. To accomplish the stated goals, FuCWO: an approach of contention window optimization through fuzzy logic is proposed, which makes use of FLC the Fuzzy Logic Controller. Two algorithms are proposed for this purpose: Algo-1: FuCWO Procedure for WBANs and Algo-2: Fuzzy Logic Procedure for FuCWO. The proposed FuCWO approach is evaluated for ABEB i.e. Alternate Binary-Exponential Back-off, which was chosen as a de facto standard, and an improvement of the Contention Window (Improved-CSMA/CA) algorithm, chosen as a foundation-based paper and implemented in Castalia OMNeT++. For ABEB, Improved-CSMA/CA, and FuCWO, the simulated values of four participating nodes/sensors were recorded. These values were later used to calculate the PDR, PLR, E2D delay for low and heavy traffic, and also produced a graphical illustration of the results. Furthermore, the obtained results from the proposed FuCWO approach for PDR, PLR are minimum 3% to maximum 9%, and for E2D minimum 2% to maximum 11% on average are better, which confirmed that the FuCWO significantly improved performance.},
  archive      = {J_APIN},
  author       = {Qureshi, Imran Ali and Asghar, Sohail and Noor, Muhammad Asim},
  doi          = {10.1007/s10489-022-04001-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12132-12148},
  shortjournal = {Appl. Intell.},
  title        = {FuCWO: A novel fuzzy-based approach of contention window optimization for IEEE-802.15.6 WBANs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymmetric similarity-preserving discrete hashing for image
retrieval. <em>APIN</em>, <em>53</em>(10), 12114–12131. (<a
href="https://doi.org/10.1007/s10489-022-04167-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have been widely studied in the image research community due to their low storage and fast computation. However, generating compact hash codes is still a challenging task. In this paper, we propose a novel Asymmetric Similarity-Preserving Discrete Hashing (ASPDH) method to learn compact binary codes for image retrieval. Specifically, the pairwise similarity matrix is approximated in the asymmetric learning manner with two different real-valued embeddings. In addition, ASPDH constructs two distinct hash functions from the kernel feature and label consistency embeddings. Therefore, similarity preservation and hash code learning can be simultaneously achieved and interactively optimized, which further improves the discriminative capability of the learned binary codes. Then, a well-designed iterative algorithm is developed to efficiently solve the optimization problem, resulting in high-quality binary codes with reduced quantization errors. Extensive experiments on three public datasets show the rationality and effectiveness of our proposed method.},
  archive      = {J_APIN},
  author       = {Ren, Xiuxiu and Zheng, Xiangwei and Cui, Lizhen and Wang, Gang and Zhou, Huiyu},
  doi          = {10.1007/s10489-022-04167-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12114-12131},
  shortjournal = {Appl. Intell.},
  title        = {Asymmetric similarity-preserving discrete hashing for image retrieval},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Additive consistent triangular fuzzy preference relation and
likelihood comparison algorithm based group decision making.
<em>APIN</em>, <em>53</em>(10), 12098–12113. (<a
href="https://doi.org/10.1007/s10489-022-04024-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the likelihood comparison algorithm of triangular fuzzy numbers (TFNs) and the additive consistency of triangular fuzzy preference relation (TFPR) for group decision making (GDM). According to the likelihood of intervals, the likelihood of TFNs is defined to design a likelihood comparison algorithm for ranking TFNs. The additive consistency of TFPR is defined based on the additive consistency of fuzzy preference relation. Then considering the risk attitude of the decision maker (DM), the linear programming models are constructed to derive the corresponding priority weights from the TFPR in the optimistic, pessimistic, and neutral cases, respectively. Using a linear programming model, the derived priority weights are integrated to the normalized triangular fuzzy priority weights (TFPWs). In GDM, DMs&#39; weights are determined by the deviation degrees and dispersion degrees of DMs. Aggregating all the individual TFPRs, the collective TFPR is obtained. The normalized TFPWs are determined from the collective TFPR. The ranking order of alternatives is generated by the designed likelihood comparison algorithm. Therefore, a new method is proposed to solve GDM with TFPRs. Finally, the effectiveness and advantage of the proposed method are illustrated by some examples.},
  archive      = {J_APIN},
  author       = {Wang, Feng and Wan, Shuping},
  doi          = {10.1007/s10489-022-04024-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12098-12113},
  shortjournal = {Appl. Intell.},
  title        = {Additive consistent triangular fuzzy preference relation and likelihood comparison algorithm based group decision making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge-driven monarch butterfly optimization algorithm
with self-learning mechanism. <em>APIN</em>, <em>53</em>(10),
12077–12097. (<a
href="https://doi.org/10.1007/s10489-022-03999-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Monarch Butterfly Optimization (MBO) algorithm has been proved to be an efficient meta-heuristic to directly address continuous optimization problems. In the MBO algorithm, the migration operator cooperates with the butterfly adjusting operator to generate the entire offspring population. Since the individual iterations of the MBO algorithm are not self-learning, the cooperative intelligence mechanism is a random process. In this study, an improved MBO algorithm with a knowledge-driven learning mechanism (KDLMBO) is presented to enable the algorithm to evolve effectively with a self-learning capacity. The neighborhood information extracted from the candidate solutions is treated as the prior knowledge of the KDLMBO algorithm. The learning mechanism consists of the learning migration operator and the learning butterfly adjusting operator. Then, the self-learning collective intelligence is realized by the two cooperative operators in the iterative process of the algorithm. The experimental results demonstrate and validate the efficiency and significance of the proposed KDLMBO algorithm.},
  archive      = {J_APIN},
  author       = {Xu, Tianpeng and Zhao, Fuqing and Tang, Jianxin and Du, Songlin and Jonrinaldi},
  doi          = {10.1007/s10489-022-03999-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12077-12097},
  shortjournal = {Appl. Intell.},
  title        = {A knowledge-driven monarch butterfly optimization algorithm with self-learning mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic segmentation model for lumbar MRI images using
divergence loss. <em>APIN</em>, <em>53</em>(10), 12063–12076. (<a
href="https://doi.org/10.1007/s10489-022-04118-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic diagnosis of lumbar diseases is very important for improving diagnostic efficiency and optimizing the allocation of medical resources. Lumbar spinal stenosis (LSS) is a common disease of the lumbar spine that causes lower back pain and leg pain. Some geometric indicators of axial lumbar magnetic resonance imaging (MRI), such as the intervertebral disc area, the dural sac area, and the anteroposterior diameter, are important for diagnosis and treatment. An important prestep for the automatic measurement of these geometric indicators is semantic image segmentation. Traditional medical image semantic segmentation models generally use the pixel-wise loss function as the optimization objective, which produces illogical segmentation results during inferring and interferes with subsequent measurements of geometric indicators. We introduce Gaussian divergence loss (DVG-loss) and, combined with contour loss, propose a new loss function to optimize the segmentation model and achieve better results in the lumbar MRI image segmentation task. The improvement brought by the proposed loss function is mainly reflected in the geometrical appearance of segmentation results instead of the pixel-wise quantitative metrics. But we must make sure that the quantitative metrics won’t deteriorate. So first, we compare the proposed method with previous models quantitatively. And then, ablation studies are conducted on different loss functions and it is shown that our proposed loss function considerably reduces the irregular edges and isolated island regions caused by misclassification.},
  archive      = {J_APIN},
  author       = {Hou, Chao and Zhang, Weiqi and Wang, Hongbo and Liu, Fei and Liu, Defeng and Chang, Jingyuan},
  doi          = {10.1007/s10489-022-04118-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12063-12076},
  shortjournal = {Appl. Intell.},
  title        = {A semantic segmentation model for lumbar MRI images using divergence loss},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective variational autoencoder: An application for
smart infrastructure maintenance. <em>APIN</em>, <em>53</em>(10),
12047–12062. (<a
href="https://doi.org/10.1007/s10489-022-04163-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-way data analysis has become an essential tool for capturing underlying structures in higher-order data sets where standard two-way analysis techniques often fail to discover the hidden correlations between variables in multi-way data. We propose a multi-objective variational autoencoder (MO-VAE) method for smart infrastructure damage detection and diagnosis in multi-way sensing data based on the reconstruction probability of autoencoder deep neural network (ADNN). Our method fuses data from multiple sensors in one ADNN at which informative features are being extracted and utilized for damage identification. It generates probabilistic anomaly scores to detect damage, asses its severity and further localize it via a new localization layer introduced in the ADNN. We evaluated our method on multi-way laboratory-based and real-life structural datasets in the area of structural health monitoring for damage diagnosis purposes. The data was collected from our deployed data acquisition system on a cable-stayed bridge in Western Sydney, a reinforced concrete cantilever beam which replicates one of the major structural components on the Sydney Harbour Bridge and a laboratory based building structure obtained from Los Alamos National Laboratory (LANL). Experimental results show that the proposed method can accurately detect structural damage. It was also able to estimate the different levels of damage severity, and capture damage locations in an unsupervised aspect. Compared to the state-of-the-art approaches, our proposed method shows better performance in terms of damage detection and localization.},
  archive      = {J_APIN},
  author       = {Anaissi, Ali and Zandavi, Seid Miad and Suleiman, Basem and Naji, Mohamad and Braytee, Ali},
  doi          = {10.1007/s10489-022-04163-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12047-12062},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective variational autoencoder: An application for smart infrastructure maintenance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReLMKG: Reasoning with pre-trained language models and
knowledge graphs for complex question answering. <em>APIN</em>,
<em>53</em>(10), 12032–12046. (<a
href="https://doi.org/10.1007/s10489-022-04123-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of complex question answering over knowledge bases (KBQA) is to find an answer entity in a knowledge graph. Recent information retrieval-based methods have focused on the topology of the knowledge graph, ignoring inconsistencies between knowledge graph embeddings and natural language embeddings, and cannot effectively utilize both implicit and explicit knowledge for reasoning. In this paper, we propose a novel model, ReLMKG, to address this challenge. This approach performs joint reasoning on a pre-trained language model and the associated knowledge graph. The complex question and textual paths are encoded by the language model, bridging the gap between the question and the knowledge graph and exploiting implicit knowledge without introducing additional unstructured text. The outputs of different layers in the language model are used as instructions to guide a graph neural network to perform message propagation and aggregation in a step-by-step manner, which utilizes the explicit knowledge contained in the structured knowledge graph. We analyse the reasoning ability of the ReLMKG model for knowledge graphs with different degrees of sparseness and evaluate the generalizability of the model. Experiments conducted on the Complex WebQuestions and WebQuestionsSP datasets demonstrate the effectiveness of our approach on KBQA tasks.},
  archive      = {J_APIN},
  author       = {Cao, Xing and Liu, Yun},
  doi          = {10.1007/s10489-022-04123-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12032-12046},
  shortjournal = {Appl. Intell.},
  title        = {ReLMKG: Reasoning with pre-trained language models and knowledge graphs for complex question answering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A training sample selection method for predicting software
defects. <em>APIN</em>, <em>53</em>(10), 12015–12031. (<a
href="https://doi.org/10.1007/s10489-022-04044-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defect Prediction (SDP) is an important method to analyze software quality and reduce development cost. Data from software life cycle has been widely used to predict the defect prone of software modules, and although many machine learning-based SDP models have been proposed, their predictive performance is not always satisfactory. Traditional machine learning-based classifiers usually assume that all samples have the same contribution to the training of SDP, which is not true. In fact, different training samples have different effects on the performance of the SDP model, the performance of machine learning-based SDP models is heavily dependent on the quality of training samples. For the above shortcoming of traditional machine learning-based classifiers, the contributions of this paper are as follows: (1) Inspired by the clustering algorithm, a method to calculate the contribution of each training sample to the SDP model is proposed, which not only considers the relationship between the contributions of the training samples to the SDP model, and also analyzes the influence of the distance between the sample and the category boundary on the performance of the SDP model, so it is different from the existing calculation method of sample contribution. (2) A Sample Selection (SS) method is proposed to improve the performance of the SDP model. It first calculates the contribution of each training sample based on several nearest neighbors of the sample and the label information of these neighbors, and then implements SS according to Hoeffding probability inequality and the contribution of each sample. To confirm the validity of the proposed SDP model, some experimental results are given. Both direct observations and statistical tests of the experimental results show that the SS method is very effective for improving the predictive performance of the SDP model.},
  archive      = {J_APIN},
  author       = {Jin, Cong},
  doi          = {10.1007/s10489-022-04044-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {12015-12031},
  shortjournal = {Appl. Intell.},
  title        = {A training sample selection method for predicting software defects},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved bald eagle search algorithm with dimension
learning-based hunting for autonomous vehicle including vision dynamics.
<em>APIN</em>, <em>53</em>(10), 11997–12014. (<a
href="https://doi.org/10.1007/s10489-022-04059-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lateral deviations on account of the penetrations of road curvature and the parameters uncertainties are the main issues against the autonomous vehicles (AVs) controller to provide effective performance including less error, fast response, and small overshoot. In this regard, this paper suggests a new improvement for the bald eagle search (BES) algorithm by dimension learning-based hunting (DLH). The proposed DLH strategy enhances the exploration behavior of the original BES to tackle different issues such as slow convergence, trapping in local optima, and the loss of the diversity in early stage. The proposed strategy improves the learning of each eagle from its neighbors rather than the knowledge of all individuals in the population. The improved BES (I-BES) is dedicated to overcome the tuning issue of the model predictive control (MPC) for AVs including vision dynamics. Besides, frequency domain bounds are formulated based on Hermite-Biehler theorem to handle the parameters uncertainty issue due to the variation of the AV speed and the road curvature during the tuning of the MPC gains. The optimization process is performed to ameliorate the damping performance of the AV response particularly by decreasing the system steady-state error, settling time, and maximum overshoot. A newly developed multi-objective formula is utilized to achieve the diminishing of the performance criteria simultaneously. The proposed I-BES and BES are tested against various standard benchmark optimization test functions and different statistical tests to confirm that the I-BES can perform better than BES. Moreover, the proposed I-BES is confirmed with the default BES, neural network algorithm (NNA), and genetic algorithm (GA) in cases of AV controllers. Furthermore, the response of the inspired robust MPC based on the I-BES algorithm can tackle the lateral deviation due to the variations of the road curvature with small overshoot and short settling time less than 0.1745% and 0.05 s respectively better than the fuzzy logic controller. Various scenarios are carried out to confirm the effectiveness of the suggested technique against the road curvature variation and the AV parameters uncertainty. The results emphasize the superiority of the inspired robust MPC based on the I-BES algorithm to provide the best-damped response and stabilize the AV system against the parameters uncertainty and the road curvature variation compared with the other techniques.},
  archive      = {J_APIN},
  author       = {Elsisi, M. and Essa, Mohamed El-Sayed M.},
  doi          = {10.1007/s10489-022-04059-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11997-12014},
  shortjournal = {Appl. Intell.},
  title        = {Improved bald eagle search algorithm with dimension learning-based hunting for autonomous vehicle including vision dynamics},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed noise face hallucination via adaptive weighted
residual and nuclear-norm regularization. <em>APIN</em>,
<em>53</em>(10), 11979–11996. (<a
href="https://doi.org/10.1007/s10489-022-04018-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination has been extensively studied in recent years. The majority of existing approaches operate admirably in noise-free or single camera/atmospheric noise (i.e., Gaussian or impulse) situations. However, when both Gaussian and impulse noises (mixed Gaussian-impulse noise (MIXGIN)) jointly contaminate an image, face hallucination becomes a challenging task. To deal with mixture image corruption, we offer an Adaptive Weighted Residual and Nuclear-Norm Regularization approach in this paper. The method reveals some of the most crucial components of the face hallucination problem. By assigning the weights adaptively to the pixels according to their coding residuals, the proposed method could self-identify the mixed noise and alleviate its effects on the coding process. Moreover, considering the similarity of the training samples and the sparsity of representation coefficients, a nuclear-norm regularization is employed to represent and hallucinate a high resolution face image using correlated and discriminative samples. The method is adaptive to produce suitable coefficients in the presence of mixed noise. To optimize the model, the alternating direction method of multipliers (ADMM) is introduced. The experimental results show that the proposed algorithm gives better performance than the existing face hallucination methods.},
  archive      = {J_APIN},
  author       = {Tang, Songze and Shu, Zhenqiu},
  doi          = {10.1007/s10489-022-04018-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11979-11996},
  shortjournal = {Appl. Intell.},
  title        = {Mixed noise face hallucination via adaptive weighted residual and nuclear-norm regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unveil the unseen: Exploit information hidden in noise.
<em>APIN</em>, <em>53</em>(10), 11966–11978. (<a
href="https://doi.org/10.1007/s10489-022-04102-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise and uncertainty are usually the enemy of machine learning, noise in training data leads to uncertainty and inaccuracy in the predictions. However, we develop a machine learning architecture that extracts crucial information out of the noise itself to improve the predictions. The phenomenology computes and then utilizes uncertainty in one target variable to predict a second target variable. We apply this formalism to PbZr0.7Sn0.3O3 crystal, using the uncertainty in dielectric constant to extrapolate heat capacity, correctly predicting a phase transition that otherwise cannot be extrapolated. For the second example – single-particle diffraction of droplets – we utilize the particle count together with its uncertainty to extrapolate the ground truth diffraction amplitude, delivering better predictions than when we utilize only the particle count. Our generic formalism enables the exploitation of uncertainty in machine learning, which has a broad range of applications in the physical sciences and beyond.},
  archive      = {J_APIN},
  author       = {Zviazhynski, Bahdan and Conduit, Gareth},
  doi          = {10.1007/s10489-022-04102-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11966-11978},
  shortjournal = {Appl. Intell.},
  title        = {Unveil the unseen: Exploit information hidden in noise},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-aspect heterogeneous information network for MOOC
knowledge concept recommendation. <em>APIN</em>, <em>53</em>(10),
11951–11965. (<a
href="https://doi.org/10.1007/s10489-022-04025-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the Massive Open Online Course (MOOC) learning system has become a popular education platform, and the information overload issue within it is becoming much more serious, due to the increasing of online education needs. Consequently, the studies that focus on recommending learning resources (e.g., courses and knowledge concepts) have become a hot research topic. Moreover, as courses are composed of sets of knowledge concepts, directly recommending courses will ignore the students’ fine grained learning state. In this work, we focus on the Knowledge Concept Recommendation (KCR) task that aims at exploring the concepts that a student needs to master. Existing methods on KCR are mainly well designed graph convolutional models that pay more attention to exploring students’ preference similarity. However, most of these methods only have limited representation ability, as they mainly rely on a single type of nodes, and the diversified relationships among students and concepts are not fully explored. In light of this, we propose a Multi-aspect Heterogeneous Information Network (Multi-HIN) for KCR. Specifically, to consider the impact of multi-types of entities on preference learning, we first construct a Heterogeneous Information Network (HIN) to link concepts and multiple entities in a network. Then, to learn a more accurate node representation, we dynamically assign an aspect context to each node by viewing aspects as students’ interest dimensions. To learn items’ representations based on the current aspect, we further conduct the aspect selection process via the Gumbel–Softmax method. Finally, we use an extended Matrix Factorization (MF) method to make concept recommendations. We conduct extensive experiments on a real-world dataset to demonstrate the superiority of our proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Xinhua and Jia, Linzhao and Guo, Lei and Liu, Fangai},
  doi          = {10.1007/s10489-022-04025-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11951-11965},
  shortjournal = {Appl. Intell.},
  title        = {Multi-aspect heterogeneous information network for MOOC knowledge concept recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alzheimer’s disease classification using distilled
multi-residual network. <em>APIN</em>, <em>53</em>(10), 11934–11950. (<a
href="https://doi.org/10.1007/s10489-022-04084-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early human intervention is crucial for diagnosing Alzheimer’s Disease (AD), since AD is irreversible and leads to progressive impairment of memory. In recent years, Convolutional Neural Networks (CNNs) have achieved dramatic breakthroughs in AD diagnosis. However, existing CNNs have difficulties in extracting subtle contextual information because of their structural limitations, i.e., it is difficult to extract discriminative features of several regions, such as hippocampus, parietal, temporal lobe tissues, and so on. In addition, current networks have difficulty in classifying imaging features with imbalanced data categories. Some loss functions can alleviate the above problems to some extent, but they are affected by outliers and trapped in local optimum easily. To address this issue, a Distilled Multi-Residual Network (DMRNet) is proposed for the early diagnosis of AD. The DMRNet consists of three main components: 1) Dense Connection Block, 2) Focus Attention Block, and 3) Multi-scale Fusion Module. The dense features are extracted by the Dense Connection Block while the local abnormal regions are refined by the Focus Attention Block and Multi-scale Fusion Module. Besides, to explore the hidden knowledge between each feature, a dilated classifier with self-distillation is proposed to ensemble several items pertaining to feature knowledge from feature space. Finally, the Remix Balance Sampler (RBS) is proposed to alleviate the influences of outliers. The proposed DMRNet is evaluated on baseline sMRI scans of the ADNI dataset. The result of experiments demonstrated that the proposed DMRNet not only achieves 7.15% greater accuracy than state-of-the-art methods but also successfully identifies some AD-related regions.},
  archive      = {J_APIN},
  author       = {Liang, Xuehu and Wang, Zhuowei and Chen, Ziyang and Song, Xiaoyu},
  doi          = {10.1007/s10489-022-04084-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11934-11950},
  shortjournal = {Appl. Intell.},
  title        = {Alzheimer’s disease classification using distilled multi-residual network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-supervision-based equilibrated fusion mechanism of
local and global attention for semantic segmentation. <em>APIN</em>,
<em>53</em>(10), 11918–11933. (<a
href="https://doi.org/10.1007/s10489-022-04085-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, weakly supervised semantic segmentation has become one of the hot research directions, but the problems of object location accuracy and small activation areas are still a challenge. In this paper, we propose a network with an equilibrated fusion mechanism of local and global attention based on cross-supervision(CSEFM). To accomplish multitask learning, we design the class activation branch and decoding prediction branch, which share the same backbone, to generate high-quality pseudo labels and obtain semantic segmentation results. Specifically, the network is first trained to update the backbone weights using images with strong labels. Images with weak labels are then used to help obtain a reliable class activation mapping (CAM) at the class activation branch, and the dense conditional random fields (DenseCRFs) are used to generate high-quality pseudo labels. Finally, the strong label images and the weak label images that have obtained pseudo labels are fed to the network for retraining, and the result of segmentation is predicted by the decoding prediction branch. Given the training method, the dataset is divided into several groups as a few images with strong labels and several images with weak labels and fed to the network successively for cross-supervised learning. Our proposed network is trained and validated on the PASCAL VOC 2012 dataset, and the results show that the mean Intersection over Union (mIoU) on the validation set is 65.6%. Compared with other mainstream methods, better segmentation is achieved and the performance gap between image-level and pixel-level semantic segmentation is reduced when using our approach.},
  archive      = {J_APIN},
  author       = {Yuan, Wenhao and Lu, Xiaoyan and Zhang, Rongfen and Liu, Yuhong},
  doi          = {10.1007/s10489-022-04085-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11918-11933},
  shortjournal = {Appl. Intell.},
  title        = {Cross-supervision-based equilibrated fusion mechanism of local and global attention for semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying suicidal emotions on social media through
transformer-based deep learning. <em>APIN</em>, <em>53</em>(10),
11885–11917. (<a
href="https://doi.org/10.1007/s10489-022-04060-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suicide-related cases throughout the world are increasing on a day-to-day basis. This is owing to lack of control on negative human emotions which lead one to take one’s life. Due to rapid development of social media, users are posting their views and daily activities including texts related to suicide. This research identifies negative emotions in suicidal postings which describe an individual’s mental health. We proposed two models for detecting negative emotions like anger, anxiety, depression, guilt, fear, sadness, and stress on social media. Our first model is a context-based bidirectional gated recurrent unit with multi-head attention and a convolutional neural network (C-BiGRU-MHA-CNN) that is meant for preserving contextual data and for maintaining long-term dependencies. This paper proposes the masked language modeling (MLM) and self-attention (SA) mechanism procedures for model training and to detect contextual features over context-free models. We also suggested the lexicon-based bidirectional long short-term memory with multi-head attention and convolutional neural network (L-BiLSTM-MHA-CNN) model. It is a single channel-based model that performs better when it comes to dealing with the lexicon-based approach against erstwhile methods. It combines input features with parts-of-speech (POS) tagging can train on embedding representations for identifying emotions, while recognizing those which are the most dominant in suicide-related texts. We compared the performance of our models with various word embeddings. We also conducted an ablation study in order to highlight the contribution of the most essential components in our models for achieving better performance. Our proposed models with the bidirectional encoder representations from transformers (BERT) mechanism have resulted in an outstanding performance against the state-of-the-art methods meant to identify emotions on text sequence data.},
  archive      = {J_APIN},
  author       = {Kodati, Dheeraj and Tene, Ramakrishnudu},
  doi          = {10.1007/s10489-022-04060-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11885-11917},
  shortjournal = {Appl. Intell.},
  title        = {Identifying suicidal emotions on social media through transformer-based deep learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for the fusion of non-exclusive and incomplete
information on the basis of d number theory. <em>APIN</em>,
<em>53</em>(10), 11861–11884. (<a
href="https://doi.org/10.1007/s10489-022-03960-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty is of great concern in information fusion and artificial intelligence. Dempster-Shafer theory is a popular tool to deal with uncertainty, but it cannot effectively represent and fuse uncertain information involving non-exclusiveness and incompleteness. In order to solve that problem, an idea of D number theory (DNT) has been proposed. In this paper, the basic theory of DNT for the fusion of non-exclusive and incomplete information is studied to strengthen its theoretical foundation, including concept formalization, uncertainty representation, information modelling and fusion. At first, the non-exclusiveness in DNT is defined formally and its basic properties are discussed. Secondly, new measures of belief and plausibility for D numbers are developed. Thirdly, the combination rule for D numbers is studied by extending the exclusive conflict redistribution rule. Fourthly, a method to combine information-incomplete D numbers is proposed. The proposed concepts, definitions, and methods are analyzed mathematically and exemplified through illustrative examples.},
  archive      = {J_APIN},
  author       = {Deng, Xinyang and Jiang, Wen},
  doi          = {10.1007/s10489-022-03960-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11861-11884},
  shortjournal = {Appl. Intell.},
  title        = {A framework for the fusion of non-exclusive and incomplete information on the basis of d number theory},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Sea-horse optimizer: A novel nature-inspired meta-heuristic
for global optimization problems. <em>APIN</em>, <em>53</em>(10),
11833–11860. (<a
href="https://doi.org/10.1007/s10489-022-03994-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel swarm intelligence-based metaheuristic called as sea-horse optimizer (SHO), which is inspired by the movement, predation and breeding behaviors of sea horses in nature. In the first two stages, SHO mimics different movements patterns and the probabilistic predation mechanism of sea horses, respectively. In detail, the movement modes of a sea horse are divided into floating spirally affected by the action of marine vortices or drifting along the current waves. For the predation strategy, it simulates the success or failure of the sea horse for capturing preys with a certain probability. Furthermore, due to the unique characteristic of the male pregnancy, in the third stage, the proposed algorithm is designed to breed offspring while maintaining the positive information of the male parent, which is conducive to increase the population diversity. These three intelligent behaviors are mathematically expressed and constructed to balance the local exploitation and global exploration of SHO. The performance of SHO is evaluated on 23 well-known functions and CEC2014 benchmark functions compared with six state-of-the-art metaheuristic algorithms. Finally, five real-world engineering problems are utilized to test the effectiveness of SHO. The experimental results demonstrate that SHO is a high-performance optimizer and positive adaptability to deal with constraint problems. SHO source code is available from: https://www.mathworks.com/matlabcentral/fileexchange/115945-sea-horse-optimizer},
  archive      = {J_APIN},
  author       = {Zhao, Shijie and Zhang, Tianran and Ma, Shilin and Wang, Mengchen},
  doi          = {10.1007/s10489-022-03994-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11833-11860},
  shortjournal = {Appl. Intell.},
  title        = {Sea-horse optimizer: A novel nature-inspired meta-heuristic for global optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel adaptive optimization method for deep learning with
application to froth floatation monitoring. <em>APIN</em>,
<em>53</em>(10), 11820–11832. (<a
href="https://doi.org/10.1007/s10489-022-04083-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Adam and its variants are widely used optimization methods, they have issues such as non-convergence and slow optimization speed. Researches have shown that weighting more of the past information in the the second moment estimation could be beneficial to the optimization process. But the design of update formulas and the standard of ideal switching time need to be considered. In this paper, a novel optimization method called Adaptive learning Rate switCH (ARCH) is proposed. According to a well-designed update formula, ARCH can increase the weight of historical information in the second moment estimation continuously. Besides, the switching time can be selected adaptively and automatically through experimental performance. A theoretical proof of the convergence of the proposed algorithm is also presented. To verify the performance of ARCH, a series of comparative experiments, which compare ARCH with other optimization methods in several classical convolution neural networks, are carried out. Experimental results have shown that ARCH has fast convergence speed as well as good generalization performance. Moreover, the algorithm proposed in this paper is also applied in practical froth flotation monitoring and results show that ARCH can perform excellently in practical application as well.},
  archive      = {J_APIN},
  author       = {Ma, Boyan and Du, Yangyi and Zhou, Xiaojun and Yang, Chunhua},
  doi          = {10.1007/s10489-022-04083-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11820-11832},
  shortjournal = {Appl. Intell.},
  title        = {A novel adaptive optimization method for deep learning with application to froth floatation monitoring},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified lévy flight distribution algorithm for global
optimization and parameters estimation of modified three-diode
photovoltaic model. <em>APIN</em>, <em>53</em>(10), 11799–11819. (<a
href="https://doi.org/10.1007/s10489-022-03977-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems demand optimization, minimization of costs and maximization of profits, and meta-heuristic algorithms have proficiently proved their ability to achieve optimum results. This study proposes an alternative algorithm of Lévy Flight Distribution (LFD) by integrating Opposition-based learning (OBL) operator, termed LFD-OBL, for resolving intrinsic drawbacks of the canonical LFD. The proposed approach adopts OBL operator for catering search stagnancy to ensure faster convergence rate. We validate the usefulness of our approach through IEEE CEC’20 test suite, and compare results with original LFD and several other counterparts such as Moth-flame optimization, whale optimization algorithm, grasshopper optimisation algorithm, thermal exchange optimization, sine-cosine algorithm, artificial ecosystem-based optimization, Henry gas solubility optimization, and Harris’ hawks optimization. To further validate the efficiency of LFD-OBL, we apply it on parameters optimization of Solar Cell based on the Three-Diode Photovoltaic model. The qualitative and quantitative results of all the experiments performed in this study suggest superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Houssein, Essam H. and Hassan, Mohamed H. and Kamel, Salah and Hussain, Kashif and Hashim, Fatma A.},
  doi          = {10.1007/s10489-022-03977-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11799-11819},
  shortjournal = {Appl. Intell.},
  title        = {Modified lévy flight distribution algorithm for global optimization and parameters estimation of modified three-diode photovoltaic model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved multi-directional local search algorithm for
vehicle routing problem with time windows and route balance.
<em>APIN</em>, <em>53</em>(10), 11786–11798. (<a
href="https://doi.org/10.1007/s10489-022-04061-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicle Routing Problem with Route Balance (VRPRB) aims to balance distribution costs and workloads and achieve important nonmonetary benefits with a more equitable distribution scheme. Considering time window constraints for the VRPRB will have an impact on the workload balance, which has rarely been studied before. The existence of a time window constraint can significantly affect the allocation of duration, and the analysis method under the traditional model is no longer applicable. This paper combined the time window constraint, established the Vehicle Routing Problem with Time Windows and Route Balance (VRPTWRB) model, and conducted a numerical study on the reasonable selection of workload resources and equity functions. An improved multi-directional local search (IMDLS) algorithm was proposed to solve the model and approximate the Pareto frontier. The IMDLS algorithm limits the archive size and adaptively determines the number of current solutions and the search direction. A large neighbourhood search (LNS) framework was employed as an local search to find effective solutions and update the approximate Pareto frontier in each iteration. The performance of the IMDLS was compared to the MDLS, and the effect of the choice of workload resource and the equity function on fairness were further studied. The computational results showed that the duration was more suitable for evaluating workload resources than distance when considering the time window constraints; and more complex equity functions could effectively find high-quality nondominated solutions with good equity.},
  archive      = {J_APIN},
  author       = {Feng, Bin and Wei, Lixin},
  doi          = {10.1007/s10489-022-04061-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11786-11798},
  shortjournal = {Appl. Intell.},
  title        = {An improved multi-directional local search algorithm for vehicle routing problem with time windows and route balance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-attribute group decision-making model for selecting
the most suitable construction company using the linguistic
interval-valued t-spherical fuzzy TOPSIS method. <em>APIN</em>,
<em>53</em>(10), 11768–11785. (<a
href="https://doi.org/10.1007/s10489-022-04103-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation and selection problems are becoming increasingly difficult due to human judgment and unknown evaluation risks. These circumstances mean that contractor selection is impacted by a number of factors, many of which have a hybrid uncertainty of fuzziness and probability. The objective of this paper is to propose a multi-attribute group decision-making (MAGDM) model for the selection of the most suitable construction company. A decision problem based on the decisions of many experts, with multiple conflicting criterion, should be taken into account to evaluate multiple companies. For this purpose, we propose the notion of a linguistic interval-valued T-spherical fuzzy set (LIVt-SFS) to allow decision-makers to provide their evaluations in a wider space and to better deal with vague information. In this study, we developed some basic operations and score and accuracy functions to compare LIVt-SF numbers (LIVt-SFNs). Based on these operations, two aggregation operators: LIVt-SF weighted averaging and LIVt-SF weighted geometric operators, are established. Moreover, the technique for order of preference by similarity to ideal solution (TOPSIS) method is extended to solve the MAGDM problem under LIVt-SFS information. Furthermore, an example of the selection of the best construction company is given to demonstrate the effectiveness and feasibility of the proposed model. The computational results demonstrate that the suggested MAGDM model is capable of dealing with imprecision and subjectivity in complex decision-making situations. In addition, a sensitivity analysis is conducted to observe the influence of the parameter ‘q’ on the decision results. Finally, a comparison with existing decision-making methods is presented for the authentication of our proposed approach.},
  archive      = {J_APIN},
  author       = {Gurmani, Shahid Hussain and Chen, Huayou and Bai, Yuhang},
  doi          = {10.1007/s10489-022-04103-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11768-11785},
  shortjournal = {Appl. Intell.},
  title        = {Multi-attribute group decision-making model for selecting the most suitable construction company using the linguistic interval-valued T-spherical fuzzy TOPSIS method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining closed high utility patterns with negative utility in
dynamic databases. <em>APIN</em>, <em>53</em>(10), 11750–11767. (<a
href="https://doi.org/10.1007/s10489-022-03876-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility itemsets mining algorithms with negative utility have more practical applications because they can handle datasets containing negative items. Existing algorithms that consider negative items assume that the database is static and contain a lot of redundant itemsets information in the result set. To solve these problems, a first algorithm for mining closed high utility itemsets containing negative items in dynamic databases, called CHUInd, is proposed. Dynamic list index structure designed in the algorithm to quickly access and update the information stored in the list based on the index values. Memory reuse strategy is applied to reduce memory usage and quickly update item information during batch insertion. Extensive experimental evaluations on real datasets show the efficiency as well as the feasibility of the algorithm, which exhibits excellent performance in terms of both runtime memory usage.},
  archive      = {J_APIN},
  author       = {Han, Meng and Zhang, Ni and Wang, Le and Li, Xiaojuan and Cheng, Haodong},
  doi          = {10.1007/s10489-022-03876-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11750-11767},
  shortjournal = {Appl. Intell.},
  title        = {Mining closed high utility patterns with negative utility in dynamic databases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global and session item graph neural network for
session-based recommendation. <em>APIN</em>, <em>53</em>(10),
11737–11749. (<a
href="https://doi.org/10.1007/s10489-022-04034-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation algorithm is a research hotspot with economic significance and research value. Most of the algorithms are based on how to represent users and items better. Deep learning has made many achievements in the recommendation area due to its strong representation ability. Nevertheless, in recent years, the excellent performance of the graph neural network in network representation provides many inspirations. Numerous recommendation algorithms based on graph neural networks only consider building a graph for each session to handle recommendation tasks. The relationship between items in the entire data set is ignored. Therefore, we propose a recommendation algorithm called GS-GNN for integrating items’ global features and local features by graph neural network. By modeling the entire data as a global graph, we use the graph attention network to learn global representations of the items. We model each session as a session graph and use a gated graph neural network to learn local representations of the items. Sessions’ representations are obtained through the fusion of items. The task is to recommend top-k items for each session by items’ and sessions’ representations. We did a comparative experiment and performance analysis experiment. The comparative experiments prove the effectiveness of GS-GNN, and we also conduct a detailed analysis of the model through experiments.},
  archive      = {J_APIN},
  author       = {Sheng, Jinfang and Zhu, Jiafu and Wang, Bin and Long, Zhendan},
  doi          = {10.1007/s10489-022-04034-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11737-11749},
  shortjournal = {Appl. Intell.},
  title        = {Global and session item graph neural network for session-based recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Which courses to choose? Recommending courses to groups of
students in online tutoring platforms. <em>APIN</em>, <em>53</em>(10),
11727–11736. (<a
href="https://doi.org/10.1007/s10489-022-03993-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the scope of this paper, we solve the course mismatch problem of recommending courses to a group of students on online tutoring platforms. Traditional recommendation methods are designed to simulate individual activities, but, they cannot meet all recommendation requirements for tasks with multiple participants. Thus, it is important to explore how to improve student participation by surveying background information. For this purpose, we develop a group recommendation model, -C2C(Course to Choose), considering the preferences of each user in the group. Specifically, we first obtain students’ free choice of classes formed by free enrolment based on the community detection algorithm. Then, we propose a single user rating method that takes into account the degree of student participation based on the amount of time spent watching each video. Finally, the service object of the group recommender system is further extended from a single user to a group member. The experimental results show that our method is more effective than all the baseline methods.},
  archive      = {J_APIN},
  author       = {Jiang, Lu and Wang, Yuqi and Xie, Shasha and Wu, Jun and Yin, Minghao and Wang, Jianan},
  doi          = {10.1007/s10489-022-03993-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11727-11736},
  shortjournal = {Appl. Intell.},
  title        = {Which courses to choose? recommending courses to groups of students in online tutoring platforms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel hierarchical attention-based triplet network with
unsupervised domain adaptation for network intrusion detection.
<em>APIN</em>, <em>53</em>(10), 11705–11726. (<a
href="https://doi.org/10.1007/s10489-022-04076-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Intrusion Detection Systems(NIDSs) are crucial for resisting cyber threats. However, NIDSs equipped with supervised learning models do not generalize well to unknown attacks because the training samples for previously unseen new intrusions are usually not available in advance. Thus, a new framework based on a Hierarchical Attention-based Triplet network with Unsupervised Domain Adaptation(HAT-UDA) is proposed for this purpose. Concretely, a joint loss is introduced to force HAT-UDA to learn compact and discriminative embeddings for benign network traffic while being far from the representations of known attacks. Then, a One-class Support Vector Machine(OCSVM) model is trained on top of the benign embeddings for the unknown attack detection task. Furthermore, we propose an unsupervised domain adaptation module in an adversarial manner to reduce the false positives of HAT-UDA when applied to new network scenarios. HAT-UDA provides a novel approach for building a robust NIDS from benign traffic and available (known) attacks. This is particularly meaningful since collecting samples for benign traffic and known attacks is much easier than obtaining instances for unseen new attacks. Extensive experiments show that HAT-UDA outperforms other state-of-the-art methods and significantly improves the detection rate of unknown attacks.},
  archive      = {J_APIN},
  author       = {Lan, Jinghong and Liu, Xudong and Li, Bo and Zhao, Jun},
  doi          = {10.1007/s10489-022-04076-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11705-11726},
  shortjournal = {Appl. Intell.},
  title        = {A novel hierarchical attention-based triplet network with unsupervised domain adaptation for network intrusion detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel thresholding image segmentation using
meta-heuristic optimization algorithms: Comparative analysis, open
challenges and new trends. <em>APIN</em>, <em>53</em>(10), 11654–11704.
(<a href="https://doi.org/10.1007/s10489-022-04064-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studied the multilevel threshold image segmentation-based metaheuristics optimization methods and their applications. Image segmentation is a common problem in the image processing domain, and it is an essential process in image analysis, directly impacting image analysis results. Thresholding is one of the most manageable and extensively utilized methods for handling image segmentation problems. In this paper, four main parts are given; (1) We present the main procedures and definitions of the multilevel threshold image segmentation problem. The standard fitness function and the evaluation criteria are also given to facilitate the problem representation for the new researchers in this domain. (2) All the related works that have used optimization methods in solving the multilevel threshold image segmentation problems are presented in more detail, focusing on the image segmentation problem and its solutions. The given related works are outlined according to the used algorithms. (3) Comprehensive results and analysis of several well-known optimization algorithms are conducted to solve the multilevel threshold image segmentation problems. These comparative methods include Aquila Optimizer (AO), Whale Optimization Algorithm (WOA), Salp Swarm Algorithm (SSA), Arithmetic Optimization Algorithm (AOA), Particle Swarm Optimizer (PSO), Marine Predators Algorithm (MPA), Krill Herd Algorithm (KHA), Multi-verse Optimizer (MVO), and Gray Wolf Optimizer (GWO). Eight standard benchmark images are used to test the comparative methods. The results are evaluated using three standard measures: fitness function, PeakSignal-to-Noise Ratio (PSNR), and the Structural Similarity Index (SSIM). (4) Discussion, open challenging, and new trends are given to help the scholars in future research get near the common problems and defect in that domain. The collected data in this review has been taken from google scholar using the stander search method. The main keywords that have been used in the search are multilevel, threshold, image, segmentation, optimization, and algorithm. We covered all the published papers in detail according to the given information, focusing on finding the common problems that still need further investigation. Furthermore, future research directions based on recently evolving designs are outlined, which should undoubtedly aid current researchers and practitioners and pave the way for new researchers interested in multilevel threshold image segmentation to seek their research in the field.},
  archive      = {J_APIN},
  author       = {Abualigah, Laith and Almotairi, Khaled H. and Elaziz, Mohamed Abd},
  doi          = {10.1007/s10489-022-04064-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11654-11704},
  shortjournal = {Appl. Intell.},
  title        = {Multilevel thresholding image segmentation using meta-heuristic optimization algorithms: Comparative analysis, open challenges and new trends},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adversarial transferability metric based on SVD of
jacobians to disentangle the correlation with robustness. <em>APIN</em>,
<em>53</em>(10), 11636–11653. (<a
href="https://doi.org/10.1007/s10489-022-04066-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferability of adversarial samples under different convolutional neural network (CNN) models is one of the metrics indicators to assess the efficiency of adversarial examples and an important research direction in defense of that. Transferability isolate models employ a particular alternative model to avoid black-box attacks. Meanwhile, recent research has revealed that adversarial transferability across sub-models may be utilized to express the diversity requirements of sub-models under ensemble robustness abstractly. Due to the lack of mathematical description for this adversarial transferability, it was utilized to be abstractly described as the diversity of different hypotheses. This paper employs the Jacobians matrix’s singular value decomposition (SVD) to provide a more accurate mathematical description of the transferability of adversarial samples between models and proposes a corresponding evaluation metric. Based on this metric, a new regularization constraint is introduced into the ensemble training, and the adversarial transferability between the sub-models is isolated optimally without the prior information of the adversarial samples. Based on the proposed metric accurately defining the transferability, further ensemble robustness experiments under small-scale dataset disentangle the correlation between transferability and robustness, indicating that the transferability isolation can only achieve robustness under an alternative transfer-based attack with partial sub-models of the ensemble.},
  archive      = {J_APIN},
  author       = {Qin, Ruoxi and Wang, Linyuan and Du, Xuehui and Ma, Shuxiao and Chen, Xingyuan and Yan, Bin},
  doi          = {10.1007/s10489-022-04066-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11636-11653},
  shortjournal = {Appl. Intell.},
  title        = {An adversarial transferability metric based on SVD of jacobians to disentangle the correlation with robustness},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal hybrid features in 3D ear recognition.
<em>APIN</em>, <em>53</em>(10), 11618–11635. (<a
href="https://doi.org/10.1007/s10489-022-04071-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite being the most rapidly evolving biometric trait, the ear suffers from a few drawbacks, such as being affected by posture, illumination, and scaling in the two-dimensional domain. To address these issues, researchers focused on the 3D domain, as the intrinsic features of the 3D ear have significantly contributed to performance enhancement. However, it has also been observed that combining 2D and 3D ear features improves recognition better than using 3D ear features alone. This article presents a hybrid descriptor where the feature vectors for ear recognition are derived from the multimodal ear using classical and learning-based approaches. The classical approach generates features using a covariance matrix, whereas the learning-based approach uses a deep auto-encoder to generate features. Further, these features are combined to create a hybrid descriptor. Thorough experiments were carried out on the largest publicly accessible ear database to demonstrate the effectiveness of the proposed approach and compare its performance to that of state-of-the-art techniques.},
  archive      = {J_APIN},
  author       = {Ganesan, Karthika and A, Chilambuchelvan and Ganapathi, Iyyakutti Iyappan and Javed, Sajid and Werghi, Naoufel},
  doi          = {10.1007/s10489-022-04071-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11618-11635},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal hybrid features in 3D ear recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust semi-supervised data representation and imputation by
correntropy based constraint nonnegative matrix factorization.
<em>APIN</em>, <em>53</em>(10), 11599–11617. (<a
href="https://doi.org/10.1007/s10489-022-03884-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods have been proposed recently for high-dimensional data representation to reduce the dimensionality of the data. Matrix Factorization (MF) as an efficient dimension-reduction method is increasingly used in a wide range of applications. However, these methods are often unable to handle data with missing entries. In a Semi-Supervised Learning (SSL) scenario, many commonly used missing value imputation methods, e.g., KNN imputation, cannot utilize the existing information on the labels, which is one of the most discriminative information in the data. Considering the outliers in the observed entries, in this paper, we propose an algorithm called Correntropy based Constraint Nonnegative Matrix Factorization Completion (CCNMF) for simultaneous construction of robust representation and imputation of high-dimensional data in an SSL scenario. Specifically, the Maximum Correntropy Criterion (MCC) is used to construct the model of the CCNMF method to alleviate the negative effects of non-Gaussian noise and outliers in the data. To solve the optimization problem, an iterative algorithm based on a Fenchel Conjugate (FC) and Block Coordinate Update (BCU) framework is proposed. We show that the proposed algorithm can satisfy not only objective sequential convergence but also iterate sequence convergence. The experiments are conducted on the real-world image dataset and community health dataset. In many cases, it is shown that the proposed method outperforms several state-of-the-art methods for both representation and imputation.},
  archive      = {J_APIN},
  author       = {Zhou, Nan and Du, Yuanhua and Liu, Jun and Huang, Xiuyu and Shen, Xiao and Choi, Kup-Sze},
  doi          = {10.1007/s10489-022-03884-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11599-11617},
  shortjournal = {Appl. Intell.},
  title        = {Robust semi-supervised data representation and imputation by correntropy based constraint nonnegative matrix factorization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary framework for automatic security guards
deployment in large public spaces. <em>APIN</em>, <em>53</em>(10),
11586–11598. (<a
href="https://doi.org/10.1007/s10489-022-03975-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of security guards in large public spaces is a promising research topic with a wide range of applications. Existing methods are mainly based on manual design approaches, which are neither effective nor flexible enough for large-scale scenarios. To address this issue, this paper proposes an evolutionary framework to automatically generate the optimal deployment strategy of security guards in large public spaces. The proposed method includes a new metric for automatically evaluating deployment strategies, as well as an evolutionary solver based on differential evolution to optimize the deployment strategy automatically. To evaluate its effectiveness, the proposed evolutionary framework is tested on two synthetic scenarios with different characteristics and one real-world scenario. The results demonstrate that the proposed framework outperforms several commonly used strategies in terms of the response time of security guards.},
  archive      = {J_APIN},
  author       = {Ma, Zhitong and Zhong, Jinghui and Liu, Wei-Li and Yu, Wei-Jie},
  doi          = {10.1007/s10489-022-03975-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11586-11598},
  shortjournal = {Appl. Intell.},
  title        = {An evolutionary framework for automatic security guards deployment in large public spaces},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavior imitation of individual board game players.
<em>APIN</em>, <em>53</em>(10), 11571–11585. (<a
href="https://doi.org/10.1007/s10489-022-04050-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and predicting player behavior is of the utmost importance in game development and matchmaking. A variety of methods have been proposed to build artificial intelligence (AI), human-like players. However, these human-like players have a limited ability to imitate the behavior of individual players. In this paper, we propose a player behavior imitation method using imitation learning under the framework of meta-learning. A generic behavior model of game players was learned from historical records using adversarial imitation learning. Then, we personalized the policy by imitating the behavior of each individual player. Convolutional neural networks were used to construct the feature extractor of game board states. The experiments were conducted using the Reversi game, and 18,000 game records of different players were used to train the generic behavior model. The behavior of each new player was learned using only hundreds of records. The results demonstrate that our method can be utilized to imitate individual behavior in terms of action similarity well.},
  archive      = {J_APIN},
  author       = {Pan, Chao-Fan and Min, Xue-Yang and Zhang, Heng-Ru and Song, Guojie and Min, Fan},
  doi          = {10.1007/s10489-022-04050-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11571-11585},
  shortjournal = {Appl. Intell.},
  title        = {Behavior imitation of individual board game players},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grafting constructive algorithm in feedforward neural
network learning. <em>APIN</em>, <em>53</em>(10), 11553–11570. (<a
href="https://doi.org/10.1007/s10489-022-04082-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructive algorithm provides a gradually building mechanism by increasing nodes from zero. By this means, the neural network can independently and efficiently determine its structure. However, this mechanism has an essential issue: the algorithm that adds nodes one by one is too greedy to keep an efficient construction way and the global optimal solution may be missed. Therefore, this paper proposes a novel grafting mechanism to add block nodes of any number by training a sub-network during the construction. Then, a fast-training approach of the added block neurons is presented by selecting a small sub-network from the large initialized network and the corresponding grafting constructive algorithm (GCA) is established. To obtain a compact network structure, a fine-tuning scheme is developed according to GCA to adjust all parameters as a hybrid fashion and the hidden weights are extended to deal with matrix input in image classification. The experimental results on regression and classification tasks demonstrate that the proposed GCA can achieve a more compact network than other constructive algorithms and a faster error convergence rate than traditional gradient-based optimization algorithms.},
  archive      = {J_APIN},
  author       = {Zhang, Siyuan and Xie, Linbo},
  doi          = {10.1007/s10489-022-04082-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11553-11570},
  shortjournal = {Appl. Intell.},
  title        = {Grafting constructive algorithm in feedforward neural network learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CDGAT: A graph attention network method for credit card
defaulters prediction. <em>APIN</em>, <em>53</em>(10), 11538–11552. (<a
href="https://doi.org/10.1007/s10489-022-03996-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing potential defaulters is a crucial problem for financial institutions. Therefore, many credit scoring methods have been proposed in the past to address this issue. However, these methods rarely consider the interaction among customers such as bank transfer and remittance. With rapid growth in the number of customers adopting online banking services, such interaction information plays a significant role in assessing their credit score. In this paper, we propose a novel scalable credit scoring approach called CDGAT (Graph attention network for credit card defaulters) for predicting potential credit card defaulters. In CDGAT, a customer’s credit score is calculated based on transaction embedding and neighborhood embedding. To obtain the neighborhood embedding, CDGAT first utilizes the Amount-bias Sampling (AbS) strategy to extract a subgraph for each customer. Next, CDGAT directly aggregates neighbors’ features according to their influence weights. The experimental results on the dataset from Industrial and Commercial Bank of China (Macau) Limited (ICBC (Macau)) show that CDGAT significantly outperforms the baseline methods. Furthermore, experimental results reveal that the proposed method is also superior to several state-of-the-art Graph Convolutional Neural Network models in terms of scalability and performance.},
  archive      = {J_APIN},
  author       = {Wu, Jun and Zhao, XiongFei and Yuan, Hang and Si, Yain-Whar},
  doi          = {10.1007/s10489-022-03996-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11538-11552},
  shortjournal = {Appl. Intell.},
  title        = {CDGAT: A graph attention network method for credit card defaulters prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Embedded mutual learning: A novel online distillation
method integrating diverse knowledge sources. <em>APIN</em>,
<em>53</em>(10), 11524–11537. (<a
href="https://doi.org/10.1007/s10489-022-03974-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is a feasible and effective way to obtain small networks with outstanding properties that can be deployed on hardware-constrained devices. Earlier KD methods were primarily implemented offline and used single information contained in logits or features as the source of KD, resulting in the limited impact of KD on the network. And the online distillation structure also lacks flexibility. To address those issues, we propose embedded mutual learning (EML), a novel online distillation method. By embedding an ensemble branch and an adaptive fusion branch between two parallel peer networks, EML can use the ensemble information and overall feature representations of all peer networks and the logits to complete online KD. Diverse knowledge helps fully mine the potential of the networks during online distillation. Through extensive experiments conducted on CIFAR-10, CIFAR-100, and ImageNet, we demonstrate that the EML is superior to the state-of-the-art online KD methods on image classification task. Meanwhile, we provide insights on why our EML method is effective from different perspectives. In particular, the embedded implementation makes EML highly flexible. The EML can be widely used in different network combinations composed of heterogeneous or homogeneous networks. The implementation code of our EML model is obtainable from https://github.com/lcxlcx/EML .},
  archive      = {J_APIN},
  author       = {Li, Chuanxiu and Li, Guangli and Zhang, Hongbin and Ji, Donghong},
  doi          = {10.1007/s10489-022-03974-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11524-11537},
  shortjournal = {Appl. Intell.},
  title        = {Embedded mutual learning: A novel online distillation method integrating diverse knowledge sources},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attentional-walk-based autoencoder for community
detection. <em>APIN</em>, <em>53</em>(10), 11505–11523. (<a
href="https://doi.org/10.1007/s10489-021-02957-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of community detection is to discover closely connected groups of entities in complex networks such as interest groups, proteins and vehicles in social, biological and transportation networks. Recently, autoencoders have become a popular technique to extract nonlinear relationships between nodes by learning their representation vectors through an encoder-decoder neural structure, which is beneficial to discovering communities with vague boundaries. However, most of the existing autoencoders take restoring a network’s adjacency matrix as their objective, which puts emphasis on the first-order relationships between the nodes and neglects their higher-order relationships that may be more useful for community detection. In this paper, we propose a novel attentional-walk-based autoencoder (AWBA) which integrates random walk considering attentional coefficients between each pair of nodes into the encoder to mine their high-order relationships. First, the attention layers are added to the encoder to learn the influence of a node’s different neighbors on it in encoding. Second, we develop a new random walk strategy that embeds the attention coefficients and the community membership of the nodes obtained by a seed-expansion-based clustering algorithm into the computation of the transition probability matrix to instill both low and high order relationships between the nodes into the representation vectors. The experimental results on synthetic and real-world networks verify the superiority of our algorithm over the baseline algorithms.},
  archive      = {J_APIN},
  author       = {Guo, Kun and Zhang, Peng and Guo, Wenzhong and Chen, Yuzhong},
  doi          = {10.1007/s10489-021-02957-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11505-11523},
  shortjournal = {Appl. Intell.},
  title        = {An attentional-walk-based autoencoder for community detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global wasserstein margin maximization for boosting
generalization in adversarial training. <em>APIN</em>, <em>53</em>(10),
11490–11504. (<a
href="https://doi.org/10.1007/s10489-022-03480-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent researches on adversarial robustness boosting, the trade-off between standard and robust generalization has been widely concerned, in which margin, the average distance from samples to the decision boundary, has become the bridge between the two ends. In this paper, the problems of the existing methods to improve the adversarial robustness by maximizing the margin are discussed and analyzed. On this basis, a new method to approximate the margin from a global point of view through the Wasserstein Distance of distribution of representation is proposed, which is called Global Wasserstein Margin. By maximizing the Global Wasserstein Margin in the process of adversarial training, the generalization capability of the model can be improved, reflected as the standard and robust accuracy advantages on the latest baseline of adversarial training.},
  archive      = {J_APIN},
  author       = {Yu, Tingyue and Wang, Shen and Yu, Xiangzhan},
  doi          = {10.1007/s10489-022-03480-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11490-11504},
  shortjournal = {Appl. Intell.},
  title        = {Global wasserstein margin maximization for boosting generalization in adversarial training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UAVs rounding up inspired by communication multi-agent depth
deterministic policy gradient. <em>APIN</em>, <em>53</em>(10),
11474–11489. (<a
href="https://doi.org/10.1007/s10489-022-03986-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAVs rounding up is a game between UAV swarm and targets. The main challenge lies in achieving efficient collaboration between UAVs and the setting of rounding-up points. This paper extends our work in three aspects, including establishing an information interaction strategy model, dynamic rounding-up points, and detailed reward function settings. Inspired by the intelligence of the biological swarm, this paper constructs a communication multi-agent depth deterministic policy gradient (COM-MADDPG) framework, based on the communication topology during the rounding-up process, which proposes an information interaction strategy as action policy in reinforcement learning. When carrying out rounding up, it is no longer limited to a fixed threshold, and a dynamic rounding-up points is proposed to judge the success of the mission, each UAV has own area of rounding-up and cooperate to complete the swarm mission. In view of the situation where the target is at the corner or edge, the reward function of reinforcement learning is redefined, which effectively avoids the problem of rounding-up failure under special circumstances. Furthermore, the simulation results verify the COM-MADDPG framework perform better than DDPG and MADDPG in rounding-up tasks, and can be help for improving the success rate, which confirms the effectiveness of decision-making in those special situations. Those all have shown promise due to their robustness.},
  archive      = {J_APIN},
  author       = {Jiang, Longting and Wei, Ruixuan and Wang, Dong},
  doi          = {10.1007/s10489-022-03986-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11474-11489},
  shortjournal = {Appl. Intell.},
  title        = {UAVs rounding up inspired by communication multi-agent depth deterministic policy gradient},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving evolutionary algorithms with information feedback
model for large-scale many-objective optimization. <em>APIN</em>,
<em>53</em>(10), 11439–11473. (<a
href="https://doi.org/10.1007/s10489-022-03964-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many evolutionary algorithms have been proposed. Compared to other algorithms, the core of the many-objective evolutionary algorithm using a one-by-one selection strategy is to select offspring one by one in environmental selection. However, it does not perform well in resolving large-scale many-objective optimization problems. In addition, a large amount of meaningful information in the population of the previous iteration is not retained. The information feedback model is an effective strategy to reuse the information from previous populations and integrate it into the update process of the offspring. Based on the original algorithm, this paper proposes a series of many-objective evolutionary algorithms, including six new algorithms. Experiments were carried out in three different aspects. Using the same nine benchmark problems, we compared the original algorithm with six new algorithms. Algorithms with excellent performance were selected and compared with the latest studies using the information feedback model from two aspects. Then, the best one was selected for comparison with six state-of-the-art many-objective evolutionary algorithms. Additionally, non-parametric statistical tests were conducted to evaluate the different algorithms. The comparison, with up to 15 objectives and 1500 decision variables, showed that the proposed algorithm achieved the best performance, indicating its strong competitiveness.},
  archive      = {J_APIN},
  author       = {Wang, Yong and Zhang, Qian and Wang, Gai-Ge},
  doi          = {10.1007/s10489-022-03964-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11439-11473},
  shortjournal = {Appl. Intell.},
  title        = {Improving evolutionary algorithms with information feedback model for large-scale many-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete salp swarm algorithm for euclidean travelling
salesman problem. <em>APIN</em>, <em>53</em>(10), 11420–11438. (<a
href="https://doi.org/10.1007/s10489-022-03976-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Salp Swarm Algorithm (SSA) is one of the recently proposed swarm intelligence-based algorithms, which finds its motivation in the swarming behaviour of salps when navigating and foraging in the ocean. The SSA is essentially derived to solve optimization problems that have continuous search space. SSA is a simple to implement and competitive algorithm that has been proven helpful in a variety of real-world applications. It has been explored over several optimization problems so far. In this study, an enhanced discrete version of SSA is proposed to solve the Travelling Salesman Problem (TSP). As TSP is a combinatorial optimization problem, the classical SSA is modified using swap, shift and symmetry operators for global exploration and local exploitation. Also, the 2-opt method has been incorporated with the proposed algorithm to improve the local search ability of the algorithm while addressing discrete problems. The proposed Discrete Salp Swarm Algorithm (DSSA) has evaluated over 45 TSP instances, and the computational results showed that it is a promising algorithm. To assess its performance, the proposed algorithm’s results are compared with well-known algorithms such as Genetic Algorithm, Artificial Bee Colony, Spider Monkey Algorithm, Jaya Algorithm, Black Hole, Symbiotic Organism Search etc. The proposed algorithm significantly outperformed these algorithms for a majority of TSP instances.},
  archive      = {J_APIN},
  author       = {Panwar, Karuna and Deep, Kusum},
  doi          = {10.1007/s10489-022-03976-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11420-11438},
  shortjournal = {Appl. Intell.},
  title        = {Discrete salp swarm algorithm for euclidean travelling salesman problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time series cross-correlation network for wind power
prediction. <em>APIN</em>, <em>53</em>(10), 11403–11419. (<a
href="https://doi.org/10.1007/s10489-022-04004-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power is an indispensable part of clean energy, but due to its inherent instability, it is necessary to predict the power generation of wind turbines after a period of time accurately. Most recent approaches are based on machine learning methods, which map time series data to a high-dimensional space, follow Markov process in the time dimension, and extract time series features following the chronological order. However, due to the time-instability and highly spatially correlated nature of wind power data, the prediction methods which follow the chronological order cannot extract all features in wind power time series. This will lose the information contained in the sequence and lack spatially relevant information. This paper proposes a TSCN-LSTM model that includes a Time Series Cross-correlation Network (TSCN) and a long short term memory (LSTM) decoder to predict wind power generation after 30 minutes. TSCN-LSTM not only collaboratively encodes the neighboring area and the neighboring time to fully tap the potential spatiotemporal correlation by TSCN, but also uses the LSTM decoder to enhance the timing relationship and to prevent the loss of timing information. At the same time, a data preprocessing method is proposed to enhance the spatial representation of the data. It makes that TSCN-LSTM can integrate the temporal and spatial feature extraction process to enhance the semantic expression of features. By establishing extensive interconnections between data in time dimension and space dimension, multiple types of cross-correlation information are generated which enables the model to distinguish different meteorological features and discover temporal and spatial correlations. A large number of experiments show that compared with the current leading SVR, LSTM, LSTM-EFG, GRU, Bi-LSTM and SATCN-LSTM methods, the wind power prediction mean square error is reduced by an average of 13.06%, 12.44%, 4.05%, 5.11%, 6.94% and 8.76%, respectively.},
  archive      = {J_APIN},
  author       = {Yu, Ruiguo and Sun, Yingzhou and Li, Xuewei and Yu, Jian and Gao, Jie and Liu, Zhiqiang and Yu, Mei},
  doi          = {10.1007/s10489-022-04004-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {10},
  pages        = {11403-11419},
  shortjournal = {Appl. Intell.},
  title        = {Time series cross-correlation network for wind power prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Closed-loop feedback registration for consecutive images of
moving flexible targets. <em>APIN</em>, <em>53</em>(9), 10647–10667. (<a
href="https://doi.org/10.1007/s10489-022-04068-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancement of imaging techniques enables consecutive image sequences to be acquired for quality monitoring of manufacturing production lines. Registration for these image sequences is essential for in-line pattern inspection and metrology, e.g., in the printing process of flexible electronics. However, conventional image registration algorithms cannot produce accurate results when the images contain duplicate and deformable patterns in the manufacturing process. Such a failure originates from the fact that the conventional algorithms only use spatial and pixel intensity information for registration. Considering the nature of temporal continuity of the product images, in this paper, we propose a closed-loop feedback registration algorithm. The algorithm leverages the temporal and spatial relationships of the consecutive images for fast, accurate, and robust point matching. The experimental results show that our algorithm finds about 100% more matching point pairs with a lower root mean squared error and reduces up to 86.5% of the running time compared to other state-of-the-art outlier removal algorithms.},
  archive      = {J_APIN},
  author       = {Ma, Rui and Du, Xian},
  doi          = {10.1007/s10489-022-04068-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10647-10667},
  shortjournal = {Appl. Intell.},
  title        = {Closed-loop feedback registration for consecutive images of moving flexible targets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). MultiHop attention for knowledge diagnosis of mathematics
examination. <em>APIN</em>, <em>53</em>(9), 10636–10646. (<a
href="https://doi.org/10.1007/s10489-022-04033-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent educational diagnosis can effectively promote the development of artificial intelligence in education. The knowledge diagnosis of specific domains (e.g., mathematics, physics) plays an important role in intelligent educational diagnosis but typically relies on complex semantic information. Most existing methods only produce single sentence representations that have difficulty detecting multiple knowledge points from text. The resources of knowledge point diagnosis of specific domains are also relatively sparse. In this study, we build a dataset about mathematics that is collected from real mathematical examination and artificially annotated 18 knowledge points. We also propose the MultiHop Attention mechanism (MHA) model to focus on different important information in mathematical questions using a multiple attention mechanism. Each attention mechanism obtains different attention weights for different parts of mathematical questions. The MHA allows us to effectively obtain a comprehensive semantic representation of mathematical questions. Additionally, because the ALBERT model is advanced and efficient, we use it for word embedding in this study. The proposed method synthetically considers multiple keywords related to knowledge points in mathematical questions for knowledge diagnosis research. Experimental results with the proposed mathematical dataset show that MHA achieves marked improvements compared to existing methods.},
  archive      = {J_APIN},
  author       = {He, Xinyu and Zhang, Tongxuan and Zhang, Guiyun},
  doi          = {10.1007/s10489-022-04033-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10636-10646},
  shortjournal = {Appl. Intell.},
  title        = {MultiHop attention for knowledge diagnosis of mathematics examination},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer models used for text-based question answering
systems. <em>APIN</em>, <em>53</em>(9), 10602–10635. (<a
href="https://doi.org/10.1007/s10489-022-04052-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The question answering system is frequently applied in the area of natural language processing (NLP) because of the wide variety of applications. It consists of answering questions using natural language. The problem is, in general, solved by employing a dataset that consists of an input text, a query, and the text segment or span from the input text that provides the question’s answer. The ability to make human-level predictions from data has improved significantly thanks to deep learning models, particularly the Transformer architecture, which has been state-of-the-art in text-based models in recent years. This paper reviews studies related to the use of transformer models in the implementation of question-answering (QA) systems. The paper’s first focus is on the attention and transformer models. A brief description of the architectures is presented by classifying them into models based on encoders, decoders, and on both Encoder-Decoder. Following that, we examine the most recent research trends in textual QA datasets by highlighting the architecture of QA systems and categorizing them according to various criteria. We survey also a significant set of evaluation metrics that have been developed in order to evaluate the models’ performance. Finally, we highlight solutions built to simplify the implementation of Transformer models.},
  archive      = {J_APIN},
  author       = {Nassiri, Khalid and Akhloufi, Moulay},
  doi          = {10.1007/s10489-022-04052-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10602-10635},
  shortjournal = {Appl. Intell.},
  title        = {Transformer models used for text-based question answering systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weight grouping operators selection strategy for a
multiobjective evolutionary algorithm based on decomposition.
<em>APIN</em>, <em>53</em>(9), 10585–10601. (<a
href="https://doi.org/10.1007/s10489-022-03900-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiobjective evolutionary algorithm based on decomposition (MOEA/D) works by using a set of uniformly distributed weight vectors to decompose a multiobjective optimization problem (MOP) into multiple single objective optimization subproblems for simultaneous optimization. Selecting parents from the neighborhood when performing reproduction is highly probable, thus different operators and neighborhoods may produce different offspring. However, one omnipotent operator usually cannot handle all different complex MOPs very well, for this reason, we propose a weight grouping operators selection (WGOS) strategy. Firstly, we divide the weight vectors into several groups and assign different reproduction operators to each group. The subproblems in each group are optimized by using the reproduction operators assigned. Then, the size of each group is dynamically adjusted according to the quality of the generated offspring, and the group of the reproduction operator that generates the better offspring expands, and the group of the reproduction operator that generates the worse offspring shrinks correspondingly until all the groups are merged into one and all adopt the same reproduction operator. This process will eventually pick out one operator for subsequent evolution. Finally, we will verify the performance of the selected operator to decide whether to continue using it or switch to another operator. A large number of comparative experiments have proved this strategy has better performance.},
  archive      = {J_APIN},
  author       = {Shi, Lin and Tan, Yanyan and Yan, Zeyuan and Meng, Lili and Liu, Li},
  doi          = {10.1007/s10489-022-03900-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10585-10601},
  shortjournal = {Appl. Intell.},
  title        = {Weight grouping operators selection strategy for a multiobjective evolutionary algorithm based on decomposition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transpose convolution based model for super-resolution image
reconstruction. <em>APIN</em>, <em>53</em>(9), 10574–10584. (<a
href="https://doi.org/10.1007/s10489-022-03745-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image resolution is a noticeably challenging issue that targets to acquire a high-resolution output out of one of its low-resolution variants. Many existing approaches for single-image resolutions are based on the direct solving details by using pre-defined up sampling operators. Therefore, it is challenging for the reconstruction process when the image has a larger upsampling factor. Recently, convolution neural networks (CNNs) made easy progress on super-resolution (SR) image with good results. However, the majority of methods are based on pre-defined up sampling, which uses the bicubic interpolation technique for upscaling the low-resolution (LR) image and employs feature maps to reconstruct the final high-resolution (HR) image. This leads to visual artifacts in reconstructed images and can be difficult to train such a model with a larger network. Therefore, we remove the proposed transposed convolution layer method with a novel architecture and avoid the usage of pre-defined up sampling operators. We purpose an efficient method for the usage of transposed convolution with a new architecture design and use a recurrent residual block for mapping extraction in a step-by-step manner. Finally, we generate the desired super-resolution image with low complexity and fewer parameters. Experiments and state-of-art results show better performance than existing models.},
  archive      = {J_APIN},
  author       = {Sahito, Faisal and Zhiwen, Pan and Sahito, Fahad and Ahmed, Junaid},
  doi          = {10.1007/s10489-022-03745-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10574-10584},
  shortjournal = {Appl. Intell.},
  title        = {Transpose convolution based model for super-resolution image reconstruction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A density-grid-based method for clustering k-dimensional
data. <em>APIN</em>, <em>53</em>(9), 10559–10573. (<a
href="https://doi.org/10.1007/s10489-022-03711-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel density-grid-based method for clustering k-dimensional data. KIDS, an acronym for K-dimensional Ink Drop Spread, detects densely-connected pieces of data in k-dimensional grids. It enables one to simultaneously exploit the advantages of fuzzy logic, as well as both density-based and grid-based clustering. In the proposed method, the k-dimensional data space is divided into different cells. Input data records are mapped to the cells. The data points are then spread in the k-dimensional cells, just like what happens to ink drops in water. So the cells adjacent to the data cells also represent the data. Eventually, the impacts of all data grid cells are condensed and compared with the threshold to compute the final clusters. The experimental results show that the method has superior quality and efficiency in both low and high dimensions. In addition, the method is not only robust to noise but it is also capable of finding clusters of arbitrary shapes.},
  archive      = {J_APIN},
  author       = {Kashani, Elham S. and Bagheri Shouraki, Saeed and Norouzi, Yaser and De Baets, Bernard},
  doi          = {10.1007/s10489-022-03711-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10559-10573},
  shortjournal = {Appl. Intell.},
  title        = {A density-grid-based method for clustering k-dimensional data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion-cause pair extraction based on interactive
attention. <em>APIN</em>, <em>53</em>(9), 10548–10558. (<a
href="https://doi.org/10.1007/s10489-022-03873-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a new fine-grained task has been proposed in the field of sentiment analysis, the emotion-cause pair extraction (ECPE) task, whose purpose is to extract all emotions and their causes from a document. Most of existing methods produce effective emotion-cause pairs by filtering all possible pairs. However, this types of methods ignore the relationship between emotion clauses and cause clauses when learning the representations of emotions and causes clauses. In order to solve the above problem, we propose an end-to-end framework, which uses interactive attention and its fusion mechanism to learn the relationship between emotions and causes, and then pair them. Experimental results on quasi-base corpus shows our proposed method outperform the state-of-the-art baseline.},
  archive      = {J_APIN},
  author       = {Huang, Weichun and Yang, Yixue and Huang, Xiaohui and Peng, Zhiying and Xiong, Liyan},
  doi          = {10.1007/s10489-022-03873-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10548-10558},
  shortjournal = {Appl. Intell.},
  title        = {Emotion-cause pair extraction based on interactive attention},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced spatial-temporal freedom for video frame
interpolation. <em>APIN</em>, <em>53</em>(9), 10535–10547. (<a
href="https://doi.org/10.1007/s10489-022-03787-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the introduction of deformable convolution, the kernel-based Video Frame Interpolation (VFI) has made significant progress. However, there are still some problems, such as the limited spatial-temporal freedom degree in the sampling point extraction stage and the insufficient utilization of spatial-temporal information in the feature extraction stage of the kernel-based methods. In this paper, we propose a video frame interpolation method based on Enhanced Spatial-Temporal Freedom (ESTF), which consists of four modules for VFI. Specifically, we first combine 3D and deformable convolutions to extract spatial-temporal feature information in the proposed enhanced spatial-temporal feature extraction module. Then, we utilize an enhanced freedom fusion module to adaptively estimate parameters and generate intermediate frames by adaptive estimators and deformable fusion layers, respectively. Finally, a context extraction module and a residual contextual refinement module are utilized to extract context features for optimizing the generated frames. Extensive experimental results on various popular benchmarks such as Vimeo90K, GOPRO, and Adobe240 demonstrate that the proposed method achieves competitive performance against most existing methods, especially when dealing with complex motions.},
  archive      = {J_APIN},
  author       = {Li, Hao-Dong and Yin, Hui and Liu, Zhi-Hao and Huang, Hua},
  doi          = {10.1007/s10489-022-03787-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10535-10547},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced spatial-temporal freedom for video frame interpolation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spectral clustering algorithm based on attribute
fluctuation and density peaks clustering algorithm. <em>APIN</em>,
<em>53</em>(9), 10520–10534. (<a
href="https://doi.org/10.1007/s10489-022-04058-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering (SC) has become a popular choice for data clustering by converting a dataset to a graph structure and then by identifying optimal subgraphs by graph partitioning to complete the clustering. However, k-means is taken at the clustering stage to randomly select the initial cluster centers, which leads to unstable performance. Notably, k-means needs to specify the number of clusters (prior knowledge). Second, SC calculates the similarity matrix using the linear Euclidean distance, losing part of the effective information. Third, real datasets usually contain redundant features, but traditional SC does not adequately address multi-attribute data. To solve these issues, we propose an SC algorithm based on the attribute fluctuation and density peaks clustering algorithm (AFDSC) to improve the clustering accuracy and effect. Furthermore, to verify the idea of the AFDSC algorithm, we extract the attribute fluctuation factor and propose a histogram clustering algorithm based on attribute fluctuation (AFHC) divorced from spectral clustering. Experimental results show that both the AFDSC algorithm and AFHC algorithm have achieved better performance on fifteen UCI datasets compared with other clustering algorithms.},
  archive      = {J_APIN},
  author       = {Song, Xin and Li, Shuhua and Qi, Ziqiang and Zhu, Jianlin},
  doi          = {10.1007/s10489-022-04058-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10520-10534},
  shortjournal = {Appl. Intell.},
  title        = {A spectral clustering algorithm based on attribute fluctuation and density peaks clustering algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new hybrid prediction model with entropy-like kernel
function for dynamic multi-objective optimization. <em>APIN</em>,
<em>53</em>(9), 10500–10519. (<a
href="https://doi.org/10.1007/s10489-022-03934-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-objective problems (DMOPs) permeate all aspects of daily life and practical applications. As the variables of the search space or target space alter in pace with time, savants are also deepening the research on DMOPs, among which methods based on prediction mechanisms have been extensively developed. The historical optimal solutions can effectively predict the trend and location of the optimal solutions in the future. In this paper, a new hybrid prediction model (HPM) integrating the fuzzy linear prediction model with entropy-like kernel function and the one-step prediction model is developed to sort out DMOPs. In the method, the predicted center by the HPM prediction model is combined with the approximate manifold of PS to generate a trail population, and the linear one-step prediction model is utilized to generate another trail population. When the environment changes, the initial PS at the next moment is obtained by randomly crossing these two trail populations. To assess the proposed HPM model, it is compared with the reinitialization strategy, feedforward prediction strategy, population prediction strategy, T-S nonlinear regression strategy with multistep prediction and individual-based transfer learning under different MOEA optimizers for 22 benchmark problems. The results indicate that HPM has great advantages in solving these dynamic optimization problems.},
  archive      = {J_APIN},
  author       = {Cao, Siyu and Zou, Feng and Chen, Debao and Liu, Hui and Ji, Xuying and Zhang, Yan},
  doi          = {10.1007/s10489-022-03934-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10500-10519},
  shortjournal = {Appl. Intell.},
  title        = {A new hybrid prediction model with entropy-like kernel function for dynamic multi-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing customer attrition management system: Discovering
action rules for making recommendations to retain customers.
<em>APIN</em>, <em>53</em>(9), 10485–10499. (<a
href="https://doi.org/10.1007/s10489-022-03614-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer churn, a major concern for most of the companies, leads to higher customer acquisition cost, lower volume of service consumption and reduced product purchase. Thus, it is critical for companies to take effective strategies to reduce customer outflow. In this paper, we aim to discover high quality action rules and provide valid and trustworthy recommendations to improve customer churn rate. We propose a Semantic-aided Customer Attrition Management System (SaCAMS), in which we use reducts for feature engineering, apply hierarchical clustering to build the semantic similarity relationship among clients, run action rule mining to discover the actionable patterns, and extract meta-actions to get the final recommendations. The experimental results show that SaCAMS can discover high quality action rules. Moreover, based on the improved action rules, SaCAMS can extract effective meta-actions to generate recommendations. Last but not least, SaCAMS utilizes meta-node to provide decision-makers with valid and trustworthy strategies, which are quantified by effectiveness scores.},
  archive      = {J_APIN},
  author       = {Duan, Yuehua and Ras, Zbigniew W.},
  doi          = {10.1007/s10489-022-03614-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10485-10499},
  shortjournal = {Appl. Intell.},
  title        = {Developing customer attrition management system: Discovering action rules for making recommendations to retain customers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PDDNet: Lightweight congested crowd counting via pyramid
depth-wise dilated convolution. <em>APIN</em>, <em>53</em>(9),
10472–10484. (<a
href="https://doi.org/10.1007/s10489-022-03967-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of crowd counting is susceptible to scale variations of crowd head in the congested scene. Some counting networks, such as crowd density pre-classification networks or multi-column counting networks, are proposed to model the different scales of crowd head. However, most of them own a complex network structure with many network parameters, making deploying a crowd counting network in practical application scenarios challenging. To this end, we propose a lightweight crowd counting network termed PDDNet. The front-end of the PDDNet chooses the first 13 layers of GhostNet to extract the crowd feature, and the back-end of the PDDNet is implemented with the proposed lightweight pyramidal convolution modules (LPC) to extract the multi-scale features. Finally, the extracted multi-scale features are fed to transposed convolution layers to regress the final crowd density map. We conduct extensive experiments on the commonly-used crowd counting datasets, i.e., ShanghaiTech, UCF_QNRF, and NWPU_Crowd. The experiment results show the superiority of our model compared with state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liang, Lanjun and Zhao, Huailin and Zhou, Fangbo and Ma, Mingyang and Yao, Feng and Ji, Xiaojun},
  doi          = {10.1007/s10489-022-03967-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10472-10484},
  shortjournal = {Appl. Intell.},
  title        = {PDDNet: Lightweight congested crowd counting via pyramid depth-wise dilated convolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Document-level paraphrase generation base on attention
enhanced graph LSTM. <em>APIN</em>, <em>53</em>(9), 10459–10471. (<a
href="https://doi.org/10.1007/s10489-022-04031-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paraphrase generation is one of the long-standing and important tasks in natural language processing. Existing literature has mainly focused on the generation of sentence-level paraphrases, in which the relationship between sentences was ignored, such as sentence reordering, sentence splitting, and sentence merging. In this paper, while paying attention to the relationship within sentences, we also explore the relationship between sentences. For the task of document-level interpretation generation, we focus on reordering documents to enhance inter-sentence diversity. We use the attention-enhanced graph long short-term memory (LSTM) to encode the relationship graph between sentences, so that each sentence generates a coherent representation that conforms to the context. Based on the sentence-level paraphrase generation model, we constructed a pseudo-document-level paraphrase dataset. The automatic evaluation shows that our model achieves higher scores in terms of semantic relevance and diversity scores than other strong baseline models. In the manual evaluation, the validity of our model is also confirmed. Experiments show that our model retains the semantics of the source document, while generating paraphrase documents with high diversity. When we reorder the sentences, the output paraphrase documents can still preserve the coherence between sentences with higher scores.},
  archive      = {J_APIN},
  author       = {Qiu, Dong and Chen, Lei and Yu, Yang},
  doi          = {10.1007/s10489-022-04031-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10459-10471},
  shortjournal = {Appl. Intell.},
  title        = {Document-level paraphrase generation base on attention enhanced graph LSTM},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information learning-driven consensus reaching process in
group decision-making with bounded rationality and imperfect
information: China’s urban renewal negotiation. <em>APIN</em>,
<em>53</em>(9), 10444–10458. (<a
href="https://doi.org/10.1007/s10489-022-04019-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the preferences of DMs with bounded rationality are heterogeneous and cannot be directly observed in most cases, GDM with bounded rationality under imperfect information is ubiquitous. To solve these problems, the paper proposes consensus models that minimize cost and maximize utility with prospect theory value function and imperfect information based on stochastic programming. The proposed consensus models are incorporated with a new information learning strategy that combines prior information learning based on existing information observed before negotiation and feedback obtained during negotiation to achieve group consensus. Moreover, the paper applies the proposed consensus models to the demolition negotiation problem in China’s urban renewal projects, and the effectiveness of the proposed information learning strategy is tested with comparative experiments. We further analyze the effects of the accuracy of the prior information, the moderator’s confidence in the prior information, the number of rounds of feedback, and the number of residents on GDM, which can provide practical suggestions for the government’s negotiation strategies for demolition and rebuilding projects.},
  archive      = {J_APIN},
  author       = {Zha, Quanbo and Cai, Jinfan and Gu, Jianping and Liu, Guiwen},
  doi          = {10.1007/s10489-022-04019-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10444-10458},
  shortjournal = {Appl. Intell.},
  title        = {Information learning-driven consensus reaching process in group decision-making with bounded rationality and imperfect information: China’s urban renewal negotiation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal transformer using two-level visual features for
fake news detection. <em>APIN</em>, <em>53</em>(9), 10429–10443. (<a
href="https://doi.org/10.1007/s10489-022-04055-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news with multimedia data is ubiquitous on the Internet nowadays, and it is difficult for users to distinguish them. Therefore, it is necessary to design automatic multi-modal fake news detectors. However, the existing works make poor utilization of visual information, and do not fully consider the semantic interaction of multi-modal data. In this paper, we propose the multi-modal transformer using two-level visual features (MTTV) for fake news detection. First, we model texts and images from news uniformly as sequences that can be processed by transformer, and two-level visual features, i.e. global feature and entity-level feature, are used to improve the utilization of news images. Second, we extend the transformer model for natural language processing to multi-modal transformer which can make multi-modal data interact fully and capture the semantic relationships between them. In addition, we propose a scalable classifier to improve the classification balance of fine-grained fake news detection with the problem of class imbalance. Extensive experiments on two public datasets demonstrate that our method achieved significant performance improvement compared to the state-of-the-art methods. The source code is available at https://github.com/cqu-wb/MTTV .},
  archive      = {J_APIN},
  author       = {Wang, Bin and Feng, Yong and Xiong, Xian-cai and Wang, Yong-heng and Qiang, Bao-hua},
  doi          = {10.1007/s10489-022-04055-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10429-10443},
  shortjournal = {Appl. Intell.},
  title        = {Multi-modal transformer using two-level visual features for fake news detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). BERT-based chinese text classification for emergency
management with a novel loss function. <em>APIN</em>, <em>53</em>(9),
10417–10428. (<a
href="https://doi.org/10.1007/s10489-022-03946-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an automatic Chinese text categorization method for solving the emergency event report classification problem. Since the bidirectional encoder representations from transformers (BERT) has achieved great success in the natural language processing domain, it is employed to derive emergency text features in this study. To overcome the data imbalance problem in the distribution of emergency event categories, a novel loss function is proposed to improve the performance of the BERT-based model. Meanwhile, in order to avoid the negative impacts of the extreme learning rate, the Adabound optimization algorithm that achieves a gradual smooth transition from Adam optimizer to stochastic gradient descent optimizer is employed to learn the parameters of the model. The feasibility and competitiveness of the proposed method are validated on both imbalanced and balanced datasets. Furthermore, the generic BERT, BERT ensemble LSTM-BERT (BERT-LB), Attention-based BiLSTM fused CNN with gating mechanism (ABLG-CNN), TextRCNN, Att-BLSTM, and DPCNN are used as benchmarks on these two datasets. Meanwhile, sampling methods, including random sampling, ADASYN, synthetic minority over-sampling techniques (SMOTE), and Borderline-SMOTE, are employed to verify the performance of the proposed loss function on the imbalance dataset. Compared with benchmarking methods, the proposed method has achieved the best performance in terms of accuracy, weighted average precision, weighted average recall, and weighted average F1 values. Therefore, it is promising to employ the proposed method for real applications in smart emergency management systems.},
  archive      = {J_APIN},
  author       = {Wang, Zhongju and Wang, Long and Huang, Chao and Sun, Shutong and Luo, Xiong},
  doi          = {10.1007/s10489-022-03946-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10417-10428},
  shortjournal = {Appl. Intell.},
  title        = {BERT-based chinese text classification for emergency management with a novel loss function},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pareto front estimation-based constrained multi-objective
evolutionary algorithm. <em>APIN</em>, <em>53</em>(9), 10380–10416. (<a
href="https://doi.org/10.1007/s10489-022-03990-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The balance of convergence, diversity, and feasibility plays a pivotal role in constrained multi-objective optimization problems. To address this issue, in this paper a novel method named PeCMOEA is proposed, in which the pivotal solutions, which are designed for estimating the constrained Pareto front, are identified through an achievement scalarizing function. In addition, two different adaptive fitness functions are formulated to evaluate convergence- and diversity-oriented populations, respectively. Finally, the promising solutions from the two populations are reserved by their fitness values in the environmental selection while a self-adaptive penalty function is designed to repair infeasible solutions and ensure their feasibility. The performance of PeCMOEA is compared with five state-of-the-art constrained multi-objective evolutionary algorithms on five test suites. The experimental results illustrate that PeCMOEA exhibits competitive performance when utilised for this family of problems.},
  archive      = {J_APIN},
  author       = {Cao, Jie and Yan, Zesen and Chen, Zuohan and Zhang, Jianlin},
  doi          = {10.1007/s10489-022-03990-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10380-10416},
  shortjournal = {Appl. Intell.},
  title        = {A pareto front estimation-based constrained multi-objective evolutionary algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized multi-task reinforcement learning policy
gradient method with momentum over networks. <em>APIN</em>,
<em>53</em>(9), 10365–10379. (<a
href="https://doi.org/10.1007/s10489-022-04028-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To find the optimal policy quickly for reinforcement learning problems, policy gradient (PG) method is very effective, it parameters the policy and updates policy parameter directly. Besides, momentum methods are commonly employed to improve convergence performance in the training of centralized deep networks, which can accelerate training rate by changing the descending direction of gradients. However, decentralized variants with momentum of PG are rarely investigated. For this reason, we propose a Decentralized Policy Gradient algorithm with Momentum called DPGM for solving multi-task reinforcement learning problems. Moreover, this article makes theoretical analysis on the convergence performance of DPGM rigorously, it can reach the rate of O(1/T), where T denotes the number of iterations. This rate can match the state of the art of decentralized PG methods. Furthermore, we provide experimental verification on decentralized reinforcement learning environment to support the theoretical result.},
  archive      = {J_APIN},
  author       = {Junru, Shi and Qiong, Wang and Muhua, Liu and Zhihang, Ji and Ruijuan, Zheng and Qingtao, Wu},
  doi          = {10.1007/s10489-022-04028-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10365-10379},
  shortjournal = {Appl. Intell.},
  title        = {Decentralized multi-task reinforcement learning policy gradient method with momentum over networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph embedding by projection and rotation on
hyperplanes for link prediction. <em>APIN</em>, <em>53</em>(9),
10340–10364. (<a
href="https://doi.org/10.1007/s10489-022-03983-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge is increasingly completed due to connections formed in a knowledge graph, enabling a complete understanding of reality. Link prediction plays an important role in this process. Among the multiple methods that exist to tackle this problem, the geometry-based prediction method has attracted attention due to its intuitiveness and capacity to flexibly address various types of relations. We propose the rotation embedding of entities on separate relation-specific hyperplanes as an alternative to the translation method. Moreover, instead of optimizing the model by tight constraints, we add several soft constraints to minimize the loss function. Experimenting on standard datasets with numerous evaluation metrics, the proposed model outperforms both state-of-the-art and baseline models. We also analyze the model on multiple batch sizes and negative sample size values, along with various embedding dimensions and optimizers. Thereby, we demonstrate the impact of the parameters on the geometry-based link prediction model and provide a basis for future improvement.},
  archive      = {J_APIN},
  author       = {Le, Thanh and Huynh, Ngoc and Le, Bac},
  doi          = {10.1007/s10489-022-03983-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10340-10364},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge graph embedding by projection and rotation on hyperplanes for link prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards non-linear regression-based prediction of use case
point (UCP) metric. <em>APIN</em>, <em>53</em>(9), 10326–10339. (<a
href="https://doi.org/10.1007/s10489-022-04002-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Effort Estimation (SEE) is a procedure to estimate the effort required to develop software. The researchers have been dealing with SEE issues for a long time. Several methods were developed until the formulation of Function Point (FP) and Constructive Cost Estimation (COCOMO) methods. However, these methods were useful only for procedurally developed software, not for modern object-oriented software. On the other hand, using the Use Case Point (UCP) metric acquired from the UML diagrams can be more suitable, as the use case is the fundamental unit of an object-oriented system. An ample amount of research has already been done for UCP prediction using linear regression-based models. However, various nonlinear regression models have not been explored for predicting UCP values from different UCP parameters. Although, some of the researchers have used nonlinear regression models for predicting effort, given the UCP value. Motivated by this, the current work investigates different nonlinear regression models such as a k-nearest neighbor, decision tree, random forest, support vector machine, and multilayer perceptron for UCP prediction. The experimental investigation has been conducted on two publicly available UCP estimation datasets. Further, we compared the performance of nonlinear regression models with the linear regression-based models using different performance measures. The results suggest that the nonlinear regression models perform better than the linear regression-based models.},
  archive      = {J_APIN},
  author       = {Shukla, Suyash and Kumar, Sandeep},
  doi          = {10.1007/s10489-022-04002-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10326-10339},
  shortjournal = {Appl. Intell.},
  title        = {Towards non-linear regression-based prediction of use case point (UCP) metric},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep gaussian mixture model based instance relevance
estimation for multiple instance learning applications. <em>APIN</em>,
<em>53</em>(9), 10310–10325. (<a
href="https://doi.org/10.1007/s10489-022-04045-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL) is a type of supervised learning, where instead of receiving a collection of individually labeled examples, the learner is given weakly labeled bags of instances. If the bag contains at least one positive instance, the bag is assigned a positive label, otherwise, the bag is assigned a negative label. The positive bags in MIL may contain instances from different classes, which results in instance-level ambiguity in the bag and complicates the learning process. In this case, identifying relevant instances is important in bag classification and model interpretation. To identify the relevant instances in MIL, this paper proposes a deep subspace-based Gaussian mixture model instance relevance estimation network with Fisher vector encoding (DGMIR-FV). To be specific, the proposed approach uses an estimation network for instance relevance estimation and selects the instances from each bag based on relevance scores. Afterwards, selected instances are encoded using the Fisher vector encoding and fed to an ensemble network for classification. Compared to the existing MIL pooling methods and encoding schemes, the DGMIR-FV improves the model’s generalization ability by employing estimation network for instance relevance estimation and incorporating relevant instances in the encoding process. The experimental results demonstrate the efficiency of DGMIR-FV on several MIL benchmark datasets.},
  archive      = {J_APIN},
  author       = {Waqas, Muhammad and Tahir, Muhammad Atif and Qureshi, Rizwan},
  doi          = {10.1007/s10489-022-04045-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10310-10325},
  shortjournal = {Appl. Intell.},
  title        = {Deep gaussian mixture model based instance relevance estimation for multiple instance learning applications},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved large-scale sparse multi-objective evolutionary
algorithm using unsupervised neural network. <em>APIN</em>,
<em>53</em>(9), 10290–10309. (<a
href="https://doi.org/10.1007/s10489-022-04037-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sparse multi-objective optimization problems (LSSMOPs) widely exist in the real world, such as portfolio optimization, neural network training problems, and so on. In recent years, a number of multi-objective optimization evolutionary algorithms (MOEAs) have been proposed to deal with LSSMOPs. To improve the search efficiency of the operator, using unsupervised neural networks to reduce the search space is one of the dimensionality reduction methods in sparse MOEAs. However, it is not efficient enough that existing algorithms using neural networks consume much time to train networks in each evolutionary generation. In addition, most sparse MOEAs ignore the relationship between binary vectors and real vectors, which determine the decision variables. Thus, this paper proposes an evolutionary algorithm for solving LSSMOPs. The proposed algorithm adopts an adaptive dimensionality reduction method to achieve a balance between convergence and efficiency. The algorithm groups the binary vectors and adaptively uses a restricted Boltzmann machine to reduce the search space of binary vectors. Then, the generation of real vectors is guided by binary vectors, which enhance the relationship between both parts of the decision variables. According to the experimental results on eight benchmark problems and neural network training problems, the proposed algorithm achieves better performance than existing state-of-the-art evolutionary algorithms for LSSMOPs.},
  archive      = {J_APIN},
  author       = {Geng, Huantong and Shen, Junye and Zhou, Zhengli and Xu, Ke},
  doi          = {10.1007/s10489-022-04037-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10290-10309},
  shortjournal = {Appl. Intell.},
  title        = {An improved large-scale sparse multi-objective evolutionary algorithm using unsupervised neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-task learning based multi-energy load prediction in
integrated energy system. <em>APIN</em>, <em>53</em>(9), 10273–10289.
(<a href="https://doi.org/10.1007/s10489-022-04054-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate multi-energy load prediction plays a very crucial role in integrated energy system management. To address the load characteristics of strong relational coupling, volatility and uncertainty in user-side integrated energy systems, this paper proposes a multi-task learning based short-term multi-energy load prediction method. Load participation factor is proposed to portray the proportion of different loads in the total load demand, and multi-task learning method is introduced to deeply explore the coupling relationships among them. Then, the proposed method is validated on a real-world dataset. The results show that the method has higher prediction accuracy than the existing methods, and the prediction accuracy is improved by at least 1.3%.},
  archive      = {J_APIN},
  author       = {Wang, Lulu and Tan, Mao and Chen, Jie and Liao, Chengchen},
  doi          = {10.1007/s10489-022-04054-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10273-10289},
  shortjournal = {Appl. Intell.},
  title        = {Multi-task learning based multi-energy load prediction in integrated energy system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Baseline-independent stress classification based on facial
StO2. <em>APIN</em>, <em>53</em>(9), 10255–10272. (<a
href="https://doi.org/10.1007/s10489-022-04041-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress is an affective human state. Different types of stress have varying effects on human health. Previous stress classifications using tissue oxygen saturation (StO2) have relied on baselines to extract features in a way that limits practical application since the baselines cannot be preacquired. In this paper, we explored the correlation and the classification computability of the baseline-independent facial StO2 across four states involving baseline, emotional stress, high-intensity physical stress and low-intensity physical stress. Three analytical approaches verify that facial StO2 is affection-related and state-specific. Building on this theoretical foundation, we validated the baseline-independent facial StO2 before and after modified data augmentation using three input forms as well as two network structures. We also proposed a dual-stream (Channel stream and Spatial stream) attention network, named as CSnet, to perform baseline-independent stress classification. Experimental results suggest that the proposed method can achieve a unweighted average recall (UAR) of 0.6935 and unweighted F1-score (UF1) of 0.6991, which is higher than traditional image descriptors methods, and is more competitive than the baseline-dependent classification with respect of real applications.},
  archive      = {J_APIN},
  author       = {Liu, Xinyu and Chen, Dong and Zhou, Ju and Chen, Tong},
  doi          = {10.1007/s10489-022-04041-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10255-10272},
  shortjournal = {Appl. Intell.},
  title        = {Baseline-independent stress classification based on facial StO2},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A segmentation-based sequence residual attention model for
KRAS gene mutation status prediction in colorectal cancer.
<em>APIN</em>, <em>53</em>(9), 10232–10254. (<a
href="https://doi.org/10.1007/s10489-022-04011-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the computer-aided diagnosis of colorectal cancer, accurate determination of kratom sarcoma (KRAS) gene mutation status is crucial to provide better treatment for patients. In recent years, deep learning methods have excelled in computer vision. However, in the KRAS gene mutation status prediction, deep learning methods are usually designed for only classification tasks, ignoring the potential facilitation of segmentation tasks for classification tasks. In this paper, we propose a Segmentation-based Sequence Residual Attention Model (SSRAM) that facilitates the execution of the classification task by transferring the captured helpful information and the generated lesion masks in the segmentation task to the classification task, which in turn predicts the KRAS gene mutation status of the patient. This model consists of a Pixel Gated Segmentation Network (PG-SN) and a Channel Guided Classification Network (CG-CN). Specifically, PG-SN captures the segmentation features of lesions at different levels and generates lesion masks to provide CG-CN with prior guidance. After obtaining the precise localisation information of lesions provided by PG-SN, CG-CN shares encoders and decoders with PG-SN. That enables CG-CN to acquire the necessary high-level semantic features to enrich the classification features for accurate KRAS gene mutation status prediction. Meanwhile, to better optimise our SSRAM, we design a new boundary loss and use it jointly with Combo loss in PG-SN to solve the problem that the lesion boundary pixels are difficult to be classified correctly. We evaluate the proposed SSRAM on the T2-weighted MRI datasets and achieve an accuracy of 87.5% and an AUC of 94.74% in the KRAS gene mutation status prediction, which is superior to the performance of current non-invasive methods for predicting KRAS gene mutation status in colorectal cancer. The results suggest that our SSRAM, which accomplishes the classification task by segmentation task facilitates classification task, can effectively improve the performance and effectiveness of the classification task, thereby better helping physicians diagnose the KRAS gene mutation status of patients. The code is publicly available.},
  archive      = {J_APIN},
  author       = {Zhao, Lin and Song, Kai and Ma, Yulan and Cai, Meiling and Qiang, Yan and Sun, Jingyu and Zhao, Juanjuan},
  doi          = {10.1007/s10489-022-04011-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10232-10254},
  shortjournal = {Appl. Intell.},
  title        = {A segmentation-based sequence residual attention model for KRAS gene mutation status prediction in colorectal cancer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based 3D target detection for indoor scenes.
<em>APIN</em>, <em>53</em>(9), 10218–10231. (<a
href="https://doi.org/10.1007/s10489-022-03888-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D target detection is a research hotspot in recent years. In the field of autonomous driving, 3D target detection is mainly targeted at outdoor scenes that the camera height is constant. In a few indoor scenes, 3D target detection is mostly at the category level. However, it is difficult to generate instance-level 3D target detection datasets. In complex indoor scenes, instance-level 3D target detection is used as the research object in this paper. The indoor 3D target detection dataset is constructed by Aruco marker. A pixel-by-pixel key point voting network for joint semantic segmentation of RGB images is established, and a new key point assumption strategy is proposed. Combined with depth images, the key point detection is extended to three dimensions, and the bit pose is optimized by the ICP algorithm. The evaluation metrics and visualization of the model are analyzed and compared. It is tested and visualized under validation set, truncated validation set and unlabeled. The generalization of the method in this paper is proved, and 3D target detection in indoor scene based on RGB image and RGB-D image is achieved.},
  archive      = {J_APIN},
  author       = {Liu, Ying and Jiang, Du and Xu, Chao and Sun, Ying and Jiang, Guozhang and Tao, Bo and Tong, Xiliang and Xu, Manman and Li, Gongfa and Yun, Juntong},
  doi          = {10.1007/s10489-022-03888-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10218-10231},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning based 3D target detection for indoor scenes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical attention-based context-aware network for
long-term forecasting of chlorophyll. <em>APIN</em>, <em>53</em>(9),
10202–10217. (<a
href="https://doi.org/10.1007/s10489-022-03242-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chlorophyll forecasting is helpful for understanding red tides’ characteristics, thus enabling early warning. It is usually formulated as a multivariate time series forecasting problem, which aims to predict the future chlorophyll concentration with the observation of relevant exogenous factors (e.g., PH, turbidity, etc.) and historical chlorophyll. However, existing methods are difficult to satisfy the demand of chlorophyll forecasting because the complicated and changeable interaction existed in observed data. In this work, we propose a fine-grained hierarchical attention-based context-aware network (HACN). To adaptively extract significant influential factors, HACN consists of two branch networks, with one named EF-net focusing on exogenous factors and the other named TF-net executing on chlorophyll. In EF-net, factor-level attention and sequence-level attention are respectively leveraged to learn exogenous factors and time steps that are beneficial to prediction. In TF-net, a contextual long short-term memory network (Con-LSTM) is designed to nest a context-aware attention and introduce a filtration gate. Context-aware attention can capture the influence of exogenous factors series on historical chlorophyll. Filtration gate can suppress noisy information of external impact. Finally, we employ a gated fusion network to fuse the outputs of EF-net and TF-net. Experiments on two real-world datasets reveal that HACN not only outperforms state-of-the-art methods, but also provides interpretability for chlorophyll prediction.},
  archive      = {J_APIN},
  author       = {He, Xiaoyu and Shi, Suixiang and Geng, Xiulin and Xu, Lingyu},
  doi          = {10.1007/s10489-022-03242-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10202-10217},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical attention-based context-aware network for long-term forecasting of chlorophyll},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic resource allocation strategy for collaborative
constrained multi-objective optimization algorithm. <em>APIN</em>,
<em>53</em>(9), 10176–10201. (<a
href="https://doi.org/10.1007/s10489-022-03820-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infeasible solutions are helpful for finding the feasible regions, but how many feasible and infeasible solutions should be invested to achieve the optimal search efficiency remains to be further studied. Combined with the recently proposed collaborative constrained multi-objective framework, the contributions of the helper population and original population in different types of CMOPs are discussed. It is unreasonable to assign equal resources to these two populations in different CMOPs and different searching stages. This paper aims to investigate resource allocation in a constraint environment to efficiently utilize the limited resources and obtain a better performance. Therefore, the concept of return on investment (ROI) is first introduced to measure the contributions of two populations, and then guide the population size allocation (APS). To prevent the ROI from continuously declining as the population size decreases, an evolutionary resource allocation strategy (AER) is proposed to adjust their evolutionary state according to the cooperative relationship, and to further increase their ROI and again compete for population size, to maximize the evolutionary efficiency of the two populations in competition and cooperation. The proposed CCMODRA is compared with seven popular algorithms that cover three types of CMOEAs and test them on three benchmarks that cover four types of CMOPs. The comprehensive performance of CCMODRA is better than the other seven CMOEAs on 71% of the 3-objective CDTLZs, 57% of the 5-objective CDTLZs and 46% of the MWs. The effectiveness of the APS and AER strategies are verified on generating contribution solutions and DOC test problems. In addition, the total profit obtained by CCMODRA in the knapsack problem with capacity constraints is improved by 0.2% to 216% compared with the other seven algorithms.},
  archive      = {J_APIN},
  author       = {Pan, Xiaotian and Wang, Liping and Zhang, Menghui and Qiu, Qicang},
  doi          = {10.1007/s10489-022-03820-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10176-10201},
  shortjournal = {Appl. Intell.},
  title        = {A dynamic resource allocation strategy for collaborative constrained multi-objective optimization algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised medical image classification via increasing
prediction diversity. <em>APIN</em>, <em>53</em>(9), 10162–10175. (<a
href="https://doi.org/10.1007/s10489-022-04012-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have achieved remarkable success in medical imaging analysis. However, existing methods are primarily focused on supervised learning, which requires a massive amount of training data. Recent studies have explored semi-supervised learning approaches to address this issue, where data augmentation was applied to unlabeled data. However, there are still two unsolved challenges in applying data augmentation to unlabeled medical images: it can i) result in the lesion features loss and ii) reduce the discriminability of prediction results. Thus, in this work, weak data augmentation is applied to unlabeled data to avoid losing lesions features. Also, we propose nuclear-norm maximization to achieve entropy minimization without losing prediction diversity. Experimental results on two public datasets show that the proposed method outperforms the compared models.},
  archive      = {J_APIN},
  author       = {Liu, Peng and Qian, Wenhua and Cao, Jinde and Xu, Dan},
  doi          = {10.1007/s10489-022-04012-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10162-10175},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised medical image classification via increasing prediction diversity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised monocular depth estimation based on
pseudo-pose guidance and grid regularization. <em>APIN</em>,
<em>53</em>(9), 10149–10161. (<a
href="https://doi.org/10.1007/s10489-022-04006-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation (SMDE) has emerged as a promising alternative to generate a dense depth map in outdoor scenarios because of its low requirements for training data and sensors. However, training with only consecutive temporal frames without depth ground truth causes problems such as a lack of global supervision information and inaccurate depth estimation in low-texture areas. In this study, we propose a pseudo-label generation (PLG) module, a two-stream pose estimation (TSPE) structure, and a grid regularization loss function to address these issues. Here, the PLG is used to automatically generate pseudo-grid and pseudo-pose in the data preprocessing stage. The pseudo-grid provides reliable global position and direction information for supervision, while the pseudo-pose is used in TSPE to provide more geometric information. The TSPE fuses the geometry-based pseudo-pose and the network-based pose with a simple structure. The generalization of the pose estimation is improved by providing more geometric information. By handling the negligible depth error in the low-texture area, the proposed grid regularization loss function improves the depth estimation performance. Experiments show that our methods can improve the depth estimation performance, especially in the object boundary and low-texture area, with no additional training data or model parameters.},
  archive      = {J_APIN},
  author       = {Xiao, Ying and Chen, Weiting and Wang, Jiangtao},
  doi          = {10.1007/s10489-022-04006-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10149-10161},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised monocular depth estimation based on pseudo-pose guidance and grid regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bar transformer: A hierarchical model for learning long-term
structure and generating impressive pop music. <em>APIN</em>,
<em>53</em>(9), 10130–10148. (<a
href="https://doi.org/10.1007/s10489-022-04049-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently many deep learning-based automatic music generation models have been proposed. How to generate long pieces of pop music with distinctive musical characteristics remains a challenging problem, as it relies heavily on musical structures. Some transformer-based models take advantage of self-attention for generating long-sequence music; however, most pay little attention to well-organized musical structures. In this article, we propose a novel note-to-bar hierarchical model named the Bar Transformer to address long-term dependency issues and generate impressive and structurally meaningful music. In particular, we propose a novel note-to-bar approach that pre-processes the notes within each individual bar to provide a strong structural constraint to increase our model’s awareness of the note-to-bar structure in music. The Bar Transformer is constructed using an encoder-decoder framework, including a two-layer encoder and an arrangement decoder. In the two-layer encoder, the bottom is a note-level encoder, which outputs embeddings by learning the relation between notes within an individual bar, and the top is a bar-level encoder, which uses these embeddings to encode each bar from the melody and chord. The decoder is an arrangement decoder used to generalize the interrelationships among the bars and simultaneously generate melodies and chords. The experimental results of the structural analysis and the aural evaluations demonstrate that our approach outperforms the Music Transformer model and other regressive models used for music generation.},
  archive      = {J_APIN},
  author       = {Qin, Yang and Xie, Huiming and Ding, Shuxue and Tan, Benying and Li, Yujie and Zhao, Bin and Ye, Mao},
  doi          = {10.1007/s10489-022-04049-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10130-10148},
  shortjournal = {Appl. Intell.},
  title        = {Bar transformer: A hierarchical model for learning long-term structure and generating impressive pop music},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient homomorphic encryption framework for
privacy-preserving regression. <em>APIN</em>, <em>53</em>(9),
10114–10129. (<a
href="https://doi.org/10.1007/s10489-022-04015-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic encryption (HE) has recently attracted considerable attention as a key solution for privacy-preserving machine learning because HE can apply to various areas that require to delegate outsourcing computations of user’s data. Nevertheless, its computational inefficiency still hinders its wider application. In this study, we propose an alternative to bridge the gap between the privacy and efficiency of HE by encrypting only a small amount of private information. We first derive an exact solution to HE-friendly ridge regression with multiple private variables, while linearly reducing the computational complexity of this algorithm over the number of variables. The proposed method has the advantage that it can be implemented using any HE scheme. Moreover, we propose an adversarial perturbation method that can prevent potential attacks on private variables, which have rarely been explored in HE-based machine learning studies. An extensive experiment on real-world benchmarking datasets supports the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Byun, Junyoung and Park, Saerom and Choi, Yujin and Lee, Jaewook},
  doi          = {10.1007/s10489-022-04015-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10114-10129},
  shortjournal = {Appl. Intell.},
  title        = {Efficient homomorphic encryption framework for privacy-preserving regression},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot learning based cross-lingual sentiment analysis
for sanskrit text with insufficient labeled data. <em>APIN</em>,
<em>53</em>(9), 10096–10113. (<a
href="https://doi.org/10.1007/s10489-022-04046-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel method for analyzing the sentiments portrayed by Sanskrit text has been proposed. Sanskrit is one of the world’s most ancient languages; however, natural language processing tasks such as machine translation and sentiment analysis have not been explored for it to the full potential because of the unavailability of sufficient labeled data. We solved this issue using a zero-shot learning-based cross-lingual sentiment analysis (CLSA) approach. The CLSA uses the resources from the source language to enhance the sentiment analysis of the target language having insufficient resources. The proposed work translates the text from Sanskrit, a language with insufficient labeled data, to English, with sufficient labeled data for sentiment analysis using a transformer model. A generative adversarial network-based strategy has been proposed to evaluate the maturity of the translations. Then a bidirectional long short-term memory-based model has been implemented to classify the sentiments using the embeddings obtained through translations. The proposed technique has achieved 87.50% accuracy for machine translation and 92.83% accuracy for sentiment classification. Sanskrit-English translations used in this work have been collected through web scraping techniques. In the absence of the ground-truth sentiment class labels, a strategy for evaluating the sentiment scores of the proposed sentiment analysis model has also been presented. A new dataset of Sanskrit text, along with their English translations and sentiment scores, has been constructed.},
  archive      = {J_APIN},
  author       = {Kumar, Puneet and Pathania, Kshitij and Raman, Balasubramanian},
  doi          = {10.1007/s10489-022-04046-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10096-10113},
  shortjournal = {Appl. Intell.},
  title        = {Zero-shot learning based cross-lingual sentiment analysis for sanskrit text with insufficient labeled data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Social decision-making in a large-scale MultiAgent system
considering the influence of empathy. <em>APIN</em>, <em>53</em>(9),
10068–10095. (<a
href="https://doi.org/10.1007/s10489-022-03933-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E mpathy is the ability to spontaneously or purposefully place oneself in another’s situation. Under the continuous effect of empathy, an individual’s preference for things will inevitably be affected by the local and non-local social environment. Inspired by neuropsychology, this paper constructs an extended empathy model to compensate for the shortcomings of previous models in describing the global preference (utility) coupling between individuals, and analyzes how to make efficient decisions based on this model in a large-scale multiagent system. Empathy is abstracted as a random experience process in the form of nonstationary Markov chains, and empathetic utility is defined as the expectation of preference experienced under the corresponding transition probability distribution. By structurally introducing the self-other separation mechanism and energy attenuation mechanism, the model can exhibit social attributes, including absorbency, inhibition, and anisotropy. An extended iterative candidate elimination (EICE) algorithm is designed for the decision problem defined by the proposed model. This algorithm correlates the error upper bound of the objective function with that of the empathy utility to perform the iterative estimation of the candidate strategies. For a polynomial objective function, the EICE under affective empathy can reduce the algorithm complexity from $O\left (n^{x}\right )$ to $O\left (n^{y}\right )$ (1 ≤ y ≤ 2 ≤ x ≤ 3). In terms of application prospects, the model and the corresponding decision algorithm are proved to be not only suitable for human society but also able to match the engineering application scenarios such as human-machine interaction and unmanned aerial vehicle (UAV) formation under specific requirements.},
  archive      = {J_APIN},
  author       = {Chen, Jize and Liu, Bo and Zhang, Dali and Qu, Zhenshen and Wang, Changhong},
  doi          = {10.1007/s10489-022-03933-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10068-10095},
  shortjournal = {Appl. Intell.},
  title        = {Social decision-making in a large-scale MultiAgent system considering the influence of empathy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label-reconstruction-based pseudo-subscore learning for
action quality assessment in sporting events. <em>APIN</em>,
<em>53</em>(9), 10053–10067. (<a
href="https://doi.org/10.1007/s10489-022-03984-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing action quality assessment (AQA) methods provide only an overall quality score for the input video and lack an evaluation of each substage of the movement process; thus, these methods cannot provide detailed feedback for users. Moreover, the existing datasets do not provide labels for substage quality assessment. To address these problems, in this work, a new label-reconstruction-based pseudo-subscore learning (PSL) method is proposed for AQA in sporting events. In the proposed method, the overall score of an action is not only regarded as a quality label but also used as a feature of the training set. A label-reconstruction-based learning algorithm is built to generate pseudo-subscore labels for the training set. Moreover, based on the pseudo-subscore labels and overall score labels, a multi-substage AQA model is fine-tuned from the PSL model to predict the action quality score of each substage and the overall score for an athlete. Several ablation experiments are performed to verify the effectiveness of each module. The experimental results show that our approach achieves state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Zhang, Hong-Bo and Dong, Li-Jia and Lei, Qing and Yang, Li-Jie and Du, Ji-Xiang},
  doi          = {10.1007/s10489-022-03984-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10053-10067},
  shortjournal = {Appl. Intell.},
  title        = {Label-reconstruction-based pseudo-subscore learning for action quality assessment in sporting events},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hiding sensitive frequent itemsets by item removal via
two-level multi-objective optimization. <em>APIN</em>, <em>53</em>(9),
10027–10052. (<a
href="https://doi.org/10.1007/s10489-022-03808-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy Preserving Data Mining (PPDM) is an important research area in data mining, which aims at protecting the privacy during the data mining process so that personal data and sensitive information is not revealed to unauthorized persons. PPDM is a critical task as data often contain sensitive information about individuals that can easily compromise their privacy such as their financial status, political beliefs or medical history. In particular, many algorithms were proposed to hide sensitive frequent itemsets (values that frequently co-occur) in a transaction database. However, a major problem is that those algorithms are based on removing entire transactions (records) instead of single items (values). Hence, many changes are made to databases, which lead to losing useful information. To avoid making too many changes to a database while still preserving privacy, this paper proposes a novel PPDM algorithm called NSGAII4ID for hiding sensitive frequent itemsets by only removing some items from transactions rather than removing whole transactions. To our best knowledge, this is the first paper exploring this approach. Sanitization is treated as a multi-objective optimization problem where the goal is to reduce four side effects, namely hiding failure, missing cost, artificial cost, and database dissimilarity. However, we observe that only three side effects matter in our case (the artificial cost is always zero). Besides, another key difference is that NSGAII4ID sanitizes a database by performing optimization at two levels. At the transaction level, a multi-objective optimization algorithm is applied to find a subset of candidate transactions to be modified. Then, at the item level, NSGAII4ID searches for an optimal subset of items to be removed from each candidate transaction. We show that this problem is exactly the Set cover Problem (SCP) which we solve by using a fast greedy polynomial algorithm. We have conducted extensive experiments on four datasets to compare NSGAII4DT with state-of-the-art PPDM algorithms in terms of runtime, memory cost and total number of removed items during sanitization. The obtained results show that NSGAII4DT achieves a very good balance in minimizing side effects compared with the state-of-the art algorithms.},
  archive      = {J_APIN},
  author       = {Lefkir, Mira and Nouioua, Farid and Fournier-Viger, Philippe},
  doi          = {10.1007/s10489-022-03808-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10027-10052},
  shortjournal = {Appl. Intell.},
  title        = {Hiding sensitive frequent itemsets by item removal via two-level multi-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multi-level framework for anomaly detection in time
series data. <em>APIN</em>, <em>53</em>(9), 10009–10026. (<a
href="https://doi.org/10.1007/s10489-022-04016-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a challenging problem in science and engineering that appeals to numerous scholars. It is of great relevance to detect anomalies and analyze their potential implications. In this study, a multi-level anomaly detection framework with information granules of higher type and higher order is developed based on the principle of justifiable granularity and Fuzzy C-Means (FCM) clustering algorithm, including two different types of approaches, namely abstract level approach (ALA) and detailed level approach (DLA). The ALA approach is implemented at a comparatively abstract level (viz., level-1), in which two distinct types of information granules of order-1 (viz., information granules of type-1 and type-2) are employed for anomaly detection. The DLA approach is formulated and derived from the ALA approach at a more detailed level (viz., level-2), which generates more detailed information granules, namely information granules of order-2, through successive splitting information granules and the FCM clustering algorithm to refine the problem at various levels. Furthermore, a similarity measurement algorithm is designed for anomaly detection utilizing information granules of higher type and higher order. Comprehensive performance indexes are produced to quantify the performance of the proposed framework compared with the methods of two single-level approaches and two multi-level approaches. Synthetic data and several real-world data coming from various areas are engaged to demonstrate and support the superiority of the proposed approaches over other classical methods in terms of detection accuracy and data anomaly resolution.},
  archive      = {J_APIN},
  author       = {Zhou, Yanjun and Ren, Huorong and Zhao, Dan and Li, Zhiwu and Pedrycz, Witold},
  doi          = {10.1007/s10489-022-04016-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {10009-10026},
  shortjournal = {Appl. Intell.},
  title        = {A novel multi-level framework for anomaly detection in time series data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IrisMarkNet: Iris feature watermarking embedding and
extraction network for image copyright protection. <em>APIN</em>,
<em>53</em>(9), 9992–10008. (<a
href="https://doi.org/10.1007/s10489-022-04047-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the urgent need for copyright protection in the digital era, the accuracy of iris recognition technology for authentication must be considered. Ensuring the accuracy of high-precision authentication has tremendous challenges in improving imperceptibility and robustness, especially the longer iris features, as watermarks lead to reduced imperceptibility. A novel digital watermarking method called IrisMarkNet, which embeds the copyright owner’s binary iris features into the cover image based on deep neural networks, is first proposed to protect image copyright. It utilizes a novel pyramid feature fusion module PFF based on a multiscale feature fusion strategy to obtain better imperceptibility and well-enhanced robustness, which performs better than other watermark algorithms adopting single-scale feature fusion. Additionally, for different mini-batches, the noise in the noise layer is randomly selected for adversarial training to advance the robustness of the proposed model. In addition, we suggest utilizing Convolutional Block Attention Module (CBAM) Woo et al (1), which can help to learn better iris features in the decoding stage and propose a novel authenticator to achieve authentication of the image copyright owner. The extensive experimental and comparative results have demonstrated the superior performance of the proposed scheme compared with the state-of-the-art watermark algorithms. Under all experimental distortions, such as JPEG compression, crop attack, Gaussian filter, salt-and-pepper noise, Gaussian noise, and median filter, IrisMarkNet realizes well-improved robustness and imperceptibility along with a good accuracy rate in the authentication of digital images.},
  archive      = {J_APIN},
  author       = {Shen, Wenzhong and Rong, Ji and Liu, Yingfeng and Zhao, Yan},
  doi          = {10.1007/s10489-022-04047-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9992-10008},
  shortjournal = {Appl. Intell.},
  title        = {IrisMarkNet: Iris feature watermarking embedding and extraction network for image copyright protection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SA-net: Scene-aware network for cross-domain stereo
matching. <em>APIN</em>, <em>53</em>(9), 9978–9991. (<a
href="https://doi.org/10.1007/s10489-022-04003-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the recent stereo matching methods based on deep learning achieve unprecedented state-of-the-art performance, the accuracy of these approaches suffers a drastic drop when dealing with environments much different in context from those observed at training time. In this paper, we propose a novel Scene-Aware Network (SA-Net) that integrates scene information to achieve cross-domain stereo matching. Specifically, we design a Scene-Aware Module (SAM) to extract rich scene details, which can make the network with it have better generalization ability between different domain. In order to use rich scene information to perfectly guide shallow features to realize cost aggregation, we introduce a new Multi-element Feature Fusion Strategy (MFFS). Extensive quantitative and qualitative evaluations on different domain illustrate that our SA-Net achieves competitive performance and in particular obtains better ability of domain generalization.},
  archive      = {J_APIN},
  author       = {Chong, Ai-Xin and Yin, Hui and Wan, Jin and Liu, Yan-Ting and Du, Qian-Qian},
  doi          = {10.1007/s10489-022-04003-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9978-9991},
  shortjournal = {Appl. Intell.},
  title        = {SA-net: Scene-aware network for cross-domain stereo matching},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian network parameter learning using constraint-based
data extension method. <em>APIN</em>, <em>53</em>(9), 9958–9977. (<a
href="https://doi.org/10.1007/s10489-022-03941-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are one of the most compelling theoretical models in uncertain knowledge representation and inference. However, many domains are encountering the dilemma of insufficient data. Learning BN parameters using raw data may lead to low learning accuracy. Therefore, this paper seeks to solve the problem via two novel data extension methods. First, a constraint-based nonparametric bootstrap (CNB) method is proposed, which extends the raw data and guides the parameter distribution of the extended data through a constraint-based sample scoring function. The experimental results on 12 BNs show that the extended data can improve the parameter learning accuracy and enhance the existing parameter learning approaches. The CNB is still valid for medium and large networks with relatively large data. When the original data are of inferior quality, the CNB is unattainable to extend it. Then, a constraint-based parametric bootstrap (CPB) method is proposed, creating a new parameter distribution by constraints and the original samples. The experimental results for the missing data demonstrate that the extended data perform better. The CPB is insensitive to the proportion of missing data and remains superior in relatively large data.},
  archive      = {J_APIN},
  author       = {Ru, Xinxin and Gao, Xiaoguang and Wang, Yangyang and Liu, Xiaohan},
  doi          = {10.1007/s10489-022-03941-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9958-9977},
  shortjournal = {Appl. Intell.},
  title        = {Bayesian network parameter learning using constraint-based data extension method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensembling validation indices to estimate the optimal number
of clusters. <em>APIN</em>, <em>53</em>(9), 9933–9957. (<a
href="https://doi.org/10.1007/s10489-022-03939-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unsupervised learning tasks, one of the most significant and challenging aspects is how to estimate the optimal number of clusters (NC) for a particular set of data. Identifying NC in a given dataset is an essential criterion of cluster validity in clustering analysis. The purpose of cluster analysis is to group data points of similar characteristics, which helps determine distributions and correlations of patterns in large datasets. Recently, the availability and diversity of vast data have inspired researchers to identify an optimal NC in such data. In this paper, an ensemble approach is proposed called Ensemble Cluster Validity Index ECVI, to determine the optimal NC based on integrating and optimising several clustering validity indices, namely the Silhouette (Sil) index, the Davies–Bouldin (DB) index, the Calinski-Harabasz (CH) index, and the Gap statistic. The proposed ECVI aims to enhance the selection of the proper NC, which can be used as a measure of a dataset’s partitioning correctness to represent the actual structure of the dataset. The clustering solution (outcome) of the proposed ECVI is used as an input parameter for the k-means clustering algorithm. In other words, the proposed ECVI is concentrated to develop and validate an internal validity method in order to identify a suitable NC. The experimental comparison with the ground-truth labels for given datasets collected from the UCI repository demonstrates that the proposed ECVI outperforms and produces promising outcomes when finding the optimal ECVI in such datasets. The ECVI evaluates the clustering results obtained using a specific algorithm (e.g., k-means or affinity propagation) and identifies the optimal NC for twenty-two UCI datasets. The effectiveness of the proposed ECVI is illustrated by the theoretical analysis and then demonstrated by extensive experiments. ECVI was compared to fifteen recently published and state-of-the-art validity indices, including DB, SIL, CH, Gap, STR, EM with STR, K-means with STR, KL, Hart, Wint, IGP, Dunn, BWC, PBM, and SC indices. The experimental results show that ECVI surpasses all the compared indices in terms of the optimal NC and accuracy rate.},
  archive      = {J_APIN},
  author       = {Sowan, Bilal and Hong, Tzung-Pei and Al-Qerem, Ahmad and Alauthman, Mohammad and Matar, Nasim},
  doi          = {10.1007/s10489-022-03939-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9933-9957},
  shortjournal = {Appl. Intell.},
  title        = {Ensembling validation indices to estimate the optimal number of clusters},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for fault-tolerant workflow
scheduling in cloud environment. <em>APIN</em>, <em>53</em>(9),
9916–9932. (<a
href="https://doi.org/10.1007/s10489-022-03963-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is widely used in various fields, which can provide sufficient computing resources to address users’ demands (workflows) quickly and effectively. However, resource failure is inevitable, and a challenge to optimize the workflow scheduling is to consider the fault tolerance. Most of previous algorithms are based on failure prediction and fault-tolerant strategies, which can cause the time delay and waste of resources. In this paper, combining the above two methods through a deep reinforcement learning framework, an adaptive fault-tolerant workflow scheduling framework called RLFTWS is proposed, aiming to minimize the makespan and resource usage rate. In this framework, the fault-tolerant workflow scheduling is formulated as a markov decision process. Resubmission and replication strategy are as two actions. A heuristic algorithm is designed for the task allocation and execution according to the selected fault-tolerant strategy. And, double deep Q network framework (DDQN) is developed to select the fault-tolerant strategy adaptively for each task under the current environment state, which is not only prediction but also learning in the process of interacting with the environment. Simulation results show that the proposed RLFTWS can efficiently balance the makespan and resource usage rate, and achieve fault tolerance.},
  archive      = {J_APIN},
  author       = {Dong, Tingting and Xue, Fei and Tang, Hengliang and Xiao, Chuangbai},
  doi          = {10.1007/s10489-022-03963-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9916-9932},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning for fault-tolerant workflow scheduling in cloud environment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A neighborhood weighted-based method for the detection of
outliers. <em>APIN</em>, <em>53</em>(9), 9897–9915. (<a
href="https://doi.org/10.1007/s10489-022-03258-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlierdetection is an important research direction in data mining, including fraud detection, activity monitoring, medical research, network intrusion detection, etc. Many outlier detection methods have been proposed; however, most of them are not suitable for complex patterns because they do not extract appropriate neighbor information and cannot estimate the density accurately. Additionally, their performance is not stable and depends heavily on the number of nearest neighbors (k) selected. To overcome the above defects, we proposed a neighborhood weighted-based outlier detection(NWOD) algorithm that can obtain correct detection result in a variety of situations. In our algorithm, the local density of an object is measured by constructing a weighted nearest neighbor graph and quantifying how difficult it is for the object and its nearest neighbors to reach each other. Furthermore, the proposed neighborhood weighted local outlier factor (NWLOF) compares the differences of the neighborhood weighted local density between a given object and the objects in its neighborhood, and then the degree of being an outlier of an object can be judged. The larger the NWLOF of an object is, the more likely it is to be an outlier. In addition, due to our proposed algorithm being based on the concept of a natural stable structure, its performance does not rely on the value of k. Experiments conducted on both synthetic and real-world datasets show the superiority of our algorithm.},
  archive      = {J_APIN},
  author       = {Xiong, Zhong-Yang and Long, Hua and Zhang, Yu-Fang and Wang, Xiao-Xia and Gao, Qin-Qin and Li, Lin-Tao and Zhang, Min},
  doi          = {10.1007/s10489-022-03258-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9897-9915},
  shortjournal = {Appl. Intell.},
  title        = {A neighborhood weighted-based method for the detection of outliers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prevention method of block withholding attack based on
miners’ mining behavior in blockchain. <em>APIN</em>, <em>53</em>(9),
9878–9896. (<a
href="https://doi.org/10.1007/s10489-022-03889-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a prevention method of block withholding attack (PMBWA) based on miners’ mining behavior in blockchain to prevent the block withholding attack. The PMBWA first performs the data pre-processing based on the box chart detection algorithm for data cleaning and preliminary verification. Then the PMBWA uses the behavior reward, punishment mechanism, and credit model to comprehensively evaluate the contribution of miners. The PMBWA proposes a credit level classification algorithm (CLCA) of miners that weighs posterior probability and similarity to detect the malicious miners. Finally, the PMBWA allocates the corresponding income weight for miners of different credit levels. The simulation results show that regardless of how the numbers of blocks and malicious computing power change, the PMBWA can allocate low-income weight to the corresponding malicious computing power, and significantly improve the precision rate and recall rate of malicious computing power detection in the defensive mining pool. The PMBWA can largely reduce the average cumulative income of malicious computing power and improve the average cumulative income of non-malicious computing power. The PMBWA outperforms the state-of-the-art methods such as ICIAS, SRIAS, and IASCM.},
  archive      = {J_APIN},
  author       = {Chen, Hao and Chen, Yourong and Xiong, Zhenyu and Han, Meng and He, Zaobo and Liu, Banteng and Wang, Zhangquan and Ma, Zhenghua},
  doi          = {10.1007/s10489-022-03889-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9878-9896},
  shortjournal = {Appl. Intell.},
  title        = {Prevention method of block withholding attack based on miners’ mining behavior in blockchain},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using a weighted method in interval-valued
decision information systems. <em>APIN</em>, <em>53</em>(9), 9858–9877.
(<a href="https://doi.org/10.1007/s10489-022-03987-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in big data applications have heightened the need for understanding and processing high-dimensional data. It is necessary to extract some excellent features that effect the learning performance in high-dimensional data. Feature selection algorithm based on rough set theory as an important preprocessing method has been widely used in practical applications. Meanwhile, it should be noted that different attributes have different effects on model evaluation. Nevertheless, each feature or attribute has the same degree of importance in the interval-valued information system by using rough set models, ignoring the imbalance between features. Moreover, the monotonic classification effect of interval-valued data is easily affected by noise. For these two issues, we introduce different weights into neighborhood relations and propose a novel approach for feature selection-based weighted neighborhood rough sets for interval-valued information systems in this study. First, weighted neighborhood relations and some important properties are proposed by considering different attribute weights in the interval-valued information system. Then, we construct an interval-valued-based weighted neighborhood rough set (IVWNRS) model to solve the contradiction between the degree of dependency and the classification ability of the attribute subset. Furthermore, a heuristic algorithm is designed according to the degree of dependency to select an attribute subset that has both strong correlation and high dependency. Finally, we compare it with six other representative feature selection algorithms on fifteen public datasets to evaluate the performance of the proposed algorithm. Experimental results on different classifiers show that the IVWNRS algorithm has higher classification performance and is significantly effective.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaoyan and Jiang, Zongying and Xu, Weihua},
  doi          = {10.1007/s10489-022-03987-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9858-9877},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection using a weighted method in interval-valued decision information systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Oriented-tooth recognition using a five-axis
object-detection approach. <em>APIN</em>, <em>53</em>(9), 9846–9857. (<a
href="https://doi.org/10.1007/s10489-022-03544-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray images are essential data sources for checking the condition of the teeth, gums, jaws, and bone structure of the mouth. Tooth recognition is fundamental in image-processing-based diagnoses. In most previous recognition studies, only four-axis-based object-detection models have been considered because they perform normal object detection while the object is resting on a flat surface. However, because the teeth have various orientations, the existing four-axis-based model leads to inaccurate and inefficient recognition results. Thus, in this study, we propose a five-axis-based object-detection model that considers the orientation of the tooth. Based on a tooth-image dataset labeled using the five-axis ground truth, our proposed method processed five-axis annotated data by employing a variant of the faster region-based convolutional neural network. In the experiment, our proposed method outperformed the existing four-axis approach, both qualitatively and quantitatively. The experimental results indicated that the proposed five-axis-based recognition model will be an important basis for a dental-image-based diagnosis.},
  archive      = {J_APIN},
  author       = {Park, Jonghwan and Lee, Younghoon},
  doi          = {10.1007/s10489-022-03544-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9846-9857},
  shortjournal = {Appl. Intell.},
  title        = {Oriented-tooth recognition using a five-axis object-detection approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic clustering of colour images using quantum inspired
meta-heuristic algorithms. <em>APIN</em>, <em>53</em>(9), 9823–9845. (<a
href="https://doi.org/10.1007/s10489-022-03806-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores the effectiveness and robustness of quantum computing by conjoining the principles of quantum computing with the conventional computational paradigm for the automatic clustering of colour images. In order to develop such a computationally efficient algorithm, two population-based meta-heuristic algorithms, viz., Particle Swarm Optimization (PSO) algorithm and Enhanced Particle Swarm Optimization (EPSO) algorithm have been consolidated with the quantum computing framework to yield the Quantum Inspired Particle Swarm Optimization (QIPSO) algorithm and the Quantum Inspired Enhanced Particle Swarm Optimization (QIEPSO) algorithm, respectively. This paper also presents a comparison between the proposed quantum inspired algorithms with their corresponding classical counterparts and also with three other evolutionary algorithms, viz., Artificial Bee Colony (ABC), Differential Evolution (DE) and Covariance Matrix Adaption Evolution Strategies (CMA-ES). In this paper, twenty different sized colour images have been used for conducting the experiments. Among these twenty images, ten are Berkeley images and ten are real life colour images. Three cluster validity indices, viz., PBM, CS-Measure (CSM) and Dunn index (DI) have been used as objective functions for measuring the effectiveness of clustering. In addition, in order to improve the performance of the proposed algorithms, some participating parameters have been adjusted using the Sobol’s sensitivity analysis test. Four segmentation evaluation metrics have been used for quantitative evaluation of the proposed algorithms. The effectiveness and efficiency of the proposed quantum inspired algorithms have been established over their conventional counterparts and the three other competitive algorithms with regards to optimal computational time, convergence rate and robustness.},
  archive      = {J_APIN},
  author       = {Dey, Alokananda and Bhattacharyya, Siddhartha and Dey, Sandip and Platos, Jan and Snasel, Vaclav},
  doi          = {10.1007/s10489-022-03806-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9823-9845},
  shortjournal = {Appl. Intell.},
  title        = {Automatic clustering of colour images using quantum inspired meta-heuristic algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on machine learning in array databases.
<em>APIN</em>, <em>53</em>(9), 9799–9822. (<a
href="https://doi.org/10.1007/s10489-022-03979-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an in-depth survey on the integration of machine learning and array databases. First,machine learning support in modern database management systems is introduced. From straightforward implementations of linear algebra operations in SQL to machine learning capabilities of specialized database managers designed to process specific types of data, a number of different approaches are overviewed. Then, the paper covers the database features already implemented in current machine learning systems. Features such as rewriting, compression, and caching allow users to implement more efficient machine learning applications. The underlying linear algebra computations in some of the most used machine learning algorithms are studied in order to determine which linear algebra operations should be efficiently implemented by array databases. An exhaustive overview of array data and relevant array database managers is also provided. Those database features that have been proven of special importance for efficient execution of machine learning algorithms are analyzed in detail for each relevant array database management system. Finally, current state of array databases capabilities for machine learning implementation is shown through two example implementations in Rasdaman and SciDB.},
  archive      = {J_APIN},
  author       = {Villarroya, Sebastián and Baumann, Peter},
  doi          = {10.1007/s10489-022-03979-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9799-9822},
  shortjournal = {Appl. Intell.},
  title        = {A survey on machine learning in array databases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel video-based pedestrian re-identification method of
sequence feature distribution similarity measurement combined with deep
learning. <em>APIN</em>, <em>53</em>(9), 9779–9798. (<a
href="https://doi.org/10.1007/s10489-022-04021-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel video-sequence-based pedestrian re-identification method using the feature distribution similarity measurement between pedestrian video sequences (PRI-FDSM). We use the multiple granularity network combined with generative adversarial skew correction to extract and generate the feature point sets of the corrected pedestrian sequences. Then, we construct the corresponding probability function estimators for each pedestrian sequence using a radial basis function neural network to describe the feature distributions of specific sequences. Finally, we measure the similarity between the feature distributions of sequences to obtain re-identification results. The proposed method uses the distribution similarity measurement of the feature point sets of different sequences to make full use of all the image information of the specific pedestrian in a sequence. Thus, our method can mitigate the problem of insufficient use of the details of some images in a sequence, which commonly occurs in existing fusion feature point measurement methods. Besides, we correct the input skewed pedestrian sequences and achieve posture unification for the input sequences, which effectively mitigates the posture skewing problem of the photographed pedestrians in real-world surveillance scenes. We also build a dataset that more accurately represents the real-world surveillance scenes that contain pedestrian sequences with skewed postures. The results of the ablation experiment on iLIDS-VID and this dataset demonstrate the effectiveness of the proposed distribution-based similarity measurement method. We also compare the performance of the proposed method and several state-of-the-art methods on our dataset. Experimental results show that the indices of our method are all higher than those of the existing methods, and its mAP, Rank-1 and Rank-5 surpass the second best by 3.7%, 1.3% and 1.7% respectively.},
  archive      = {J_APIN},
  author       = {Pei, Jihong and Zhang, Jichen and Ni, Ziyang and Zhao, Yang},
  doi          = {10.1007/s10489-022-04021-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9779-9798},
  shortjournal = {Appl. Intell.},
  title        = {A novel video-based pedestrian re-identification method of sequence feature distribution similarity measurement combined with deep learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-guided multi-scale human skeleton action
recognition. <em>APIN</em>, <em>53</em>(9), 9763–9778. (<a
href="https://doi.org/10.1007/s10489-022-03968-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of depth sensors and pose estimation algorithms, action recognition technology based on the human skeleton has attracted wide attention from researchers. The human skeleton action recognition methods embedded with semantic information have excellent performance in terms of computational cost and recognition results by extracting spatio-temporal features of all joints, nevertheless, they will cause information redundancy and are of limitations in extracting long-term context spatio-temporal features. In this work, we propose a semantic-guided multi-scale neural network (SGMSN) method for skeleton action recognition. For spatial modeling, the key insight of our approach is to achieve multi-scale graph convolution by manipulating the data level (without adding additional computational cost). For temporal modeling, we build the multi-scale temporal convolutional network with a multi-scale receptive field across the temporal dimensions. Several experiments have been carried out on two publicly available large-scale skeleton datasets, NTU RGB+D and NTU RGB+D 120. On the NTU RGB+D datasets, the accuracy is 90.1% (cross-subject) and 95.8% (cross-view) respectively. The experimental results show that the performance of the proposed network architecture is superior to most current state-of-the-art action recognition models.},
  archive      = {J_APIN},
  author       = {Qi, Yongfeng and Hu, Jinlin and Zhuang, Liqiang and Pei, Xiaoxu},
  doi          = {10.1007/s10489-022-03968-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {9},
  pages        = {9763-9778},
  shortjournal = {Appl. Intell.},
  title        = {Semantic-guided multi-scale human skeleton action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image classification based on self-distillation.
<em>APIN</em>, <em>53</em>(8), 9396–9408. (<a
href="https://doi.org/10.1007/s10489-022-04008-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been widely used in various application scenarios. To extend the application to some areas where accuracy is critical, researchers have been investigating methods to improve accuracy using deeper or broader network structures, which creates exponential growth in computation and storage costs and delays in response time. In this paper, we propose a self-distillation image classification algorithm that significantly improves performance while decreasing training costs. In traditional self-distillation, the student model needs to improve its ability to acquire global information and focus on key features due to the lack of guidance from the teacher model. For this reason, we improved the traditional self-distillation algorithm by using a positional attention module and a residual block with attention. Experimental results show that the method achieves better performance compared with traditional knowledge distillation methods and attention networks.},
  archive      = {J_APIN},
  author       = {Li, Yuting and Qing, Linbo and He, Xiaohai and Chen, Honggang and Liu, Qiang},
  doi          = {10.1007/s10489-022-04008-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9396-9408},
  shortjournal = {Appl. Intell.},
  title        = {Image classification based on self-distillation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive evolutionary algorithm with coordinated
selection strategies for many-objective optimization. <em>APIN</em>,
<em>53</em>(8), 9368–9395. (<a
href="https://doi.org/10.1007/s10489-022-03982-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Striking a balance between convergence and diversity matters considerably for evolutionary algorithms in solving many-objective optimization problems. The performance of these algorithms depends on their capability of obtaining a set of uniformly distributed solutions as close to the Pareto optimal front as possible. However, most existing evolutionary algorithms encounter challenges in solving many-objective optimization problems. Thus, in this paper, an adaptive many-objective evolutionary algorithm with coordinated selection strategies, labeled ACS-MOEA, is proposed to balance the convergence and diversity. The coordinated selection strategies include three selection strategies, i.e., the selection based on shifted-dominated distance, the selection based on objective vector angle, and the selection based on Non-Euclidean geometry distance. The first is used in the mating selection process to select high-quality parents for the generation of good offspring. Both the second and the third selection strategies are employed in the environmental selection process to delete poor solutions one by one for preserving the elitist solutions of the next generation. The performance of ACS-MOEA is verified by comparing it with six state-of-the-art algorithms on several well-known benchmark test suites with up to 10 objectives. Experimental results have fully demonstrated the competitiveness of ACS-MOEA in balancing convergence and diversity. Moreover, the proposed ACS-MOEA has also been verified to be effective in solving constrained many-objective optimization problems.},
  archive      = {J_APIN},
  author       = {Gu, Qinghua and Luo, Jiale and Li, Xuexian and Lu, Caiwu},
  doi          = {10.1007/s10489-022-03982-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9368-9395},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive evolutionary algorithm with coordinated selection strategies for many-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A symmetric neural cryptographic key generation scheme for
iot security. <em>APIN</em>, <em>53</em>(8), 9344–9367. (<a
href="https://doi.org/10.1007/s10489-022-03904-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of CMOS technology to generate neural session keys is presented in this research for incorporation with the Internet of Things (IoT) to improve security. Emerging technology advancements in the IoT era have enabled better tactics to exacerbate energy efficiency and security difficulties. Current safety solutions do not effectively address the security of IoT. Regarding IoT integration, a tiny logic area ASIC design of a re-keying enabled Triple Layer Vector-Valued Neural Network (TLVVNN) is presented utilizing CMOS designs with measurements of 65 and 130 nanometers. There hasn’t been much study into optimizing the value of neural weights for faster neural synchronization. Harris’ Hawks is used in this instance to optimize the neural network’s weight vector for faster coordination. Once this process is completed, the synchronized weight becomes the session key. This method offers several advantages, namely (1) production of the session key by mutual neural synchronization over the public channel is one of the advantages of this technology. (2) It facilitates Hawks-based neural weight vector optimization for faster neural synchronization across public channels. (3) As per behavioral modeling, the synchronization duration might be reduced from 1.25 ms to less than 0.7 ms for a 20% weight imbalance in the re-keying phase. (4) Geometric, brute force, and majority attacks are all prohibited. Experiments to validate the suggested method’s functionality are carried out, and the results show that the proposed approach outperforms current similar techniques in terms of efficiency.},
  archive      = {J_APIN},
  author       = {Sarkar, Arindam},
  doi          = {10.1007/s10489-022-03904-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9344-9367},
  shortjournal = {Appl. Intell.},
  title        = {A symmetric neural cryptographic key generation scheme for iot security},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A many-objective evolutionary algorithm based on corner
solution and cosine distance. <em>APIN</em>, <em>53</em>(8), 9321–9343.
(<a href="https://doi.org/10.1007/s10489-022-03883-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most many-objective optimization algorithms focus on balancing convergence and diversity, instead of attaching importance to the contribution of the boundary solution. The boundary solution is beneficial for enhancing the PF coverage; therefore, we propose a many-objective evolutionary algorithm based on the corner solution and cosine distance (MaOEA-CSCD) to balance convergence and diversity, as well as protect the PF boundary. We set a corner solution archive to store the corner solutions and apply these corner solutions and cosine distance in the mating strategy to improve the quality of the parents to generate high-quality offspring. In environmental selection, a greedy strategy is applied to select the corner solution and the solution with better convergence to overcome the insufficient selection pressure while protecting the PF boundary and guaranteeing the search space. Then, a selection–deletion strategy is used to balance convergence and diversity, it first selects solutions based on the maximum cosine distance, and then considers replacement solutions based on convergence. The comparison of MaOEA-CSCD with six algorithms on 25 benchmark and three real-world optimization problems shows that it is competitive.},
  archive      = {J_APIN},
  author       = {Wang, Mengzhen and Ge, Fangzhen and Chen, Debao and Liu, Huaiyu},
  doi          = {10.1007/s10489-022-03883-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9321-9343},
  shortjournal = {Appl. Intell.},
  title        = {A many-objective evolutionary algorithm based on corner solution and cosine distance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reminding the incremental language model via data-free
self-distillation. <em>APIN</em>, <em>53</em>(8), 9298–9320. (<a
href="https://doi.org/10.1007/s10489-022-03678-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental language learning, which involves retrieving pseudo-data from previous tasks, can alleviate catastrophic forgetting. However, previous methods require a large amount of pseudo-data to approach the performance of multitask learning, and the performance decreases dramatically when there is significantly less pseudo-data than new task data. This decrease occurs because the pseudo-data are learned inefficiently and deviate from the real data. To address these issues, we propose reminding the incremental language model via data-free self-distillation (DFSD), which includes 1) self-distillation based on the Earth mover’s distance (SD-EMD) and 2) hidden data augmentation (HDA). SD-EMD can increase the efficiency of the model by adaptively estimating the knowledge distribution in all GPT-2 layers and effectively transferring data from the teacher model to the student model via adaptive self-multilayer-to-multilayer mapping. HDA can reduce deviations by decomposing the generation process via data augmentation and bootstrapping. Our experiments on decaNLP and text classification tasks with low pseudo-data sampling ratios reveal that the DFSD model outperforms previous state-of-the-art incremental methods. The advantages of DFSD become more apparent when there is less pseudo-data and larger deviations.},
  archive      = {J_APIN},
  author       = {Wang, Han and Fu, Ruiliu and Li, Chengzhang and Zhang, Xuejun and Zhou, Jun and Bai, Xing and Yan, Yonghong and Zhao, Qingwei},
  doi          = {10.1007/s10489-022-03678-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9298-9320},
  shortjournal = {Appl. Intell.},
  title        = {Reminding the incremental language model via data-free self-distillation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MACC net: Multi-task attention crowd counting network.
<em>APIN</em>, <em>53</em>(8), 9285–9297. (<a
href="https://doi.org/10.1007/s10489-022-03954-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting and Crowd density map estimation face several challenges, including occlusions, non-uniform density, and intra-scene scale and perspective variations. Significant progress has been made in the development of most crowd counting approaches in recent years, especially with the emergence of deep learning and massive crowd datasets. The purpose of this work is to address the problem of crowd density estimatation in both sparse and crowded situations. In this paper, we propose a multi-task attention based crowd counting network (MACC Net), which consists of three contributions: 1) density level classification, which offers the global contextual information for the density estimation network; 2) density map estimation; and 3) segmentation guided attention to filter out the background noise from the foreground features. The proposed MACC Net is evaluated on four popular datasets including ShanghaiTech, UCF-CC-50, UCF-QRNF, and a recently launched dataset HaCrowd. The MACC Net achieves the state of the art in estimation when applied to HaCrowd and UCF-CC-50, while on the others, it obtains competitive results.},
  archive      = {J_APIN},
  author       = {Aldhaheri, Sahar and Alotaibi, Reem and Alzahrani, Bandar and Hadi, Anas and Mahmood, Arif and Alhothali, Areej and Barnawi, Ahmed},
  doi          = {10.1007/s10489-022-03954-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9285-9297},
  shortjournal = {Appl. Intell.},
  title        = {MACC net: Multi-task attention crowd counting network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust and singular point independent fingerprint shell.
<em>APIN</em>, <em>53</em>(8), 9270–9284. (<a
href="https://doi.org/10.1007/s10489-022-04038-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication systems have evolved immensely ever since the use of biometrics in the field of security has started. Nonetheless, extensive usage of biometric systems has also resulted in growing fear of identity theft as storing biometric user templates in databases opens up severe security concerns. Consequently, the scope of biometrics invariably depends on the ability of the system to manifest security and robustness against biometric identity theft along with achieving acceptable recognition performance. This paper proposes a user template protection technique to secure a fingerprint based user template used in a biometric authentication system. It is based on the fingerprint shell, which computes alignment-free, singular point independent, and non-invertible user templates. The secure user template generated by the proposed technique comprises fingerprint shell curves computed using the transformed pair-polar structure of the minutia points. The proposed technique has been evaluated on five fingerprint databases of Fingerprint Verification Competitions, namely, FVC2002, FVC2004, and FVC2006. The effectiveness of the technique is analyzed in terms of revocability, diversity, security, and recognition performance. The comparison of results with that of the existing techniques demonstrates the robustness and efficacy of the proposed technique.},
  archive      = {J_APIN},
  author       = {Baghel, Vivek Singh and Ali, Afeeza and Prakash, Surya},
  doi          = {10.1007/s10489-022-04038-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9270-9284},
  shortjournal = {Appl. Intell.},
  title        = {A robust and singular point independent fingerprint shell},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transform networks for cooperative multi-agent deep
reinforcement learning. <em>APIN</em>, <em>53</em>(8), 9261–9269. (<a
href="https://doi.org/10.1007/s10489-022-03924-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Multi-agent Deep Reinforcement Learning Algorithm has been developing rapidly, in which the value method-based algorithm plays an important role (such as Monotonic Value Function Factorisation (QMIX) and Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning (QTRAN)). In spite of the fact, the performance of current value-based multi-agent algorithm under complex scene still can be further improved. In value function-based model, a mixing network is usually used to mix the local action value of each agent to get joint action value when the partial observability will cause the problem of misalignment and unsatisfying mixing results. This paper proposes a multi-agent model called Transform Networks that transform the individual local action-value function gotten by agent network to individual global action-value function, which will avoid the problem of misalignment caused by partial observability when the individual action value is mixed, and the joint action value can represent the cooperative conditions of all agents well. Using the StarCraft Multi-Agent Challenge (SMAC) as the experimental platform, the comparison of the performance of algorithms on five different maps proved that the proposed method has better effect than the current most advanced baseline algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Hongbin and Xie, Xiaodong and Zhou, Lianke},
  doi          = {10.1007/s10489-022-03924-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9261-9269},
  shortjournal = {Appl. Intell.},
  title        = {Transform networks for cooperative multi-agent deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging semantic property for temporal knowledge graph
completion. <em>APIN</em>, <em>53</em>(8), 9247–9260. (<a
href="https://doi.org/10.1007/s10489-022-03981-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graphs (TKGs) have become an effective tool for numerous intelligent applications. Due to their incompleteness, TKG embedding methods have been proposed to infer the missing temporal facts, and work by learning latent representations for entities, relations and timestamps. However, these methods primarily focus on measuring the plausibility of the whole temporal fact, and ignore the semantic property that there exists a bias between any relation and its involved entities at various time steps. In this paper, we present a novel temporal knowledge graph completion framework, which imposes relational constraints to preserve the semantic property implied in TKGs. Specifically, we borrow ideas from two well-known transformation functions, i.e., tensor decomposition and hyperplane projection, and design relational constraints associated with timestamps. We then adopt suitable regularization schemes to accommodate specific relational constraints, which combat overfitting and enforce temporal smoothness. Experimental studies indicate the superiority of our proposal compared to existing baselines on the task of temporal knowledge graph completion.},
  archive      = {J_APIN},
  author       = {Li, Mingda and Sun, Zhengya and Zhang, Wensheng and Liu, Wei},
  doi          = {10.1007/s10489-022-03981-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9247-9260},
  shortjournal = {Appl. Intell.},
  title        = {Leveraging semantic property for temporal knowledge graph completion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced VAEGAN: A zero-shot image classification method.
<em>APIN</em>, <em>53</em>(8), 9235–9246. (<a
href="https://doi.org/10.1007/s10489-022-03869-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to classify samples of unseen categories for which no training data is available. At present, the VAEGAN framework which combines Generative Adversarial Networks (GAN) with Variational Auto-Encoder (VAE) has achieved good performance in zero-shot image classification. Based on the VAEGAN, we propose a new zero-shot image classification method named Enhanced VAEGAN (E-VAEGAN). Firstly, we design a feature alignment module to align visual features and attribute features. Then, the aligned features are fused with the hidden layer features of the encoder to improve output features of the encoder. Secondly, the triplet loss is applied during the encoder training, which further increases the discriminability of features. Finally, the hidden layer features of the discriminator are input into a transform module and then fed back to the generator, which improves the quality of the generated fake samples. The originality of this paper is that we design a new E-VAEGAN which employs the feature alignment module, triplet loss and transform module to reduce the ambiguity between categories and make the generated fake features similar to the real features. Experiments show that our method outperforms the compared methods on five zero-shot learning benchmarks.},
  archive      = {J_APIN},
  author       = {Ding, Bo and Fan, Yufei and He, Yongjun and Zhao, Jing},
  doi          = {10.1007/s10489-022-03869-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9235-9246},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced VAEGAN: A zero-shot image classification method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and adaptive incentive selection for crowdsourcing
contests. <em>APIN</em>, <em>53</em>(8), 9204–9234. (<a
href="https://doi.org/10.1007/s10489-022-03593-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of crowdsourcing projects relies critically on motivating a crowd to contribute. One particularly effective method for incentivising participants to perform tasks is to run contests where participants compete against each other for rewards. However, there are numerous ways to implement such contests in specific projects, that vary in how performance is evaluated, how participants are rewarded, and the sizes of the prizes. Also, the best way to implement contests in a particular project is still an open challenge, as the effectiveness of each contest implementation (henceforth, incentive) is unknown in advance. Hence, in a crowdsourcing project, a practical approach to maximise the overall utility of the requester (which can be measured by the total number of completed tasks or the quality of the task submissions) is to choose a set of incentives suggested by previous studies from the literature or from the requester’s experience. Then, an effective mechanism can be applied to automatically select appropriate incentives from this set over different time intervals so as to maximise the cumulative utility within a given financial budget and a time limit. To this end, we present a novel approach to this incentive selection problem. Specifically, we formalise it as an online decision making problem, where each action corresponds to offering a specific incentive. After that, we detail and evaluate a novel algorithm, HAIS, to solve the incentive selection problem efficiently and adaptively. In theory, in the case that all the estimates in HAIS (except the estimates of the effectiveness of each incentive) are correct, we show that the algorithm achieves the regret bound of $\mathcal {O}(\sqrt {B/c})$ , where B denotes the financial budget and c is the average cost of the incentives. In experiments, the performance of HAIS is about 93% (up to 98%) of the optimal solution and about 9% (up to 40%) better than state-of-the-art algorithms in a broad range of settings, which vary in budget sizes, time limits, numbers of incentives, values of the standard deviation of the incentives’ utilities, and group sizes of the contests (i.e., the numbers of participants in a contest).},
  archive      = {J_APIN},
  author       = {Truong, Nhat Van-Quoc and Dinh, Le Cong and Stein, Sebastian and Tran-Thanh, Long and Jennings, Nicholas R.},
  doi          = {10.1007/s10489-022-03593-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9204-9234},
  shortjournal = {Appl. Intell.},
  title        = {Efficient and adaptive incentive selection for crowdsourcing contests},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning based dislocation detection method for
cylindrical silicon growth process. <em>APIN</em>, <em>53</em>(8),
9188–9203. (<a
href="https://doi.org/10.1007/s10489-022-03800-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of preparing heavily doped silicon crystals for power electronics and solar power by the Czochralski(CZ) method, dislocations occur due to the presence of impurities and other factors. The presence of dislocations can affect the quality of single-crystal silicon crystalline columns. Therefore, we proposed the improved Yolov4-tiny model (Yolo-SPI) for detecting the occurrence of dislocations. For resolving the problem of low detection accuracy of the original model, we improve the neck part of the model by referring to the structure of the Path Aggregation Network (PAnet). Furthermore, we propose a feature enhancement module to improve the feature extraction ability of the model and introduce depthwise separable convolution to reduce the parameters. We produced the single-crystal silicon habit line dataset by using the industrial camera. The experimental results on our dataset show that the Yolo-SPI model outperforms Ghostnet-Yolov4, Mobilenetv3-Yolov4, EfficientDet-v0, and Nanodet. The Yolo-SPI model improves the Precision from 73.28% to 98.01% compared to the original Yolov4-tiny model and the Recall is also improved to 97.51%. At the same time, the number of parameters in the Yolo-SPI model is decreased from 5.87M to 2.05M compared to the Yolov4-tiny model. Our model also beats other models in the speed of detection, reaching 133FPS. In practical applications, our model achieves higher accuracy and faster detection speed.},
  archive      = {J_APIN},
  author       = {Yuting, She and Hongxing, Li},
  doi          = {10.1007/s10489-022-03800-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9188-9203},
  shortjournal = {Appl. Intell.},
  title        = {A deep learning based dislocation detection method for cylindrical silicon growth process},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid adversarial training for deep learning model and
denoising network resistant to adversarial examples. <em>APIN</em>,
<em>53</em>(8), 9174–9187. (<a
href="https://doi.org/10.1007/s10489-022-03991-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial attacks that generate adversarial examples by adding small perturbations to the clean images. To combat adversarial attacks, the two main defense methods used are denoising and adversarial training. However, both methods result in the DNN having lower classification accuracy for clean images than conventionally trained DNN models. To overcome this problem, we propose a hybrid adversarial training (HAT) method that trains the denoising network and DNN model simultaneously. The proposed HAT method uses both clean images and adversarial examples denoised by the denoising network and non-denoised clean images and adversarial examples to train the DNN model. The results of experiments conducted on the MNIST, CIFAR-10, CIFAR-100, and GTSRB datasets show that the HAT method results in a higher classification accuracy than both conventional training with a denoising network and previous adversarial training methods. They also indicate that training with the HAT method results in average improvements in robustness of 0.84%, 27.33%, 28.99%, and 17.61% against adversarial attacks compared with several state-of-the-art adversarial training methods on the MNIST, CIFAR-10, CIFAR-100, and GTSRB datasets, respectively. Thus, the proposed HAT method results in improved robustness for DNNs against a wider range of adversarial attacks.},
  archive      = {J_APIN},
  author       = {Ryu, Gwonsang and Choi, Daeseon},
  doi          = {10.1007/s10489-022-03991-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9174-9187},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid adversarial training for deep learning model and denoising network resistant to adversarial examples},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Balancing the trade-off between cost and reliability for
wireless sensor networks: A multi-objective optimized deployment method.
<em>APIN</em>, <em>53</em>(8), 9148–9173. (<a
href="https://doi.org/10.1007/s10489-022-03875-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of the sensor nodes (SNs) always plays a decisive role in the system performance of wireless sensor networks (WSNs). In this work, we propose an optimal deployment method for practical heterogeneous WSNs which gives a deep insight into the trade-off between the reliability and deployment cost. Specifically, this work aims to provide the optimal deployment of SNs to maximize the coverage degree and connection degree, and meanwhile minimize the overall deployment cost. In addition, this work fully considers the heterogeneity of SNs (i.e. differentiated sensing range and deployment cost) and three-dimensional (3-D) deployment scenarios. This is a multi-objective optimization problem, non-convex, multimodal and NP-hard. To solve it, we develop a novel swarm-based multi-objective optimization algorithm, known as the competitive multi-objective marine predators algorithm (CMOMPA) whose performance is verified by comprehensive comparative experiments with ten other state-of-the-art multi-objective optimization algorithms. The computational results demonstrate that CMOMPA is superior to others in terms of convergence and accuracy and shows excellent performance on multimodal multi-objective optimization problems. Sufficient simulations are also conducted to evaluate the effectiveness of the CMOMPA based optimal SNs deployment method. The results show that the optimized deployment can balance the trade-off among deployment cost, sensing reliability and network reliability. The source code is available on https://github.com/iNet-WZU/CMOMPA .},
  archive      = {J_APIN},
  author       = {Chen, Long and Xu, Yingying and Xu, Fangyi and Hu, Qian and Tang, Zhenzhou},
  doi          = {10.1007/s10489-022-03875-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9148-9173},
  shortjournal = {Appl. Intell.},
  title        = {Balancing the trade-off between cost and reliability for wireless sensor networks: A multi-objective optimized deployment method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-interpretable module for deep image classification on
small data. <em>APIN</em>, <em>53</em>(8), 9115–9147. (<a
href="https://doi.org/10.1007/s10489-022-03886-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are the driving force of the recent explosion of machine learning applications in everyday life. However, they usually require a lot of training data to work well, and they act as black-boxes, making predictions without any explanation about them. This paper presents Memory Wrap, a module (i.e, a set of layers) that can be added to deep learning models to improve their performance and interpretability in settings where few data are available. Memory Wrap adopts a sparse content-attention mechanism between the input and some memories of past training samples. We show that adding Memory Wrap to standard deep neural networks improves their performance when they learn from a limited set of data, and allows them to reach comparable performance when they learn from the full dataset. We discuss how the analysis of its structure and content-attention weights helps to get insights about its decision process and makes their predictions more interpretable, compared to the same networks without Memory Wrap. We test our approach on image classification tasks using several networks on three different datasets, namely CIFAR10, SVHN, and CINIC10.},
  archive      = {J_APIN},
  author       = {La Rosa, Biagio and Capobianco, Roberto and Nardi, Daniele},
  doi          = {10.1007/s10489-022-03886-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9115-9147},
  shortjournal = {Appl. Intell.},
  title        = {A self-interpretable module for deep image classification on small data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the robustness of machine reading comprehension
via contrastive learning. <em>APIN</em>, <em>53</em>(8), 9103–9114. (<a
href="https://doi.org/10.1007/s10489-022-03947-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained language models achieve high performance on machine reading comprehension task, but these models lack robustness and are vulnerable to adversarial samples. Most of the current methods for improving model robustness are based on data enrichment. However, these methods do not solve the problem of poor context representation of the machine reading comprehension model. We find that context representation plays a key role in the robustness of the machine reading comprehension model, dense context representation space results in poor model robustness. To deal with this, we propose a Multi-task machine Reading Comprehension learning framework via Contrastive Learning. Its main idea is to improve the context representation space encoded by the machine reading comprehension models through contrastive learning. This special contrastive learning we proposed called Contrastive Learning in Context Representation Space(CLCRS). CLCRS samples sentences containing context information from the context as positive and negative samples, expanding the distance between the answer sentence and other sentences in the context. Therefore, the context representation space of the machine reading comprehension model has been expanded. The model can better distinguish between sentence containing correct answers and misleading sentence. Thus, the robustness of the model is improved. Experiment results on adversarial datasets show that our method exceeds the comparison models and achieves state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Feng, Jianzhou and Sun, Jiawei and Shao, Di and Cui, Jinman},
  doi          = {10.1007/s10489-022-03947-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9103-9114},
  shortjournal = {Appl. Intell.},
  title        = {Improving the robustness of machine reading comprehension via contrastive learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized thresholding algorithm with dimension
reduction for device-free localization in IoT. <em>APIN</em>,
<em>53</em>(8), 9089–9102. (<a
href="https://doi.org/10.1007/s10489-022-03925-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location information is one of the most important factors for many location-based services (LBSs) in the Internet of Things (IoT). Device-free localization (DFL) has received more attention as it achieves localization without attaching any electronic device to the target. DFL can be applied to many special scenarios, such as monitoring the elderly living alone, health care of inpatients, and emergency rescue. In applications based on traditional localization methods, the numerous receive signal strength (RSS) measurements are collected from wireless sensor networks (WSNs) comprised of sensor pairs to construct the atoms of learning dictionaries. With recovery algorithms, solutions can be obtained from undetermined equations using learning dictionaries, which can be mapped to the position index of the target to estimate the accurate coordinates. However, the numerous RSS data produced by WSN sensor generate high-dimensional learning dictionaries that cost the sparse recovery algorithm more iterative computation time to derive the target location and more space for data storage, thus affecting the real-time DFL performance. In this paper, we propose a data dimension reduction method based on the generalized iterative thresholding algorithm for DFL. Firstly, we reduced the column and row dimensions of the dictionary, respectively, via principal components analysis (PCA). Then, the dimension of the observed vector was reduced correspondingly. Finally, the new underdetermined equation was solved via sparse coding with an iterative p-thresholding algorithm in signal subspace, and the target location was estimated accurately. Experiments on public datasets demonstrated that the proposed method outperforms the current alternatives by improving the computation efficiency of DFL systems and taking less time to locate the target, implying its good applicability to IoT scenarios with high real-time requirements.},
  archive      = {J_APIN},
  author       = {Cheng, Qin and Zhang, Linghua and Xue, Bo and Shu, Feng and Wang, Xu},
  doi          = {10.1007/s10489-022-03925-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9089-9102},
  shortjournal = {Appl. Intell.},
  title        = {A generalized thresholding algorithm with dimension reduction for device-free localization in IoT},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advanced environmental adaptation method. <em>APIN</em>,
<em>53</em>(8), 9068–9088. (<a
href="https://doi.org/10.1007/s10489-022-03923-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many nature-inspired algorithms have been designed to solve optimization problems by combining randomization with exploitation and exploration. Randomization plays a vital role in defining the convergence rate and performance of the algorithm. However, excessive use of randomization may adversely affect the performance of the algorithm. Therefore, a proper balance in randomization, exploitation, and exploration is required to improve the convergence rate of the algorithm. One of the methods to solve the optimization problems is Environmental Adaptation Method. It is a randomized algorithm that works on the theory of adaptive learning. It was followed by an enhanced version named Improved Environmental Adaptation Method. Both of these algorithms used binary encoding to represent the solutions. Since binary encoding requires extra efforts to convert a binary solution to a real solution, a real parameter version of the algorithm will be a good alternative to solve problems. In this paper, we present a new real parameter algorithm named Advanced Environmental Adaptation Method with a novel approach to balance randomization, exploitation, and exploration. This is achieved using operators that make it efficient to search for optimal global solutions. We compare the performance of this new algorithm with other state-of-the-art algorithms. The results show the superiority of the proposed algorithm over existing algorithms. We also demonstrate the effectiveness of the proposed algorithm for real-life problems by applying it to the salient object detection problem, which is an emerging problem in computer vision.},
  archive      = {J_APIN},
  author       = {Mishra, K. K. and Singh, Navjot and Punhani, Akash and Bhatia, Sanjiv},
  doi          = {10.1007/s10489-022-03923-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9068-9088},
  shortjournal = {Appl. Intell.},
  title        = {Advanced environmental adaptation method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image hashing retrieval based on generative adversarial
networks. <em>APIN</em>, <em>53</em>(8), 9056–9067. (<a
href="https://doi.org/10.1007/s10489-022-03970-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the current problem of low retrieval accuracy in deep hashing image retrieval, an improved generative adversarial network (GAN)-based image hashing retrieval method using supervised contrast learning (SconGAN) is proposed. The method augments training samples by introducing dense residual blocks and content fidelity items into the generative network to synthesize more diverse images with higher quality; moreover, it enhances the feature discrimination ability of the discriminative network by introducing pyramidal convolution and supervised contrast learning. Meanwhile, it improves the hashing code generation quality by introducing new pairwise similarity loss, semantic retention loss and quantization loss into the hashing network. In addition, discriminative networks and hashing networks share the core network structure to reduce resource consumption and improve training efficiency. Comprehensive experiments on the CIFAR-10 and NUS-WIDE benchmark datasets show that the proposed method greatly outperforms the comparison methods and obtains the best mean average precision (MAP).},
  archive      = {J_APIN},
  author       = {Lei, Lei and Guo, Dongen and Shen, Zhen and Wu, Zechen},
  doi          = {10.1007/s10489-022-03970-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9056-9067},
  shortjournal = {Appl. Intell.},
  title        = {Image hashing retrieval based on generative adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modal complementary fusion network for RGB-t salient object
detection. <em>APIN</em>, <em>53</em>(8), 9038–9055. (<a
href="https://doi.org/10.1007/s10489-022-03950-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T salient object detection (SOD) combines thermal infrared and RGB images to overcome the light sensitivity of RGB images in low-light conditions. However, the quality of RGB-T images could be unreliable under complex imaging scenarios, and direct fusion of these low-quality images will lead to sub-optimal detection results. In this paper, we propose a novel Modal Complementary Fusion Network (MCFNet) to alleviate the contamination effect of low-quality images from both global and local perspectives. Specifically, we design a modal reweight module (MRM) to evaluate the global quality of images and adaptively reweight RGB-T features by explicitly modelling interdependencies between RGB and thermal images. Furthermore, we propose a spatial complementary fusion module (SCFM) to explore the complementary local regions between RGB-T images and selectively fuse multi-modal features. Finally, multi-scale features are fused to obtain the salient detection result. Experiments on three RGB-T benchmark datasets demonstrate that our MCFNet achieved outstanding performance compared with the latest state-of-the-art methods. We have also achieved competitive results in RGB-D SOD tasks, which proves the generalization of our method. The source code is released at https://github.com/dotaball/MCFNet .},
  archive      = {J_APIN},
  author       = {Ma, Shuai and Song, Kechen and Dong, Hongwen and Tian, Hongkun and Yan, Yunhui},
  doi          = {10.1007/s10489-022-03950-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9038-9055},
  shortjournal = {Appl. Intell.},
  title        = {Modal complementary fusion network for RGB-T salient object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient bi-level multi objective approach for
budget-constrained dynamic bag-of-tasks scheduling problem in
heterogeneous multi-cloud environment. <em>APIN</em>, <em>53</em>(8),
9009–9037. (<a
href="https://doi.org/10.1007/s10489-022-03942-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bag-of-Tasks is a well-known model that processes big-data applications supporting embarrassingly parallel jobs with independent tasks. Scheduling Bag-of-Tasks in a dynamic multi-cloud environment is an NP-hard problem that has attracted a lot of attention in the last years. Such a problem can be modeled using a bi-level optimization framework due to its hierarchical scheme. Motivated by this issue, in this paper, an efficient bi-level multi-follower algorithm, based on hybrid metaheuristics, is proposed to solve the multi-objective budget-constrained dynamic Bag-of-Tasks scheduling problem in a heterogeneous multi-cloud environment. In our proposed model, the objective function differs depending on the scheduling level: The upper level aims to minimize the makespan of the whole Bag-of-Tasks under budget constraints; while each follower aims to minimize the makespan and the execution cost of tasks belonging to the Bag-of-Tasks. Since multiple conflicting objectives exist in the lower level, we propose an improved variant of the Non-dominated Sorting Genetic Algorithm II (NSGA-II) called Efficient NSGA-II (E-NSGA-II), applying a recently proposed quick non-dominated sorting algorithm (QNDSA) with quasi-linear average time complexity. By performing experiments on proposed synthetic datasets, our algorithm demonstrates high performance in terms of makespan and execution cost while respecting budget constraints. Statistical analysis validates the outperformance of our proposal regarding the considering metrics.},
  archive      = {J_APIN},
  author       = {Karaja, Mouna and Chaabani, Abir and Azzouz, Ameni and Ben Said, Lamjed},
  doi          = {10.1007/s10489-022-03942-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {9009-9037},
  shortjournal = {Appl. Intell.},
  title        = {Efficient bi-level multi objective approach for budget-constrained dynamic bag-of-tasks scheduling problem in heterogeneous multi-cloud environment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-perspective convolutional neural networks for citywide
crowd flow prediction. <em>APIN</em>, <em>53</em>(8), 8994–9008. (<a
href="https://doi.org/10.1007/s10489-022-03980-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flow prediction is an important problem of urban computing with many applications, such as public security. Inspired by the success of deep learning, various deep learning models have been proposed to solve this problem. Although existing methods have achieved good prediction performance, they cannot effectively capture richer spatial-temporal correlations that are important for crowd flow prediction. To address the limitation of existing methods, we propose a novel 2D CNN-based (convolutional neural networks) model via multiple perspectives called the MPCNN to capture richer spatial-temporal correlations. In particular, three perspective CNNs are included in the MPCNN: the front CNN, the side CNN and the top CNN. Then, we propose a fusion layer to combine the results of the three CNNs. In addition, in the MPCNN, we use external factors to enhance prediction performance. Based on four real-world datasets, we performed a series of experiments to compare the proposed method with existing methods, and experimental results demonstrate the effectiveness and efficiency of the proposed method.},
  archive      = {J_APIN},
  author       = {Dai, Genan and Kong, Weiyang and Liu, Yubao and Ge, Youming and Zhang, Sen},
  doi          = {10.1007/s10489-022-03980-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8994-9008},
  shortjournal = {Appl. Intell.},
  title        = {Multi-perspective convolutional neural networks for citywide crowd flow prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). BO-aug: Learning data augmentation policies via bayesian
optimization. <em>APIN</em>, <em>53</em>(8), 8978–8993. (<a
href="https://doi.org/10.1007/s10489-022-03790-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has been an essential technique to increase the amount and diversity of datasets, thus improving deep learning models. To pursue further performance, several automated data augmentation approaches have recently been proposed to find data augmentation policies automatically. However, there are still some key issues that deserve further exploration, i.e., a precise policy search space definition, the instructive policy evaluation method, and the low computational cost of policy search. In this paper, we propose a novel method named BO-Aug that attempts to solve the above issues. Empirical verification on three widely used image classification datasets shows that the proposed method can achieve state-of-the-art or comparable performance compared with advanced automated data augmentation methods, with a relatively low cost. Our code is available at https://github.com/Zhangcx19/BO-Aug .},
  archive      = {J_APIN},
  author       = {Zhang, Chunxu and Li, Ximing and Zhang, Zijian and Cui, Jiaxu and Yang, Bo},
  doi          = {10.1007/s10489-022-03790-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8978-8993},
  shortjournal = {Appl. Intell.},
  title        = {BO-aug: Learning data augmentation policies via bayesian optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimum volume simplex-based scene representation and
attribute recognition with feature fusion. <em>APIN</em>,
<em>53</em>(8), 8959–8977. (<a
href="https://doi.org/10.1007/s10489-022-03697-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene attribute recognition is to identify attribute labels of one scene image based on scene representation for deeper semantic understanding of scenes. In the past decades, numerous algorithms for scene representation have been proposed by feature engineering or deep convolutional neural network. For models based on only one kind of image feature, it is still difficult to learn the representation of multiple attributes from local image region. For models based on deep learning, despite multi-label can be directly used for learning attributes representation, huge training data are usually necessary to build the multi-label model. In this paper, we investigate the problem by the way of scene representation modeling with multi-feature and non-deep learning. Firstly, we introduce linear mixing model (LMM) for scene image modeling, then present a novel approach, referred to as the mini-batch minimum simplex estimation (MMSE), for attribute-based scene representation learning from highly complex image data. Finally, a two-stage multi-feature fusion method is proposed to further improve the feature representation for scene attribute recognition. The proposed method takes advantage of the fast convergence of nonnegative matrix factorization (NMF) schemes, and at the same time using mini-batch to speed up the computation for large-scale scene dataset. The experimental results based on real image scene demonstrate that the proposed method outperforms several other advanced scene attribute recognition approaches.},
  archive      = {J_APIN},
  author       = {Zou, Zhiyuan and Liu, Weibin and Xing, Weiwei and Zhang, Shunli},
  doi          = {10.1007/s10489-022-03697-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8959-8977},
  shortjournal = {Appl. Intell.},
  title        = {Minimum volume simplex-based scene representation and attribute recognition with feature fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A belief rényi divergence for multi-source information
fusion and its application in pattern recognition. <em>APIN</em>,
<em>53</em>(8), 8941–8958. (<a
href="https://doi.org/10.1007/s10489-022-03768-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source information fusion technology has been widely used because it can maximize the use of information that collected from multiple data sources for decision fusion. As an uncertain information processing theory, Dempster-Shafer (D-S) evidence theory is prevalent in the field of multi-source information fusion. However, there is still room for improvement in the handling of uncertain problems involving highly conflicting evidence sources. For the purpose of improving the practicality and efficiency in handling highly conflicting evidence sources, a new belief Rényi divergence is defined to measure the discrepancy between evidences in D-S evidence theory. The proposed belief Rényi divergence takes belief function (Bel) and plausibility function (Pl) into account, thus allowing it to provide a more rational and telling approach for measuring differences between evidence. Moreover, some important properties of belief Rényi divergence have been studied, the belief Rényi divergence regards to Hellinger distance, Kullback-Leibler divergence and χ2 divergence, which ensures that the metric has a wider range of application scenarios. Based on the proposed belief Rényi divergence measure, a novel multi-source information fusion method is designed. The proposed belief Rényi divergence is used to measure difference between evidence; Deng entropy is used to quantify the uncertainty, thereby calculating information volume of the evidence. Accordingly, the proposed method can fully assess relationship among evidences and information volume of each evidence. Through a comprehensive analysis and experiments, practicality and effectiveness of the proposed method for multi-source information fusion are verified. Finally, an iris dataset-based experiment is implemented to verify the new proposed divergence measure and the multi-source information fusion algorithm has a more extensive applicability.},
  archive      = {J_APIN},
  author       = {Zhu, Chaosheng and Xiao, Fuyuan},
  doi          = {10.1007/s10489-022-03768-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8941-8958},
  shortjournal = {Appl. Intell.},
  title        = {A belief rényi divergence for multi-source information fusion and its application in pattern recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The application of SOFNN based on PSO-ILM algorithm in
nonlinear system modeling. <em>APIN</em>, <em>53</em>(8), 8927–8940. (<a
href="https://doi.org/10.1007/s10489-022-03879-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy neural network (FNN) is the product of the combination of fuzzy theory and neural network. It combines the advantages of neural network and fuzzy theory, which has achieved great success in various fields. However, on the one hand, when the practical input is intricate and high dimensional data, the existing FNN can’t achieve a good modeling effect in the speed of convergence, the accuracy of modeling and generalization ability. On the other hand, the number of rules in FNN is fixed, which will also lead to the above problems in nonlinear system modeling. In this paper, a self-organizing fuzzy neural network based on Particle Swarm Optimization with improved Levenberg-Marquardt learning algorithm (SOFNN-PSO-ILM) is proposed for nonlinear system modeling. First, SOFNN based on PSO-ILM is built online by a method of constantly learning parameters and structures. In the process of structures learning, the number of fuzzy rules that have been set can be self-designed with the growing and pruning algorithm, which is based on the size of the singular value. In the process of parameters learning, PSO algorithm combined with ILM algorithm is used to update parameters. Then, the convergence and stability of SOFNN based on PSO-ILM are analyzed. Finally, the proposed method is used to model in the nonlinear system by three examples. The modeling results demonstrate that the proposed SOFNN based on PSO-ILM can model in nonlinear systems effectively.},
  archive      = {J_APIN},
  author       = {Deng, Huaijun and Liu, Linna and Fang, Jianyin and Yan, Li},
  doi          = {10.1007/s10489-022-03879-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8927-8940},
  shortjournal = {Appl. Intell.},
  title        = {The application of SOFNN based on PSO-ILM algorithm in nonlinear system modeling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic interaction-based feature selection algorithm for
maximal relevance minimal redundancy. <em>APIN</em>, <em>53</em>(8),
8910–8926. (<a
href="https://doi.org/10.1007/s10489-022-03922-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In feature selection, distinguishing the redundancy and dependency relationships between features is a challenging task. In recent years, scholars have constantly put forward some solutions, but most of them fail to effectively distinguish dependent features from redundant features. In addition, the influence of feature-relevant complementary item on candidate feature is also ignored, which further reduces the distinguishing ability. In order to improve the distinguishing ability further, the concept of feature interaction degree is proposed, on which the new discriminant criteria of feature redundancy and dependency are defined. With the discriminant criteria and the feature-relevant complementary item, the dynamic interaction weight is constructed. Then a filter feature selection algorithm DIMRMR is proposed to effectively solve the problem of confusing redundancy and dependency. The experimental results shows that the proposed algorithm can achieve the optimal classification performance on most of the datasets.},
  archive      = {J_APIN},
  author       = {Yin, Kexin and Xie, Aifeng and Zhai, Junren and Zhu, Jianqi},
  doi          = {10.1007/s10489-022-03922-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8910-8926},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic interaction-based feature selection algorithm for maximal relevance minimal redundancy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampled-data control for synchronization of markovian
jumping neural networks with packet dropout. <em>APIN</em>,
<em>53</em>(8), 8898–8909. (<a
href="https://doi.org/10.1007/s10489-022-03379-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addressed the master-slave synchronization problem of Markovian jumping neural networks with control packet dropout and sampled-data control. The packet dropout process is modelled as certain Bernoulli distributed white noise sequences. Under the zero-input strategy, a new stochastic switched sampled-data controller is proposed. Based on Lyapunov theory, an improved Lyapunov-Krasovskii function is constructed to derive the stability criteria. By using the technique of convex combination and free-matrix-based inequality, sufficient conditions can be obtained to guarantee the synchronization even if the packet dropout happens randomly. By employing the proposed scheme, the corresponding sampled-data controller is acquired through solving the linear matrix inequalities. The numerical example is provided to verify the feasibility and advantages of the approach.},
  archive      = {J_APIN},
  author       = {Wang, Hong and Ni, Yongjing and Wang, Jiawei and Tian, Jiaping and Ge, Chao},
  doi          = {10.1007/s10489-022-03379-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8898-8909},
  shortjournal = {Appl. Intell.},
  title        = {Sampled-data control for synchronization of markovian jumping neural networks with packet dropout},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urban ride-hailing demand prediction with multi-view
information fusion deep learning framework. <em>APIN</em>,
<em>53</em>(8), 8879–8897. (<a
href="https://doi.org/10.1007/s10489-022-03966-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban online ride-hailing demand forecasting is an important component of smart city transportation systems. An accurate online ride-hailing demand prediction model can help cities allocate online ride-hailing resources reasonably, reduce energy waste, and reduce traffic congestion. With the massive popularity of online ride-hailing, we can collect a large amount of order data, and how to use deep learning models for improving order prediction accuracy has become a hot research topic. Most of the urban online taxi demand forecasting methods do not sufficiently consider the influencing factors and cannot model the complex nonlinear spatio-temporal relationships. Therefore, we propose a multi-view deep spatio-temporal network framework (MVDSTN) architecture to obtain the spatio-temporal relationships for online ride-hailing demand prediction. Our proposed model includes five views,up-passenger order view, down-passenger order view, POI view, spatial GCN view, POI view and weather view, applies LSTM with attention mechanism to achieve demand prediction for urban online taxi bodies. Experiments Haikou Didi Taxi datasets and Wuhan Taxi datasets prove that our model has good robustness and the prediction method outperforms current methods.},
  archive      = {J_APIN},
  author       = {Wu, Yonghao and Zhang, Huyin and Li, Cong and Tao, Shiming and Yang, Fei},
  doi          = {10.1007/s10489-022-03966-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8879-8897},
  shortjournal = {Appl. Intell.},
  title        = {Urban ride-hailing demand prediction with multi-view information fusion deep learning framework},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RT-net: Replay-and-transfer network for class incremental
object detection. <em>APIN</em>, <em>53</em>(8), 8864–8878. (<a
href="https://doi.org/10.1007/s10489-022-03509-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable performance achieved by DNN-based object detectors, class incremental object detection (CIOD) remains a challenge, in which the network has to learn to detect novel classes sequentially. Catastrophic forgetting is the main problem underlying this difficulty, as neural networks tend to detect new classes only when training samples for old classes are absent. In this paper, we propose the Replay-and-Transfer Network (RT-Net) to address this issue and accomplish CIOD. We develop a generative replay model to replay features of old classes during learning of new ones for the RoI (Region of Interest) head, using the stored latent feature distributions. To overcome the drastic changes of the RoI feature space, guided feature distillation and feature translation are introduced to facilitate knowledge transfer from the old model to the new one. In addition, we propose holistic ranking transfer, which transfers ranking orders of proposals to the new model, to enable the region proposal network to identify high quality proposals for old classes. Importantly, this framework provides a general solution for CIOD, which can be successfully applied to two task settings: set-overlapped, in which the old and new training sets are overlapped, and set-disjoint, in which the old and new tasks have unique samples. Extensive experiments on standard benchmark datasets including PASCAL VOC and COCO show that RT-Net can achieve state-of-the-art performance for CIOD.},
  archive      = {J_APIN},
  author       = {Cui, Bo and Hu, Guyue and Yu, Shan},
  doi          = {10.1007/s10489-022-03509-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8864-8878},
  shortjournal = {Appl. Intell.},
  title        = {RT-net: Replay-and-transfer network for class incremental object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning heuristics for weighted CSPs through deep
reinforcement learning. <em>APIN</em>, <em>53</em>(8), 8844–8863. (<a
href="https://doi.org/10.1007/s10489-022-03992-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighted constraint satisfaction problems (WCSPs) are one of the most important constraint programming models aiming to find a cost-minimal solution. Due to its NP-hardness, solving a WCSP usually requires efficient heuristics to explore high-quality solutions. Unfortunately, such heuristics are hand-crafted and may not be generalizable across different cases. On the other hand, although Deep Learning (DL) has been proven to be a promising way to learn heuristics for combinatorial optimization problems, the existing DL-based methods are unsuitable for WCSPs since they fail to exploit the problem structure of WCSPs. Besides, such methods are often based on Supervised Learning (SL), making the learned heuristics less efficient since it is challenging to generate a sufficiently large training corpus. Therefore, we propose a novel Deep Reinforcement Learning (DRL) framework to train the model on large-scale problems, so that the model could mine more sophisticated patterns and provide high-quality heuristics. By exploiting the problem structure, we effectively decompose the problem by using a pseudo tree, and formulate the solution construction process as a Markov decision process with multiple independent transition states. With Graph Attention Networks (GATs) parameterized deep Q-value network, we learn the optimal Q-values through a modified Bellman equation that considers the multiple transition states, and extract the solution construction heuristics from the trained network. Besides constructing solutions greedily, our heuristics can also be applied to many meta-heuristics such as beam search and large neighborhood search. The experiments show that our DRL-boosted algorithms significantly outperform the counterparts with their heuristics derived from the SL model, their counterparts with traditional tabular-based heuristics and state-of-the-art methods on benchmark problems.},
  archive      = {J_APIN},
  author       = {Chen, Dingding and Chen, Ziyu and He, Zhongshi and Gao, Junsong and Su, Zhizhuo},
  doi          = {10.1007/s10489-022-03992-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8844-8863},
  shortjournal = {Appl. Intell.},
  title        = {Learning heuristics for weighted CSPs through deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controlling non-stationarity and periodicities in time
series generation using conditional invertible neural networks.
<em>APIN</em>, <em>53</em>(8), 8826–8843. (<a
href="https://doi.org/10.1007/s10489-022-03742-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generated synthetic time series aim to be both realistic by mirroring the characteristics of real-world time series and useful by including characteristics that are useful for subsequent applications, such as forecasting and missing value imputation. To generate such realistic and useful time series, we require generation methods capable of controlling the non-stationarity and periodicities of the generated time series. However, existing approaches do not consider such explicit control. Therefore, in the present paper, we present a novel approach to control non-stationarity and periodicities with calendar and statistical information when generating time series. We first define the requirements for methods to generate time series with non-stationarity and periodicities, which we show are not fulfilled by existing generation methods. Second, we formally describe the novel approach for controlling non-stationarity and periodicities in generated time series. Thirdly, we introduce an exemplary implementation of this approach using a conditional Invertible Neural Network (cINN). We evaluate this cINN empirically in experiments with real-world data sets and compare it to state-of-the-art time series generation methods. Our experiments show that the evaluated cINN can generate time series with controlled periodicities and non-stationarity, and it also generally outperforms the selected benchmarks.},
  archive      = {J_APIN},
  author       = {Heidrich, Benedikt and Turowski, Marian and Phipps, Kaleb and Schmieder, Kai and Süß, Wolfgang and Mikut, Ralf and Hagenmeyer, Veit},
  doi          = {10.1007/s10489-022-03742-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8826-8843},
  shortjournal = {Appl. Intell.},
  title        = {Controlling non-stationarity and periodicities in time series generation using conditional invertible neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConvNet-based performers attention and supervised
contrastive learning for activity recognition. <em>APIN</em>,
<em>53</em>(8), 8809–8825. (<a
href="https://doi.org/10.1007/s10489-022-03937-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition based on generated sensor data plays a major role in a large number of applications such as healthcare monitoring and surveillance system. Yet, accurately recognizing human activities is still challenging and active research due to people’s tendency to perform daily activities in a different and multitasking way. Existing approaches based on the recurrent setting for human activity recognition have some issues, such as the inability to process data parallelly, the requirement for more memory and high computational cost albeit they achieved reasonable results. Convolutional Neural Network processes data parallelly, but, it breaks the ordering of input data, which is significant to build an effective model for human activity recognition. To overcome these challenges, this study proposes causal convolution based on performers-attention and supervised contrastive learning to entirely forego recurrent architectures, efficiently maintain the ordering of human daily activities and focus more on important timesteps of the sensors’ data. Supervised contrastive learning is integrated to learn a discriminative representation of human activities and enhance predictive performance. The proposed network is extensively evaluated for human activities using multiple datasets including wearable sensor data and smart home environments data. The experiments on three wearable sensor datasets and five smart home public datasets of human activities reveal that our proposed network achieves better results and reduces the training time compared with the existing state-of-the-art methods and basic temporal models.},
  archive      = {J_APIN},
  author       = {Hamad, Rebeen Ali and Yang, Longzhi and Woo, Wai Lok and Wei, Bo},
  doi          = {10.1007/s10489-022-03937-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8809-8825},
  shortjournal = {Appl. Intell.},
  title        = {ConvNet-based performers attention and supervised contrastive learning for activity recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum algorithm of dempster rule of combination.
<em>APIN</em>, <em>53</em>(8), 8799–8808. (<a
href="https://doi.org/10.1007/s10489-022-03877-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster rule of combination is a powerful combination tool. It has been widely used in many fields, such as information fusion and decision-making. However, the computational complexity of Dempster rule of combination increases exponentially with the increase of frame of discernment. To address this issue, leveraging the parallel advantage of quantum computing, we present a quantum algorithm of Dempster rule of combination. The new method includes four steps. First, the quantum superposition states corresponding to arbitrary mass functions are prepared. Next, the superposition states corresponding to the two mass functions are combined by the tensor product. Effective qubits are then measured. Finally, the measurement results are normalized to obtain the combined results. The new method not only realizes most of the functions of Dempster rule of combination, but also effectively reduces the computational complexity of Dempster rule of combination in the quantum computer. Finally, we carry out the simulation experiments on the quantum cloud platform of IBM, and the experimental results show that the new method is reasonable. Compared with the traditional combination rule, this method effectively reduces the computational complexity. As the frame of discernment becomes larger, the advantages of the proposed approach in terms of running time become larger and larger.},
  archive      = {J_APIN},
  author       = {Pan, Lipeng and Gao, Xiaozhuan and Deng, Yong},
  doi          = {10.1007/s10489-022-03877-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8799-8808},
  shortjournal = {Appl. Intell.},
  title        = {Quantum algorithm of dempster rule of combination},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning-based classification for topic detection of
audiovisual documents. <em>APIN</em>, <em>53</em>(8), 8776–8798. (<a
href="https://doi.org/10.1007/s10489-022-03938-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The description of the audiovisual documents aims essentially at providing meaningful and explanatory information about their content. Despite the multiple efforts made by several researchers to extract descriptions, the lack of pertinent semantic descriptions always persists. We introduce, in this paper, a new approach to improve the semantic descriptions of the cinematic audiovisual documents. To ensure a high description level, we combine different sources of information related to the content (the script of the movie and the superposed text of the image). This process is mainly based on a semantic segmentation algorithm. The Structured Topic Model (STM) and the LSCOM Ontology ( http://www.ee.columbia.edu/ln/dvmm/lscom/ ) (Large Scale Concept ontologyMultimedia) are adapted for knowledge and descriptions extraction. Deep classification techniques, such as LSTM (long short-term memory) and softmax regression, are used to classify the generic topics into specific topics. The performance of the developed approach is assessed as follows. First, STM topic is adapted and evaluated using the CMU movie summary corpus. Then, the topics detection and classification processes are applied and their results are compared to those provided by human judgments employing the MoviLens dataset. Finally, quantitative evaluation is performed utilizing the M-VAD (Montreal Video Annotation Dataset) [44] and MPII-MD (large scale movie description datasets) [35] databases. The comparative study proves that the suggested approach outperforms the existing ones in terms of the precision of the obtained topics.},
  archive      = {J_APIN},
  author       = {Fourati, Manel and Jedidi, Anis and Gargouri, Faiez},
  doi          = {10.1007/s10489-022-03938-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8776-8798},
  shortjournal = {Appl. Intell.},
  title        = {A deep learning-based classification for topic detection of audiovisual documents},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label prompt for multi-label text classification.
<em>APIN</em>, <em>53</em>(8), 8761–8775. (<a
href="https://doi.org/10.1007/s10489-022-03896-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification has been widely concerned by scholars due to its contribution to practical applications. One of the key challenges in multi-label text classification is how to extract and leverage the correlation among labels. However, it is quite challenging to directly model the correlations among labels in a complex and unknown label space. In this paper, we propose a Label Prompt Multi-label Text Classification model (LP-MTC), which is inspired by the idea of prompt learning of pre-trained language model. Specifically, we design a set of templates for multi-label text classification, integrate labels into the input of the pre-trained language model, and jointly optimize by Masked Language Models (MLM). In this way, the correlations among labels as well as semantic information between labels and text with the help of self-attention can be captured, and thus the model performance is effectively improved. Extensive empirical experiments on multiple datasets demonstrate the effectiveness of our method. Compared with BERT, LP-MTC improved 3.4% micro-F1 on average over the four public datasets.},
  archive      = {J_APIN},
  author       = {Song, Rui and Liu, Zelong and Chen, Xingbing and An, Haining and Zhang, Zhiqi and Wang, Xiaoguang and Xu, Hao},
  doi          = {10.1007/s10489-022-03896-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8761-8775},
  shortjournal = {Appl. Intell.},
  title        = {Label prompt for multi-label text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving temporal knowledge graph embedding using tensor
factorization. <em>APIN</em>, <em>53</em>(8), 8746–8760. (<a
href="https://doi.org/10.1007/s10489-021-03149-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approach of knowledge graph embedding (KGE) enables it possible to represent facts of a knowledge graph (KG) in low-dimensional continuous vector spaces. Consequently, it can significantly reduce the complexity of those operations performed on the underlying KG, and has attracted a lot of attention in recent years. However, most of KGE approaches have only been developed over static facts and ignore the time attribute. As a matter of effect, in some real-world KGs, a fact might only be valid for a specific time interval or point in time. For instance, the fact (Barack Obama, is president of, US, [2009-2017]) is only valid between 2009 and 2017. To conquer this issue, based on a famous tensor factorization approach, canonical polyadic decomposition, we propose two new temporal KGE models called TSimplE and TNTSimplE that integrates time information besides static facts. A non-temporal component is also added to deal with heterogeneous temporal KGs that include both temporal and non-temporal relations. We prove that the proposed models are fully expressive which has a bound on the dimensionality of their embeddings, and can incorporate several important types of background knowledge including symmetry, antisymmetry and inversion. In addition, our models are capable of dealing with two common challenges in real-world temporal KGs, i.e., modeling time intervals and predicting time for facts with missing time information. We conduct extensive experiments on three real-world temporal KGs: ICEWS, YAGO3 and Wikidata. The results indicate that our models achieve start-of-the-art performance with lower time or space complexity.},
  archive      = {J_APIN},
  author       = {He, Peng and Zhou, Gang and Zhang, Mengli and Wei, Jianghong and Chen, Jing},
  doi          = {10.1007/s10489-021-03149-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8746-8760},
  shortjournal = {Appl. Intell.},
  title        = {Improving temporal knowledge graph embedding using tensor factorization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D real-time human reconstruction with a single RGBD camera.
<em>APIN</em>, <em>53</em>(8), 8735–8745. (<a
href="https://doi.org/10.1007/s10489-022-03969-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human reconstruction is an important technology connecting the real world and the virtual world, but most of previous work needs expensive computing resources, making it difficult in real-time scenarios. We propose a lightweight human body reconstruction system based on parametric model, which employs only one RGBD camera as input. To generate a human model end to end, we build a fast and lightweight deep-learning network named Fast Body Net (FBN). The network pays more attention on the face and hands to enrich the local details. Additionally, we train a denoising auto-encoder to reduce unreasonable states of human model. Due to the lack of human dataset based on RGBD images, we propose an Indoor-Human dataset to train the network, which contains a total of 2500 frames of action data of five actors collected by Azure Kinect camera. Depth images avoid using RGB to extract depth features, which makes FBN lightweight and high-speed in reconstructing parametric human model. Qualitative and quantitative analysis on experimental results show that our method can improve at least 57% in efficiency with similar accuracy, as compared to state-of-the-art methods. Through our study, it is also demonstrated that consumer-grade RGBD cameras can provide great applications in real-time display and interaction for virtual reality.},
  archive      = {J_APIN},
  author       = {Lu, Yang and Yu, Han and Ni, Wei and Song, Liang},
  doi          = {10.1007/s10489-022-03969-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8735-8745},
  shortjournal = {Appl. Intell.},
  title        = {3D real-time human reconstruction with a single RGBD camera},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Adaptive neural output feedback control of automobile PEM
fuel cell air-supply system with prescribed performance. <em>APIN</em>,
<em>53</em>(8), 8712–8734. (<a
href="https://doi.org/10.1007/s10489-022-03765-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oxygen excess ratio (OER) is a key specification of fuel cells, which influences the net power and health state. To reconstitute the unmeasurable variable and achieve precise tracking accuracy, the observer-based adaptive neural network control using a prescribed performance function is proposed for the polymer electrolyte membrane (PEM) fuel cell air-supply system. Firstly, an observer is designed to recover the unmeasurable variable based on the transformed canonical system. Secondly, a finite-time prescribed performance function is constructed to guarantee the maximal overshoot and steady-state tracking error within the quantitative boundary. The contribution of the proposed control scheme can be concluded that: 1) the different errors are simultaneously used to update the neural network weights for the improvement of the observer performance; 2) the restriction that the initial error is required to be within the performance function bound is relaxed by proposing a tuning function and 3) the convergence time and residual set of OER tracking error can be determined qualitatively. The signals included in the air-supply system are proved to be uniformly ultimately bounded. Different numerical simulations and hardware-in-loop (HIL) experiments show that the more accurate estimation is provided by the proposed observer. Meanwhile, the tracking errors are restricted within the predefined bounds. From the experimental results, the proposed observer and controller show the best performance indexes including the root mean square error (RMSE), the mean absolute error (MAE) and the standard deviation (SD) in different conditions.},
  archive      = {J_APIN},
  author       = {Wang, Yunlong and Liu, Yan and Wang, Yongfu},
  doi          = {10.1007/s10489-022-03765-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8712-8734},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive neural output feedback control of automobile PEM fuel cell air-supply system with prescribed performance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework to enhance generalization of deep metric
learning methods using general discriminative feature learning and class
adversarial neural networks. <em>APIN</em>, <em>53</em>(8), 8693–8711.
(<a href="https://doi.org/10.1007/s10489-022-03959-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Metric Learning (DML) methods automatically extract features from data and learn a non-linear transformation from the input to a semantically embedding space. Many DML methods focused to enhance the discrimination power of the learned metric by proposing novel sampling strategies or loss functions. This approach is very helpful when both the training and test examples are selected from the same set of categories. However, it is less effective in many applications of DML such as image retrieval and person-reidentification. Here, the DML should learn general semantic concepts from observed classes and employ them to rank or identify objects from unseen categories. Neglecting the generalization ability of the learned representation and just emphasizing to learn a more discriminative embedding on the observed classes may lead to the overfitting problem. To address this limitation, we propose a framework to enhance the generalization power of existing DML methods in a Zero-Shot Learning (ZSL) setting by general yet discriminative representation learning and employing a class adversarial neural network. To learn a general representation, we employ feature maps of intermediate layers in a deep neural network and enhance their discrimination power through an attention mechanism. Besides, a class adversarial network is utilized to force the deep model to seek class invariant features. We evaluate our work on widely used machine vision datasets in a ZSL setting. Extensive experimental results confirm that our framework can improve the generalization of existing DML methods, and it consistently outperforms baseline DML algorithms on unseen classes.},
  archive      = {J_APIN},
  author       = {Al-Kaabi, Karrar and Monsefi, Reza and Zabihzadeh, Davood},
  doi          = {10.1007/s10489-022-03959-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8693-8711},
  shortjournal = {Appl. Intell.},
  title        = {A framework to enhance generalization of deep metric learning methods using general discriminative feature learning and class adversarial neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous build outcome prediction: An experimental
evaluation and acceptance modelling. <em>APIN</em>, <em>53</em>(8),
8673–8692. (<a
href="https://doi.org/10.1007/s10489-023-04523-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous Build Outcome Prediction (CBOP) is a lightweight implementation of Continuous Defect Prediction (CDP). CBOP combines: 1) results of continuous integration (CI) and 2) the data mined from the version control system with 3) machine learning (ML) to form a practice that evolved from software defect prediction (SDP) where a failing build is treated as a defect to fight against. Here, we explain the CBOP idea, where we use historical build results together with metrics derived from a software repository to create a model that classifies changes the developer is introducing to the source code during her work in a just-in-time manner. To evaluate the CBOP idea, we perform a small-n repeated measure with two conditions and replicate experiment in a real-life, business-driven software project. In this preliminary evaluation of CBOP, we study whether the practice will reduce the Failed Build Ratio (FBR) - the ratio of failing build results to all other build results. We calculate effect size and p-value of change in FBR while using the CBOP practice, provide an analysis of our model, and perform and report the results of a Technology Acceptance Model (TAM)-inspired survey that we conducted among experiment participants and industry specialists to assess the acceptance of CBOP and the tool.},
  archive      = {J_APIN},
  author       = {Kawalerowicz, Marcin and Madeyski, Lech},
  doi          = {10.1007/s10489-023-04523-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8673-8692},
  shortjournal = {Appl. Intell.},
  title        = {Continuous build outcome prediction: An experimental evaluation and acceptance modelling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing multi-objective evolutionary neural architecture
search with training-free pareto local search. <em>APIN</em>,
<em>53</em>(8), 8654–8672. (<a
href="https://doi.org/10.1007/s10489-022-04032-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS), that automates the design process of high-performing neural network architectures, is a multi-objective optimization problem. A single ideal architecture, that optimizes both predictive performance (e.g., the network accuracy) and computational costs (e.g., the model size, the number of parameters, the number of floating-point operations), does not exist. Instead, there is a Pareto front of multiple candidate architectures where each one represents an optimal trade-off between the competing objectives. Multi-Objective Evolutionary Algorithms (MOEAs) are often employed to approximate such Pareto-optimal fronts for NAS problems. In this article, we introduce a local search method, namely Potential Solution Improving (PSI), that aims to improve certain potential solutions on approximation fronts to enhance the performance of MOEAs. The main bottleneck in NAS is the considerable computation cost that incurs from having to train a large number of candidate architectures to evaluate their accuracy. Recently, the Synaptic Flow has been proposed as a metric that relatively characterizes the performance of deep neural networks without running any training epoch. We thus propose that our PSI method can make use of this training-free metric as a proxy for network accuracy during local search steps. We conduct experiments with the well-known MOEA Non-dominated Sorting Genetic Algorithm II (NSGA-II) coupled with the training-free PSI local search in solving NAS problems created from the standard benchmarks NAS-Bench-101 and NAS-Bench-201. Experimental results confirm the efficiency enhancements brought about by our proposed method, which reduces the computational cost by four times compared to the baseline approach. The source code for the experiments in the article can be found at: https://github.com/ELO-Lab/MOENAS-TF-PSI .},
  archive      = {J_APIN},
  author       = {Phan, Quan Minh and Luong, Ngoc Hoang},
  doi          = {10.1007/s10489-022-04032-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8654-8672},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing multi-objective evolutionary neural architecture search with training-free pareto local search},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid genetic algorithms for the determination of DNA
motifs to satisfy postulate 2-optimality. <em>APIN</em>, <em>53</em>(8),
8644–8653. (<a
href="https://doi.org/10.1007/s10489-022-03491-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, determining DNA motifs or consensus plays an indispensable role in bioinformatics. Many postulates have been proposed for finding a consensus. Postulate 2-Optimality is essential for this task. A consensus satisfying postulate 2-Optimality is the best representative of a profile, and its distances to the profile members are uniform. However, this postulate has not been widely investigated in identifying a DNA motif or consensus for a DNA motif profile. The HDC algorithm is the best at this task in the literature. This study focuses on determining DNA motifs that satisfy postulate 2-Optimality. We propose a new hybrid genetic (HG1) algorithm based on the elitism strategy and local search. Subsequently, a novel elitism strategy and longest distance strategy are introduced to maintain the balance of exploration and exploitation. A new hybrid genetic (HG2) algorithm is developed based on the proposed exploration and exploitation balance approach. The simulation results show that these algorithms provide a high-quality DNA motif. The HG2 algorithm provides a DNA motif with the best quality.},
  archive      = {J_APIN},
  author       = {Dang, Dai Tho and Nguyen, Ngoc Thanh and Hwang, Dosam},
  doi          = {10.1007/s10489-022-03491-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8644-8653},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid genetic algorithms for the determination of DNA motifs to satisfy postulate 2-optimality},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated deep active learning for attention-based
transaction classification. <em>APIN</em>, <em>53</em>(8), 8631–8643.
(<a href="https://doi.org/10.1007/s10489-022-04388-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical transactions can be clustered and classified using an effective vector representation. Traditionally, methods for finding patterns have relied on heuristics and pruning. Despite the high dimensionality of transactional data, approaches that use frequent item sets as features can be affected by dimensionality, sparsity, and privacy issues. As a result, FIs are not uniformly distributed over the data. In this paper, we propose an embedded architecture for transaction classification based on distributed learning. The model transforms transaction data into frequent sets and implements attention mechanisms based on encoder-decoder structures. Consequently, the model can learn continuous vectors in low dimensions while maintaining context and colocation. We analyzed a high-dimensional transaction dataset to test attention-based methods and federated learning. To improve decision limits while maintaining privacy and security, the proposed model lowers the global loss function. In the experiment, four datasets are used for comparison. The data are randomly selected and distributed to different clients for each dataset. We run each experiment with five different random partitions of the dataset to evaluate the active learning. The training set is the same size in each round, and the test set is not evaluated in each round. Our methods are compared to the best performing baseline method using the F1 score and the percentage of the dataset used. Compared to the baseline model, the proposed model performed better in terms of percentage increases and output classes, namely retail (1.7%, 2), cancer (17.38%, 3), food (0.74%, 2), and snippet (3.47%, 8).},
  archive      = {J_APIN},
  author       = {Ahmed, Usman and Lin, Jerry Chun-Wei and Fournier-Viger, Philippe},
  doi          = {10.1007/s10489-022-04388-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8631-8643},
  shortjournal = {Appl. Intell.},
  title        = {Federated deep active learning for attention-based transaction classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPSRS: A fusion approach for paper submission recommendation
system. <em>APIN</em>, <em>53</em>(8), 8614–8630. (<a
href="https://doi.org/10.1007/s10489-022-04117-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been increasingly popular in entertainment and consumption, and they are evident in academics, especially for applications that suggest submitting scientific articles to scientists. However, due to the various acceptance rates, impact factors, and rankings in different publishers, searching for a proper venue or journal to submit a scientific work usually takes a lot of time and effort. In this paper, we aim to present two new approaches extended from our paper (Huynh et al. 2021) presented at IAE/AIE 2021. In the first approach, we employ RNN structures besides using Conv1D. In the second approach, we introduce a new method, using DistillBert for two cases of uppercase and lowercase words. It can help vectorize features (such as Title, Abstract, and Keywords) and then use Conv1d to perform feature extraction. Furthermore, we propose a new calculation method for similarity score for Aim &amp; Scope with other features (we name this approach is DistilBertAims). It can help keep the weights of similarity score calculation continuously updated and then continue to fit more data. The experimental results show that the second approach could obtain a better performance, which are 62.46%, 90.32%, 94.89%, 97.96% while the best performance in previous studies barely gained 50.02%, 78.89%, 86.27%, 93.23% in terms of Top K Accuracy (K = 1, 3, 5, 10). Interestingly, our best approach in this paper is higher than 12.44% the best of the previous study (Huynh et al. 2021) in terms of the Top 1 accuracy, which was presented in the conference IEA/AIE 2021.},
  archive      = {J_APIN},
  author       = {Huynh, Son T. and Dang, Nhi and Nguyen, Dac H. and Huynh, Phong T. and Nguyen, Binh T.},
  doi          = {10.1007/s10489-022-04117-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8614-8630},
  shortjournal = {Appl. Intell.},
  title        = {FPSRS: A fusion approach for paper submission recommendation system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-policy and on-policy reinforcement learning with the
tsetlin machine. <em>APIN</em>, <em>53</em>(8), 8596–8613. (<a
href="https://doi.org/10.1007/s10489-022-04297-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tsetlin Machine is a recent supervised learning algorithm that has obtained competitive accuracy- and resource usage results across several benchmarks. It has been used for convolution, classification, and regression, producing interpretable rules in propositional logic. In this paper, we introduce the first framework for reinforcement learning based on the Tsetlin Machine. Our framework integrates the value iteration algorithm with the regression Tsetlin Machine as the value function approximator. To obtain accurate off-policy state-value estimation, we propose a modified Tsetlin Machine feedback mechanism that adapts to the dynamic nature of value iteration. In particular, we show that the Tsetlin Machine is able to unlearn and recover from the misleading experiences that often occur at the beginning of training. A key challenge that we address is mapping the intrinsically continuous nature of state-value learning to the propositional Tsetlin Machine architecture, leveraging probabilistic updates. While accurate off-policy, this mechanism learns significantly slower than neural networks on-policy. However, by introducing multi-step temporal-difference learning in combination with high-frequency propositional logic patterns, we are able to close the performance gap. Several gridworld instances document that our framework can outperform comparable neural network models, despite being based on simple one-level AND-rules in propositional logic. Finally, we propose how the class of models learnt by our Tsetlin Machine for the gridworld problem can be translated into a more understandable graph structure. The graph structure captures the state-value function approximation and the corresponding policy found by the Tsetlin Machine.},
  archive      = {J_APIN},
  author       = {Rahimi Gorji, Saeed and Granmo, Ole-Christoffer},
  doi          = {10.1007/s10489-022-04297-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8596-8613},
  shortjournal = {Appl. Intell.},
  title        = {Off-policy and on-policy reinforcement learning with the tsetlin machine},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient anomaly identification in temporal and
non-temporal industrial data using tree based approaches. <em>APIN</em>,
<em>53</em>(8), 8562–8595. (<a
href="https://doi.org/10.1007/s10489-022-03940-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies indicate impending failures in expensive industrial devices. Manufacturers of such devices or Plant Managers depend heavily on anomaly detection algorithms to perform monitoring and predictive maintenance activities. Since false alarms directly impact any industrial manufacturer’s revenue, it is crucial to reduce the number of false alarms generated by anomaly detection algorithms. Here in this paper, we have proposed multiple solutions to address this ongoing problem in the industry. The proposed unsupervised solution, Multi-Generations Tree (MGTree) algorithm, not only reduced the false positive alarms but is also equally effective on small and large datasets. MGTree has been applied to multiple industrial datasets such as Yahoo, AWS, GE, and machine sensors for evaluation purposes. Our empirical evaluation shows that MGTree performs favorably to Isolation Forest (iForest), One Class Support Vector Machine (OCSVM), and Elliptic Envelope in terms of correctness of the identification (True-Positive and False-Positive) of the anomalies. We have also proposed a time series prediction algorithm Weighted Time-Window Moving Estimation (WTM), which does not rely on the dataset’s stationary characteristics and is evaluated on multiple time-series datasets. The hybrid combination of WTM and MGTree, Uni-variate Multi-Generations Tree (UVMGTree) worked very well in anomaly identification of the time series datasets and outperformed OCSVM, iForest, SARIMA, and Elliptic Envelope. Our approach can have a profound impact on the predictive maintenance and health monitoring of the industrial systems across the domains where the operations team can save significant time and effort in handling false alarms. .},
  archive      = {J_APIN},
  author       = {Sarkar, Jyotirmoy and Saha, Snehanshu and Sarkar, Santonu},
  doi          = {10.1007/s10489-022-03940-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8562-8595},
  shortjournal = {Appl. Intell.},
  title        = {Efficient anomaly identification in temporal and non-temporal industrial data using tree based approaches},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HDSHUI-miner: A novel algorithm for discovering spatial
high-utility itemsets in high-dimensional spatiotemporal databases.
<em>APIN</em>, <em>53</em>(8), 8536–8561. (<a
href="https://doi.org/10.1007/s10489-022-04436-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial high-utility itemset (SHUI) mining is a significant big data analysis technique. It aims to locate all geographically interesting itemsets with high utility in a spatiotemporal database. An SHUI-Miner algorithm was presented in the literature to find the desired itemsets. Unfortunately, this algorithm suffered from performance issues when dealing with high-dimensional spatiotemporal databases. Based on this finding, this paper extends the state-of-the-art method by proposing a novel algorithm known as the high-dimensional SHUI-miner (HDSHUI-Miner). Our algorithm explores several novel pruning strategies to decrease the search space and computational cost required to find the desired itemsets. Experimental results obtained on seven real-world databases demonstrate that HDSHUI-Miner outperforms SHUI-Miner with respect to memory consumption, runtime, and scalability. Finally, we present two real-world case studies to illustrate the usefulness of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Uday Kiran, Rage and Veena, Pamalla and Ravikumar, Penugonda and Venus Vikranth Raj, Bathala and Dao, Minh-Son and Zettsu, Koji and Bommisetti, Sai Chithra},
  doi          = {10.1007/s10489-022-04436-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8536-8561},
  shortjournal = {Appl. Intell.},
  title        = {HDSHUI-miner: A novel algorithm for discovering spatial high-utility itemsets in high-dimensional spatiotemporal databases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The analysis of data metamodels’ extensional layer via
extended generalized graph. <em>APIN</em>, <em>53</em>(8), 8510–8535.
(<a href="https://doi.org/10.1007/s10489-022-04440-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are several limitations known in data modeling discipline, which are related directly to the traditionally used data modeling languages expressiveness. The strong limitations of the expressiveness of the existing well known data modelling languages combined with the lack of a very general universal data modeling language have negative impact to modelling naturalness. As the result of mentioned limits the reality must be transformed to avoid (workaround) the limits introduced by the modelling language. In turn, the transformation process requires extra effort. The problem is strengthened by the lack of mechanisms, which can be used to measure the expressiveness of a particular data modeling language. Some limitations of the existing data modeling languages result from both their metamodel (abstract syntax) and model (metamodel instance) graph-like structure constraints. This kind of limits also has negative impact to a domain-specific modeling naturalness. The paper addresses all problems mentioned above. The problems can be solved with the help of the EGG data modeling language introduced in the paper. First, a universal and customizable EGG data modeling language together with the customization mechanisms (extensions and generalizations) is introduced. According to the first usage scenario the EGG may be applied for domain-specific data modelling tasks in place of other data modeling languages. Second, the paper proposes and applies (for some data modeling languages: RDF, XML, RDBM, UML and AOM) a novel concept of measuring and comparing data modelling languages via mapping their metamodels to the EGG metamodel. So, according to the second usage scenario the EGG metamodel can be used as a reference metamodel for the data modeling language expressiveness comparative studies. It may also support the decision process when a data modeling language must be chosen for a particular domain-specific data modeling task. Third, the EGG introduced in the paper helps to avoid transforming reality to the needs resulting from the data modeling language as the EGG is general enough for the domain data modeling task. Complete abstract syntax of the Extended Generalized Graph is introduced and is expressed through its implementations in terms of the Association-Oriented Metamodel and the Unified Modeling Language. Semantics of each syntactical category of abstract syntax is described. Two complete concrete syntaxes for the Extended Generalized Graph are also introduced in the paper. The case studies related to both social network and knowledge modeling illustrate the applicability and usefulness of the EGG. Abstract syntax is compared to several other metamodels. The comparative study of the case study models created first in different metamodels and then expressed in the Extended Generalized Graph metamodel is summarized quantitatively in the form of a proposed measure.},
  archive      = {J_APIN},
  author       = {Jodłowiec, Marcin and Krótkiewicz, Marek and Zabawa, Piotr},
  doi          = {10.1007/s10489-022-04440-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8510-8535},
  shortjournal = {Appl. Intell.},
  title        = {The analysis of data metamodels’ extensional layer via extended generalized graph},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial note of the special issue on emerging topics in
artificial intelligence selected from IEA/AIE2021. <em>APIN</em>,
<em>53</em>(8), 8507–8509. (<a
href="https://doi.org/10.1007/s10489-023-04524-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Chun-Wei Lin, Jerry and Selamat, Ali},
  doi          = {10.1007/s10489-023-04524-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {8},
  pages        = {8507-8509},
  shortjournal = {Appl. Intell.},
  title        = {Editorial note of the special issue on emerging topics in artificial intelligence selected from IEA/AIE2021},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Causal heterogeneity discovery by bottom-up pattern search
for personalised decision making. <em>APIN</em>, <em>53</em>(7),
8180–8194. (<a
href="https://doi.org/10.1007/s10489-022-03860-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In personalised decision making, evidence is required to determine whether an action (treatment) is suitable for an individual. Such evidence can be obtained by modelling treatment effect heterogeneity in subgroups. The existing interpretable modelling methods take a top-down approach to search for subgroups with heterogeneous treatment effects and they may miss the most specific and relevant context for an individual. In this paper, we design a Treatment effect pattern (TEP) to represent treatment effect heterogeneity in data. To achieve an interpretable presentation of TEPs, we use a local causal structure around the outcome to explicitly show how those important variables are used in modelling. We also derive a formula for unbiasedly estimating the Conditional Average Causal Effect (CATE) using the local structure in our problem setting. In the discovery process, we aim at minimising heterogeneity within each subgroup represented by a pattern. We propose a bottom-up search algorithm to discover the most specific patterns fitting individual circumstances the best for personalised decision making. Experiments show that the proposed method models treatment effect heterogeneity better than three other existing tree based methods in synthetic and real world data sets.},
  archive      = {J_APIN},
  author       = {Li, Jiuyong and Liu, Lin and Zhang, Shisheng and Ma, Saisai and Le, Thuc Duy and Liu, Jixue},
  doi          = {10.1007/s10489-022-03860-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8180-8194},
  shortjournal = {Appl. Intell.},
  title        = {Causal heterogeneity discovery by bottom-up pattern search for personalised decision making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fractional-order multiscale attention feature pyramid
network for time series classification. <em>APIN</em>, <em>53</em>(7),
8160–8179. (<a
href="https://doi.org/10.1007/s10489-022-03859-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For time series classification, it is a key problem needed to be solved that the deep learning methods do not consider the relationships between different feature layers in neural networks. Additionally, the deep learning methods are slightly inadequate for the feature-learning ability of single-channel time series data. To address these problems, we propose the fractional-order multiscale attention feature pyramid network(FO-MAFPN) to extract feature sequences of different scales from multichannel time series and fuse features of different levels. First, FO-MAFPN forms multichannel time series using a convolutional layer based on fractional-order to increase sample diversity. Subsequently, it uses a pyramid network based on the multiscale attention mechanism to combine low-level detail and high-level abstract semantic information. This pyramid network adaptively learns channel-attention scores and promotes effective feature sequences while suppressing invalid feature sequences. Finally, FO-MAFPN classifies time series by fusing the feature sequences of a long short-term memory layer with the output of the feature pyramid network. The results of experiments conducted on 85 datasets from the UCR archives indicate the superiority of the FO-MAFPN model for time series classification problems.},
  archive      = {J_APIN},
  author       = {Pan, Wen and Zhang, Weihua and Pu, Yifei},
  doi          = {10.1007/s10489-022-03859-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8160-8179},
  shortjournal = {Appl. Intell.},
  title        = {Fractional-order multiscale attention feature pyramid network for time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). High-efficiency online planning using composite bounds
search under partial observation. <em>APIN</em>, <em>53</em>(7),
8146–8159. (<a
href="https://doi.org/10.1007/s10489-022-03914-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion planning in uncertain environments is a common challenge and essential for autonomous robot operations. Representatively, the determinized sparse partially observable tree (DESPOT) algorithm shows reasonable performance for planning under uncertainty. However, DESPOT may generate a low-quality solution due to inaccurate searches and low efficiencies in the belief tree construction. Therefore, this paper proposes a high-efficiency online planning method built upon the DESPOT algorithm, namely, the DESPOT with discounted upper and lower bounds (DESPOT-DULB) algorithm, to simultaneously improve the efficiency and performance of motion planning. Particularly, the node’s information is represented by combining the upper and lower bounds of the node (ULB) in the forward exploration of the action space to reasonably assist the optimal action selection. Then, a discounted factor based on the depth information of the belief tree is introduced to reduce the gap between the upper bound and lower bound both in the action space and observation space. As a result, the proposed method can comprehensively represent the information of the node to ensure a near-optimal forward search. The theoretical proofs of the proposed method are provided as well. The simulation results, including three representative scenario comparisons and a parameter sensitivity analysis, demonstrate that the proposed method exhibits favorable performances in many examples of interest.},
  archive      = {J_APIN},
  author       = {Chen, Yanjie and Liu, Jiangjiang and Huang, Yibin and Zhang, Hui and Wang, Yaonao},
  doi          = {10.1007/s10489-022-03914-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8146-8159},
  shortjournal = {Appl. Intell.},
  title        = {High-efficiency online planning using composite bounds search under partial observation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning with partial multi-labeled data by leveraging
low-rank constraint and decomposition. <em>APIN</em>, <em>53</em>(7),
8133–8145. (<a
href="https://doi.org/10.1007/s10489-022-03989-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial Multi-label Learning (PML) refers to the task of learning from the noisy data that are annotated with candidate labels but only some of them are valid. To resolve it, the existing methods recover the accurate supervision from candidate labels by estimating the ground-truth confidence, while inducing the prediction model with it. Generally speaking, the performance of PML methods is dominated by the quality of the ground-truth confidence estimation. In this paper, we propose a novel PML method, namely Partial Multi-label Learning with Low-rank Constraint and Decomposition (PML-lcd). Specifically, we not only compute the low-rank approximation of the candidate label matrix, but also decompose the approximation into a low-rank ground-truth confidence matrix and a noisy matrix, i.e., an auxiliary matrix defined to capture noisy irrelevant labels. The objective of PML-lcd can be efficiently solved by alternating direction method of multipliers. Experimental results validate that PML-lcd performs better and more stable than the state-of-the-art baselines with different noise levels.},
  archive      = {J_APIN},
  author       = {Wang, Ying and Guan, Yuanyuan and Wang, Bing and Li, Ximing},
  doi          = {10.1007/s10489-022-03989-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8133-8145},
  shortjournal = {Appl. Intell.},
  title        = {Learning with partial multi-labeled data by leveraging low-rank constraint and decomposition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust infrared and visible image fusion framework via
multi-receptive-field attention and color visual perception.
<em>APIN</em>, <em>53</em>(7), 8114–8132. (<a
href="https://doi.org/10.1007/s10489-022-03952-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a robust infrared and visible image fusion scheme that joins a dual-branch multi-receptive-field neural network and a color vision transfer algorithm is designed to aggregate infrared and visible video sequences. The proposed method enables the fused image to effectively recognize thermal objects, contain rich texture information and ensure visual perception quality. The fusion network is an integrated encoder-decoder modal with a multi-receptive-field attention mechanism that is implemented via hybrid dilated convolution (HDC) and a series of convolution layers to form an unsupervised framework. Specifically, the multi-receptive-field attention mechanism aims to extract comprehensive spatial information to enable the encoder to separately focus on the substantial thermal radiation from the infrared modal and the environmental characteristics from the visible modal. In addition, to ensure that the fused image has rich color, high fidelity and steady brightness, a color vision transfer method is proposed to recolor the fused gray results by deriving a map from the visible image serving as a reference. Extensive experiments verify the importance and robustness of each step in the subjective and objective evaluation and demonstrate that our work represents a trade-off among color fidelity, fusion performance and computational efficiency. Moreover, we will publish our research content, data and code publicly at https://github.com/DZSYUNNAN/RGB-TIR-image-fusion .},
  archive      = {J_APIN},
  author       = {Ding, Zhaisheng and Li, Haiyan and Zhou, Dongming and Liu, Yanyu and Hou, Ruichao},
  doi          = {10.1007/s10489-022-03952-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8114-8132},
  shortjournal = {Appl. Intell.},
  title        = {A robust infrared and visible image fusion framework via multi-receptive-field attention and color visual perception},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining detailed appearance and multi-scale
representation: A structure-context complementary network for human pose
estimation. <em>APIN</em>, <em>53</em>(7), 8097–8113. (<a
href="https://doi.org/10.1007/s10489-022-03909-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is a fundamental and challenging task in the field of computer vision. Hard scenarios, such as occlusion and background confusion, set a great challenge for high-level feature representation because both detailed and multi-scale context must be correctly reasoned. In this paper, we propose a structure-context complementary network (SCC-Net) characterized by the complementarity between a pixel-wise enhanced attention mechanism and atrous convolution-based module. The proposed cross-coordinate attention bottleneck (CCAB) aims to utilize a cross-guide mechanism to promote the robustness of the existing coordinate attention module (CAM) for the background impact. As a complementary module for CCAB, waterfall residual atrous pooling (WRAP) is proposed to refine structure consistency by generating multi-scale features without the feature sparse defect of atrous-based methods. We evaluate our proposed modules and holistic SCC-Net on the COCO and MPII benchmark datasets. Ablation experiments demonstrate that our proposed modules can efficiently boost the performance of body joint detection. Competitive performance is also achieved by our holistic SCC-Net compared to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Dong, Kaiwen and Sun, Yanjing and Cheng, Xiaozhou and Wang, Xiaolin and Wang, Bin},
  doi          = {10.1007/s10489-022-03909-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8097-8113},
  shortjournal = {Appl. Intell.},
  title        = {Combining detailed appearance and multi-scale representation: A structure-context complementary network for human pose estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hostility measure for multi-level study of data complexity.
<em>APIN</em>, <em>53</em>(7), 8073–8096. (<a
href="https://doi.org/10.1007/s10489-022-03793-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complexity measures aim to characterize the underlying complexity of supervised data. These measures tackle factors hindering the performance of Machine Learning (ML) classifiers like overlap, density, linearity, etc. The state-of-the-art has mainly focused on the dataset perspective of complexity, i.e., offering an estimation of the complexity of the whole dataset. Recently, the instance perspective has also been addressed. In this paper, the hostility measure, a complexity measure offering a multi-level (instance, class, and dataset) perspective of data complexity is proposed. The proposal is built by estimating the novel notion of hostility: the difficulty of correctly classifying a point, a class, or a whole dataset given their corresponding neighborhoods. The proposed measure is estimated at the instance level by applying the k-means algorithm in a recursive and hierarchical way, which allows to analyze how points from different classes are naturally grouped together across partitions. The instance information is aggregated to provide complexity knowledge at the class and the dataset levels. The validity of the proposal is evaluated through a variety of experiments dealing with the three perspectives and the corresponding comparative with the state-of-the-art measures. Throughout the experiments, the hostility measure has shown promising results and to be competitive, stable, and robust.},
  archive      = {J_APIN},
  author       = {Lancho, Carmen and Martín De Diego, Isaac and Cuesta, Marina and Aceña, Víctor and Moguerza, Javier M.},
  doi          = {10.1007/s10489-022-03793-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8073-8096},
  shortjournal = {Appl. Intell.},
  title        = {Hostility measure for multi-level study of data complexity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot short-text classification with language
representations and centroid similarity. <em>APIN</em>, <em>53</em>(7),
8061–8072. (<a
href="https://doi.org/10.1007/s10489-022-03880-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of insufficient labelled samples and low-generalization performance in text classification tasks, this paper studies text classification problems under the condition of few labelled samples and proposes a few-shot short-text classification method (Meta-FCS) that combines the advantages of text semantic vector representation, meta-learning, fine-tuning and vector similarity measurement. The method not only effectively transfers the common features of different fields but also highlights the individual features of this field through fine-tuning. In addition, to facilitate the downstream text classification task, a deep language representation model is proposed. On this basis, the similarity between the query set and the class centroid of the support set is compared to determine the query set category. We evaluate the proposed method on a well-studied sentiment classification dataset, an entity-relationship classification dataset and an news topic dataset. The experimental results show that on these three datasets, the proposed method significantly outperforms the existing state-of-the-art approaches. It can thus be further suggested that the combination of deep language representation, episode training mechanism, and similarity measurement can be a promising solution for few-shot learning (FSL) of natural language processing (NLP) tasks.},
  archive      = {J_APIN},
  author       = {Liu, Wenfu and Pang, Jianmin and Li, Nan and Yue, Feng and Liu, Guangming},
  doi          = {10.1007/s10489-022-03880-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8061-8072},
  shortjournal = {Appl. Intell.},
  title        = {Few-shot short-text classification with language representations and centroid similarity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved MLTSVM using label-specific features with
missing labels. <em>APIN</em>, <em>53</em>(7), 8039–8060. (<a
href="https://doi.org/10.1007/s10489-022-03634-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label twin support vector machine (MLTSVM) is an excellent multi-label classification algorithm, which has attracted much attention. Although MLTSVM can effectively solve the multi-label classification problem, it has some drawbacks. a) MLTSVM uses the same feature representation for each label, but in practice, each label has its own specific features. Therefore, MLTSVM might not obtain the optimal classification results. b) In practical applications, there are a large number of samples with missing labels and only a small number of samples with complete labels, because it is expensive to obtain all labels of samples. However, MLTSVM can only use expensive samples with complete labels, not cheap samples with missing labels. For the above drawbacks, we propose an improved MLTSVM using label-specific features with missing labels (LSFML-MLTSVM) in this paper. LSFML-MLTSVM first extracts label-specific features via using semi-supervised clustering analysis and then obtains the structure information of samples and the geometry information of the marginal distribution. Furthermore, in the label-specific feature space, the above two valuable information is introduced into MLTSVM to reconstruct the classification model. Finally, the successive overrelaxation (SOR) algorithm is used to solve the classification model efficiently. Experimental results on benchmark multi-label datasets show that LSFML-MLTSVM has better classification performance.},
  archive      = {J_APIN},
  author       = {Ai, Qing and Li, Fei and Li, Xiangna and Zhao, Ji and Wang, Wenhui and Gao, Qingyun and Zhao, Fei},
  doi          = {10.1007/s10489-022-03634-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8039-8060},
  shortjournal = {Appl. Intell.},
  title        = {An improved MLTSVM using label-specific features with missing labels},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-rebalanced wasserstein distance for multi-source
domain adaptation. <em>APIN</em>, <em>53</em>(7), 8024–8038. (<a
href="https://doi.org/10.1007/s10489-022-03810-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of machine learning, multi-source domain adaptation (MSDA) handles multiple datasets which are collected from different distributions by using domain-invariant knowledge extraction. However, the current studies mainly employ features and raw labels on the joint space to perform domain alignment, neglecting the intrinsic structure of label distribution that can harm the performance of adaptation. Therefore, to make better use of label information when aligning joint feature-label distribution, we propose a rebalancing scheme, class-rebalanced Wasserstein distance (CRWD), for unsupervised MSDA under class-wise imbalance and data correlation. Based on the optimal transport for domain adaptation (OTDA) framework, CRWD mitigates the impact of the biased label structure by rectifying the Wasserstein mapping from source to target space. Technically, the class proportions are utilized to encourage distributional transportation between minor classes and principal components, which reweigh the optimal transport plan and reinforce the ground metric of Mahalanobis distance to better metricise the differences among domains. In addition, the scheme measures both inter-domain and intra-source discrepancies to enhance adaptation. Extensive experiments are conducted on various benchmarks, and the results prove that CRWD has competitive advantages.},
  archive      = {J_APIN},
  author       = {Wang, Qi and Wang, Shengsheng and Wang, Bilin},
  doi          = {10.1007/s10489-022-03810-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8024-8038},
  shortjournal = {Appl. Intell.},
  title        = {Class-rebalanced wasserstein distance for multi-source domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identify influential nodes in network of networks from the
view of weighted information fusion. <em>APIN</em>, <em>53</em>(7),
8005–8023. (<a
href="https://doi.org/10.1007/s10489-022-03856-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network of networks (NONs) is a case of multiplex networks, when mining key nodes in the network, the information between the various sub-networks needs to be considered. In this paper, a weighted information fusion (WIF) method is proposed to identify the influential nodes of NONs. We first divide NONs into many individual networks and then perform weighted fusion. In the process, relevant information of nodes is measured to construct the basic probability assignment (BPA) for every single network. Besides, by considering the topological structure of the network, the method of effective distance is used to describe the weight of each BPA. Finally, to measure the influential nodes of NONs, the information of all single networks is fused to obtain structural information of NONs through WIF method. More than that, the influential nodes of four real-world NONs (including Neuronal and Social two types) are measured by the proposed method, and the results are compared with other five methods, which shows that WIF method is effective in identifying the influence of nodes of NONs.},
  archive      = {J_APIN},
  author       = {Lei, Mingli and Liu, Lirong and Xiao, Fuyuan},
  doi          = {10.1007/s10489-022-03856-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {8005-8023},
  shortjournal = {Appl. Intell.},
  title        = {Identify influential nodes in network of networks from the view of weighted information fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the solution of the graph bandwidth problem by means of
search methods. <em>APIN</em>, <em>53</em>(7), 7988–8004. (<a
href="https://doi.org/10.1007/s10489-022-03802-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Bandwidth Problem is a well-known and important graph layout problem with a large number of applications in scientific and engineering fields. The problem is proved to be NP-complete, and so far, a variety of methods have been proposed for its solution. Among these methods, the most popular ones include search methods, in particular informed search methods. An informed search method normally requires a metric to guide the search toward high-quality solutions. The most frequently used metric in previous studies on the Graph Bandwidth Problem is simply the bandwidth itself, i.e., the most obvious quality measure. In this paper, it is shown that this metric is not always appropriate for comparing the quality of solutions produced by various search methods, and its use may result in a significant reduction in the performance of such methods. In order to address this issue, a new metric is presented, and its effectiveness is verified by a considerable number of numerical experiments on benchmark problems.},
  archive      = {J_APIN},
  author       = {Koohestani, Behrooz},
  doi          = {10.1007/s10489-022-03802-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7988-8004},
  shortjournal = {Appl. Intell.},
  title        = {On the solution of the graph bandwidth problem by means of search methods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Light transformer learning embedding for few-shot
classification with task-based enhancement. <em>APIN</em>,
<em>53</em>(7), 7970–7987. (<a
href="https://doi.org/10.1007/s10489-022-03951-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of the computer vision field is dependent on the large volume of labelled data, and it is a challenge to replicate these successes in real tasks with few labelled data. Fortunately, few-shot learning methods have made many promising attempts on few labelled data. In this paper, we propose a light transformer-based few-shot classification network under the framework of prototypical nets (PN) which has two distinctive hallmarks. First, we provide the local features combined with global features as the sample embedding, where the local features are gained by a CNN encoder and the global features are obtained by light transformer-based global information with a saliency detection structure (LT-GSE) simultaneously. Second, for each task, we use the class approximate degree as prior knowledge to interact with information among query samples at the category level, which makes the low-dimensional embedding space distribution more reasonable. The experimental results show that the proposed model can achieve 82.28% and 86.56% on the 5-way 5-shot classification task of mini ImageNet and tiered ImageNet respectively, which are the best performances of all comparable models. Moreover,few-shot task experiments on the Stanford Dogs and CUB-200 datasets also verify the superiority and robustness of the proposed model.},
  archive      = {J_APIN},
  author       = {Zhu, Hegui and Zhao, Rong and Gao, Zhan and Tang, Qingsong and Jiang, Wuming},
  doi          = {10.1007/s10489-022-03951-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7970-7987},
  shortjournal = {Appl. Intell.},
  title        = {Light transformer learning embedding for few-shot classification with task-based enhancement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGB-d saliency detection via complementary and selective
learning. <em>APIN</em>, <em>53</em>(7), 7957–7969. (<a
href="https://doi.org/10.1007/s10489-022-03612-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous RGB-D saliency detection methods adopt different fusion schemes to fuse the RGB images and depth maps or their saliency maps. However, both the feature maps from different modalities and the different features within the same maps are not of equal importance. To address this problem, We present a new precise RGB-D saliency detection framework in this work that selectively fuses features of different resolutions from two modalities, considering the global location and local detail complementarity. Depth data contains superior position discrimination, which has been shown to enhance saliency prediction. However, errors or missing areas in a depth map or random distribution along an object boundary will introduce negative effect. Therefore, we design a backbone network and an edge detection module that can select useful representations from RGB images and depth maps with attention mechanism and effectively integrate macroscopic and microscopic features from the two modalities. The accurate location of salient objects with fine edge details is realized by cross-modal selective fusion and complementation. We also propose a triple loss function to improve the credibility of the network for hard sample detection. Extensive quantitative and qualitative evaluation experiments on six benchmark datasets show that our method has a superior performance compared with 11 existing state-of-the-art methods with various evaluation metrics.},
  archive      = {J_APIN},
  author       = {Pan, Wenwen and Sun, Xiaofei and Qian, Yunsheng},
  doi          = {10.1007/s10489-022-03612-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7957-7969},
  shortjournal = {Appl. Intell.},
  title        = {RGB-D saliency detection via complementary and selective learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Processing data stream with chunk-similarity model
selection. <em>APIN</em>, <em>53</em>(7), 7931–7956. (<a
href="https://doi.org/10.1007/s10489-022-03826-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of data stream susceptible to the concept drift phenomenon has been a field of intensive research for many years. One of the dominant strategies of the proposed solutions is the application of classifier ensembles with the member classifiers validated on their actual prediction quality. This paper is a proposal of a new ensemble method – Covariance-signature Concept Selector – which, like state-of-the-art solutions, uses both the model accumulation paradigm and the detection of changes in the data posterior probability, but in the integrated procedure. However, instead of ensemble fusion, it performs a static classifier selection, where model similarity assessment to the currently processed data chunk serves as a concept selector. The proposed method was subjected to a series of computer experiments assessing its temporal complexity and efficiency in classifying streams with synthetic and real concepts. The conducted experimental analysis allows concluding the advantage of this proposal over state-of-the-art methods in the identified pool of problems and high potential in practical applications.},
  archive      = {J_APIN},
  author       = {Ksieniewicz, Pawel},
  doi          = {10.1007/s10489-022-03826-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7931-7956},
  shortjournal = {Appl. Intell.},
  title        = {Processing data stream with chunk-similarity model selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian noise robust face hallucination via average
filtering based data fidelity and locality regularization.
<em>APIN</em>, <em>53</em>(7), 7917–7930. (<a
href="https://doi.org/10.1007/s10489-022-03901-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In surveillance scenarios, the captured face images are often of low-resolution and contaminated by Gaussian noise. Noise creates problems in weighted linear representation, an essential process in attaining the reconstruction coefficients through conventional least square representation-based face hallucination models. To address this problem, a new face hallucination framework using average filtering-based data fidelity and locality regularization is proposed in this paper. In the proposed framework, an additional fidelity term is introduced which is accomplished through average filtered input and dictionary images. It helps in minimizing the reconstruction error. Moreover, the average filtering-based similarity matrix is introduced in the regularization term which succors the proposed framework in achieving more accurate locality, a quintessential component for face hallucination. The experimental analysis investigated on widely used public human face databases and locally captured CCTV footages reveal the better performance of the proposed framework over the compared state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Rajput, Shyam Singh},
  doi          = {10.1007/s10489-022-03901-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7917-7930},
  shortjournal = {Appl. Intell.},
  title        = {Gaussian noise robust face hallucination via average filtering based data fidelity and locality regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tri-stage competitive swarm optimizer for constrained
multi-objective optimization. <em>APIN</em>, <em>53</em>(7), 7892–7916.
(<a href="https://doi.org/10.1007/s10489-022-03874-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective optimization and constraint satisfaction should be considered simultaneously when dealing with constrained multi-objective optimization problems (CMOPs). But it is difficult for existing constraint multi-objective evolutionary algorithms (CMOEAs) to strike a good balance between them, especially for CMOPs with complex constraints. To address this issue, this paper proposes a tri-stage competitive swarm optimizer (CSO), namely TSCSO, where objective optimization and constraint satisfaction receive different attention in different stages. In Stage-I, the population converges to the vicinity of the unconstrained Pareto front (PF) without considering any constraints. In Stage-II, a balance strategy and ranking approach based on convergence, diversity, and feasibility are proposed to enhance the diversity of the population and explore more feasible regions. An external archive is used to store the feasible solutions explored during the evolutionary process. In Stage-III, the population is first initialized by the feasible solutions in the archive, and the CSO operator with efficient search is used to search for the feasible regions omitted in Stage-II. Statistical results on two benchmark suites with twenty-eight problems and five real-world problems indicate that the proposed algorithm performs better than other state-of-the-art CMOEAs overall.},
  archive      = {J_APIN},
  author       = {Dong, Jun and Gong, Wenyin and Ming, Fei},
  doi          = {10.1007/s10489-022-03874-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7892-7916},
  shortjournal = {Appl. Intell.},
  title        = {A tri-stage competitive swarm optimizer for constrained multi-objective optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Switching-aware multi-agent deep reinforcement learning for
target interception. <em>APIN</em>, <em>53</em>(7), 7876–7891. (<a
href="https://doi.org/10.1007/s10489-022-03821-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the multi-agent interception problem under switching topology based on deep reinforcement learning. Due to communication restrictions or network attacks, the connectivity between every two intercepting agents may change during the entire tracking process before the successful interception. That is, the topology of the multi-agent system is switched, which leads to a partial missing or dynamic jump of each agent’s observation. To solve this issue, a novel multi-agent level-fusion actor-critic (MALFAC) approach is proposed with a direction assisted (DA) actor and a dimensional pyramid fusion (DPF) critic. Besides, an experience adviser (EA) function is added to the learning process of the actor. Furthermore, a reward factor is proposed to balance the relationship between individual reward and shared reward. Experimental results show that the proposed method performs better than recent algorithms in the multi-agent interception scenarios with switching topologies, which achieves the highest successful interception with the least average steps. The ablation study also verifies the effectiveness of the innovative components in the proposed method. The extensive experimental results demonstrate the scalability of our method in different scenarios.},
  archive      = {J_APIN},
  author       = {Fan, Dongyu and Shen, Haikuo and Dong, Lijing},
  doi          = {10.1007/s10489-022-03821-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7876-7891},
  shortjournal = {Appl. Intell.},
  title        = {Switching-aware multi-agent deep reinforcement learning for target interception},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open-set domain adaptation by deconfounding domain gaps.
<em>APIN</em>, <em>53</em>(7), 7862–7875. (<a
href="https://doi.org/10.1007/s10489-022-03805-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-Set Domain Adaptation (OSDA) aims to adapt the model trained on a source domain to the recognition tasks in a target domain while shielding any distractions caused by open-set classes, i.e., the classes “unknown” to the source model. Compared to standard DA, the key of OSDA lies in the separation between known and unknown classes. Existing OSDA methods often fail the separation because of overlooking the confounders (i.e., the domain gaps), which means their recognition of “unknown classes” is not because of class semantics but domain difference (e.g., styles and contexts). We address this issue by explicitly deconfounding domain gaps (DDP) during class separation and domain adaptation in OSDA. The mechanism of DDP is to transfer domain-related styles and contexts from the target domain to the source domain. It enables the model to recognize a class as known (or unknown) because of the class semantics rather than the confusion caused by spurious styles or contexts. In addition, we propose a module of ensembling multiple transformations (EMT) to produce calibrated recognition scores, i.e., reliable normality scores, for the samples in the target domain. Extensive experiments on two standard benchmarks verify that our proposed method outperforms a wide range of OSDA methods, because of its advanced ability of correctly recognizing unknown classes.},
  archive      = {J_APIN},
  author       = {Zhao, Xin and Wang, Shengsheng and Sun, Qianru},
  doi          = {10.1007/s10489-022-03805-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7862-7875},
  shortjournal = {Appl. Intell.},
  title        = {Open-set domain adaptation by deconfounding domain gaps},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameters auto-tuning for biped robots in whole-body
stabilization and active impedance control applications. <em>APIN</em>,
<em>53</em>(7), 7848–7861. (<a
href="https://doi.org/10.1007/s10489-022-03792-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a parameters auto-tuning strategy for biped locomotion in whole-body stabilization control (inverse kinematics based and inverse dynamics based) and active impedance control based on Bayesian optimization(BO). Using the domain knowledge, the parameter space is divided into three sub-spaces and optimized by decoupling BO and alternating BO algorithms. The effectiveness of the proposed method is demonstrated in simulation using a torque-controlled biped robot that we developed. The 32 control parameters are tuned in less than 400 evaluations. In addition, the auto-tuned parameters are robust to different top-level velocity inputs and show compliant behavior with balance in push recovery scenarios. To the best of our knowledge, this is the first work to automatically tune the parameters of the three controllers (inverse kinematics, inverse dynamics and active impedance control) jointly.},
  archive      = {J_APIN},
  author       = {Li, Jingchao and Yuan, Zhaohui and Dong, Sheng and Kang, Jian and Yang, Pengfei and Zhang, Jianrui and Li, Yingxing},
  doi          = {10.1007/s10489-022-03792-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7848-7861},
  shortjournal = {Appl. Intell.},
  title        = {Parameters auto-tuning for biped robots in whole-body stabilization and active impedance control applications},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Unsupervised few-shot image classification via one-vs-all
contrastive learning. <em>APIN</em>, <em>53</em>(7), 7833–7847. (<a
href="https://doi.org/10.1007/s10489-022-03750-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings innately possess the ability to perceive novel concepts from only a few samples. As a setting to imitate the learned ability of human beings, few-shot image classification (FSIC) has recently aroused a research boom. FSIC aims to distinguish the novel class when given scarce samples. However, most of the existing few-shot methods build on the assumption that an adequate labeled dataset is provided in the source domain, ie, the base class dataset. Due to the expensive burden of labeled samples in the base class, this assumption may not be practical in a real-world application. To solve this labeled burden, in the paper, we propose a novel unsupervised few-shot image classification via One-vs-All contrastive learning. In this approach, to generate positive pairs in each instance, we first adopt a data augmentation technique to build the instance invariance. With the positive pairs, a neural projection with only a fully connected layer is then applied to maintain the structure consistent with the corresponding features of positive pairs. Finally, the One-Vs-All (OVA) contrastive learning is devised to pull one positive pair close while pushing all negative pairs away in a minibatch. By doing this OVA contrastive learning, the model can effectively acquire the discriminative feature and improve the generalization ability to recognize the novel class. We also further develop a theory of the generalized upper bound of the model for the OVA contrastive loss. Our experimental analyses suggest that the proposed approach achieves better performance compared to most existing few-shot methods, and various modules in the approach demonstrate their utility by conducting ablation studies.},
  archive      = {J_APIN},
  author       = {Zheng, Zijun and Feng, Xiang and Yu, Huiqun and Li, Xiuquan and Gao, Mengqi},
  doi          = {10.1007/s10489-022-03750-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7833-7847},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised few-shot image classification via one-vs-all contrastive learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Utilizing enhanced membership functions to improve the
accuracy of a multi-inputs and single-output fuzzy system.
<em>APIN</em>, <em>53</em>(7), 7818–7832. (<a
href="https://doi.org/10.1007/s10489-022-03799-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Primarily the successful implementation of a fuzzy logic system (FLS) depends on some subjective decision-making parameters, for example, membership function (MF). Compared with the input MF, the FLS executes an enhanced output MF to increase the performance, accuracy, and robustness of the FLS. The most suitable relation between input and output MFs is presented to allocate the identical input MFs and enhanced output MFs in the discourse. Various simulations of the 4-Inputs 1-Output FLS are carried out in numerous non-linear processes. Then under similar circumstances, compare the experimental results of identical distribution and enhanced distribution output MFs. Experimental results and simulation results are in a benign contract. Experimental results show that the root mean square error (RMSE) is decreased by about 59.8%, and the relative error is reduced to an acceptable range (≤± 10%). The RMSE is reduced by FLS with enhanced distributed output MFs, which improves control accuracy and improves robustness. In addition, the efficiency of the cost and energy of any FLS will be enhanced by using the most suitable relation of input and output MFs to achieve enhanced distributed output MFs.},
  archive      = {J_APIN},
  author       = {Khokhar, Salah-ud-din and Peng, QinKe},
  doi          = {10.1007/s10489-022-03799-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7818-7832},
  shortjournal = {Appl. Intell.},
  title        = {Utilizing enhanced membership functions to improve the accuracy of a multi-inputs and single-output fuzzy system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triangular mutation-based manta-ray foraging optimization
and orthogonal learning for global optimization and engineering
problems. <em>APIN</em>, <em>53</em>(7), 7788–7817. (<a
href="https://doi.org/10.1007/s10489-022-03899-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trapping in local solutions is the main issue in several metaheuristic techniques. To solve such drawbacks by enhancing the search agents, a modified search strategy becomes a more attractive tactic. In this paper, an innovative version of Manta Ray Foraging Optimization (MRFO) is proposed to solve its crucial drawbacks while handling global and engineering optimization problems. The proposed version presents an integrated variant of MRFO with the triangular mutation operator and orthogonal learning strategy, called MRTMO. The two approaches are considered to achieve a robust equipoise between algorithm cores and provide a reliable mechanism to guide the search agents during the optimization process. The proposed MRTMO was tested with challenging CEC2005 and CEC2017 functions and six engineering problems to show its performance. Additionally, several evaluation metrics were employed to ensure the efficiency and robustness of the proposed MRTMO. Furthermore, extensive comparisons with existing optimization algorithms were carried out to ensure the superiority of MRTMO. The numerical experiments proved the competitive performance of the proposed MRTMO in solving all tested CEC optimization and engineering problems.},
  archive      = {J_APIN},
  author       = {Elaziz, Mohamed Abd and Abualigah, Laith and Ewees, Ahmed A and Al-qaness, Mohammed AA and Mostafa, Reham R and Yousri, Dalia and Ibrahim, Rehab Ali},
  doi          = {10.1007/s10489-022-03899-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7788-7817},
  shortjournal = {Appl. Intell.},
  title        = {Triangular mutation-based manta-ray foraging optimization and orthogonal learning for global optimization and engineering problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-relational attention network for vehicle
re-identification. <em>APIN</em>, <em>53</em>(7), 7776–7787. (<a
href="https://doi.org/10.1007/s10489-022-03801-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle re-identification task aims to retrieve the same vehicle from multiple cameras. One of the challenges of this task is that images of different vehicles may look very similar under the same perspective, which affects the performance of vehicle re-identification tasks. Recent studies have shown that the attention mechanism is effective for vehicle re-identification, because it encourages models to focus on information related to the identity of the target rather than interference information such as background. In this paper, we first propose a dual-relational attention module (DRAM), which simultaneously constructs the importance of a feature point in the spatial and channel dimensions, and further models the attention of the feature point in the three-dimensional space. Then, we integrate the dual-relational attention module into a three-branch network, called dual-relational attention network (DRA-Net). In addition, to ensure that the network can extract a large number of diverse information, we add a non-similarity constraint in dual-relational attention network (DRA-Net) to make different branches focus on different locations in an image. In the experiments, our method is verified on two datasets (VeRi-776 and VehicleID), and the experimental results indicate that the proposed method is superior to several advanced methods.},
  archive      = {J_APIN},
  author       = {Zheng, Yanli and Pang, Xiyu and Jiang, Gangwu and Tian, Xin and Meng, Qinglan},
  doi          = {10.1007/s10489-022-03801-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7776-7787},
  shortjournal = {Appl. Intell.},
  title        = {Dual-relational attention network for vehicle re-identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a software tool for general meal optimisation.
<em>APIN</em>, <em>53</em>(7), 7751–7775. (<a
href="https://doi.org/10.1007/s10489-022-03935-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The following work presents a software solution capable of designing general meal plans which approach an optimal match of nutritional characteristics submitted by the user. A thorough review of existing literature indicates the absence of a software solution to this problem in its most general form. Existing solutions tend to address particular forms of the problem in specific contexts, for example, optimising culturally typical diets in response to specific medical conditions. Conversely, this work focuses on developing a nutritional software model with sufficient flexibility to be described as general, paired with a simple, specifically designed optimisation algorithm for working with the proposed prototype system. The resulting software tool can express the following characteristics: arbitrary nutritional content; economic characteristics; binary food-type classifications (e.g. vegetarian); and, because of the optimisation framework, can capture goals for any number of meals; meal composition (combinations of recipes for a given meal at a particular time); a maximum economic cost per meal; and nutritional content within each meal. The work outlines a prototype user interface to enable nutritional data and user goals to be entered and validated. Finally, based on ten specific test problems containing varied dietary goals, a basic algorithm tuning approach is described. The results suggest that the proposed prototype system can address the general meal optimisation problem. There is a discussion of several future developments to improve system capabilities and usability further.},
  archive      = {J_APIN},
  author       = {Izzard, James and Caraffini, Fabio and Chiclana, Francisco},
  doi          = {10.1007/s10489-022-03935-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7751-7775},
  shortjournal = {Appl. Intell.},
  title        = {Towards a software tool for general meal optimisation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View-relation constrained global representation learning for
multi-view-based 3D object recognition. <em>APIN</em>, <em>53</em>(7),
7741–7750. (<a
href="https://doi.org/10.1007/s10489-022-03949-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view observations provide complementary clues for 3D object recognition, but also include redundant information that appears different across views due to view-dependent projection, light reflection and self-occlusions. This paper presents a view-relation constrained global representation network (VCGR-Net) for 3D object recognition that can mitigate the view interference problem at all phases, from view-level source feature generation to multi-view feature aggregation. Specifically, we determine inter-view relations via LSTM implicitly. Based on the relations, we construct a two-stage feature selection module to filter features at each view according to their importance to the global representation and their reliability as observations at specific views. The selected features are then aggregated by referring to intra- and inter-view spatial context to generate global representation for 3D object recognition. Experiments on the ModelNet40 and ModelNet10 datasets demonstrate that the proposed method can suppress view interference and therefore outperform state-of-the-art methods in 3D object recognition.},
  archive      = {J_APIN},
  author       = {Xu, Ruchang and Mi, Qing and Ma, Wei and Zha, Hongbin},
  doi          = {10.1007/s10489-022-03949-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7741-7750},
  shortjournal = {Appl. Intell.},
  title        = {View-relation constrained global representation learning for multi-view-based 3D object recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent abnormal behavior detection using double
sparseness method. <em>APIN</em>, <em>53</em>(7), 7728–7740. (<a
href="https://doi.org/10.1007/s10489-022-03903-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent detection of abnormal behaviors meets the need of engineering applications for identifying anomalies and alerting operators. However, most existing methods tackle the high-dimensional sequential video data with key frame extraction, which ignore the redundancy effect of inter- and intra- video frames. In this paper, a novel Abnormal Detection method based on double sparseness LSSVMoc (AD_LSSVMoc) is proposed, which combine both sample (i.e. frame) selection and feature selection simultaneously in a uniform sparse model. For the feature extraction, both handcrafted features and learned features are aggregated into effective descriptors. To achieve feature selection and sample selection, a improved LSSVMoc is proposed with sparse primal and dual optimization strategy, and alternating direction method of multipliers is used to solve the constrained linear equations problem raised in AD_LSSVMoc. Experiments show that the proposed AD_LSSVMoc method achieves a competitive detection performance and high detecting speed compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Mu, Huiyu and Sun, Ruizhi and Chen, Zeqiu and Qin, Jia},
  doi          = {10.1007/s10489-022-03903-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7728-7740},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent abnormal behavior detection using double sparseness method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual local learning regularized NMF with sparse and
orthogonal constraints. <em>APIN</em>, <em>53</em>(7), 7713–7727. (<a
href="https://doi.org/10.1007/s10489-022-03881-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) has shown remarkable competitiveness in the past few years. To fully exploit various known prior knowledge hidden in data, this paper proposes a dual local learning regularized NMF with sparse and orthogonal constraints (DLLNMF-SO) algorithm. DLLNMF-SO constructs two local learning regularizers to consider the geometric structure and discriminative information embedded in data and feature space, respectively. Besides, it makes full use of sparse self-representation information by adding the l2,1-norm constraint. Meanwhile, the orthogonal constraint is imposed on the basis vectors to preserve the correspondence between samples and basic vectors. We give an efficient iterative updating scheme for the optimization problem of DLLNMF-SO and provides its convergence guarantee. We demonstrate that our proposed approach outperforms other competitors by conducting serval experiments on three benchmark datasets.},
  archive      = {J_APIN},
  author       = {Shu, Zhenqiu and Zuo, Furong and Wu, Wenli and You, Congzhe},
  doi          = {10.1007/s10489-022-03881-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7713-7727},
  shortjournal = {Appl. Intell.},
  title        = {Dual local learning regularized NMF with sparse and orthogonal constraints},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning background-aware and spatial-temporal regularized
correlation filters for visual tracking. <em>APIN</em>, <em>53</em>(7),
7697–7712. (<a
href="https://doi.org/10.1007/s10489-022-03868-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual tracking, correlation Filters (CFs) have attracted increasing research attention and achieved superior performance. However, owing to the larger search area, more background information is introduced to the shifted samples, meaning that tracking errors are prone to appear in the detection stage. Accordingly, in this work, firstly, hand-crafted features and deep features extracted from pre-trained convolutional networks are combined to improve the representation ability of object appearance. For deep features, we use two different VGG networks for extraction. Secondly, in an attempt to solve the problem of the object background of the traditional CF model not being modeled over time, and owing to the lack of spatial-temporal information of the image, we propose a new background-aware and spatial-temporal regularized correlation filters model (BSTCF) that introduces the background constraint and spatial-temporal regularization. The proposed BSTCF can effectively model not only the background but also variations in the background over time. Finally, we transform the objective function of BSTCF into an unconstrained Augmented Lagrange multiplier formular to promote convergence to the global optimum solution. Moreover, we adopt the alternating direction multiplier method (ADMM) to produce three sub-problems with closed-form solution, then propose a corresponding algorithm. Based on the above, we construct an intelligent tracking system and carry out extensive experiments to test its performance on OTB-2013, OTB-2015, TC128, UAV123, and VOT2016 public datasets. The experimental results demonstrate that the tracking algorithm achieves superior performance.},
  archive      = {J_APIN},
  author       = {Zhang, Jianming and He, Yaoqi and Feng, Wenjun and Wang, Jin and Xiong, Neal N.},
  doi          = {10.1007/s10489-022-03868-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7697-7712},
  shortjournal = {Appl. Intell.},
  title        = {Learning background-aware and spatial-temporal regularized correlation filters for visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEMG-based upper limb motion recognition using improved
sparrow search algorithm. <em>APIN</em>, <em>53</em>(7), 7677–7696. (<a
href="https://doi.org/10.1007/s10489-022-03824-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of motion intention recognition of patients has become one of the key directions in the current research on human-machine coordination control of rehabilitation robots. To improve the accuracy of motion intention recognition and shorten the recognition time, in this work, an improved sparrow search algorithm based on multi-strategy (MSISSA) is designed for improving the prediction performance of the classification algorithm for motion pattern recognition of human upper limbs based on 4-channel sEMG signals. For the poor quality of the initial solution of the population, elite initial solutions are defined by an opposition-based learning strategy to enhance the diversity and traversal of the population. Due to the lack of effective step size control and variation mechanism in the iterative process, a nonlinear exponential decreasing strategy is proposed to balance the global search and local exploitation ability of the algorithm, and a vertical and horizontal crossover strategy is introduced after the individual position update in the population to improve the ability of the algorithm to jump out of the local optimum. The cross-sectional and longitudinal experiments show that the proposed MSISSA algorithm has certain advantages in terms of convergence speed, solution accuracy and robustness, and the classifier optimized based on the MSISSA algorithm has a 2.835% improvement in the accuracy of sEMG signal recognition compared with the original classifier, which is of positive significance for the application in acquiring patient intention for robot-assisted rehabilitation motions.},
  archive      = {J_APIN},
  author       = {Chen, Peng and Wang, Hongbo and Yan, Hao and Du, Jiazheng and Ning, Yuansheng and Wei, Jian},
  doi          = {10.1007/s10489-022-03824-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7677-7696},
  shortjournal = {Appl. Intell.},
  title        = {SEMG-based upper limb motion recognition using improved sparrow search algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel chinese relation extraction method using polysemy
rethinking mechanism. <em>APIN</em>, <em>53</em>(7), 7665–7676. (<a
href="https://doi.org/10.1007/s10489-022-03817-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The methods of Chinese relation extraction(CRE) based on the neural network can be divided into two categories according to the input mode(word-based and character-based). The performance of word-based models depends on the accuracy of word segmentation. Unfortunately, there are still errors in existing word segmentation tools (methods). Among the character-based models, Lattice LSTM-based models have been successful in CRE. However, such RNN-based models cannot meet the requirements of parallel computing and thus have natural drawbacks in model training and inference. There is much word polysemy in Chinese that constrains the performance of CRE. Most CRE models are built on English datasets, which often perform poorly on Chinese datasets. To address the above issues, we propose a method for CRE with the Polysemy Rethinking Mechanism. In this method, (1) we use a CNN-based architecture in which input is characters. It can incorporate word-level information through the lexicon to correct the error caused by word segmentation. (2) We propose a Polysemy Rethinking Mechanism, which can alleviate the problems caused by multiple meanings of one word by adding multiple sense information to the model. (3) Compared with the Lattice LSTM-based model, our model improves computational efficiency to gain results. We conduct experiments on two real-world datasets of CRE. The results show that our method achieves better performance than the state-of-the-art ones.},
  archive      = {J_APIN},
  author       = {Zhao, Qihui and Gao, Tianhan and Guo, Nan},
  doi          = {10.1007/s10489-022-03817-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7665-7676},
  shortjournal = {Appl. Intell.},
  title        = {A novel chinese relation extraction method using polysemy rethinking mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multichannel embedding and arithmetic optimized stacked
bi-GRU model with semantic attention to detect emotion over text data.
<em>APIN</em>, <em>53</em>(7), 7647–7664. (<a
href="https://doi.org/10.1007/s10489-022-03907-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion analysis of social media data is essential for understanding a person’s mindset and judgment on a particular event. The emotion detection framework needs sophisticated deep learning models such as Long short-term memory networks and convolutional neural networks (CNN) for their development. These models did not give adequate emotional information in targeted emotion analysis. This paper proposes a new deep learning-based emotion detection framework using an optimized, stacked, bidirectional Gated Recurrent Unit with sematic attention (SRBi-GRU-SA). A multichannel word embedding model is proposed for representing the text document by combining the emotional information with the semantic information obtained from a natural language model (BERT), Term-frequency inverse gravity moment (TF-IGM) based term weighted word embedding model with trigrams. The proposed SRBi-GRU-SA includes semantic attention (SA) model for highlighting the most significant features based on attention score. Also, an Arithmetic Optimization Algorithm (AOA) is introduced to optimize the weight parameters of the SRBi-GRU-SA model. The evaluation results of the proposed optimized SRBi-GRU-SA model prove its effectiveness against the existing models by increasing the F1 score to 84.51%, 85.93%, 85.25%, and 87.12% on International Survey on Emotion Antecedents and Reactions (ISEAR), Stance Sentiment Emotion Corpus (SSEC), Daily dialogs, and Grounded-Emotions datasets respectively},
  archive      = {J_APIN},
  author       = {Pradhan, Anima and Ranjan Senapati, Manas and Sahu, Pradip Kumar},
  doi          = {10.1007/s10489-022-03907-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7647-7664},
  shortjournal = {Appl. Intell.},
  title        = {A multichannel embedding and arithmetic optimized stacked bi-GRU model with semantic attention to detect emotion over text data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge inference model for question answering on an
incomplete knowledge graph. <em>APIN</em>, <em>53</em>(7), 7634–7646.
(<a href="https://doi.org/10.1007/s10489-022-03927-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering over knowledge graph (KGQA) is a task to solve natural language questions on knowledge graphs (KGs). Multi-hop KGQA requires multi-steps reasoning on the KG to find the correct answers to complex questions. However, it is difficult to find the triple required by the question directly when solving complex multi-hop questions for KGs with missing links. To mitigate this challenge, we propose an effective reasoning model that fuses neighbor interaction and a relation recognition module for multi-hop QA. Specifically, we adopt neighbor interaction networks to learn a better entity representation. The model identifies the relations contained in the questions through neural networks to further precisely determine the range of answers. Our method selectively captures the complex hidden information within the KG and overcomes the limitation of the answer range. It can perform well without the help of additional text corpora. The experimental results on two datasets show that our model can effectively capture richer semantic information for reasoning and achieve better results than all baseline models.},
  archive      = {J_APIN},
  author       = {Guo, Qimeng and Wang, Xue and Zhu, Zhenfang and Liu, Peiyu and Xu, Liancheng},
  doi          = {10.1007/s10489-022-03927-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7634-7646},
  shortjournal = {Appl. Intell.},
  title        = {A knowledge inference model for question answering on an incomplete knowledge graph},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A cell phone app for facial acne severity assessment.
<em>APIN</em>, <em>53</em>(7), 7614–7633. (<a
href="https://doi.org/10.1007/s10489-022-03774-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acne vulgaris, the most common skin disease, can cause substantial economic and psychological impacts to the people it affects, and its accurate grading plays a crucial role in the treatment of patients. In this paper, we firstly proposed an acne grading criterion that considers lesion classifications and a metric for producing accurate severity ratings. Due to similar appearance of acne lesions with comparable severities and difficult-to-count lesions, severity assessment is a challenging task. We cropped facial skin images of several lesion patches and then addressed the acne lesion with a lightweight acne regular network (Acne-RegNet). Acne-RegNet was built by using a median filter and histogram equalization to improve image quality, a channel attention mechanism to boost the representational power of network, a region-based focal loss to handle classification imbalances and a model pruning and feature-based knowledge distillation to reduce model size. After the application of Acne-RegNet, the severity score is calculated, and the acne grading is further optimized by the metadata of the patients. The entire acne assessment procedure was deployed to a mobile device, and a phone app was designed. Compared with state-of-the-art lightweight models, the proposed Acne-RegNet significantly improves the accuracy of lesion classifications. The acne app demonstrated promising results in severity assessments (accuracy: 94.56%) and showed a dermatologist-level diagnosis on the internal clinical dataset.The proposed acne app could be a useful adjunct to assess acne severity in clinical practice and it enables anyone with a smartphone to immediately assess acne, anywhere and anytime.},
  archive      = {J_APIN},
  author       = {Wang, Jiaoju and Luo, Yan and Wang, Zheng and Hounye, Alphonse Houssou and Cao, Cong and Hou, Muzhou and Zhang, Jianglin},
  doi          = {10.1007/s10489-022-03774-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7614-7633},
  shortjournal = {Appl. Intell.},
  title        = {A cell phone app for facial acne severity assessment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting PSO-SVM and sample entropy in BEMD for the
prediction of interval-valued time series and its application to daily
PM2.5 concentration forecasting. <em>APIN</em>, <em>53</em>(7),
7599–7613. (<a
href="https://doi.org/10.1007/s10489-022-03835-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the serious harm to human health caused by atmospheric fine particulate matter (PM2.5), accurate prediction of high concentrations of PM2.5 can help to provide timely warnings. On the other hand, due to the complexity of the formation and transmission process, it is difficult to accurately predict PM2.5. The aim of this paper is to develop a hybrid interval-valued time series prediction model, namely, BEMDCR-SE-PSO-SVM, by considering daily changes in pollutant concentrations and thereby realize interval-valued PM2.5 concentration prediction with high accuracy. The theoretical contributions in this paper include (1) the problem of edge effects corresponding to BEMD associated with interval-valued time-series is addressed by using the mirror extension method, and (2) the transformation between interval-valued time series and complex-valued signals is renewed from the perspective of centre/radius so that lower data fluctuations can be obtained. Technologically, sample entropy is introduced to provide an objective way to integrate decomposed similar IMFs so that subsequent prediction processes can be simplified. Finally, a numerical example is shown to illustrate the feasibility and validity of the developed hybrid interval-valued time series prediction model.},
  archive      = {J_APIN},
  author       = {Jiang, Liyuan and Tao, Zhifu and Zhu, Jiaming and Zhang, Junting and Chen, Huayou},
  doi          = {10.1007/s10489-022-03835-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7599-7613},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting PSO-SVM and sample entropy in BEMD for the prediction of interval-valued time series and its application to daily PM2.5 concentration forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental tree-based successive POI recommendation in
location-based social networks. <em>APIN</em>, <em>53</em>(7),
7562–7598. (<a
href="https://doi.org/10.1007/s10489-022-03842-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a sequential rules-based recommendation system, called STS-Rec. It addresses the main drawbacks of sequential patterns mining approaches for POI (Point of interest) recommendation by considering both temporal and social influences to perform short-term recommendations. STS-Rec first transforms mobility data into location sequences. Then, it incrementally mines sequential recommendation rules in these sequences. In contrast with standard sequential recommenders, the proposal (1) discovers rules that tolerate locations’ order variations by loosening the strict ordering constraint of location sequences, (2) builds a tree-based model to incrementally mine recommendation rules, and (3) supports short and long-term POI recommendation by using a user-defined window by extracting patterns that appear within a maximum number of consecutive locations. To take the temporal influence into account, STS-Rec adapts its mining strategy to include the temporal context in location data. Hence, the conventional rule mining problem is redefined to mine time-extended recommendation rules. An experimental evaluation conducted on two large-scale real check-in datasets from Gowalla and Brightkite shows that the proposed model outperforms two state-of-the-art sequential models in terms of accuracy and coverage.},
  archive      = {J_APIN},
  author       = {Amirat, Hanane and Lagraa, Nasreddine and Fournier-Viger, Philippe and Ouinten, Youcef and Kherfi, Mohammed Lamine and Guellouma, Younes},
  doi          = {10.1007/s10489-022-03842-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7562-7598},
  shortjournal = {Appl. Intell.},
  title        = {Incremental tree-based successive POI recommendation in location-based social networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new hybrid localization approach in wireless sensor
networks based on particle swarm optimization and tabu search.
<em>APIN</em>, <em>53</em>(7), 7546–7561. (<a
href="https://doi.org/10.1007/s10489-022-03872-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many works have proposed solutions for indoor localization in Wireless Sensor Networks (WSN). The challenge in these different works is above all to improve localization accuracy. New trends in the field are the use of optimization techniques to improve the accuracy in determining the location of a sensor. Thus, this study aims to propose a new contribution to the indoor localization problem in WSN based on optimization techniques. The designed approach improves the performance of particle swarm optimization (PSO). In this improved version of PSO, on the one hand, a form of tabu search is used by each particle to determine its best local neighbor in order to accelerate its possibilities of convergence towards a better solution. On the other hand, limit and performance checks are introduced into the PSO algorithm to evolve only with better particles belonging to the search space constructed by constraint analysis, around an initial solution obtained by trilateration. This proposed approach called FPSOTS uses the received signal strength indicator (RSSI) method to evaluate inter-sensor distances. Localization accuracy and convergence performances of the FPSOTS approach were evaluated in simulation and compared with other recent localization approaches based on optimization techniques. Results show that FPSOTS succeeds in locating unknown nodes of a WSN with fast convergence and better accuracy than recent state-of-the-art approaches such as HPSOVNS, NS-IPSO, ECS-NL and GTOA. Indeed, in comparison with these four approaches, the accuracy of FPSOTS approach was better by 40%, 35%, 44% and 22% respectively.},
  archive      = {J_APIN},
  author       = {Tagne Fute, Elie and Nyabeye Pangop, Doris-Khöler and Tonye, Emmanuel},
  doi          = {10.1007/s10489-022-03872-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7546-7561},
  shortjournal = {Appl. Intell.},
  title        = {A new hybrid localization approach in wireless sensor networks based on particle swarm optimization and tabu search},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On threshold based approximations of mf-rough sets.
<em>APIN</em>, <em>53</em>(7), 7528–7545. (<a
href="https://doi.org/10.1007/s10489-022-03784-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new notion of upper/lower approximations using rough membership functions. These are defined with respect to some thresholds and their algebraic properties are studied. A game theoretic method is suggested to determine the thresholds. A comparison is made between present notions with the other threshold-based approximations extant in rough set theory. The comparison suggests a possible application of the new concepts.},
  archive      = {J_APIN},
  author       = {Majumder, Sandip and Kar, Samarjit and Chakraborty, Mihir Kr},
  doi          = {10.1007/s10489-022-03784-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7528-7545},
  shortjournal = {Appl. Intell.},
  title        = {On threshold based approximations of mf-rough sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepFake detection algorithm based on improved vision
transformer. <em>APIN</em>, <em>53</em>(7), 7512–7527. (<a
href="https://doi.org/10.1007/s10489-022-03867-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A DeepFake is a manipulated video made with generative deep learning technologies, such as generative adversarial networks or auto encoders that anyone can utilize. With the increase in DeepFakes, classifiers consisting of convolutional neural networks (CNN) that can distinguish them have been actively created. However, CNNs have a problem with overfitting and cannot consider the relation between local regions as global feature of image, resulting in misclassification. In this paper, we propose an efficient vision transformer model for DeepFake detection to extract both local and global features. We combine vector-concatenated CNN feature and patch-based positioning to interact with all positions to specify the artifact region. For the distillation token, the logit is trained using binary cross entropy through the sigmoid function. By adding this distillation, the proposed model is generalized to improve performance. From experiments, the proposed model outperforms the SOTA model by 0.006 AUC and 0.013 f1 score on the DFDC test dataset. For 2,500 fake videos, the proposed model correctly predicts 2,313 as fake, whereas the SOTA model predicts 2,276 in the best performance. With the ensemble method, the proposed model outperformed the SOTA model by 0.01 AUC. For Celeb-DF (v2) dataset, the proposed model achieves a high performance of 0.993 AUC and 0.978 f1 score, respectively.},
  archive      = {J_APIN},
  author       = {Heo, Young-Jin and Yeo, Woon-Ha and Kim, Byung-Gyu},
  doi          = {10.1007/s10489-022-03867-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7512-7527},
  shortjournal = {Appl. Intell.},
  title        = {DeepFake detection algorithm based on improved vision transformer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MRS-net: An image inpainting algorithm with multi-scale
residual attention fusion. <em>APIN</em>, <em>53</em>(7), 7497–7511. (<a
href="https://doi.org/10.1007/s10489-022-03866-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image repair is to repair partially damaged images. At present, when repairing the images with arbitrary missing shape and the images with large defect area, the current methods have some problems, such as fuzzy image repair and differences in the repaired joints. Therefore, this paper proposes an image repair model of MRS-Net(Multiscale Residual Squeeze-and-congestion Networks) by using the generated countermeasure network to repair the images with arbitrary missing shape and large defect area. The generator adds a residual attention module in the connection layer between the encoder and the decoder to improve the repair ability of the model, and generates a network through multi-scale joint feedback against loss, reconstruction loss, perception loss, style loss and total variation loss, so as to ensure the visual consistency between the repair boundary and the surrounding real image. At the same time, the binary cross entropy loss feedback discriminant network is used. The proposed model is trained and tested on the dataset CelebA and Oxford buildings. Experiments show that the proposed model can effectively extract the missing information, Meanwhile, the repair results have natural transition boundaries and clear details. MRS-Net can improve SSIM(structural similarity) index by about 2% - 5% and PSNR(peak signal-to-noise ratio)index by about 1-3. The FID(Frechet Inception Distance score) straight index is reduced by 2-8, and the evaluation indexes are improved.when the missing area accounts for 5% - 10%, 11% - 20%, 21% - 30%, 31% - 40% and 41% - 50%,MRS-Net has better image restoration effect, which can reflect that MRS-Net has better robustness.The proposed image restoration method can repair faces, buildings and other scenes. At the same time, it can repair defects in different shapes and areas.},
  archive      = {J_APIN},
  author       = {Deng, Hongxia and Qian, Guanyu and Luo, Dongsheng and Lv, Xindong and Liu, Haoqi and Li, Haifang},
  doi          = {10.1007/s10489-022-03866-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7497-7511},
  shortjournal = {Appl. Intell.},
  title        = {MRS-net: An image inpainting algorithm with multi-scale residual attention fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge enhanced zero-resource machine translation using
image-pivoting. <em>APIN</em>, <em>53</em>(7), 7484–7496. (<a
href="https://doi.org/10.1007/s10489-022-03997-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero resource machine translation usually means that there are no parallel corpora in the training of machine translation models, which can be solved with the help of extra information such as images. However, the ambiguity in the text, together with the irrelevant information in images, may cause the problem of translation errors of some key words. In order to alleviate the problem of image-text alignment deviation caused by word ambiguity, we introduce knowledge entities as an extra modality for the source language to enhance the representations of the source text to clarify its semantics. Specifically, we use additional multi-modal information including images and knowledge entities as an auxiliary hint for the source text in the Transformer-based zero-resource translation framework. We also solve the problem of the structural difference between the training and inference stages to handle the cases where there is no longer visual information in the inference stage. The proposed method achieves state-of-the-art BLEU scores in the field of zero-resource machine translation with the image as the pivot.},
  archive      = {J_APIN},
  author       = {Huang, Ping and Zhao, Jing and Sun, Shilinag and Lin, Yichu},
  doi          = {10.1007/s10489-022-03997-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7484-7496},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge enhanced zero-resource machine translation using image-pivoting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal resonant graph network for representation learning
on dynamic graphs. <em>APIN</em>, <em>53</em>(7), 7466–7483. (<a
href="https://doi.org/10.1007/s10489-022-03919-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating dynamic node representation via limited samples at a time is a challenging task. Meanwhile, the human brain can quickly capture dynamic features without much information in a changing environment. Motivated by the idea of brain functions from the adaptive resonance theory (ART), in this paper, we designed and built a dynamic model for node representation generation. Based on the data flow of the ART model, we replaced the classic ART components with deep learnable modules. Our proposed method is suitable for capturing different graph evolution events even with very sparse changes. Experimental results on several synthetic and real-world dynamic graphs for many vital applications show the superiority of our proposed method.},
  archive      = {J_APIN},
  author       = {Yin, Zidu and Yue, Kun},
  doi          = {10.1007/s10489-022-03919-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7466-7483},
  shortjournal = {Appl. Intell.},
  title        = {Temporal resonant graph network for representation learning on dynamic graphs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiscale convolutional gragh network using only
structural information for entity alignment. <em>APIN</em>,
<em>53</em>(7), 7455–7465. (<a
href="https://doi.org/10.1007/s10489-022-03916-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential method of knowledge graph fusion, entity alignment aims to find entities that refer to the same real-world object in different knowledge graphs. Current entity alignment methods usually adopt extra information such as entity names, attribute triples except for structural information of the knowledge graph. However, due to the difficult availability and possibly low effectiveness of extra information, it is necessary to improve the performance of entity alignment when using only structural information. In this paper, a novel entity alignment method based on the multiscale convolutional graph network (MCEA) is proposed, which utilizes only structural information of the graph for entity alignment. Firstly, the convolution region of long-tail entities is extended to enhance the ability of information capture of the graph network. Secondly, intermediate results from semi-supervised learning are introduced to negative sampling in order to improve sampling quality. Thirdly, the stable marriage algorithm is chosen as the alignment strategy to obtain final alignment results. The experimental results show that this method has achieved better performance on Hits@K and MRR than the state-of-the-art methods, especially in the case of less labeled data. Moreover, we also find that the impact of the alignment strategy has become limited when the model generates sufficient accurate entity embeddings.},
  archive      = {J_APIN},
  author       = {Qi, Donglin and Chen, Shudong and Sun, Xiao and Luan, Ruipeng and Tong, Da},
  doi          = {10.1007/s10489-022-03916-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7455-7465},
  shortjournal = {Appl. Intell.},
  title        = {A multiscale convolutional gragh network using only structural information for entity alignment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A partial order framework for incomplete data clustering.
<em>APIN</em>, <em>53</em>(7), 7439–7454. (<a
href="https://doi.org/10.1007/s10489-022-03887-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose in this paper a partial order framework for clustering incomplete data. The paramount feature of this framework is that it spans over a partial order that can be leveraged to establish data similarity. We present the underlying theoretical foundations and study the convergence of clustering algorithms in this framework. In addition, we present a partial order-based clustering algorithm (POK-means) that illustrates the embedding of K-means clustering algorithm in our framework. The first contribution of our method is that unlike methods based on imputation of the missing values, our method does not make any assumptions about missing data. Another important contribution is that it alleviates false dismissals caused by other interval-based similarity measures. The experimental results show that, although our method do not assume any prior knowledge of (or assumptions about) missing data, it is competitive to most of published incomplete data clustering methods that are based on assumptions about input data or imputation (e.g. methods based on partial or interval kernel distances) in accuracy and performance.},
  archive      = {J_APIN},
  author       = {Yahyaoui, Hamdi and AboElfotoh, Hosam and Shu, Yanjun},
  doi          = {10.1007/s10489-022-03887-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7439-7454},
  shortjournal = {Appl. Intell.},
  title        = {A partial order framework for incomplete data clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A reference vector adaptive strategy for balancing
diversity and convergence in many-objective evolutionary algorithms.
<em>APIN</em>, <em>53</em>(7), 7423–7438. (<a
href="https://doi.org/10.1007/s10489-022-03545-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition-based multiobjective evolutionary algorithms (MOEA/D) have achieved great success in the field of evolutionary multiobjective optimization, and their outstanding performance in solving for the Pareto-optimal solution set has attracted attention. This type of algorithm uses reference vectors to decompose the multiobjective problem into multiple single-objective problems and searches them collaboratively, hence the choice of reference vectors is particularly important. However, predefined reference vectors may not be suitable for dealing with many-objective optimization problems with complex Pareto fronts (PFs), which can affect the performance of MOEA/D. To solve this problem, we introduce a reference vector initialization strategy, namely, scaling of the reference vectors (SRV), and also propose a new reference vector adaptation strategy, that is, transformation of the solution positions (TSP) based on the ideal point solution, to deal with irregular PFs. The TSP strategy can adaptively redistribute the reference vectors through periodic adjustment to endow that the solution set with better convergence and a better distribution. Both strategies are introduced into a representative MOEA/D, called 𝜃-DEA-TSP, which is compared with five state-of-the-art algorithms to verify the effectiveness of the proposed TSP strategy.},
  archive      = {J_APIN},
  author       = {Zhang, Lin and Wang, Liping and Pan, Xiaotian and Qiu, Qicang},
  doi          = {10.1007/s10489-022-03545-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7423-7438},
  shortjournal = {Appl. Intell.},
  title        = {A reference vector adaptive strategy for balancing diversity and convergence in many-objective evolutionary algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer noise reshaping and perceptual optimization for
effective adversarial attack of images. <em>APIN</em>, <em>53</em>(7),
7408–7422. (<a
href="https://doi.org/10.1007/s10489-022-03838-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attack aims to fail the deep neural network by adding a small amount of perturbation to the input image, in which the attack success rate and resulting image quality are maximized under the lp norm perturbation constraint. However, the lp norm is not accurately correlated to human perception of image quality. Attack methods based on l0 norm constraint usually suffer from the high computational cost due to the iterative search for candidate pixels to modify. In this work, we explore how perceptual quality optimization can be incorporated into the adversarial attack design and propose a two-stage attack method to reshape the adversarial noise by an initial attack and optimize the visual quality of the attacked images without sacrificing the attack success rate. Specifically, we construct a visual attention network to generate a perceptual attention map to modulate the adversarial noise generated by a base attack method. The network is trained to maximize the visual quality in Structural Similarity Index Metric (SSIM) while achieving the same attack success rate. To improve the image perceptual quality further, we propose a fast search algorithm to perform an iterative block-wise pruning of the adversarial noise. We evaluate our method on the mini-ImageNet dataset against three different defense schemes. The results have demonstrated that our method can achieve better attack performance in image quality, attack success rate, and efficiency than the state-of-the-art attack methods.},
  archive      = {J_APIN},
  author       = {He, Zhiquan and Lan, Xujia and Yuan, Jianhe and Cao, Wenming},
  doi          = {10.1007/s10489-022-03838-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7408-7422},
  shortjournal = {Appl. Intell.},
  title        = {Multi-layer noise reshaping and perceptual optimization for effective adversarial attack of images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic and intelligent focused crawler based on semantic
vector space model and membrane computing optimization algorithm.
<em>APIN</em>, <em>53</em>(7), 7390–7407. (<a
href="https://doi.org/10.1007/s10489-022-03180-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focused crawler downloads web pages related to the given topic from the Internet. In many research studies, most of focused crawler predict the priority values of unvisited hyperlinks by integrating the topic similarities based on the text similarity model and equivalent weighted factors based on the manual method. However, in these focused crawlers, there are flaws in the text similarity models, and weighted factors are arbitrarily determined for calculating priorities of unvisited URLs. To solve these problems, this paper proposes a semantic and intelligent focused crawler based on the Semantic Vector Space Model (SVSM) and the Membrane Computing Optimization Algorithm (MCOA). Firstly, the SVSM method is used to calculate topic similarities between texts and the given topic. Secondly, the MCOA method is used to optimize four weighted factors based on the evolution rules and the communication rule. Finally, this proposed focused crawler predicts the priority of each unvisited hyperlink by integrating the topic similarities of four texts and the optimal four weighted factors. The experiment results indicate that the proposed SVSM-MCOA Crawler improve the evaluation indicators compared with the other four focused crawlers. In conclusion, the proposed SVSM and MCOA method promotes the focused crawler to have semantic understanding and intelligent learning ability.},
  archive      = {J_APIN},
  author       = {Liu, Wenjun and Gan, Zurui and Xi, Tiejun and Du, Yajun and Wu, Jing and He, Yu and Jiang, Pengjun and Liu, Xing and Lai, Xia},
  doi          = {10.1007/s10489-022-03180-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7390-7407},
  shortjournal = {Appl. Intell.},
  title        = {A semantic and intelligent focused crawler based on semantic vector space model and membrane computing optimization algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early straggler tasks detection by recurrent neural network
in a heterogeneous environment. <em>APIN</em>, <em>53</em>(7),
7369–7389. (<a
href="https://doi.org/10.1007/s10489-022-03837-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity is common in parallel and distributed environments used for extensive computations such as MapReduce. Stragglers are the tasks that are running on inferior performing nodes in the cluster. Early detection of stragglers is always challenging in such environments. In the previously proposed approaches, late detection of straggler tasks and estimation of time to end (TTE) for all the tasks running in a heterogeneous environment delays the entire job execution. Early straggler detection help to speculate a task at the early stages of task execution which indirectly improves the complete job execution. This article proposed early straggler detection by a recurrent neural network (ESDRNN) that collects the task and node information every three seconds from ApplicationMaster to train the RNN. It classifies the straggler tasks pretty early by RNN, between thirty to forty seconds of task execution, and transfers a list of classified tasks to an agent running on ResourceManager. RNN is a type of artificial neural network that is prevalent for processing sequential time-series data. Then, the agent predicts the TTE of these classified tasks by the Autoregressive integrated moving average (ARIMA) model. Finally, it sorts and refreshes the list with higher TTE after every ten seconds and speculates the tasks for the early completion of the MapReduce job. This proposed technique’s performance is evaluated on the HiBench benchmark suite of Hadoop’s most popular benchmark. Finally, compared with the default speculation technique and different techniques, the proposed speculation technique detects the stragglers early within 35 to 40 seconds of task execution. As a result, it decreases the job execution time by an average of 21% to 38% significantly for different workloads in a heterogeneous Hadoop cluster.},
  archive      = {J_APIN},
  author       = {Bawankule, Kamalakant Laxman and Dewang, Rupesh Kumar and Singh, Anil Kumar},
  doi          = {10.1007/s10489-022-03837-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7369-7389},
  shortjournal = {Appl. Intell.},
  title        = {Early straggler tasks detection by recurrent neural network in a heterogeneous environment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel rule generation and activation method for extended
belief rule-based system based on improved decision tree. <em>APIN</em>,
<em>53</em>(7), 7355–7368. (<a
href="https://doi.org/10.1007/s10489-022-03803-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many classifiers applied to classification problems, the extended belief rule-based (EBRB) system is a powerful tool with the ability to handle both qualitative and quantitative information under uncertainty. However, it may face the problems of low inference efficiency and inconsistent rule activation in some applications due to the limitations of the conventional EBRB generation and activation method. To this end, a novel rule generation and activation method for EBRB system based on improved decision tree is proposed in this paper. Firstly, the distributed EBRB is generated by using the improved decision tree construction method designed in this paper. Then, the conventional EBRB activation scheme is modified by selecting input-related sub-EBRB for a given input as inference rule base to search suitable belief rules as well as introducing the attribute sorting knowledge in the process of activation. Moreover, the original procedures of the inference process and class estimation are retained from conventional EBRB system. Experiments were carried out to validate the efficiency and effectiveness of the proposed method in comparison with different types of classifiers under eleven standard classification datasets. The comparison results show that the proposed method could obtain satisfactory classification accuracy and rule inference efficiency. Additionally, the proposed method performs well on datasets with fewer classes, and it can efficiently process multi-attribute datasets.},
  archive      = {J_APIN},
  author       = {Ma, Junwen and Zhang, An and Gao, Fei and Bi, Wenhao and Tang, Changhong},
  doi          = {10.1007/s10489-022-03803-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7355-7368},
  shortjournal = {Appl. Intell.},
  title        = {A novel rule generation and activation method for extended belief rule-based system based on improved decision tree},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TGAS-ReID: Efficient architecture search for person
re-identification via greedy decisions with topological order.
<em>APIN</em>, <em>53</em>(7), 7343–7354. (<a
href="https://doi.org/10.1007/s10489-021-03097-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (Re-ID) technology is being developed rapidly due to the successful application of deep convolutional neural networks. However, the prevailing Re-ID models are usually built upon manually design backbones. In this paper, we propose using the TGAS-ReID which is automatically designed convolutional network backbones for Re-ID to substitute the backbones originally designed for classification such as ResNet and VGG. In the Re-ID tasks to search for a cell structure, greedy decisions are made instead of deriving the architecture after comprehensive training. In other words, at each decision epoch, according to the topological order, we first decide the candidates’ pool of the edges to progressively reduce the coupling of the internal nodes of the DAG. An edge is then selected based on edge importance, edge certainty, and selection stability. We then make a greedy optimal choice for the selected edge and prune the relevant parameters. To further improve the backbone’s representation capability of the features, we further introduce the triplet loss with batch hard mining as the retrieval loss. Extensive experiments demonstrate that the searched structure of the backbones reaches a performance level close to the previous work with a 20.8% shorter searching time. The proposed method also prevents the final CNNs network from suffering the well-known performance collapse by avoiding aggregation of the skip-connections.},
  archive      = {J_APIN},
  author       = {Chen, Shengbo and Jiang, Kai and Liu, Xianrui and Yang, Kangkang and Lei, Zhou},
  doi          = {10.1007/s10489-021-03097-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {7},
  pages        = {7343-7354},
  shortjournal = {Appl. Intell.},
  title        = {TGAS-ReID: Efficient architecture search for person re-identification via greedy decisions with topological order},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient utility-list based high-utility itemset mining
algorithm. <em>APIN</em>, <em>53</em>(6), 6992–7006. (<a
href="https://doi.org/10.1007/s10489-022-03850-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility itemset mining (HUIM) is an important task in data mining that can retrieve more meaningful and useful patterns for decision-making. One-phase HUIM algorithms based on the utility-list structure have been shown to be the most efficient as they can mine high-utility itemsets (HUIs) without generating candidates. However, storing itemset information for the utility-list is time-consuming and memory consuming. To address this problem, we propose an efficient simplified utility-list-based HUIM algorithm (HUIM-SU). In the proposed HUIM-SU algorithm, the simplified utility-list is proposed to obtain all HUIs effectively and reduce memory usage in the depth-first search process. Based on the the simplified utility-list, repeated pruning according to the transaction-weighted utilisation (TWU) reduces the number of items. In addition, a construction tree and compressed storage are introduced to further reduce the search space and the memory usage. The extension utility and itemset TWU are then proposed to be the upper bounds, which reduce the search space considerably. Extensive experimental results on dense and sparse datasets indicate that the proposed HUIM-SU algorithm is highly efficient in terms of the number of candidates, memory usage, and execution time.},
  archive      = {J_APIN},
  author       = {Cheng, Zaihe and Fang, Wei and Shen, Wei and Lin, Jerry Chun-Wei and Yuan, Bo},
  doi          = {10.1007/s10489-022-03850-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6992-7006},
  shortjournal = {Appl. Intell.},
  title        = {An efficient utility-list based high-utility itemset mining algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified framework of graph structure learning, graph
generation and classification for brain network analysis. <em>APIN</em>,
<em>53</em>(6), 6978–6991. (<a
href="https://doi.org/10.1007/s10489-022-03891-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, functional brain networks have been employed for classifying neurological disorders, such as autism spectrum disorders (ASDs). Graph convolutional networks (GCNs) have been shown to be successful in modeling applications with graph structures. However, brain network data is in general of complex structure with small sample size, and the use of GCNs on available datasets remains a big challenge. Driven by this important issue, three questions arise: 1) how to capture the critical structures of brain networks by removing noisy connections, to facilitate the following GAN and GCN; 2) how to generate graphs by generative adversarial networks (GANs) to preserve the local graph topology as well as the global data distribution; 3) how to sufficiently leverage the real and generated graphs with domain gaps for improved classification. In this paper, we proposed a three-stage framework, named BrainGC-Net, which coherently joins the power of graph pooling, GAN and GCN for brain network generation and classification. Given the original brain network with a large number of noisy connections, we propose graph pooling to enhance the important connections with a supervision scheme. Then, based on the coarsened brain networks, we propose a graph GAN model, named EG-GAN, to focus on the global data distribution in the embedding space and local graph topology in the graph space simultaneously. Finally, a domain consistent GCN model is proposed to take sufficient advantage of the two domains rather than simply merging by incorporating multiple consistent regularizations from view correlation, class correlation and sample correlation. With extensive experiments on the ASD classification problem, we validate the effectiveness of our method and it achieves consistent improvements over state-of-the-art methods on the public ABIDE dataset.},
  archive      = {J_APIN},
  author       = {Cao, Peng and Wen, Guangqi and Yang, Wenju and Liu, Xiaoli and Yang, Jinzhu and Zaiane, Osmar},
  doi          = {10.1007/s10489-022-03891-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6978-6991},
  shortjournal = {Appl. Intell.},
  title        = {A unified framework of graph structure learning, graph generation and classification for brain network analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised reflectance-guided 3d shape reconstruction
from single-view images. <em>APIN</em>, <em>53</em>(6), 6966–6977. (<a
href="https://doi.org/10.1007/s10489-022-03724-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D shape reconstruction from a single-view image is an utterly ill-posed and challenging problem, while multi-view methods can reconstruct an object’s shape only from raw images. However, these raw images should be shot in a static scene, to promise that corresponding features in the images can be mapped to the same spatial location. Recent single-view methods need only single-view images of static or dynamic objects, by turning to prior knowledge to mine the latent multi-view information in single-view images. Some of them utilize prior models (e.g. rendering-based or style-transfer-based) to generate novel-view images, which are however not sufficiently accurate, to feed their model. In this paper, we represent Augmented Self-Supervised 3D Reconstruction with Monotonous Material (ASRMM) approach, trained end-to-end in a self-supervised manner, to obtain the 3D reconstruction of a category-specific object, without any relevant prior models for novel-view images. Our approach draws inspiration from the experience that (1) high quality multi-view images are difficult to obtain, and (2) the shape of an object of single material can be visually inferred more easily, rather than of multiple kinds of complex material. As to practice these motivations, ASRMM makes material monotonous in its diffuse part by setting reflectance an identical value, and apply this idea on the source and reconstruction images. Experiments show that our model can reasonably reconstruct the 3D model of faces, cats, cars and birds from their collections of single-view images, and the experiments also show that our approach can be generalized to different reconstruction tasks, including unsupervised depth-based reconstruction and 2D supervised mesh reconstruction, and achieve promising improvement in the quality of the reconstructed shape and the texture.},
  archive      = {J_APIN},
  author       = {Fang, Binbin and Xiao, Nanfeng},
  doi          = {10.1007/s10489-022-03724-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6966-6977},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised reflectance-guided 3d shape reconstruction from single-view images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable knowledge integrated sequence model for
detecting fake online reviews. <em>APIN</em>, <em>53</em>(6), 6953–6965.
(<a href="https://doi.org/10.1007/s10489-022-03822-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews have a great influence on customers’ shopping decisions. However, countless fake reviews are posted on shopping platforms, which seriously interfere with customers’ shopping decisions and pollute the fair e-commerce environment. In this paper, we propose EKI-SM, an explainable knowledge integrated sequence model, to detect fake reviews. Compared with existing models, the EKI-SM displays four advantages: 1) It integrates a set of important knowledge and learns high-dimensional word embedding from reviews to guide fake review detection tasks; in addition, this knowledge explains the results of the model. 2) It learns a continuous sequence model from discrete observations with high-dimensional features, which helps to learn more discriminating fake review features. 3) It fuses the one-dimensional convolutional network, the long short-term memory network, and the residual connector to capture the local and global dependency of the sequence and make the prediction model more robust. 4) Inspired by the idea of interpretable deep learning, we explain the EKI-SM and find the important critical words for detecting fake online reviews, which derive some interesting insights. Experiments on actual fake review datasets demonstrate that the EKI-SM achieves higher accuracy in fake review detection than that of other state-of-the-art methods; indeed, it benefits from the integration of knowledge and multi-modal features.},
  archive      = {J_APIN},
  author       = {Han, Shu and Wang, Hong and Li, Wei and Zhang, Hui and Zhuang, Luhe},
  doi          = {10.1007/s10489-022-03822-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6953-6965},
  shortjournal = {Appl. Intell.},
  title        = {Explainable knowledge integrated sequence model for detecting fake online reviews},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpreting a deep reinforcement learning model with
conceptual embedding and performance analysis. <em>APIN</em>,
<em>53</em>(6), 6936–6952. (<a
href="https://doi.org/10.1007/s10489-022-03788-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weak interpretability of the deep reinforcement learning (DRL) model becomes a serious impediment to the application of DRL agents in certain areas requiring high reliability. To interpret the behavior of a DRL agent, researchers use saliency maps to discover important parts of the agent’s observation that influence its decision. However, the representations of saliency maps still cannot explicitly present the cause and effect between an agent’s actions and its observations. In this paper, we analyze the inference procedure with respect to the DRL architecture and propose embedding interpretable intermediate representations for an agent’s policy, the intermediate representations that are compressed and abstracted for explanation. We utilize a conceptual embedding technique to regulate the latent representation space of the deep models that can produce interpretable causal factors aligned with human concepts. Furthermore, the information loss of intermediate representation is analyzed to define the model performance upper bound and to measure the model performance degeneration. Experiments validate the effectiveness of the proposed method and the relationship between the observation information and an agent’s performance upper bound.},
  archive      = {J_APIN},
  author       = {Dai, Yinglong and Ouyang, Haibin and Zheng, Hong and Long, Han and Duan, Xiaojun},
  doi          = {10.1007/s10489-022-03788-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6936-6952},
  shortjournal = {Appl. Intell.},
  title        = {Interpreting a deep reinforcement learning model with conceptual embedding and performance analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-perspective context aggregation for document-level
relation extraction. <em>APIN</em>, <em>53</em>(6), 6926–6935. (<a
href="https://doi.org/10.1007/s10489-022-03731-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of document-level relation extraction (RE) involves integration of information within and across multiple sentences of a document and extraction of complex semantic relations between multiple named entities. However, effective aggregation of local and nonlocal contexts information in the document continues to be a challenging research question. This study proposes a novel document-level RE model, called the multi-perspective context aggregation (MPCA), that aggregates document context information from multi-perspective at different layers. Specifically, this aggregated context information not only comes from the pre-training stage but is also reflected during node construction and classification. Experimental results show that our model achieves desirable performance on two public datasets for document-level RE and is particularly effective in extracting relations between entities with multiple mentions.},
  archive      = {J_APIN},
  author       = {Ding, Xiaoyao and Zhou, Gang and Zhu, Taojie},
  doi          = {10.1007/s10489-022-03731-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6926-6935},
  shortjournal = {Appl. Intell.},
  title        = {Multi-perspective context aggregation for document-level relation extraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BatchedGreedy: A batch processing approach for influence
maximization with candidate constraint. <em>APIN</em>, <em>53</em>(6),
6909–6925. (<a
href="https://doi.org/10.1007/s10489-022-03854-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) aims to find k seed nodes from social network G to maximize the spread of influence under a given diffusion model. However, in real social marketing activities, only some users are connected with marketing initiators. In addition, not all users are willing to be a seed for a specific marketing activity. These factors restrict the range of nodes that can act as seed nodes. Therefore, we first propose the candidate constrained influence maximization (CCIM) problem. Here, only seeds from a predefined set of candidate nodes are selected. Despite the similarity in definition between IM and CCIM, many state-of-the-art algorithms for IM cannot be directly applied to CCIM . We propose a batch processing approach BatchedGreedy for CCIM, which utilizes the efficiency of bit operation in a computer to estimate the spread of influence of nodes in batches. Furthermore, for the traditional influence maximization(IM) problem, we propose the filtering-based BatchedGreedy (FB-BG) algorithm by incorporating node filtering with the BatchedGreedy approach. From experimental statistics, it is shown that FB-BG not only provides better performance than state-of-the-art algorithms in comparable running time, but is also more scalable to larger networks.},
  archive      = {J_APIN},
  author       = {Han, Xiaowei and Yao, Xiaopeng and Huang, Hejiao},
  doi          = {10.1007/s10489-022-03854-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6909-6925},
  shortjournal = {Appl. Intell.},
  title        = {BatchedGreedy: A batch processing approach for influence maximization with candidate constraint},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Instance-based deep transfer learning with attention for
stock movement prediction. <em>APIN</em>, <em>53</em>(6), 6887–6908. (<a
href="https://doi.org/10.1007/s10489-022-03755-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock movement prediction is one of the most challenging problems in time series analysis due to the stochastic nature of financial markets. In recent years, a plethora of statistical methods and machine learning algorithms were proposed for stock movement prediction. Specifically, deep learning models are increasingly applied for the prediction of stock movement. The success of deep learning models relies on the assumption that massive training data are available. However, this assumption is impractical for stock movement prediction. In stock markets, a large number of stocks do not have enough historical data, especially for the companies which underwent initial public offering in recent years. In these situations, the accuracy of deep learning models to predict the stock movement could be affected. To address this problem, in this paper, we propose novel instance-based deep transfer learning models with attention mechanism. In the experiments, we compare our proposed methods with state-of-the-art prediction models. Experimental results on three public datasets reveal that our proposed methods significantly improve the performance of deep learning models when limited training data are available.},
  archive      = {J_APIN},
  author       = {He, Qi-Qiao and Siu, Shirley Weng In and Si, Yain-Whar},
  doi          = {10.1007/s10489-022-03755-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6887-6908},
  shortjournal = {Appl. Intell.},
  title        = {Instance-based deep transfer learning with attention for stock movement prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-label classification of legal text based on label
embedding and capsule network. <em>APIN</em>, <em>53</em>(6), 6873–6886.
(<a href="https://doi.org/10.1007/s10489-022-03455-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning technology and the disclosure of legal texts, the classification of legal texts has attracted the attention of researchers. At present, research on the classification of legal texts is mainly focused on multiclass classification. There are few studies on multi-label classification for legal texts. This paper addresses the use of a label sequence generation model to study the multi-label classification of legal texts at the sentence level. The current general multi-label classification methods are often designed for long texts and ignore the transfer relationships between labels. We propose a method based on label embedding and a capsule neural network for the multi-label classification of legal text. Our proposed method applies the graph convolutional network to learn label embeddings and the correlations between labels, a fusion layer to combine the label information with the contextual semantic information of texts and a capsule neural network to extract the spatial feature information of text. Experimental results on three legal text datasets show that our proposed model outperforms the baseline methods, verifying the effectiveness of our proposed model for legal text with an uncertain number of characters in words and short lengths. In addition, we experimented on two datasets that are usually applied in multi-label classification, and the performance of the model shows that the method we proposed is competitive with state-of-the-art models of multi-label text classification.},
  archive      = {J_APIN},
  author       = {Chen, Zhe and Li, Shang and Ye, Lin and Zhang, Hongli},
  doi          = {10.1007/s10489-022-03455-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6873-6886},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label classification of legal text based on label embedding and capsule network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). REBDT: A regular expression boundary-based decision tree
model for chinese logistics address segmentation. <em>APIN</em>,
<em>53</em>(6), 6856–6872. (<a
href="https://doi.org/10.1007/s10489-022-03511-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese logistics address segmentation is a specific domain of the address resolution, which is very challenging due to language, culture, user privacy, business value, etc. Although deep learning can effectively solve problems where traditional segmentation methods are overly dependent on domain knowledge, it faces the dilemma of costly manual labeling. In this context, a decision tree model based on regular expression boundaries is proposed, which requires no additional data and manual labeling. First, different from traditional methods of describing the entire address elements, a regular expressions rule library (RERL) is constructed, which only describes the boundaries of address elements. Second, the binary split attribute is defined according to the boundary matching algorithm based on RERL. A decision tree model is then constructed concerning the distribution law of address element types to segment an address and to evaluate its effect. The final experimental results demonstrate the improvement of our model and further substantiate that our proposal can provide a high-quality labeling training set for deep learning models without any professional domain knowledge, even if in low-resource scenarios.},
  archive      = {J_APIN},
  author       = {Ling, Guangming and Xu, Aiping and Wang, Chao and Wu, Jie},
  doi          = {10.1007/s10489-022-03511-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6856-6872},
  shortjournal = {Appl. Intell.},
  title        = {REBDT: A regular expression boundary-based decision tree model for chinese logistics address segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic segmentation of 3D LiDAR data using deep learning:
A review of projection-based methods. <em>APIN</em>, <em>53</em>(6),
6844–6855. (<a
href="https://doi.org/10.1007/s10489-022-03930-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR sensor is an active remote sensing sensor that is increasingly used to capture 3D information of real-world objects. Real-time decision-making applications such as autonomous driving heavily rely on 3D information to navigate an urban environment. LiDAR data processing is, however, very complex and resource-intensive. Deep learning on point cloud is a recent advancement that is aimed to extract 3D information. Deep learning implementations include procedures where raw points are fed to neural networks and converted to 3D voxels. Individual voxels are fed to 3D convolutional layers and techniques that transform the 3D points into 2D images and utilize the well-established 2D CNNs. Of these, the two former methods are majorly reviewed, while the projection-based methods are less reviewed although the technique is widely used in numerous applications. To fill the gap, this paper examines the existing literature on projection-based methods by detailing the recent progress made. Identifying the state-of-the-art methodology and summarizing the important interventions are among the significant tasks covered in this paper.},
  archive      = {J_APIN},
  author       = {Jhaldiyal, Alok and Chaudhary, Navendu},
  doi          = {10.1007/s10489-022-03930-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6844-6855},
  shortjournal = {Appl. Intell.},
  title        = {Semantic segmentation of 3D LiDAR data using deep learning: A review of projection-based methods},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-aware edge-assisted lightweight semantic segmentation
network for power transmission line inspection. <em>APIN</em>,
<em>53</em>(6), 6826–6843. (<a
href="https://doi.org/10.1007/s10489-022-03932-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for real-time efficient scene comprehension has been increasing rapidly in the drone-based automatic inspection of power transmission lines (PTL). The extensive application of semantic segmentation in urban scenes proves that it can meet the requirements for scene understanding. However, existing methods have difficulty adapting to changes in the scene, which leads to problems of performance degradation and fuzzy contours of segmented objects. To overcome the existing problems, a class-aware edge-assisted lightweight semantic segmentation network is proposed in this paper. Class-aware edge detection is introduced as an auxiliary task, and a two-branch network is designed to locate instances and refine contours. Specifically, hybrid graph learning uses task-specific graph-based structures to reason attention information of region and edge features. Based on the complementary characteristic of region and edge features, cascaded shared decoders adopt specific interaction functions to enhance the ability of region features to locate targets and the ability of edge features to improve contour details. In addition, to verify the effectiveness of the proposed method, we construct two datasets named the transmission tower component recognition dataset (TTCRD) and the transmission line regional classification dataset (TLRCD). Comprehensive experiments on TTCRD and TLRCD prove that the proposed method can accurately refine the contour of objects and overcome the challenges in the two datasets. Comparison experiments and ablation experiments also demonstrate the superior performance of the proposed method and the effectiveness of each component in our architecture.},
  archive      = {J_APIN},
  author       = {Zhou, Qingkai and Li, Qingwu and Xu, Chang and Lu, Qiuyu and Zhou, Yaqin},
  doi          = {10.1007/s10489-022-03932-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6826-6843},
  shortjournal = {Appl. Intell.},
  title        = {Class-aware edge-assisted lightweight semantic segmentation network for power transmission line inspection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). M2GCN: Multi-modal graph convolutional network for modeling
polypharmacy side effects. <em>APIN</em>, <em>53</em>(6), 6814–6825. (<a
href="https://doi.org/10.1007/s10489-022-03839-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treating patients with complex diseases or co-existing conditions by polypharmacy (i.e., the use of drug combination) is very common. However, due to drug-drug interactions, polypharmacy often results in unpredictable side effects, which may endanger patients’ life. Moreover, since adverse drug reactions are rare, discovering polypharmacy side effects only from sparse drug-drug interactions remains challenging. Thus, it is necessary to explore the knowledge of polypharmacy side effects from the interaction network with complex relationships. In this paper, we propose a novel multi-modal graph convolutional neural network (M2GCN) for link prediction in multi-modal networks which consist of protein-protein interactions, drug-protein interactions and drug-drug interactions. Specifically, we first propose a propagation strategy to perform graph aggregations on each subgraph. Then we leverage consistency regularization to align the consistency across different subgraphs. Finally, referring to the DistMult method, we use the embeddings obtained above to calculate the probability of side effects between drugs. Experimental results on benchmark dataset show that our method significantly outperforms the compared network embedding models.},
  archive      = {J_APIN},
  author       = {Liu, Qidong and Yao, Enguang and Liu, Chaoyue and Zhou, Xin and Li, Yafei and Xu, Mingliang},
  doi          = {10.1007/s10489-022-03839-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6814-6825},
  shortjournal = {Appl. Intell.},
  title        = {M2GCN: Multi-modal graph convolutional network for modeling polypharmacy side effects},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic multichannel fusion mechanism based on a graph
attention network and BERT for aspect-based sentiment classification.
<em>APIN</em>, <em>53</em>(6), 6800–6813. (<a
href="https://doi.org/10.1007/s10489-022-03851-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment classification aims to predict the sentiment polarity on given aspect terms in a sentence. Recent works have incorporated syntactic information by developing graph neural networks (GNNs) over dependency trees to better establish connections between aspect items and their related context. However, the advancement is restricted because the dependency tree derived by the external parser is not entirely accurate, especially for high complexity and arbitrary expression datasets. To address this constraint, we propose a dynamic multichannel fusion mechanism based on the Graph AttenTion network and BERT (DMF-GAT-BERT), which regards the complementarity of semantic and syntactic information captured by GAT and BERT, respectively. Specifically, to alleviate the damage of incorrect dependency tree information to the model, we propose a two-layer dynamic fusion mechanism to adaptively adjust the fusion weight of semantic and syntax-related information channels. In addition, to capture accurate syntactic features, we propose an attentive layer ensemble (ALE) to integrate the contextual features learned by GAT in different layers. We conducted experiments on four datasets with different complexity, the Laptop, Restaurant, Twitter, and MAMS datasets, and achieved 80.38%, 86.10%, 76.22%, and 83.86% accuracy, respectively, outperforming robust baseline approaches.},
  archive      = {J_APIN},
  author       = {Zhou, Xiaotang and Zhang, Tao and Cheng, Chao and Song, Shinan},
  doi          = {10.1007/s10489-022-03851-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6800-6813},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic multichannel fusion mechanism based on a graph attention network and BERT for aspect-based sentiment classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced sooty tern optimization algorithm using multiple
search guidance strategies and multiple position update modes for
solving optimization problems. <em>APIN</em>, <em>53</em>(6), 6763–6799.
(<a href="https://doi.org/10.1007/s10489-022-03635-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sooty Tern Optimization Algorithm (STOA) is a newly proposed bio-inspired algorithm that mimics the migration and attacking behaviors of the sea bird sooty tern in nature. STOA has several excellent advantages, including fewer parameters, a simple structure, a fast convergence rate, and high exploitation. Nevertheless, it is difficult to find the global optimal solution and prone to losing population diversity when dealing with complex optimization problems due to its single search guidance strategy and position update method. An enhanced STOA (ESTOA) is proposed to address these shortcomings that incorporates multiple search guidance strategies and position update modes. In terms of search guidance, in addition to the best individual in the original STOA, the mean individual and a randomly selected individual are also designed to guide the search. Six position update modes are proposed in conjunction with the guidance strategies, including one improved scaling mode with an extended spiral radius and five other modes based on offset operations. Due to their distinct design objectives, these guidance strategies and position update modes exhibit varying levels of search intensity and optimization effect. However, they complement one another and work cooperatively to achieve a good balance of global exploration and local exploitation. Several widely used sets of benchmark functions with a wide range of dimensions and varying degrees of complexity are used to validate ESTOA’s performance. The obtained results are compared to those of other state-of-the-art optimization algorithms in terms of convergence accuracy and a variety of numerical performance evaluation parameters. A significant improvement in solution quality demonstrates that ESTOA can increase population diversity and maintain a good balance between global exploring and local exploiting abilities.},
  archive      = {J_APIN},
  author       = {He, Jieguang and Peng, Zhiping and Cui, Delong and Qiu, Jingbo and Li, Qirui and Zhang, Hao},
  doi          = {10.1007/s10489-022-03635-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6763-6799},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced sooty tern optimization algorithm using multiple search guidance strategies and multiple position update modes for solving optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TC-net: Transformer combined with cnn for image denoising.
<em>APIN</em>, <em>53</em>(6), 6753–6762. (<a
href="https://doi.org/10.1007/s10489-022-03785-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an effective and novel network architecture based on the transformer TC-Net. The architecture is composed of several transformer blocks and convolutions for image denoising. The following four core designs of TC-Net ensure that it is suitable for image denoising: (1) An extra skip-connection for feature fusion ensures more effective transmission and utilization of low-level and high-level features. (2) Window multihead self-attention greatly reduces the amount of calculation and captures the dependence of capturing long distances. (3) A convolution-based forward network further improves the ability to capture local information. (4) Ingeniously adding a deep residual shrinkage network into a transformer block improves the networks ability to deal with noise and its robustness in complex scenes. Not just for denoising tasks. To deal with similar low-level visual tasks, only the dataset needs to be changed. The model architecture remains the same and is trained separately. Then, the pretraining models of other tasks can be obtained. A large number of experiments proved the ability of TC-Net in image restoration and demonstrated its efficiency and effectiveness in various scenes.},
  archive      = {J_APIN},
  author       = {Xue, Tao and Ma, Pengsen},
  doi          = {10.1007/s10489-022-03785-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6753-6762},
  shortjournal = {Appl. Intell.},
  title        = {TC-net: Transformer combined with cnn for image denoising},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bioinspired cooperative control method of a pursuer group
vs. A faster evader in a limited area. <em>APIN</em>, <em>53</em>(6),
6736–6752. (<a
href="https://doi.org/10.1007/s10489-022-03892-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of a faster evader hunted by Np pursuers in a limited area has been a significant subject in recent years. Nevertheless, it is still challenging to develop a cooperative strategy for pursuers and an escape strategy with boundary restrictions for the evader. We solved this problem using an artificial potential function and set several interaction rules inspired by some basic concepts from ecology to achieve the pursuit and evasion phenomena. In this paper, a decentralized, real-time strategy called the dynamic cooperative hunting strategy based on the head-pursuit mechanism and the combined escape strategy is proposed. The pursuers share state information but compute their control inputs independently. For pursuers, the head-pursuit mechanism enhances each pursuer’s cognitive prediction ability. A dynamic repulsive force between pursuers is introduced to improve the cooperative ability, which enables the pursuer group to be more adaptive in a rapidly changing situation. For the evader, the combined escape strategy consists of three escape actions, namely, direct escape, border escape, and gap escape. The evader can adopt any of the three actions to escape based on the real-time relative position between each pursuer and itself and between the boundaries and itself. In addition, evaluation functions (survival time) are defined to quantify the pursuit-evasion. Extensive simulations validate the feasibility and effectiveness of the proposed method. Moreover, the experimental results demonstrate that the proposed method has application potential in mobile robots.},
  archive      = {J_APIN},
  author       = {Fu, Xiaowei and Zhang, Yuxuan and Zhu, Jindong and Wang, Qianglong},
  doi          = {10.1007/s10489-022-03892-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6736-6752},
  shortjournal = {Appl. Intell.},
  title        = {Bioinspired cooperative control method of a pursuer group vs. a faster evader in a limited area},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal time-based strategy for automated negotiation.
<em>APIN</em>, <em>53</em>(6), 6710–6735. (<a
href="https://doi.org/10.1007/s10489-022-03662-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years are showing increased adoption of AI technology to automate business and production processes thanks to the recent successes of machine learning techniques. This leads to increased interest in automated negotiation as a method for achieving win-win agreements among self-interested agents. Research in automated negotiation can be traced back to the Nash bargaining game in the mid 20th century. Nevertheless, finding an optimal negotiation strategy against an unknown opponent with an unknown utility function is still an open area of research. The most recent result in this area is the Greedy Concession Algorithm (GCA) which can be shown to be optimal under specific constraints on both the negotiation protocol (non-repeating offers), opponent (static acceptance-model) and search space (deterministic time-based strategies). In this paper, we extend this line of work by providing an algorithmically faster version of GCA called Quick GCA which reduces the time-complexity of the search process from O(K2T) to O(KT) where K is the size of the outcome-space and T is the number of negotiation rounds allowed. Moreover, we show that GCA/QGCA can be applied in a more general setting; Namely with repeating-offers protocols and to search the more general probabilistic time-based strategies. Finally, we heuristically extend QGCA to more general opponents with general time-dependent acceptance-model and negotiation settings (real-time limited negotiations) in three steps called QGCA+, PBS, and PA that iteratively and greedily modify the policy proposed by QGCA+ applied to an approximate static acceptance model. The paper evaluates the proposed approach empirically against state of the art negotiation strategies (winners of all relevant ANAC competition winners) and shows that it outperforms them in a wide variety of negotiation scenarios.},
  archive      = {J_APIN},
  author       = {Mohammad, Yasser},
  doi          = {10.1007/s10489-022-03662-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6710-6735},
  shortjournal = {Appl. Intell.},
  title        = {Optimal time-based strategy for automated negotiation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An event-based opinion summarization model for long chinese
text with sentiment awareness and parameter fusion mechanism.
<em>APIN</em>, <em>53</em>(6), 6682–6709. (<a
href="https://doi.org/10.1007/s10489-022-03231-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the outbreak of a specific social event, end-to-end automatic opinion summarization is needed to analyze the surge of text related to the event. However, in the Chinese domain, the major existing works either emphasize salient aspects or sentence extraction in a discrete fashion with no consideration of human readability, or focus on short Chinese text. To remedy the drawbacks of these methods, in this paper, an event-based opinion summarization model for long Chinese text with a parameter fusion mechanism is proposed to address the human readability and imbalance issue of the event-based datasets. In particular, to capture the sentiment information in the source article in an end-to-end manner, a sentiment attention layer and a sentiment cross-entropy loss function are presented. In addition, when facing the issue of imbalance in event-based datasets, a parameter fusion mechanism inspired by the federated learning is proposed, which can further improve the human readability of the output. Finally, the efficacy of the proposed model is substantiated via comprehensive experiments performed on the collected event-based datasets, the Chinese long text summarization dataset (CLTS), and the cable news network/daily mail (CNN/DM) dataset using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and sentiment classification accuracy metrics. In addition, the source code is made available at https://github.com/ShawnYoung97/opinion-sum .},
  archive      = {J_APIN},
  author       = {Liao, Shan and Li, Xiaoyang and Liu, Jiayong and Zhou, Anmin and Li, Kai and Peng, Siqi},
  doi          = {10.1007/s10489-022-03231-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6682-6709},
  shortjournal = {Appl. Intell.},
  title        = {An event-based opinion summarization model for long chinese text with sentiment awareness and parameter fusion mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Improving entity alignment via attribute and external
knowledge filtering. <em>APIN</em>, <em>53</em>(6), 6671–6681. (<a
href="https://doi.org/10.1007/s10489-022-03744-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of entity alignment is to find the equivalent entity pairs in different Knowledge Graphs (KGs), which is a key step of KG fusion. Recent developments often take embedding-based methods, which mainly focus on embedding structure information (relationship triples) of KGs to align entities. However, attribute information (attribute triples) and external knowledge (some public knowledge bases) can also provide extremely valuable information but have not been well explored yet. In this paper, we propose a Selective Filtering Entity Alignment (SFEA) framework for improving entity alignment via attribute and external knowledge filtering. Our framework proposes two kinds of selective filtering mechanisms to filter the candidate set by using attribute information and external knowledge, so as to delete most of wrong entities in the candidate set. The framework mainly uses selective filtering mechanism and external knowledge for entity alignment. Experiments show that our framework achieves state-of-the-art on the dataset DBP15K compared to the existing similar methods of using structure and attribute information. Specifically, our framework outperforms the best state-of-the-art methods by 1.2%-4.9% in terms of Hits@1, and also achieves better or comparable performance when compared with other methods of incorporating multiple sources of information.},
  archive      = {J_APIN},
  author       = {Zhang, Fu and Li, Jianwei and Cheng, Jingwei},
  doi          = {10.1007/s10489-022-03744-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6671-6681},
  shortjournal = {Appl. Intell.},
  title        = {Improving entity alignment via attribute and external knowledge filtering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved black widow optimization algorithm for surfaces
conversion. <em>APIN</em>, <em>53</em>(6), 6629–6670. (<a
href="https://doi.org/10.1007/s10489-022-03715-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bézier surfaces and Q-Bézier surfaces are effective modeling tools for shape design in computer-aided geometric design, computer vision, and computer graphics. The mutual conversion between these two kinds of surfaces is a pivotal and knotty technique in CAD/CAM. In this paper, the conversion between Q-Bézier surfaces and rectangular Bézier surfaces is investigated. However, due to the uncertain parameters of the Q-Bézier surfaces, the approximation conversion from rectangular Bézier surfaces to Q-Bézier surfaces can be regarded as an optimization problem, which is effectively dealt with by swarm intelligence algorithm. In this regard, an enhanced Black Widow Optimization called LDBWO is proposed to find more suitable shape parameters to obtain optimal approximation Q-Bézier surfaces, which are closer to the given Bézier surfaces. The LDBWO algorithm overcomes the shortcomings of standard BWO algorithm such as low accuracy, slow convergence, and is easy to fall into local optimum by introducing golden sine learning strategy and diffusion process. Furthermore, to confirm and validate the performance of the LDBWO, eight well-known intelligent algorithms are compared with the LDBWO on various benchmark functions and engineering examples. Finally, by minimizing the conversion error defined by the L2-norm, the optimization model of the approximation conversion from rectangular Bézier surfaces to Q-Bézier surfaces is established. Several representative numerical examples are provided to illustrate the accuracy and efficiency of the proposed methods.},
  archive      = {J_APIN},
  author       = {Hu, Gang and Du, Bo and Wang, Xiaofeng},
  doi          = {10.1007/s10489-022-03715-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6629-6670},
  shortjournal = {Appl. Intell.},
  title        = {An improved black widow optimization algorithm for surfaces conversion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-level decision making in techno-economic planning and
probabilistic analysis of community based sector-coupled energy system.
<em>APIN</em>, <em>53</em>(6), 6604–6628. (<a
href="https://doi.org/10.1007/s10489-022-03794-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a bi-level programming model for community participation in techno-economic planning of an integrated energy system. Distribution system operator/System planner considered as a upper level decision maker anticipates the optimal operational response of community operator by solving lower level problem to make financial decisions while planning a sector coupled integrated energy system.To establish reliable, scalable, robust and data-driven decision making tool in techno-economic planning, a smart framework is proposed to quantify the effects of solar power uncertainties, assess hourly electric vehicle (EV) charging profile, analyze probabilistic reliability and obtain optimal financial decision by solving bi-level programming model using four meta-heuristics algorithms. During the preliminary stage, the potential effects of seasonal and climatic variations on solar radiation are quantified through nine distinct probabilistic classifiers trained using support vector classification approach. It serves as a data driven scenario generation tool for robust exploration of bi-level decision making process. Additionally, the impact of EV arrivals in a limited server charging facility is studied using queueing theory to estimate the mean hourly EV charging demand. Finally, the optimal settings of particle swarm optimisation (PSO), genetic algorithm (GA), red deer algorithm (RDA) and evolve class topper optimization (E-CTO) are identified through Taguchi’s design of experiments. Based on these optimal settings, the proposed bi-level model is solved to obtain the best investment decision for planning a community based energy system. The results suggest that E-CTO shows a better response in terms of computational speed, convergence rate, accuracy and reliability. A minimum value of Rs 4.18 per kWh is obtained as the cost of energy using E-CTO. Numerical investigations were studied on a two area consumer centric energy system connected to the local grid to show the effectiveness of the proposed bi-level approach and meta-heuristic algorithms.},
  archive      = {J_APIN},
  author       = {Kumar, Nishant and Namrata, Kumari and Samadhiya, Akshit},
  doi          = {10.1007/s10489-022-03794-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6604-6628},
  shortjournal = {Appl. Intell.},
  title        = {Bi-level decision making in techno-economic planning and probabilistic analysis of community based sector-coupled energy system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-task learning model with graph convolutional
networks for aspect term extraction and polarity classification.
<em>APIN</em>, <em>53</em>(6), 6585–6603. (<a
href="https://doi.org/10.1007/s10489-022-03573-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-task learning of aspect-based sentiment analysis aims to extract the aspect terms and predict the sentiment polarities for such terms. The majority of research has focused on improving the representations of syntactical information, while ignoring the importance of syntactic dependency. The internal relationship between the aspect terms extraction and the aspect polarity classification has not been well exploited. In this paper, we propose a unified model with Location-aware Graph Convolutional Networks (L-GCNs) for aspect-based multi-task learning. Firstly, we reconstruct graph convolutional networks by considering the location information of special aspect terms to highlight the aspect-related dependency information, which is adopted to extract aspect terms. Then, we redesign the dependency tree by considering long-distance dependencies to aggregate more context-related dependency information. Finally, combined with the dependency information, the sentiment features of special aspect terms can be captured by applying attention encoding. We evaluate the proposed model for four benchmark datasets and our experimental results show that the unified model achieves state-of-the-art performance of multi-task learning. In addition, ablation studies and different variants verify the rationality and effectiveness of our model.},
  archive      = {J_APIN},
  author       = {Zhao, Meng and Yang, Jing and Qu, Lianwei},
  doi          = {10.1007/s10489-022-03573-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6585-6603},
  shortjournal = {Appl. Intell.},
  title        = {A multi-task learning model with graph convolutional networks for aspect term extraction and polarity classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMDM: Tackling zero-shot relation extraction with semantic
max-divergence metric learning. <em>APIN</em>, <em>53</em>(6),
6569–6584. (<a
href="https://doi.org/10.1007/s10489-022-03596-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In zero-shot relation extraction, existing methods usually learn semantic features from seen relations to infer unseen relations. However, because there is no instance of unseen relation that can be used for training, it is still a challenge for the existing models to learn the semantic gap between seen relations and unseen relations, resulting in poor generalization performance of the learned semantic features. Therefore, we propose a Semantic Max-Divergence Metric (SMDM) based method to measure the distances between relations from both direct and indirect semantic differences. For that, we learn multiple binary feature reference-spaces to extract the semantic divergences of each unseen relation instance relative to each seen relation, which can be converted to a relative-affinity (RA) matrix as indirect semantic metrics. Furthermore, we combine RA with direct semantic metrics based on BERT to maximum the divergences between unseen relation instances and get clearer unseen relation boundaries. Empirical results on benchmark datasets demonstrate SMDM can the superior improvement on F1-score and external indicators of SMDM compared to the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Bosen and Xu, Yajing and Li, Jinglei and Wang, Shusen and Ren, Boya and Gao, Sheng},
  doi          = {10.1007/s10489-022-03596-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6569-6584},
  shortjournal = {Appl. Intell.},
  title        = {SMDM: Tackling zero-shot relation extraction with semantic max-divergence metric learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A syntactic distance sensitive neural network for event
argument extraction. <em>APIN</em>, <em>53</em>(6), 6554–6568. (<a
href="https://doi.org/10.1007/s10489-022-03598-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event argument extraction aims at identifying event arguments from texts as well as determining their respective roles in an event. Despite some neural networks applied for this task, their performance are still not satisfactory due to the following shortcomings. Syntactic information were not well explored; Event arguments were independently extracted; Pattern knowledge were not explicitly exploited. In this paper, we propose a Syntactic Distance Sensitive Neural Network model to tackle these problems. Our model first captures long-range dependencies in between event triggers and event arguments through performing graph convolution over syntactic trees, where we introduce syntactic distance to weight the importance of each word. Furthermore, we design an argument interaction module to mine argument-argument interactions according to the shortest dependency distances in between arguments. To enjoy pattern knowledge, we design a pattern-aware argument classification module to ensure the reasonability of extracted arguments. Extensive experiments have validated the superiority of the proposed model, which achieves the state-of-the-art results in terms of better F1-score on both argument identification and role classification.},
  archive      = {J_APIN},
  author       = {Dai, Lu and Wang, Bang and Xiang, Wei and Mo, Yijun},
  doi          = {10.1007/s10489-022-03598-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6554-6568},
  shortjournal = {Appl. Intell.},
  title        = {A syntactic distance sensitive neural network for event argument extraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering probabilistically weighted sequential patterns
in uncertain databases. <em>APIN</em>, <em>53</em>(6), 6525–6553. (<a
href="https://doi.org/10.1007/s10489-022-03699-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining useful sequential patterns has been a recent trend in data mining as the real-life applications are mostly sequence oriented. Researchers have developed many algorithms to find frequent sub-sequences from sequential databases to find useful information. The emerging and tremendous development of technology has been increasing the number of applications that deal with uncertainty. Ordinary uncertain pattern mining algorithms deal with expected support or probabilistic frequentness of a pattern, ignoring the importance of individual items. However, in real-life, different items can have different importance. Some approaches consider the weight (importance) of items but fail to capture the interestingness of mined patterns. The objective of the work is to address weighted sequential uncertain pattern mining in Possible World Semantics (PWS) to better capture inherent relations among the items and events with different weights and developing a novel method uWSpan. Our proposed approach contains some pruning techniques to provide faster mining capability and introduces itemset extension for the first time in PWS. We have analyzed the performance of our proposed approach both theoretically and empirically where we found uWSpan efficient, scalable and effective. Our approach outperforms existing approaches most of the time when compared using approved datasets. We also analyzed the applicability, efficiency and effectiveness of our proposed method. Finally, the paper concludes with future research directions and a gist of the outcomes of the research.},
  archive      = {J_APIN},
  author       = {Islam, Md Sahidul and Kar, Pankaj Chandra and Samiullah, Md and Ahmed, Chowdhury Farhan and Leung, Carson Kai-Sang},
  doi          = {10.1007/s10489-022-03699-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6525-6553},
  shortjournal = {Appl. Intell.},
  title        = {Discovering probabilistically weighted sequential patterns in uncertain databases},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Membership score machine for highly nonlinear classification
for small data. <em>APIN</em>, <em>53</em>(6), 6511–6524. (<a
href="https://doi.org/10.1007/s10489-022-03652-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel classification algorithm, called the membership score machine (MSM), for highly nonlinear classification for small data, which is particularly applicable for highly irregular/non-globular dataset of arbitrarily many classes. Given a training dataset, the method utilizes within-class clustering and dimensionality reduction to extract useful geometric features, for each of classes. For data points in a class, the method first performs clustering and then applies the principal component analysis (PCA) for each cluster to form a reliable geometric representation in lower dimensions. The goal in the training stage is to draw out reliable geometric features and related anisotropic measures, one for each cluster. At the prediction stage, it calculates the membership scores based on the anisotropic measures with respect to each of the clusters; a test data point is classified for the class which contains the cluster that makes the maximum membership score. The proposed algorithm, the MSM, turns out to be scalable and more effective than existing algorithms in accuracy, especially for small and highly irregular datasets. The main idea behind the MSM is to represent the dataset geometrically by expressing it as a combination of multiple easy-to-classify clusters transformed into principal components in low dimensions. Numerical experiments are presented and compared with existing popular classifiers, to demonstrate its superior performances.},
  archive      = {J_APIN},
  author       = {Lee, Byungjoon and Kim, Hwamog and Kim, Seongjai},
  doi          = {10.1007/s10489-022-03652-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6511-6524},
  shortjournal = {Appl. Intell.},
  title        = {Membership score machine for highly nonlinear classification for small data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video object segmentation based on temporal frame context
information fusion and feature enhancement. <em>APIN</em>,
<em>53</em>(6), 6496–6510. (<a
href="https://doi.org/10.1007/s10489-022-03693-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, a large number of video object segmentation algorithms only use a small amount of frame information to guide the segmentation of the current frame, but fail to fully exploit the information of the historical frames, which makes the network model difficult for the network model to adapt to complex environmental changes, causing the phenomenon of object drift; at the same time, the mask refinement method is also rough, resulting in blurred edges of the generated mask. To solve this problem, this paper proposes a video object segmentation algorithms based on temporal frame context information fusion and feature enhancement. First, in order to make full use of historical frame information, this paper proposes a temporal frame residual fusion module to adaptively fuse historical frame information. Second, a spatial cascade mask refinement module is established to enhance the spatial information of the shallow features of the backbone network and refine the edge information of the fusion features. The experimental results show that our algorithm achieves the performance (J&amp;F) of 87.4% and 76.6% on DAVIS2016 and DAVIS2017 respectively and the segmentation speed (FPS) also meets the real-time requirements, reaching 26FPS on DAVIS2016 validation set. Contrast to many mainstream algorithms in recent years, it has obvious advantages in performance.},
  archive      = {J_APIN},
  author       = {Hou, Zhiqiang and Li, Fucheng and Wang, Shuiyuan and Dai, Nan and Ma, Sugang and Fan, Jiulun},
  doi          = {10.1007/s10489-022-03693-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6496-6510},
  shortjournal = {Appl. Intell.},
  title        = {Video object segmentation based on temporal frame context information fusion and feature enhancement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-guided salient object detection using autoencoder
regularization. <em>APIN</em>, <em>53</em>(6), 6481–6495. (<a
href="https://doi.org/10.1007/s10489-022-03917-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A saliency detection task simulates the attention mechanism of the human visual system, which focuses on what draws the most attention in a picture, and performs accurate localization and pixel-level segmentation of the object. Existing detection methods based on neural networks usually perform calculation of the object position information and edge information separately in each layer, resulting in calculation redundancy and insufficient utilization of information. To address this issue, this paper proposes an attention-guided detection network using an autoencoder (AGA-Net). First, using a proposed attention-guided multi-scale (AM) module, results from deep layers can be used to highlight features of the foreground and suppress features of the background and extract different scale features that are more relevant to the detection task. Second, a bi-refinement (BR) module composed of two sub-networks is proposed. One sub-network extracts information of the foreground to find redundant areas in the prediction results, and the other uses background information to supplement missing boundary information. Finally, the new model uses a variational autoencoder (VAE) branch to realize the image restoration task. It shares the encoder module with the object detection task and helps it escape from a local minimum in the converging process. Extensive experiments on six benchmark datasets were conducted and the proposed method is compared with 19 state-of-the-art methods, demonstrating that the new method has the best results.},
  archive      = {J_APIN},
  author       = {Xu, Cheng and Liu, Xianhui and Zhao, Weidong},
  doi          = {10.1007/s10489-022-03917-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6481-6495},
  shortjournal = {Appl. Intell.},
  title        = {Attention-guided salient object detection using autoencoder regularization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal semantic network for ENSO forecasting over
long time horizon. <em>APIN</em>, <em>53</em>(6), 6464–6480. (<a
href="https://doi.org/10.1007/s10489-022-03861-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {El Niño-Southern Oscillation (ENSO) has substantial influence on global climate variability and is responsible for extreme weather events such as drought and heavy rains, along with global ecosystem modifications. The successful prediction of ENSO is of considerable interest to reducing economic and social adverse effects. Recently, deep learning models show great potential in this task. However, despite decades of efforts, predicting ENSO at lead times of more than one year remains a major challenge and most of these existing studies focus on the single channel of meteorological data, ignoring the spatial and temporal dependence of these factors. To capture the spatiotemporal information of meteorological factors as well as promote the skill of ENSO forecasting over longer time horizon, we propose an end-to-end deep learning model SpatioTemporal Semantic Network named STSNet, which consists of three main modules: (1) Geographic Semantic Enhancement Module (GSEM) distinguishes the geographic semantics of various latitudes and longitudes through a learnable adaptive weight matrix. (2) A novel SpatioTemporal Convolutional Module(STCM) is designed specially to extract the multidimensional features by alternating the execution of temporal and spatial convolution and temporal attention. (3) Multi-scale temporal information is combined and exploited in Three-stream Temporal Scale Module (3sTSM) to further enhance the performance. Integrating these modules gives a powerful feature extractor STSNet, which has multi-scale receptive fields across both spatial and temporal dimensions. In order to verify the effectiveness and progressiveness of our model, we execute experiments on Historical Climate Observation and Simulation Dataset. The results show that STSNet can simultaneously provide effective ENSO prediction for 16 months, with higher correlation and lower deviation compared with other deep learning models.},
  archive      = {J_APIN},
  author       = {Zhao, Jiakun and Luo, Hailun and Sang, Weiguang and Sun, Kun},
  doi          = {10.1007/s10489-022-03861-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6464-6480},
  shortjournal = {Appl. Intell.},
  title        = {Spatiotemporal semantic network for ENSO forecasting over long time horizon},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method for measuring similarity of time series based on
series decomposition and dynamic time warping. <em>APIN</em>,
<em>53</em>(6), 6448–6463. (<a
href="https://doi.org/10.1007/s10489-022-03716-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic time warping (DTW) is one of the most important similarity measurement methods for time series analysis. In view of the high complexity and pathological alignment of DTW, a lot of variants of DTW have been proposed. However, the existing methods calculate the similarity between the original time series through dynamic programming directly, and ignore the characteristic that different components in the time series often have different degrees of importance. This paper proposes a time series similarity measurement method based on series decomposition and fast DTW, which combines time series decomposition method and DTW method. Series decomposition is an important means of time series analysis which can decompose time series into trend, seasonality, and remainder components. In this paper, after using the Seasonal-Trend decomposition using Loess (STL) method to decompose the time series, the similarity between the trend components and the similarity between seasonal components are respectively measured. The impact of the more important component is amplified, and then the comprehensive similarity measurement result will be obtained. Experimental results on 20 UCR time series datasets show that, compared with the existing fast DTW and constrained DTW and their variants, the method proposed in this paper achieves a higher classification accuracy. Simultaneously, combining the advantage of low complexity of fast DTW, the computational complexity of proposed method is still first-order linearly related to the length of time series.},
  archive      = {J_APIN},
  author       = {Zhang, Qingzhen and Zhang, Chaoqi and Cui, Langfu and Han, Xiaoxuan and Jin, Yang and Xiang, Gang and Shi, Yan},
  doi          = {10.1007/s10489-022-03716-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6448-6463},
  shortjournal = {Appl. Intell.},
  title        = {A method for measuring similarity of time series based on series decomposition and dynamic time warping},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIGA: Social influence modeling integrating graph
autoencoder for rating prediction. <em>APIN</em>, <em>53</em>(6),
6432–6447. (<a
href="https://doi.org/10.1007/s10489-022-03748-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the revival of social networks, many studies try to integrate social relations of users to improve the accuracy of rating prediction. However, most existing methods cannot accurately reflect how social relations affect user preferences. The main reason is that these methods only investigate the directly or indirectly reachable nodes in social networks, while ignoring global transitivity of influence. Actually, for a particular user, his preference will not only affect his neighbors but also some people he does not know, especially when the person is an opinion leader. In this paper, we propose a social influence model, SIGA, which integrates graph autoencoder and is used for the rating prediction task. First, we establish a new method to quantify the social influence of users from the perspective of information dissemination in social networks. Second, we employ graph autoencoder (GAE) to model the interaction between users and items from the view of message passing on bipartite graph, which can obtain high-quality representations of users and items. By building a hybrid architecture of social modeling and GAE, it is expected to be endowed with both benefits from them. In addition, our model is interpretable at both the structural level and attribute level. Extensive experimental results on four real-world datasets have shown the effectiveness and generalization of the proposed model.},
  archive      = {J_APIN},
  author       = {Liu, Jinxin and Xiao, Yingyuan and Zheng, Wenguang and Hsu, Ching-Hsien},
  doi          = {10.1007/s10489-022-03748-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6432-6447},
  shortjournal = {Appl. Intell.},
  title        = {SIGA: Social influence modeling integrating graph autoencoder for rating prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint-wise 2D to 3D lifting for hand pose estimation from a
single RGB image. <em>APIN</em>, <em>53</em>(6), 6421–6431. (<a
href="https://doi.org/10.1007/s10489-022-03764-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For monocular RGB based 3D hand pose estimation task, z coordinates are more difficult to estimate compared to the 2D hand joint coordinates due to the intrinsic depth ambiguity, thus some works firstly estimate the 2D hand joint coordinates and then apply a 2D to 3D lifting module to estimate the z coordinates. In this paper, we propose a new 2D to 3D lifting module. Differ from existing methods which estimate z coordinates of all hand joints simultaneously, we propose to estimate the z coordinate of each hand joint individually with its 2D joint features and the global image features as input. It can divide the complex task into simple sub-tasks, which makes it easier to lift the 2D coordinates to 3D. Besides, our 2D to 3D lifting module use only convolutional operation with shared convolutional kernel, which has fewer network parameters compared with existing methods usually with fully connected layers. Furthermore, we introduce a new inter joint attention module in our model to learn the correlation between every two hand joints. We conduct experiments on two popular hand pose datasets. From the experimental results we can see, our model gets state-of-the-art performance compared with existing methods. Ablation study also verifies the validity of each components proposed in our model.},
  archive      = {J_APIN},
  author       = {Chen, Zheng and Sun, Yi},
  doi          = {10.1007/s10489-022-03764-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6421-6431},
  shortjournal = {Appl. Intell.},
  title        = {Joint-wise 2D to 3D lifting for hand pose estimation from a single RGB image},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Misinformation influence minimization by entity protection
on multi-social networks. <em>APIN</em>, <em>53</em>(6), 6401–6420. (<a
href="https://doi.org/10.1007/s10489-022-03798-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive amounts of information are generated in various social media and spread across multi-social networks through individual forwarding and sharing, which greatly enhance the speed and scope of transmission, but also bring great challenges to the control and governance of misinformation. The characteristics of the spread of misinformation across multi-social networks are considered, this article investigates the novel problem of misinformation influence minimization by entity protection on multi-social networks, and systematically tackling this problem. We analyse the hardness and the approximation property of the problem. We construct a multi-social networks coupled method and devise a pruning and filtering rule. We develop a two-stage discrete gradient descent (TD-D) algorithm to solve NP-Hard problems. We also construct a two-stage greedy (TG) algorithm with the approximate guarantee to verify the algorithm we developed. Finally, the effectiveness of our proposed methods is analysed in synthetic and real multi-network datasets (contains up to 202K nodes and 2.5M edges). The results show that the ability of the TD-D and TG algorithms to suppress the spread of misinformation is basically the same, but the running time of the TG algorithm is much higher than (far more than 10 times) that of the TD-D algorithm.},
  archive      = {J_APIN},
  author       = {Ni, Peikun and Zhu, Jianming and Wang, Guoqing},
  doi          = {10.1007/s10489-022-03798-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6401-6420},
  shortjournal = {Appl. Intell.},
  title        = {Misinformation influence minimization by entity protection on multi-social networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced discrete dragonfly algorithm for solving four-color
map problems. <em>APIN</em>, <em>53</em>(6), 6372–6400. (<a
href="https://doi.org/10.1007/s10489-022-03791-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic combinatorial optimization problem of graph coloring is one of the most famous NP-complete problems. One example of the graph coloring problem is the four-color map problem. There have been many applications of swarm intelligence optimization algorithms to this problem, but to date, such algorithms can only solve the four-color map problem with fewer than 100 regions. This article proposes an enhanced discrete dragonfly algorithm (EDDA) for four-color map problems. We use global and local discrete alternate search strategies-when there is at least one adjacent dragonfly around the i-th dragonfly, a global search is performed; when there are no other dragonflies around, a local search is performed. A greedy strategy, local differential cross strategy, and single-point switching strategy are then used to solve the problem of conflicts among adjacent nodes. Finally, six real-life maps are colored to verify the effectiveness of the proposed algorithm. The experimental results show that the proposed EDDA algorithm can solve the four-color map problem with more than 100 regions.},
  archive      = {J_APIN},
  author       = {Zhong, Lianlian and Zhou, Yongquan and Zhou, Guo and Luo, Qifang},
  doi          = {10.1007/s10489-022-03791-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6372-6400},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced discrete dragonfly algorithm for solving four-color map problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Two novel style-transfer palmprint reconstruction attacks.
<em>APIN</em>, <em>53</em>(6), 6354–6371. (<a
href="https://doi.org/10.1007/s10489-022-03862-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint has been widely used for personal authentication in many applications, such that the assessment of recognition system security is important. Online attacks of palmprint recognition are much more difficult than offline attacks due to the fewer permissible login and authentication attempts, the unusability of the matching scores, and less training data. A cross-database attack is another challenging problem, where the images reconstructed from a template can still be effective in attacking the systems with other templates. To achieve online cross-database attacks and ensure that the reconstructed images are high-quality, two novel style-transfer methods are proposed to attack coding-based palmprint recognition systems. The two methods are both based on a convolutional neural network, but their optimization objects are different. In the first method, the optimization object is the input image, where a high-quality image can be reconstructed from the binary template. In the second method, the style-transfer neural network is trained with a template dataset and only one style image to reduce the style loss between the source and target domains. The trained style-transfer network can reconstruct approximately 270 images per second. The two methods have highly impressive attack success rates and satisfactorily meet the requirements of the evaluation system.},
  archive      = {J_APIN},
  author       = {Yang, Ziyuan and Leng, Lu and Zhang, Bob and Li, Ming and Chu, Jun},
  doi          = {10.1007/s10489-022-03862-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6354-6371},
  shortjournal = {Appl. Intell.},
  title        = {Two novel style-transfer palmprint reconstruction attacks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight and efficient model for surface tiny defect
detection. <em>APIN</em>, <em>53</em>(6), 6344–6353. (<a
href="https://doi.org/10.1007/s10489-022-03633-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the tiny size and the fuzzy pixels of tiny objects, tiny defect detection is a thorny problem in industry application. Meanwhile, the real-time detection of tiny defects is highly required in industrial assembly line. That is to say, tiny defect detection algorithms must ensure high accuracy while maintaining a low computational cost. This paper designs a lightweight and efficient network for tiny defect detection. In the backbone network, Diagonal Feature Pyramid (DFP) is proposed to improve the performance of tiny defect detection. For higher accuracy, DFP fuses more original features if they are at the same level. For less computational cost, DFP reduces the model size by eliminating the bottom-up pathway and removing some non-original same-level features. In the neck network, a multi-scale neck network with some fusion strategies is designed to suit multi-scale tiny defect detection. Finally, an adaptive localization loss function is designed to adjust the sensitivity of tiny defects. Based on a public PCB (Printed Circuit Board) dataset, the comparative experiments show that our model has better mAP and higher speed than various mainstream defect detection algorithms.},
  archive      = {J_APIN},
  author       = {Yu, Zhilong and Wu, Yuxiang and Wei, Binqian and Ding, Zikang and Luo, Fei},
  doi          = {10.1007/s10489-022-03633-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6344-6353},
  shortjournal = {Appl. Intell.},
  title        = {A lightweight and efficient model for surface tiny defect detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rehabilitation robot following motion control algorithm
based on human behavior intention. <em>APIN</em>, <em>53</em>(6),
6324–6343. (<a
href="https://doi.org/10.1007/s10489-022-03823-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the current problem of low intelligence of mobile lower limb motor rehabilitation aids. This paper proposes an intelligent control scheme based on human movement behavior in order to control the rehabilitation robot to follow the patient’s movement. Firstly, a multi-sensor data acquisition system is designed according to the rehabilitation needs of the patient and the movement characteristics of the human body. A mathematical model of movement behavior is then established. By analyzing and processing motion data, the change in the center of gravity of the human body and the behavior intention signal are derived and used as a control command for the robot to follow the human body’s movement. Secondly, in order to improve the control effect of rehabilitation robot following human motion, an adaptive radial basis function neural network sliding mode controller (ARBFNNSMC) is designed based on the robot dynamic model. The adaptive adjustment of switching gain coefficient is performed by radial basis function neural network. The controller can overcome the influence caused by the change of robot control system parameters due to the fluctuation of the center of gravity of human body, enhance the adaptability of the system to other disturbance factors, and improve the accuracy of following human body motion. Finally, the motion following experiment of the rehabilitation robot is performed. The experimental results show that the robot can recognize the motion intention of human body and perform the training goal of following different subjects to complete straight lines and curves. The correctness of human motion behavior model and robot control algorithm is verified, which shows the feasibility of the intelligent control method proposed in this paper.},
  archive      = {J_APIN},
  author       = {Miao, Ming da and Gao, Xue shan and Zhao, Jun and Zhao, Peng},
  doi          = {10.1007/s10489-022-03823-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6324-6343},
  shortjournal = {Appl. Intell.},
  title        = {Rehabilitation robot following motion control algorithm based on human behavior intention},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local structure consistency and pixel-correlation
distillation for compact semantic segmentation. <em>APIN</em>,
<em>53</em>(6), 6307–6323. (<a
href="https://doi.org/10.1007/s10489-022-03656-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art semantic segmentation methods usually contain millions of parameters and require high computational resources, which limit their applications in the low resources cases. Knowledge distillation is one promising way to achieve a good trade-off between performance and efficiency. In this paper, we propose a novel local structure consistency distillation (LSCD) to improve the segmentation accuracy of compact networks. Different from previous works mainly transferring the pixel-level and image-level knowledge, we propose to transfer the patch-level knowledge. Specially, we propose the local structure consistency as the patch-level knowledge, which integrate the structural similarity index measure into our framework to provide some local structural constrains between the outputs of teacher and the student. Furthermore, we propose the pixel-correlation distillation to capture the contextual dependencies between any two pixels of the feature maps in a global view. Distilling such pixel correlations from the teacher to the student could help the student mimic the teacher better in terms of contextual dependencies, and thus improve the segmentation accuracy. To validate the effectiveness of the proposed approach, extensive experiments have been conducted on three widely adopted benchmarks: Cityscapes, CamVid, and Pascal VOC 2012. Experimental results show that the proposed approach could consistently improve state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Chen and Zhong, Jiang and Dai, Qizhu and Li, Rongzhen and Yu, Qien and Fang, Bin},
  doi          = {10.1007/s10489-022-03656-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6307-6323},
  shortjournal = {Appl. Intell.},
  title        = {Local structure consistency and pixel-correlation distillation for compact semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization of the structural complexity of artificial
neural network for hardware-driven neuromorphic computing application.
<em>APIN</em>, <em>53</em>(6), 6288–6306. (<a
href="https://doi.org/10.1007/s10489-022-03783-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the optimization of the structural complexity of a single-layer feedforward neural network (SLFN) for neuromorphic hardware implementation. The singular value decomposition (SVD) method is used for the determination of the effective number of neurons in the hidden layer for Modified National Institute of Standards and Technology (MNIST) dataset classification. The proposed method is also verified on a SLFN using weights derived from a synaptic transistor device. The effectiveness of this methodology in estimating the reduced number of neurons in the hidden layer makes this method highly useful in optimizing complex neural network architectures for their hardware realization.},
  archive      = {J_APIN},
  author       = {Udaya Mohanan, Kannan and Cho, Seongjae and Park, Byung-Gook},
  doi          = {10.1007/s10489-022-03783-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6288-6306},
  shortjournal = {Appl. Intell.},
  title        = {Optimization of the structural complexity of artificial neural network for hardware-driven neuromorphic computing application},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer perceptron based fake news classification using
knowledge base triples. <em>APIN</em>, <em>53</em>(6), 6276–6287. (<a
href="https://doi.org/10.1007/s10489-022-03627-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent attempts to detect fake news have relied on the implementation of machine or deep learning models that have been trained on text. These models, on the other hand, are insufficient for classifying knowledge base facts or triples as fake or true. However, it is critical to assess the credibility of facts before they are included to the knowledge base. Hence, this paper suggests using a Multi-layer Perceptron to categorize a given triple as fake or true. Furthermore, extant works embed the features using either frequency or prediction based word embedding models, and thus both document and word level features are not captured. To address this issue, a data modeling approach is proposed that vectorizes the triples using two cutting-edge word embedding models, Wrod2Vec and GloVe, as well as TF-IDF and Counter Vectorizer. Empirical results show that the Multi-layer Perceptron with GloVe and count vectorizer outperforms the baseline model in terms of accuracy. Moreover, named entity tags associated with the entities, such as PERSON, add an extra feature for training the models. As a result, an algorithm that jointly extracts the triples along with named entity tags is also proposed. Experiments demonstrated that models trained on triples with named entity tags produce high accuracy.},
  archive      = {J_APIN},
  author       = {K, Srinivasa and Thilagam, P Santhi},
  doi          = {10.1007/s10489-022-03627-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6276-6287},
  shortjournal = {Appl. Intell.},
  title        = {Multi-layer perceptron based fake news classification using knowledge base triples},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Root quantization: A self-adaptive supplement STE.
<em>APIN</em>, <em>53</em>(6), 6266–6275. (<a
href="https://doi.org/10.1007/s10489-022-03691-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low precision deep neural network model quantization can further reveal stronger abilities of models such as shorter inference time, lower energy consumption and memory usage, but meanwhile induce performance degradation and instability during training. Straight Through Estimator (STE) is widely used in Quantization-Aware-Training (QAT) to overcome these shortcomings, and achieves good results on (2-, 3-, 4-bit) quantization. Different STE function may achieve different performance under various quantization precision settings. In order to explore the applicable bit-width settings range of STE functions and stabilize the training process, we propose Root Quantization. Root Quantization combines two estimators, the linear estimator and the root estimator. While linear estimator is based on existing methods of training quantizer and weights under task loss function, root estimator is based on high degree root and acts as a correction module to fine-tune the weights, which not only approximates the gradient of quantization error, but also makes the gradient more accurate. Root estimator can also adapt and adjust each layer’s root degree to the most suitable value through the task loss gradient. Extensive experimental results on CIFAR-10 and ImageNet, with different network architectures under various bit-width range, show the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Zhang, Luoming and He, Yefei and Lou, Zhenyu and Ye, Xin and Wang, Yuxing and Zhou, Hong},
  doi          = {10.1007/s10489-022-03691-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6266-6275},
  shortjournal = {Appl. Intell.},
  title        = {Root quantization: A self-adaptive supplement STE},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph cooperation deep reinforcement learning for ecological
urban traffic signal control. <em>APIN</em>, <em>53</em>(6), 6248–6265.
(<a href="https://doi.org/10.1007/s10489-022-03208-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions.},
  archive      = {J_APIN},
  author       = {Yan, Liping and Zhu, Lulong and Song, Kai and Yuan, Zhaohui and Yan, Yunjuan and Tang, Yue and Peng, Chan},
  doi          = {10.1007/s10489-022-03208-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6248-6265},
  shortjournal = {Appl. Intell.},
  title        = {Graph cooperation deep reinforcement learning for ecological urban traffic signal control},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A vehicle alarm network for high-temperature fault diagnosis
of electric vehicles. <em>APIN</em>, <em>53</em>(6), 6230–6247. (<a
href="https://doi.org/10.1007/s10489-022-03615-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual checking is necessary for electric vehicles because error diagnosis exist in the actual electric vehicle fault diagnosis process. To effectively perform detection tasks, we propose a VANet model to classify the true and the false third-level high-temperature fault of EVs of unbalanced samples. Firstly, we introduce an improved method which is called Weighted SMOTE to mitigate the imbalance between samples, that determines the number of positive and negative samples by calculating the weight of minority samples. Secondly, we design a two-Conv-one-Maxpool module to build the VANet network, using the GAP and the softmax as the output layer. And we add the dropout to reduce hidden layer neurons to avoid overfitting. Finally, we employ the least square method to optimize the custom loss function to deal with the problem of ignoring incorrect label features. The experimental results show that compared with other convolutional networks, our model can enhance the recognition ability of minority fault samples, and has a strong anti-interference ability against sample noise.},
  archive      = {J_APIN},
  author       = {Hou, Qing and Liu, Jun and Zhang, Jianxing and Xu, Zihan and Chen, Xiao and Chen, Peng},
  doi          = {10.1007/s10489-022-03615-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6230-6247},
  shortjournal = {Appl. Intell.},
  title        = {A vehicle alarm network for high-temperature fault diagnosis of electric vehicles},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FGO-net: Feature and gaussian optimization network for
visual saliency prediction. <em>APIN</em>, <em>53</em>(6), 6214–6229.
(<a href="https://doi.org/10.1007/s10489-022-03647-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have become a major driving force for visual saliency prediction. However, since the features of different layers have diverse characteristics, not all of them are effective for saliency detection, and some even cause interference. To effectively fuse multiscale features from different layers, in this paper, we propose a feature and Gaussian optimization network (FGO-Net) for saliency prediction. Specifically, we first design a novel attention ConvLSTM (ACL) module that contains circular soft layer attention to iteratively reweight multilevel features from different layers. Besides, previous works generally employed Gaussian blur with fixed kernel size after saliency maps generation as post-processing. To this end, we design an adaptive Gaussian blur (AGB) module that can automatically select the appropriate Gaussian kernel size to blur the saliency map without post-processing. Extensive experiments on several public saliency datasets demonstrate that the proposed FGO-Net achieves competitive results in terms of various evaluation metrics.},
  archive      = {J_APIN},
  author       = {Pei, Jialun and Zhou, Tao and Tang, He and Liu, Chao and Chen, Chuanbo},
  doi          = {10.1007/s10489-022-03647-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6214-6229},
  shortjournal = {Appl. Intell.},
  title        = {FGO-net: Feature and gaussian optimization network for visual saliency prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards better generalization in quadrotor landing using
deep reinforcement learning. <em>APIN</em>, <em>53</em>(6), 6195–6213.
(<a href="https://doi.org/10.1007/s10489-022-03503-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the autonomous landing of unmanned aerial vehicles (UAVs) has attracted extensive attention due to the widespread applications of UAVs. With the rapid improvements in machine learning and artificial intelligence, recent research has begun to explore deep reinforcement learning (DRL) to learn the landing policy directly from raw observation data. However, current DRL-based solutions tend to suffer poor generalization to unseen environments. To deal with this issue, we formulate the landing problem as a two-stage DRL problem and bootstrap the DRL procedures by augmenting regular DRL loss with an auxiliary localization task. The auxiliary localization task provides dense supervision signals that aid in landing-relevant representation learning. In particular, two marker localization approaches are delicately designed based on deep classification and regression models, and differences between the two configurations are explored, aiming to answer the fundamental question of how to exploit localization better for representation learning. Furthermore, we propose a novel and flexible sampling strategy called Dynamic Partitioned Experience Replay to stabilize and accelerate the training procedure. Experimental results show that the auxiliary localization tasks combined with the improved sampling strategy aid the trained model to generalize in unseen environments. In addition, the trained model can be seamlessly transferred to the real-world quadrotors and has achieved outstanding landing performances.},
  archive      = {J_APIN},
  author       = {Wang, Jiawei and Wang, Teng and He, Zichen and Cai, Wenzhe and Sun, Changyin},
  doi          = {10.1007/s10489-022-03503-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6195-6213},
  shortjournal = {Appl. Intell.},
  title        = {Towards better generalization in quadrotor landing using deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary digital twin model with an agent-based
discrete-event simulation method. <em>APIN</em>, <em>53</em>(6),
6178–6194. (<a
href="https://doi.org/10.1007/s10489-022-03507-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A digital twin model provides the ability to adjust candidate behavior based on feedback from its physical part. However, small interactions between different subsystems and using real-time data about physical workshops are the primary problems in digital twin models. The essence of the digital twin model is the combination of the physical simulation method and the data-driven simulation method, and agent-based discrete-event modeling theory is an advanced way to build a digital twin model. Thus, the theoretical framework of the Digital Twin Workshop model is improved from the underlying modeling logic based on this new theory. By combining reinforcement learning with the digital twin workshop model, an evolutionary digital twin workshop model is developed in this study. This model is then applied to a real-world case. A comparison is made between the Digital Twin Workshop model with reinforcement learning policy and a heuristic policy and a random policy. The simulation results verify the validity and performance of the proposed model.},
  archive      = {J_APIN},
  author       = {Qiu, Hongbin and Chen, Yong and Zhang, Huaxiang and Yi, Wenchao and Li, Yingde},
  doi          = {10.1007/s10489-022-03507-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6178-6194},
  shortjournal = {Appl. Intell.},
  title        = {Evolutionary digital twin model with an agent-based discrete-event simulation method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-lingual knowledge graph entity alignment based on
relation awareness and attribute involvement. <em>APIN</em>,
<em>53</em>(6), 6159–6177. (<a
href="https://doi.org/10.1007/s10489-022-03797-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment is an effective means of matching entities from various knowledge graphs (KGs) that represent the equivalent real-world object. With the development of representation learning, recent entity alignment methods learn entity structure representation by embedding KGs into a low-dimensional vector space, and then entity alignment relies on the distance between entity vectors. In addition to the graph structures, relations and attributes are also critical to entity alignment. However, most existing approaches ignore the helpful features included in relations and attributes. Therefore, this paper presents a new solution RAEA (Relation Awareness and Attribute Involvement for Entity Alignment), which includes relation and attribute features. Relation representation is incorporated into entity representation by Dual-Primal Graph CNN (DPGCNN), which alternates convolution-like operations on the original graph and its dual graph. Structure representation and attribute representation are learned by graph convolutional networks (GCNs). To further enrich the entity embedding, we integrate the textual information of the entity into the entity graph embedding. Moreover, we fine-tune the entity similarity matrix by integrating fine-grained features. Experimental results on three benchmark datasets from real-world KGs show that our approach has superior performance to other representative entity alignment approaches in most cases.},
  archive      = {J_APIN},
  author       = {Zhu, Beibei and Bao, Tie and Liu, Lu and Han, Jiayu and Wang, Junyi and Peng, Tao},
  doi          = {10.1007/s10489-022-03797-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6159-6177},
  shortjournal = {Appl. Intell.},
  title        = {Cross-lingual knowledge graph entity alignment based on relation awareness and attribute involvement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Harris hawks optimizer based on the novice protection
tournament for numerical and engineering optimization problems.
<em>APIN</em>, <em>53</em>(6), 6133–6158. (<a
href="https://doi.org/10.1007/s10489-022-03743-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Harris hawks optimizer (HHO) is a novel meta-heuristic algorithm that imitates a Harris hawk’s hunting behavior and has an efficient exploitation mode. However, it suffers from low exploration because the transition of the search style is mainly based on the escape energy and it focuses on exploitation in the middle and later periods of the algorithm. In this paper, to overcome the weaknesses of the HHO, a Harris hawks optimizer based on the novice protection tournament (NpTHHO) is proposed to overcome the weaknesses of the HHO. Inspired by the root-mean-square prop (RMSProp) in machine learning, we first propose a novice protection mechanism to better reallocate resources. Then, we add a mutation mechanism to the exploration stage to further improve the global search efficiency of the HHO. Finally, we take into consideration 23 benchmark functions and several engineering optimization problems to verify the performance of the proposed algorithm. Experimental results indicate the proposed algorithm’s competitive performance compared to the HHO and other well-established algorithms.},
  archive      = {J_APIN},
  author       = {Li, Wenyu and Shi, Ronghua and Dong, Jian},
  doi          = {10.1007/s10489-022-03743-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6133-6158},
  shortjournal = {Appl. Intell.},
  title        = {Harris hawks optimizer based on the novice protection tournament for numerical and engineering optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-branch detection network based on trigger attention
for pedestrian detection under occlusion. <em>APIN</em>, <em>53</em>(6),
6119–6132. (<a
href="https://doi.org/10.1007/s10489-022-03747-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Center and Scale Prediction (CSP) first introduced the Anchor-free method to the field of pedestrian detection. Pedestrian detection often occurs in complex scenes subject to occlusion, and it is difficult to extract pedestrian features in a single centre point prediction in CSP. To solve this problem, this paper presents a multi-branch detection network (MBDN) based on trigger attention. Firstly, a multi-centre point prediction branch feature extraction model (multi-centre) is proposed to solve the problem of CSP missed detections in occlusion scenarios. Secondly, a novel trigger attention module is designed. The module uses visible parts as triggers to automatically learn the weight relationships of multiple branches, let the network automatically learn the confidence of the centre points of different branches, and automatically strengthen the branch where the visible area on the feature map is located. Finally, a channel non-maximum suppression (NMS) module is used in the MBDN network to reduce the redundant bounding boxes. Then experiments results show that the log-average missing rate (MR−2) of the heavy subset is reduced from 49.63% to 45.51% while maintaining the performance on a reasonable subset. Code and models can be accessed at ( https://github.com/weidalin/MBDN ).},
  archive      = {J_APIN},
  author       = {Wang, Zhuowei and Lin, Weida and Cheng, Lianglun and Song, Xiaoyu and Wang, Yang},
  doi          = {10.1007/s10489-022-03747-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {6},
  pages        = {6119-6132},
  shortjournal = {Appl. Intell.},
  title        = {Multi-branch detection network based on trigger attention for pedestrian detection under occlusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network input feature selection using structured l2 −
norm penalization. <em>APIN</em>, <em>53</em>(5), 5732–5749. (<a
href="https://doi.org/10.1007/s10489-022-03539-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks are referred to as universal approximators due to their inherent ability to reconstruct complex linear and nonlinear output maps conceived as input-output relationships from data sets. This can be done by reducing large networks via regularization in order to establish compact models containing fewer parameters aimed at describing vital dependencies in data sets. In situations where the data sets contain non-informative input features, devising a continuous, optimal input feature selection technique can lead to improved prediction or classification. We propose a continuous input selection technique through a dimensional reduction mechanism using a ‘structured’ l2 − norm regularization. The implementation is done by identifying the most informative feature subsets from a given data set via an adaptive training mechanism. The adaptation involves introducing a novel, modified gradient approach during training to deal with the non-differentiability associated with the gradient of the structured norm penalty. When the method is applied to process data sets, results indicate that the most informative inputs of artificial neural networks can be selected using a structured l2 − norm penalization.},
  archive      = {J_APIN},
  author       = {Egwu, Nathaniel and Mrziglod, Thomas and Schuppert, Andreas},
  doi          = {10.1007/s10489-022-03539-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5732-5749},
  shortjournal = {Appl. Intell.},
  title        = {Neural network input feature selection using structured l2 − norm penalization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved deep belief neural network based civil unrest
event forecasting in twitter. <em>APIN</em>, <em>53</em>(5), 5714–5731.
(<a href="https://doi.org/10.1007/s10489-022-03746-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, event forecasting in Twitter can be considered an essential, significant and difficult issue. Maximum conventional methods are focusing on temporal events like sports or elections. These methods do not calculate the spatial features too their correlation analysis. Hence, this paper proposes an Improved Deep Belief Neural Network (iDBNN) for civil unrest event forecasting in twitter data. This proposed method is utilized to forecast the future event with the consideration of the tweets. The proposed method is designed with three phases named as pre-processing phase, feature extraction phase, and civil unrest event forecasting. Initially, the proposed method is used to train the Hong Kong Protest event 2019 tweet data for forecasting events. In the pre-processing phase, removal of special symbol, removal of URL, username removal, tokenization and stop word removal are done. After that, the essential features such as domain weight, event weight, textual similarity, spatial similarity, temporal similarity, and Relative Document-Term Frequency Difference (RDTFD) are extracted and then applied for training the proposed model. To empower the training phase of proposed iDBNN method, the Jellyfish Algorithm is utilized to select optimal weight parameter coefficients of DBNN for training the model parameters. The projected technique is authenticated by statistical capacities and compared with the conventional methods such as Hidden Markov Model (HMM) and Random Forest (RF) respectively. Comparing with other traditional methods, the proposed model shows better performance in terms of prediction and processing time. The iDBNN model shows 91% prediction accuracy that is much higher than the traditional DBNN.},
  archive      = {J_APIN},
  author       = {Iyda, J. Joslin and Geetha, P.},
  doi          = {10.1007/s10489-022-03746-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5714-5731},
  shortjournal = {Appl. Intell.},
  title        = {An improved deep belief neural network based civil unrest event forecasting in twitter},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IV-GNN: Interval valued data handling using graph neural
network. <em>APIN</em>, <em>53</em>(5), 5697–5713. (<a
href="https://doi.org/10.1007/s10489-022-03780-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued data is an effective way to represent complex information where uncertainty, inaccuracy etc. are involved in the data space and they are worthy of taking into account. Interval analysis together with neural network has proven to work well on Euclidean data. However, in real-life scenarios, data follows a much more complex structure and is often represented as graphs, which is non-Euclidean in nature. Graph Neural Network is a powerful tool to handle graph like data with countable feature space. So, there is a research gap between the interval-valued data handling approaches and existing GNN model. No model in GNN literature can handle a graph with interval-valued features and, on the other hand, Multi Layer Perceptron (MLP) based on interval mathematics can not process the same due to non-Euclidean structure behind the graph. This article proposes an Interval-Valued Graph Neural Network, a novel GNN model where, for the first time, we relax the restriction of the feature space being countable without compromising the time complexity of the best performing GNN model in the literature. Our model is much more general than existing models as any countable set is always a subset of the universal set $\mathbb {R}^{n}$ , which is uncountable. Here, to deal with interval-valued feature vectors, we propose a new aggregation scheme of intervals and show its expressive power to capture different interval structures. We validate our theoretical findings about our model for graph classification task by comparing its performance with those of the state-of-the-art models on several benchmark and synthetic network datasets.},
  archive      = {J_APIN},
  author       = {Dawn, Sucheta and Bandyopadhyay, Sanghamitra},
  doi          = {10.1007/s10489-022-03780-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5697-5713},
  shortjournal = {Appl. Intell.},
  title        = {IV-GNN: Interval valued data handling using graph neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ExpRec: Deep knowledge-awared question routing in software
question answering community. <em>APIN</em>, <em>53</em>(5), 5681–5696.
(<a href="https://doi.org/10.1007/s10489-022-03369-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software question answering community (SQAC) as an effective platform of knowledge sharing has achieved rapid development. In SQAC, one critical and challenging problem is question routing (or expert recommendation). To solve this problem, previous approaches focus on learning the relevance between the question and answerers. However, such approaches usually suffer from the data sparsity and noise issues which may reduce the accuracy of the question routing. Moreover, previous approaches also ignored the response quality and timeliness of the question routing. To tackle those issues, we study the question routing problem from two aspects: 1) the answerer’s relevance to the given question, and 2) the answerer’s capability. We first propose a deep knowledge-awared question routing framework (termed ExpRec) which leverages the attentive embedding propagates and their high-order connectivities to learn the answerer’s relevance to the given question. Then we explicitly model the answerer’s capability and incorporate it with the answerer’s relevance to the given question. Finally, to evaluate the performance of ExpRec, we conduct extensive experiments on two real-world datasets. The experimental results show that ExpRec outperforms other five state-of-the-art approaches significantly.},
  archive      = {J_APIN},
  author       = {Liu, Jiahui and Deng, Ansheng and Xie, Xinqiang and Xie, Qiuju},
  doi          = {10.1007/s10489-022-03369-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5681-5696},
  shortjournal = {Appl. Intell.},
  title        = {ExpRec: Deep knowledge-awared question routing in software question answering community},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater vision enhancement based on GAN with dehazing
evaluation. <em>APIN</em>, <em>53</em>(5), 5664–5680. (<a
href="https://doi.org/10.1007/s10489-022-03789-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater vision faces the problem of visual degradation and haze caused by absorption and scattering of light. Therefore, underwater vision enhancement need consider both dehazing and processing speed in a changing underwater environment. In this paper, a generative adversarial network with dehazing evaluation (GAN-DE) is proposed to realize underwater vision enhancement. The comprehensive training datasets which include synthetic underwater image training datasets and underwater image training datasets are proposed to train the networks. Besides, the structure of generator is designed as combined Unet and Variety of View Network (VoVnet). In particular, the color-line loss function is designed based on color-line model which is used to describe the degree of haze. The discriminator includes color-line loss and adversarial loss so as to simultaneously remove haze while preserving the underwater image content. Qualitative, quantitative, and color-accuracy analyses based on experimental results show the superiority of GAN-DE over current state-of-the-art methods. More importantly, a video enhancement experiments conducted on the seabed and obtained satisfactory results.},
  archive      = {J_APIN},
  author       = {Yu, Haifeng and Li, Xinbin and Feng, Yankai and Han, Song},
  doi          = {10.1007/s10489-022-03789-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5664-5680},
  shortjournal = {Appl. Intell.},
  title        = {Underwater vision enhancement based on GAN with dehazing evaluation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards continuous consistency axiom. <em>APIN</em>,
<em>53</em>(5), 5635–5663. (<a
href="https://doi.org/10.1007/s10489-022-03710-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is shown for the first time in this paper, that Kleinberg’s (2002) (self-contradictory) axiomatic system for distance-based clustering fails (that is one of the data transforming axioms, consistency axiom, turns out to be identity transformation) in fixed-dimensional Euclidean space due to the consistency axiom limitations and that its replacement with inner-consistency or outer consistency does not help if continuous data transformations are required. Therefore we formulate a new, sound axiomatic framework for cluster analysis in the fixed dimensional Euclidean space, suitable for k-means like algorithms. The system incorporates centric consistency axiom and motion consistency axiom which induce clustering preserving transformations useful e.g. for deriving new labelled sets for testing clustering procedures. It is suitable for continuous data transformations so that labelled data with small perturbations can be derived. Unlike Kleinberg’s consistency, the new axioms do not lead the data outside of Euclidean space nor cause increase in data dimensionality. Our cluster preserving transformations have linear complexity in data transformation and checking. They are in practice less restrictive, less rigid than Kleinberg’s consistency as they do not enforce inter-cluster distance increase and inner cluster distance decrease when performing clustering preserving transformation.},
  archive      = {J_APIN},
  author       = {Kłopotek, Mieczysław A. and Kłopotek, Robert A.},
  doi          = {10.1007/s10489-022-03710-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5635-5663},
  shortjournal = {Appl. Intell.},
  title        = {Towards continuous consistency axiom},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiple classifiers time-serial ensemble pruning
algorithm based on the mechanism of forward supplement. <em>APIN</em>,
<em>53</em>(5), 5620–5634. (<a
href="https://doi.org/10.1007/s10489-022-03855-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because there are lots of typical applications and urgent needs, the research on the efficient classification learning about accumulated big data in nonstationary environments has become one of the hot topics in the field of data mining recently. The LearnNSE algorithm is an important research result in this field. For the long-term accumulated big data, the LearnNSE-Pruned-Age, a pruning version of LearnNSE, was given, which has received widespread attentions. However, it is found that the pruning mechanism of the LearnNSE-Pruned-Age algorithm is not perfect, which lost the core ability of the LearnNSE algorithm to reuse the learned classification knowledge. Therefore, the ensemble mechanism of LearnNSE is adjusted in this paper, and a novel ensemble mechanism is designed. The new mechanism uses the integration of the latest base-classifiers to track the changes of the data generation environment, and then selects the old base-classifiers that contribute to the current classification for forward supplementary integration. On this basis, a new pruned algorithm named FLearnNSE-Pruned-Age is proposed. The experiment results show that the FLearnNSE-Pruned-Age algorithm has the ability to reuse the learned classification knowledge and it can achieve the very close classification accuracy compared to LearnNSE, even better in some scenes. In addition, it improves the efficiency of ensemble learning and is suitable for the fast classification learning of accumulated big data.},
  archive      = {J_APIN},
  author       = {Shen, Yan and Jing, Luyi and Gao, Tian and Song, Zizhao and Ma, Ji},
  doi          = {10.1007/s10489-022-03855-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5620-5634},
  shortjournal = {Appl. Intell.},
  title        = {A multiple classifiers time-serial ensemble pruning algorithm based on the mechanism of forward supplement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge distilled pre-training model for
vision-language-navigation. <em>APIN</em>, <em>53</em>(5), 5607–5619.
(<a href="https://doi.org/10.1007/s10489-022-03779-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language-navigation(VLN) is a challenging task that requires a robot to autonomously move to a destination based on visual observation following a human’s natural language instructions. To improve the performance and generalization ability, the pre-training model based on the transformer is used instead of the traditional methods. However, the pre-training model is not suitable for sustainable computing and practical application because of its complex computations and large amount of hardware occupation. Therefore, we propose a slight pre-training model through knowledge distillation. Through knowledge distillation, the plenty of knowledge encoded in a large “teacher” model can be well transferred to a small “student” model, which greatly reduces the model parameters and inference time while maintaining the original performance. In the experiments, the model size is reduced by 87%, and the average inference time is reduced by approximately 86%. It can be trained and run much faster. At the same time, 95% performance of the original model was maintained, which is still better than the traditional VLN models.},
  archive      = {J_APIN},
  author       = {Huang, Bo and Zhang, Shuai and Huang, Jitao and Yu, Yijun and Shi, Zhicai and Xiong, Yujie},
  doi          = {10.1007/s10489-022-03779-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5607-5619},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge distilled pre-training model for vision-language-navigation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image classification based on quaternion-valued capsule
network. <em>APIN</em>, <em>53</em>(5), 5587–5606. (<a
href="https://doi.org/10.1007/s10489-022-03849-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel quaternion-valued (QV) capsule module is designed to construct QV capsule networks for image classification. The quaternion algebra is introduced into the capsule networks to effectively capture the external dependencies and internal structural information. Moreover, the QV capsules can enhance the representation of complex information and alleviate the information loss of vanilla capsule networks. Particularly, a non-iterative quaternion routing algorithm is proposed to integrate QV capsules, considering both the membership and the consistency of QV capsules in two stages. Extensive experiments are conducted on classic image datasets, hyperspectral image datasets, and face datasets, which demonstrate that: firstly, the QV capsule network achieves higher classification accuracy, reaching 92.95% in UC Merced Land Use and 95.02% in CIFAR 10; secondly, the QV capsule module is more adaptable to different backbone networks than the vanilla capsule module; finally, the QV capsule network shows high performance with limited training samples.},
  archive      = {J_APIN},
  author       = {Zhou, Heng and Zhang, Chunlei and Zhang, Xin and Ma, Qiaoyu},
  doi          = {10.1007/s10489-022-03849-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5587-5606},
  shortjournal = {Appl. Intell.},
  title        = {Image classification based on quaternion-valued capsule network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chaos embedded opposition based learning for gravitational
search algorithm. <em>APIN</em>, <em>53</em>(5), 5567–5586. (<a
href="https://doi.org/10.1007/s10489-022-03786-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its robust search mechanism, Gravitational search algorithm (GSA) has achieved a lot of popularity in different research communities. However, stagnation reduces its searchability towards global optima for rigid and complex multi-modal problems. This paper proposes a GSA variant that incorporates chaos-embedded opposition-based learning into the basic GSA for the stagnation-free search. Additionally, a sine-cosine based chaotic gravitational constant is introduced to balance the trade-off between exploration and exploitation capabilities more effectively. The proposed variant is tested over 23 classical benchmark problems, 15 test problems of CEC 2015 test suite, and 15 test problems of CEC 2014 test suite. Different graphical, as well as empirical analyses, reveal the superiority of the proposed algorithm over conventional meta-heuristics and most recent GSA variants.},
  archive      = {J_APIN},
  author       = {Joshi, Susheel Kumar},
  doi          = {10.1007/s10489-022-03786-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5567-5586},
  shortjournal = {Appl. Intell.},
  title        = {Chaos embedded opposition based learning for gravitational search algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic choroid layer segmentation in OCT images via
context efficient adaptive network. <em>APIN</em>, <em>53</em>(5),
5554–5566. (<a
href="https://doi.org/10.1007/s10489-022-03723-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Coherence Tomography (OCT) is a non-invasive and newly-developing technique to image human retina and choroid. Many ocular diseases such as pathological myopia and Age-related Macular Degeneration (AMD) are related to the morphological changes of the choroid. Consequently, the automatic choroid segmentation becomes an important step to the examination and diagnosis of those choroid-related diseases. However, there are still challenges such as the inseparability of the histogram between the choroid and sclera boundaries and the inconsistency of the choroid layer texture and intensity. To solve those challenges, we propose a Context Efficient Adaptive network (CEA-Net) that includes a module of Efficient Channel Attention (ECA), a novel block called adaptive morphological refinement (AMR) and a new loss function called Choroidal Convex Boundary (CCB) regularization. The Adaptive Morphological Refinement (AMR) block is designed to avoid the segmentation of discrete subtle objects in choroid. The new Choroidal Convex Boundary (CCB) loss is proposed to refine the segmented choroidal boundaries. The proposed method is applied to two OCT datasets acquired from two different manufacturers respectively in order to evaluate its effectiveness. The results show that the AMR block and CCB loss function enable the deep network to obtain more accurate choroid segmentations. In addition, for the first time in the field of medical image analysis, we construct a dedicated OCT choroid layer segmentation dataset (OCHID), which consists of 640 OCT images with choroidal boundaries annotations. This dataset is available for public use to assist community researchers in their research on related topics.},
  archive      = {J_APIN},
  author       = {Yan, Qifeng and Gu, Yuanyuan and Zhao, Jinyu and Wu, Wenjun and Ma, Yuhui and Liu, Jiang and Zhang, Jiong and Zhao, Yitian},
  doi          = {10.1007/s10489-022-03723-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5554-5566},
  shortjournal = {Appl. Intell.},
  title        = {Automatic choroid layer segmentation in OCT images via context efficient adaptive network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SRIF-RCNN: Sparsely represented inputs fusion of different
sensors for 3D object detection. <em>APIN</em>, <em>53</em>(5),
5532–5553. (<a
href="https://doi.org/10.1007/s10489-022-03594-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection is a vital task in many practical applications, such as autonomous driving, augmented reality and robot navigation. Significant advances have been made in recent LiDAR-only 3D detection methods, but sensor fusion 3D detection methods received less attention and have not made much progress. This paper aims to lift the 3D detection performance of sensor fusion methods. To this end, we present a novel sensor fusion strategy to effectively extract and fuse the features from different sensors. Firstly, the different sensor outputs are transformed in to sparsely represented inputs. Secondly, features are extracted from the inputs through an efficient backbone. Finally, the extracted features of different sensors are fused in a point-wise manner with the help of a gate mechanism. In addition, color supervision is also introduced to learn color distribution for the first time, which can provide discriminative features for proposal refinement. Based on the sensor fusion strategy and color distribution estimation, a multi-sensor 3D object detection network, named Sparsely Represented Inputs Fusion RCNN (SRIF-RCNN), is proposed. It achieves state-of-the-art performance on the highly competitive KITTI official 3D detection leaderboard, which ranks 1st and 2nd among sensor fusion methods and LiDAR-only methods with published works, respectively. Extensive experiments were implemented, and the effectiveness of the proposed network was validated},
  archive      = {J_APIN},
  author       = {Li, Xiaowei and Kong, Deming},
  doi          = {10.1007/s10489-022-03594-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5532-5553},
  shortjournal = {Appl. Intell.},
  title        = {SRIF-RCNN: Sparsely represented inputs fusion of different sensors for 3D object detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enhancement model based on dense atrous and inception
convolution for image semantic segmentation. <em>APIN</em>,
<em>53</em>(5), 5519–5531. (<a
href="https://doi.org/10.1007/s10489-022-03448-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of semantic segmentation is to classify each pixel in the image, so as to segment out the specific contour of the target. Most previous semantic segmentation models cannot generate enough semantic information for each pixel to understand the content of complex scenes. In this paper, we propose a novel semantic segmentation model Ince-DResAsppNet based on dense convoluted separation convolution. Unlike the previous model, our model revolves around reducing semantic information loss and enhancing detailed information. In the feature extraction part of the model, the idea of Dense and Ince is introduced to expand the number of channels on the basis of feature reuse. In the feature fusion part, Dense and Atrous’s idea of ​​dense dilated based on coprime factors is introduced, combined with multi-scale feature information to expand the receptive field and collect more dense pixels. Experiments conducted on the dataset PASCAL VOC 2012 and the CityScapes dataset show that our method performs better than the existing semantic segmentation model. Our model achieves 83.3% and 78.1% segmentation accuracy on the mIoU indicator, which surpasses many classical semantic segmentation models.},
  archive      = {J_APIN},
  author       = {Zhou, Erjing and Xu, Xiang and Xu, Baomin and Wu, Hongwei},
  doi          = {10.1007/s10489-022-03448-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5519-5531},
  shortjournal = {Appl. Intell.},
  title        = {An enhancement model based on dense atrous and inception convolution for image semantic segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure relay selection scheme for traffic congested zone in
VANET using grasshopper optimization and modified authentication key
agreement algorithms. <em>APIN</em>, <em>53</em>(5), 5497–5518. (<a
href="https://doi.org/10.1007/s10489-022-03572-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular ad hoc network (VANET) has increased its popularity in the modern era of communication and promises to provide ubiquitous connectivity, ultra-reliable, and low-latency transmissions. In day-to-day scenario, accidents or vehicular congestion may lead to a major problem and form a Traffic Congested Zone (TCZ). To overcome this situation relay-assisted vehicular communication is used to transmit the emergency and alert information in TCZ. Selecting the relay vehicle in the dynamic vehicular environment with high node density results in difficulties as network stability and also securing the vehicle’s communication remains one of the prevailing concerns in VANET applications. The proposed scheme jointly focuses on both optimal relay vehicle (RVOBU) selection and ensuring the security of RVOBU in TCZ. Initially, the location of the traffic-congested zone (TCZ), emergency/alert information, and security parameters of each vehicle are identified and stored in temporary supporting database (TS-DB) entries that are attached to each RSU. The optimal RVOBU is then selected by using the cluster-based grasshopper optimization algorithm (GOA) and securing the RVOBU using a modified AKA algorithm. Finally, an optimal and secured RVOBU is selected by verification with original and TS-DB entries. The results are compared with other optimization techniques and security is analyzed with existing work. The proposed strategy significantly forms a minimum number of a cluster with optimal RVOBU and deliberates the objective value with secured RVOBU in each iteration. As the result, the proposed scheme outperforms the existing scheme in terms of cluster lifetime, load balancing factor, verification delay, packet delivery ratio (PDR), communication overhead, and computation time.},
  archive      = {J_APIN},
  author       = {Monisha, A. Anu and Reshmi, T. R. and Murugan, K.},
  doi          = {10.1007/s10489-022-03572-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5497-5518},
  shortjournal = {Appl. Intell.},
  title        = {Secure relay selection scheme for traffic congested zone in VANET using grasshopper optimization and modified authentication key agreement algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Forecasting oil consumption with attention-based IndRNN
optimized by adaptive differential evolution. <em>APIN</em>,
<em>53</em>(5), 5473–5496. (<a
href="https://doi.org/10.1007/s10489-022-03720-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of oil consumption plays a dominant role in oil supply chain management. However, because of the effects of the coronavirus disease 2019 (COVID-19) pandemic, oil consumption has exhibited an uncertain and volatile trend, which leads to a huge challenge to accurate predictions. The rapid development of the Internet provides countless online information (e.g., online news) that can benefit predict oil consumption. This study adopts a novel news-based oil consumption prediction methodology–convolutional neural network (CNN) to fetch online news information automatically, thereby illustrating the contribution of text features for oil consumption prediction. This study also proposes a new approach called attention-based JADE-IndRNN that combines adaptive differential evolution (adaptive differential evolution with optional external archive, JADE) with an attention-based independent recurrent neural network (IndRNN) to forecast monthly oil consumption. Experimental results further indicate that the proposed news-based oil consumption prediction methodology improves on the traditional techniques without online oil news significantly, as the news might contain some explanations of the relevant confinement or reopen policies during the COVID-19 period.},
  archive      = {J_APIN},
  author       = {Wu, Binrong and Wang, Lin and Lv, Sheng-Xiang and Zeng, Yu-Rong},
  doi          = {10.1007/s10489-022-03720-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5473-5496},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting oil consumption with attention-based IndRNN optimized by adaptive differential evolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CVAM: Continuous-valued associative memory for one-to-many
associations. <em>APIN</em>, <em>53</em>(5), 5462–5472. (<a
href="https://doi.org/10.1007/s10489-022-03814-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a CVAM (continuous-valued associative memory for one-to-many associations) with back-propagation learning and analyze the performance in detail. Conventional associative memories often deal with binary patterns, however, most of the data handled today are continuous-valued data. The basic architecture of the proposed CVAM is a three-layer perceptron with multiple sub-layers in the hidden layer. The multiple sub-layers enable one-to-many associations using back-propagation (BP) learning algorithm; each sub-layer memorizes single one-to-one association and the multiple sub-layers enables one-to-many associations. We carried out experiments to analyze the important properties such as memory capacity and noise tolerance performance using continuous-valued data. In addition, we conducted a demonstrative experiment to visually confirm the behavior of the proposed CVAM as an associative memory model using the CIFAR-10 image data set.},
  archive      = {J_APIN},
  author       = {Kano, Shunsuke and Hagiwara, Masafumi},
  doi          = {10.1007/s10489-022-03814-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5462-5472},
  shortjournal = {Appl. Intell.},
  title        = {CVAM: Continuous-valued associative memory for one-to-many associations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study on the joint impact of feature selection
and data resampling on imbalance classification. <em>APIN</em>,
<em>53</em>(5), 5449–5461. (<a
href="https://doi.org/10.1007/s10489-022-03772-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets exhibit imbalanced distributions, in which the majority classes have sufficient samples, whereas the minority classes often have a very small number of samples. Data resampling has proven to be effective in alleviating such imbalanced settings, while feature selection is a commonly used technique for improving classification performance. However, the joint impact of feature selection and data resampling on two-class imbalance classification has rarely been addressed before. This work investigates the performance of two opposite imbalanced classification frameworks in which feature selection is applied before or after data resampling. We conduct a large-scale empirical study with a total of 9225 experiments on 52 publicly available datasets. The results show that both frameworks should be considered for finding the best performing imbalanced classification model. We also study the impact of classifiers, the ratio between the number of majority and minority samples (IR), and the ratio between the number of samples and features (SFR) on the performance of imbalance classification. Overall, this work provides a new reference value for researchers and practitioners in imbalance learning.},
  archive      = {J_APIN},
  author       = {Zhang, Chongsheng and Soda, Paolo and Bi, Jingjun and Fan, Gaojuan and Almpanidis, George and García, Salvador and Ding, Weiping},
  doi          = {10.1007/s10489-022-03772-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5449-5461},
  shortjournal = {Appl. Intell.},
  title        = {An empirical study on the joint impact of feature selection and data resampling on imbalance classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning dynamic causal mechanisms from non-stationary data.
<em>APIN</em>, <em>53</em>(5), 5437–5448. (<a
href="https://doi.org/10.1007/s10489-022-03843-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery from non-stationary time series is an important but challenging task. Most existing non-stationary approaches only consider the changes of causal coefficients, which are merely satisfied in real-world scenarios. In this paper, we introduce a Gaussian-based Variational Temporal Abstraction model (GVTA) to detect and learn non-stationary causal mechanisms from multiple time series. First, we utilize a hierarchical cyclic state-space model to detect the stationary states from the non-stationary time series. Second, we use the Gaussian process algorithm to estimate the causal mechanism for each stationary state. Experimental results on both simulation and real-world data demonstrate the correctness and effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Cai, Ruichu and Huang, Liting and Chen, Wei and Qiao, Jie and Hao, Zhifeng},
  doi          = {10.1007/s10489-022-03843-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5437-5448},
  shortjournal = {Appl. Intell.},
  title        = {Learning dynamic causal mechanisms from non-stationary data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty measurement for a gene space based on
class-consistent technology: An application in gene selection.
<em>APIN</em>, <em>53</em>(5), 5416–5436. (<a
href="https://doi.org/10.1007/s10489-022-03657-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of data mining, artificial intelligence, neural network, expert system and machine learning, information system (i-system) becomes more and more important. If the objects, attributes and information values in an i-system are replaced by cells, genes and gene expression values, respectively, then the i-system is said to be a gene space. Because gene expression data is characterized by small samples, high dimension and noise, there is considerable uncertainty in a gene space. Traditional machine learning and statistical methods are often powerless to a gene space. Granular computing (GrC) can effectively deal with various uncertainties. This paper studies the uncertainty measurement of gene space based on the class-consistent technology and discusses its application in gene selection from the perspective of GrC. A class-consistent relation between cells in a gene space is first established by the gene expression values of cells on the basis of class-consistent technology. Then, the information granules (i-granules) are obtained from a gene space by using the class-consistent relation. Next, two metrics (information granularity and information entropy) to measure the uncertainty of gene space are defined and their properties are also investigated. The results of numerical experiments and statistical tests verify their effectiveness. Furthermore, as their application to gene space, two gene selection algorithms are proposed. Finally, the clustering experiments and statistical tests on 16 gene spaces show that the designed gene selection algorithms outperform some state-of-the-art feature selection algorithms in terms of three clustering performance indicators.},
  archive      = {J_APIN},
  author       = {Li, Zhaowen and Zhang, Qinli and Wang, Pei and Song, Yan and Wen, Ching-Feng},
  doi          = {10.1007/s10489-022-03657-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5416-5436},
  shortjournal = {Appl. Intell.},
  title        = {Uncertainty measurement for a gene space based on class-consistent technology: An application in gene selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cascaded spatiotemporal attention network for dynamic
facial expression recognition. <em>APIN</em>, <em>53</em>(5), 5402–5415.
(<a href="https://doi.org/10.1007/s10489-022-03781-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic facial expression recognition (DFER) is a promising research area because it concerns the dynamic change pattern of facial expressions, but it is difficult to effectively capture the facial appearances and dynamic temporal information of each image in an image sequence. In this paper, a cascaded spatiotemporal attention network (CSTAN) is proposed to learn and integrate spatial and temporal emotional information in the process of facial expression change. Three types of attention modules are embedded into the cascaded network to enable it to extract more informative spatiotemporal features for the DFER task in different dimensions. A channel attention module helps the network focus on the meaningful spatial feature maps for the DFER task, a spatial attention module focuses on the regions of interest among the spatial feature maps, and a temporal attention module aims to explore the dynamic temporal information when an expression changes. The experimental results on three public facial expression recognition datasets prove the good performance of the CSTAN, and it can extract representative spatiotemporal features. Meanwhile, the visualization results reveal that the CSTAN can locate regions of interest and contributing timesteps, which illustrates the effectiveness of the multidimensional attention modules.},
  archive      = {J_APIN},
  author       = {Ye, Yaoguang and Pan, Yongqi and Liang, Yan and Pan, Jiahui},
  doi          = {10.1007/s10489-022-03781-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5402-5415},
  shortjournal = {Appl. Intell.},
  title        = {A cascaded spatiotemporal attention network for dynamic facial expression recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NOSMFuse: An infrared and visible image fusion approach
based on norm optimization and slime mold architecture. <em>APIN</em>,
<em>53</em>(5), 5388–5401. (<a
href="https://doi.org/10.1007/s10489-022-03591-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing infrared and visible image fusion algorithms, it is usually difficult to maintain a good balance of meaningful information between two source images, which easily leads to the omission of important fractional information in a particular source image. To address this issue, a novel fusion algorithm based on norm optimization and slime mold architecture, called NOSMFuse, is proposed. First, an interactive information decomposition method based on mutually guided image filtering is devised and utilized to obtain the corresponding base and detail layers. Subsequently, the differentiation feature extraction operator is formulated and employed to fuse the base layers. In addition, we design a norm optimization-based fusion strategy for the detail layers and a loss function that considers both the intensity fidelity and the gradient constraint. Finally, to further balance the useful information of the base and detail layers contained in the fusion image, we propose a slime mold architecture based image reconstruction method that generates fusion results through adaptive optimization. The experimental results show that the proposed NOSMFuse is superior to 12 other state-of-art fusion algorithms, both qualitatively and quantitatively.},
  archive      = {J_APIN},
  author       = {Hao, Shuai and He, Tian and Ma, Xu and An, Beiyi and Wen, Hu and Wang, Feng},
  doi          = {10.1007/s10489-022-03591-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5388-5401},
  shortjournal = {Appl. Intell.},
  title        = {NOSMFuse: An infrared and visible image fusion approach based on norm optimization and slime mold architecture},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A DRL based cooperative approach for parking space
allocation in an automated valet parking system. <em>APIN</em>,
<em>53</em>(5), 5368–5387. (<a
href="https://doi.org/10.1007/s10489-022-03757-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated valet parking (AVP) is one of the most advanced technologies for improving parking efficiency and security. However, in an AVP system, the traditional vehicle-side greedy search strategy for available parking spaces is likely to achieve low global efficiency and poses a high risk of collision. Therefore, in this study, a system-side deep reinforcement learning (DRL)-based cooperative approach is proposed to solve the parking space allocation problem in a large AVP environment. First, the problem of parking space allocation is formulated as a Markov decision process (MDP). Then, a reward shaping method oriented to the global objective is designed. Next, because the current reinforcement learning methods are difficult to apply to parking space allocation involving large numbers of discrete actions, a cost-based method of parking allocation action embedding is proposed to embed the discrete parking actions in a continuous space, which the actor can generalize. After action embedding, the deep deterministic policy gradient (DDPG) is employed as the training algorithm. The experimental results show that the proposed DRL -based cooperative approach can converge in the parking space allocation problem involving a large AVP system and achieve greater improvement of global AVP efficiency than can the other parking methods.},
  archive      = {J_APIN},
  author       = {Xie, Jun and He, Zhaocheng and Zhu, Yiting},
  doi          = {10.1007/s10489-022-03757-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5368-5387},
  shortjournal = {Appl. Intell.},
  title        = {A DRL based cooperative approach for parking space allocation in an automated valet parking system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active constrained deep embedded clustering with dual
source. <em>APIN</em>, <em>53</em>(5), 5337–5367. (<a
href="https://doi.org/10.1007/s10489-022-03752-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering using a deep neural network (DNN) is widely used for simultaneously learning feature representation and clustering. The existing constrained deep clustering methods utilize prior knowledge for improving deep clustering. However, most of these methods randomly select prior knowledge (pairwise constraints) and fail to use it appropriately in the deep clustering process. The present study aims to address this limitation by proposing a new scheme for integrating and improving constrained deep clustering by active learning from dual source. The scheme is DNN for initializing the nonlinear transformation of the original feature space, clustering layer, as well as the constrained clustering layer which is parallel to the clustering layer and uses prior knowledge as a set of neighborhoods. In addition, active learning uses the above-mentioned two layers as a source simultaneously as the proposed scheme for selecting informative and diverse data. The suggested method can simultaneously lead to constrained clustering, learn the latent feature space with the guidance of the constraints set, and indirectly cause the data belonging to one neighborhood to be closer to its center (i.e. away from other neighborhoods centers). Different experiments on different datasets indicate the efficiency and robustness of the proposed method.},
  archive      = {J_APIN},
  author       = {Hazratgholizadeh, R. and Balafar, M. A. and Derakhshi, M. R. F.},
  doi          = {10.1007/s10489-022-03752-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5337-5367},
  shortjournal = {Appl. Intell.},
  title        = {Active constrained deep embedded clustering with dual source},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principal views selection based on growing graph convolution
network for multi-view 3D model recognition. <em>APIN</em>,
<em>53</em>(5), 5320–5336. (<a
href="https://doi.org/10.1007/s10489-022-03775-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D technologies, 3D model recognition has attracted substantial attention in various areas, such as automatic driving, virtual/augmented reality, and computer-aided design. Many researchers are devoted to 3D model recognition and obtain some achievements in research. However, the abundant structure information of the 3D model also brings a huge challenge in model representation. In recent years, many researchers focus on classical computer vision technologies, which are utilized to represent the multi-view information of the 3D model. However, redundant visual information also brings a new challenge in model representation. In this paper, we focus on the multi-view 3D model data and propose a novel growing graph convolution network (GGCN) to handle the principal views selection problem, which can guarantee the performance of 3D model representation and effectively reduce the cost time. The proposed method mainly includes two modules: 1) principal views selection module: we utilize the selected views to describe the 3D model, which can effectively remove the redundant information and reduce computational complexity. 2) growing GCN module: we propose an effective growing GCN model, which focuses on gathering nodes that were less related to each other to ensure the result of multi-view fusion. It can indirectly retain the structure information and also reduce redundant information. In the process of graph growing, the GGCN model gradually adds view information to make up for the lack of characterization and guarantee the final performance. More specially, these two modules can guide each other to improve the performance of the principal views module and indirectly increase the final recognition accuracy. To evaluate the effectiveness of our proposed method, we test the classification accuracy and retrieval performance on the ModelNet40 dataset and ShapeNet dataset. The experimental results demonstrate the superiority of our proposed method.},
  archive      = {J_APIN},
  author       = {Liang, Qi and Li, Qiang and Nie, Weizhi and Su, Yuting},
  doi          = {10.1007/s10489-022-03775-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5320-5336},
  shortjournal = {Appl. Intell.},
  title        = {Principal views selection based on growing graph convolution network for multi-view 3D model recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physical-layer secret key generation based on
domain-adversarial training of autoencoder for spatial correlated
channels. <em>APIN</em>, <em>53</em>(5), 5304–5319. (<a
href="https://doi.org/10.1007/s10489-022-03777-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel approach to enhance information security, physical-layer key generation is based on the channel reciprocity and spatial decorrelation of the wireless channels between two legitimate sides. Due to the half-duplex mode of communication systems, the channel responses detected by the two sides are not exactly reciprocal. Also, the eavesdropper may be extremely close to the legitimate side, and information leakage could arise due to the spatial correlation in this case. To solve this problem, this paper proposes an efficient physical-layer key generation scheme based on the combination of an autoencoder and Domain-Adversarial Training of Neural Networks (DANN), i.e., Domain-Adversarial Training of Autoencoder (DAAE). DAAE extracts the reciprocal channel features of the legitimate sides while maximizing the feature difference on the eavesdropper. Simulation experiments are conducted to verify the effectiveness of DAAE, and the experimental results show that based on DAAE, the correlation of extracted features between the eavesdropper and its adjacent node is weakened without significant influence on the legitimate sides. Meanwhile, after quantization, the primary key shows a good consistency rate and randomness. The proposed method has a good potential for industrial applications.},
  archive      = {J_APIN},
  author       = {Zhou, Jingqi and Zeng, Xin},
  doi          = {10.1007/s10489-022-03777-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5304-5319},
  shortjournal = {Appl. Intell.},
  title        = {Physical-layer secret key generation based on domain-adversarial training of autoencoder for spatial correlated channels},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Inference of isA commonsense knowledge with lexical
taxonomy. <em>APIN</em>, <em>53</em>(5), 5290–5303. (<a
href="https://doi.org/10.1007/s10489-022-03680-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense knowledge is a crucial resource to help the machine understand the human world. However, the conventional methods of extracting commonsense knowledge with isA relation (or isA commonsense knowledge) from text corpora generally do not work well since commonsense knowledge is typically omitted in communication. In this paper, we mainly focus on the inference of isA commonsense knowledge (the definition of isA here to express a hypernym-hyponym relationship and we concentrate on whether the description of (s, isA, o) is correct based on this relationship, e.g., (mammal, isA, animal), (Hello Kitty, isA, cat)) with a special kind of knowledge graph: lexical taxonomy. Lexical and semantic features of terms are both extracted from three relationships including exclusive, compatible, andinclusive relationships then a simple but effective classification model is further utilized to predict whether isA commonsense holds or not. Besides, we implement our model on a lexical taxonomy: Probase. A series of comparative experiments prove the effectiveness of our approach with an accuracy of over 96%, and we infer 200k isA commonsense knowledge from 1 million new pairs.},
  archive      = {J_APIN},
  author       = {Wang, Chao and Liu, Jingping and Liu, Juntao and Wang, Wei},
  doi          = {10.1007/s10489-022-03680-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5290-5303},
  shortjournal = {Appl. Intell.},
  title        = {Inference of isA commonsense knowledge with lexical taxonomy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Weighted mean field reinforcement learning for large-scale
UAV swarm confrontation. <em>APIN</em>, <em>53</em>(5), 5274–5289. (<a
href="https://doi.org/10.1007/s10489-022-03840-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the optimal game strategy is a difficult problem in unmanned aerial vehicle (UAV) swarm confrontation. As an effective solution to the sequential decision-making problem, multi-agent reinforcement learning (MARL) provides a promising way to realize intelligent countermeasures. However, there are two challenges in applying MARL to large-scale UAV swarm confrontation: i) the curse of dimensionality caused by the excessive scale of UAV clusters and ii) the generalization problem caused by the dynamically changing UAV cluster size. To address these problems, we propose a novel MARL paradigm, called Weighted Mean Field Reinforcement Learning, where the pairwise communication between any UAV and its neighbors is modeled as that between a central UAV and the virtual UAV, which is abstracted from the weighted mean effect of neighboring UAVs. This approach reduces the multi-agent problem to a two-agent problem, which can reduce the input dimension of the agent and adapt to the changing cluster size. The communication content between UAVs includes actions and local observations. Actions can enhance the cooperation between UAVs and alleviate the non-stationarity of the environment, while local observations can expand the perception range of the central UAV so that it can obtain more useful information about the environment. The attention mechanism is leveraged to enable UAVs to select more valuable information flexibly, making our method more scalable than other algorithms. Combining this paradigm with double Q-learning and actor-critic algorithms, we propose weighted mean field Q-learning (WMFQ) and weighted mean field actor-critic (WMFAC) algorithms. Experiments on our constructed UAV swarm confrontation environment verify the effectiveness and scalability of our algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Baolai and Li, Shengang and Gao, Xianzhong and Xie, Tao},
  doi          = {10.1007/s10489-022-03840-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5274-5289},
  shortjournal = {Appl. Intell.},
  title        = {Weighted mean field reinforcement learning for large-scale UAV swarm confrontation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-scale gated network for retinal hemorrhage
detection. <em>APIN</em>, <em>53</em>(5), 5259–5273. (<a
href="https://doi.org/10.1007/s10489-022-03476-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal hemorrhage detection is of great significance for clinical diagnosis and disease control. However, most of the traditional methods need to obtain candidate lesions firstly, and then determine the true lesions. To address this problem, we propose an end-to-end multi-scale gated network (MGNet) to directly detect hemorrhage. Taken the U-Net as the backbone, we first add a skip connection gated module (SGate) to the skip connection to suppress useless information. Secondly, we propose a high-resolution and low-resolution multi-scale fusion module (HLMS) to improve the representation capacity of network through fusing information from the adjacent decoder layers. Furtherly, we propose a weighted Dice loss (W-Dice loss) to focus on the hard samples. Extensive experiments on two publicly available datasets: DIARETDB1 and IDRiD, demonstrate that the proposed MGNet achieves competitive results of hemorrhage detection compared with the state-of-the-art works.},
  archive      = {J_APIN},
  author       = {Xia, Haiying and Rao, Zengyan and Zhou, Zuoshan},
  doi          = {10.1007/s10489-022-03476-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5259-5273},
  shortjournal = {Appl. Intell.},
  title        = {A multi-scale gated network for retinal hemorrhage detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate and fast time series classification based on
compressed random shapelet forest. <em>APIN</em>, <em>53</em>(5),
5240–5258. (<a
href="https://doi.org/10.1007/s10489-022-03852-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving accurate, fast, and interpretable time series classification (TSC) has attracted considerable attention from the data mining community over the past decades. In this paper, we propose an efficient algorithm, called Compressed Random Shapelet Forest (CRSF), to tackle this problem. Different from most of the shapelet-based TSC methods, CRSF obtains promising performance by greatly compressing the shapelet features space. In order to achieve the aim of compression, the time series dataset, as well as the shapelets, are represented by Symbolic Aggregate approXimation (SAX) at first. Then, the shapelet-based decision trees are built upon a pool of high-quality shapelet candidates of which the useless shapelets and the self-similar shapelets have been pre-pruned. A new function for measuring the distance between two SAX-represented time series is also introduced. Extensive experiments were conducted on 50 UCR time series datasets. The results show that (1) CRSF can achieve the highest average accuracy on the datasets and it outperforms most of the existing shapelet-based TSC methods; (2) CRSF is slightly superior to gRSF in terms of accuracy and is significantly superior to gRSF in terms of time cost. Specifically, it is on average 41 times faster than gRSF according to the experimental results.},
  archive      = {J_APIN},
  author       = {Yang, Jun and Jing, Siyuan and Huang, Guanying},
  doi          = {10.1007/s10489-022-03852-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5240-5258},
  shortjournal = {Appl. Intell.},
  title        = {Accurate and fast time series classification based on compressed random shapelet forest},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LDN-RC: A lightweight denoising network with residual
connection to improve adversarial robustness. <em>APIN</em>,
<em>53</em>(5), 5224–5239. (<a
href="https://doi.org/10.1007/s10489-022-03847-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are prone to produce incorrect prediction results under the attack of adversarial samples. To cope with this problem, some defense methods are presented. However, most of them are based on adversarial training, which has great computational consumption and does not start from strengthening the architecture of the network model itself to resist the adversarial attack. Recent studies have shown that feature denoising can remove the adversarial perturbations in the adversarial samples. In this paper, we propose a lightweight denoising network with residual connection (LDN-RC), on which the internal denoising block and the intermediate denoising block are introduced for feature denoising and sample denoising, respectively; the two denoising blocks are combined in the network model, which can withstand the interference of the adversarial perturbations in the adversarial samples to a large extent and also save computational resources. In the training strategy, a two-stage denoising approach and fine-tuning are presented to train the RESNET network model on MNIST, CIFAR-10, and SVHN datasets, and the accuracy of the enhanced network model exceeds 60% on all three datasets under the $${L}_{\infty }$$ -PGD white-box attack, which demonstrate that LDN-RC can effectively improve the adversarial robustness of the network model.},
  archive      = {J_APIN},
  author       = {Chai, Xiuli and Wei, Tongtong and Chen, Zhen and He, Xin and Gan, Zhihua and Wu, Xiangjun},
  doi          = {10.1007/s10489-022-03847-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5224-5239},
  shortjournal = {Appl. Intell.},
  title        = {LDN-RC: A lightweight denoising network with residual connection to improve adversarial robustness},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A brain storm optimization algorithm with feature
information knowledge and learning mechanism. <em>APIN</em>,
<em>53</em>(5), 5199–5223. (<a
href="https://doi.org/10.1007/s10489-022-03762-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various optimization problems with multiple decision variables and complex constraints, which exist widely in the real world, are difficult to be solved by traditional methods. Brain storm optimization (BSO) algorithm, an advanced swarm intelligence optimization method, has high efficiency and flexibility in solving large-scale problems independent of problem characteristics. The essence of swarm intelligence optimization algorithm is that a population iteratively searches for the optimal solution in the solution space, and the process has randomness and blindness. To enhance the searching ability of BSO and strengthen the theoretical guidance of the algorithm, a brain storm optimization algorithm with feature information knowledge and learning mechanism (FIBSO) is proposed in this paper. In the process of BSO iteration, information interaction exists between individuals of each generation. The new individual is generated from the old individual, and the dominant individual contributes to the new individual. Theoretically, using the knowledge of characteristic information between individuals guides the evolution of the population in the dominant direction. Moreover, three search strategies guided by global and local optimal individuals are presented to balance the global and local search capabilities of the algorithm. The results of FIBSO and several comparison algorithms on the CEC2017 test suite indicate that FIBSO has superior performance to the state-of-the arts algorithms. The FIBSO is introduced to the no-wait flow shop scheduling problem, and the results show that FIBSO has the significant ability to solve practical engineering problems.},
  archive      = {J_APIN},
  author       = {Zhao, Fuqing and Hu, Xiaotong and Wang, Ling and Xu, Tianpeng and Zhu, Ningning and Jonrinaldi},
  doi          = {10.1007/s10489-022-03762-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5199-5223},
  shortjournal = {Appl. Intell.},
  title        = {A brain storm optimization algorithm with feature information knowledge and learning mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncover the reasons for performance differences between
measurement functions (provably). <em>APIN</em>, <em>53</em>(5),
5179–5198. (<a
href="https://doi.org/10.1007/s10489-022-03726-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, an exciting experimental conclusion in Li et al. (Knowl Inf Syst 62(2):611–637, 1) about measures of uncertainty for knowledge bases has attracted great research interest for many scholars. However, these efforts lack solid theoretical interpretations for the experimental conclusion. The main limitation of their research is that the final experimental conclusions are only derived from experiments on three datasets, which makes it still unknown whether the conclusion is universal. In our work, we first review the mathematical theories, definitions, and tools for measuring the uncertainty of knowledge bases. Then, we provide a series of rigorous theoretical proofs to reveal the reasons for the superiority of using the knowledge amount of knowledge structure to measure the uncertainty of the knowledge bases. Combining with experiment results, we verify that knowledge amount has much better performance for measuring uncertainty of knowledge bases. Hence, we prove an empirical conclusion established through experiments from a mathematical point of view. In addition, we find that for some knowledge bases that cannot be classified by entity attributes, such as ProBase (a probabilistic taxonomy), our conclusion is still applicable. Therefore, our conclusions have a certain degree of universality and interpretability and provide a theoretical basis for measuring the uncertainty of many different types of knowledge bases, and the findings of this study have a number of important implications for future practice.},
  archive      = {J_APIN},
  author       = {Wang, Chao and Feng, Jianchuan and Liu, Linfang and Jiang, Sihang and Wang, Wei},
  doi          = {10.1007/s10489-022-03726-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5179-5198},
  shortjournal = {Appl. Intell.},
  title        = {Uncover the reasons for performance differences between measurement functions (Provably)},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An estimation of distribution algorithm with multiple
intensification strategies for two-stage hybrid flow-shop scheduling
problem with sequence-dependent setup time. <em>APIN</em>,
<em>53</em>(5), 5160–5178. (<a
href="https://doi.org/10.1007/s10489-022-03853-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of distribution algorithm (EDA) has recently emerged as a promising alternative to the traditional evolutionary algorithms for solving combinatorial optimization problems. In this paper, an estimation of distribution algorithm with multiple intensification strategies (EDA-MIS) is proposed to solve a typical kind of hybrid flow-shop scheduling problem. The two-stage heterogeneous hybrid flow-shop scheduling problem is investigated. The sequence-dependent setup time at the first stage is also considered. In the proposed EDA-MIS, the initial population is constructed through the heuristic method and random strategy. An order matrix is established to estimate the probabilistic model of promising solutions. Then the solutions of the algorithm are evolved through the processes of selection, recombination, sampling, and local search. The obtained results indicate that the EDA-MIS provides good solutions in the aspects of solution quality and computational efficiency.},
  archive      = {J_APIN},
  author       = {Liu, Huan and Zhao, Fuqing and Wang, Ling and Cao, Jie and Tang, Jianxin and Jonrinaldi},
  doi          = {10.1007/s10489-022-03853-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5160-5178},
  shortjournal = {Appl. Intell.},
  title        = {An estimation of distribution algorithm with multiple intensification strategies for two-stage hybrid flow-shop scheduling problem with sequence-dependent setup time},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SC2Net: Scale-aware crowd counting network with pyramid
dilated convolution. <em>APIN</em>, <em>53</em>(5), 5146–5159. (<a
href="https://doi.org/10.1007/s10489-022-03648-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate crowd counting is still challenging due to the variations of crowd heads. Most of crowd counting methods adopt multi-branch networks to extract multi-scale information. However, these networks are too complex to be optimized. To solve these problems, we propose an efficient scale-aware crowd counting network named SC2Net, which adopts the encoder-decoder framework. The encoder uses the first ten layers of VGG16 to extract the primary feature information. The decoder is mainly consisted of our proposed residual pyramid dilated convolution (ResPyDConv) modules to regress predicted density maps. Specifically, the ResPyDConv module is composed of pyramid dilated convolution (PyDConv). Each PyDConv adopts dilated convolutions with different dilated rates. PyDConv divides feature maps into different groups and extracts multi-scale feature information. Extensive experiments are conducted on ShanghaiTech, UCF_CC_50, UCF_QNRF, and NWPU_Crowd datasets. Qualitative and quantitive results show the superiority of our proposed network to the other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liang, Lanjun and Zhao, Huailin and Zhou, Fangbo and Zhang, Qing and Song, Zhili and Shi, Qingxuan},
  doi          = {10.1007/s10489-022-03648-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5146-5159},
  shortjournal = {Appl. Intell.},
  title        = {SC2Net: Scale-aware crowd counting network with pyramid dilated convolution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network rule extraction under the network formal context
based on three-way decision. <em>APIN</em>, <em>53</em>(5), 5126–5145.
(<a href="https://doi.org/10.1007/s10489-022-03672-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge discovery combined with network structure is an emerging field of network data analysis and mining. Three-way concept analysis is a method that can fit the human mind in uncertain decisions and analysis. In reality, when three-way concept analysis is placed in the background of a network, not only the three-way rules need to be obtained, but also the network characteristic values of these rules should be obtained, which is of great significance for concept cognition in the network. This paper mainly combines complex network analysis with the formal context of three-way decision. Firstly, the network formal context of three-way decision (NFC3WD) is proposed to unify the two studies mentioned above into one data framework. Then, the network weaken-concepts of three-way decision (NWC3WD) and their corresponding sub-networks are studied. Therefore, we can not only find out the network weaken-concepts but also know the average influence of the sub-network, as well as the influence difference within the sub-network. Furthermore, the concept logic of network and the properties of its operators are put forward, which lays a foundation for designing the algorithm of rule extraction. Subsequently, the bidirectional rule extraction algorithm and reduction algorithm based on confidence degree are also explored. Meanwhile, these algorithms are applied to the diagnosis examples of COVID-19 from which we can not only get diagnostic rules, but also know the importance of the population corresponding to these diagnostic rules in the network through network eigenvalues. Finally, experimental analysis is made to show the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Fan, Min and Luo, Shan and Li, Jinhai},
  doi          = {10.1007/s10489-022-03672-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5126-5145},
  shortjournal = {Appl. Intell.},
  title        = {Network rule extraction under the network formal context based on three-way decision},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel dynamic interpolation method based on both temporal
and spatial correlations. <em>APIN</em>, <em>53</em>(5), 5100–5125. (<a
href="https://doi.org/10.1007/s10489-022-03815-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are very common in environmental monitoring activities. Traditional interpolation methods often view the problem from either spatial or temporal perspective and do not make good use of valuable related information, thereby leading to low prediction accuracy. Although many hybrid spatiotemporal interpolation methods combine temporal interpolation methods with spatial interpolation methods and show superiority over other single temporal or spatial interpolation methods, they do not finely treat each missing value in terms of its specific features. In this paper, two dynamic spatiotemporal interpolation (DST) methods, coarse-grained DST (CGDST) and fine-grained DST (FGDST) are proposed by using both temporal and spatial interpolation results. Different from other hybrid spatiotemporal interpolation methods, they create differences in the contribution of temporal and spatial interpolation results and assign them with different weights. Both CGDST and FGDST treat each missing value differently and fill it by considering the reliability of both temporal and spatial interpolation results in terms of the lengths of its column gap and row gap. CGDST treats each missing value in a continuous missing area equally and all missing values have the same lengths of column and row gaps. FGDST goes beyond CGDST and treats each missing value differently based on its temporal distance to the nearest real observed values in both forward and backward directions. To demonstrate their superiority, the experimental datasets were generated randomly to simulate missing values in real life. The results of the extensive experiments showed that FGDST improved by 2.44% $\sim $ 8.21% in terms of symmetric mean absolute percentage error (SMAPE) compared with the best baseline method. Compared with CGDST, the improvements of FGDST range from 1.05% to 2.87% in terms of SMAPE. Moreover, their superiority is more obvious if there are more continuous missing values in the temporal dimension.},
  archive      = {J_APIN},
  author       = {Gao, Shiping and He, Dongjie and Zhang, Zhouzhuo and Tang, Xiaoqian and Zhao, Zhili},
  doi          = {10.1007/s10489-022-03815-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5100-5125},
  shortjournal = {Appl. Intell.},
  title        = {A novel dynamic interpolation method based on both temporal and spatial correlations},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep convolutional transfer learning-based structural damage
detection with domain adaptation. <em>APIN</em>, <em>53</em>(5),
5085–5099. (<a
href="https://doi.org/10.1007/s10489-022-03713-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most data-driven structural damage detection methods are built upon the assumption that enough labeled data is available and both training and test data have the same underlying distribution, which limit their successful applications in practical engineering. To solve the problem, a novel structural damage detection method is proposed by using deep convolutional transfer learning. In the method, one-dimensional deep convolutional neural network and two-dimensional deep convolutional neural network are combined to mine more fine-grained features with spatiotemporal characteristic from raw vibration data. And, a novel domain adaptation technology, combining multikernel maximum mean discrepancy and local maximum mean discrepancy, is developed to align the distribution of global domains and relevant subdomains among different domains, which could mine more fine-grained features for each category and improve the transfer performance. Transfer experiments on two different structures are implemented to verify the effectiveness of the proposed method. Furthermore, a new solution is found by taking advantage of the damage knowledge learnt from other structure to implement damage detection when very small damage samples are available. The results show that the proposed method achieved superior detection performance over the existing popular methods.},
  archive      = {J_APIN},
  author       = {Chen, Zuoyi and Wang, Chao and Wu, Jun and Deng, Chao and Wang, Yuanhang},
  doi          = {10.1007/s10489-022-03713-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5085-5099},
  shortjournal = {Appl. Intell.},
  title        = {Deep convolutional transfer learning-based structural damage detection with domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TBDRI: Block decomposition based on relational interaction
for temporal knowledge graph completion. <em>APIN</em>, <em>53</em>(5),
5072–5084. (<a
href="https://doi.org/10.1007/s10489-022-03601-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion (KGC) can be interpreted as the task of missing inferences to real-world facts. Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static knowledge graphs. The data they are applied to usually evolves with time, such as friend graphs in social networks. Therefore, developing temporal knowledge graph completion (temporal KGC) models is an increasingly important topic, although it is difficult due to data non-stationarity, and its complex temporal dependencies. In this paper, we propose block decomposition based on relational interaction for temporal knowledge graph completion (TBDRI), a novel model based on block term decomposition (which can be seen as a special variant of CP decomposition and Tucker decomposition) of the binary tensor representation of knowledge graph quadruples. TBDRI considers that inverse relations, as one of the most important types of relations, occupy an important share in the real world. Although some existing models introduce inverse relation into the model, it is not enough to only learn the inverse relation independently. TBDRI learns inverse relation in an enhanced way to strengthen the binding of forward and inverse relation. Furthermore, TBDRI first uses the core tensor as temporal information to bind timestamps more adequately. We prove TBDRI is full expressiveness and derive the bound on its entity, relation, and timestamp embedding dimensionality. We show that TBDRI is able to outperform most previous state-of-the-art models on the four benchmark datasets for temporal knowledge graph completion.},
  archive      = {J_APIN},
  author       = {Yu, Mei and Guo, Jiujiang and Yu, Jian and Xu, Tianyi and Zhao, Mankun and Liu, Hongwei and Li, Xuewei and Yu, Ruiguo},
  doi          = {10.1007/s10489-022-03601-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5072-5084},
  shortjournal = {Appl. Intell.},
  title        = {TBDRI: Block decomposition based on relational interaction for temporal knowledge graph completion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical attention network for multivariate time series
long-term forecasting. <em>APIN</em>, <em>53</em>(5), 5060–5071. (<a
href="https://doi.org/10.1007/s10489-022-03825-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series long-term forecasting has always been the subject of research in various fields such as economics, finance, and traffic. In recent years, attention-based recurrent neural networks (RNNs) have received attention due to their ability of reducing error accumulation. However, the existing attention-based RNNs fail to eliminate the negative influence of irrelevant factors on prediction, and ignore the conflict between exogenous factors and target factor. To tackle these problems, we propose a novel Hierarchical Attention Network (HANet) for multivariate time series long-term forecasting. At first, HANet designs a factor-aware attention network (FAN) and uses it as the first component of the encoder. FAN weakens the negative impact of irrelevant exogenous factors on predictions by assigning small weights to them. Then HANet proposes a multi-modal fusion network (MFN) as the second component of the encoder. MFN employs a specially designed multi-modal fusion gate to adaptively select how much information about the expression of current time come from target and exogenous factors. Experiments on two real-world datasets reveal that HANet not only outperforms state-of-the-art methods, but also provides interpretability for prediction.},
  archive      = {J_APIN},
  author       = {Bi, Hongjing and Lu, Lilei and Meng, Yizhen},
  doi          = {10.1007/s10489-022-03825-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5060-5071},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical attention network for multivariate time series long-term forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vote-based integration of review spam detection algorithms.
<em>APIN</em>, <em>53</em>(5), 5048–5059. (<a
href="https://doi.org/10.1007/s10489-022-03807-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the growth of online review data, detecting fake or fraudulent reviews is becoming an urgent issue. One barrier to effective detection of fake reviews/reviewers is the great difficulty of collecting ground-truth data—fake reviews are hard to judge, even by human experts. As researchers propose a large number of methods to detect review spam from a variety of perspectives, e.g., text-based or behavior-based, there is a need to combine these methods to improve the overall detection performance. In this paper, we raise the important question of how to integrate multiple ranking lists generated by different types of review spam detection algorithms into an overall ranking list. To address this problem, we propose a novel unsupervised integration model, namely SpamVote, that combines multiple ranking lists together by voting. In view of the diversity of review spam strategies, we model the fitness of a particular algorithm to detect a specific item as latent, and learn the latent variables from the ranking data. Extensive experiments on real-world datasets with various kinds of algorithms show that the integrated ranking list created by SpamVote outperforms the voting lists with a large probability.},
  archive      = {J_APIN},
  author       = {Wang, Zhuo and Li, Hui and Wang, Huiyan},
  doi          = {10.1007/s10489-022-03807-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5048-5059},
  shortjournal = {Appl. Intell.},
  title        = {Vote-based integration of review spam detection algorithms},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An incomplete probabilistic linguistic multi-attribute group
decision making method based on a three-dimensional trust network.
<em>APIN</em>, <em>53</em>(5), 5029–5047. (<a
href="https://doi.org/10.1007/s10489-022-03738-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is necessary to consider the trust relationship among experts in the process of group decision-making, however the trust network and preference information among experts may be incomplete. Therefore, this paper proposes an incomplete probabilistic linguistic multi-attribute group decision-making method based on a three-dimensional trust network. Firstly, the two-dimensional trust network is extended to the three-dimensional form, and the probabilistic linguistic term sets are used to express the trust relationship and degree among experts. On this basis, considering the situation of incomplete information, the trust transfer function is designed to complete the establishment of the trust network. Secondly, in order to complete the incomplete probabilistic linguistic decision preference information of experts, the relative trust of experts and the cosine similarity of preference relations are comprehensively considered. Then, the least average method is used to determine the evaluation information that needs to be adjusted, and different opinion adoption factors are set for a personalized recommendation. Finally, an evaluation case of the national wetland park pilot and comparative analysis are used to demonstrate the effectiveness and applicability of the proposed method.},
  archive      = {J_APIN},
  author       = {Zhao, Meng and Kou, Dan and Li, Ling and Lin, Mingwei},
  doi          = {10.1007/s10489-022-03738-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5029-5047},
  shortjournal = {Appl. Intell.},
  title        = {An incomplete probabilistic linguistic multi-attribute group decision making method based on a three-dimensional trust network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RBFPDet: An anchor-free helmet wearing detection method.
<em>APIN</em>, <em>53</em>(5), 5013–5028. (<a
href="https://doi.org/10.1007/s10489-022-03664-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearing a safety helmet can reduce the accident rate in production and construction, and it is a necessary part of safety production management. At present, the effective supervision of helmet wearing still relies on manual on-site work, which is inefficient and wastes manpower and material resources. Therefore, the automatic supervision of helmet wearing detection is of great significance. Due to the detection difficulties such as small helmet target, complex background and variety of helmet shape with the posture of workers, helmet wearing detection has always been one of the most difficult tasks in the field of computer vision. To address this issue, we propose a novel object detection model based on anchor-free mechanism—Recurrent Bidirectional Feature Pyramid Detector (RBFPDet). Different from most other detection methods, we regard helmet wearing detection as a strong semantic feature points detection task. In order to prove the effectiveness of our method, we conduct control experiment and ablation study on two mainstream safety helmet wearing datasets. The experiment results show that our method significantly improves the accuracy of helmet wearing detection compared with other outstanding detection models in this field and our model can realize real-time detection under complex background. At the same time, we further intuitively illustrate the effectiveness of our method by means of feature map visualization.},
  archive      = {J_APIN},
  author       = {Song, Renjie and Wang, Ziming},
  doi          = {10.1007/s10489-022-03664-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5013-5028},
  shortjournal = {Appl. Intell.},
  title        = {RBFPDet: An anchor-free helmet wearing detection method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosted support vector machines with genetic selection.
<em>APIN</em>, <em>53</em>(5), 4996–5012. (<a
href="https://doi.org/10.1007/s10489-022-03712-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the experimental studies of ensembles of binary classifiers conformed of individual support vector machines. The GenBoost-SVM method is proposed to construct such ensembles. Our ensembles considered an adaptive boosting algorithm. We analyzed different pre-selections using genetic algorithms to reduce the size of the training samples and hence the training times. These genetic selections addressed the imbalanced data challenge directly. Furthermore, in our ensembles, diversity and early stopping were considered to help to reduce the generalization error. We proposed 56 different types of ensembles that permute the support vector machine kernels, genetic selections and diversity. We found that our ensembles, which consider genetic selections and diversity, exhibit competitive performances when compared with various popular classifiers, and for imbalanced data, they outperform most of the considered popular classifiers. We show that using different support vector machine kernels leads to enhanced performances. To the best of our knowledge, this is the first study that combines adaptive boosted ensembles, genetic selections, and support vector machines.},
  archive      = {J_APIN},
  author       = {Ramirez-Morales, A. and Salmon-Gamboa, J. U. and Li, Jin and Sanchez-Reyna, A. G. and Palli-Valappil, A.},
  doi          = {10.1007/s10489-022-03712-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4996-5012},
  shortjournal = {Appl. Intell.},
  title        = {Boosted support vector machines with genetic selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RSPMP: Real-time semantic perception and motion planning for
autonomous navigation of unmanned ground vehicle in off-road
environments. <em>APIN</em>, <em>53</em>(5), 4979–4995. (<a
href="https://doi.org/10.1007/s10489-022-03283-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering autonomous navigation of an unmanned ground vehicle (UGV) in off-road environments, it faces various problems, such as semantic perception and motion planning. This paper proposes an intelligent approach to perception and planning for UGV in field environments. Firstly, a semantic image of environment is generated in real time based on an improved Convolutional Neural Network (CNN). Secondly, we provide two practical extensions to an open-source 3D mapping framework. One is the semantic point cloud fusion based on 3D LIDAR and Camera, and the other is the generation of traversability cost map using both semantic and geometric information. Thirdly, we propose a new kinodynamic semantic-aware planner which adds the dynamic window approach to the receding horizon planner so that the latter can meet the kinodynamic while perceiving semantic labels. Finally, the above methods, along with a localization module, are integrated into a complete autonomous navigation system with real-time semantic perception and motion planning (RSPMP). In the experiments, the proposed method was successfully applied for safe autonomous perception navigation in off-road environments.},
  archive      = {J_APIN},
  author       = {Chen, Denglong and Zhuang, Mingxi and Zhong, Xunyu and Wu, Wenhong and Liu, Qiang},
  doi          = {10.1007/s10489-022-03283-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4979-4995},
  shortjournal = {Appl. Intell.},
  title        = {RSPMP: Real-time semantic perception and motion planning for autonomous navigation of unmanned ground vehicle in off-road environments},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical initialization of intrinsic k-means clustering
on homogeneous manifolds. <em>APIN</em>, <em>53</em>(5), 4959–4978. (<a
href="https://doi.org/10.1007/s10489-022-03698-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The K-means algorithm is widely applied for clustering, and its clustering effect is influenced by its initialization. However, most existing works focus on the initialization of K and centers in Euclidean spaces, but few works in the literature deal with the initialization of K-means clustering on Riemannian manifolds. In this paper, we propose a unified scheme for learning K and selecting the initial centers for intrinsic K-means clustering on homogeneous manifolds, which can also be generalized to other types of manifolds. First, geodesic verticality is presented based on the geometric properties abstracted from the definition of orthogonality in Euclidean spaces. Then, geodesic projection on Riemannian manifolds is proposed for learning K, which achieves nonlinear dimensionality reduction and improves the computing efficiency. Additionally, the Riemannian metric of $\mathbb {S}^{n}$ is derived for the statistical initialization of the centers to improve the clustering accuracy. Finally, the intrinsic K-means algorithm for clustering on homogeneous manifolds based on the Karcher mean is given by applying the proposed manifold initialization, which improves the clustering effect. Simulations and experimental studies are conducted to show the effectiveness and accuracy of the proposed K-means scheme on manifolds.},
  archive      = {J_APIN},
  author       = {Tan, Chao and Zhao, Huan and Ding, Han},
  doi          = {10.1007/s10489-022-03698-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4959-4978},
  shortjournal = {Appl. Intell.},
  title        = {Statistical initialization of intrinsic K-means clustering on homogeneous manifolds},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A relative labeling importance estimation algorithm based on
global-local label correlations for multi-label learning. <em>APIN</em>,
<em>53</em>(5), 4940–4958. (<a
href="https://doi.org/10.1007/s10489-022-03700-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning, considering the relative importance between labels can yield better performance than considering the equal importance. To explore relative labeling importance, many existing algorithms introduce the global label correlations. However, the global correlations can only reflect the semantic relation between labels, while ignoring the label correlation differences between different instances. In practical applications, labels with high semantic relevance may not be highly relevant in all instances. In this paper, we consider both global and local label correlations to estimate relative labeling importance. Firstly, we calculate a global label correlation matrix in the whole label space. Secondly, each instance subset is assigned a local label correlation matrix, which is learned from the cosine similarity of labels within the cluster. Based on the assumption that label correlations can be transferred from the original categorical space to the numerical label space, we add global and local label correlation regularization terms. Finally, we integrate the importance estimating and the model training into a unified framework, and propose an alternative optimization algorithm to solve it. To validate the effectiveness of the proposed algorithm, we conduct experiments on thirteen multi-label datasets. Experimental results show that the proposed algorithm outperforms existing multi-label learning algorithms.},
  archive      = {J_APIN},
  author       = {Liu, Yilu and Cao, Fuyuan},
  doi          = {10.1007/s10489-022-03700-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4940-4958},
  shortjournal = {Appl. Intell.},
  title        = {A relative labeling importance estimation algorithm based on global-local label correlations for multi-label learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Reciprocal question representation learning network for
visual dialog. <em>APIN</em>, <em>53</em>(5), 4924–4939. (<a
href="https://doi.org/10.1007/s10489-022-03795-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual dialog task entails an agent to answer a series of questions based on an image and the dialog history. Biases are often observed when the agent over relies on the dialog history. Thus, balanced usage of dialog history is crucial. Existing models usually drop several rounds of dialog history or learn a sparse dialog structure to address the overreliance on such history; however, bias might still exist in the selected dialog history. Therefore, we propose a new model, reciprocal question representation learning network (RQRLN), with less bias from dialog history by learning more accurate history-aware representations of questions. Initially, RQRLN adaptively selects favorable information at the token level from two representations of a question encoded with and without a dialog history. Later, the adaptive question representation is assembled with the corresponding image for the final decoder. We also used a new entropy loss function which further reduces the dialog history-based bias, enabling two different types of representations of the same token to learn interactively. Analysis results on the VisDial v1.0 dataset showed that our proposed model achieved state-of-the-art results in terms of normalized discounted cumulative gain (NDCG). We also demonstrate that our model shows lesser bias and infers more generic answers in comparison with models that use the entire history.},
  archive      = {J_APIN},
  author       = {Zhang, Hongwei and Wang, Xiaojie and Jiang, Si},
  doi          = {10.1007/s10489-022-03795-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4924-4939},
  shortjournal = {Appl. Intell.},
  title        = {Reciprocal question representation learning network for visual dialog},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domestic pig sound classification based on TransformerCNN.
<em>APIN</em>, <em>53</em>(5), 4907–4923. (<a
href="https://doi.org/10.1007/s10489-022-03581-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excellent performance has been demonstrated in implementing challenging agricultural production processes using modern information technology, especially in the use of artificial intelligence methods to improve modern production environments. However, most of the existing work uses visual methods to train models that extract image features of organisms to analyze their behavior, and it may not be truly intelligent. Because vocal animals transmit information through grunts, the information obtained directly from the grunts of pigs is more useful to understand their behavior and emotional state, which is important for monitoring and predicting the health conditions and abnormal behavior of pigs. We propose a sound classification model called TransformerCNN, which combines the advantages of CNN spatial feature representation and the Transformer sequence coding to form a powerful global feature perception and local feature extraction capability. Through detailed qualitative and quantitative evaluations and by comparing state-of-the-art traditional animal sound recognition methods with deep learning methods, we demonstrate the advantages of our approach for classifying domestic pig sounds. The scores for domestic pig sound recognition accuracy, AUC and recall were 96.05%, 98.37% and 90.52%, respectively, all higher than the comparison model. In addition, it has good robustness and generalization capability with low variation in performance for different input features.},
  archive      = {J_APIN},
  author       = {Liao, Jie and Li, Hongxiang and Feng, Ao and Wu, Xuan and Luo, Yuanjiang and Duan, Xuliang and Ni, Ming and Li, Jun},
  doi          = {10.1007/s10489-022-03581-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4907-4923},
  shortjournal = {Appl. Intell.},
  title        = {Domestic pig sound classification based on TransformerCNN},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature semantic space-based sim2real decision model.
<em>APIN</em>, <em>53</em>(5), 4890–4906. (<a
href="https://doi.org/10.1007/s10489-022-03566-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the intelligent decision model of unmanned systems can only be applied to virtual scenes, which makes it difficult to migrate to real scenes because the image gap between virtual scenes and real scenes is relatively large. The main solutions are domain randomization, domain adaptation, and image translation. However, these methods simply add noise and transform the perceptual information and do not consider the semantic information of the agent’s perceptual space. This causes the problem of low accuracy in the migration of virtual scene decision models to real scenes. Considering the above problems, we propose a feature semantic space-based sim2real decision model, which includes an environment representation module, policy optimization module and intelligent decision module. The model framework can narrow the image gap between real-world scenes and virtual scenes. First, using the environment representation module, the virtual scene and real scene are simultaneously mapped to the feature semantic space through semantic segmentation. Then, in the policy optimization module, we propose an AMDDPG policy optimization algorithm. The algorithm obtains the local and global experience in the learning process through the global and local network architecture. It also solves the problem of the slow learning rate of sim2real. Finally, in the intelligent decision module, the data in the semantic space integrating virtual scene and real scene features are used as the training data of the agent autonomous decision model. Experimental results confirm that our method has more effective generalization and robustness of the model in the real scene and can be better migrated to the real scene.},
  archive      = {J_APIN},
  author       = {Xiao, Wenwen and Luo, Xiangfeng and Xie, Shaorong},
  doi          = {10.1007/s10489-022-03566-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4890-4906},
  shortjournal = {Appl. Intell.},
  title        = {Feature semantic space-based sim2real decision model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attentive LSTM based approach for adverse drug reactions
prediction. <em>APIN</em>, <em>53</em>(5), 4875–4889. (<a
href="https://doi.org/10.1007/s10489-022-03721-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse drug reactions (ADRs), which are harmful physical reactions of patients to drug treatments, are inherent to the nature of drugs; the reactions can occur with any drug and are becoming a leading cause of patient morbidity and mortality during medical procedures. ADRs can be hazardous and even fatal to patients. In traditional methods, ADRs are detected through clinical trials. To obtain a comprehensive collection of ADRs, sufficient experimental samples and time are required before a drug comes to the market, which is not a realistic possibility. Moreover, even if extensive clinical trials are performed, many undetected ADRs might still be discovered after a drug is released to the market. ADRs can lead to disastrous consequences for humanity, which obviates a dramatically increased need for precise predictions of potential ADRs as early as possible. In this paper, we propose an encoder-decoder framework based on attention mechanism and the long short-term memory (LSTM) model to predict potential ADRs. We regard the prediction of ADRs as a sequence-to-sequence problem and improve the encoder-decoder framework based on the attention mechanism to learn the interrelationships between ADRs. Unlike other classical methods utilizing molecular drug structures, our model is based solely on ADRs, which is an independent but parallel approach compared to traditional methods. We capitalize on the mask method to generate the target data and use the 5-fold cross-validation method to cyclically verify the performance of our proposed model. Based on the Top-k accuracy test results, our model outperforms the baseline models in potential ADRs predictions.},
  archive      = {J_APIN},
  author       = {Qian, Jiahui and Qiu, Xihe and Tan, Xiaoyu and Li, Qiong and Chen, Jue and Jiang, Xiaoyan},
  doi          = {10.1007/s10489-022-03721-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4875-4889},
  shortjournal = {Appl. Intell.},
  title        = {An attentive LSTM based approach for adverse drug reactions prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using self-information uncertainty
measures in neighborhood information systems. <em>APIN</em>,
<em>53</em>(4), 4524–4540. (<a
href="https://doi.org/10.1007/s10489-022-03760-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neighborhood rough set model (NRS) has been widely applied to study feature selection. Nevertheless, the dependency, as a significant feature evaluation function in NRS, only focuses on the classification information in the lower approximation and ignores the classification information in the upper approximation, which affects the evaluation effect of this function. Consequently, this paper first defines the fuzziness using the upper approximation and proposes two self-information uncertainty measures based on the dependency and fuzziness. Second, combining the above two self-information uncertainty measures, a more comprehensive approximate self-information is proposed for evaluating the uncertainty of the classification information of feature subsets. Furthermore, a heuristic feature selection algorithm is constructed based on the approximate self-information. Third, to reduce the time cost of the constructed algorithm in processing high-dimensional datasets, we propose a two-stage selection strategy, in which the first stage adopts the Fisher score dimensionality reduction method (FS) with low time cost and stable performance to retain important features in the high-dimensional dataset as a candidate feature subset. Then, the second stage employs our algorithm to further reduce the candidate feature subset. Finally, the results of various feature selection algorithms on eleven datasets are presented, and the comparison results confirm that our algorithm is efficient.},
  archive      = {J_APIN},
  author       = {Xu, Jiucheng and Qu, Kanglin and Sun, Yuanhao and Yang, Jie},
  doi          = {10.1007/s10489-022-03760-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4524-4540},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection using self-information uncertainty measures in neighborhood information systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation and memory optimized spectral domain
convolutional neural network for throughput and energy-efficient
inference. <em>APIN</em>, <em>53</em>(4), 4499–4523. (<a
href="https://doi.org/10.1007/s10489-022-03756-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional convolutional neural networks (CNNs) present a high computational workload and memory access cost (CMC). Spectral domain CNNs (SpCNNs) offer a computationally efficient approach to compute CNN training and inference. This paper investigates CMC of SpCNNs and its contributing components analytically and then proposes a methodology to optimize CMC, under three strategies, to enhance inference performance. In this methodology, output feature map (OFM) size, OFM depth or both are progressively reduced under an accuracy constraint to compute performance-optimized CNN inference. Before conducting training or testing, it can provide designers guidelines and preliminary insights regarding techniques for optimum performance, least degradation in accuracy and a balanced performance–accuracy trade-off. This methodology was evaluated on MNIST and Fashion MNIST datasets using LeNet-5 and AlexNet architectures. When compared to state-of-the-art SpCNN models, LeNet-5 achieves up to 4.2× (batch inference) and 4.1× (single-image inference) higher throughputs and 10.5× (batch inference) and 4.2× (single-image inference) greater energy efficiency at a maximum loss of 3% in test accuracy. When compared to the baseline model used in this study, AlexNet delivers 11.6× (batch inference) and 5× (single-image inference) increased throughput and 25× (batch inference) and 8.8× (single-image inference) more energy-efficient inference with just 4.4% reduction in accuracy.},
  archive      = {J_APIN},
  author       = {Rizvi, Shahriyar Masud and Rahman, Ab Al-Hadi Ab and Sheikh, Usman Ullah and Fuad, Kazi Ahmed Asif and Shehzad, Hafiz Muhammad Faisal},
  doi          = {10.1007/s10489-022-03756-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4499-4523},
  shortjournal = {Appl. Intell.},
  title        = {Computation and memory optimized spectral domain convolutional neural network for throughput and energy-efficient inference},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic signal control using a cooperative EWMA-based
multi-agent reinforcement learning. <em>APIN</em>, <em>53</em>(4),
4483–4498. (<a
href="https://doi.org/10.1007/s10489-022-03643-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary urban, traffic signal control is still enormously difficult. Multi-agent reinforcement learning (MARL) is a promising ways to solve this problem. However, most MARL algorithms can not effectively transfer learning strategies when the agents increase or decrease. This paper proposes a new MARL algorithm called cooperative dynamic delay updating twin delayed deep deterministic policy gradient based on the exponentially weighted moving average (CoTD3-EWMA) to solve the problem. By introducing mean-field theory, the algorithm implicitly models the interaction between agents and environment. It reduces the dimension of action space and improves the scalability of the algorithm. In addition, we propose a dynamic delay updating method based on the exponentially weighted moving average (EWMA), which improves the Q value overestimation problem of the traditional TD3 algorithm. Moreover, a joint reward allocation mechanism and state sharing mechanism are proposed to improve the global strategy learning ability and robustness of the agent. The simulation results show that the performance of the new algorithm is better than the current state-of-the-art algorithms, which effectively reduces the delay time of vehicles and improves the traffic efficiency of the traffic network.},
  archive      = {J_APIN},
  author       = {Qiao, Zhimin and Ke, Liangjun and Wang, Xiaoqiang},
  doi          = {10.1007/s10489-022-03643-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4483-4498},
  shortjournal = {Appl. Intell.},
  title        = {Traffic signal control using a cooperative EWMA-based multi-agent reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selection of a solar water heater for large-scale group
decision making with hesitant fuzzy linguistic preference relations
based on the best-worst method. <em>APIN</em>, <em>53</em>(4),
4462–4482. (<a
href="https://doi.org/10.1007/s10489-022-03688-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the promotion of sustainable development policies, solar water heaters have quickly occupied the market. Choosing the best-performing solar water heater has become a popular group decision-making problem. This paper presents a best-worst large-scale group decision making (LSGDM) approach based on hesitant fuzzy linguistic preference relations (HFLPRs) and applies it to the selection of solar water heaters. First, we establish an optimization model to normalize hesitant fuzzy linguistic term sets (HFLTSs) with diverse lengths. Then, a satisfaction degree function based on the additive consistency of HFLPRs is proposed, and the fluctuation deviation level of HFLPRs is described. Next, we develop a clustering process combining the satisfaction degree and fluctuation deviation level to classify large-scale experts into several subgroups. The clustering algorithm not only improves the fusion of subgroups but also enhances the satisfaction of subgroups. Moreover, a best-worst model based on the credibility measure of subgroups is presented to determine the weights of the subgroups. Finally, a numerical example of a solar water heater which involves normalizing the HFLPRs given by 20 experts into HFLTSs with a length of 3, clustering the 20 experts into 7 subgroups with weights of 0.5153, 0.2125, 0.1332, 0.0155, 0.0854, 0.0287 and 0.0094, and selecting P1 as the most satisfactory alternative and a comparative analysis are utilized to demonstrate the availability of the proposed method.},
  archive      = {J_APIN},
  author       = {Zhou, Yuanyuan and Zheng, Chengli and Zhou, Ligang and Chen, Huayou},
  doi          = {10.1007/s10489-022-03688-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4462-4482},
  shortjournal = {Appl. Intell.},
  title        = {Selection of a solar water heater for large-scale group decision making with hesitant fuzzy linguistic preference relations based on the best-worst method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multisensor-based tool wear diagnosis using 1D-CNN and
DGCCA. <em>APIN</em>, <em>53</em>(4), 4448–4461. (<a
href="https://doi.org/10.1007/s10489-022-03773-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bad conditions during machining cause tool chatter, wear, or breakage, which affect the tool life and consequently the surface quality and dimensional accuracy of the machined workpiece. Therefore, for the purposes of production efficiency and economics, monitoring and diagnostics of the tool’s condition are important. This study presents a one-dimensional convolutional neural network (1D-CNN) and deep generalized canonical correlation analysis (DGCCA) for multiple sensors-based tool wear diagnosis. In particular, 1D-CNN is used to extract features from 1D raw data, such as force, vibration, and sound, whereas DGCCA with attention mechanism is used to fuse the feature output from each 1D-CNN by removing irrelevant or redundant information. Experiments are performed using PHM2010 and NASA data sets. The experimental results show that our proposed approach can achieve satisfactory accuracy of 95.6% and near real-time performance. Results of our study can be implemented in real tool wear diagnosis, and thus identify novel opportunities toward realizing Industry 4.0.},
  archive      = {J_APIN},
  author       = {Yin, Yong and Wang, Shuxin and Zhou, Jian},
  doi          = {10.1007/s10489-022-03773-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4448-4461},
  shortjournal = {Appl. Intell.},
  title        = {Multisensor-based tool wear diagnosis using 1D-CNN and DGCCA},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A location-aware siamese network for high-speed visual
tracking. <em>APIN</em>, <em>53</em>(4), 4431–4447. (<a
href="https://doi.org/10.1007/s10489-022-03636-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately locating the target position is a challenging task during high-speed visual tracking. Most Siamese trackers based on shallow networks can maintain a fast speed, but they have poor positioning performance. The underlying reason for this is that the appearance features extracted from the shallow network are not effective enough, making it difficult to accurately locate the target from the complex background. Therefore, we present a location-aware Siamese network to address this issue. Specifically, we propose a novel context enhancement module (CEM), which contributes to capturing distinguished object information from both the local and the global levels. At the local level, the features of image local blocks contain more discriminative information that is conductive to locating the target. At the global level, global context information can effectively model long-range dependency, meaning that our tracker can better understand the tracking scene. Then, we construct a well-designed feature fusion network (F-net) to make full use of context information at different scales, where the location block can dynamically adjust to the convolution direction according to the geometry of the target. Finally, Distance-IoU loss (DIoU) is employed to guide the tracker to obtain a more accurate estimation of the target position. Extensive experiments on seven benchmarks including the VOT2016, VOT2018, VOT2019, OTB50, OTB100, UAV123 and LaSOT demonstrate that our tracker achieves competitive results while running at over 200 frames-per-second (FPS).},
  archive      = {J_APIN},
  author       = {Zhou, Lifang and Ding, Xiang and Li, Weisheng and Leng, Jiaxu and Lei, Bangjun and Yang, Weibin},
  doi          = {10.1007/s10489-022-03636-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4431-4447},
  shortjournal = {Appl. Intell.},
  title        = {A location-aware siamese network for high-speed visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A robust spatial-temporal correlation filter tracker for
efficient UAV visual tracking. <em>APIN</em>, <em>53</em>(4), 4415–4430.
(<a href="https://doi.org/10.1007/s10489-022-03727-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filters (CFs) have exhibited remarkable performance in visual target tracking, especially in aerial tracking of unmanned aerial vehicles (UAVs). Most existing CF-based trackers focus on how to effectively settle the unwanted boundary effect problem, while ignoring the different contributions of the discriminative features, which would lead to suboptimal performance in tracking. In this work, a robust spatial-temporal correlation filter, i.e., the temporal regularized background-aware correlation filter (TRBCF), is proposed. In detail, by extracting real background patches as negative samples and introducing a temporal regularization term, TRBCF improves the discriminability between the target and background in the spatial domain, and achieves continuous tracking in temporal sequences. Moreover, in order to selectively highlight the informative features and effectively represent the target, a novel multi-feature fusion mechanism based on the channel-wise response maps is proposed. Extensive experiments are conducted to evaluate the effectiveness of the proposed TRBCF on three classical UAV datasets (UAV123@10fps, DTB70, and UAVDT), and TRBCF performs favorably compared with the state-of-the-art trackers, with a real-time speed (41.38 fps) on a single CPU.},
  archive      = {J_APIN},
  author       = {Chen, Lin and Liu, Yungang},
  doi          = {10.1007/s10489-022-03727-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4415-4430},
  shortjournal = {Appl. Intell.},
  title        = {A robust spatial-temporal correlation filter tracker for efficient UAV visual tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reservoir calculation model and its application in the
field of temperature and humidity prediction. <em>APIN</em>,
<em>53</em>(4), 4393–4414. (<a
href="https://doi.org/10.1007/s10489-022-03685-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of global climate change, an effective prediction of temperature and humidity can improve people’s living environment and quality of life. To handle the problem of temperature and humidity series prediction, this study proposes a deep reservoir calculation model, namely DeepSALR. The DeepSALR uses a deep neural network to pre-train the datapoints before capturing multiple degrees of abstract information in the series. Then, the feature extraction datapoints are fed into the reservoir calculation model for supervised prediction. Considering the problem that using single neurons as the excitation function is prone to produce singular solutions, this study proposes to use wavelet neurons to replace part of sigmoid neurons, and then obtains an enhanced DeepSALR model, namely eDeepSALR. At the same time, this study also proposes a hybrid particle swarm optimization (HPSO) algorithm to determine the node numbers in deep neural networks. Extensive experimental results show that the eDeepSALR is capable of solving temperature and humidity prediction problems, and that it outperforms existing models in terms of prediction accuracy and short-term memory.},
  archive      = {J_APIN},
  author       = {Zhang, Minghui and Zhou, Yatong and Liu, Yabo},
  doi          = {10.1007/s10489-022-03685-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4393-4414},
  shortjournal = {Appl. Intell.},
  title        = {Deep reservoir calculation model and its application in the field of temperature and humidity prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial multi-task deep learning for signer-independent
feature representation. <em>APIN</em>, <em>53</em>(4), 4380–4392. (<a
href="https://doi.org/10.1007/s10489-022-03649-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research has achieved remarkable progress in Sign Language Recognition (SLR). However, for robust open-set SLR applications, it is necessary to solve signer-independent SLR. This paper proposes a novel adversarial multi-task deep learning (MTL) framework that can incorporate multiple modalities for isolated SLR. Employing the identity recognition task as the competition task to the target SLR task, the proposed model can effectively extract signer-independent features by deviating the optimization direction of the competitive task. Furthermore, the proposed adversarial MTL multi-modality framework can jointly incorporate positive and negative task learning with the target task. Combining multi-modality in the adversarial MTL, our model can extract robust signer-independent representation. We evaluate our method on multiple benchmark datasets from different sign languages. The experimental results demonstrate that the proposed adversarial MTL multi-modality model can effectively realize signer-independent SLR by compensation with relevant tasks and competition with irrelevant tasks.},
  archive      = {J_APIN},
  author       = {Fang, Yuchun and Xiao, Zhengye and Cai, Sirui and Ni, Lan},
  doi          = {10.1007/s10489-022-03649-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4380-4392},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial multi-task deep learning for signer-independent feature representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective inter-aspect words modeling for aspect-based
sentiment analysis. <em>APIN</em>, <em>53</em>(4), 4366–4379. (<a
href="https://doi.org/10.1007/s10489-022-03630-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a prominent and challenging issue in natural language processing tasks. It aims to analyze the emotion of the aspect words in given subjective sentences. A subjective sentence usually contains one or more aspect words, and there are potential associations between different aspect words. At present, many works in the literature ignore the potential relationship between aspect words. Therefore, in this paper, we propose an oriented inter-aspect modeling hierarchical network (IA-HiNET), which aims to mine and strengthen the relationship between different aspect words, and further realize the task of sentence-level sentiment analysis based on aspect words. Specifically, we introduce part-of-speech information and position information as a priori knowledge, and then construct a graph convolution network (GCN) based on sentence dependency to capture emotional cues related to aspect words. We design an aspect-oriented self-attention mechanism to map different aspect words with the same attribute into the same vector space to determine the correlation between different aspect words. Furthermore, we design a novel information gate mechanism to filter the emotional features unrelated to aspect words. The indicative importance between different aspect words is also used to assist the aspect-based sentence-level affective analysis task. We carry out experiments on four benchmark datasets, and excellent experimental results show the effectiveness of our model.},
  archive      = {J_APIN},
  author       = {Gu, Tiquan and Zhao, Hui and Li, Min},
  doi          = {10.1007/s10489-022-03630-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4366-4379},
  shortjournal = {Appl. Intell.},
  title        = {Effective inter-aspect words modeling for aspect-based sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A compensation method for gyroscope random drift based on
unscented kalman filter and support vector regression optimized by
adaptive beetle antennae search algorithm. <em>APIN</em>,
<em>53</em>(4), 4350–4365. (<a
href="https://doi.org/10.1007/s10489-022-03734-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random error of fiber optic gyroscope (FOG) is an important factor affecting its performance. In this paper, a novel and efficient compensation scheme for random drift is presented. It is a new model based on fusing unscented Kalman filter (UKF) with support vector regression (SVR) optimized by the adaptive beetle antennae search (ABAS) algorithm. At first, to make up for the shortcomings of the basic beetle antennae search algorithm, an adaptive decay factor is proposed to dynamically adjust the update of the search step size. The proposed ABAS algorithm exhibits more superior global optimization capability in hyperparameter optimization of SVR. And then to better characterize the nonlinearity and randomness of random drift, the optimized SVR is presented for the modeling of random drift data. Considering the improvement of modeling accuracy, this study also presents to preprocess the raw data by using the variational mode decomposition (VMD) algorithm and sliding window method. Furthermore, as an online processing method, UKF is introduced and fused with optimized SVR modeling, and a hybrid model is constructed by designing state space equations. Finally, experiments are conducted on the measured data of FOG to verify the superiority of the proposed model. The experimental results show that compared with the conventional method, in terms of the compensation accuracy for random drift data, noise intensity (NI) and Durbin-Watson (DW) value of the proposed scheme are reduced and improved by 28.57% and 9.06%, respectively.},
  archive      = {J_APIN},
  author       = {Wang, Pengfei and Li, Guangchun and Gao, Yanbin},
  doi          = {10.1007/s10489-022-03734-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4350-4365},
  shortjournal = {Appl. Intell.},
  title        = {A compensation method for gyroscope random drift based on unscented kalman filter and support vector regression optimized by adaptive beetle antennae search algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel efficient clustering algorithm based on
possibilistic approach and kernel technique for image clustering
problems. <em>APIN</em>, <em>53</em>(4), 4327–4349. (<a
href="https://doi.org/10.1007/s10489-022-03703-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed a new clustering algorithm named KGPFCM algorithm, based on the Generalized Possibilistic Fuzzy C-Means (GPFCM) algorithm and the Kernel method. The presented algorithm projects features in a nonlinear high dimensional input space into a high-dimensional linear output space, which preserves the locality of structural properties, thus improving the ability of the GPFCM algorithm to handle high-dimensional data. However. In addition, KGPFCM algorithm uses in addition to the Euclidean distance, further powerful norms more adapted to various complex problems. Moreover, compared to PFCM, GPFCM and two other recent algorithms like RSFCM and LPFCM methods, KGPFCM algorithm corrects many shortcomings, and the use of Kernel method provides a solution to the high dimensional space and the problem of nonlinear separable input space, hence, KGPFCM is regarded as a unified model that combines two important approaches in clustering technique. Therefore, we applied the KGPFCM algorithm as a new image clustering method based on the kernel technique and we utilized Jacobi orthogonal moments to extract the feature vectors from the images. Experimental results on some benchmark data sets show the effectiveness of KGPFCM algorithm in comparison with GPFCM and some state-of-the-art methods to detect cluster centers accurately and to give satisfactory results in image clustering field.},
  archive      = {J_APIN},
  author       = {Azzouzi, Souad and Hjouji, Amal and EL-Mekkaoui, Jaouad and EL Khalfi, Ahmed},
  doi          = {10.1007/s10489-022-03703-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4327-4349},
  shortjournal = {Appl. Intell.},
  title        = {A novel efficient clustering algorithm based on possibilistic approach and kernel technique for image clustering problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning a dual-branch classifier for class incremental
learning. <em>APIN</em>, <em>53</em>(4), 4316–4326. (<a
href="https://doi.org/10.1007/s10489-022-03556-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic forgetting is a non-trivial challenge for class incremental learning, which is caused by new knowledge learning and data imbalance between old and new classes. To alleviate this challenge, we propose a class incremental learning method with dual-branch classifier. First, inspired by ensemble learning, the proposed method constructs a dual network consisting of two complementary branches to alleviate the impact of data imbalance. Second, activation transfer loss is employed to reduce the catastrophic forgetting from the view of feature representation, preserving the feature separability of old classes. Third, we use the nearest class mean classifier with natural advantages for classification. Moreover, we formulate an end-to-end training algorithm for the feature extraction and classifier, to boost module matching degree. Extensive evaluation results show our proposed method achieves nice incremental recognition ability with less training time. Moreover, the ablation study shows the importance and necessity of dual-branch structure, end-to-end training, and activation transfer loss.},
  archive      = {J_APIN},
  author       = {Guo, Lei and Xie, Gang and Qu, Youyang and Yan, Gaowei and Cui, Lei},
  doi          = {10.1007/s10489-022-03556-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4316-4326},
  shortjournal = {Appl. Intell.},
  title        = {Learning a dual-branch classifier for class incremental learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilingual attention based neural machine translation.
<em>APIN</em>, <em>53</em>(4), 4302–4315. (<a
href="https://doi.org/10.1007/s10489-022-03563-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Recurrent Neural Network based Neural Machine Translation (RNN-based NMT) equipped with an attention mechanism from the decoder to encoder, has achieved great advancements and exhibited good performance in many language pairs. However, little work has been done on the attention mechanism for the target side, which has the potential to further improve NMT. To address this issue, in this paper, we propose a novel bilingual attention based NMT, where its bilingual attention mechanism exploits decoding history and enables the NMT model to better dynamically select and exploit source side and target side information. Compared with previous RNN-based NMT models, our model has two advantages: First, our model exercises a dynamic control over the ratios at which source and target contexts respectively contribute to the generation of the next target word. In this way, the weakly induced structure relations on both sides can be exploited for NMT. Second, through short-cut connections, the training errors of our model can be directly back-propagated, which effectively alleviates the gradient vanishing or exploding issue. Experimental results and in-depth analyses on Chinese-English, English-German, and English-French translation tasks show that our model with proper configurations can significantly surpass the dominant NMT model, Transformer. Particularly, our proposed model has won the first prize in the English-Chinese translation task of WMT2018.},
  archive      = {J_APIN},
  author       = {Kang, Liyan and He, Shaojie and Wang, Mingxuan and Long, Fei and Su, Jinsong},
  doi          = {10.1007/s10489-022-03563-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4302-4315},
  shortjournal = {Appl. Intell.},
  title        = {Bilingual attention based neural machine translation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFNet: Multi-level fusion aware feature pyramid based
multi-view stereo network for 3D reconstruction. <em>APIN</em>,
<em>53</em>(4), 4289–4301. (<a
href="https://doi.org/10.1007/s10489-022-03754-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient multi-view stereo (MVS) network for 3D reconstruction from multi-view images. While the existing state-of-the-art methods have achieved satisfactory results, the accuracy and scalability remain an open problem due to unreliable dense matching and memory-consuming cost volume regularization. To this end, we propose a multi-level fusion aware feature pyramid based multi-view stereo network (MFNet) for reliable depth inference. First, we adopt a coarse-to-fine strategy that achieves high-resolution depth estimation based on the coarse depth map. This strategy gradually narrows the depth search interval by using the prior information from the previous stage, which dramatically reduces memory consumption. Second, we conduct multi-level fusions to construct the feature pyramid such that the different level features receive information from each other, thus enabling rich multi-level feature representations. Finally, the group-wise correlation similarity measure is introduced to replace the variance-based approach used in previous works for cost volume construction, resulting in a lightweight and effective cost volume representation. Experimental results on the DTU, Tanks &amp; Temples, and BlendedMVS benchmark datasets show that MFNet achieves better results than the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Cai, Youcheng and Li, Lin and Wang, Dong and Liu, Xiaoping},
  doi          = {10.1007/s10489-022-03754-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4289-4301},
  shortjournal = {Appl. Intell.},
  title        = {MFNet: Multi-level fusion aware feature pyramid based multi-view stereo network for 3D reconstruction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework to detect DDoS attack in ryu controller based
software defined networks using feature extraction and classification.
<em>APIN</em>, <em>53</em>(4), 4268–4288. (<a
href="https://doi.org/10.1007/s10489-022-03565-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defined Network(SDN) is an emerging network architecture and is being used in many IT industries and academia. Its popularity in the present age has attracted many attacks in SDN. Distributed Denial of Service(DDoS) attack is a common issue in the domain of network security. In this work, DDoS attack detection is done using feature extraction and classification from the live traffic of SDN. An effective feature extraction mechanism will not only help in filtering the most suitable task-relevant data but also improve the performance of machine learning algorithms. To identify the best performing classifier with these extracted features, some well-known classifiers namely Support Vector Machine (SVM), Random Forest(RF), K-Nearest Neighbor, eXtreme Gradient Boosting(XGBoost) and Naive Bayes(NB) are trained and tested with the extracted features. It is found that SVM is outperforming other classifiers under some performance measuring metrics namely accuracy, precision, recall, False Alarm Rate(FAR),F1 value, and AUC value. Also, its performance is better than some other state-of-the art works so, it is selected for deployment in the SDN controller which can detect the attack in live traffic.},
  archive      = {J_APIN},
  author       = {Chouhan, Ravindra Kumar and Atulkar, Mithilesh and Nagwani, Naresh Kumar},
  doi          = {10.1007/s10489-022-03565-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4268-4288},
  shortjournal = {Appl. Intell.},
  title        = {A framework to detect DDoS attack in ryu controller based software defined networks using feature extraction and classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual discriminant adversarial cross-modal retrieval.
<em>APIN</em>, <em>53</em>(4), 4257–4267. (<a
href="https://doi.org/10.1007/s10489-022-03653-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the accuracy of cross-modal retrieval tasks and achieve flexible retrieval between different modalities, we propose a Dual Discriminant Adversarial cross-modal Retrieval (DDAC) method in this paper. First, DDAC integrates adversarial learning and minimization of feature projection distances and introduces label information in it. It can eliminate the same semantic heterogeneity between modalities while maintaining the distinguishability of different semantic features between modalities. Then, cosine distance is used to minimize and maximize the inter-modal distance of features with the same and different labels respectively to solve the inter-modal discrimination problem. Different from the general method, DDAC performs dual discrimination in the label space and solves the intra-modal discrimination problem from two perspectives of probability distribution and distance. Extensive experiments carried out on three public datasets validate that the proposed DDAC outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {He, Pei and Wang, Meng and Tu, Ding and Wang, Zhuo},
  doi          = {10.1007/s10489-022-03653-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4257-4267},
  shortjournal = {Appl. Intell.},
  title        = {Dual discriminant adversarial cross-modal retrieval},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approaches for coarsest granularity based near-optimal
reduct computation. <em>APIN</em>, <em>53</em>(4), 4231–4256. (<a
href="https://doi.org/10.1007/s10489-022-03571-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, the shortest length has been used as the optimality criterion in rough set based optimal / near-optimal reduct computation. A more generalizable alternative to the optimal reduct computation approach was recently introduced, with the coarsest granular space as the optimality criterion. However, owing to exponential time complexity, it is not scalable to even moderate-sized data sets. This article investigates to formulate two near-optimal reduct computation alternatives for scaling comparatively larger data sets. The first algorithm employs a controlled A∗ search based strategy to find a near-optimal reduct while reducing both space utilization and computational time. Whereas, the second algorithm employs a greedy sequential backward elimination (SBE) strategy on the higher granular space attribute ordering for achieving coarsest granular space based near-optimal reduct. The comparative experimental study is conducted among the proposed approaches with the coarsest granular space based optimal reduct algorithm A∗RSOR and state-of-the-art shortest length based optimal and near-optimal reduct algorithms. The experimental study amply validates the relevance of the proposed approaches in obtaining near-optimal reduct with increased scalability and comparable or improved generalizable classification models induction.},
  archive      = {J_APIN},
  author       = {Bar, Abhimanyu and Prasad, P. S. V. S. Sai},
  doi          = {10.1007/s10489-022-03571-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4231-4256},
  shortjournal = {Appl. Intell.},
  title        = {Approaches for coarsest granularity based near-optimal reduct computation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FTCF: Full temporal cross fusion network for violence
detection in videos. <em>APIN</em>, <em>53</em>(4), 4218–4230. (<a
href="https://doi.org/10.1007/s10489-022-03708-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic violence detection in video is a meaningful yet challenging task. Violent actions can be characterized both by intense sequential frames and by continuous spatial moves, imposing more complexity than other human actions. However, most existing approaches focus on general spatiotemporal features with local convolution and ignore the full temporal inference based on violence characteristics. To this end, we propose a novel full temporal cross fusion network (FTCF Net) to investigate an effective inference way for violence detection. Specifically, we design two essential components in each FTCF block: a spatial processor and a temporal processor by neural networks. The former is to capture the local structural features of each frame by a 3D CNN with a (3×3×1) filter to infer the continuous spatial moves, while the latter is to perform the cross-frame feature interaction step by step for each channel by a group of processing units to infer the intense and wide variation of violence in full temporal. The two branches are fused at the end of each FTCF block in the FTCF Net efficiently. We conduct extensive experiments on four benchmark datasets: Hockey Fight, Movie Fight, Violent Flow, and Real-life Violence Situations, and the experimental results show that FTCF Net outperforms 20 comparison methods in terms of predictive accuracy. The accuracy goes up to 99.5%, 100.0%, 98.0% and 98.5% in the four datasets respectively, validating the effectiveness of our proposed approach for violence detection. Moreover, the approach proposed in this paper obtains relative steady prediction performance superior to existing methods under different scale of training sets. We hope this work to be a baseline of violence detection, and the whole original codes and pre-trained weights are publicly available at https://github.com/TAN-OpenLab/FTCF-NET .},
  archive      = {J_APIN},
  author       = {Zhenhua, Tan and Zhenche, Xia and Pengfei, Wang and Chang, Ding and Weichao, Zhai},
  doi          = {10.1007/s10489-022-03708-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4218-4230},
  shortjournal = {Appl. Intell.},
  title        = {FTCF: Full temporal cross fusion network for violence detection in videos},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating structured emotion commonsense knowledge and
interpersonal relation into context-aware emotion recognition.
<em>APIN</em>, <em>53</em>(4), 4201–4217. (<a
href="https://doi.org/10.1007/s10489-022-03729-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional emotion recognition technology often focuses on recognizing human biometrics such as facial expressions or body postures. However, psychological research shows that the context (contextual information) also plays an important role in perceiving the emotions of others. Existing research methods, that are based on contextual information, have heavily relied on the semantic features of images. They do not take into account the interrelationships between objects and fail to use external knowledge. Meanwhile, external knowledge is likely to be very helpful in perceiving emotion. In this paper, by incorporating external structured emotion commonsense knowledge, two methods are proposed for constructing emotion knowledge graphs based on the objective text of images, and a multi-modal emotion recognition model is designed. The model has three branches, one of which focuses on human biometrics, and another two branches employ emotion knowledge graphs to perceive emotion from contextual information. Before constructing the emotion knowledge graphs, we convert the visual content into the text information to obtain the prime and ample contextual information from the object, scene, and the relationship between the objects. This approach can reduce redundant and invalid information. After that, the structured emotion commonsense knowledge is integrated into the objective text in word sharing. A large-scale emotion knowledge graph based on all valid words (LEKG) and a small-scale emotion knowledge graph based on the document itself (TEKG) are constructed, respectively. We propose two fusion modules, one of which is attention-based, and the other is a deep reasoning module that incorporates interpersonal relation. We conduct extensive experiments on the benchmark dataset EMOTIC. The experimental results prove that our method is superior to the most advanced methods, and it has obvious advantages in global context-aware tasks.},
  archive      = {J_APIN},
  author       = {Chen, Jing and Yang, Tao and Huang, Ziqiang and Wang, Kejun and Liu, Meichen and Lyu, Chunyan},
  doi          = {10.1007/s10489-022-03729-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4201-4217},
  shortjournal = {Appl. Intell.},
  title        = {Incorporating structured emotion commonsense knowledge and interpersonal relation into context-aware emotion recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SVF-net: Spatial and visual feature enhancement network for
brain structure segmentation. <em>APIN</em>, <em>53</em>(4), 4180–4200.
(<a href="https://doi.org/10.1007/s10489-022-03706-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain structure segmentation in Magnetic Resonance Images (MRI) is essential to the assessment and treatment of medical disorders, especially neuropsychiatric diseases. The key to semantic segmentation is to understand the low-level visual semantics and the high-level spatial semantics of the image. Due to the complex anatomical structures, the current approaches lack the ability to effectively extract rich semantic information, resulting in the inevitable loss of details in prediction results. To address this problem, we propose a novel spatial and visual feature enhancement network (SVF-Net) to accurately segment the brain structures. The SVF-Net is designed as a multi-task learning framework, in which an auxiliary coarse segmentation task is used for spatial information acquisition, an auxiliary image reconstruction task is used for visual information preservation, and a major refined segmentation task is used for brain structure segmentation. In this algorithm, multitasking optimization is mainly based on two strategies: firstly, a spatial feature enhancement (SFE) module is introduced to extract the location and spatial relationships of objects from the coarse prediction, which are then sent to the refined segmentation model for spatial information enhancement. Secondly, a visual feature preservation (VFP) model is introduced for image reconstruction, which shares the feature extractor with the refined segmentation model, so as to retain more useful low-level visual features for the model. Extensive experiments are performed on three public brain MRI T1 scan datasets (the IBSR dataset, the MALC dataset and the LPBA dataset) to evaluate the effectiveness of the proposed algorithm. The experimental results show that the SVF-Net achieves the best performance compared with the state-of-the-art methods. In addition, the ablation experiments and the noise interference experiments demonstrate that proposed SFE and VFP module have obvious advantages in improving segmentation accuracy and resisting noise interference.},
  archive      = {J_APIN},
  author       = {Hu, Qian and Wei, Ying and Li, Xiang and Wang, Chuyuan and Wang, Huan and Wang, Shanze},
  doi          = {10.1007/s10489-022-03706-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4180-4200},
  shortjournal = {Appl. Intell.},
  title        = {SVF-net: Spatial and visual feature enhancement network for brain structure segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small-target ship detection in SAR images based on densely
connected deep neural network with attention in complex scenes.
<em>APIN</em>, <em>53</em>(4), 4162–4179. (<a
href="https://doi.org/10.1007/s10489-022-03683-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection research has made great progress with deep learning method in recent years. Because some ship targets are too small and the complex scenes are difficult to distinguish from the ship, the accuracy of small-target ship detection in synthetic aperture radar (SAR) images still requires improvement. In order to effectively reduce the interference of complex scenes and locate the small-target more precisely, we propose a model (Dense-YOLOv4-CBAM) based on a densely connected deep neural network with an attention mechanism. The main improvements and contributions of our propose model are three aspects. First, the dense connections is increased in the backbone network (CSP-Darknet53) based on the YOLOv4 framework to enhance the transmission of image features. Second, to refine the transmission of top-down semantic features and bottom-up positioning features in the feature fusion module, we notivatively insert a spatial and channel convolutional attention mechanism (CBAM) into the feature fusion module. This mechanism can reduce the interference in complex scenes and enable a more effective use of each layer’s features to reduce the loss of the semantic information of small targets. Finally, we introduce two kinds of accuracy evaluation metrics to evaluate the effectiveness and robustness of the proposed method in extensive experiments. The results show that the proposed model achieves an optimal performance between the accuracy and calculation time compared to other state-of-the-art detection methods on public synthetic aperture radar ship detection datasets.},
  archive      = {J_APIN},
  author       = {Sun, Bowen and Wang, Xiaofeng and Li, Hao and Dong, Feng and Wang, Yang},
  doi          = {10.1007/s10489-022-03683-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4162-4179},
  shortjournal = {Appl. Intell.},
  title        = {Small-target ship detection in SAR images based on densely connected deep neural network with attention in complex scenes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A local context focus learning model for joint multi-task
using syntactic dependency relative distance. <em>APIN</em>,
<em>53</em>(4), 4145–4161. (<a
href="https://doi.org/10.1007/s10489-022-03684-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a significant task in natural language processing. Although many ABSA systems have been proposed, the correlation between the aspect’s sentiment polarity and local context semantic information was not a point of focus. Moreover, aspect term extraction and aspect sentiment classification are fundamental tasks of aspect-based sentiment analysis. However, most existing systems have failed to recognize the natural relation between these two tasks and therefore treat them as relatively independent tasks. In this work, a local context focus method is proposed. It represents semantic distance using syntactic dependency relative distance which is calculated on the basis of an undirected dependency graph. We introduced this method into a multi-task learning framework with a multi-head attention mechanism for aspect term extraction and aspect sentiment classification joint task. Compared with existing models, the proposed local context focus method measures the semantic distance more precisely and helps our model capture more effective local semantic information. In addition, a multi-head attention mechanism is employed to further enhance local semantic representation. Furthermore, the proposed model makes full use of aspect terminology information and aspect sentiment information provided by the two subtasks, thereby improving the overall performance. The experimental results on four datasets show that the proposed model outperforms single task and multi-task models on the aspect term extraction and aspect sentiment classification tasks.},
  archive      = {J_APIN},
  author       = {Qi, Rui-Hua and Yang, Ming-Xin and Jian, Yue and Li, Zheng-Guang and Chen, Heng},
  doi          = {10.1007/s10489-022-03684-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4145-4161},
  shortjournal = {Appl. Intell.},
  title        = {A local context focus learning model for joint multi-task using syntactic dependency relative distance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The state of the art in open domain complex question
answering: A survey. <em>APIN</em>, <em>53</em>(4), 4124–4144. (<a
href="https://doi.org/10.1007/s10489-022-03732-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on question answering (QA) systems has a long tradition. QA systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in QA systems. In this paper, we have a comparative study on QA approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
  archive      = {J_APIN},
  author       = {Etezadi, Romina and Shamsfard, Mehrnoush},
  doi          = {10.1007/s10489-022-03732-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4124-4144},
  shortjournal = {Appl. Intell.},
  title        = {The state of the art in open domain complex question answering: A survey},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforced domain adaptation with attention and adversarial
learning for unsupervised person re-ID. <em>APIN</em>, <em>53</em>(4),
4109–4123. (<a
href="https://doi.org/10.1007/s10489-022-03640-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods of unsupervised person re-identification (re-ID) still suffer from two aspects of challenges: inter-domain inconsistency and pseudo-label inaccuracy. To alleviate the two problems, we propose an reinforced domain adaptation (RDA) re-ID method by innovatively employing adversarial learning and spatial-channel attention. Specifically, to handle the inter-domain inconsistency problem, we specially design an adversarial learning module to reduce the feature discrepancy between target domain image and translated source domain image, and take Wasserstein distance as the discriminative function because that it can provide an effective gradient for model optimization regardless of the distribution difference between the source domain and the target domain. To handle the pseudo-label inaccuracy problem, we design an attention module to highlight the person region of the image so as to improve accuracy of person clustering and matching. An improved re-ID model can therefore be obtained by jointly training the translated source-domain images with ground-truth identities and target-domain images with pseudo identities. In addition, in order to maintain the semantic consistency of source domain images before and after style translation, we design a closed-loop training mechanism to refine the style translation based on the feedback from person re-ID result, finally making the style translation and person re-ID collaboratively converge to their best state. In the experiments, our proposed framework is shown to outperform state-of-the-art methods on multiple tasks of unsupervised person re-ID.},
  archive      = {J_APIN},
  author       = {Wei, Peiyi and Zhang, Canlong and Tang, Yanping and Li, Zhixin and Wang, Zhiwen},
  doi          = {10.1007/s10489-022-03640-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4109-4123},
  shortjournal = {Appl. Intell.},
  title        = {Reinforced domain adaptation with attention and adversarial learning for unsupervised person re-ID},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast adaptive algorithm for training deep neural networks.
<em>APIN</em>, <em>53</em>(4), 4099–4108. (<a
href="https://doi.org/10.1007/s10489-022-03629-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the adaptive algorithms, Adam is the most widely used algorithm, especially for training deep neural networks. However, recent studies have shown that it has a weak generalization ability, and even cannot converge in extreme cases. AdaX (2020) is a variant of Adam, which modifies the second moment of Adam, making the algorithm enjoy good generalization ability compared to SGD. This work aims to improve the AdaX algorithm with faster convergence speed and higher training accuracy. The first moment of AdaX is essentially a classical momentum term, while the Nesterov’s accelerated gradient (NAG) is theoretically and experimentally superior to this classical momentum. Therefore, we replace the classical momentum term of the first moment of AdaX with NAG, and obtain the resulting algorithm named Nesterov’s accelerated AdaX (Nadax). Extensive experiments on deep learning tasks show that training models with our proposed Nadax can bring favorable benefits.},
  archive      = {J_APIN},
  author       = {Gui, Yangting and Li, Dequan and Fang, Runyue},
  doi          = {10.1007/s10489-022-03629-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4099-4108},
  shortjournal = {Appl. Intell.},
  title        = {A fast adaptive algorithm for training deep neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining deep reinforcement learning decisions in complex
multiagent settings: Towards enabling automation in air traffic flow
management. <em>APIN</em>, <em>53</em>(4), 4063–4098. (<a
href="https://doi.org/10.1007/s10489-022-03605-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the objective to enhance human performance and maximize engagement during the performance of tasks, we aim to advance automation for decision making in complex and large-scale multi-agent settings. Towards these goals, this paper presents a deep multi agent reinforcement learning method for resolving demand - capacity imbalances in real-world Air Traffic Management settings with thousands of agents. Agents comprising the system are able to jointly decide on the measures to be applied to resolve imbalances, while they provide explanations on their decisions: This information is rendered and explored via appropriate visual analytics tools. The paper presents how major challenges of scalability and complexity are addressed, and provides results from evaluation tests that show the abilities of models to provide high-quality solutions and high-fidelity explanations.},
  archive      = {J_APIN},
  author       = {Kravaris, Theocharis and Lentzos, Konstantinos and Santipantakis, Georgios and Vouros, George A. and Andrienko, Gennady and Andrienko, Natalia and Crook, Ian and Garcia, Jose Manuel Cordero and Martinez, Enrique Iglesias},
  doi          = {10.1007/s10489-022-03605-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4063-4098},
  shortjournal = {Appl. Intell.},
  title        = {Explaining deep reinforcement learning decisions in complex multiagent settings: Towards enabling automation in air traffic flow management},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact of preprocessing and word embedding on extreme
multi-label patent classification tasks. <em>APIN</em>, <em>53</em>(4),
4047–4062. (<a
href="https://doi.org/10.1007/s10489-022-03655-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patent classification is a necessary step in the efficient processing of patent data and ensuring convenient information access to users. To address the present inefficiency of patent classification, many algorithms and deep learning-based techniques have been developed. However, there is a scarcity of studies on the impacts of preprocessing, word embedding, and data fields on patent classification. In this study, we examined three different scenarios to evaluate and analyze the effects of generalizing words via stemming on the classification performance considering the characteristics of patent data. Comparative experiments between pre-trained word embedding models and embedding models that underwent learning using a newly created patent dataset were conducted. Detailed descriptions of the preprocessing and word embedding techniques are provided. We found that the continuous bag-of-words (CBoW) embedding model that underwent learning using the patent dataset best reflected the words contained in the patent documents, and the hierarchical International Patent Classification (IPC) that is used in more than 100 countries had the biggest impact on the classification performance. Furthermore, the relationship between the number of embedded words and the classification performance was investigated. Finally, we performed classification experiments using different data fields and classification models. When the IPC was incorporated, the classification performance was substantially enhanced, and a high classification accuracy was achieved when a classification model that considered the relationship between labels and words was employed. We used the most commonly used indices, P@N and NDCG@N, to compare the performance of all models. Using the model with the best performance as determined via the aforementioned experiments, accuracies of P@1 = 71.896%, P@3 = 36.697%, and P@5 = 24.301% were obtained using two simple ensembles of LAHA models. We provide an in-depth investigation into patent classification methods that elucidates the effects of various parameters on the patent classification process. The results of this study will serve to improve the efficiency of patent research and classification tasks.},
  archive      = {J_APIN},
  author       = {Jung, Guik and Shin, Junghoon and Lee, Sangjun},
  doi          = {10.1007/s10489-022-03655-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4047-4062},
  shortjournal = {Appl. Intell.},
  title        = {Impact of preprocessing and word embedding on extreme multi-label patent classification tasks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TPE-ISE: Approximate thumbnail preserving encryption based
on multilevel DWT information self-embedding. <em>APIN</em>,
<em>53</em>(4), 4027–4046. (<a
href="https://doi.org/10.1007/s10489-022-03597-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud storage, more and more users upload images to the cloud. However, images stored in the cloud face the risk of unauthorized data mining by cloud service providers and being stolen by hackers. Encryption can protect image privacy, but traditional image encryption algorithms sacrifice image usability for security. To protect privacy while preserving image usability, two approximate thumbnail-preserving encryption (TPE) schemes, called dynamic range preserving encryption (DRPE) and approximate TPE with LSB Embedding (TPE-LSB), have been presented by Marohn in 2017. However, there is the possibility of decryption failure for DRPE, the cipher image robustness is poor for TPE-LSB, which cannot resist noise attacks. Additionally, both methods have the problems of the high file expansion rate of cipher image and poor perceptual quality of cipher image thumbnail. To solve these problems, a multi-level DWT information self-embedding method for thumbnail preserving encryption (TPE-ISE) is proposed. Compared with the previous works, the TPE-ISE scheme achieves a controllable compression ratio of cipher images, the perceptual quality of cipher images is closer to that of plain images, and the ability of cipher images to resist noise attacks is stronger. A series of experiments verify the superiority of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Wang, Yinjing and Chai, Xiuli and Gan, Zhihua and Zhang, Yushu and Chen, Xiuhui and He, Xin},
  doi          = {10.1007/s10489-022-03597-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {4027-4046},
  shortjournal = {Appl. Intell.},
  title        = {TPE-ISE: Approximate thumbnail preserving encryption based on multilevel DWT information self-embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deepfakes generation and detection: State-of-the-art, open
challenges, countermeasures, and way forward. <em>APIN</em>,
<em>53</em>(4), 3974–4026. (<a
href="https://doi.org/10.1007/s10489-022-03766-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Easy access to audio-visual content on social media, combined with the availability of modern tools such as Tensorflow or Keras, and open-source trained models, along with economical computing infrastructure, and the rapid evolution of deep-learning (DL) methods have heralded a new and frightening trend. Particularly, the advent of easily available and ready to use Generative Adversarial Networks (GANs), have made it possible to generate deepfakes media partially or completely fabricated with the intent to deceive to disseminate disinformation and revenge porn, to perpetrate financial frauds and other hoaxes, and to disrupt government functioning. Existing surveys have mainly focused on the detection of deepfake images and videos; this paper provides a comprehensive review and detailed analysis of existing tools and machine learning (ML) based approaches for deepfake generation, and the methodologies used to detect such manipulations in both audio and video. For each category of deepfake, we discuss information related to manipulation approaches, current public datasets, and key standards for the evaluation of the performance of deepfake detection techniques, along with their results. Additionally, we also discuss open challenges and enumerate future directions to guide researchers on issues which need to be considered in order to improve the domains of both deepfake generation and detection. This work is expected to assist readers in understanding how deepfakes are created and detected, along with their current limitations and where future research may lead.},
  archive      = {J_APIN},
  author       = {Masood, Momina and Nawaz, Mariam and Malik, Khalid Mahmood and Javed, Ali and Irtaza, Aun and Malik, Hafiz},
  doi          = {10.1007/s10489-022-03766-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3974-4026},
  shortjournal = {Appl. Intell.},
  title        = {Deepfakes generation and detection: State-of-the-art, open challenges, countermeasures, and way forward},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-local self-similarity recurrent neural network: Dataset
and study. <em>APIN</em>, <em>53</em>(4), 3963–3973. (<a
href="https://doi.org/10.1007/s10489-022-03616-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The images and videos of the high-voltage copper contact are disturbed by various noises in the factory. In this paper, an improved Non-local Self-similarity Recurrent Neural Network(NSRNN) is proposed for image denoising. The sparse representation is used for initializing the images, and then NSRNN is trained and tested based on the image datasets with different noise levels and magnification. Due to the similarity and the time correlation between the sequential images, RNN is used to improve the parameter utilization and model robustness. By measuring the self-similarity of the neighborhood features, NSRNN model outperforms other state-of-the-art methods in term of image denoising performance.},
  archive      = {J_APIN},
  author       = {Han, Lili and Wang, Yang and Chen, Mingshu and Huo, Jiaofei and Dang, Hongtao},
  doi          = {10.1007/s10489-022-03616-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3963-3973},
  shortjournal = {Appl. Intell.},
  title        = {Non-local self-similarity recurrent neural network: Dataset and study},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preference-corrected multimodal graph convolutional
recommendation network. <em>APIN</em>, <em>53</em>(4), 3947–3962. (<a
href="https://doi.org/10.1007/s10489-022-03681-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users purchase goods to form a user-item interaction graph, where goods are usually displayed in multiple modes. The main role of the recommendation system is to obtain users’ preferences after analyzing users’ purchasing behavior. However, there are some deviations in the interaction between users and items. For example, users buy goods they are not interested in, which directly affects the user’s preference analysis. Most existing models do not focus on analyzing and correcting user preference errors in the user-item relationship graph, which leads to errors. There is little work on the multimodal information of commodities, resulting in the loss of information. In this paper, a preference-corrected multimodal graph convolution recommendation network (PMGCRN) is proposed to provide multimodal recommendation services for users. First, a multichannel network is designed to obtain user preference information under different modes. Then, a positive attention mechanism is proposed to deal with the implicit noise edges that do not match users’ interactions in the user-item graph to correct errors in user preferences. Additionally, the simplified graph convolution network acts on the structural information of the user-item bipartite graph as an additional channel to enhance the robustness of the model. Finally, a self-attention mechanism and layer-by-layer superposition are applied to obtain multilevel modal and structural information, respectively, so that valuable information can be obtained by the fusion matrix. Experiments show that our proposed PMGCRN outperforms other baselines on all three datasets, MovieLens, Amazon - Sports and Outdoors, and Douban.},
  archive      = {J_APIN},
  author       = {Jia, Xiangen and Dong, Yihong and Zhu, Feng and Xin, Yu and Qian, Jiangbo},
  doi          = {10.1007/s10489-022-03681-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3947-3962},
  shortjournal = {Appl. Intell.},
  title        = {Preference-corrected multimodal graph convolutional recommendation network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised GAN with similarity constraint for mode
diversity. <em>APIN</em>, <em>53</em>(4), 3933–3946. (<a
href="https://doi.org/10.1007/s10489-022-03771-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mode collapse is a very common issue in Generative Adversarial Networks. To alleviate the mode collapse, we introduce a novel semi-supervised GAN-based generative model and propose a quantitative criterion to describe the degree of mode collapse. We design several schemes in the experiments to observe the effect of semi-supervised learning on mode collapse. In addition, the semi-supervised model can capture both supervised and unsupervised disentangled representation at the same time by introducing similarity constraint loss, so that generated image is higher-quality and more varied. The architecture leverages a few labels to control some factors on the class-conditional representation and captures other interpretable unsupervised representations with a large amount of unlabeled data. Both quantitative and visual results on the CIFAR-10 and SVHN datasets verify the ability of the proposed architecture.},
  archive      = {J_APIN},
  author       = {Li, Xiaoqiang and Luan, Yinxiang and Chen, Liangbo},
  doi          = {10.1007/s10489-022-03771-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3933-3946},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised GAN with similarity constraint for mode diversity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale patch-GAN with edge detection for image
inpainting. <em>APIN</em>, <em>53</em>(4), 3917–3932. (<a
href="https://doi.org/10.1007/s10489-022-03577-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting with large missing blocks is quite challenging to obtain visual consistency and realistic effect. In this paper, the multi-scale patch generative adversarial networks with edge detection image inpainting (MPGE) was proposed. Firstly, an edge detector was introduced into the generator of multi-scale generative adversarial networks (GAN) to guide the inpainting of the edge contour in the image inpainting, which improved the inpainting effect of image posture and expression. Secondly, we designed a patch-GAN as the local discriminant to capture high frequency, and a function L2-loss was utilized to keep the high resolution and style of the original image. Thirdly, a multi-head attention mechanism was introduced into the generator and local discriminator to build a multilevel and multi-dimensional dependent network model for image subspaces, which improved the global consistency of the inpainted image. Finally, by finding the minimum data set with similar network expression ability, we quickly obtained the optimal value of multi-head. Thereby, a lot of training time was saved. The experiments conducted on Celeba dataset proved that our proposed algorithm quantitatively and qualitatively outperformed the baselines.},
  archive      = {J_APIN},
  author       = {Chen, Gang and Zhang, Guipeng and Yang, Zhenguo and Liu, Wenyin},
  doi          = {10.1007/s10489-022-03577-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3917-3932},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale patch-GAN with edge detection for image inpainting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MinimalGAN: Diverse medical image synthesis for data
augmentation using minimal training data. <em>APIN</em>, <em>53</em>(4),
3899–3916. (<a
href="https://doi.org/10.1007/s10489-022-03609-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image synthesis techniques have limited application in the medical field due to unsatisfactory authenticity and precision. Additionally, synthesizing diverse outputs is challenging when the training data are insufficient, as in many medical datasets. In this work, we propose an image-to-image network named the Minimal Generative Adversarial Network (MinimalGAN), to synthesize annotated, accurate, and diverse medical images with minimal training data. The primary concept is to make full use of the internal information of the image and decouple the style from the content by separating them in the self-coding process. After that, the generator is compelled to concentrate on content detail and style separately to synthesize diverse and high-precision images. The proposed MinimalGAN includes two image synthesis techniques; the first is style transfer. We synthesized a stylized retinal fundus dataset. The style transfer deception rate is much higher than that of traditional style transfer methods. The blood vessel segmentation performance increased when only using synthetic data. The other image synthesis technique is target variation. Unlike the traditional translation, rotation, and scaling on the whole image, this approach only performs the above operations on the segmented target being annotated. Experiments demonstrate that segmentation performance improved after utilizing synthetic data.},
  archive      = {J_APIN},
  author       = {Zhang, Yipeng and Wang, Quan and Hu, Bingliang},
  doi          = {10.1007/s10489-022-03609-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3899-3916},
  shortjournal = {Appl. Intell.},
  title        = {MinimalGAN: Diverse medical image synthesis for data augmentation using minimal training data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SQLSketch-TVC: Type, value and compatibility based approach
for SQL queries. <em>APIN</em>, <em>53</em>(4), 3889–3898. (<a
href="https://doi.org/10.1007/s10489-022-03587-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the complexity of the translation of Natural Language (NL) sentences to SQL queries becomes an essential part in the resolution process. The majority of the proposed models either focus on simple queries or suffer when exposed to unseen domains or new schemas structures; This can be understood as the greater part of solutions are based on limited datasets or treat the problem in an end-to-end perspective. Our previously proposed model which is SQLSketch that provides an intelligent method for handling complex queries was able to outperform all the state-of-the-art models on the GreatSQL dataset. This paper addresses the problem of translating NL sentences to SQL queries in an effective way by leveraging our previous SQLSketch model with a type aware layer, a values classification method as well as a compatibility based module that enhance the quality of the predicted items (SQLSketch-TVC). We evaluate the new model using the Components and Exact matching metrics. The results show that SQLSketch-TVC outperforms the other models on all SQL components and provides a novel way for inferring values from the input Question.},
  archive      = {J_APIN},
  author       = {Ahkouk, Karam and Machkour, Mustapha},
  doi          = {10.1007/s10489-022-03587-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3889-3898},
  shortjournal = {Appl. Intell.},
  title        = {SQLSketch-TVC: Type, value and compatibility based approach for SQL queries},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised multitask learning using convolutional
autoencoder for faulty code detection with limited data. <em>APIN</em>,
<em>53</em>(4), 3877–3888. (<a
href="https://doi.org/10.1007/s10489-022-03663-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting faults in source code to fix is an important task in the software quality assurance. Building automated detectors using machine learning has been faced two big challenges of data imbalance and shortages. To address the issues, this paper proposes a deep neural network and training procedures to allow learning with limited annotated data. The network is composed of an unsupervised auto-encoder and a supervised classifier. The two components share some first layers that plays as a program feature extractor. Interestingly, we can leverage a large amount of unlabeled data from various sources to train the auto-encoder independently then transfer to the target domain. Additionally, sharing layers, and jointly training the reconstruction and the classification tasks stimulate the generation of the sophisticated features. We conducted the experiments on four real datasets with different amount of labeled data and with adding more unlabeled data. The results have confirmed that the multi-task outperforms the single-task and leveraging the unlabeled data is beneficial. Specifically, when reducing the labeled data from 100% to 75%, 50%, 25%, the performance of several deep networks drops sharply, while it reduces gradually for our model.},
  archive      = {J_APIN},
  author       = {Phan, Anh Viet and Nguyen, Khanh Duy Tung and Bui, Lam Thu},
  doi          = {10.1007/s10489-022-03663-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3877-3888},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised multitask learning using convolutional autoencoder for faulty code detection with limited data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised method for 3D human pose estimation with
consistent shape and viewpoint factorization. <em>APIN</em>,
<em>53</em>(4), 3864–3876. (<a
href="https://doi.org/10.1007/s10489-022-03714-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose estimation from monocular images has shown great success due to the sophisticated deep network architectures and large 3D human pose datasets. However, it is still an open problem when such datasets are unavailable. Estimating 3D human poses from monocular images is an ill-posed inverse problem. In our work, we propose a novel self-supervised method, which effectively trains a 3D human pose estimation network without any extra 3D pose annotations. Different from the commonly used GAN-based technique, our method overcomes the projection ambiguity problem by fully disentangling the camera viewpoint information from the 3D human shape. Specifically, we design a factorization network to predict the coefficients of canonical 3D human pose and camera viewpoint in two separate channels. Here, we represent the canonical 3D human pose as a combination of pose basis from a dictionary. To guarantee consistent factorization, we design a simple yet effective loss function taking advantage of multi-view information. Besides, in order to generate robust canonical reconstruction from the 3D pose coefficient, we exploit the underlying 3D geometry of human poses to learn a novel hierarchical dictionary from 2D poses. The hierarchical dictionary has stronger 3D pose expressibility than the traditional single-level dictionary. We comprehensively evaluate the proposed method on two public 3D human pose datasets, Human3.6M and MPI-INF-3DHP. The experimental results show that our method can maximally disentangle 3D human shapes and camera viewpoints, as well as reconstruct 3D human poses accurately. Moreover, our method achieves state-of-the-art results compared with recent weakly/self-supervised methods.},
  archive      = {J_APIN},
  author       = {Ma, Zhichao and Li, Kan and Li, Yang},
  doi          = {10.1007/s10489-022-03714-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3864-3876},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised method for 3D human pose estimation with consistent shape and viewpoint factorization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label driven latent subspace learning for multi-view
multi-label classification. <em>APIN</em>, <em>53</em>(4), 3850–3863.
(<a href="https://doi.org/10.1007/s10489-022-03600-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view multi-label learning (MVML), each instance is described by several heterogeneous feature representations and associated with multiple valid labels simultaneously. The key to learn from MVML data lies in how to seek a more discriminative latent subspace to exploit the consensus information across different views. In this paper, we propose a Label-Dependent Multi-view Multi-label method named M2LD, which incorporates the label information into the feature subspace to learn a more discriminative feature subspace for model induction. Specifically, we first construct a multi-view shared latent subspace across diverse views by matrix decomposition, and then the consistency relationship between labels and features is embedded to make the learned subspace label-dependent. In this way, we can preserve the local geometric structure while exploiting the consensus information of multi-view data, which leads the learned feature subspace be more discriminative. Finally, we induce the multi-view multi-label classifier by directly mapping the discriminative feature subspace to the label space. Extensive experiments on six real-world datasets indicate that our proposed M2LD can achieve superior or comparable performance against state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liu, Wei and Yuan, Jiazheng and Lyu, Gengyu and Feng, Songhe},
  doi          = {10.1007/s10489-022-03600-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3850-3863},
  shortjournal = {Appl. Intell.},
  title        = {Label driven latent subspace learning for multi-view multi-label classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust visual tracking for UAVs with dynamic feature weight
selection. <em>APIN</em>, <em>53</em>(4), 3836–3849. (<a
href="https://doi.org/10.1007/s10489-022-03719-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low reliability of feature and tracking box detection remains a problem in visual object tracking. Currently, most discriminative correlation filter (DCF) trackers integrate multiple types of features with fixed weights. However, these approaches cannot be adapted to complex scenes when fixed weights are used for features. In addition, the tracking box of traditional DCF trackers lacks scale and aspect ratio adaptability, which can inevitably lead to excessive background noise. To address these problems, we propose a robust tracking method for unmanned aerial vehicles (UAVs) using dynamic feature weight selection. Specifically, we define a feature weight pool that contains multiple weights for different features. In each frame, we select a weight combination with high reliability from the weight pool. This approach is a form of dynamic feature weight selection since the feature weight may be different for each frame. Furthermore, EdgeBoxes is combined with the DSST and can adapt well to the scale and aspect ratio of the tracking box. Extensive experiments based on UAV123@10fps, VisDrone2018-test-dev, and UAVDT show that our tracker is superior to other state-of-the-art trackers. It is noteworthy that the proposed dynamic feature weight selection method can be embedded into any tracking model using multiple features.},
  archive      = {J_APIN},
  author       = {An, Zhiyong and Wang, Xiumin and Li, Bo and Xiang, Zhongliang and Zhang, Bin},
  doi          = {10.1007/s10489-022-03719-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3836-3849},
  shortjournal = {Appl. Intell.},
  title        = {Robust visual tracking for UAVs with dynamic feature weight selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated fuzzy-grey relational analysis approach to
portfolio optimization. <em>APIN</em>, <em>53</em>(4), 3804–3835. (<a
href="https://doi.org/10.1007/s10489-022-03499-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper combines two approaches (Fuzzy set theory and Grey Relational Analysis) for modelling an investor’s imprecise linguistic expectations and the uncertain returns of assets. We propose a novel maximization-type risk measure capable of incorporating the investor’s individual preferences. The investor provides the expectations of what is considered the “ideal” return from the portfolio. We use Credibility theory to capture the investors’ subjective and imprecise expectations in a precise mathematical form. We construct a portfolio return sequence using the assets’ actual return data and an ideal sequence based on investors’ preferences. Subsequently, we calculate the Grey similitude and the closeness incidence degree between the two sequences. The closer the portfolio return is to the ideal return, the better. In this manner, we develop a new risk measure that can quantify an investor’s perception of risk. This measure is intuitive and easy to calculate. It does not involve estimating many parameters, something which would increase the estimation risk. We use a genetic algorithm to solve the resulting portfolio optimization model. We illustrate this method with two case studies: (i) a case study of 100 assets of the U.S. stock market’s NASDAQ-100 index and (ii) a case study of 50 assets of the Indian stock market’s NIFTY-50 index. We comprehensively analyze the model’s out-of-sample performance and discuss its implications. The portfolios obtained using the proposed approach exhibit healthy growth outside the in-sample period. We also compare the out-of-sample performance of the proposed model with several approaches in the literature to establish its superiority.},
  archive      = {J_APIN},
  author       = {Mehlawat, Mukesh Kumar and Gupta, Pankaj and Khan, Ahmad Zaman},
  doi          = {10.1007/s10489-022-03499-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3804-3835},
  shortjournal = {Appl. Intell.},
  title        = {An integrated fuzzy-grey relational analysis approach to portfolio optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph representations for the analysis of multi-agent
spatiotemporal sports data. <em>APIN</em>, <em>53</em>(4), 3783–3803.
(<a href="https://doi.org/10.1007/s10489-022-03631-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing tactical patterns in invasion games using multi-agent spatiotemporal data is a challenging task at the intersection of computer and sports science. A fundamental yet understudied problem in this area is finding an optimal data representation for processing athlete trajectories using machine learning algorithms. In the present work, we address this gap by discussing common representations in use and propose Tactical Graphs, an alternative graph-based format capable of producing integrative, contextualized models for machine learning applications. We provide an in-depth, domain-specific motivation of the proposed data representation scheme and show how this approach exploits inherent data traits. We propose Tactical Graph Networks (TGNets), a light-weight, hybrid machine learning architecture sensitive to player interactions. Our method is evaluated with an extensive ablation study and the first comprehensive state of the art comparison between standard feature, state vector, and image-based methods on the same dataset. Experiments were conducted using real-world football data containing short sequences of defensive play labelled according to the outcome of ball winning attempts. The results indicate that TGNets are on par with state-of-the-art deep learning models while exhibiting only a fraction of their complexity. We further demonstrate that selecting the right data representation is crucial as it has a significant influence on model performance. The theoretical findings and the proposed method provide insights and a strong methodological alternative for all classification, prediction or pattern recognition applications in the areas of collective movement analysis, automated match analysis, and performance analysis.},
  archive      = {J_APIN},
  author       = {Raabe, Dominik and Nabben, Reinhard and Memmert, Daniel},
  doi          = {10.1007/s10489-022-03631-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3783-3803},
  shortjournal = {Appl. Intell.},
  title        = {Graph representations for the analysis of multi-agent spatiotemporal sports data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical triple-level alignment for multiple source and
target domain adaptation. <em>APIN</em>, <em>53</em>(4), 3766–3782. (<a
href="https://doi.org/10.1007/s10489-022-03638-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to bridge the domain gap between the source and target domains. Most existing approaches concentrate on one target domain setting adapted from one or multiple source domains while neglecting the importance of multitarget domain setting. This inevitably causes a problem with suboptimal solutions in practical applications. To address this problem, we focus on a challenging but realistic scenario, unsupervised multisource-multitarget domain adaptation (UMDA), where multiple labeled source domains and multiple unlabeled target domains are available. In this paper, we propose a Hierarchical Triple-level Alignment (HTA) method for UMDA in which domain label, class label, and data structure information can be incorporated into a unified framework for effective knowledge transfer. The innovative points of this paper are as follows: 1) we devise a triple-level alignment mechanism including domain-level alignment, class-level alignment, and structure-level alignment, which effectively reduces the domain shift among multiple source and target domains; and 2) we develop a novel hierarchical gradient synchronization strategy to enhance class-level alignment, which can greatly reduce class distribution differences among multiple domains and preserve their individual class discrimination. Similarly, the hierarchical gradient synchronization strategy is also applied to structure-level alignment. As such, structure discrepancy reduction and individual structure preservation can both be achieved. To the best of our knowledge, HTA is the first attempt to simultaneously consider domain label, class label, and data structure information in the UMDA setting and can be regarded as a well-performing baseline for UMDA tasks. Experimental results on three standard benchmarks demonstrate the superiority of the proposed framework for multiple source-and-target domain adaptation.},
  archive      = {J_APIN},
  author       = {Wu, Zhuanghui and Meng, Min and Liang, Tianyou and Wu, Jigang},
  doi          = {10.1007/s10489-022-03638-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3766-3782},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical triple-level alignment for multiple source and target domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A q-learning approach to attribute reduction. <em>APIN</em>,
<em>53</em>(4), 3750–3765. (<a
href="https://doi.org/10.1007/s10489-022-03696-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is a paradigm covering both theories and techniques of selecting required attributes with constraints related to rough set. Currently, though various searching strategies have been developed for achieving such a purpose, few of them take the rewards of identifying attributes into account. In this study, inspired by the popular reinforcement learning mechanism, a Q-learning based procedure is designed to search qualified attributes and then construct the expected reduct. Specifically, state is regarded as the temporary result of selected attributes, action is regarded as variation of such a temporary result if random strategy is performed. Immediately, the reward can be obtained, which offers guidance on identifying attributes with the greatest reward. Moreover, considering the random factors emerge in our scheme, an ensemble device is also used to further improve classification performance of selected attributes in reduct. Finally, comprehensive experiments over a total of 15 UCI datasets clearly validates the superiorities of our study against 5 state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Liu, Yuxin and Gong, Zhice and Liu, Keyu and Xu, Suping and Ju, Hengrong and Yang, Xibei},
  doi          = {10.1007/s10489-022-03696-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3750-3765},
  shortjournal = {Appl. Intell.},
  title        = {A Q-learning approach to attribute reduction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end multi-domain neural networks with explicit
dropout for automated bone age assessment. <em>APIN</em>,
<em>53</em>(4), 3736–3749. (<a
href="https://doi.org/10.1007/s10489-022-03725-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pediatric skeletal bone age assessment (BAA) is a common clinical practice to diagnose endocrine and metabolic disorders in child development. Recently, several automated methods have been developed to assist the diagnosis. For most children, the chronological age will be close to the bone age. Besides, features of the left hand between males and females are different by using either Greulich and Pyle (G&amp;P) method or Tanner Whitehouse (TW) method. However, it is truly challenging to learn a unified representation based on the male and female image samples that have completely different characteristics. We argue that chronological age and gender are necessary pieces of information for automated BAA, and delve into the auxiliary of chronological age and gender for BAA. In this paper, a new multi-domain neural network (MD-BAA) is proposed to assess bone age of males and females in a separative and end-to-end manner. Furthermore, we introduce two regularization approaches to improve the network training: 1) an explicit dropout approach to select either the male domain or the female domain; 2) a chronological age preserving loss function to prevent the predicted bone age discrepant too much from the chronological age. Experimental results show the proposed method outperforms the state-of-the-art models on two datasets.},
  archive      = {J_APIN},
  author       = {Tang, He and Pei, Xiaobing and Li, Xinzhe and Tong, Haihui and Li, Xin and Huang, Shilong},
  doi          = {10.1007/s10489-022-03725-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3736-3749},
  shortjournal = {Appl. Intell.},
  title        = {End-to-end multi-domain neural networks with explicit dropout for automated bone age assessment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Incremental maintenance of three-way regions with
variations of objects and values in hybrid incomplete decision systems.
<em>APIN</em>, <em>53</em>(4), 3713–3735. (<a
href="https://doi.org/10.1007/s10489-022-03736-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, numerous mixed data are collected, which often leads to incomplete data due to human and nonhuman factors. The data in an actual information system often evolve dynamically with time. The modification of data values and the addition and deletion of objects are common dynamic variations in data sets, which usually occur simultaneously. With dynamic changes of data, static methods of knowledge acquisition require recalculated from scratch, resulting in repeated calculations and greater time consumption. Aiming to address the above multi-dimensional dynamic variations in data values and object sets under mixed incomplete data, this paper studies strategies and methods of dynamically updating three-way regions (viz. positive, negative and boundary regions) based on a matrix. First, for a hybrid incomplete decision system (HIDS), we develop a hybrid normalized distance metric for objects and present a matrix-based method of calculating positive, boundary and negative regions. For cases of modifying attribute values and adding and deleting object sets concurrently, the incremental mechanisms of the maintenance of three-way regions are researched for the multi-dimensional dynamic variations of HIDS; meanwhile, we develop a matrix-based incremental algorithm to update three-way regions. A series of experiments implemented on nine UCI data sets demonstrate that the proposed dynamic algorithm is superior to the static algorithm for handling the multi-dimensional dynamic variations of the HIDS.},
  archive      = {J_APIN},
  author       = {Yang, Chuanjian and Ge, Hao and Xu, Yi},
  doi          = {10.1007/s10489-022-03736-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3713-3735},
  shortjournal = {Appl. Intell.},
  title        = {Incremental maintenance of three-way regions with variations of objects and values in hybrid incomplete decision systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple kernel-based anchor graph coupled low-rank tensor
learning for incomplete multi-view clustering. <em>APIN</em>,
<em>53</em>(4), 3687–3712. (<a
href="https://doi.org/10.1007/s10489-022-03735-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMVC) attempts to give an optimal clustering solution for incomplete multi-view data that suffer from missing instances in certain views. However, most existing IMVC methods still have various drawbacks in practical applications, such as arbitrary incomplete scenarios cannot be handled; the computational cost is relatively high; most valuable nonlinear relations among samples are often ignored; complementary information among views is not sufficiently exploited. To address the above issues, in this paper, we present a novel and flexible unified graph learning framework, called Multiple Kernel-based Anchor Graph coupled low-rank Tensor learning for Incomplete Multi-View Clustering (MKAGT_IMVC), whose goal is to adaptively learn the optimal unified similarity matrix from all incomplete views. Specifically, according to the characteristics of incomplete multi-view data, MKAGT_IMVC innovatively improves an anchor selection strategy. Then, a novel cross-view anchor graph fusion mechanism is introduced to construct multiple fused complete anchor graphs, which captures more the intra-view and inter-view nonlinear relations. Moreover, a graph learning model combining low-rank tensor constraint and consensus graph constraint is developed, where all fused complete anchor graphs are regarded as prior knowledge to initialize this model. Extensive experiments conducted on eight incomplete multi-view datasets clearly show that our method delivers superior performance relative to some state-of-the-art methods in terms of clustering ability and time-consuming.},
  archive      = {J_APIN},
  author       = {Wang, Senhong and Cao, Jiangzhong and Lei, Fangyuan and Jiang, Jianjian and Dai, Qingyun and Ling, Bingo Wing-Kuen},
  doi          = {10.1007/s10489-022-03735-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3687-3712},
  shortjournal = {Appl. Intell.},
  title        = {Multiple kernel-based anchor graph coupled low-rank tensor learning for incomplete multi-view clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-term traffic forecasting based on adaptive graph cross
strided convolution network. <em>APIN</em>, <em>53</em>(4), 3672–3686.
(<a href="https://doi.org/10.1007/s10489-022-03739-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting aims to use historical information to predict future traffic values, to achieve the purpose of easing traffic pressure. However, most existing methods can not extract spatial-temporal features from historical data comprehensively and maintain high-accuracy forecasting in long-term forecasting continuously. In this paper, we design a adaptive graph cross strided convolution network (AGCSCN) for long-term traffic forecasting, which mainly includes two deep learning components: crossd stride convolution network (CSCN) for temporal features extraction and adaptive graph convolution network (AGCN) for spatial features extraction. CSCN component ensures that all the historical information can be perceived, and uses parallel cross convolution kernels to enhance long-term forecasting ability by reflecting the difference over forecasting horizons. AGCN component further learns the spatial correlation of period and trend segments respectively on the basis of global adaptive spatial features perception. The experimental results on four real-world traffic datasets show that the proposed AGCSCN model outperforms the state-of-art baselines and achieves optimal forecasting accuracy over all forecasting horizons.},
  archive      = {J_APIN},
  author       = {Li, Zhao and Zhang, Yong and Guo, Da and Zhou, Xu and Wang, Xing and Zhu, Lin},
  doi          = {10.1007/s10489-022-03739-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3672-3686},
  shortjournal = {Appl. Intell.},
  title        = {Long-term traffic forecasting based on adaptive graph cross strided convolution network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view knowledge graph fusion via knowledge-aware
attentional graph neural network. <em>APIN</em>, <em>53</em>(4),
3652–3671. (<a
href="https://doi.org/10.1007/s10489-022-03667-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) play a vital role in natural language processing (NLP), which can serve several downstream tasks. Because different views of KGs are usually constructed independently, the multi-view knowledge graph fusion (MVKGF) becomes a hotspot. Although multi-view learning studied very well in past decades, MVKGF is still not well tackled because of the heterogeneous relations and the multi-view KGs. To overcome MVKGF, entity alignment is the most studied. Existing entity alignment methods are dominated by embedding based methods, such as TransE and Graph Neural Networks (GNNs), where the alignment is achieved by measuring the similarities between entity embeddings. However, most previous approaches suffer from the issues of the diverse knowledge facts and the complex neighboring structures. In this paper, we propose a novel K nowledge-aware A ttentional G raph N eural N etwork (KAGNN) model to carefully incorporate both knowledge facts and neighboring structures. In particular, a knowledge-aware attention mechanism is designed to preserve the original semantics and determine the importance of each knowledge fact. Furthermore, a three-layered GCN with highway gates is adopted to learn better entity representations from the neighboring structure information. Thus, our model can be regarded as a multi-view extension of GNN. We validate our model on three cross-lingual datasets and the results show our model beats the state-of-the-art baselines by a large margin.},
  archive      = {J_APIN},
  author       = {Huang, Zhichao and Li, Xutao and Ye, Yunming and Zhang, Baoquan and Xu, Guangning and Gan, Wensheng},
  doi          = {10.1007/s10489-022-03667-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3652-3671},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view knowledge graph fusion via knowledge-aware attentional graph neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unpaired multi-modal tumor segmentation with structure
adaptation. <em>APIN</em>, <em>53</em>(4), 3639–3651. (<a
href="https://doi.org/10.1007/s10489-022-03610-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal learning of unpaired anatomical and functional modalities is challenging because the large discrepancies between feature representations, and the absence of registration, hinder deeper cross-modality information extraction. To address this issue, we propose a novel unpaired multi-modal learning framework for lung tumor segmentation in anatomical and functional images (i.e., T2w-MRI and DWI-MRI). The framework consists of two models: a multi-modal segmentation network (Segmentor) and an adversarial learning-based structure adaptation (SA) model. The Segmentor contains a modality-specific encoder for intensity offset reduction and a shared decoder for cross-modality information fusion. The SA model helps the Segmentor obtain high-level features with similar distributions from different modalities through adversarial training. To address the modality imbalance problem caused by the large gap in the number of training samples and image characteristics, we introduce a harmonic constraint term derived from two weighted dice losses to maintain the balance of multi-modal training. Experimental results based on a clinical dataset of 326 T2w-MRI and 112 DWI-MRI scans show that the proposed framework improves segmentation performance compared with several state-of-the-art unpaired multi-modal segmentation methods.},
  archive      = {J_APIN},
  author       = {Zhou, Pei and Chen, Houjin and Li, Yanfeng and Peng, Yahui},
  doi          = {10.1007/s10489-022-03610-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {4},
  pages        = {3639-3651},
  shortjournal = {Appl. Intell.},
  title        = {Unpaired multi-modal tumor segmentation with structure adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Index migration directed by lattice reduction for feature
data fusion. <em>APIN</em>, <em>53</em>(3), 3291–3303. (<a
href="https://doi.org/10.1007/s10489-022-03588-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the opinion of data representation, feature data fusion is a process of transforming the redundant source representation into the concise object representation by removing redundant data from source feature data. Based on the structured lattice representation of source feature data, this paper addresses the transformation of data representation by reducing the quantum representations of lattice nodes, and then proposes the fusion method based on lattice reduction directed index migration. This method classifies all lattice nodes into different node subsets through the gradual migration of the indexes of the qubits in different lattice nodes. The source lattice nodes in a subset will be fused into a new object node based on their measurement probabilities. The experimental data evaluation results demonstrate that the proposed fusion method can obtain concise and reliable fusion results for intelligent decision-making.},
  archive      = {J_APIN},
  author       = {Peng, Weimin and Chen, Aihong and Chen, Jing and Xu, Haitao},
  doi          = {10.1007/s10489-022-03588-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3291-3303},
  shortjournal = {Appl. Intell.},
  title        = {Index migration directed by lattice reduction for feature data fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMRAN: A novel model for finger vein recognition based on a
residual attention mechanism. <em>APIN</em>, <em>53</em>(3), 3273–3290.
(<a href="https://doi.org/10.1007/s10489-022-03645-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finger vein recognition is an advanced biometric recognition technology that offers high precision and high security. It recognizes or authenticates individuals using irradiating vein texture images collected from fingers with near-infrared light. In this paper, we propose a new finger vein recognition model (MMRAN) based on a multiscale and multistage residual attention network. First, to fully adapt to the low-resolution, grayscale pixels, and linear patterns of finger vein images, we designed an architecture that combines a fusion residual attention block (FRAB) and a multistage residual attention connection (MRAC). The FRAB contains two distinct subpaths: the main vein path (MVP) and the guided attention path (GAP). The MVP extracts finger vein features at multiple scales by using a multibranch residual structure, while the GAP uses an hourglass network to generate weight maps to guide the setting of eigenvalues at corresponding locations in the main vein feature map. MRAC integrates venous features extracted at different learning stages through the above two pathways. The proposed multiscale and multistage extraction model is effective at extracting various types of digital vein features, including those whose shapes change in width, direction, curvature, and so on. We combine the various dimensions of vein features through multistage learning to further improve the model performance for extracting high-level abstract features. To evaluate the performance of our proposed model, we conducted a large number of experiments on five publicly available finger vein datasets. The experimental results show that the proposed model not only achieves a recognition accuracy above 98%, which is an improvement compared to the current state-of-the-art methods, but it can also be implemented with fewer parameters, which improves training and inference.},
  archive      = {J_APIN},
  author       = {Liu, Weiye and Lu, Huimin and Wang, Yifan and Li, Yupeng and Qu, Zhenshen and Li, Yang},
  doi          = {10.1007/s10489-022-03645-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3273-3290},
  shortjournal = {Appl. Intell.},
  title        = {MMRAN: A novel model for finger vein recognition based on a residual attention mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A texture detail-oriented generative adversarial network:
Motion deblurring for multi-textured images. <em>APIN</em>,
<em>53</em>(3), 3255–3272. (<a
href="https://doi.org/10.1007/s10489-022-03628-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to motion deblurring in multi-textured images is to extract more accurate feature information from the original input images. Recent studies on the design of feature extraction models have demonstrated the remarkable effectiveness of hierarchically representing multi-scale features to improve model performance. Nevertheless, these approaches generally neglect the size of the receptive field in each layer, which is essential for ensuring the full exchange of information. This paper proposes two feature extraction methods based on lightweight networks that can be flexibly plugged into networks. In addition, a novel model is formed by embedding lighter and more effective feature extraction methods (e.g., our symmetrical depthwise convolution feature extraction block (SDB) and multi-scale iterative channel feature extraction block (MIB)) into a generative adversarial network (GAN), and we name this model the texture detail-oriented GAN (TDGAN). To make the generated image closer to the target image at the visual level, we also integrate the perceptual style loss and the structural similarity loss into the generator loss function. Extensive experiments conducted on GoPro and AssIMG datasets demonstrate that the proposed model outperforms the state-of-the-art methods in terms of accuracy and computational complexity.},
  archive      = {J_APIN},
  author       = {Zhang, Xiao and Chen, Ming and Zhang, Zhengqin and Lu, Shenglian},
  doi          = {10.1007/s10489-022-03628-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3255-3272},
  shortjournal = {Appl. Intell.},
  title        = {A texture detail-oriented generative adversarial network: Motion deblurring for multi-textured images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based residual autoencoder for video anomaly
detection. <em>APIN</em>, <em>53</em>(3), 3240–3254. (<a
href="https://doi.org/10.1007/s10489-022-03613-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic anomaly detection is a crucial task in video surveillance system intensively used for public safety and others. The present system adopts a spatial branch and a temporal branch in a unified network that exploits both spatial and temporal information effectively. The network has a residual autoencoder architecture, consisting of a deep convolutional neural network-based encoder and a multi-stage channel attention-based decoder, trained in an unsupervised manner. The temporal shift method is used for exploiting the temporal feature, whereas the contextual dependency is extracted by channel attention modules. System performance is evaluated using three standard benchmark datasets. Result suggests that our network outperforms the state-of-the-art methods, achieving 97.4% for UCSD Ped2, 86.7% for CUHK Avenue, and 73.6% for ShanghaiTech dataset in term of Area Under Curve, respectively.},
  archive      = {J_APIN},
  author       = {Le, Viet-Tuan and Kim, Yong-Guk},
  doi          = {10.1007/s10489-022-03613-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3240-3254},
  shortjournal = {Appl. Intell.},
  title        = {Attention-based residual autoencoder for video anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grid-DPC: Improved density peaks clustering based on spatial
grid walk. <em>APIN</em>, <em>53</em>(3), 3221–3239. (<a
href="https://doi.org/10.1007/s10489-022-03705-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering methods need to find the initial centers first. A reasonable cluster center can improve the efficiency and accuracy of the algorithm. However, finding centers is not an easy task. It often needs much calculation and easily falls into local optimal points. In allusion to the problem, an improved density peaks clustering algorithm based on spatial grid walk (Grid-DPC) is proposed. Grid-DPC uses a spatial grid walk method to determine the initial cluster centers, avoiding the traditional method requiring multiple iterations to optimize the centers and preventing falling into the local optimal. In terms of density definition, Grid-DPC uses grid density instead of the traditional method, which reduces the time cost of finding neighbors. In terms of the random walk strategy, the adjacent higher grid weight direction and Lévy Flight based step length method are adopted to improve the convergence speed of the algorithm effectively. Aiming at the phenomenon of “target loss” that may occur in random walks, the variable neighborhood search method is used to help the algorithm find high-weight grids and prolong the number of steps per walk as long as possible. Simulation experiments show that the algorithm can effectively find clustering centers, complete the task of clustering arbitrarily distributed data, and has high efficiency for massive data.},
  archive      = {J_APIN},
  author       = {Liang, Bo and Cai, JiangHui and Yang, HaiFeng},
  doi          = {10.1007/s10489-022-03705-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3221-3239},
  shortjournal = {Appl. Intell.},
  title        = {Grid-DPC: Improved density peaks clustering based on spatial grid walk},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving aspect term extraction via span-level tag data
augmentation. <em>APIN</em>, <em>53</em>(3), 3207–3220. (<a
href="https://doi.org/10.1007/s10489-022-03558-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect term extraction (ATE), a fundamental subtask in aspect-based sentiment analysis, aims to extract explicit aspect term from reviewers’ expressed opinions. However, the distribution of samples containing different numbers of aspect terms is long-tailed. Due to the scarcity of long-tailed samples and the existence of multiple variable-length aspect terms inside each sample, most ATE models converge to an inferior state because they have difficulty capturing features. Popular data augmentation techniques used for addressing this problem, such as synonym replacement and back translation, cannot produce substantial improvements when using pretrained language models. In this paper, we present a novel span-level aspect term extraction (SATE) framework, which includes three main components: a simple and effective tag data augmentation (TaDA) module, an original pretrained language model, and an optimized heuristic decoding algorithm module. TaDA is based on a span-level tagging scheme and generates new pseudo training samples for long-tailed multiaspect samples. The pretrained model, deemed a general feature extractor, yields contextual token representations. Then, the decoding algorithm adopts an adjustment factor to extract the variable-length aspect terms simultaneously. All the techniques are seamlessly integrated into the stacked SATE framework to pinpoint the aspect terms. Empirical experiments on SemEval benchmark datasets of multiple domains achieve F1-scores of 86.92% and 86.28% for laptops and restaurants, respectively, demonstrating the superiority of our model compared with the well-known baseline models.},
  archive      = {J_APIN},
  author       = {Liu, Bin and Lin, Tao and Li, Ming},
  doi          = {10.1007/s10489-022-03558-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3207-3220},
  shortjournal = {Appl. Intell.},
  title        = {Improving aspect term extraction via span-level tag data augmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust anomaly-based intrusion detection system for
in-vehicle network by graph neural network framework. <em>APIN</em>,
<em>53</em>(3), 3183–3206. (<a
href="https://doi.org/10.1007/s10489-022-03412-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet of Vehicles (IoVs) techniques, many emerging technologies and their applications are integrated with IoVs. The application of these new technologies requires vehicles to communicate with external networks frequently, which makes the in-vehicle network more vulnerable to hacker attacks. It is imperative to detect hacker attacks on in-vehicle networks. A control area network graph attention networks (CAN-GAT) model is proposed to implement the anomaly detection of in-vehicle networks, and a graph neural network (GNN) anomaly-based detection framework using graph convolution, graph attention and CAN-GAT network model for in-vehicle network based on CAN bus is presented. In this detection framework, a graph is designed with the traffic on the CAN bus to capture the correlation between the change of the traffic bytes and the state of other traffic bytes effectively and help improve the detection accuracy and efficiency. Compared simulation experiments are conducted to test the proposed model, and the obtained model performance metrics results show that the CAN-GAT-2 model based on two-layer CAN-GAT achieves better performance. In addition, the visualization and quantitative analysis methods are used to explain how can the attention mechanism of CAN-GAT-2 improve the performance, which can help to construct better GNNs in anomaly detection of in-vehicle network. The model performance evaluation results show that CAN-GAT-2 achieved improved accuracy among the compared baseline methods, and has good detection speed performance.},
  archive      = {J_APIN},
  author       = {Xiao, Junchao and Yang, Lin and Zhong, Fuli and Chen, Hongbo and Li, Xiangxue},
  doi          = {10.1007/s10489-022-03412-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3183-3206},
  shortjournal = {Appl. Intell.},
  title        = {Robust anomaly-based intrusion detection system for in-vehicle network by graph neural network framework},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A brick-up model for recombining metaheuristic optimisation
algorithm using analytic hierarchy process. <em>APIN</em>,
<em>53</em>(3), 3166–3182. (<a
href="https://doi.org/10.1007/s10489-022-03586-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most swarm intelligence algorithms are stochastic metaheuristic algorithms in nature, and thus they may not solve all optimisation problems perfectly. Different algorithms may have different advantages, and the different real cases should be analysed independently. In this paper, a new brick-up re- building method for metaheuristic algorithms is proposed and discussed. This brick-up method creatively separates the metaheuristic algorithms into components (bricks) and generate a brick pool for further use. Then a new and best fitting algorithm will be generated custom-made to different problem and suggested to user as the best solution available in metaheuristic design. The main contributions for this research are the metaheuristic brick selection rules analysis and brick-up system model simulation. The proposed model has been tested on CEC 2015 benchmark function sets to verify its performance. The experimental results show that this recombination model can produce a metaheuristic algorithm that is as efficient as each individual candidate algorithm or better.},
  archive      = {J_APIN},
  author       = {Song, Qun and Li, Tengyue and Fong, Simon and Liu, Shuang},
  doi          = {10.1007/s10489-022-03586-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3166-3182},
  shortjournal = {Appl. Intell.},
  title        = {A brick-up model for recombining metaheuristic optimisation algorithm using analytic hierarchy process},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault diagnosis model of rolling bearing based on parameter
adaptive AVMD algorithm. <em>APIN</em>, <em>53</em>(3), 3150–3165. (<a
href="https://doi.org/10.1007/s10489-022-03562-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the intrinsic aspect that the weak features of the early fault information of rolling bearings are not easy to extract, a parameter Adaptive Variational Modal Decomposition (AVMD) based algorithm is proposed for bearing fault signal feature extraction. Since the number of Variational Modal Decomposition (VMD) decomposition and penalty factor play an important role in VMD decomposition effect, the irregularities in the selection of these two influencing parameters are analyzed. We exploit the stronger global search capability of the improved sparrow search algorithm (LSSA) for adaptive parameter selection of the VMD algorithm. In this paper, the Levy flight algorithm is introduced and chaos is added to initialize sparrow population position to prevent sparrow from falling into the disadvantage of local optimum in the search process. In addition, this paper also combines the maximum kurtosis index, the minimum envelope entropy index and the number of iterations of VMD to form the objective function of the LSSA. The VMD algorithm with optimized parameters decomposes the signal to be measured, the decomposed IMFs was reconstructed, finally the validity of the model was verified by calculating 20 features (time domain and frequency domain) of the reconstructed signal as the input vector of the SVM classifier. Finally, the feasibility of this model in fault diagnosis of rolling bearing is verified using simulation and example experiments.},
  archive      = {J_APIN},
  author       = {Li, Meixuan and Yan, Chun and Liu, Wei and Liu, Xinhong and Zhang, Mengchao and Xue, Jiankai},
  doi          = {10.1007/s10489-022-03562-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3150-3165},
  shortjournal = {Appl. Intell.},
  title        = {Fault diagnosis model of rolling bearing based on parameter adaptive AVMD algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rumor detection on social media using hierarchically
aggregated feature via graph neural networks. <em>APIN</em>,
<em>53</em>(3), 3136–3149. (<a
href="https://doi.org/10.1007/s10489-022-03592-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet and big data, online social media platforms have been developing rapidly, which accelerate rumors circulation. Rumor detection on social media is a worldwide challenging task due to rumor’s feature of high speed, fragmental information and extensive range. Most existing approaches identify rumors based on single-layered hybrid features like word features, sentiment features and user characteristics, or multimodal features like the combination of text features and image features. Some researchers adopted the hierarchical structure, but they neither used rumor propagation nor made full use of its retweet posts. In this paper, we propose a novel model for rumor detection based on Graph Neural Networks (GNN), named Hierarchically Aggregated Graph Neural Networks (HAGNN). This task focuses on capturing different granularities of high-level representations of text content and fusing the rumor propagation structure. It applies a Graph Convolutional Network (GCN) with a graph of rumor propagation to learn the text-granularity representations with the spreading of events. A GNN model with a document graph is employed to update aggregated features of both word and text granularity, it helps to form final representations of events to detect rumors. Experiments on two real-world datasets demonstrate the superiority of the proposed method over the baseline methods. Our model achieves the accuracy of 95.7% and 88.2% on the Weibo dataset Ma et al. 2017 and the CED dataset Song et al. IEEE Trans Knowl Data Eng 33(8):3035–3047, 2019respectively.},
  archive      = {J_APIN},
  author       = {Xu, Shouzhi and Liu, Xiaodi and Ma, Kai and Dong, Fangmin and Riskhan, Basheer and Xiang, Shunzhi and Bing, Changsong},
  doi          = {10.1007/s10489-022-03592-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3136-3149},
  shortjournal = {Appl. Intell.},
  title        = {Rumor detection on social media using hierarchically aggregated feature via graph neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation patterns extraction from high-dimensional climate
data with complicated multi-variables using deep neural networks.
<em>APIN</em>, <em>53</em>(3), 3124–3135. (<a
href="https://doi.org/10.1007/s10489-022-03737-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate data consists of multiple high-dimensional time series and multiple-dimensional space series with unknown series. These unknown series in climate data hide the complex co-variation relation patterns. By exploring these co-variation relation patterns, we can further reveal the complex representations between time series and space series in climate data. Therefore, it is a tough task to explore what kinds of relation patterns from high-dimensional climate data containing unknown complicated multi-variables. To address this, we proposed neural networks with three layers according to Brenier’s theorem. Brenier’s theorem rigorously proves that the data distribution in the background space is consistent with the data distribution in the reconstructed feature space with greatest probability, thereby ensuring that the relation patterns extracted by the proposed model are as close as possible to the original relation patterns. For the three series sets (i.e., a time series set, a spatial series set containing longitude, and a spatial series set containing latitude) in the climate dataset, we adopted the compact coding manner that one layer encodes a series set correspondingly, in order to maintain the consistency between a time series and two spatial series. Results on the ECMWF climate datasets show that the proposed method gains deeper relation patterns than competitors, i.e., the relation patterns captured by our method outperforms competitors in terms of regularity (for spatial series) and periodicity (for time series). We demonstrate that by this compact coding manner, neural networks capture deeper relation patterns from high-dimensional data containing complicated multiple variables due to this coding manner can accurately filter out more non-eigenvalue information from those complicated data.},
  archive      = {J_APIN},
  author       = {Zheng, Jian and Wang, Qingling and Liu, Cong and Wang, Jianfeng and Liu, Hongling and Li, Jiang},
  doi          = {10.1007/s10489-022-03737-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3124-3135},
  shortjournal = {Appl. Intell.},
  title        = {Relation patterns extraction from high-dimensional climate data with complicated multi-variables using deep neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive capsule network for implicit sentiment analysis.
<em>APIN</em>, <em>53</em>(3), 3109–3123. (<a
href="https://doi.org/10.1007/s10489-022-03584-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing sentiment analysis models mainly rely on evident emotive words within phrases. When the apparent emotional words within phrases are eliminated, the performance of these models will inevitably decrease. The implicit communication of emotion without the use of explicit emotional phrases is highly widespread in several cultures. As a result, a classification model is required to learn the link between contexts and the emotions they trigger in an automatic way. Based on whether the sentence should be segmented at the keyword position, existing methods apply either segmented or nonsegmented approaches. When emotional words are removed from a sentence, the nonsegmented approaches may lose syntactic information. To address these issues, an interactive iapsule network was proposed in this paper to extend the segmented approach. Taking the keyword as the segmented position, the network initializes two BERT models from a pretrained checkpoint with shared parameters as the encoder to process both contexts separately. By using both interactive attention and the capsule network with a dynamic routing algorithm, the model can automatically learn the insightful relationship between the former and the latter contexts. After fusing the former and latter context features, the interactive capsule network leverages both local and global attention to complete the sentiment analysis task. Experimental results on both English and Chinese corpora show that the proposed interactive attention model achieves a better performance than existing methods during implicit sentiment analysis tasks. In addition, the proposed model outperformed the top 3 models on WASSA-2018 implicit English shared tasks.},
  archive      = {J_APIN},
  author       = {Qian, Yanjun and Wang, Jin and Li, Dawei and Zhang, Xuejie},
  doi          = {10.1007/s10489-022-03584-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3109-3123},
  shortjournal = {Appl. Intell.},
  title        = {Interactive capsule network for implicit sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute self-representation steered by exclusive lasso for
zero-shot learning. <em>APIN</em>, <em>53</em>(3), 3095–3108. (<a
href="https://doi.org/10.1007/s10489-022-03497-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize new unseen classes by transferring knowledge from seen classes, which assumes that both seen and unseen classes share a common semantic space. Mainstream ZSL methods focus on learning visual-semantic projection by one-vs-all strategy in which the class-level attribute vector is matched with many visual features. However, human-annotated attributes have some limitations: 1) The attribute semantics are subjective to be manually annotated, which could lead to inaccuracy. 2) Since human-annotated attributes may focus on details, some attributes are weakly associated with the object and contribute little information to recognition. Meanwhile, most ZSL methods also suffer from the severe problem of domain shift problem. Thus, in this paper, we propose a novel ZSL model to address the above problems. Specifically, we introduce the attribute relabeling approach to rectify the original attributes by exploiting the intrinsic attribute correlation information. Then, we employ the property of group-wise competition of exclusive lasso to encourage different categories to compete for prominent category-specific attributes, respectively, which benefits learning visual-semantic projection. Besides, our model integrates the encoder-decoder paradigm to alleviate the projection shift problem. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed model. In particular, in the setting of generalized ZSL, our proposed model has higher unseen class accuracy and thus achieves the highest harmonic mean value.},
  archive      = {J_APIN},
  author       = {Mi, Jian-Xun and Zhang, Zhonghao and Tai, Debao and Zhou, Li-Fang},
  doi          = {10.1007/s10489-022-03497-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3095-3108},
  shortjournal = {Appl. Intell.},
  title        = {Attribute self-representation steered by exclusive lasso for zero-shot learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advanced defensive distillation with ensemble voting and
noisy logits. <em>APIN</em>, <em>53</em>(3), 3069–3094. (<a
href="https://doi.org/10.1007/s10489-022-03495-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we provide an approach for deep neural networks that protects against adversarial examples in image classification-type problems. blackUnlike adversarial training, our approach is independent to the obtained adversarial examples through min-max optimization. The approach relies on the defensive distillation mechanism. This defence mechanism, while very successful at the time, was defeated in less than a year due to a major intrinsic vulnerability: the availability of the neural network’s logit layer to the attacker. We overcome this vulnerability and enhance defensive distillation by two mechanisms: 1) a mechanism to hide the logit layer (noisy logit) which increases robustness at the expense of accuracy, and, 2) a mechanism that improves accuracy but does not always increase robustness (ensemble network). We show that by combining the two mechanisms and incorporating a voting method, we can provide protection against adversarial examples while retaining accuracy. We formulate potential attacks on our approach with different threat models. The experimental results demonstrate the effectiveness of our approach. We also provide a robustness guarantee along with an interpretation for the guarantee.},
  archive      = {J_APIN},
  author       = {Liang, Yuting and Samavi, Reza},
  doi          = {10.1007/s10489-022-03495-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3069-3094},
  shortjournal = {Appl. Intell.},
  title        = {Advanced defensive distillation with ensemble voting and noisy logits},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified model free dynamic programming: An augmented
approach for unmanned aerial vehicle. <em>APIN</em>, <em>53</em>(3),
3048–3068. (<a
href="https://doi.org/10.1007/s10489-022-03510-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design complexities of trending UAVs necessitates formulation of C ontrol L aws that are both robust and model-free besides being self-capable of handling the evolving dynamic environments. In this research, a unique intelligent control architecture is presented which aims at maximizing the glide range of an experimental UAV having unconventional controls. To handle control complexities, while keeping them computationally acceptable, a distinct RL technique namely Modified Model Free Dynamic Programming (MMDP) is proposed. The methodology is novel as RL based Dynamic Programming algorithm has been specifically modified to configure the problem in continuous state and control space domains without knowledge of the underline UAV model dynamics. Major challenge during the research was the development of a suitable reward function which helps in achieving the desired objective of maximising the glide performance. The efficacy of the results and performance characteristics, demonstrated the ability of the presented algorithm to dynamically adapt to the changing environment, thereby making it suitable for UAV applications. Non-linear simulations performed under different environmental and varying initial conditions demonstrated the effectiveness of the proposed methodology over the conventional classical approaches.},
  archive      = {J_APIN},
  author       = {Din, Adnan Fayyaz Ud and Akhtar, Suhail and Maqsood, Adnan and Habib, Muzaffar and Mir, Imran},
  doi          = {10.1007/s10489-022-03510-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3048-3068},
  shortjournal = {Appl. Intell.},
  title        = {Modified model free dynamic programming: An augmented approach for unmanned aerial vehicle},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise-residual mixup for unsupervised adversarial domain
adaptation. <em>APIN</em>, <em>53</em>(3), 3034–3047. (<a
href="https://doi.org/10.1007/s10489-022-03709-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) methods based on deep adversarial learning are successful for many practical fields. The deep adversarial UDA methods can promote knowledge transfer by learning domain invariant features. However, these UDA methods have the following problems. The inter-domain information in shared latent space between different domains can not be fully considered. And some low-level feature information of deep neural network is usually lost after multiple convolutions and layer by layer training. We propose noise-residual mixup for unsupervised adversarial domain adaptation (NMADA) to solve these problems in UDA methods based on deep adversarial learning. Our method NMADA is designed with two strategies: one is mixup linearly interpolation, this is the first time that noise mixup is incorporated into UDA. This strategy can enrich the cross domain feature information, further explore the inter-domain information in shared latent space and reduce the domain shift. The other strategy is the noise residual module. As a module that connects different convolutional layers of neural network, it can combine noise to make full use of different level feature information and consider intrisnic feature structure of different levels for better domain adaptation. In our method, the feature-level consideration from different levels and cross-domain sides can make better use of intra-domain content and inter-domain information. Compared with the mainstream methods, NMADA jointly considers different level feature information and richer cross domain information to improve robustness and performance of models. Experiments on unsupervised domain adaptation benchmark datasets validate the effectiveness and superiority of our approach.},
  archive      = {J_APIN},
  author       = {He, Chunmei and Tan, Taifeng and Fan, Xianjun and Zheng, Lanqing and Ye, Zhengchun},
  doi          = {10.1007/s10489-022-03709-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3034-3047},
  shortjournal = {Appl. Intell.},
  title        = {Noise-residual mixup for unsupervised adversarial domain adaptation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning label-specific features with global and local label
correlation for multi-label classification. <em>APIN</em>,
<em>53</em>(3), 3017–3033. (<a
href="https://doi.org/10.1007/s10489-022-03386-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label algorithms often use an identical feature space to build classification models for all labels. However, labels generally express different semantic information and should have their own characteristics. A few algorithms have been proposed to find label-specific features to construct discriminative classification models. Some use global label correlation to make the reconstructed features more discriminative, but they usually neglect the local correlation between labels. To solve this problem, we propose a new algorithm, named learning Label-specific Features with Global and Local label Correlation (LFGLC). The algorithm integrates both global and local label correlation to extract label-specific features for each label. Specifically, global label correlation is calculated by the label co-occurrence frequency between label pairs, and local label correlation is learned from the neighborhood of each instance. Comprehensive experiments on 12 multi-label data sets clearly manifest that the proposed algorithm performs competitively in feature selection and multi-label classification.},
  archive      = {J_APIN},
  author       = {Weng, Wei and Wei, Bowen and Ke, Wen and Fan, Yuling and Wang, Jinbo and Li, Yuwen},
  doi          = {10.1007/s10489-022-03386-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3017-3033},
  shortjournal = {Appl. Intell.},
  title        = {Learning label-specific features with global and local label correlation for multi-label classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded parallel crowd counting network with
multi-resolution collaborative representation. <em>APIN</em>,
<em>53</em>(3), 3002–3016. (<a
href="https://doi.org/10.1007/s10489-022-03639-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the size and density distribution of a crowd from images is of great importance to public safety and crowd management during the COVID-19 pandemic, but it is very challenging as it is affected by many complex factors, including perspective distortion and background noise information. In this paper, we propose a novel multi-resolution collaborative representation framework called the cascaded parallel network (CP-Net), consisting of three parallel scale-specific branches connected in a cascading mode. In the framework, the three cascaded multi-resolution branches efficiently capture multi-scale features through their specific receptive fields. Additionally, multi-level feature fusion and information filtering are performed continuously on each branch to resist noise interference and perspective distortion. Moreover, we design an information exchange module across independent branches to refine the features extracted by each specific branch and deal with perspective distortion by using complementary information of multiple resolutions. To further improve the robustness of the network to scale variance and generate high-quality density maps, we construct a multi-receptive field fusion module to aggregate multi-scale features more comprehensively. The performance of our proposed CP-Net is verified on the challenging counting datasets (UCF_CC_50, UCF-QNRF, Shanghai Tech A&amp;B, and WorldExpo’10), and the experimental results demonstrate the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Lyu, Lei and Han, Run and Chen, Ziming},
  doi          = {10.1007/s10489-022-03639-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3002-3016},
  shortjournal = {Appl. Intell.},
  title        = {Cascaded parallel crowd counting network with multi-resolution collaborative representation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A modified multifactorial differential evolution algorithm
with optima-based transformation. <em>APIN</em>, <em>53</em>(3),
2989–3001. (<a
href="https://doi.org/10.1007/s10489-022-03537-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifactorial evolution algorithm (MFEA) is a powerful search paradigm with the purpose of addressing multiple optimization tasks simultaneously in the field of evolutionary computation. The assortative mating of MFEA is a key component to make it outperform traditional single-task optimization algorithms. However, the optimal solution of each generation has still not been well utilized to accelerate convergence in the process of assortative mating operation for existing MFEAs. This paper proposes a multifactorial differential evolution algorithm with optima-based transformation (MFDE-OBT), which employs the optimal solution of each generation to design an improved assortative mating operation based on the DE/rand/2 mutation. The improved operation generates an offspring for each individual by running a perturbation around a current optimal individual. The object of the assortative mating is a vector added for the perturbation, which is the sum of two difference vectors generated by a random sample with a certain probability from an individual set that can be same as or different from the one of the task involving the optimal individual. In addition, MFDE-OBT integrates an opposition-based search strategy behind the assortative mating operation to balance exploitation and exploration of each search space. Experimental results on benchmark problems constituted by tasks with different degree of similarity and intersection demonstrate the advantage of the proposed MFDE-OBT algorithm over some state-of-the-art algorithms, in terms of solution precision and convergence performance.},
  archive      = {J_APIN},
  author       = {Shi, Lingyi and Hu, Zhongbo and Su, Qinghua and Miao, Yongfei},
  doi          = {10.1007/s10489-022-03537-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2989-3001},
  shortjournal = {Appl. Intell.},
  title        = {A modified multifactorial differential evolution algorithm with optima-based transformation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards designing a generic and comprehensive deep
reinforcement learning framework. <em>APIN</em>, <em>53</em>(3),
2967–2988. (<a
href="https://doi.org/10.1007/s10489-022-03550-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has emerged as an effective approach for building an intelligent system, which involves multiple self-operated agents to collectively accomplish a designated task. More importantly, there has been a renewed focus on RL since the introduction of deep learning that essentially makes RL feasible to operate in high-dimensional environments. However, there are many diversified research directions in the current literature, such as multi-agent and multi-objective learning, and human-machine interactions. Therefore, in this paper, we propose a comprehensive software architecture that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. For this reason, we design a deep RL-based framework that strictly ensures flexibility, robustness, and scalability. To enforce generalization, the proposed architecture also does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents.},
  archive      = {J_APIN},
  author       = {Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Pham, Nhat Truong and Nguyen, Hai and Nguyen, Dang Tu and Nguyen, Thanh Dang and Lim, Chee Peng and Johnstone, Michael and Bhatti, Asim and Creighton, Douglas and Nahavandi, Saeid},
  doi          = {10.1007/s10489-022-03550-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2967-2988},
  shortjournal = {Appl. Intell.},
  title        = {Towards designing a generic and comprehensive deep reinforcement learning framework},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrepant mutual learning fusion network for unsupervised
domain adaptation on person re-identification. <em>APIN</em>,
<em>53</em>(3), 2951–2966. (<a
href="https://doi.org/10.1007/s10489-022-03532-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods mainly assign pseudo labels by clustering algorithms to solve the problems in domain adaptive pedestrian re-identification caused by unlabeled target domain data and scene style differences between the source and the target domains. But clustering algorithms with single network may generate incorrect noisy pseudo labels which affects the effectiveness of domain adaptation. To address this problem, the dual-branch teacher-student networks uses two networks with almost the same structures to assign soft pseudo labels. However, the soft pseudo labels assigned are largely the same because of the similar structure of the dual networks, which may lead to the same result with single-network training. Therefore, this paper proposes the discrepant mutual learning fusion network to improve the performance of unsupervised domain adaptive person re-identification by both increasing the difference between dual networks and enhancing their feature expressiveness. Firstly, this paper proposes the discrepant dual-branch network (DDNet) to mine the global and local features of the network by constructing two branches with different depths, and constructs the feature random scaling (FRS) module to further enhance the diversity of the extracted feature. Secondly, the feature fusion (FF) module is built to fuse the discrepant features generated by DDNet, and achieve mutual supervise learning of the fusion classification results, which enhances the ability of network feature expression. Experiments show that the proposed method outperforms most classical domain adaptive methods in recognition accuracy.},
  archive      = {J_APIN},
  author       = {Yun, Xiao and Wang, Qunqun and Cheng, Xiaozhou and Song, Kaili and Sun, Yanjing},
  doi          = {10.1007/s10489-022-03532-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2951-2966},
  shortjournal = {Appl. Intell.},
  title        = {Discrepant mutual learning fusion network for unsupervised domain adaptation on person re-identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deformable image registration with attention-guided fusion
of multi-scale deformation fields. <em>APIN</em>, <em>53</em>(3),
2936–2950. (<a
href="https://doi.org/10.1007/s10489-022-03659-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable medical image registration plays a crucial role in theoretical research and clinical application. Traditional methods suffer from low registration accuracy and efficiency. Recent deep learning-based methods have made significant progresses, especially those weakly supervised by anatomical segmentations. However, the performance still needs further improvement, especially for images with large deformations. This work proposes a novel deformable image registration method based on an attention-guided fusion of multi-scale deformation fields. Specifically, we adopt a separately trained segmentation network to segment the regions of interest to remove the interference from the uninterested areas. Then, we construct a novel dense registration network to predict the deformation fields of multiple scales and combine them for final registration through an attention-weighted field fusion process. The proposed contour loss and image structural similarity index (SSIM) based loss further enhance the model training through regularization. Compared to the state-of-the-art methods on three benchmark datasets, our method has achieved significant performance improvement in terms of the average Dice similarity score (DSC), Hausdorff distance (HD), Average symmetric surface distance (ASSD), and Jacobian coefficient (JAC). For example, the improvements on the SHEN dataset are 0.014, 5.134, 0.559, and 359.936, respectively.},
  archive      = {J_APIN},
  author       = {He, Zhiquan and He, Yupeng and Cao, Wenming},
  doi          = {10.1007/s10489-022-03659-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2936-2950},
  shortjournal = {Appl. Intell.},
  title        = {Deformable image registration with attention-guided fusion of multi-scale deformation fields},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly-supervised object localization with gradient-pyramid
feature. <em>APIN</em>, <em>53</em>(3), 2923–2935. (<a
href="https://doi.org/10.1007/s10489-022-03686-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a basic task of computer vision task, object localization plays an important role in many computer vision based applications. Supervised methods employ manual location labels to learn to localize the objects directly, but incomplete or incorrectly assigned location labels affect localization accuracy, and the cost of manual labelling should also be extremely large. This paper proposes a weakly-supervised localization method based on a multi-scale gradient-pyramid feature, which employs the weighted gradient features on the multiple convolutional layers in order to generate a gradient-pyramid feature for object localization. Pairs of gradients and features from different layers are first extracted to compute the gradient features. Then, during the fusion of the gradient features through a pyramid model, the larger value is selected as the result of the fusion task without using the concatenated method. Finally, the multi-scale gradient-pyramid feature is obtained and used to have a more accurate object localization by using the region scaling operation. Our proposed method can be directly integrated into the pre-trained classification model to perform object localization without additional training. Experimental results on the ILSVRC 2016 dataset and CUB-200-2011 dataset show that the proposed method can achieve better object localization performance.},
  archive      = {J_APIN},
  author       = {Mao, Zhongjie and Zhou, Yipeng and Sun, Jun and Wu, Hao and Pan, Feng and Ahmad, Bilal},
  doi          = {10.1007/s10489-022-03686-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2923-2935},
  shortjournal = {Appl. Intell.},
  title        = {Weakly-supervised object localization with gradient-pyramid feature},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A greedy search based evolutionary algorithm for electric
vehicle routing problem. <em>APIN</em>, <em>53</em>(3), 2908–2922. (<a
href="https://doi.org/10.1007/s10489-022-03555-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, there have been many variations of the Vehicle Routing Problem created to fit the actual needs of society, one of which is the Electric Vehicle Routing Problem (EVRP). EVRP is a more complex and challenging combinatorial optimization than the conventional vehicle routing problem. This paper considers a specific model for the tram routing problem and proposes a clustering-inspired greedy search algorithm GS. GS algorithm aims to cluster charging routes and greedily search charging stations for the optimal route output. In this paper, we purposely implement GS into a meta-heuristic genetic algorithm GA to utilize GA’s finding a globally optimal, leading to the formulation of the GSGA algorithm. To evaluate performance, we use a benchmark dataset found in the CEC-12 Tram Routing Problem CEC-12 Competition at the World Congress on Computational Intelligence (WCCI) 2020. The experiment evaluates GS’s effectiveness when applied to other algorithms such as genetic algorithms and simulated annealing. The experiments results show that our proposed algorithm has better solution quality than previous algorithms.},
  archive      = {J_APIN},
  author       = {Hien, Vu Quoc and Dao, Tran Cong and Binh, Huynh Thi Thanh},
  doi          = {10.1007/s10489-022-03555-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2908-2922},
  shortjournal = {Appl. Intell.},
  title        = {A greedy search based evolutionary algorithm for electric vehicle routing problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing face detection in video sequences by video
segmentation preprocessing. <em>APIN</em>, <em>53</em>(3), 2897–2907.
(<a href="https://doi.org/10.1007/s10489-022-03608-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, some learning-based methods are proposed to detect and locate humans in real-time via convolutional neural networks (CNN). However, high-performance graphics processing units (GPUs) are required in those methods. To resolve this problem, a preprocessing procedure based on video segmentation is proposed to speed up face detection. Meanwhile, an accelerating toolkit is employed in this study to perform face detection in real-time on a standard central processing unit (CPU). Experimental results indicate that the proposed method can achieve an F1-Score of 93.2% and 4.5 times of real-time speed with one CPU on 155883 test frames from the RAI dataset, YouTube, and YOUKU. Notably, when the video sequence is with fewer frames of human faces, the highest speed is nearly 18 times faster than that without video segmentation.},
  archive      = {J_APIN},
  author       = {Liu, Huibin and Fan, Zuoxun and Chen, Qiang and Zhang, Xiaomei},
  doi          = {10.1007/s10489-022-03608-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2897-2907},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing face detection in video sequences by video segmentation preprocessing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient power set mapping space blocking algorithm for
sensor selection in uncertain systems with quantified diagnosability
requirements. <em>APIN</em>, <em>53</em>(3), 2879–2896. (<a
href="https://doi.org/10.1007/s10489-022-03290-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For uncertain systems, the impacts of model uncertainties and measurement noise are considered during the modeling process. The distinguishability that is derived from the Kullback-Leibler divergence quantifies fault diagnosability for uncertain systems. Selecting a set of sensors that fulfill the quantified diagnosability requirements with the lowest cost is a significant step of fault detection and isolation in uncertain systems. In this paper, we propose a two-form power set mapping space blocking algorithm(PMSBA) for this problem, which includes three main strategies: stochastic search that is motivated by an inclusion relation strategy (SSMIRS) ,the BILP-based search space blocking strategy (BSSBS) and the best from multiple selections as measured by overlapping space strategy(BMSMOSS). SSMIRS, which is motivated by inclusion relations, is an efficient strategy for searching for local optimal solutions. BSSBS is used to block the spatialce area that is unnecessary to explore. BMSMOSS considers the overlapping space between each blocking, and can only be applied in the complete form of PMSBA. By modifying the parameter N, PMSBA can be transformed from an incomplete algorithm to a complete algorithm. The incomplete form of PMSBA is suitable for large-scale systems. The complete form is suitable mainly for small-scale or medium-scale systems and can find the optimal solution. To evaluate the performance of PMSBA, experiments are performed on four uncertain system instances, namely,two theoretical use cases and two practical use cases. The experimental results show that, compared with a state-of-the-art algorithm, namely, the incomplete form of PMSBA obtains superior solutions and performs better in terms of efficiency. The complete form of PMSBA is the first complete algorithm to be proposed for sensor selection in uncertain systems. It provides great effectiveness advantages over the depth-first search algorithm for most feasible solution spaces.},
  archive      = {J_APIN},
  author       = {Sun, Rui and Ouyang, Dantong and Tian, Xinliang and Zhang, Liming},
  doi          = {10.1007/s10489-022-03290-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2879-2896},
  shortjournal = {Appl. Intell.},
  title        = {An efficient power set mapping space blocking algorithm for sensor selection in uncertain systems with quantified diagnosability requirements},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PTP-STGCN: Pedestrian trajectory prediction based on a
spatio-temporal graph convolutional neural network. <em>APIN</em>,
<em>53</em>(3), 2862–2878. (<a
href="https://doi.org/10.1007/s10489-022-03524-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is the prerequisite to ensure the safety of road users in traffic scenes for the application of autonomous vehicles. Pedestrians are the main participants in traffic scenes, and reasonable inference and prediction of their future trajectories are crucial for autonomous driving technology and road safety. Pedestrian trajectories are highly dynamic, apparently random, and complex to interact with traffic environment agents; therefore, selective modeling of spatial interaction and temporal dependence of pedestrians is necessary. To address this challenge, this paper proposes a novel pedestrian trajectory prediction model based on a spatio-temporal graph convolutional network (PTP-STGCN). Specifically, a new crowd interaction attention (CIA) function is defined to quantify the interaction information between adjacent pedestrians better. This function captures the spatial interaction features of pedestrians at each time step by a spatial graph convolution network (S-GCN). Meanwhile, the time-series motion features of each pedestrian are extracted by a temporal transformer network (T-transformer), and a spatio-temporal interaction graph of pedestrian features is constructed by the STGCN composed of the S-GCN and T-transformer. Finally, a time-extrapolator convolutional neural network (TXP-CNN) is used to predict pedestrian trajectories in the time dimension of the STGCN. The experimental results on the ETH and UCY datasets show that the proposed model achieves better performance than the state-of-the-art baselines regarding the average displacement error (ADE) and final displacement error (FDE). Due to the excellent performance in pedestrian trajectory prediction, the proposed network achieves more predictive final planned trajectory of an autonomous vehicle, while avoiding unnecessary trajectory changes and collision risk, thus providing a promising solution for practical pedestrian trajectory prediction applications.},
  archive      = {J_APIN},
  author       = {Lian, Jing and Ren, Weiwei and Li, Linhui and Zhou, Yafu and Zhou, Bin},
  doi          = {10.1007/s10489-022-03524-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2862-2878},
  shortjournal = {Appl. Intell.},
  title        = {PTP-STGCN: Pedestrian trajectory prediction based on a spatio-temporal graph convolutional neural network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). What is wrong with deep knowledge tracing? Attention-based
knowledge tracing. <em>APIN</em>, <em>53</em>(3), 2850–2861. (<a
href="https://doi.org/10.1007/s10489-022-03621-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientifically and effectively tracking student knowledge states is a significant and fundamental task in personalized education. Many neural network-based models, e.g., deep knowledge tracing (DKT), have achieved remarkable results on knowledge tracing. DKT does not require handcrafted knowledge and can capture more complex representations of student knowledge. However, a severe problem of DKT is that the output fluctuates wildly. In this paper, we utilize a finite state automaton (FSA), a mathematical computation model, to interpret the waviness of DKT because an FSA has observable state evolution in response to external input. With the support of an FSA, we discover that DKT cannot handle long sequential inputs, which leads to unstable predictions. Accordingly, we introduce two novel attention-based models that solve the above problems by directly capturing the relationships among each item of the input sequence. Extensive experimentation on five well-known datasets shows that our two proposed models achieve state-of-the-art performance compared to existing knowledge tracing approaches.},
  archive      = {J_APIN},
  author       = {Wang, Xianqing and Zheng, Zetao and Zhu, Jia and Yu, Weihao},
  doi          = {10.1007/s10489-022-03621-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2850-2861},
  shortjournal = {Appl. Intell.},
  title        = {What is wrong with deep knowledge tracing? attention-based knowledge tracing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A water cycle algorithm based on quadratic interpolation for
high-dimensional global optimization problems. <em>APIN</em>,
<em>53</em>(3), 2825–2849. (<a
href="https://doi.org/10.1007/s10489-022-03428-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The water cycle algorithm (WCA) is easily trapped in local optimal solutions when dealing with high-dimensional optimization problems and has low precision and slow convergence. A WCA based on quadratic interpolation (QIWCA) is proposed in this study to address these drawbacks. First, a new nonlinear adjustment strategy for distance control parameters is designed to balance the exploration and exploitation capabilities of the algorithm. Second, during the search process of the algorithm, mutation operations are probabilistically performed to enhance the global exploration capability of the algorithm. Lastly, the quadratic interpolation operator is introduced to improve the local exploitation capability of the algorithm. QIWCA is also compared with several of the most advanced meta-heuristic algorithms on 46 benchmark functions. Experimental results show that QIWCA outperforms the compared algorithms in terms of convergence speed, global exploration capability, solution accuracy, and reliability.},
  archive      = {J_APIN},
  author       = {Ye, Jiahao and Xie, Lirong and Wang, Hongwei},
  doi          = {10.1007/s10489-022-03428-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2825-2849},
  shortjournal = {Appl. Intell.},
  title        = {A water cycle algorithm based on quadratic interpolation for high-dimensional global optimization problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the anti-occlusion ability of correlation
filter-based trackers via segmentation. <em>APIN</em>, <em>53</em>(3),
2815–2824. (<a
href="https://doi.org/10.1007/s10489-021-03058-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, correlation filter (CF)-based trackers have undergone rapid development and achieved state-of-the-art performance. However, CF-based trackers lack the ability to perceive the local variation of the target, such as occlusion, because they rely on the features from a bounding box to distinguish the target. In contrast, segmentation-based trackers, which distinguish the target based on pixel- or superpixel-level information, can sufficiently perceive local variation. However, their robustness is inferior to that of CF methods due to the lack of high-level semantic information. In this research, the advantages of both methods were combined to improve the anti-occlusion ability of CF trackers. The image segmentation-based occlusion estimation agency (ISBOEA) method is proposed to perceive occlusions, after which the occlusion information is used to guide the training and searching of CF trackers. In experiments conducted in this study, two CF-based trackers, namely the background-aware CF (BACF) and the efficient convolution operators: hand-crafted feature version (ECO_HC), were employed as baselines for tracking. Moreover, the minimum barrier distance (MBD) transform method was employed to conduct image segmentation. Extensive experiments were performed on standard benchmark datasets, namely OTB50, OTB100, GOT-10k and VIVID. The results demonstrate that the proposed ISBOEA method can remarkably improve the anti-occlusion ability of CF-based trackers.},
  archive      = {J_APIN},
  author       = {Jiang, Kaiwen and Yan, Lei and Zhang, Zihan and Zhao, Hongying},
  doi          = {10.1007/s10489-021-03058-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2815-2824},
  shortjournal = {Appl. Intell.},
  title        = {Improving the anti-occlusion ability of correlation filter-based trackers via segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying dominant emotional state using handwriting and
drawing samples by fusing features. <em>APIN</em>, <em>53</em>(3),
2798–2814. (<a
href="https://doi.org/10.1007/s10489-022-03552-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expertise to quickly and non-invasively determine a subject’s emotional state can contribute to a milestone in research on emotionally intelligent computing systems. The ability to identify emotions via everyday activities such as writing and drawing is beneficial to one’s well-being. Tablet devices and other human-machine interfaces have made collecting handwriting and drawing samples simpler. To understand more about writing and drawing signals there is a need to investigate them in the temporal, spectral, and cepstral domains for discovering new insights. Extracting more information will help to improve classification accuracy. This study combines temporal, spectral, and Mel Frequency Cepstral Coefficient (MFCC) methods to extract features from such signals, and finds its correlation with depression, anxiety, and stress emotional state of the humans. To examine spatial features, velocities are also computed as variations of displacement in the x- and y-directions while performing the tasks. Bidirectional Long-Short Term Memory (BiLSTM) network is used to classify the generated features’ vectors. To evaluate the proposed work, multiple publically available benchmark datasets are utilized. This work determine which activities and features help describe a specific emotional state through in-depth investigation. The results of the experiments demonstrate that fusing several features improve recognition accuracy significantly. For emotions identification like depression, anxiety, and stress states, this work achieved a higher classification improvement ranging from 5.32% to 8.9% as compared to the baseline approaches.},
  archive      = {J_APIN},
  author       = {Rahman, Atta Ur and Halim, Zahid},
  doi          = {10.1007/s10489-022-03552-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2798-2814},
  shortjournal = {Appl. Intell.},
  title        = {Identifying dominant emotional state using handwriting and drawing samples by fusing features},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-machine co-intelligence through symbiosis in the SMV
space. <em>APIN</em>, <em>53</em>(3), 2777–2797. (<a
href="https://doi.org/10.1007/s10489-022-03574-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a rapidly-growing research agenda that explores the combined, integrated, and collective intelligence of humans and machines working together as a team. This paper contributes to the same line of research with three main objectives: a) to introduce the concept of the SMV (Symbols-Meaning-Value) space for describing, understanding, and representing human/machine perception, cognition, and action, b) to revisit the notion of human-machine symbiosis, and c) to outline a conceptual framework of human-machine co-intelligence (i.e., the third intelligence) through human-machine symbiosis in the SMV space. By following the principle of three-way decision as thinking in threes, triads of three things are used for building an easy-to-understand, simple-to-remember, and practical-to-use framework. The three elements of the SMV space, namely, Symbols, Meaning, and Value, are closely related to the three basic human/machine functions of perception, cognition, and action, which can be metaphorically described as the seeing-knowing-doing triad or concretely interpreted as the data-knowledge-wisdom (DKW) hierarchy. Human-machine co-intelligence emerges from human-machine symbiosis in the SMV space. As the third intelligence, human-machine co-intelligence relies on and combines human intelligence and machine intelligence, is a higher level of intelligence above either human intelligence or machine intelligence alone, and is greater than the sum of human intelligence and machine intelligence. There are three basic principles of human-machine symbiosis, i.e., unified oneness, division of labor, and coevolution, for nurturing human-machine co-intelligence.},
  archive      = {J_APIN},
  author       = {Yao, Yiyu},
  doi          = {10.1007/s10489-022-03574-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2777-2797},
  shortjournal = {Appl. Intell.},
  title        = {Human-machine co-intelligence through symbiosis in the SMV space},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal biometric decision fusion security technique to
evade immoral social networking sites for minors. <em>APIN</em>,
<em>53</em>(3), 2751–2776. (<a
href="https://doi.org/10.1007/s10489-022-03538-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Social Networks (OSNs) are quickly becoming the most popular platform, with practically everyone using them on a daily basis. Recent advancements in neosocial networking sites have sparked interest among people, particularly young people. In this paper, a unique method that provides access control for minors is proposed. The biometric features collected from users are verified using the individual’s Unique Identity (UID). To address this issue, numerous multimodal authentication systems have been devised that partially solve the problem, however challenges in classifier selection and decision level fusion in a multimodal biometric system are still being investigated. Hence, the Multimodal Biometric Authentication System Utilizing Game Theoretic Social Network Analysis (GTSNA) is designed as a one-of-a-kind technique for identifying minors by verifying their age using the UID repository. As a result, two systems are built: one to authenticate a user with a Genuine Acceptance Rate (GAR) of 100% and a False Acceptance Rate (FAR) of 1%, and the other to govern minors accessing immoral information on the web using this newly designed Multimodal system.},
  archive      = {J_APIN},
  author       = {Shalini P and Shankaraiah},
  doi          = {10.1007/s10489-022-03538-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2751-2776},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal biometric decision fusion security technique to evade immoral social networking sites for minors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A temporal and channel-combined attention block for action
segmentation. <em>APIN</em>, <em>53</em>(3), 2738–2750. (<a
href="https://doi.org/10.1007/s10489-022-03569-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of video action segmentation is to classify an untrimmed long video at the frame level. With the requirement of processing long-term feature sequences containing much information, many computing units and auxiliary-training structures are required. The redundant information in these features can interfere with classification inference. It is an effective feature optimization mechanism to distinguish between useful and useless information by adjusting weight distribution. Such a method can adaptively calibrate complex features without increasing many calculations and improve frame-wise classification performance. Therefore, this study proposes a temporal and channel-combined attention block (TCB) that can be used for temporal sequences. It combines the attention of temporal and channel dimensions to reasonably assign weights to features. TCB contains two submodules: multi-scale temporal attention (MTA) and channel attention (CHA). MTA can adapt to different action instances with varying durations in a video using multilayer dilated convolution to capture multi-scale temporal relations and generate frame-wise attention weights. CHA captures the dependencies between channels and generates channel-wise attention weights to selectively increase the weights of important features. We combined the two attention modules to form a two-dimensional attention mechanism to improve action segmentation performance. We inserted TCB on boundary-aware cascade networks for simulation testing. The results show that our attention mechanism can improve action segmentation performance. In the three action segmentation datasets GTEA, 50Salads, and Breakfast, the accuracy Acc increased by an average of 1.4%, the Edit score increased by an average of 2.1%, and the F1 score increased by an average of approximately 2.1%.},
  archive      = {J_APIN},
  author       = {Yang, Dawei and Cao, Zhe and Mao, Lin and Zhang, Rubo},
  doi          = {10.1007/s10489-022-03569-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2738-2750},
  shortjournal = {Appl. Intell.},
  title        = {A temporal and channel-combined attention block for action segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salient and consensus representation learning based
incomplete multiview clustering. <em>APIN</em>, <em>53</em>(3),
2723–2737. (<a
href="https://doi.org/10.1007/s10489-022-03530-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview learning has a great potential to achieve a better performance than the conventional single-view based methods. However, some views of multiview data may be missing in the real-world applications. To cluster such incomplete multiview data, researchers tend to fill in the absent instances with the average vector of the available data for each view or adopt the pre-defined graphs from different views to learn a consensus representation. These approaches neither achieve the real distribution of the data nor explore the salient structure for each view influenced by noise. To address these problems, we propose a salient and consensus representation learning based method for incomplete multiview clustering (SCR_IMC). Firstly, we adaptively learn the salient structures of the collected data from different views adopting the self-representation property of the data. Then, the salient structures from different views are translated into the complete sub-spaces by a set of pre-defined projection matrices. Finally, a salient and consensus representation reflecting the common structure across different views is learned for the final clustering task. Experimental results on several real-world databases illustrate that the proposed method achieves competitive performance in comparison with the other classical and state-of-the-art methods, which proves the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Zhao, Shuping and Cui, Zhongwei and Wu, Lian and Xu, Yong and Zuo, Yu and Fei, Lunke},
  doi          = {10.1007/s10489-022-03530-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2723-2737},
  shortjournal = {Appl. Intell.},
  title        = {Salient and consensus representation learning based incomplete multiview clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhance understanding and reasoning ability for image
captioning. <em>APIN</em>, <em>53</em>(3), 2706–2722. (<a
href="https://doi.org/10.1007/s10489-022-03624-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to generate a grammatically correct and semantically accurate natural language description of a given image. To better capture the complex information contained in an image and expand the relevant external knowledge outside the image to generate a better image caption, this paper proposes an end-to-end image captioning framework named Enhance Understanding and Reasoning Ability for Image Captioning (EURAIC) based on the Transformer model. EURAIC provides an enhanced visual understanding ability and caption reasoning ability to improve image captioning performance. To achieve this goal, we use the semantic features of the core objects detected from the image to guide the visual features, which incorporate information on the spatial position relationships between the objects. Then, we introduce an external knowledge network to obtain information other than the inherent content of the image. In this way, a high-quality image caption sentence can be generated for the given image. Experiments on the MSCOCO dataset prove that our method is superior to the baseline model and comparable to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wei, Jiahui and Li, Zhixin and Zhu, Jianwei and Ma, Huifang},
  doi          = {10.1007/s10489-022-03624-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2706-2722},
  shortjournal = {Appl. Intell.},
  title        = {Enhance understanding and reasoning ability for image captioning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining expert-based beliefs and answer sets.
<em>APIN</em>, <em>53</em>(3), 2694–2705. (<a
href="https://doi.org/10.1007/s10489-022-03669-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answer Set Programming (ASP) is a declarative knowledge representation language that uses a non-monotonic reasoning mechanism to search for all answer sets or models of a specific problem. This makes it suitable for problem-solving activities, such as expertise, where there is lack of knowledge, and where defeasible reasoning is required. However, this language is not equipped with a means to select a preferred model among its answer sets as done by experts in expertise processes. Clearly, in expertise processes, experts who have acquired knowledge from their experience will express possible explanations and based on their beliefs and reasoning, will select the most appropriate ones for the problem. To have the best of both ASP and human expert knowledge in expertise process activities, we propose and illustrate a general and domain-independent framework that extends ASP using experts’ knowledge and belief functions to systematically draw explanations for expertise activities. This extension provides a means to evaluate ASP models’ beliefs using experts’ evidence distributions, while reducing the knowledge-intensive load of the expertise process.},
  archive      = {J_APIN},
  author       = {Sounchio, Serge Sonfack and Geneste, Laurent and Foguem, Bernard Kamsu},
  doi          = {10.1007/s10489-022-03669-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2694-2705},
  shortjournal = {Appl. Intell.},
  title        = {Combining expert-based beliefs and answer sets},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TERMS: Textual emotion recognition in multidimensional
space. <em>APIN</em>, <em>53</em>(3), 2673–2693. (<a
href="https://doi.org/10.1007/s10489-022-03567-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microblogs generate a vast amount of data in which users express their emotions regarding almost all aspects of everyday life. Capturing affective content from these context-dependent and subjective texts is a challenging task. We propose an intelligent probabilistic model for textual emotion recognition in multidimensional space (TERMS) that captures the subjective emotional boundaries and contextual information embedded in a text for robust emotion recognition. It is implausible with discrete label assignment;therefore, the model employs a soft assignment by mapping varying emotional perceptions in a multidimensional space and generates them as distributions via the Gaussian mixture model (GMM). To strengthen emotion distributions, TERMS integrates a probabilistic emotion classifier that captures the contextual and linguistic information from texts. The integration of these aspects, the context-aware emotion classifier and the learned GMM parameters provide a complete coverage for accurate emotion recognition. The large-scale experimentation shows that compared to baseline and state-of-the-art models, TERMS achieved better performance in terms of distinguishability, prediction, and classification performance. In addition, TERMS provide insights on emotion classes, the annotation patterns, and the models application in different scenarios.},
  archive      = {J_APIN},
  author       = {Ghafoor, Yusra and Jinping, Shi and Calderon, Fernando H. and Huang, Yen-Hao and Chen, Kuan-Ta and Chen, Yi-Shin},
  doi          = {10.1007/s10489-022-03567-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2673-2693},
  shortjournal = {Appl. Intell.},
  title        = {TERMS: Textual emotion recognition in multidimensional space},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Person re-identification via semi-supervised adaptive graph
embedding. <em>APIN</em>, <em>53</em>(3), 2656–2672. (<a
href="https://doi.org/10.1007/s10489-022-03570-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video surveillance is an indispensable part of the smart city for public safety and security. Person Re-Identification (Re-ID), as one of elementary learning tasks for video surveillance, is to track and identify a given pedestrian in a multi-camera scene. In general, most existing methods has firstly adopted a CNN based detector to obtain the cropped pedestrian image, it then aims to learn a specific distance metric for retrieval. However, unlabeled gallery images are generally overlooked and not utilized in the training. On the other hands, Manifold Embedding (ME) has well been applied to Person Re-ID as it is good to characterize the geometry of database associated with the query data. However, ME has its limitation to be scalable to large-scale data due to the huge computational complexity for graph construction and ranking. To handle this problem, we in this paper propose a novel scalable manifold embedding approach for Person Re-ID task. The new method is to incorporate both graph weight construction and manifold regularized term in the same framework. The graph we developed is discriminative and doubly-stochastic so that the side information has been considered so that it can enhance the clustering performances. The doubly-stochastic property can also guarantee the graph is highly robust and less sensitive to the parameters. Meriting from such a graph, we then incorporate the graph construction, the subspace learning method in the unified loss term. Therefore, the subspace results can be utilized into the graph construction, and the updated graph can in turn incorporate discriminative information for graph embedding. Extensive simulations is conducted based on three benchmark Person Re-ID datasets and the results verify that the proposed method can achieve better ranking performance compared with other state-of-the-art graph-based methods.},
  archive      = {J_APIN},
  author       = {Liu, Jiao and Lin, Mingquan and Zhao, Mingbo and Zhan, Choujun and Li, Bing and Chui, John Kwok Tai},
  doi          = {10.1007/s10489-022-03570-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2656-2672},
  shortjournal = {Appl. Intell.},
  title        = {Person re-identification via semi-supervised adaptive graph embedding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample hardness guided softmax loss for face recognition.
<em>APIN</em>, <em>53</em>(3), 2640–2655. (<a
href="https://doi.org/10.1007/s10489-022-03504-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition (FR) has received remarkable attention for improving feature discrimination with the development of deep convolutional neural networks (CNNs). Although the existing methods have achieved great success in designing margin-based loss functions by using hard sample mining strategy, they still suffer from two issues: 1) the neglect of some training status and feature position information and 2) inaccurate weight assignment for hard samples due to the coarse hardness description. To solve these issues, we develop a novel loss function, namely Hardness Loss, to adaptively assign weights for the misclassified (hard) samples guided by their corresponding hardness, which accounts for multiple training status and feature position information. Specifically, we propose an estimator to provide the real-time training status to precisely compute the hardness for weight assignment. To the best of our knowledge, this is the first attempt to design a loss function by using multiple pieces of information about the training status and feature positions. Extensive experiments on popular face benchmarks demonstrate that the proposed method is superior to the state-of-the-art (SOTA) losses under various FR scenarios.},
  archive      = {J_APIN},
  author       = {Sun, Zhengzheng and Tian, Lianfang and Du, Qiliang and Bhutto, Jameel A.},
  doi          = {10.1007/s10489-022-03504-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2640-2655},
  shortjournal = {Appl. Intell.},
  title        = {Sample hardness guided softmax loss for face recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An event-based automatic annotation method for datasets of
interpersonal relation extraction. <em>APIN</em>, <em>53</em>(3),
2629–2639. (<a
href="https://doi.org/10.1007/s10489-022-03547-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision (DS) has been widely used in automatic annotation of interpersonal relation datasets recently. However, DS suffers from high cost of constructing knowledge sets, low generalization ability and cannot effectively handle missing or adaptive relationships. To address these issues, we propose a novel event-based automatic annotation method for interpersonal datasets, namely, event distant supervision (EDS). In this method, We design an event-based set to replace the knowledge set, which reduces the cost for reconstructing the knowledge set and offers considerably higher generalization ability. Moreover, we design an event alignment label-annotation mechanism and a scoring mechanism to reduce the possibility of inaccurate and incomplete annotation. Experiments demonstrate that the annotation performance of EDS is significantly superior to that of DS in terms of cost, generalization ability, and effectiveness, particularly for annotating entity pairs with adaptive relationships.},
  archive      = {J_APIN},
  author       = {Li, Fangfang and Chen, Guikai and Liu, Xiyao},
  doi          = {10.1007/s10489-022-03547-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2629-2639},
  shortjournal = {Appl. Intell.},
  title        = {An event-based automatic annotation method for datasets of interpersonal relation extraction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple yet effective 3D ego-pose lift-up based on vector and
distance for a mounted omnidirectional camera. <em>APIN</em>,
<em>53</em>(3), 2616–2628. (<a
href="https://doi.org/10.1007/s10489-022-03417-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the advances in convolutional neural networks and synthetic data generation, 3D egocentric body pose estimations from a mounted fisheye camera have been developed. Previous works estimated 3D joint positions from raw image pixels and intermediate supervision during the process. The mounted fisheye camera captures notably different images that are affected by the optical properties of the lens, angle of views, and setup positions. Therefore, 3D ego-pose estimation from a mounted fisheye camera must be trained for each set of camera optics and setup. We propose a 3D ego-pose estimation from a single mounted omnidirectional camera that captures the entire circumference by back-to-back dual fisheye cameras. The omnidirectional camera can capture the user’s body in the 360∘ field of view under a wide variety of motions. We also propose a simple feed-forward network model to estimate 3D joint positions from 2D joint locations. The lift-up model can be used in real time yet obtains accuracy comparable to those of previous works on our new dataset. Moreover, our model is trainable with the ground truth 3D joint positions and the unit vectors toward the 3D joint positions, which are easily generated from existing publicly available 3D mocap datasets. This advantage alleviates the data collection and training burden due to changes in the camera optics and setups, although it is limited to the effect after the 2D joint location estimation.},
  archive      = {J_APIN},
  author       = {Miura, Teppei and Sako, Shinji},
  doi          = {10.1007/s10489-022-03417-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2616-2628},
  shortjournal = {Appl. Intell.},
  title        = {Simple yet effective 3D ego-pose lift-up based on vector and distance for a mounted omnidirectional camera},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary simulation-optimization approach for the
problem of order allocation with flexible splitting rule in
semiconductor assembly. <em>APIN</em>, <em>53</em>(3), 2593–2615. (<a
href="https://doi.org/10.1007/s10489-022-03701-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose an evolutionary simulation-optimization algorithm for the order allocation problem with flexible splitting rule in semiconductor assembly (SA). There are numerous complex production constraints associated with the operational problems in SA, such as identical and unrelated machines, flexible order lot split, and stochastic processing time, which hinder the decision-making process (i.e., which order allocates which machines and the most efficient lot-split size for a production system). To address complex production constraints in SA, this study constructed a simulation model to evaluate the system performance of each design alternative with minimization of the expected flow time of all orders. Due to the large design alternatives, this study proposes a simulation optimization algorithm to efficiently determine the design alternative. Owing to the high time consumption involved in using a high-fidelity simulation model to evaluate system performance, this algorithm employed a ranking and selection method known as the optimal replication allocation strategy (ORAS), to efficiently allocate computing resources. The ORAS reduced the additional computing cost of non-critical solutions and generated an elite set, which contained elite members not significantly different compared to the global best (Gbest), in each generation of the search algorithm. As this problem is complex and involves numerous local and global optima, an enhanced genetic algorithm (EGA) is proposed to utilize the elite set to enhance the diversity and further improve the solution quality. The proposed algorithm was validated by comparing its performance using statistical methods on 12 instances with those of several state-of-the-art algorithms. The results demonstrated the superior solution quality and search efficiency of the proposed algorithm compared to those of the competitors.},
  archive      = {J_APIN},
  author       = {Chiu, Chun-Chih and Lai, Chyh-Ming and Chen, Chien-Ming},
  doi          = {10.1007/s10489-022-03701-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2593-2615},
  shortjournal = {Appl. Intell.},
  title        = {An evolutionary simulation-optimization approach for the problem of order allocation with flexible splitting rule in semiconductor assembly},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vehicle re-identification based on keypoint segmentation of
original image. <em>APIN</em>, <em>53</em>(3), 2576–2592. (<a
href="https://doi.org/10.1007/s10489-022-03192-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the earlier days, part segmentation methods for vehicle re-id were based on segmenting the feature map of the last convolutional layer. However, by calculating the receptive field, we can see that the size of the receptive field of each point in the feature map of the last convolutional layer exceeds that of the original input image. Therefore, it is very difficult to segment vehicle parts according to feature map. In order to overcome such difficulty, we propose a vehicle re-identification method based on keypoint segmentation of the original image (KSOI). We segment the original image into two parts, which are termed as the window part (upper part) and the below-window part (lower part). Then we use three branches to extract the features of the window part image, the below-window part image and the original vehicle image respectively, which will be fused in the inference stage. In order to achieve accurate segmentation, we label the orientations for all the images in the training set and the keypoint coordinates of the two bottom vertices of the rectangle bounding boxes of the visible windows for some images in the training set. We then train an orientation extraction network and a keypoint detection network to obtain the visible keypoints, and segment the original image with the coordinates of visible keypoints. The experimental results of the proposed KSOI reach the state-of-the-art level.},
  archive      = {J_APIN},
  author       = {Hu, Zhijun and Xu, Yong and Raj, Raja Soosaimarian Peter and Cheng, Xianjing and Sun, Lilei and Wu, Lian},
  doi          = {10.1007/s10489-022-03192-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2576-2592},
  shortjournal = {Appl. Intell.},
  title        = {Vehicle re-identification based on keypoint segmentation of original image},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering via multiple kernel k-means coupled graph and
enhanced tensor learning. <em>APIN</em>, <em>53</em>(3), 2564–2575. (<a
href="https://doi.org/10.1007/s10489-022-03679-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel k-means based and spectral clustering (SC) based multi-kernel clustering (MKC) has been widely used in recent years due to the efficiency in grouping nonlinear data. However, (1) the methods based on the above two categories only focus on clustering indicator matrix learning or graph learning, few of them have noticed the connection between the two; and (2) it is hard for existing methods to consider the high-order similarities of all pre-defined base kernels, which leads to the waste of inter-kernel information. To solve these problems, we propose kernel k-means coupled graph and enhanced tensor learning (KKG-ETL). Concretely, a new graph learning paradigm, kernel k-means coupled graph (KKG), is proposed to establish the theoretical relation between clustering indicator matrix and affinity graph. Therefore, a better candidate affinity graph can be obtained for each base kernel. Then, enhanced tensor learning (ETL) is proposed to capture the high-order similarities of all candidate graphs via an auto-weighted Schatten p-norm. In this framework, we integrate the indication properties of kernel k-means and the manifold excavation capability of SC, while exploring the high-order similarities among all kernels. Comprehensive experiments on 8 widely used datasets verify the validity and feasibility of our proposed KKG-ETL.},
  archive      = {J_APIN},
  author       = {You, Jiali and Han, Chiyu and Ren, Zhenwen and Li, Haoran and You, Xiaojian},
  doi          = {10.1007/s10489-022-03679-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2564-2575},
  shortjournal = {Appl. Intell.},
  title        = {Clustering via multiple kernel k-means coupled graph and enhanced tensor learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LioNets: A neural-specific local interpretation technique
exploiting penultimate layer information. <em>APIN</em>, <em>53</em>(3),
2538–2563. (<a
href="https://doi.org/10.1007/s10489-022-03351-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is having an enormous impact on the rise of technology in every sector. Indeed, AI-powered systems are monitoring and deciding on sensitive economic and societal issues. The future is moving towards automation, and we must not prevent it. Many people, though, have opposing views because of the fear of uncontrollable AI systems. This concern could be reasonable if it originated from considerations associated with social issues, like gender-biased or obscure decision-making systems. Explainable AI (XAI) is a tremendous step towards reliable systems, enhancing the trust of people in AI. Interpretable machine learning (IML), a subfield of XAI, is also an urgent topic of research. This paper presents a small but significant contribution to the IML community. We focus on a local-based, neural-specific interpretation process applied to textual and time series data. Therefore, the proposed technique, which we call “LioNets”, introduces novel approaches to present feature importance-based interpretations. We propose an innovative way to produce counterfactual words in textual datasets. Through a set of quantitative and qualitative experiments, we present competitiveness of LioNets compared to other techniques and suggest its usefulness.},
  archive      = {J_APIN},
  author       = {Mollas, Ioannis and Bassiliades, Nick and Tsoumakas, Grigorios},
  doi          = {10.1007/s10489-022-03351-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2538-2563},
  shortjournal = {Appl. Intell.},
  title        = {LioNets: A neural-specific local interpretation technique exploiting penultimate layer information},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised statistical concept drift detection for
behaviour abnormality detection. <em>APIN</em>, <em>53</em>(3),
2527–2537. (<a
href="https://doi.org/10.1007/s10489-022-03611-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal behaviour can be an indicator for a medical condition in older adults. Our novel unsupervised statistical concept drift detection approach uses variational autoencoders for estimating the parameters for a statistical hypothesis test for abnormal days. As feature, the Kullback–Leibler divergence of activity probability maps derived from power and motion sensors were used. We showed the general feasibility (min. F1-Score of 91 %) on an artificial dataset of four concept drift types. Then we applied our new method to our real–world dataset collected from the homes of 20 (pre–)frail older adults (avg. age 84.75 y). Our method was able to find abnormal days when a participant suffered from severe medical condition.},
  archive      = {J_APIN},
  author       = {Friedrich, Björn and Sawabe, Taishi and Hein, Andreas},
  doi          = {10.1007/s10489-022-03611-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2527-2537},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised statistical concept drift detection for behaviour abnormality detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Parallel gravitational clustering based on grid
partitioning for large-scale data. <em>APIN</em>, <em>53</em>(3),
2506–2526. (<a
href="https://doi.org/10.1007/s10489-022-03661-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gravitational clustering algorithm is a dynamic clustering model that achieves outstanding performance in uncovering the hidden clusters of a complex dataset with any shape, density and distribution. This algorithm is very suitable for mining irregular and unbalanced clusters from large-scale datasets with noise. However, the unbearable time overhead makes this algorithm ineffective to apply at large scales. Therefore, a parallel gravitational clustering algorithm based on grid partitioning (PGCGP) is developed in this paper. First, a grid partitioning strategy is designed to divide a large-scale dataset into multiple grids as evenly as possible. Second, a neighbourhood repair strategy is proposed to work with the gravitational clustering algorithm to accurately mine the clusters of a single grid. Finally, a border point alignment strategy is devised to determine whether to merge two small clusters located in different grids to discover the real clusters of the original large dataset by merging multiple grids. Extensive experiments on multiple artificial and real-world datasets verify that our PGCGP approach achieves good performance.},
  archive      = {J_APIN},
  author       = {Chen, Lei and Chen, Fadong and Liu, Zhaohua and Lv, Mingyang and He, Tingqin and Zhang, Shiwen},
  doi          = {10.1007/s10489-022-03661-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2506-2526},
  shortjournal = {Appl. Intell.},
  title        = {Parallel gravitational clustering based on grid partitioning for large-scale data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision making for autonomous vehicles in highway scenarios
using harmonic SK deep SARSA. <em>APIN</em>, <em>53</em>(3), 2488–2505.
(<a href="https://doi.org/10.1007/s10489-022-03357-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of taking decisions for an autonomous vehicle (AV) to avoid road accident fatalities, provide safety, comfort, and reduce traffic raises the need for improvements in the field of decision making. To solve these challenges, many algorithms and techniques were applied, and the most common ones were reinforcement learning (RL) algorithms combined with deep learning techniques. Therefore, in this paper we proposed a novel extension of the popular “SARSA” (State-Action-Reward-State-Action) RL technique called “Harmonic SK Deep SARSA” that takes advantage of the stability which SARSA algorithm provides and uses the notion of similar and cumulative states saved in an alternative memory to enhance the stability of the algorithm and achieve remarkable performance that SARSA could not accomplish due to its on policy nature. Through the investigation of our novel extension the adaptability of the algorithm to unexpected situations during learning and to unforeseen changes in the environment was proved while reducing the computational load in the learning process and increasing the convergence rate that plays a key role in upgrading decision making application that require numerous real time consecutive decisions, including autonomous vehicles, industrial robots, gaming, aerial navigation... The novel algorithm was tested in a gym environment simulator called “Highway-env” with multiple highway situations (multiple lanes configurations, highway with dynamic number of lanes (from 4-lane to 2-lane, from 4-lane to 6-lane), merge) with numerous dynamic obstacles. For the purpose of comparison, we used a benchmark of cutting edge algorithms known for their prominent performance. The experimental results showed that the proposed algorithm outperformed the comparison algorithms in learning stability and performance that were validated by the following metrics: average loss value per episode, average accuracy per episode, maximum speed value reached per episode, average speed per episode, and the total reward per episode.},
  archive      = {J_APIN},
  author       = {Rais, Mohamed Saber and Boudour, Rachid and Zouaidia, Khouloud and Bougueroua, Lamine},
  doi          = {10.1007/s10489-022-03357-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2488-2505},
  shortjournal = {Appl. Intell.},
  title        = {Decision making for autonomous vehicles in highway scenarios using harmonic SK deep SARSA},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MmLwThV framework: A masked face periocular recognition
system using thermo-visible fusion. <em>APIN</em>, <em>53</em>(3),
2471–2487. (<a
href="https://doi.org/10.1007/s10489-022-03517-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In wake of COVID-19, the world has adapted to a new order. People have started wearing mask on their faces to prevent getting infected. The present face recognition models are no longer proving to be efficient in the current circumstances. This is because, most of the informative part of the face is covered by mask. The periocular recognition therefore holds the key to future of face recognition. However, the periocular region proves to be insufficiently enough to generate highly discriminative features. Also, most of the pre-COVID-19 algorithms fail to work in cases, where the number of training images available is very less. We propose a lightweight periocular recognition framework that uses thermo-visible features and ensemble subspace network classifier to improve upon the existing periocular recognition systems named as Masked Mobile Lightweight Thermo-visible Face Recognition (MmLwThV). The framework successfully improves the accuracy over a single visible modality by mitigating the effect of noise present in the thermo-visible features. The experiments on WHU-IIP dataset and an in-house collected dataset named, CVBL masked dataset, successfully validate the efficacy of our proposed framework. The MmLwFR framework is lightweight and can be easily deployed on mobile phones with a visible and an infrared camera.},
  archive      = {J_APIN},
  author       = {Mishra, Nayaneesh Kumar and Kumar, Sumit and Singh, Satish Kumar},
  doi          = {10.1007/s10489-022-03517-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2471-2487},
  shortjournal = {Appl. Intell.},
  title        = {MmLwThV framework: A masked face periocular recognition system using thermo-visible fusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring semantic awareness via graph representation for
text classification. <em>APIN</em>, <em>53</em>(2), 2088–2097. (<a
href="https://doi.org/10.1007/s10489-022-03526-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is a fundamental problem in natural language processing. Nowadays, text classification based on GNN attracts the attention of researchers. However, the existing works not fulfill well the transmission of contextual semantic information, and they pay more attention to capturing the local features instead of global. Such methods ignore the importance of keyword information features, so they can not fully mine the text-level semantic representation. To relieve such problems, we propose the GText model for discovering the basic features with words and establishing a deeper relationship representation between words and documents. Specially, we utilize semantic features graphs to achieve text semantic representation. Meanwhile, we propose semantic information passing(SIP) mechanism to transmit contextual semantic information, which can enhance the semantic representation from multi-views. In addition, the gate mechanism can further mine the explicit keywords of the whole document. With GText, the test accuracy on MR improved about 2% and on Ohsumed at most 9%, which illustrates GText can better achieve the mining and transmission of text semantic information. Experiments on several authoritative datasets show that our method is superior to the existing text classification methods.},
  archive      = {J_APIN},
  author       = {Li, Yahui and Liu, Yifan and Zhu, Zhenfang and Liu, Peiyu},
  doi          = {10.1007/s10489-022-03526-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2088-2097},
  shortjournal = {Appl. Intell.},
  title        = {Exploring semantic awareness via graph representation for text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph attention information fusion for siamese adaptive
attention tracking. <em>APIN</em>, <em>53</em>(2), 2068–2087. (<a
href="https://doi.org/10.1007/s10489-022-03502-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single target tracker based on a Siamese network regards tracking as a process of similarity matching. The convolution features of the template branch and search area branch realize similarity matching and information fusion by a correlation operation. However, the correlation operation is a local linear matching, which limits the tracker to capturing the complex nonlinear relationship between the template branch and search area branch. In addition, it is easy to lose useful information. Moreover, most trackers do not update the template. The template branch and the search area branch compute convolution features independently without information exchange. To solve these existing problems, a graph attention information fusion for Siamese adaptive attention tracking network (GIFT) is proposed. The information flow between the template branch and search area branch is connected by designing a Siamese adaptive attention module (SAA), and the template information is updated indirectly. The graph attention information fusion module (GAIF) is proposed to effectively fuse the information of the template branch and search area branch and realize the similarity matching of their corresponding parts. Layerwise aggregation makes full use of the shallow and deep features of neural networks. This further improves tracking performance. Experiments on 6 challenging benchmarks, including GOT-10k, OTB100, VOT2018, VOT2019, UAV123 and LaSOT, demonstrate that GIFT has the leading performance and runs at 28.34 FPS, which surpasses the real-time level of 25 FPS.},
  archive      = {J_APIN},
  author       = {Wei, Lixin and Xi, Zeyu and Hu, Ziyu and Sun, Hao},
  doi          = {10.1007/s10489-022-03502-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2068-2087},
  shortjournal = {Appl. Intell.},
  title        = {Graph attention information fusion for siamese adaptive attention tracking},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective shortcut technique for generative adversarial
networks. <em>APIN</em>, <em>53</em>(2), 2055–2067. (<a
href="https://doi.org/10.1007/s10489-022-03666-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, image generation techniques based on generative adversarial network (GAN) have been used to design their generators by stacking multiple residual blocks. A residual block generally contains a shortcut, that is skip connection, which effectively supports information propagation in the network. In this paper, we propose a novel shortcut method, called the gated shortcut, which not only embraces the strength point of the residual block but also further boosts the GAN performance. Specifically, based on the gating mechanism, the proposed method allows the residual block to maintain (or remove) information that is relevant (or irrelevant) to the image being generated. To demonstrate that the proposed method significantly improves the GAN performance, this paper includes extensive experimental results on various standard datasets such as CIFAR-10, CIFAR-100, LSUN, and tiny-ImageNet. Quantitative evaluations show that the gated shortcut achieves the impressive GAN performance in terms of the Frechet inception distance (FID) and inception score (IS). For instance, the proposed method improves the FID and IS scores on the tiny-ImageNet dataset from 35.13 to 27.90 and 20.23 to 23.42, respectively.},
  archive      = {J_APIN},
  author       = {Park, Seung and Yoo, Cheol-Hwan and Shin, Yong-Goo},
  doi          = {10.1007/s10489-022-03666-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2055-2067},
  shortjournal = {Appl. Intell.},
  title        = {Effective shortcut technique for generative adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse summary generation. <em>APIN</em>, <em>53</em>(2),
2042–2054. (<a
href="https://doi.org/10.1007/s10489-022-03450-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art summary generators build on powerful language models, such as BERT, which achieves impressive performance. However, most models employ softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful since it assigns probability mass to many implausible outputs. In this paper, we propose a sparse summary generation model with a new gp-entmax transformation, which includes 1.5-entmax and gradient penalty. The 1.5-entmax has the great effect of filtering noise, retaining important information and improving model performance. Experimental results show that the generated summary has improved in both ROUGE and BLEU metrics, and when tested on the CSL summarization dataset, our method outperforms the softmax model by more than 3 ROUGE-L points. For the purpose of measuring the level of important information in model-generated summaries, we propose a new metric called M2I. Simulation tests on human evaluation showed that the summary generated by the sparse model is more fluent and closer to the text’s main idea.},
  archive      = {J_APIN},
  author       = {Zhao, Shuai and He, Tengjiao and Wen, Jinming},
  doi          = {10.1007/s10489-022-03450-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2042-2054},
  shortjournal = {Appl. Intell.},
  title        = {Sparse summary generation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Style transformed synthetic images for real world gaze
estimation by using residual neural network with embedded personal
identities. <em>APIN</em>, <em>53</em>(2), 2026–2041. (<a
href="https://doi.org/10.1007/s10489-022-03481-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze interaction is essential for social communication in many scenarios; therefore, interpreting people’s gaze direction is helpful for natural human-robot interactions and human-virtual characters. In this study, we first adopt a residual neural network (ResNet) structure with an embedding layer of personal identity (ID-ResNet) that outperformed the current best result of 2.51∘ with MPIIGaze data, a benchmark dataset for gaze estimation. To avoid using manually labelled data, we used UnityEye synthetic images with and without style transformation as the training data. We exceeded the previously reported best result with MPIIGaze data (from 2.76∘ to 2.55∘) and UT-Multiview data (from 4.01∘ to 3.40∘). In addition, it only needs to fine-tune with a few ”calibration” examples for a new person to yield significant performance gains. In addition, we presented the KLBS-eye dataset that contains 15,350 images collected from 12 participants while looking in nine known directions and received the state-of-the-art result of (0.59 ± 1.69∘).},
  archive      = {J_APIN},
  author       = {Wang, Quan and Wang, Hui and Dang, Ruo-Chen and Zhu, Guang-Pu and Pi, Hai-Feng and Shic, Frederick and Hu, Bing-liang},
  doi          = {10.1007/s10489-022-03481-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2026-2041},
  shortjournal = {Appl. Intell.},
  title        = {Style transformed synthetic images for real world gaze estimation by using residual neural network with embedded personal identities},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving uncapacitated p-median problem with reinforcement
learning assisted by graph attention networks. <em>APIN</em>,
<em>53</em>(2), 2010–2025. (<a
href="https://doi.org/10.1007/s10489-022-03453-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The P-Median Problem is one of the basic cases of facility location problems and has been studied for many years. Most methods of solving it are based on classical heuristics or meta-heuristic and they don’t perform well on large-scale problems according to time cost. In this paper, we propose the first reinforcement learning-based method which uses Multi-Talking-Heads Graph Attention Networks to learn representations and design a learnable attention mechanism to solve the uncapacitated P-Median Problem. We train the model using REINFORCE algorithm and show that it has good performance on uncapacitated P-Median Problem according to solution quality and time consumption. We also apply our model to the realistic dataset and empirically figure out that the difference between data distributions is one of the most important factors to influence the final performances.},
  archive      = {J_APIN},
  author       = {Wang, Chenguang and Han, Congying and Guo, Tiande and Ding, Man},
  doi          = {10.1007/s10489-022-03453-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {2010-2025},
  shortjournal = {Appl. Intell.},
  title        = {Solving uncapacitated P-median problem with reinforcement learning assisted by graph attention networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Teacher-student collaborative knowledge distillation for
image classification. <em>APIN</em>, <em>53</em>(2), 1997–2009. (<a
href="https://doi.org/10.1007/s10489-022-03486-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single model usually cannot learn all the appropriate features with limited data, thus leading to poor performance when test data are used. To improve model performance, we propose a teacher-student collaborative knowledge distillation (TSKD) method based on knowledge distillation and self-distillation. The method consists of two parts: learning in the teacher network and self-teaching in the student network. Learning in the teacher network allows the student network to use knowledge from the teacher network. Self-teaching in the student network is to build a multi-exit network based on self-distillation and provide deep features as supervised information for training. In the inference stage, we use ensembles to vote on the classification results of multiple sub-models in the student network. The experimental results demonstrate the superior performance of our method compared with a traditional knowledge distillation method and a self-distillation-based multi-exit network.},
  archive      = {J_APIN},
  author       = {Xu, Chuanyun and Gao, Wenjian and Li, Tian and Bai, Nanlan and Li, Gang and Zhang, Yang},
  doi          = {10.1007/s10489-022-03486-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1997-2009},
  shortjournal = {Appl. Intell.},
  title        = {Teacher-student collaborative knowledge distillation for image classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Salient double reconstruction-based discriminative
projective dictionary pair learning for crowd counting. <em>APIN</em>,
<em>53</em>(2), 1981–1996. (<a
href="https://doi.org/10.1007/s10489-022-03607-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is one of the most fundamental tasks in the field of computer vision and dictionary learning has been successfully applied to the task. However, many traditional dictionary learning-based algorithms for crowd counting often show remarkably large prediction biases on real dynamic monitoring scenes where the feature distribution of the same count is of huge divergence. Meanwhile, these methods also clumsy at revealing salient feature changes between two video frames with the same crowd count. To overcome or alleviate these issues, in this paper we treat crowd counting as a particular classification problem and propose a novel dictionary learning algorithm called salient double-reconstruction based discriminative projective dictionary learning (SDR-DPL) for crowd counting. Specifically, the proposed SDR-DPL develops a novel reconstruction strategy which jointly considers reducing the feature distribution gap and incorporating salient feature mappings into the reconstruction term. This strategy benefits to make the learned dictionaries better adapt to large variations in monitoring crowd scenes and enables to achieve more accurate prediction on the number of pedestrians in both indoor and outdoor scenes. Moreover, we adopt an efficient linear coding technique to represent the crowd features regarding the learned synthesis dictionary, with which the optimization procedure breaks through the computational bottleneck that traditional sparse coding-based methods have to face with. Extensive evaluation experiments on five benchmark datasets validate the impressive performance of the proposed SDR-DPL compared with other state-of-the-art competitors. The source code has been available at https://github.com/Evelhz/SDR-DPL.},
  archive      = {J_APIN},
  author       = {Wang, Tao and Luo, Hao and Zhang, Kaibing and Wang, Huake and Li, Minqi and Lu, Jian},
  doi          = {10.1007/s10489-022-03607-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1981-1996},
  shortjournal = {Appl. Intell.},
  title        = {Salient double reconstruction-based discriminative projective dictionary pair learning for crowd counting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A phased intelligent algorithm for dynamic seru production
considering seru formation changes. <em>APIN</em>, <em>53</em>(2),
1959–1980. (<a
href="https://doi.org/10.1007/s10489-022-03579-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product information is constantly changing in actual production; examples include batch due date changes, batch size changes, and the arrival of emergency batches. However, most studies of seru production have focused on the static seru systems. Therefore, this study investigated the total tardiness of the dynamic seru production considering seru formation changes. When product information changes, four decision processes were considered for optimal solutions: 1) selection of seru(s) for re-formation (SSR); 2) batch scheduling on the seru(s) waiting for re-formation (BSSWR); 3) seru re-formation; and 4) seru re-scheduling. A dynamic seru production model was established here based on the above process, and a phased intelligent algorithm was proposed for solving this problem. SSR was optimized using a genetic algorithm. The earliest due date (EDD) heuristic algorithm was used for BSSWR. Moreover, an adaptive cooperative coevolution algorithm was proposed, in which Q-learning was used for determining whether to optimize seru re-formation or seru re-scheduling. After extensive experiments, the dynamic seru production system considering the seru formation changes reduced the total tardiness by an average of 54% compared with the original solution, and by an average of 15% compared with the dynamic seru production system considering only the seru scheduling changes.},
  archive      = {J_APIN},
  author       = {Fu, Guanghui and Han, Cheng and Yu, Yang and Sun, Wei and Kaku, Ikou},
  doi          = {10.1007/s10489-022-03579-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1959-1980},
  shortjournal = {Appl. Intell.},
  title        = {A phased intelligent algorithm for dynamic seru production considering seru formation changes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computer-controlled diabetes disease diagnosis technique
based on fuzzy inference structure for insulin-dependent patients.
<em>APIN</em>, <em>53</em>(2), 1945–1958. (<a
href="https://doi.org/10.1007/s10489-022-03416-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this study is to the development of an automated closed-loop advice system for intense insulin therapy in clinical practice. We use a Mamdani-type fuzzy logic structure to develop an insulin advisory system. The origin of a long-term high glucose level could be due to several biological variables, with delays in insulin manufacturing, absorption, and activity being among the most common. To incorporate these reasons in the glucose-insulin structure, we adopt a non-linear delay system proposed by the author (Nilam and Rathee Discret Contin Dyn Syst - Ser B 20(9):3115–3129, 2015). The disturbance in glucose concentration due to increment in the values of delay parameters suggested by authors has been regularized using the controller in the present problems. The four separate experiments were undertaken over the structure that are i) no meal consumption, ii) multiple meal intake in a day, iii) atypical meal input, and iv) uncertainties in the model’s parameter. The proposed controller reduces the amplitude of ultradian glucose oscillation as compared to value quoted by other authors (Esna-Ashari et al. J Med Signals Sens 7(1):8–20 2017; Soylu and Danişman Turk J Electr Eng Comput Sci 26(1):172–183, 2018; Wang et al. J Biol Dyn 3(1):22–38, 2009) and yet no control mechanism has been found in the literature for limiting the amplitude of ultradian oscillations of glucose levels. These results support that a fuzzy-based intelligence technique could be used for development of an artificial pancreas of clinical patients.},
  archive      = {J_APIN},
  author       = {Sharma, Ankit and Nilam and Singh, Harendra Pal},
  doi          = {10.1007/s10489-022-03416-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1945-1958},
  shortjournal = {Appl. Intell.},
  title        = {Computer-controlled diabetes disease diagnosis technique based on fuzzy inference structure for insulin-dependent patients},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A master-apprentice evolutionary algorithm for maximum
weighted set k-covering problem. <em>APIN</em>, <em>53</em>(2),
1912–1944. (<a
href="https://doi.org/10.1007/s10489-022-03531-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum weighted set k-covering problem (MWKCP) is a fundamental optimization problem, which depicts the application scenario of resource-constrained environment and user preference selection. In this paper, the mathematical formulation of MWKCP is given for the first time. Then, a novel master-apprentice evolutionary algorithm (MAE) is proposed for solving this NP-hard optimization problem. In order to make MAE applicable to MWKCP, a path re-linking operator is designed as the mutual learning process of two individuals, and a bare bones fireworks algorithm with explosion amplitude adaptation is adopted as the self-learning stage. Experimental results on 150 classical instances show that the proposed algorithm performs best among all competitors including an exact solver and three heuristic algorithms.},
  archive      = {J_APIN},
  author       = {Zhou, Yupeng and Fan, Mingjie and Liu, Xiaofan and Xu, Xin and Wang, Yiyuan and Yin, Minghao},
  doi          = {10.1007/s10489-022-03531-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1912-1944},
  shortjournal = {Appl. Intell.},
  title        = {A master-apprentice evolutionary algorithm for maximum weighted set K-covering problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensionality reduction of SPD data based on riemannian
manifold tangent spaces and local affinity. <em>APIN</em>,
<em>53</em>(2), 1887–1911. (<a
href="https://doi.org/10.1007/s10489-022-03177-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data is increasingly used in practical applications. As a typical representative, Symmetric Positive Definite (SPD) matrices can form a Riemannian manifold rather than a flat linear space. Hence, constructing a dimensionality reduction (DR) algorithm for SPD data directly on manifolds will encounter difficulties in modeling and solving. This paper proposes a novel DR technique based on Riemannian Manifold Tangent Spaces and Local Affinity for SPD data (RMTSLA-SPDDR). Our primary contributions are listed below: (1) We transfer the data from SPD manifolds to tangent spaces, which are all Euclidean spaces. Besides, the tangent spaces are symmetric matrix spaces, which indicates the proposed method greatly preserves the data form and properties; (2) Under the Affine-Invariant Riemannian Metric (AIRM), we incorporate tangent spaces and log transformation, so as to keep the geodesic distance between SPD data and the identity matrix equal to the corresponding Euclidean distance between the transformed data and the origin of the tangent space; (3) The method adopts the local affinity criterion to determine the bilinear transformation between tangent spaces, and take it as the transformation for SPD data consequently. There are no similar reports as the technique presented in this paper before, which means it is a new attempt. On five benchmark datasets, abundant experimental results indicate RMTSLA-SPDDR outperforms the other five state-of-the-art DR methods.},
  archive      = {J_APIN},
  author       = {Gao, Wenxu and Ma, Zhengming and Xiong, Chenkui and Gao, Ting},
  doi          = {10.1007/s10489-022-03177-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1887-1911},
  shortjournal = {Appl. Intell.},
  title        = {Dimensionality reduction of SPD data based on riemannian manifold tangent spaces and local affinity},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Maximum mutual information for feature extraction from
graph-structured data: Application to alzheimer’s disease
classification. <em>APIN</em>, <em>53</em>(2), 1870–1886. (<a
href="https://doi.org/10.1007/s10489-022-03528-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain network can be constructed from various imaging modalities such as magnetic resonance imaging (MRI), representing the functional or structural connectivity between brain regions. The challenge of brain network analysis is efficient dimensionality reduction while retaining feature interpretability. We propose a new method to extract features from graph-structured data based on maximum mutual information (MMI-GSD). First, we develop a novel equation for the feature extraction from GSD and evaluate the interpretability of the features. We establish a framework to optimize the extracted features using the MMI. We conduct experiments on synthetic networks to validate the effectiveness of the proposed MMI-GSD. Next, we conduct experiments on 119 cognitively normal (CN), 105 mild cognitive impairment (MCI), and 36 Alzheimer’s disease (AD) individuals from the Alzheimer’s Disease Neuroimaging Initiative. The classification performance of the proposed method is significantly better than using traditional network metrics and existing feature extraction methods. In the clinical interpretation, we discover discriminative brain regions showing significant differences between the MCI and AD groups and identify significant abnormal connections concentrated in the left hemisphere.},
  archive      = {J_APIN},
  author       = {Yang, Jiawei and Wang, Shaoping and Wu, Teresa},
  doi          = {10.1007/s10489-022-03528-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1870-1886},
  shortjournal = {Appl. Intell.},
  title        = {Maximum mutual information for feature extraction from graph-structured data: Application to alzheimer’s disease classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature attenuation reinforced recurrent neural network for
diffusion prediction. <em>APIN</em>, <em>53</em>(2), 1855–1869. (<a
href="https://doi.org/10.1007/s10489-022-03413-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, prediction models based on deep learning have become more popular owing to their good prediction performance. Of them, the recurrent neural network (RNN) model has shown excellent learning and prediction ability in processing sequence data. However, in the field of information transmission, the existing models only treat the cascade process as a numerical sequence without considering the temporal characteristics of information diffusion and the difference of neighbor influence, making the prediction model unable to capture the characteristics of cascade data. We propose an information diffusion prediction approach based on feature attenuation reinforced recurrent network called Feature Deep Diffusion (FADD) to solve this problem. Firstly, a multi-order neighbor influence mechanism is introduced to distinguish the influence weights of neighbors of different classes, and the user feature representation is updated with the network representation method. Then, combining with the time attenuation effect, the neural network model based on feature attenuation enhancement is constructed. Finally, the model is used to predict information forwarding and information heat. A large set of experiments on two real social networks shows that the performance of the proposed method is better than that of the mainstream propagation prediction method based on an end-to-end neural network.},
  archive      = {J_APIN},
  author       = {Pan, Le and Xiong, Yao and Li, Bicheng and Huang, Tao and Wan, Wang},
  doi          = {10.1007/s10489-022-03413-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1855-1869},
  shortjournal = {Appl. Intell.},
  title        = {Feature attenuation reinforced recurrent neural network for diffusion prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving environmental awareness for autonomous vehicles.
<em>APIN</em>, <em>53</em>(2), 1842–1854. (<a
href="https://doi.org/10.1007/s10489-022-03468-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous vehicles (AVs) have multiple tasks with different priorities and safety levels where classic supervised learning techniques are no longer applicable. Thus, reinforcement learning (RL) algorithms become increasingly appropriate for this domain as the RL algorithms can act on complex problems and adapt their responses in the face of unforeseen situations and environments. The RL agent aims to perform the action that guarantees the optimal reward with the best score. The problem with this approach is if the agent finds a possible optimal action with a reasonable premium and gets stuck in this mediocre strategy, which at the same time is neither the best nor the worst solution. Therefore, the agent avoids performing a more extensive exploration to find new paths and learn alternatives to generate a higher reward. To alleviate this problem, we research the behavior of two types of noise in AVs training. We analyze the results and point out the noise method that most stimulates exploration. A vast exploration of the environment is highly relevant to AVs because they know more about the environment and learn alternative ways of acting in the face of uncertainties. With that, AVs can expect more reliable actions in front of sudden changes in the environment. According to our experiments’ results in a simulator, we can see that noise allows the autonomous vehicle to improve its exploration and increase the reward.},
  archive      = {J_APIN},
  author       = {Peixoto, Maria J. P. and Azim, Akramul},
  doi          = {10.1007/s10489-022-03468-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1842-1854},
  shortjournal = {Appl. Intell.},
  title        = {Improving environmental awareness for autonomous vehicles},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The updating methods of object-induced three-way concept in
dynamic formal contexts. <em>APIN</em>, <em>53</em>(2), 1826–1841. (<a
href="https://doi.org/10.1007/s10489-022-03646-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The methods for constructing concept lattices are vital topics in formal concept analysis. Most of the existing algorithms for constructing three-way concept lattice take care of the static formal contexts and can not deal with the dynamic data. To address this problem, we study the updating methods of object-induced three-way concept lattices for dynamic formal contexts. The main contributions of this paper are as follows: When adding attributes or objects, we propose the update methods for object-induced three-way concepts, and present two algorithms (called AAI and AAP) based on adding multiple attributes. And then, the updating methods of object-induced three-way concept are developed for the case of deleting objects or attributes, and the related algorithm (called DOP) is proposed when deleting objects. Finally, several groups of datasets are selected from UCI for comparative experiments. The experimental results exhibit that our algorithms are more effective and advantageous than the latest construction algorithms.},
  archive      = {J_APIN},
  author       = {Hu, Qian and Qin, Keyun and Yang, Lei},
  doi          = {10.1007/s10489-022-03646-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1826-1841},
  shortjournal = {Appl. Intell.},
  title        = {The updating methods of object-induced three-way concept in dynamic formal contexts},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recognition-oriented facial depth estimation from a single
image. <em>APIN</em>, <em>53</em>(2), 1807–1825. (<a
href="https://doi.org/10.1007/s10489-022-03560-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have dramatically improved 2D face recognition, which makes 3D face recognition more promising, as 3D faces are well acknowledged to be more discriminative than 2D faces. Most deep learning algorithms for 3D face recognition are based on depth information. However, it is difficult to acquire enough depth data to meet the requirements by deep learning. There are many approaches used to reconstruct 3D faces from 2D faces, but depths derived from these 3D faces are too coarse to ensure a satisfying recognition. Therefore, this paper uses real 3D depth data to search for an optimal mapping from 2D faces to facial depths by a well-trained CycleGAN network. This can greatly assist 3D face recognition. To make the CycleGAN network more recognition-oriented, an identical cycle consistency loss is employed instead of the cycle consistency loss in CycleGAN, which typically takes the form of pixel losses. With two perceptual losses enforcing at both ends, CycleGAN is asked to preserve as much identity information as possible in both the forward and backward cycles. Furthermore, another perceptual loss is incorporated to ensure that the mapped depth can preserve the same identity as the corresponding real 3D depth. To increase the generalizability of the model in cases where the data are insufficient, this paper uses massive reconstructed depth data to pre-train the U-Nets therein. Then, the modified CycleGAN network is further trained with the ND-2006 dataset(13450 images) to finally obtain the optimal U-Net network. Extensive experiments are conducted on multiple datasets to show that the proposed method is practicable and effective.},
  archive      = {J_APIN},
  author       = {Chen, Hanqin and Yan, Yao and Qin, Jin and Zhao, Tong and Guo, Tiande},
  doi          = {10.1007/s10489-022-03560-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1807-1825},
  shortjournal = {Appl. Intell.},
  title        = {Recognition-oriented facial depth estimation from a single image},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient cancellable multi-biometric recognition system
based on deep learning and bio-hashing. <em>APIN</em>, <em>53</em>(2),
1792–1806. (<a
href="https://doi.org/10.1007/s10489-021-03153-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancellable biometrics have been enrolled in several applications such as cloud computing and cyber security. This makes researchers investigate their approaches in this field. This paper presents a Cancellable Multi-Biometric System (CMBS) based on deep image style transfer and a fusion process. The main contribution is cascading style transfer processes of the human biometrics including fingerprint, finger vein and face images. Then, a fusion process is carried out on the style transferred images. The generated cancellable templates are evaluated by both visual and statistical analysis. The results of the proposed system show superior performance in terms of Area Under the Curve (AUC) and encryption quality assessment with Structural Similarity Index Measure (SSIM), Number of Changing Pixel Rate (NPCR) and other quality indices. Furthermore, the generated templates are digested using hashing algorithms including SHA-224 and SHA-256. The proposed system is compared to the works in the literature. The comparison reveals that the proposed system has a superior performance compared to other previous ones. Hence, it can be used in biometric authentication in cloud systems.},
  archive      = {J_APIN},
  author       = {Abd El-Rahiem, Basma and Abd El Samie, Fathi E. and Amin, Mohamed},
  doi          = {10.1007/s10489-021-03153-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1792-1806},
  shortjournal = {Appl. Intell.},
  title        = {Efficient cancellable multi-biometric recognition system based on deep learning and bio-hashing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient salp swarm algorithm based on scale-free
informed followers with self-adaption weight. <em>APIN</em>,
<em>53</em>(2), 1759–1791. (<a
href="https://doi.org/10.1007/s10489-022-03438-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms are often leveraged to solve complicated engineering optimization and scientific problems. Salp swarm algorithm is one of the most useful meta-heuristic algorithms in recent years. To alleviate the slow convergence speed of the salp swarm algorithm, as well as the tendency to fall into local minima, we have proposed an efficient salp swarm algorithm called E-SSA, which combines the effective evolutionary strategies of basic salp swarm algorithm and two efficient mechanisms named self-adaption weight and scale-free network. These two mechanisms have been integrated into the follower evolution process of the algorithm to achieve the balance of exploration and exploitation. The performance of the E-SSA is benchmarked against a suit of CEC’2019 series functions and 23 commonly used international benchmarks. The algorithm is further validated via three engineering application problems. The experimental results indicate that the improved algorithm has clear advantages in optimization performance compared with other existing heuristic algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Chao and Xu, Ren-qian and Ma, Lei and Zhao, Jie and Wang, Lu and Xie, Neng-gang and Cheong, Kang Hao},
  doi          = {10.1007/s10489-022-03438-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1759-1791},
  shortjournal = {Appl. Intell.},
  title        = {An efficient salp swarm algorithm based on scale-free informed followers with self-adaption weight},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel attribute reduction method based on intuitionistic
fuzzy three-way cognitive clustering. <em>APIN</em>, <em>53</em>(2),
1744–1758. (<a
href="https://doi.org/10.1007/s10489-022-03496-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction plays a critical role in the Pawlak rough set, which aims to improve the computational efficiency and accuracy of a system by removing redundant attributes. Existing attribute reduction algorithms focus on attribute importance, information entropy and discernibility matrices while ignoring the classification differences of attributes and loss cost in the reduction process. More importantly, people are not “all-around experts”, and their cognition level of domain knowledge (CLDK) may vary, which leads to multiple reduction results based on the same attribute set. In view of this, we integrate learners’ CLDK and present an attribute reduction model based on intuitionistic fuzzy three-way cognitive clustering (IF3WCC). In this scenario, we first present the concept, calculation methods and semantic interpretation of intuitionistic fuzzy cognitive entropy (IFCE) and prove its related properties. Intuitionistic fuzzy cognition similarity (IFCS) is then proposed and utilized to implement IF3WCC. We then introduce the three-way decision (3WD) model to calculate the reduction cost of attributes in various clusters and divide them into irreducible, reducible, and determined reduction sets. Next, we develop a secondary reduction strategy for uncertain reduction attributes and provide a corresponding algorithm. Finally, the rationality and effectiveness of the proposed model are verified by comparing it with existing attribute reduction methods.},
  archive      = {J_APIN},
  author       = {Xin, Xian-wei and Shi, Chun-lei and Sun, Jing-bo and Xue, Zhan-ao and Song, Ji-hua and Peng, Wei-ming},
  doi          = {10.1007/s10489-022-03496-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1744-1758},
  shortjournal = {Appl. Intell.},
  title        = {A novel attribute reduction method based on intuitionistic fuzzy three-way cognitive clustering},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning self-supervised task progression metrics: A case of
cloth folding. <em>APIN</em>, <em>53</em>(2), 1725–1743. (<a
href="https://doi.org/10.1007/s10489-022-03466-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge for smart manufacturing systems is finding relevant metrics that capture task quality and progression for process monitoring to ensure process reliability and safety. Data-driven process metrics construct features and labels from abundant raw process data, which incurs costs and inaccuracies due to the labelling process. In this work, we circumvent expensive process data labelling by distilling the task intent from video demonstrations. We present a method to express the task intent in the form of a scalar value by aligning a self-supervised learned embedding to a small set of high-quality task demonstrations. We evaluate our method on the challenging case of monitoring the progress of people folding clothing. We demonstrate that our approach effectively learns to represent task progression without manually labelling sub-steps or progress in the videos. Using case-based experiments, we find that our method learns task-relevant features and useful invariances, making it robust to noise, distractors and variations in the task and shirts. The experimental results show that the proposed method can monitor processes in domains where state representation is inherently challenging.},
  archive      = {J_APIN},
  author       = {Verleysen, Andreas and Biondina, Matthijs and wyffels, Francis},
  doi          = {10.1007/s10489-022-03466-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1725-1743},
  shortjournal = {Appl. Intell.},
  title        = {Learning self-supervised task progression metrics: A case of cloth folding},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ASFS: A novel streaming feature selection for multi-label
data based on neighborhood rough set. <em>APIN</em>, <em>53</em>(2),
1707–1724. (<a
href="https://doi.org/10.1007/s10489-022-03366-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighborhood rough set based online streaming feature selection methods have aroused wide concern in recent years and played a vital role in processing high-dimensional data. However, most of the existing methods are directly applied to handle single-label data, or to handle multi-label data by converting multi-label data into a combination of multiple single-label datasets, which ignores that the label set of multi-label data is an integral whole. In this paper, we propose a novel online streaming feature selection for multi-label learning via the neighborhoorough set model, in which feature significance, feature redundancy, and label space integrity are taken into account, simultaneously. To be specific, we first define a new adaptive neighborhood relation to avoid the setting of neighborhood parameter and restructure the neighborhood rough set model to be suitable for processing multi-label data directly. Based on this model, we introduce a evaluation criterion to select features that are important relative to label set and the currently selected features, and present an optimization objective function to update the selected feature subset and filter out redundant features. Comparative experiments on different types of data sets explicitly verify the advantages of the proposed method.},
  archive      = {J_APIN},
  author       = {Liu, Jinghua and Lin, Yaojin and Du, Jixiang and Zhang, Hongbo and Chen, Ziyi and Zhang, Jia},
  doi          = {10.1007/s10489-022-03366-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1707-1724},
  shortjournal = {Appl. Intell.},
  title        = {ASFS: A novel streaming feature selection for multi-label data based on neighborhood rough set},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-type data fusion framework based on deep
reinforcement learning for algorithmic trading. <em>APIN</em>,
<em>53</em>(2), 1683–1706. (<a
href="https://doi.org/10.1007/s10489-022-03321-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on algorithmic trading based on machine learning has been increasing. One challenge faced is getting an accurate representation of the stock market environment from multi-type data. Most existing algorithmic trading studies analyze the stock market based on a relatively single data source. However, with the complicated stock market environment, different types of data reflect the changes in the stock market from different perspectives, and how to obtain the temporal features of different types of data and integrate them to obtain a deeper representation of the stock market environment are still problems to be solved. To tackle these problems, in this study, we combine deep learning and reinforcement learning (RL) and propose a multi-type data fusion framework with deep reinforcement learning (MSF-DRL) that integrates stock data, technical indicators and candlestick charts, in which technical indicators can reduce the impact of noise in stock data. In the process of learning trading strategies under the MSF-DRL framework, the temporal features of stock data and technical indicators are extracted through a long short-term memory (LSTM) network, and a convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) are successively used to extract the features of the candlestick chart. The fused features are used as the input of the RL module, which makes trading decisions on this basis. To verify the effectiveness of the MSF-DRL framework, we conducted comparative experiments on datasets composed of Chinese stocks and some stocks of the S&amp;P 500 stock market index. Compared with the other trading strategies, our trading strategy can obtain more profits and a higher Sharpe ratio.},
  archive      = {J_APIN},
  author       = {Liu, Peipei and Zhang, Yunfeng and Bao, Fangxun and Yao, Xunxiang and Zhang, Caiming},
  doi          = {10.1007/s10489-022-03321-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1683-1706},
  shortjournal = {Appl. Intell.},
  title        = {Multi-type data fusion framework based on deep reinforcement learning for algorithmic trading},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic multitask optimization with improved knowledge
transfer mechanism. <em>APIN</em>, <em>53</em>(2), 1666–1682. (<a
href="https://doi.org/10.1007/s10489-022-03282-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitasking optimization (MTO) is promising to become the next-generation mainstream optimization paradigm for optimizing multiple tasks simultaneously with high efficiency and accuracy. However, despite dynamic tasks abound in the real world, such as flow shop scheduling, vehicle routing, IoT, machine learning, research on dynamic multitask optimization (DMTO) has been rarely reported. DMTO problems are more challenging than MTO with static tasks or a single dynamic optimization. In this paper, a dynamic multitask optimization algorithm with an improved knowledge transfer mechanism (IK_DMTO) is proposed to solve the DMTO problems. Firstly, an improved knowledge transfer mechanism is designed to promote knowledge utilization by conditionally selecting the scale of knowledge transfer and reduce negative migration by selectively performing the crossover operation between tasks. Secondly, a new individual information update strategy is applied to guide the individual updates, in which the leaders of the sub-populations formed during the knowledge transfer process are utilized to adjust the direction of individuals to make the utmost of knowledge between tasks, and an external archive management strategy is introduced to achieve a better distribution of non-dominated solutions. Finally, nine dynamic multi-objective multitask optimization (DMOMTO) problems are constructed with the dynamic multi-objective benchmark functions to verify the effectiveness of IK_DMTO. The experimental results show that IK_DMTO can perform well on convergence compared to the comparison algorithms.},
  archive      = {J_APIN},
  author       = {Ren, Kun and Xiao, Fu-Xia and Han, Hong-Gui},
  doi          = {10.1007/s10489-022-03282-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1666-1682},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic multitask optimization with improved knowledge transfer mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable systems based on evidential prospect theory
for decision-making. <em>APIN</em>, <em>53</em>(2), 1640–1665. (<a
href="https://doi.org/10.1007/s10489-022-03276-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster Shafer Theory is known for its capability of modelling information uncertainty by considering the powerset of decision alternatives. Studies in literature propose numerous solutions to resolve the open issues in DST like basic probabilities computation, and conflicting evidence combination. However, there is no widely accepted method so far which can resolve both the issues simultaneously. This work presents a Decision Support System based on descriptive decision-making model which attempts to resolve both the issues, and provides interpretable knowledge about the decision space. The proposed DSS considers triangular fuzzy number to compute basic probabilities, and multi-criteria decision-making methods, instead of DS combination rule, to assign fusion probabilities. The decision alternatives are ranked based on fusion probabilities by an optimal MCDM method, and gain-loss values from prospect theory. Experimental analysis is performed on ten benchmark datasets from various domains. A comprehensive comparison of results with traditional approaches and with recent research works are presented. It can be inferred that VIKOR method has assigned high fusion probabilities, but its prediction accuracy is less compared to TOPSIS; moreover variations in the gain-loss values corresponding to fusion probabilities is observed due to various decision-maker’s attitudes towards risk. An optimal MCDM method, TOPSIS, is chosen based on its performance and prospect values. The approaches and outcomes of this work can be used to develop explainable decision support systems for various applications.},
  archive      = {J_APIN},
  author       = {Kavya, Ramisetty and Christopher, Jabez},
  doi          = {10.1007/s10489-022-03276-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1640-1665},
  shortjournal = {Appl. Intell.},
  title        = {Interpretable systems based on evidential prospect theory for decision-making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel second-order learning algorithm based attention-LSTM
model for dynamic chemical process modeling. <em>APIN</em>,
<em>53</em>(2), 1619–1639. (<a
href="https://doi.org/10.1007/s10489-022-03515-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of chemical process continues to increase, identifying the accurate process model has become a significant task of automatic control and optimal design. The effectiveness of chemical process identification based on deep learning methods has been verified in recent years. Aiming at the characteristics of chemical process, such as temporal correlation, nonlinearity, high dimension and strong coupling, a chemical process modeling method which combines spatio-temporal attention long short-term memory structure with a novel second-order optimization algorithm (STA-SO-LSTM) is proposed. Firstly, a second-order LSTM back-propagation algorithm is proposed to improve the model accuracy and training speed. This novel optimization algorithm uses the second derivative information of the neuron activation function and the gradient information to estimate the inverse Hessian matrix without matrix inversion operation. Then, considering the correlation and the time characteristics among different chemical process variables, a model combining spatio-temporal attention and LSTM was adopted. To demonstrate the efficiency of the developed neural network structure and algorithm, various comparative results are conducted on Tennessee-Eastman (TE) process and fractionator datasets. The experimental results clearly show that the structure combining LSTM and spatio-temporal attention mechanism has good performance in establishing dynamic model. And compared with some traditional and recently proposed optimization algorithms, the proposed second-order algorithm has higher accuracy and faster convergence speed.},
  archive      = {J_APIN},
  author       = {Xu, Baochang and Wang, Yaxin and Yuan, Likun and Xu, Chaonong},
  doi          = {10.1007/s10489-022-03515-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1619-1639},
  shortjournal = {Appl. Intell.},
  title        = {A novel second-order learning algorithm based attention-LSTM model for dynamic chemical process modeling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised pedestrian re-identification via a
teacher–student model with similarity-preserving generative adversarial
networks. <em>APIN</em>, <em>53</em>(2), 1605–1618. (<a
href="https://doi.org/10.1007/s10489-022-03218-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a pedestrian re-identification algorithm, which was developed by integrating semi-supervised learning and similarity-preserving generative adversarial networks (SPGAN). The pedestrian re-identification task aimed to rapidly capture the same target using different cameras. Importantly, this process can be applied in the field of security. Because real-life environments are complex, the number of detected identities is uncertain, and the cost of manual labeling is high; therefore, it is difficult to apply the re-identification model based on supervised learning in real-life scenarios. To use the existing labeled dataset and a large amount of unlabeled data in the application environment, this report proposes a semi-supervised pedestrian re-identification model, which combines a teacher–student model with SPGAN. SPGAN was used to reduce the difference between the target domain and the source domain by transferring the style of the labeled dataset from the source domain. Additionally, the dataset from the source domain was used after the style transfer to pre-train the model; this enabled the model to adapt more rapidly to the target domain. The teacher–student model and the transformer model were then employed to generate soft pseudo-labels and hard pseudo-labels (via iterative training) and to update the parameters through distillation learning. Thus, it retained the learned features while adapting to the target domain. Experimental results indicated that the maps of the applied method on the Market-to-Duke, Duke-to-Market, Market-to-MSMT, and Duke-to-MSMT domains were 70.2, 79.3, 30.2, and 33.4, respectively.},
  archive      = {J_APIN},
  author       = {Zhao, Botong and Wang, Yanjie and Su, Keke and Ren, Hong and Han, Xiyu},
  doi          = {10.1007/s10489-022-03218-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1605-1618},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised pedestrian re-identification via a teacher–student model with similarity-preserving generative adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MS-SSD: Multi-scale single shot detector for ship detection
in remote sensing images. <em>APIN</em>, <em>53</em>(2), 1586–1604. (<a
href="https://doi.org/10.1007/s10489-022-03549-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a fundamental problem in computer vision. Although impressive results have been achieved on large/medium-sized objects, the detection performance of small objects remains a challenging task. Automatic ship detection on remote sensing images is an important module in maritime surveillance system, and it is challenging due to the high variance in appearance and scale. In this work, we thoroughly discuss the issues of SSD on multi-scale objects and propose a multi-scale single-shot detector (MS-SSD) to improve the detection effect of small ship targets and enhance the model’s robustness to scale variance. It enjoys two benefits by introducing (1) more high-level context and (2) more appropriate supervision. Extensive experiments on the Airbus Ship Detection Challenge dataset demonstrate the effectiveness of the proposed method in ship detection from complex backgrounds in remote sensing images. We also achieve better detection performance on the COCO dataset, outperforming state-of-the-art approaches, especially for small targets.},
  archive      = {J_APIN},
  author       = {Wen, Guangqi and Cao, Peng and Wang, Haonan and Chen, Hanlin and Liu, Xiaoli and Xu, Jinghui and Zaiane, Osmar},
  doi          = {10.1007/s10489-022-03549-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1586-1604},
  shortjournal = {Appl. Intell.},
  title        = {MS-SSD: Multi-scale single shot detector for ship detection in remote sensing images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random vector functional link network with subspace-based
local connections. <em>APIN</em>, <em>53</em>(2), 1567–1585. (<a
href="https://doi.org/10.1007/s10489-022-03404-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new random vector functional link (RVFL) network with subspace-based local connections (abbreviated as RVFL-SLC network) is proposed in this paper. The main innovation of RVFL-SLC network is that the local connections are utilized in the input-layer of RVFL network. To select appropriate local connections for RVFL-SLC network, a novel efficient decision entropy criterion is used to partition the original attribute space into disjoint attribute subspaces, where the different attribute subspaces correspond to the respective hidden-layer weights. The decision entropy of each attribute subspace is first defined based on the posterior probabilities of classes given the attribute subspace. Then, the attribute subspaces are iteratively updated to increase the validation accuracy of the RVFL-SLC network. In each iteration, the attribute subspace with the minimum decision entropy is selected as candidate for updating. The optimal attribute subspaces are finally obtained when the RVFL-SLC network reaches a stable validation accuracy. To evaluate the classification accuracy of RVFL-SLC, it was compared on 20 benchmark data sets with four other randomization-based networks, namely the regular RVFL network, regular extreme learning machine (ELM), ELM with local connections (ELM-LC), and the No-Prop network. Experimental results show that the proposed RVFL-SLC network not only obtains the highest testing accuracy compared to other randomization-based networks but also best prevents overfitting. This demonstrates the rationality and effectiveness of using the strategy of subspace-based local connections to improve the generalization capability of RVFL networks.},
  archive      = {J_APIN},
  author       = {He, Yu-Lin and Yuan, Zhen-Hao and Huang, Joshua Zhexue},
  doi          = {10.1007/s10489-022-03404-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1567-1585},
  shortjournal = {Appl. Intell.},
  title        = {Random vector functional link network with subspace-based local connections},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study of preprocessing techniques with
convolutional neural networks for accurate detection of chronic ocular
diseases using fundus images. <em>APIN</em>, <em>53</em>(2), 1548–1566.
(<a href="https://doi.org/10.1007/s10489-022-03490-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic Ocular Diseases (COD) such as myopia, diabetic retinopathy, age-related macular degeneration, glaucoma, and cataract can affect the eye and may even lead to severe vision impairment or blindness. According to a recent World Health Organization (WHO) report on vision, at least 2.2 billion individuals worldwide suffer from vision impairment. Often, overt signs indicative of COD do not manifest until the disease has progressed to an advanced stage. However, if COD is detected early, vision impairment can be avoided by early intervention and cost-effective treatment. Ophthalmologists are trained to detect COD by examining certain minute changes in the retina, such as microaneurysms, macular edema, hemorrhages, and alterations in the blood vessels. The range of eye conditions is diverse, and each of these conditions requires a unique patient-specific treatment. Convolutional neural networks (CNNs) have demonstrated significant potential in multi-disciplinary fields, including the detection of a variety of eye diseases. In this study, we combined several preprocessing approaches with convolutional neural networks to accurately detect COD in eye fundus images. To the best of our knowledge, this is the first work that provides a qualitative analysis of preprocessing approaches for COD classification using CNN models. Experimental results demonstrate that CNNs trained on the region of interest segmented images outperform the models trained on the original input images by a substantial margin. Additionally, an ensemble of three preprocessing techniques outperformed other state-of-the-art approaches by 30% and 3%, in terms of Kappa and F1 scores, respectively. The developed prototype has been extensively tested and can be evaluated on more comprehensive COD datasets for deployment in the clinical setup.},
  archive      = {J_APIN},
  author       = {Mayya, Veena and S, Sowmya Kamath and Kulkarni, Uma and Surya, Divyalakshmi Kaiyoor and Acharya, U Rajendra},
  doi          = {10.1007/s10489-022-03490-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1548-1566},
  shortjournal = {Appl. Intell.},
  title        = {An empirical study of preprocessing techniques with convolutional neural networks for accurate detection of chronic ocular diseases using fundus images},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransGait: Multimodal-based gait recognition with set
transformer. <em>APIN</em>, <em>53</em>(2), 1535–1547. (<a
href="https://doi.org/10.1007/s10489-022-03543-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a biological feature that can be recognized from a distance, gait has a wide range of applications such as crime prevention, judicial identification, and social security. However, gait recognition is still a challenging task with two problems in the typical gait recognition methods. First, the existing gait recognition methods have weak robustness to the pedestrians’ clothing and carryings. Second, the existing temporal modeling methods for gait recognition fail to fully exploit the temporal relationships of the sequence and require that the gait sequence maintain unnecessary sequential constraints. In this paper, we propose a new multi-modal gait recognition framework based on silhouette and pose features to overcome these problems. Joint features of silhouettes and poses provide high discriminability and robustness to the pedestrians’ clothing and carryings. Furthermore, we propose a set transformer model with a temporal aggregation operation for obtaining set-level spatio-temporal features. The temporal modeling approach is unaffected by frame permutations and can seamlessly integrate frames from different videos acquired in different scenarios, such as diverse viewing angles. Experiments on two public datasets, CASIA-B and GREW, demonstrate that the proposed method provides state-of-the-art performance. Under the most challenging condition of walking in different clothes on CASIA-B, the proposed method achieves a rank-1 accuracy of 85.8%, outperforming other methods by a significant margin (&gt; 4%).},
  archive      = {J_APIN},
  author       = {Li, Guodong and Guo, Lijun and Zhang, Rong and Qian, Jiangbo and Gao, Shangce},
  doi          = {10.1007/s10489-022-03543-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1535-1547},
  shortjournal = {Appl. Intell.},
  title        = {TransGait: Multimodal-based gait recognition with set transformer},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-modal dataset for gait recognition under occlusion.
<em>APIN</em>, <em>53</em>(2), 1517–1534. (<a
href="https://doi.org/10.1007/s10489-022-03474-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify people by the way they walk. Currently available gait recognition datasets mainly contain single-person gait data in relatively simple walking conditions, which limits research of robust gait recognition methods. In this paper, OG RGB+D dataset is presented to cope with this crucial limitation of other gait datasets. It includes the common walking conditions under occlusion in daily life, that is, those daily walking conditions in which people’s normal walking patterns are occluded, including self-occlusion caused by views, occlusion caused by clothing or objects, and mutual occlusion between people. The dataset provides multi-modal data to support different types of methods, collected by multiple Azure Kinect DK sensors using synchronous data acquisition system (Multi-Kinect SDAS). Moreover, we propose a model-based gait recognition method SkeletonGait for gait recognition in walking conditions under occlusion, which learns discriminative gait features from human dual skeleton model composed of skeleton and anthropometric features through a siamese Spatio-Temporal Graph Convolutional Network (siamese ST-GCN). The experimental results show that SkeletonGait surpasses state-of-the-art methods in the case of severe occlusion. We believe that the introduction of our dataset will enable the community to apply, adapt, and develop various robust gait recognition methods. The dataset will be available at https://github.com/cvNXE/OG-RGB-D-gait-dataset .},
  archive      = {J_APIN},
  author       = {Li, Na and Zhao, Xinbo},
  doi          = {10.1007/s10489-022-03474-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1517-1534},
  shortjournal = {Appl. Intell.},
  title        = {A multi-modal dataset for gait recognition under occlusion},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge transfer via distillation from time and frequency
domain for time series classification. <em>APIN</em>, <em>53</em>(2),
1505–1516. (<a
href="https://doi.org/10.1007/s10489-022-03485-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has achieved great success on time series classification, two issues are unsolved. First, existing methods mainly extract features in the single domain only, which means that useful information in the specific domain cannot be used. Second, multi-domain learning usually leads to an increase in the size of the model which makes it difficult to deploy on mobile devices. In this this study, a lightweight double-branch model, called Time Frequency Knowledge Reception Network (TFKR-Net) is proposed to simultaneously fuse information from the time and frequency domains. Instead of directly merging knowledge from the teacher models pretrained in different domains, TFKR-Net independently distills knowledge from the teacher models in the time and frequency domains, which helps maintain knowledge diversity. Experimental results on the UCR (University of California, Riverside) archive demonstrate that the TFKR-Net significantly reduces the model size and improves computational efficiency with a little performance loss in classification accuracy.},
  archive      = {J_APIN},
  author       = {Ouyang, Kewei and Hou, Yi and Zhang, Ye and Ma, Chao and Zhou, Shilin},
  doi          = {10.1007/s10489-022-03485-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1505-1516},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge transfer via distillation from time and frequency domain for time series classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Borderline-margin loss based deep metric learning framework
for imbalanced data. <em>APIN</em>, <em>53</em>(2), 1487–1504. (<a
href="https://doi.org/10.1007/s10489-022-03494-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imbalanced data suffer the problem where minority class is under-represented compared with majority ones. Traditional imbalanced learning algorithms only consider the class imbalance while ignoring the class overlap, which leads to an undesirable accuracy for minority samples in overlapping regions. Considering the above issue, we propose a deep metric framework with borderline-margin loss (DMFBML) for improving the intra-class coherence and inter-class difference in overlapping regions. Firstly, a flexible borderline margin is designed for each minority sample, which is adaptively adjusted according to the neighborhood’s label. The proposed margin enables to discriminate minority samples with varying overlap degrees, which significantly preserves the valuable information of classification boundary. The input data is then reconstructed into training triplets set to generate more metric constraints for minority samples, thereby increasing the difference in overlapping regions. Finally, a neural network with DMFBML is presented to achieve a better classifier performance on imbalanced data. The proposed method is verified by the comparative experiments on six synthetic datasets and eleven actual datasets.},
  archive      = {J_APIN},
  author       = {Yan, Mi and Li, Ning},
  doi          = {10.1007/s10489-022-03494-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1487-1504},
  shortjournal = {Appl. Intell.},
  title        = {Borderline-margin loss based deep metric learning framework for imbalanced data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation through multivariate scenario forecasting
in data centers using generative adversarial networks. <em>APIN</em>,
<em>53</em>(2), 1469–1486. (<a
href="https://doi.org/10.1007/s10489-022-03557-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cloud paradigm is at a critical point in which the existing energy-efficiency techniques are reaching a plateau, while the computing resources demand at Data Center facilities continues to increase exponentially. The main challenge in achieving a global energy efficiency strategy based on Artificial Intelligence is that we need massive amounts of data to feed the algorithms. This paper proposes a time-series data augmentation methodology based on synthetic scenario forecasting within the Data Center. For this purpose, we will implement a powerful generative algorithm: Generative Adversarial Networks (GANs). Specifically, our work combines the disciplines of GAN-based data augmentation and scenario forecasting, filling the gap in the generation of synthetic data in DCs. Furthermore, we propose a methodology to increase the variability and heterogeneity of the generated data by introducing on-demand anomalies without additional effort or expert knowledge. We also suggest the use of Kullback-Leibler Divergence and Mean Squared Error as new metrics in the validation of synthetic time series generation, as they provide a better overall comparison of multivariate data distributions. We validate our approach using real data collected in an operating Data Center, successfully generating synthetic data helpful for prediction and optimization models. Our research will help optimize the energy consumed in Data Centers, although the proposed methodology can be employed in any similar time-series-like problem.},
  archive      = {J_APIN},
  author       = {Pérez, Jaime and Arroba, Patricia and Moya, José M.},
  doi          = {10.1007/s10489-022-03557-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1469-1486},
  shortjournal = {Appl. Intell.},
  title        = {Data augmentation through multivariate scenario forecasting in data centers using generative adversarial networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new multi-focus image fusion method based on
multi-classification focus learning and multi-scale decomposition.
<em>APIN</em>, <em>53</em>(2), 1452–1468. (<a
href="https://doi.org/10.1007/s10489-022-03658-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a span of last several years, Deep Learning (DL) has achieved great success in image fusion. For Multi-Focus Image Fusion (MFIF) task, focus classification learning based methods are the most popular ones. This type of methods seeks to generate an all-in-focus synthetic image by combining the partial focused source images according to their focus properties. The basic premise they rely on is that the fused sources are focus complementary. However, this two-classification model is not always valid in practice and consequently, leads to the quality degradation in the focus/defocus junction regions. In addition, the widely used single-scale stitching rule makes them lack the robustness to the source misregistration, which is hard to avoid in the practical use. To address these drawbacks effectively, we propose a new multi-classification focus model and multi-scale decomposition based MFIF method, termed as MCMSCNN, in this paper. Concretely, we design and train a CNN classifier to obtain an initial relative focus probability map of the sources at first, and then we fuse the sources within a multi-classification and multi-scale decomposition based fusion framework. In this framework, we propose a multi-classification focus model to characterize the pixel focus property and fuse various categories of pixels with more specific rule in a multi-scale manner. All these means make our method presents more outstanding focus fusion performance and anti-misregistration capability. In the experiments, we contrast our method with some recently proposed MFIF methods by subjective and objective comparisons. Extensive experimental results validate the competitive performance of the proposed method.},
  archive      = {J_APIN},
  author       = {Ma, Lifeng and Hu, Yanxiang and Zhang, Bo and Li, Jiaqi and Chen, Zhijie and Sun, Wenhao},
  doi          = {10.1007/s10489-022-03658-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1452-1468},
  shortjournal = {Appl. Intell.},
  title        = {A new multi-focus image fusion method based on multi-classification focus learning and multi-scale decomposition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HGAT-BR: Hyperedge-based graph attention network for basket
recommendation. <em>APIN</em>, <em>53</em>(2), 1435–1451. (<a
href="https://doi.org/10.1007/s10489-022-03575-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event that predicts the next item that the people will buy in the next basket is defined as the next basket recommendation task. Though this task has been widely studied, previous works still have the following two challenges: 1) Previous methods usually make predictions via only considering the correlations between items within the basket, while ignoring the similarity relationships between baskets, which may also have the potential capability of improving the basket modeling. 2) Previous studies usually use the Recurrent Neural Network (RNN), especially attention-based RNN and fixed attention mechanisms, to model the relationships between items, which fail to capture the local network structures and their high-order sequential relationships. To overcome the above challenges, in this work, we propose a Hyperedge-based Graph Attention Network for Next Basket Recommendation, namely HGAT-BR, as our solution. To be more specific, to incorporate the similarity relationship between two baskets, we treat baskets as sets of items, and further model them as hyperedges in a hypergraph. Then, the basket representation learning can be converted to the hyperedge embedding task, where a hyperedge-based graph attention network is proposed. To further consider the correlation information among items, we treat items within a basket as nodes in a vanilla graph and learn node representations via another graph neural network. Then, we concatenate these two types of representations to make predictions. Note that, we train the basket and note representation learning simultaneously in an end-to-end manner. We conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our proposed method compared with several state-of-the-art next basket recommendation methods.},
  archive      = {J_APIN},
  author       = {Song, Tengshuo and Guo, Feng and Jiang, Haoran and Ma, Wenyun and Feng, Zhenbao and Guo, Lei},
  doi          = {10.1007/s10489-022-03575-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1435-1451},
  shortjournal = {Appl. Intell.},
  title        = {HGAT-BR: Hyperedge-based graph attention network for basket recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical dynamic movement primitive for the smooth
movement of robots based on deep reinforcement learning. <em>APIN</em>,
<em>53</em>(2), 1417–1434. (<a
href="https://doi.org/10.1007/s10489-022-03219-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep reinforcement learning (DRL) algorithms with experience replay have been used to solve many sequential learning problems, applications of DRL in real-world robotics still face some serious challenges, such as the problem of smooth movement. A robot’s motion trajectory needs to be smoothly coded, with no sudden acceleration or jerk. In this paper, a novel hierarchical reinforcement learning control framework named the hierarchical dynamic movement primitive (HDMP) framework is proposed to achieve the smooth movement of robots. In contrast to traditional algorithms, the HDMP framework consists of two learning hierarchies: a lower-level controller learning hierarchy and an upper-level policy learning hierarchy. In the lower-level controller learning hierarchy, modified dynamic movement primitives (DMPs) are utilized to generate a smooth motion trajectory. In the upper-level policy learning hierarchy, an improved local proximal policy optimization (L-PPO) method is proposed to endow the robot with autonomous learning capabilities. The performance achieved with the HDMP algorithm has been evaluated in a classical reaching movement task based on a Sawyer robot. The experimental results demonstrate that the proposed HDMP algorithm can endow a robot with the ability to smoothly execute motor skills and learn autonomously.},
  archive      = {J_APIN},
  author       = {Yuan, Yinlong and Yu, Zhu Liang and Hua, Liang and Cheng, Yun and Li, Junhong and Sang, Xiaohu},
  doi          = {10.1007/s10489-022-03219-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1417-1434},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical dynamic movement primitive for the smooth movement of robots based on deep reinforcement learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive distance penalty based nonnegative low-rank
representation for semi-supervised learning. <em>APIN</em>,
<em>53</em>(2), 1405–1416. (<a
href="https://doi.org/10.1007/s10489-022-03632-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR) aims to find the essential structural information of the original data. It can capture global information and has strong robustness to noise. However, the disadvantage of LRR is that the local similarity of the data is not considered. Most semi-supervised learning (SSL) methods infer unknown tags based on two-stage learning, in which the first stage is a graph construction, and the second stage is to perform SSL for classification. These methods do not share public information that is used to improve classification accuracy. This paper proposes a new semi-supervised learning classification algorithm termed adaptive distance penalty non-negative low-rank representation (ADP-NNLRR). The proposed method combines low rank representation, local constraints and SSL strategy, to make full use of label information and local manifold geometry structure from the data, which in turn, can capture the global subspace, and maintain the relationship between the local parts better. In the proposed method, distance penalty terms and non-negative constraints are introduced, and the obtained low-rank coefficient matrix is used as the similarity matrix of the graph, which can capture more discriminative information for the construction of the graph. Comparative experiments on some classical datasets and noisy datasets verify the superior performance of our proposed method.},
  archive      = {J_APIN},
  author       = {Zhang, Yixiu and Chen, Jiaxin and Liu, Zhonghua},
  doi          = {10.1007/s10489-022-03632-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1405-1416},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive distance penalty based nonnegative low-rank representation for semi-supervised learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal liquidation of foreign currencies when FX rates
follow a generalised ornstein-uhlenbeck process. <em>APIN</em>,
<em>53</em>(2), 1391–1404. (<a
href="https://doi.org/10.1007/s10489-022-03280-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the case of a multinational company realizing profits in a country other than its base country. The currencies used in the base and foreign countries are referred to as the domestic and foreign currencies respectively. For its quarterly and yearly financial statements, the company transfers its profits from a foreign bank account to a domestic bank account. Thus, the foreign currency liquidation task consists formally in exchanging over a period T a volume V of cash in the foreign currency f for a maximum volume of cash in the domestic currency d. The foreign exchange (FX) rate that prevails at time t is denoted Xd/f(t) and is defined as the worth of one unit of currency d in the currency f. We assume in this article that the natural logarithm of the FX rate $x_{t}=\log X_{d/f}(t)$ follows a discrete generalized Ornstein-Uhlenbeck (OU) process, a process which generalizes the Brownian motion and mean-reverting processes. We also assume minimum and maximum volume constraints on each transaction. Foreign currency liquidation exposes the multinational company to financial risks and can have a significant impact on its final revenues, since FX rates are hard to predict and often quite volatile. We introduce a Reinforcement Learning (RL) framework for finding the liquidation strategy that maximizes the expected total revenue in the domestic currency. Despite the huge success of Deep Reinforcement Learning (DRL) in various domains in the recent past, existing DRL algorithms perform sub-optimally in this task and the Stochastic Dynamic Programming (SDP) algorithm – which yields the optimal strategy in the case of discrete state and action spaces – is rather slow. Thus, we propose here a novel algorithm that addresses both issues. Using SDP, we first determine numerically the optimal solution in the case where the state and decision variables are discrete. We analyse the structure of the computed solution and derive an analytical formula for the optimal trading strategy in the general continuous case. Quasi-optimal parameters of the analytical formula can then be obtained via grid search. This method, simply referred to as ”Estimated Optimal Liquidation Strategy” (EOLS) is validated experimentally using the Euro as domestic currency and 3 foreign currencies, namely USD (US Dollar), CNY(Chinese Yuan) and GBP(Great British Pound). We introduce a liquidation optimality measure based on the gap between the average transaction rate captured by a strategy and the minimum rate over the liquidation period. The metric is used to compare the performance of EOLS to the Time Weighted Average Price (TWAP), SDP and the DRL algorithms Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). The results show that EOLS outperforms TWAP by 54%, and DQN and PPO by 15 − 27%. EOLS runs in average 20 times faster than DQN and PPO. It has a performance on par with SDP but runs 44 times faster. EOLS is the first algorithm that utilizes a closed-form solution of the SDP strategy to achieve quasi-optimal decisions in a liquidation task. Compared with state-of-the-art DRL algorithms, it exhibits a simpler structure, superior performance and significantly reduced compute time, making EOLS better suited in practice.},
  archive      = {J_APIN},
  author       = {Li, Linwei and Matt, Paul-Amaury and Heumann, Christian},
  doi          = {10.1007/s10489-022-03280-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1391-1404},
  shortjournal = {Appl. Intell.},
  title        = {Optimal liquidation of foreign currencies when FX rates follow a generalised ornstein-uhlenbeck process},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus models with aggregation operators for minimum
quadratic cost in group decision making. <em>APIN</em>, <em>53</em>(2),
1370–1390. (<a
href="https://doi.org/10.1007/s10489-021-02948-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group decision making (GDM), to facilitate an acceptable consensus among the experts from different fields, time and resources are paid for persuading experts to modify their opinions. Thus, consensus costs are important for the GDM process. Notwithstanding, the unit costs in the common linear cost functions are always fixed, yet experts will generally express more resistance if they have to make more compromises. In this study, we use the quadratic cost functions, the marginal costs of which increase with the opinion changes. Aggregation operators are also considered to expand the applications of the consensus methods. Moreover, this paper further analyzes the minimum cost consensus models under the weighted average (WA) operator and the ordered weighted average (OWA) operators, respectively. Corresponding approaches are developed based on strictly convex quadratic programming and some desirable properties are also provided. Finally, some examples and comparative analyses are furnished to illustrate the validity of the proposed models.},
  archive      = {J_APIN},
  author       = {Zhang, Ruonan and Huang, Jing and Xu, Yejun and Herrera-Viedma, Enrique},
  doi          = {10.1007/s10489-021-02948-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1370-1390},
  shortjournal = {Appl. Intell.},
  title        = {Consensus models with aggregation operators for minimum quadratic cost in group decision making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel quantum calculus-based complex least mean square
algorithm (q-CLMS). <em>APIN</em>, <em>53</em>(2), 1350–1369. (<a
href="https://doi.org/10.1007/s10489-022-03514-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Least Mean Square (LMS) algorithm has a slow convergence rate as it is dependent on the eigenvalue spread of the input correlation matrix. In this research, we solved this problem by introducing a novel adaptive filtering algorithm for complex domain signal processing based on q-derivative. The proposed algorithm is based on Wirtinger calculus and is called as q- Complex Least Mean Square (q-CLMS) algorithm. The proposed algorithm could be considered as an extension of the q-LMS algorithm for the complex domain. Transient and steady-state analyses of the proposed q-CLMS algorithm are performed and exact analytical expressions for mean analysis, mean square error (MSE), excess mean square error (EMSE), mean square deviation (MSD) and misadjustment are presented. Extensive experiments have been conducted and a good match between the simulation results and theoretical findings is reported. The proposed q-CLMS algorithm is also explored for whitening applications with satisfactory performance. A modification of the proposed q-CLMS algorithm called Enhanced q-CLMS (Eq-CLMS) is also proposed. The Eq-CLMS algorithm eliminates the need for a pre-coded value of the q-parameter thereby automatically adapting to the best value. Extensive experiments are performed on system identification and channel equalization tasks and the proposed algorithm is shown to outperform several benchmark and state-of-the-art approaches namely Complex Least Mean Square (CLMS), Normalized Complex Least Mean Square (NCLMS), Variable Step Size Complex Least Mean Square (VSS-CLMS), Complex FLMS (CFLMS) and Fractional-ordered-CLMS (FoCLMS) algorithms.},
  archive      = {J_APIN},
  author       = {Sadiq, Alishba and Naseem, Imran and Khan, Shujaat and Moinuddin, Muhammad and Togneri, Roberto and Bennamoun, Mohammed},
  doi          = {10.1007/s10489-022-03514-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1350-1369},
  shortjournal = {Appl. Intell.},
  title        = {A novel quantum calculus-based complex least mean square algorithm (q-CLMS)},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep graph clustering with enhanced feature representations
for community detection. <em>APIN</em>, <em>53</em>(2), 1336–1349. (<a
href="https://doi.org/10.1007/s10489-022-03381-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is to partition community nodes into groups with similar attributes and topologies. In recent years, community detection becomes a research hotspot due to its great value and broad applications to social sciences. Thus, many clustering methods are developed for community detection. In particular, deep graph clustering has become a mainstream community detection approach because of its powerful abilities of feature representation and relationship extraction. Deep graph clustering uses graph neural networks (e.g., graph autoencoders) to learn the node feature representations with abundant topological relationships. Though deep graph clustering succeeds in dealing with topological relationships of nodes, there are insufficient attribute information in its learned feature representations, which hurt detection performance. To solve this problem, we propose an enhanced feature representation approach for deep graph clustering in community detection. Firstly, we construct a basic autoencoder to learn hierarchical attribute information and then deliver it into neural layers of a graph autoencoder. The graph autoencoder organically combines the received hierarchical attribute information with its extracted topological relationships to generate enhanced feature representations for clustering. Secondly, we design a self-supervised mechanism to optimize our deep graph clustering model. This mechanism uses the reconstruction losses of two autoencoders and clustering loss as self-supervised information to efficiently guide model updates. In this way, our approach overcomes the insufficient attribute information in generated feature representations, thus is more conducive to community detection. Extensive experiments demonstrate that the deep graph clustering with enhanced feature representations improves the performance of community detection compared to the other popular deep graph clustering approaches.},
  archive      = {J_APIN},
  author       = {Hao, Jie and Zhu, William},
  doi          = {10.1007/s10489-022-03381-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1336-1349},
  shortjournal = {Appl. Intell.},
  title        = {Deep graph clustering with enhanced feature representations for community detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based denoising network for
sequential recommendation. <em>APIN</em>, <em>53</em>(2), 1324–1335. (<a
href="https://doi.org/10.1007/s10489-022-03298-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation models each user as a chronological sequence of interacted items and aims to predict what the user will buy in the near future. In this task, sequential dependency is an important factor that needs to be considered, as items the user will buy in the future are largely dependent on past items. Due to the randomness and diversity of user behaviors, not all items purchased are relevant to the next choice. Considering that these irrelevant items tend to bury the influence of the few truly relevant items, how to extract reliable items relevant to the target item to make a correct recommendation is thus a crucial and challenging task. In this paper, we propose a R einforcement LE arning-based D enoising network(RED for short) to make data denoising for sequential recommendation. Specifically, RED formalizes the sequential denoising problem into a Markov Decision Process and utilizes a reinforcement learning method to automatically separate the initial sequence into two parts: one contains items relevant to the target item, while the other one keeps irrelevant items. A Pseudo-Siamese component is further applied to these two subsequences to generate their delayed rewards, and a pairwise policy gradient strategy is designed to guarantee the robustness of the learning process. The experiments on four public datasets demonstrate that our model outperforms state-of-the-art recommendation methods on a variety of common evaluation metrics.},
  archive      = {J_APIN},
  author       = {Tong, Xiaohai and Wang, Pengfei and Niu, Shaozhang},
  doi          = {10.1007/s10489-022-03298-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1324-1335},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning-based denoising network for sequential recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Interpretable prison term prediction with reinforce
learning and attention. <em>APIN</em>, <em>53</em>(2), 1306–1323. (<a
href="https://doi.org/10.1007/s10489-022-03675-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of prison term prediction is to predict the term of penalty based on the charge and the seriousness of the sentencing plot. Most existing methods focus on improving prediction accuracy but disregard interpretability, which yields unreliable judgment results. To address this problem, we propose an interpretable prison term prediction method. First, the prison term is divided into intervals according to the charge and sentencing plot. Second, we propose a reinforcement learning principle representation model combined with an attention mechanism for regression prediction (PRRP), which extracts phrase-level principles representation as the explanatory basis of prediction results, uses the principle in conjunction with the charge semantics to predict the interval value, and extracts the interval keywords as the sentencing plot. Third, we design a novel multiangle attention mechanism to capture the distinguishing features of cases from different aspects, and a feature fusion network is employed to more effectively stitch multiple pieces of information to learn the feature-enhanced fact representation. Last, the feature-enhanced fact representation is used to predict the prison term. Experimental results on real-work datasets show the interpretability and effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Wang, Peipeng and Zhang, Xiuguo and Yu, Han and Cao, Zhiying},
  doi          = {10.1007/s10489-022-03675-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1306-1323},
  shortjournal = {Appl. Intell.},
  title        = {Interpretable prison term prediction with reinforce learning and attention},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CASSL: A cell-type annotation method for single cell
transcriptomics data using semi-supervised learning. <em>APIN</em>,
<em>53</em>(2), 1287–1305. (<a
href="https://doi.org/10.1007/s10489-022-03440-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single cell RNA sequencing (scRNA-seq) allows global transcriptomic profiling at a cellular resolution, thus, identifying underlying cell types and corresponding lineages. Such cell type identification and annotation rely heavily on models that learn by training themselves on a large amount of individual cells with accurate, annotated labels. Presently, this task of cell-type annotation is done based on inspection of marker genes from each of the statistically significant groups of cells. This is both challenging and time consuming. In this article, we have proposed a semi-supervised cell-type annotation method, called CASSL, based on Non-negative matrix factorization (NMF) coupled with recursive k-means algorithm. A semi-supervised model is capable of learning labels for a large amount of unlabelled data with the help of a limited amount of labelled data. The effectiveness of CASSL has been demonstrated on eight publicly available human and mice scRNA-seq datasets across varied organs and protocols. It has been able to correctly annotate majority of the unlabelled cells with high accuracy. It has also been evaluated for its correctness of clustering solution, robustness across varying percentage of missing labels, and time taken for execution. When compared with state-of-the-art unsupervised and semi-supervised cell-type annotation methods, CASSL has consistently outperformed others across all metrics for most of the datasets. It has also shown competitive results when compared against state-of-the-art supervised methods.},
  archive      = {J_APIN},
  author       = {Seal, Dibyendu Bikash and Das, Vivek and De, Rajat K.},
  doi          = {10.1007/s10489-022-03440-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1287-1305},
  shortjournal = {Appl. Intell.},
  title        = {CASSL: A cell-type annotation method for single cell transcriptomics data using semi-supervised learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compressed neural architecture utilizing dimensionality
reduction and quantization. <em>APIN</em>, <em>53</em>(2), 1271–1286.
(<a href="https://doi.org/10.1007/s10489-022-03221-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has become the default solution for a plethora of problems nowadays. However, one drawback of such deep learning-based solutions is that the models are very large and cumbersome to process. As such, they are difficult to use in small or embedded devices and to transmit across the web. In light of this problem, this paper presents a novel method for converting large neural networks into lightweight, compressed models. Our method utilizes the dimensionality reduction algorithm known as Principal Component Analysis to decompose the network weights into smaller matrices to create a new, compressed architecture. This compressed model is further trained to overcome the error due to the lossy compression and then the parameters are finally stored after quantization. Experiments on benchmark datasets using standard models show that we achieve high compression, with compression rates between 5 to 35 depending on the complexity of the model, with little to no fall in model accuracy. Comparison with other state-of-the-art methods shows that the performance of our compression method is similar or even better in certain cases. This is the first work where dimensionality reduction and quantization are combined to create a new, compressed model.},
  archive      = {J_APIN},
  author       = {Hasan, Md. Saqib and Alam, Rukshar and Adnan, Muhammad Abdullah},
  doi          = {10.1007/s10489-022-03221-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1271-1286},
  shortjournal = {Appl. Intell.},
  title        = {Compressed neural architecture utilizing dimensionality reduction and quantization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust semi-supervised clustering via data transductive
warping. <em>APIN</em>, <em>53</em>(2), 1254–1270. (<a
href="https://doi.org/10.1007/s10489-022-03493-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, we are more likely to face semi-supervised data with a small amount of independent class label or constraint information and many unlabeled instances. For semi-supervised clustering, taking advantage of the small portion of preliminary label information can significantly improve the discriminability of representations. Spectral clustering has the benefits of handling any shape data distribution and converging to the optimal global solution but is susceptible to noisy data. However, it is inevitable to contain noise for real-world applications that significantly reduce clustering performance. Motivated by this, we propose a novel Robust Semi-supervised Spectral Clustering method (named RSSC) to address clustering on noise semi-supervised datasets. Specifically, in terms of data transductive warping, we map the entire semi-supervised dataset into a new data space where labeled data is close to the canonical coordinate system, and unlabeled data with similar characteristics should be close to those labeled data. The noise data is close to the origin of the coordinate and form the noise cluster because there is no guidance. Finally, samples in the same cluster are close, and different clusters are separated. Extensive experimental results on sixteen real-world datasets demonstrate that RSSC outperforms other state-of-the-art clustering methods on performance and robustness.},
  archive      = {J_APIN},
  author       = {Zhou, Peng and Wang, Ni and Zhao, Shu and Zhang, Yanping},
  doi          = {10.1007/s10489-022-03493-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1254-1270},
  shortjournal = {Appl. Intell.},
  title        = {Robust semi-supervised clustering via data transductive warping},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy efficiency-driven mobile base station deployment
strategy for shopping malls using modified improved differential
evolution algorithm. <em>APIN</em>, <em>53</em>(2), 1233–1253. (<a
href="https://doi.org/10.1007/s10489-022-03358-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The short-time aggregation of human traffic places high demands on the communication capacity of cellular networks. The deployment of expensive permanent infrastructure without continuous high traffic is uneconomical, and the problem poses a challenge. In this study, a green mall traffic model based on mobile base stations with a dynamic sleep strategy is proposed for surges of shopping mall traffic. The model is addressed through a modified improved differential evolution (MIDE) algorithm based on the original improved differential evolution (IDE) algorithm. The algorithm has two sets of mutation and restart policies adapted to different traffic volumes, and can dynamically adjust according to the traffic volume. The effectiveness of the algorithm is verified by simulation experiments. Compared with the traditional differential evolution (DE) algorithm and the DE series algorithms recently published in Swarm and Evolutionary Computation, a journal with high impact factors, MIDE can effectively optimize the system model and improve its energy efficiency, saving 1.1%–56.4% in simulation experiments.},
  archive      = {J_APIN},
  author       = {Sun, Xingping and Zhang, Tian and Xu, Jing and Zhang, Haigang and Kang, Hongwei and Shen, Yong and Chen, Qingyi},
  doi          = {10.1007/s10489-022-03358-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1233-1253},
  shortjournal = {Appl. Intell.},
  title        = {Energy efficiency-driven mobile base station deployment strategy for shopping malls using modified improved differential evolution algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust newton iterative algorithm for acoustic location
based on solving linear matrix equations in the presence of various
noises. <em>APIN</em>, <em>53</em>(2), 1219–1232. (<a
href="https://doi.org/10.1007/s10489-022-03483-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many prevalent acoustic location techniques, the location problems can be modelled as solving a linear equation. Although many mature algorithms have been developed to solve the linear equation in acoustic location applications, few of them consider the inevitable noises in a real computing system that might degrade the convergence and accuracy of the algorithms or even lead to failure. Thus, to achieve promising performance when solving a linear equation in a noisy environment, a robust Newton iterative (RNI) algorithm is proposed in this paper based on control theory. Theoretical analyses indicated that the RNI algorithm can not only suppress the constant noise to zero but also maintain convergence against an increasing linear noise and random noise. In addition, extensive simulation results compared with the classic algorithms and their last variants are provided. Among these algorithms, the RNI algorithm achieves the best robustness and accuracy in the presence of noises, while it requires a longer convergence time.},
  archive      = {J_APIN},
  author       = {Wang, Guancheng and Hao, Zhihao and Zhang, Bob and Fang, Leyuan and Mao, Dianhui},
  doi          = {10.1007/s10489-022-03483-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1219-1232},
  shortjournal = {Appl. Intell.},
  title        = {A robust newton iterative algorithm for acoustic location based on solving linear matrix equations in the presence of various noises},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Personality-based and trust-aware products recommendation
in social networks. <em>APIN</em>, <em>53</em>(1), 879–903. (<a
href="https://doi.org/10.1007/s10489-022-03542-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the development of technology, the shopping approach of people has moved towards pervasive online social shopping. As a result, how to create a recommendation algorithm that offers products based on the personal and different needs and tastes of people on social networks is a significant research issue. This article proposes a personality-based and trust-aware probabilistic product recommendation algorithm in social networks. We present a dynamic method for determining how similar people in social networks are. For this purpose, we consider the personality-based features of recommendation attributes of products in social networks. Then, the level of trust of products and types of correlations among the products is considered to create a probabilistic matrix of product recommendation. Moreover, for solving the cold start problem of products, we consider qualitative aspects of products while exploiting personality-based user behavior regarding their purchases. At last, the empirical experiments are conducted to analyze the impact of the algorithm’s different influence factors using the Amazon dataset. Moreover, the results of comprehensive experiments adopted to verify the proposed personalized recommendation algorithm’s effectiveness show that the proposed algorithm has the appropriate effectiveness and the higher accuracy.},
  archive      = {J_APIN},
  author       = {Vatani, Nasim and Rahmani, Amir Masoud and Javadi, Hamid Haj Seyyed},
  doi          = {10.1007/s10489-022-03542-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {879-903},
  shortjournal = {Appl. Intell.},
  title        = {Personality-based and trust-aware products recommendation in social networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Momentum memory contrastive learning for transfer-based
few-shot classification. <em>APIN</em>, <em>53</em>(1), 864–878. (<a
href="https://doi.org/10.1007/s10489-022-03506-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the representation ability of feature extractors in few-shot classification, in this paper, we propose a momentum memory contrastive few-shot learning method based on the distance metric and transfer learning. The proposed method adopts an external memory bank and a contrastive loss function to constrain the feature representation of the samples in training. The memory bank is maintained by the dynamic momentum update of current samples. In addition, a feature representation augmentation technique is used to improve the generalization of the feature representation centroid to the samples in the testing. Furthermore, we design a spatial pyramid fusion downscaling module to improve the extraction ability of multi-scale features. Experimental results show that our method outperforms the compared methods and achieves state-of-the-art accuracy in 5-way 1-shot and 5-way 5-shot tasks on datasets including miniImageNet, CUB-200, and CIFAR-FS. The extensive study with discussions verifies the effectiveness of each proposed component in our method.},
  archive      = {J_APIN},
  author       = {Tian, Runliang and Shi, Hongmei},
  doi          = {10.1007/s10489-022-03506-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {864-878},
  shortjournal = {Appl. Intell.},
  title        = {Momentum memory contrastive learning for transfer-based few-shot classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A robust graph based multi-label feature selection
considering feature-label dependency. <em>APIN</em>, <em>53</em>(1),
837–863. (<a href="https://doi.org/10.1007/s10489-022-03425-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection for multilabel data is a challenging and meaningful work. The information contained in multilabel data is more abundant, which may help further mine knowledge and aid decision-making in various real-life applications. However, the difficulty also increases in dealing with multilabel data because the relations between labels and features need to be considered simultaneously. Missing labels and noises may exist in multilabel data, which may affect the feature selection process. Aiming at solving these problems, a robust feature selection approach is constructed under the sparse learning framework based on the least squares regression model in this study. First, a novel objective function is built by considering the robustness of the method and the manifold information. Nonnegative matrix factorization (NMF) is used to compress the label matrix to reduce false label information, which may mislead the feature selection process. The l2,1-norm is adopted to constrain the least squares regression term. Manifold regularizers are used to construct low-dimensional manifold embeddings of the original feature and label space, retaining the local manifold structure of the data. Furthermore, the correlations between features and labels are explored, and an improved weight matrix is designed. Then, an iteration algorithm is proposed to solve the objective function. Extensive experiments are performed to analyze the proposed approach, which is compared with state-of-the-art algorithms on public multilabel datasets. The experimental results verify the effectiveness of the approach.},
  archive      = {J_APIN},
  author       = {Liu, Yunfei and Chen, Hongmei and Li, Tianrui and Li, Weiyi},
  doi          = {10.1007/s10489-022-03425-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {837-863},
  shortjournal = {Appl. Intell.},
  title        = {A robust graph based multi-label feature selection considering feature-label dependency},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated BWM-entropy weighting and MULTIMOORA method with
probabilistic linguistic information for the evaluation of waste
recycling apps. <em>APIN</em>, <em>53</em>(1), 813–836. (<a
href="https://doi.org/10.1007/s10489-022-03377-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the encouragement and increasingly recycling demands of the Chinese governments, online recycling platforms based on B2C, such as loving recycling app, waste recycling alliance app, have emerged as the times require. As an indispensable part of online recycling, recycling app evaluation plays a vital role in user acceptance of the innovative recycling way. As we all know, app evaluation is a typical multiple criteria decision making (MCDM) problem involving many complicated criteria. This paper aims to propose an integrated MCDM method to solve this issue under a probabilistic linguistic context. Firstly, a special evaluation criteria system is constructed for measuring apps’ performance, which includes five main criteria namely technical feature, safety, interface design, basic requirement, service quality, as well as 12 sub-criteria. Then, we integrated the best-worst method (BWM) and entropy method with the probabilistic linguistic term to determine the subjective and objective weights. And then the comprehensive weights are calculated by multiplicative integration method. Afterwards, the MULTIMOORA method with the Borda rule, is used to rank alternatives and identify the optimal recycling app. Finally, the assessment of the four recycling apps’ performance in Beijing is presented to illustrate the validity and rationality of the proposed approach in practical applications.},
  archive      = {J_APIN},
  author       = {Ma, Yanfang and Zhao, Yuanyuan and Wang, Xiaoyu and Feng, Cuiying and Zhou, Xiaoyang and Lev, Benjamin},
  doi          = {10.1007/s10489-022-03377-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {813-836},
  shortjournal = {Appl. Intell.},
  title        = {Integrated BWM-entropy weighting and MULTIMOORA method with probabilistic linguistic information for the evaluation of waste recycling apps},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive convolution with label embedding for text
classification. <em>APIN</em>, <em>53</em>(1), 804–812. (<a
href="https://doi.org/10.1007/s10489-021-02702-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNNs) has made a breakthrough since deep learning was employed to text classification. However, the traditional CNNs usually use the same set of filters for feature extraction, where labels play a less central role of the final performance, as label information is not better utilized. To solve the problem of how to represent label information better and how to apply the learned label representations to the text classification tasks, we propose an adaptive convolution with label embedding(ACLE) in this paper, which adaptively generated convolutional filters that are conditioned on inputs and made each label embedded in the same space with the word vectors. Our method maintains the flexibility of adaptive convolution, and fully extracts label information to play an auxiliary role. The experimental results on the several large text datasets show that the proposed model is feasible and out-performs the state-of-the-art methods by a large margin in terms of accuracy.},
  archive      = {J_APIN},
  author       = {Tan, Changgeng and Ren, Yuan and Wang, Chen},
  doi          = {10.1007/s10489-021-02702-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {804-812},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive convolution with label embedding for text classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel synthetic minority oversampling technique based on
relative and absolute densities for imbalanced classification.
<em>APIN</em>, <em>53</em>(1), 786–803. (<a
href="https://doi.org/10.1007/s10489-022-03512-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a classifier from class-imbalance data is an important challenge. Among the existing solutions, SMOTE has received great praise and features an extensive range of practical applications. However, SMOTE and its extensions usually degrade due to noise generation and within-class imbalances. Although multiple variations of SMOTE are developed, few of them can solve the above problems at the same time. Besides, many improvements of SMOTE are based on advanced models with introducing external parameters. To solve imbalances between and within classes while overcoming noise generation, a novel synthetic minority oversampling technique based on relative and absolute densities is proposed. First, a novel noise filter based on relative density is proposed to remove noise and smooth class boundary. Second, sparsity and boundary weights are proposed and calculated by relative and absolute densities, respectively. Third, normalized weights based on absolute and sparse weights are proposed to generate more synthetic minority class samples in the class boundary and sparse regions. The main advantages of the proposed algorithm are that: (a) It can effectively avoid noise generation while removing noise and smoothing class the boundary in original data. (b) It generates more synthetic samples in class boundaries and sparse regions; (c) No additional parameters are introduced. Intensive experiments prove that SMOTE-RD outperforms 7 popular oversampling methods in average AUC, average F-measure and average G-mean on real data sets with the acceptable time cost.},
  archive      = {J_APIN},
  author       = {Liu, Ruijuan},
  doi          = {10.1007/s10489-022-03512-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {786-803},
  shortjournal = {Appl. Intell.},
  title        = {A novel synthetic minority oversampling technique based on relative and absolute densities for imbalanced classification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). BDLA: Bi-directional local alignment for few-shot learning.
<em>APIN</em>, <em>53</em>(1), 769–785. (<a
href="https://doi.org/10.1007/s10489-022-03479-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully exploited to various computer vision tasks, which depend on abundant annotations. The core goal of few-shot learning, in contrast, is to learn a classifier to recognize new classes from only a few labeled examples that produce a key challenge of visual recognition. However, most of the existing methods often adopt image-level features or local monodirectional manner-based similarity measures, which suffer from the interference of non-dominant objects. To tackle this limitation, we propose a Bi-Directional Local Alignment (BDLA) approach for the few-shot visual classification problem. Specifically, building upon the episodic learning mechanism, we first adopt a shared embedding network to encode the 3D tensor features with semantic information, which can effectively describe the spatial geometric representation of the image. Afterwards, we construct a forward and a backward distance by exploring the nearest neighbor search to determine the semantic region-wise feature corresponding to each local descriptor of query sets and support sets. The bi-directional distance can encourage the alignment between similar semantic information while filtering out the interference information. Finally, we design a convex combination to merge the bi-directional distance and optimize the network in an end-to-end manner. Extensive experiments also show that our proposed approach outperforms several previous methods on four standard few-shot classification datasets.},
  archive      = {J_APIN},
  author       = {Zheng, Zijun and Feng, Xiang and Yu, Huiqun and Li, Xiuquan and Gao, Mengqi},
  doi          = {10.1007/s10489-022-03479-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {769-785},
  shortjournal = {Appl. Intell.},
  title        = {BDLA: Bi-directional local alignment for few-shot learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Toward action recognition and assessment using SFAGCN and
combinative regression model of spatiotemporal features. <em>APIN</em>,
<em>53</em>(1), 757–768. (<a
href="https://doi.org/10.1007/s10489-022-03411-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton contains intuitive information of motions, therefore, it has been widely studied in action analysis tasks. As a part of action analysis, traditional models human action assessment by handcrafted-feature-based methods, such as dynamic time warping (DTW). These methods only extract the similarity of particular spatiotemporal features, whereas the global spatio-temporal relevance of action analysis tends to be ignored. In this paper, we propose a regression assessment model for action spatio-temporal features, which encodes the temporal features, spatial features and fused features respectively. The self-attention mechanism is taken advantage of to fuse the decoupling features, and then the overall score of action was calculated by regression. Specifically, via structure-feature fusion adaptive graph convolutional networks (SFAGCN), our action assessment network models the deep dependence of global spatio-temporal feature to address the difficulties of limited expressive ability and generalization. Furthermore, the topology of the skeletal graph and the features of the joints are merged by decoupling the spatio-temporal correlations. To confirm the effectiveness of our assessment model, we conduct experiments on six Olympic Games assessment tasks and exceed the state-of-the-art performance in Spearman’s rank correlation analysis.},
  archive      = {J_APIN},
  author       = {Zhang, Zhitao and Wang, Zhengyou and Zhuang, Shanna and Wang, Jiahui},
  doi          = {10.1007/s10489-022-03411-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {757-768},
  shortjournal = {Appl. Intell.},
  title        = {Toward action recognition and assessment using SFAGCN and combinative regression model of spatiotemporal features},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delving into monocular 3D vehicle tracking: A decoupled
framework and a dedicated metric. <em>APIN</em>, <em>53</em>(1),
746–756. (<a href="https://doi.org/10.1007/s10489-022-03432-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring 3D trajectories of on-road vehicles is an essential visual task for autonomous driving systems. Existing 3D vehicle tracking methods either rely on point cloud data or need to be trained on visual tracking datasets. In comparison, a decoupled monocular 3D vehicle tracking framework is proposed in this paper. Because our framework is the first of its kind, a previous decoupled LiDAR-based method is taken as the baseline by substituting its detector with a monocular one. On this foundation, we further employ global coordinates to cancel out ego motion and introduce the angular rate into the 3D Kalman filter. In order to tackle the problem of long-term association, a trajectory management scheme is proposed with our novel hibernation mechanism. Furthermore, it is pointed out that current monocular 3D tracking methods have not been tailored for the depth estimation uncertainty produced by monocular 3D detectors. In this regard, we propose a depth-aware association strategy which endows remoter vehicles with larger matching regions in the data association stage. As another contribution, we discuss the defects of current metrics for evaluating 3D tracking performance and devise a nonuniform metric which is dedicated to monocular vision. Through extensive experiments conducted on the KITTI tracking benchmark, the superiority of proposed monocular 3D vehicle tracking framework and metric is demonstrated by both quantitative results and qualitative intuition.},
  archive      = {J_APIN},
  author       = {Gao, Tianze and Jia, Zhixiang and Lin, Weiyang and Li, Yu},
  doi          = {10.1007/s10489-022-03432-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {746-756},
  shortjournal = {Appl. Intell.},
  title        = {Delving into monocular 3D vehicle tracking: A decoupled framework and a dedicated metric},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing a hybrid probabilistic model for short-term wind
speed forecasting. <em>APIN</em>, <em>53</em>(1), 728–745. (<a
href="https://doi.org/10.1007/s10489-022-03644-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic forecasting can provide quantitative and comprehensive information about future wind speed uncertainty, which exhibits increasing importance for the vital integration of wind power into power systems. However, many conventional forecasting models cannot better handle the uncertainty generated by the intermittent and stochastic nature of wind patterns. This paper develops a hybrid versatile forecasting framework that presents point estimation and different forms of probabilistic wind speed forecasting, such as interval prediction and probability density prediction. In this proposed model, the hybrid of empirical wavelet transform and neural network-based quantile regression is proposed to improve the generalization and robustness of probabilistic forecasts and to capture the probabilistic meaning output of wind speed forecasts. To further emphasize the importance of probabilistic forecasting, an adaptive density estimation method based on the Gaussian kernel function as determined by the firefly algorithm is developed to generate more predictive probability information. Wind datasets from three sites in Penglai, China, are used to validate the proposed approach. The experimental results demonstrate that the developed probabilistic wind speed forecasting model can provide value and comprehensive information about future wind speed and is a promising tool for the decision-making process in wind power operation and management.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaobo},
  doi          = {10.1007/s10489-022-03644-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {728-745},
  shortjournal = {Appl. Intell.},
  title        = {Developing a hybrid probabilistic model for short-term wind speed forecasting},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using binary monarch butterfly
optimization. <em>APIN</em>, <em>53</em>(1), 706–727. (<a
href="https://doi.org/10.1007/s10489-022-03554-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm intelligence algorithms have superior performance in searching for the optimal feature subset, where Monarch Butterfly Optimization (MBO) can solve the continuous optimization problem. However, there exist some defects for MBO such as the limited searchable positions, falling into local optimum easily and unsolved binary variables. To address these drawbacks, this paper develops two mechanisms to propose several revisions of binary MBO (BMBO) for metaheuristic feature selection. First, to make MBO suitable to solve the feature selection optimization problems, the S-shaped and V-shaped transfer functions are introduced to convert continuous space into binary, and then force the butterfly to move in the binary search space. Two updated positions of the monarch butterfly population are designed based on these above transfer functions respectively to construct two BMBO models, namely BMBO-S and BMBO-V, as the first mechanism of BMBO. Second, the new step length parameter is proposed to update the position of monarch butterfly individuals. To prevent MBO from falling into the local optimum, the local disturbance and group division strategies are added into MBO to construct new BMBO method. It follows that a mutation rate is employed to enhance the detection stage of BMBO, and the mutation operator-based BMBO (BMBO-M) is designed to avoid the premature convergence of MBO. Third, this fitness function is integrated with the KNN classifier and the weight of the feature subset length to rank the selected feature subset, and a metaheuristic feature selection algorithm with BMBO-M is developed. Experiments applied to nineteen low dimensional UCI datasets and seven high dimensional datasets demonstrate our designed algorithm has great classification efficiency when compared with the other related technologies.},
  archive      = {J_APIN},
  author       = {Sun, Lin and Si, Shanshan and Zhao, Jing and Xu, Jiucheng and Lin, Yaojin and Lv, Zhiying},
  doi          = {10.1007/s10489-022-03554-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {706-727},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection using binary monarch butterfly optimization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-attention based convolutional-LSTM for android malware
detection using network traffics grayscale image. <em>APIN</em>,
<em>53</em>(1), 683–705. (<a
href="https://doi.org/10.1007/s10489-022-03523-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately find malware in a large number of mobile APPs, and determine which family it belongs to is one of the most important challenges in Android malware detection. Existed research focuses on using the extracted features to distinguish Android malicious APPs, and less attention is paid to the category and family classification of Android malware. Meanwhile, feature selection has always been a choose-difficult issue in malware detection with machine learning methods. In this paper, SelAttConvLstm was designed to classify android malware by category and family without manually selecting features. To identify Android malware, we first convert all the network traffic flows into grayscale images according to chronological order through data preprocessing. Second, we design SelAttConvLstm, a deep learning model to detect malicious Android APPs with network flows images. This model can consider both the spatial and temporal features of network flow at the same time. In addition, to improve the performance of the model, self-attention weights are added to focus on different features of the input. Finally, comprehensive experiments are conducted to verify the effectiveness of the detection model. Experimental results showed that our method can not only effectively detect malware, but also classify malware in detail and accurately by category and family.},
  archive      = {J_APIN},
  author       = {Shen, Limin and Feng, Jiayin and Chen, Zhen and Sun, Zhongkui and Liang, Dongkui and Li, Hui and Wang, Yuying},
  doi          = {10.1007/s10489-022-03523-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {683-705},
  shortjournal = {Appl. Intell.},
  title        = {Self-attention based convolutional-LSTM for android malware detection using network traffics grayscale image},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A grouping-attention convolutional neural network for
performance degradation estimation of high-speed train lateral damper.
<em>APIN</em>, <em>53</em>(1), 658–682. (<a
href="https://doi.org/10.1007/s10489-022-03368-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance degradation of lateral damper has a direct impact on the operation of high-speed trains. Existing studies pay more attention to the fault diagnosis of lateral dampers, but the researches on the performance degradation estimation are not comprehensive. In this paper, a novel Grouping-Attention Convolutional Neural Network is proposed to estimate the performance degradation of lateral damper by regression. The proposed structure processes high-speed train vibration signals and fully considers the multi-channel characteristic of the signals. Specifically, the proposed structure consists of a Grouping-Attention part and an Output part. Before high-speed train vibration signals are input into Grouping-Attention part, the signals from different channels are first grouped according to frequency similarity. The Grouping-Attention part contains a Grouping part and an Attention Merging part. The Grouping part extracts features from the grouped signals in parallel. The Attention Merging part applies attention mechanism to merge features from each group. The Output part decodes the features obtained in Grouping-Attention part and outputs the estimation results. The effectiveness and superiority of the proposed structure are verified at 200 km/h and 300 km/h operating speeds.},
  archive      = {J_APIN},
  author       = {Ren, Junxiao and Jin, Weidong and Wu, Yunpu and Sun, Zhang},
  doi          = {10.1007/s10489-022-03368-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {658-682},
  shortjournal = {Appl. Intell.},
  title        = {A grouping-attention convolutional neural network for performance degradation estimation of high-speed train lateral damper},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank robust online distance/similarity learning based on
the rescaled hinge loss. <em>APIN</em>, <em>53</em>(1), 634–657. (<a
href="https://doi.org/10.1007/s10489-022-03419-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge in metric learning is scalability to both size and dimension of input data. Online metric learning algorithms are proposed to address this challenge. Existing methods are commonly based on Passive/Aggressive (PA) approach. Hence, they can rapidly process large volumes of data with an adaptive learning rate. However, these algorithms are based on the Hinge loss and so are not robust against outliers and label noise. We address the challenges by formulating the online Distance/Similarity learning problem with the robust Rescaled Hinge loss function. The proposed model is rather general and can be applied to any PA-based online Distance/Similarity algorithm. To achieve scalability to data dimension, we propose low-rank online Distance/Similarity methods that learn a rectangular projection matrix instead of a full Mahalanobis matrix. The low-rank approaches not only reduce the computational cost but also keep the discrimination power of the learned metrics. Also, current online methods usually assume training triplets or pairwise constraints exist in advance. However, this assumption does not hold, and generating triplets using available batch sampling methods is both time and space consuming. We address this issue by developing an efficient, yet effective robust one-pass triplet construction algorithm. We conduct several experiments on datasets from various applications. The results confirm that the proposed methods significantly outperform state-of-the-art online metric learning methods in the presence of label noise and outliers by a large margin.},
  archive      = {J_APIN},
  author       = {Zabihzadeh, Davood and Tuama, Amar and Karami-Mollaee, Ali and Mousavirad, Seyed Jalaleddin},
  doi          = {10.1007/s10489-022-03419-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {634-657},
  shortjournal = {Appl. Intell.},
  title        = {Low-rank robust online distance/similarity learning based on the rescaled hinge loss},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OAW-GAN: Occlusion-aware warping GAN for unified human video
synthesis. <em>APIN</em>, <em>53</em>(1), 616–633. (<a
href="https://doi.org/10.1007/s10489-022-03527-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Occlusion-Aware Warping GAN (OAW-GAN), a unified Human Video Synthesis (HVS) framework that can uniformly tackle human video motion transfer, attribute editing, as well as inpainting. This is the first work to our knowledge that can handle all these tasks within a one-time trained model. Although existing GAN-based HVS methods have achieved great success, they either can’t preserve appearance details due to the loss of spatial consistency between the synthesized target frames and the input source images, or generate incoherent video results due to the loss of temporal consistency among frames. Besides, most of them lack the ability to create new contents while keeping existing ones, failing especially when some regions in the target are invisible in the source due to self-occlusion. To address these limitations, we first introduce Coarse-to-Fine Flow Warping Network (C2F-FWN) to estimate spatial-temporal consistent transformation between source and target, as well as occlusion mask indicating which parts in the target are invisible in the source. Then, the flow and the mask are scaled and fed into the pyramidal stages of our OAW-GAN, guiding Occlusion-Aware Synthesis (OAS) that can be abstracted into visible part re-utilization and invisible part inpainting at the feature level, which effectively alleviates the self-occlusion problem. Extensive experiments conducted on both human video (i.e., iPER, SoloDance)Keywords are desired. please provide if necessary. and image (i.e., DeepFashion) datasets demonstrate the superiority of our approach to existing state-of-the-arts. We also show that, besides motion transfer task that previous works concern, our framework can further achieve attribute editing and texture inpainting, which paves the way towards unified HVS.},
  archive      = {J_APIN},
  author       = {Wei, Dongxu and Huang, Kejie and Ma, Liyuan and Hua, Jiashen and Lai, Baisheng and Shen, Haibin},
  doi          = {10.1007/s10489-022-03527-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {616-633},
  shortjournal = {Appl. Intell.},
  title        = {OAW-GAN: Occlusion-aware warping GAN for unified human video synthesis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image super-resolution reconstruction based on instance
spatial feature modulation and feedback mechanism. <em>APIN</em>,
<em>53</em>(1), 601–615. (<a
href="https://doi.org/10.1007/s10489-022-03625-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution reconstruction is a research hotspot in the field of computer vision. Traditional image super-resolution reconstruction methods based on deep learning mostly up-sample low-resolution images ignoring categories and instances, which will cause some problems such as unrealistic texture in the reconstructed images or sawtooth phenomenon on the edge of instance. In this manuscript, we propose an image super-resolution reconstruction method based on instance spatial feature modulation and feedback mechanism. First, the prior knowledge of instance spatial features is introduced in the reconstruction process. Instance spatial features of low-resolution images are extracted to modulate super-resolution reconstruction features. Then, based on the feedback mechanism, the modulated low-resolution image features are iteratively optimized for the reconstruction results, so that the model can finally learn instance-level reconstruction ability. Experiments on COCO-2017 show that, compared with traditional deep learning-based image super-resolution reconstruction methods, the proposed method can obtain better image reconstruction results, and the reconstructed images have more realistic instance textures.},
  archive      = {J_APIN},
  author       = {Fu, Lihua and Jiang, Hanxu and Wu, Huixian and Yan, Shaoxing and Wang, Junxiang and Wang, Dan},
  doi          = {10.1007/s10489-022-03625-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {601-615},
  shortjournal = {Appl. Intell.},
  title        = {Image super-resolution reconstruction based on instance spatial feature modulation and feedback mechanism},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse co-attention visual question answering networks based
on thresholds. <em>APIN</em>, <em>53</em>(1), 586–600. (<a
href="https://doi.org/10.1007/s10489-022-03559-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing visual question answering (VQA) models choose to model the dense interactions between each image region and each question word when learning the co-attention between the input images and the input questions. However, to correctly answer a natural language question related to the content of an image usually only requires understanding a few key words of the input question and capturing the visual information contained in a few regions of the input image. The noise information generated by the interactions between the image regions unrelated to the input questions and the question words unrelated to the prediction of the correct answers will distract VQA models and negatively affect the performance of the models. In this paper, to solve this problem, we propose a Sparse Co-Attention Visual Question Answering Network (SCAVQAN) based on thresholds. SCAVQAN concentrates the attention of the model by setting thresholds for attention scores to filter out the image features and the question features that are the most helpful for predicting the correct answers and finally improves the overall performance of the model. Experimental results, ablation studies and attention visualization results based on two benchmark VQA datasets demonstrate the effectiveness and interpretability of our models.},
  archive      = {J_APIN},
  author       = {Guo, Zihan and Han, Dezhi},
  doi          = {10.1007/s10489-022-03559-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {586-600},
  shortjournal = {Appl. Intell.},
  title        = {Sparse co-attention visual question answering networks based on thresholds},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HybridNet: Integrating GCN and CNN for skeleton-based action
recognition. <em>APIN</em>, <em>53</em>(1), 574–585. (<a
href="https://doi.org/10.1007/s10489-022-03436-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) can well-preserve the structure information of the human body. They have achieved outstanding performance in skeleton-based action recognition. Nevertheless, there are still some issues with existing GCN-based methods. First, all channels have the same adjacency matrix. However, the correlations between joints are complex and may drastically change depending on the actions. These correlations are difficult to fit by merely channel-shared adjacency matrices. Second, the interframe edges of graphs only connect the same joints, neglecting the dependencies between the different joints. Fortunately, convolutional neural networks (CNNs) can simultaneously establish the interdependence of all the points in a spatial-temporal patch. Furthermore, CNNs use different kernels among channels. They are more adaptable for modeling complicated dependencies. In this work, we design a hybrid network (HybridNet) to integrate GCNs and CNNs. The HybridNet not only utilizes structural information well but also models complicated relationships between interframe joints properly.Extensive experiments are conducted on three challenging datasets: NTU-RGB+D, NTU-RGB+D 120, and Skeleton-Kinetics. The proposed model achieves state-of-the-art performance on all these datasets by a considerable margin, demonstrating the superiority of our method. The source code is available at https://github.com/kraus-yang/HybridNet .},
  archive      = {J_APIN},
  author       = {Yang, Wenjie and Zhang, Jianlin and Cai, Jingju and Xu, Zhiyong},
  doi          = {10.1007/s10489-022-03436-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {574-585},
  shortjournal = {Appl. Intell.},
  title        = {HybridNet: Integrating GCN and CNN for skeleton-based action recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal collaborative graph for image recommendation.
<em>APIN</em>, <em>53</em>(1), 560–573. (<a
href="https://doi.org/10.1007/s10489-022-03304-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works for personalized recommendation typically emphasize their efforts on learning users’ interests from interactions. However, users make decisions depending on multiple factors, especially various attributes of items like appearance, reviews, price, etc. Therefore, in the case of image recommendation, we strive to unveil users’ interests in a multimodal manner. In this work, we propose a multimodal collaborative graph (MCG) model for image recommendation, which builds users’ interests in both visual and collaborative signals. On visual modality, visual interest filtering is designed to explore the interest non-linearity of users’ interacted images. In the pairwise collaborative module, multi-hop interactions are embedded elaborately to encode the heterogeneous structure of user-image interactions by deep interest propagation. Both visual and collaborative signals are aggregated to embed users and items and match pairwise user-item for the following personalized recommendation. Experiments are conducted on three public real-world datasets. Further analysis demonstrates the compensation capability of visual and collaborative signals in mining users’ interests and verifies the effectiveness of the proposed MCG for image recommendation.},
  archive      = {J_APIN},
  author       = {Jian, Meng and Guo, Jingjing and Shi, Ge and Wu, Lifang and Wang, Zhangquan},
  doi          = {10.1007/s10489-022-03304-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {560-573},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal collaborative graph for image recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Future frame prediction based on generative assistant
discriminative network for anomaly detection. <em>APIN</em>,
<em>53</em>(1), 542–559. (<a
href="https://doi.org/10.1007/s10489-022-03488-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection plays an important role in intelligent surveillance and has attracted increasing attention from researchers in recent years. It is generally regarded as discrimination that cannot be properly represented in most approaches. Despite its importance in probing the scarcity and indefinability of abnormal data during training, designing an effective network is exceptionally complex due to the diversity of the motion information, difficulty of parsing prediction errors, robustness and so on. To improve the ability to extract subtle features between normal and abnormal frames, and to improve the robustness to noise, a generative assistant discriminative network for anomaly detection is proposed. This method detects anomalies by predicting future frames in combination of adversary and cooperation, which mainly consists of the generator, discriminator and assistor. The generator predicts the future frames, while the discriminator distinguishes the predicted future frames from actual future frames. Moreover, by means of noise, the assistor is able to learn from pseudo abnormal future frames and predicted future frames. This helps the generator strengthen the ability to extract the discriminative features between normal and abnormal events. The motion information is used for integration into the predicted future frames. Extensive experiments are conducted on the UCSD Ped1, Ped2, CUHK Avenue and ShanghaiTech datasets. A comparison with the state-of-the-art methods shows the effectiveness and advantages of our method for anomaly detection.},
  archive      = {J_APIN},
  author       = {Li, Chaobo and Li, Hongjun and Zhang, Guoan},
  doi          = {10.1007/s10489-022-03488-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {542-559},
  shortjournal = {Appl. Intell.},
  title        = {Future frame prediction based on generative assistant discriminative network for anomaly detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource and activity clustering based on a hierarchical
cell formation algorithm. <em>APIN</em>, <em>53</em>(1), 532–541. (<a
href="https://doi.org/10.1007/s10489-022-03457-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the resource perspective of process mining and more precisely on the clustering of resources sharing the same behaviors. This problematic was addressed through the use of a well-known facility layout method: cell formation. We propose an algorithm combining the resource perspective and cell formation approach to make the best use of their respective features. We wish to identify both subgroups of resources that perform similar activities and subgroups of activities performed by common resources. This new hierarchical approach provides new insights into the clustering problematic because of its bi-dimensional clustering. Experiments are considered on synthetic and real data.},
  archive      = {J_APIN},
  author       = {Delcoucq, Landelin and Dupiereux-Fettweis, Thomas and Lecron, Fabian and Fortemps, Philippe},
  doi          = {10.1007/s10489-022-03457-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {532-541},
  shortjournal = {Appl. Intell.},
  title        = {Resource and activity clustering based on a hierarchical cell formation algorithm},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting model’s uncertainty and confidences for
adversarial example detection. <em>APIN</em>, <em>53</em>(1), 509–531.
(<a href="https://doi.org/10.1007/s10489-022-03373-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security-sensitive applications that rely on Deep Neural Networks (DNNs) are vulnerable to small perturbations that are crafted to generate Adversarial Examples. The (AEs) are imperceptible to humans and cause DNN to misclassify them. Many defense and detection techniques have been proposed. Model’s confidences and Dropout, as a popular way to estimate the model’s uncertainty, have been used for AE detection but they showed limited success against black- and gray-box attacks. Moreover, the state-of-the-art detection techniques have been designed for specific attacks or broken by others, need knowledge about the attacks, are not consistent, increase model parameters overhead, are time-consuming, or have latency in inference time. To trade off these factors, we revisit the model’s uncertainty and confidences and propose a novel unsupervised ensemble AE detection mechanism that 1) uses the uncertainty method called SelectiveNet, 2) processes model layers outputs, i.e. feature maps, to generate new confidence probabilities. The detection method is called SFAD. Experimental results show that the proposed approach achieves better performance against black- and gray-box attacks than the state-of-the-art methods and achieves comparable performance against white-box attacks. Moreover, results show that SFAD is fully robust against High Confidence Attacks (HCAs) for MNIST and partially robust for CIFAR10 datasets.1},
  archive      = {J_APIN},
  author       = {Aldahdooh, Ahmed and Hamidouche, Wassim and Déforges, Olivier},
  doi          = {10.1007/s10489-022-03373-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {509-531},
  shortjournal = {Appl. Intell.},
  title        = {Revisiting model’s uncertainty and confidences for adversarial example detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Turning traffic volume imputation for persistent missing
patterns with GNNs. <em>APIN</em>, <em>53</em>(1), 491–508. (<a
href="https://doi.org/10.1007/s10489-022-03568-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic volume data at fixed detectors are of great importance to track the time-varying states of urban traffic, and the volume of each turning movement (i.e., continuing straight, turning left, turning right) in an intersection plays a significant role in traffic control. It is a difficult issue to determine the complete road network volume due to the limited coverage of detector deployment. Existing traffic volume imputation methods seem to be hardly tractable to address this persistent missing data problem of fixed detectors because they mainly function by utilizing spatiotemporal patterns of historical data to address the problem of intermittent missing traffic data collected from probe vehicles. Considering the unattainable temporal information of the target movement, this paper aims to develop an end-to-end graph neural network-based imputation approach to impute traffic volume data with persistent missing patterns. We regard the road network as a directed graph with each turning movement as a node and employ a meta-learning-based graph attention mechanism with graph embedding in multiple spatial granularities to fully extract the relationship between the target node and its neighbours. In addition, a multitask learning mechanism is designed to improve the accuracy of the imputation results by setting auxiliary tasks according to the volume conservation constraints. The proposed approach has been validated with extensive simulation experiments and applied to field tests for Shenzhen and Shanghai, China, with considerable accuracy and reliability, satisfying the demand for traffic control. To the best of our knowledge, this is the first time that traffic volume has been imputed for persistent missing patterns with a graph neural network-based method.},
  archive      = {J_APIN},
  author       = {Liu, Ruiqiang and Kan, Yuheng and Zhao, Shuai and Cheng, Bo and Ma, Zian and Wu, Wei},
  doi          = {10.1007/s10489-022-03568-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {491-508},
  shortjournal = {Appl. Intell.},
  title        = {Turning traffic volume imputation for persistent missing patterns with GNNs},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The bombus-terrestris bee optimization algorithm for feature
selection. <em>APIN</em>, <em>53</em>(1), 470–490. (<a
href="https://doi.org/10.1007/s10489-022-03478-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms are one of the well-known methods to solve optimization problems, especially NP-hard problems. These algorithms are mainly developed based on the behavior of the organisms in nature or human behavior. One type of the meta-heuristic algorithm in solving optimization problems is swarm intelligence algorithms which are modeled based on swarm behaviors. In this paper, we focus on the behavior of a sort of bees, bombus-terrestris bee. These bees show several intelligent behaviors, such as finding food, encouraging other cloned bees to find food, learning from other bees to find food, and caring for the queen. Inspired by bombus-terrestris bee behaviors, we introduce an algorithm to solve different kinds of optimization problems, unimodal and multimodal. We mainly focus on solving the feature selection problem based on the binary version of the proposed algorithm. Experimental results show that our proposed method performs better than other meta-heuristic algorithms, such as gray wolf optimization algorithm, Grasshopper optimization algorithm, spotted hyena optimization algorithm, and Harris Hawks Optimizer (HHO), Black Widow Optimization Algorithm (BWO), Artificial bee colony (ABC) algorithm, and Water Strider Algorithm (WSA). We further apply the proposed algorithm on different problems to show the efficiency of the algorithm.},
  archive      = {J_APIN},
  author       = {Tanha, Jafar and Zarei, Zahra},
  doi          = {10.1007/s10489-022-03478-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {470-490},
  shortjournal = {Appl. Intell.},
  title        = {The bombus-terrestris bee optimization algorithm for feature selection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Leveraging mixed distribution of multi-head attention for
sequential recommendation. <em>APIN</em>, <em>53</em>(1), 454–469. (<a
href="https://doi.org/10.1007/s10489-022-03520-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism has been proven to be a useful model for sequence recommendation. Traditional multi-head self-attention architecture can exploit the entire user sequence and adaptively consider consumed items for the next item recommendation. However, the scaling between the number of heads and the size of each head in the multi-head attention model gives rise to a low-rank bottleneck problem, resulting in insufficient expression power and hurting the performance of recommendation model. In this paper, we propose a variant of self-attention called mixed distribution of multi-head attention for sequence recommendation (MMSRec), which constructs the mixed distribution model by weighted averaging of multiple simple distributions, instead of currently dominant methods by increasing the embedding size for addressing the low-rank bottleneck. Extensive experiments on four real-world datasets show that our MMSRec algorithm has significant improvements over state-of-the-art algorithms. Empirical evidence shows that the performance of our recommendation model can be effectively improved by stacking multiple low-rank distributions.},
  archive      = {J_APIN},
  author       = {Zhang, Yihao and Liu, Xiaoyang},
  doi          = {10.1007/s10489-022-03520-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {454-469},
  shortjournal = {Appl. Intell.},
  title        = {Leveraging mixed distribution of multi-head attention for sequential recommendation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An online adjusting RBF neural network for nonlinear system
modeling. <em>APIN</em>, <em>53</em>(1), 440–453. (<a
href="https://doi.org/10.1007/s10489-021-03106-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to improve the prediction accuracy and to obtain a compact structure, an online adjusting radial basis function neural network (OA-RBFNN) is proposed in this paper. The proposed OA-RBFNN realizes online modeling by combining the advantages of the sliding window strategy and clustering algorithm. First, with a small batch of samples in the first sliding window, the network is initialized using a density canopy-based k-means algorithm, and an optimal network structure and its initial parameters are determined automatically. Secondly, through the sliding window movement, the network parameters are adjusted by fine-tuning based on the changed samples, followed by the gradient-based online learning algorithm. Finally, the effectiveness of the proposed OA-RBFNN model is verified by four experiments: the function approximation, Mackey–Glass time series prediction, Lorenz time series prediction, and the effluent BOD prediction in wastewater treatment plant (WWTP), and the prediction accuracy obtained in these four experiments reached 97.11%, 99.25%, 99.69%, and 98.81%, respectively. The results demonstrate that the OA-RBFNN can achieve competitive prediction performance while having a more compact network structure than the existing online RBF neural networks.},
  archive      = {J_APIN},
  author       = {Jia, Lijie and Li, Wenjing and Qiao, Junfei},
  doi          = {10.1007/s10489-021-03106-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {440-453},
  shortjournal = {Appl. Intell.},
  title        = {An online adjusting RBF neural network for nonlinear system modeling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defense against adversarial examples based on wavelet domain
analysis. <em>APIN</em>, <em>53</em>(1), 423–439. (<a
href="https://doi.org/10.1007/s10489-022-03159-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning and deep learning, in particular, have shown powerful performance on different challenging tasks. However, research has shown that deep learning systems can be vulnerable to malicious inputs modified by perturbations crafted to be imperceptible to humans. These adversarial examples can fool the classifier into misclassifying them with high confidence, limiting the applications of deep learning systems, especially where guaranteeing the security of the learning model is necessary. In this paper, we propose a two-level defense method consisting of adversarial detection and input data reconstruction modules against adversarial attacks. The detector differentiates between normal and adversarial examples fed to a deep image classification model, and the reconstructor transforms the detected adversarial images to their corresponding normal samples. Both detection and reconstruction modules are novel and fast signal processing-based techniques depending on analyzing the attacks in the wavelet domain. We show that our defense method is effective against the most state-of-the-art attacks with neither modifying the protected classifier nor utilizing any deep learning model that could be exposed to attacks itself.},
  archive      = {J_APIN},
  author       = {Sarvar, Armaghan and Amirmazlaghani, Maryam},
  doi          = {10.1007/s10489-022-03159-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {423-439},
  shortjournal = {Appl. Intell.},
  title        = {Defense against adversarial examples based on wavelet domain analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for the dynamic and uncertain
vehicle routing problem. <em>APIN</em>, <em>53</em>(1), 405–422. (<a
href="https://doi.org/10.1007/s10489-022-03456-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and real-time tracking for real-world urban logistics has become a popular research topic in the field of intelligent transportation. While the routing of urban logistic service is usually accomplished via complex mathematical and analytical methods. However, the nature and scope of real-world urban logistics are highly dynamic, and the existing optimization technique cannot precisely formulate the dynamic characteristics of the route. To ensure customers’ demands are met, planners need to respond to these changes quickly (sometimes instantaneously). This paper proposes the formulation of a novel deep reinforcement learning framework to solve a dynamic and uncertain vehicle routing problem (DU-VRP), whose objective is to meet the uncertain servicing needs of customers in a dynamic environment. Considering uncertain information about the demands of customers in this problem, the partial observation Markov decision process is designed to frequently observe the changes in customers’ demands in a real-time decision support system that consists of a deep neural network with a dynamic attention mechanism. Besides, a cutting-edge reinforcement learning algorithm is presented to control the value function of the DU-VRP for better training the routing process’s dynamics and uncertainty. Computational experiments are conducted considering different data sources to obtain satisfactory solutions of the DU-VRP.},
  archive      = {J_APIN},
  author       = {Pan, Weixu and Liu, Shi Qiang},
  doi          = {10.1007/s10489-022-03456-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {405-422},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning for the dynamic and uncertain vehicle routing problem},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent threats solution for object detection and
resource perspective rectification of distorted anomaly identification
card images in cloud environments. <em>APIN</em>, <em>53</em>(1),
385–404. (<a href="https://doi.org/10.1007/s10489-022-03261-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical character recognition (OCR) in general and identity card recognition (IDOCR) in particular, embedded in mobile cameras, are being interested in and attracting the research community in Vietnam. Due to the variety of camera devices that capture images, the IDOCR systems often face a lot of difficulties, typically as the input images are distorted, rotated, scaled, translated, or sheared lead to weak recognition accuracy results. In this paper, we focus on the distortion document image problem of Vietnamese IDOCR system and propose an effective method to solve this problem. Our solution includes three main stages: (i) ROI detection; (ii) Image segmentation and corner points detection. (iii) distorted image area rectification. The accuracy and execution time of the method are verified on a large amount of data collected from the real environment with very differently lighting condition, shooting distance, camera angle and image size. The experimental results show that our method gains high accuracy, real-time calculation and is able to deal with the distorted input images.},
  archive      = {J_APIN},
  author       = {Tan, Nguyen Thi Thanh and Van Huy, Huynh and Kim, Do Hyeun and Ngoc, Le Anh},
  doi          = {10.1007/s10489-022-03261-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {385-404},
  shortjournal = {Appl. Intell.},
  title        = {An intelligent threats solution for object detection and resource perspective rectification of distorted anomaly identification card images in cloud environments},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised anomaly detection based method of risk
evaluation for road traffic accident. <em>APIN</em>, <em>53</em>(1),
369–384. (<a href="https://doi.org/10.1007/s10489-022-03501-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elevated road plays a very important role as corridors in urban traffic network, and the occurrence of traffic accidents often causes a great impact. In that sense, we propose a unique and enhanced Autoencoder (AE) to identify elevated road traffic accident (RTA) risk based on traffic anomaly detection in an unsupervised manner. An attention mechanism is introduced to extract the traffic condition features considering traffic spatiotemporal variation characteristics. Additionally, an enhanced loss is also introduced to optimize the ability of unsupervised anomaly detection (UAD) approach to detect anomalous RTA risk and persistent anomalous traffic condition, which can significantly boost the anomaly detection performance using the contaminated traffic condition datasets. To assess the RTA risk, the evaluation mechanism and discriminant threshold are used to quantitatively analyze the detected abnormal traffic condition. Finally, experiments on real traffic datasets demonstrate the effectiveness of the model.},
  archive      = {J_APIN},
  author       = {Zhao, Chao and Chang, Xiaokun and Xie, Tian and Fujita, Hamido and Wu, Jian},
  doi          = {10.1007/s10489-022-03501-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {369-384},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised anomaly detection based method of risk evaluation for road traffic accident},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection based on few-shot learning via
instance-level feature correlation and aggregation. <em>APIN</em>,
<em>53</em>(1), 351–368. (<a
href="https://doi.org/10.1007/s10489-022-03399-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of novel foregrounds only utilizing scarce annotated images, namely few-shot object detection, makes a detector no longer dependent on large-scale instantiated sets. The realistic challenge might lie in establishing the correlation of few instances and balancing the sensitivity between base and novel categories. In this paper, we propose a few-shot detector using instance-level feature correlation based on an interactive self-attention module to deeply mine the discriminating representations from scarce novel instances. Besides, using an extended soft threshold shrinkage, a feature aggregation procedure is introduced to eliminate redundant information while enhancing the representation sensitivity between base and novel categories. In the training phase, an orthogonal loss is applied to further enhance the feature distinguishability of inter-categories. Finally, we evaluate related competitive detectors on both benchmarks PASCAL-VOC07/12 and MS-COCO, with the results verifying the superior detection precision on AP, mAP and AR measurements of the proposed approach.},
  archive      = {J_APIN},
  author       = {Wang, Meng and Ning, Hongwei and Liu, Haipeng},
  doi          = {10.1007/s10489-022-03399-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {351-368},
  shortjournal = {Appl. Intell.},
  title        = {Object detection based on few-shot learning via instance-level feature correlation and aggregation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSCRL: Fine-grained object retrieval with switched shifted
centralized ranking loss. <em>APIN</em>, <em>53</em>(1), 336–350. (<a
href="https://doi.org/10.1007/s10489-022-03287-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval is an attractive task in computer vision that aims at browsing, searching, and returning images from a large database of digital images after delivering a retrieval query. Numerous works have focused on fine-grained object retrieval (FGOR) because it is extremely challenging and of great value in practical application. Due to the large diversity within a class and the small diversity across different classes of fine-grained objects data, a convolutional neural network (CNN) is a powerful extractor that can be used to obtain fine-grained features for distinguishing tiny variations between classes. As an indispensable part of a convolutional neural network model, the loss function is of critical importance for feature extraction. In this work, based on the global structure loss function, we propose a variant of softmax loss, named switched shifted softmax loss, to potentially reduce the overfitting phenomenon of the model. Comparative experiments with different backbone structures verify that the developed loss function with trivial transformation enhances the fine-grained retrieval performance of deep learning methods1. Furthermore, additional experiments of fine-grained object classification and person re-identification (re-ID) prove that our method has a wide spectrum of applicability to other tasks.},
  archive      = {J_APIN},
  author       = {Zeng, Xianxian and Liu, Shun and Wang, Xiaodong and Ye, Peichu and Lai, Guanyu},
  doi          = {10.1007/s10489-022-03287-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {336-350},
  shortjournal = {Appl. Intell.},
  title        = {SSCRL: Fine-grained object retrieval with switched shifted centralized ranking loss},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptively weighted three-way decision oversampling: A
cluster imbalanced-ratio based approach. <em>APIN</em>, <em>53</em>(1),
312–335. (<a href="https://doi.org/10.1007/s10489-022-03394-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oversampling is an effective method to fulfill imbalanced learning, owing to its easy-to-go capability of achieving the balance by synthesizing new samples. However, precise synthesizing in oversampling is always a significant yet challenging task due primarily to various problems such as noise samples, within-class imbalance, and selection of boundary samples. In order to solve these problems, this paper proposes a new improved oversampling method, called adaptively weighted three-way decision oversampling (AWTDO) for imbalanced learning. The working principle of the proposed AWTDO method includes three main steps. Firstly, remove the noise sample roughly, implement K-means clustering algorithm on raw data to establish multi-clusters, and calculate imbalanced ratio of each cluster. Secondly, classify all clusters into three categories according to their imbalanced ratios and three-way decision, such as positive domain, boundary domain, and negative domain. Accordingly, assign the number of synthetic samples distinguishably to each cluster regarding its category. Thirdly, determinatively select the target minority sample in each cluster and generate the new synthetic samples by using the stochastic linear interpolation technique according to different sampling weight. Finally, some comparative experiments on public datasets have shown that the proposed AWTDO method outperforms nine state-of-the-art oversampling methods.},
  archive      = {J_APIN},
  author       = {Wang, Xinli and Gong, Juan and Song, Yan and Hu, Jianhua},
  doi          = {10.1007/s10489-022-03394-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {312-335},
  shortjournal = {Appl. Intell.},
  title        = {Adaptively weighted three-way decision oversampling: A cluster imbalanced-ratio based approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KSCB: A novel unsupervised method for text sentiment
analysis. <em>APIN</em>, <em>53</em>(1), 301–311. (<a
href="https://doi.org/10.1007/s10489-022-03389-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models (e.g. Convolutional Neural Networks (CNN) and Long Short-Term Memories (LSTM)), have been successfully applied to text sentiment analysis. However, the class-imbalance and unlabeled corpus still limit the accuracy of text sentiment classification. To overcome the two issues, in this work, we propose a new classification model named KSCB (integrating K-means++, SMOTE, CNN and Bi-LSTM models) for text sentiment analysis. The K-means++-SMOTE (combining K-means++ and SMOTE) operation in KSCB is firstly used to cluster sentiment text, and further generate new corpora via imbalance ratio to adjust data distribution. Then the loss function between K-means++-SMOTE and CNN-Bi-LSTM (combining CNN and Bi-LSTM) is applied to construct end-to-end learning. Different from other deep learning models, our proposed method KSCB can adjust data distribution for different sentiment corpora via KSCB optimization. We have applied KSCB into the balanced and imbalanced corpora, and the comparison results show that KSCB is better than or comparable to the other five state-of-the-art methods in text sentiment classification. Moreover, the ablation experiment in the balanced and imbalanced corpora have demonstrated the effectiveness of KSCB in text sentiment analysis.},
  archive      = {J_APIN},
  author       = {Jiang, Weili and Zhou, Kangneng and Xiong, Chenchen and Du, Guodong and Ou, Chubin and Zhang, Junpeng},
  doi          = {10.1007/s10489-022-03389-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {301-311},
  shortjournal = {Appl. Intell.},
  title        = {KSCB: A novel unsupervised method for text sentiment analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient quantile tracking using an oracle. <em>APIN</em>,
<em>53</em>(1), 289–300. (<a
href="https://doi.org/10.1007/s10489-022-03489-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is a well-known issue that arises when working with data streams. In this paper, we present a procedure that allows a quantile tracking procedure to cope with concept drift. We suggest using expected quantile loss, a popular loss function in quantile regression, to monitor the quantile tracking error, which, in turn, is used to efficiently adapt to concept drift. The suggested procedures adapt efficiently to concept drift, and the tracking performance is close to theoretically optimal. The procedures were further applied to three real-life streaming data sets related to Twitter event detection, activity recognition, and stock trading. The results show that the procedures are efficient at adapting to concept drift, thereby documenting the real-world applicability of the procedures. We further used asymptotic theory from statistics to show the appealing theoretical property that, if the data stream distribution is stationary over time, the procedures converge to the true quantile.},
  archive      = {J_APIN},
  author       = {Hammer, Hugo L. and Yazidi, Anis and Riegler, Michael A. and Rue, Håvard},
  doi          = {10.1007/s10489-022-03489-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {289-300},
  shortjournal = {Appl. Intell.},
  title        = {Efficient quantile tracking using an oracle},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multi-module integrated intrusion detection system
for high-dimensional imbalanced data. <em>APIN</em>, <em>53</em>(1),
272–288. (<a href="https://doi.org/10.1007/s10489-022-03361-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high dimension, complexity, and imbalance of network data are hot issues in the field of intrusion detection. Nowadays, intrusion detection systems face some challenges in improving the accuracy of minority classes detection, detecting unknown attacks, and reducing false alarm rates. To address the above problems, we propose a novel multi-module integrated intrusion detection system, namely GMM-WGAN-IDS. The system consists of three parts, such as feature extraction, imbalance processing, and classification. Firstly, the stacked autoencoder-based feature extraction module (SAE module) is proposed to obtain a deeper representation of the data. Secondly, on the basis of combining the clustering algorithm based on gaussian mixture model and the wasserstein generative adversarial network based on gaussian mixture model, the imbalance processing module (GMM-WGAN) is proposed. Thirdly, the classification module (CNN-LSTM) is designed based on convolutional neural network (CNN) and long short-term memory (LSTM). We evaluate the performance of GMM-WGAN-IDS on the NSL-KDD and UNSW-NB15 datasets, comparing it with other intrusion detection methods. Finally, the experimental results show that our proposed GMM-WGAN-IDS outperforms the state-of-the-art methods and achieves better performance.},
  archive      = {J_APIN},
  author       = {Cui, Jiyuan and Zong, Liansong and Xie, Jianhua and Tang, Mingwei},
  doi          = {10.1007/s10489-022-03361-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {272-288},
  shortjournal = {Appl. Intell.},
  title        = {A novel multi-module integrated intrusion detection system for high-dimensional imbalanced data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). FMFO: Floating flame moth-flame optimization algorithm for
training multi-layer perceptron classifier. <em>APIN</em>,
<em>53</em>(1), 251–271. (<a
href="https://doi.org/10.1007/s10489-022-03484-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most popular artificial neural networks, multi-layer perceptron (MLP) has been employed to solve classification problems in many applications. The main challenge in MLP application is finding the ideal set of network connection weights and biases in the training process, which minimizes the error of MLP in processing datasets. To efficiently address this challenge, numerous swarm intelligence (SI) algorithms with powerful search capabilities have been adopted for training MLP classifiers. However, these existing algorithms often suffer from problems of local optima stagnation, premature convergence, and inefficient search. In this study, a novel floating flame moth-flame optimization (FMFO) algorithm with remarkable exploitation and exploration search capabilities is proposed, offering an advantageous option for training MLP classifiers. To verify the performance of the proposed FMFO in training MLP classifiers, the FMFO-based MLP training approach (FMFO-MLP) is evaluated on eleven classification datasets that represent a wide range of the variable dimension scale. In addition, some recently developed well-known and state-of-the-art SI algorithms are applied to compare with the proposed FMFO. Experimental results demonstrate that the proposed FMFO outperforms the other competing algorithms in terms of approximating the optimal objective function value and achieving classification accuracy. Moreover, the proposed FMFO achieves a competitive computational efficiency in the experiment, confirming that it is an efficient optimizer for training MLP classifiers in practical applications.},
  archive      = {J_APIN},
  author       = {Yang, Zhenlun},
  doi          = {10.1007/s10489-022-03484-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {251-271},
  shortjournal = {Appl. Intell.},
  title        = {FMFO: Floating flame moth-flame optimization algorithm for training multi-layer perceptron classifier},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MTSDet: Multi-scale traffic sign detection with attention
and path aggregation. <em>APIN</em>, <em>53</em>(1), 238–250. (<a
href="https://doi.org/10.1007/s10489-022-03459-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem that existing traffic signs are not easily detected leading to low detection performance due to their small sizes and external factors such as weather conditions, this paper proposes a traffic sign detection method, MTSDet (Multi-scale Traffic Sign Detection with attention and path aggregation), which focuses on the multi-scale detection problem and effectively improves the detection performance. First, the method efficiently extracts semantic features by introducing the Attention Mechanism Network(AMNet), and then feeds the multi-scale semantic features into Path Aggregation Feature Pyramid Network(PAFPN) for multi-scale feature fusion to obtain multi-scale advanced semantic features. Finally, the multi-scale advanced semantic feature map is deformable interest pooled to effectively enhance the multi-scale object detection modeling capability. In this paper, the above method is validated by two classical datasets, German traffic sign detection dataset and Chinese traffic sign detection dataset, which achieve 92.9% and 94.3% mAP, respectively, and have obvious detection accuracy improvement when compared with other classical advanced algorithms, effectively proving the superiority and generalization of the algorithm in this paper. Code is available at https://github.com/why529913/MTSDet},
  archive      = {J_APIN},
  author       = {Wei, Hongyang and Zhang, Qianqian and Qian, Yurong and Xu, Zheng and Han, Jingjing},
  doi          = {10.1007/s10489-022-03459-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {238-250},
  shortjournal = {Appl. Intell.},
  title        = {MTSDet: Multi-scale traffic sign detection with attention and path aggregation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A three-way pythagorean fuzzy correlation coefficient
approach and its applications in deciding some real-life problems.
<em>APIN</em>, <em>53</em>(1), 226–237. (<a
href="https://doi.org/10.1007/s10489-022-03415-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation coefficient (CC) is a reliable information measure for measuring interrelationship between Pythagorean fuzzy sets (PFSs). Some approaches for calculating CC of PFSs have been considered. These hitherto approaches assess only the strength of relationship between PFSs, and are described within the interval [0,1]. This paper proposes a three-way approach for the computation of CC between PFSs by using the concepts of variance and covariance, respectively. This new approach is defined within the interval [− 1,1] akin to classical statistics, shows the strength of relationship between the considered PFSs and indicates whether the PFSs are either positively or negatively correlated. By including the three conventional parameters of PFSs in the proposed technique, the possibility of error due to information leakage is reasonably minimized. The new technique is validated with some theoretical results to show its suitability as reliable information measure. Some numerical examples are considered to show the edges of the new methods over similar methods. From the comparative analysis, the proposed methods of computing CCPFSs give more reliable and reasonable results compare to similar existing methods as presented in Table 13. Certain decision-making problems involving recognition of patterns and diagnostic medicine are resolved with the aid of the new method. The three-way technique of computing correlation coefficient between PFSs can solve decision-making problems that are multi-attributes in nature.},
  archive      = {J_APIN},
  author       = {Ejegwa, Paul Augustine and Wen, Shiping and Feng, Yuming and Zhang, Wei and Liu, Jinkui},
  doi          = {10.1007/s10489-022-03415-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {226-237},
  shortjournal = {Appl. Intell.},
  title        = {A three-way pythagorean fuzzy correlation coefficient approach and its applications in deciding some real-life problems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new comprehensive automatic fault detection method for
rotating machinery using HmvAAPE and VNWOA-KELM. <em>APIN</em>,
<em>53</em>(1), 204–225. (<a
href="https://doi.org/10.1007/s10489-022-03505-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to efficiently and automatically identify the faults of rotating machinery, so as to avoid the dangers and losses caused by them, this paper proposes a new fault feature extraction method for rotating machinery named Hierarchical Multi-variate Amplitude-aware Permutation Entropy (HmvAAPE), which has integrated the advantages of Amplitude-aware Permutation Entropy (AAPE), multi-channel analysis method and hierarchical decomposition method. Therefore, the features extracted by this feature extraction method can contain more complete fault information. The t-SNE algorithm is chosen to conduct dimensional reduction of features and the Kernel Extreme Learning Machine optimized by Von Neumann Topology Whale Optimization Algorithm (VNWOA-KELM) is proposed to learn fault characteristics and classify faults automatically. By designing bearing and gearbox fault experiments and collecting their fault data to verify the effectiveness of the proposed method, it can be obtained that the average classification accuracy of this method can reach 98.9%. Through comparative experiments, conclusions can be made that this method can get both higher accuracy and higher stability at the same time.},
  archive      = {J_APIN},
  author       = {Gong, Jiancheng and Yang, Xiaoqiang and Han, Jun and Shen, Jinxing and Zhou, Fuming and Liu, Wuqiang},
  doi          = {10.1007/s10489-022-03505-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {204-225},
  shortjournal = {Appl. Intell.},
  title        = {A new comprehensive automatic fault detection method for rotating machinery using HmvAAPE and VNWOA-KELM},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative-region attention and orthogonal-view
generation model for vehicle re-identification. <em>APIN</em>,
<em>53</em>(1), 186–203. (<a
href="https://doi.org/10.1007/s10489-022-03420-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (Re-ID) is urgently demanded to alleviate the pressure caused by the increasingly onerous task of urban traffic management. Multiple challenges hamper the applications of vision-based vehicle Re-ID methods: (1) The appearances of different vehicles of the same brand/model are often similar; However, (2) the appearances of the same vehicle differ significantly from different viewpoints. Previous methods mainly use manually annotated multi-attribute datasets to assist the network in getting detailed cues and in inferencing multi-view to improve the vehicle Re-ID performance. However, finely labeled vehicle datasets are usually unattainable in real application scenarios. Hence, we propose a Discriminative-Region Attention and Orthogonal-View Generation (DRA-OVG) model, which only requires identity (ID) labels to conquer the multiple challenges of vehicle Re-ID. The proposed DRA model can automatically extract the discriminative region features, which can distinguish similar vehicles. And the OVG model can generate multi-view features based on the input view features to reduce the impact of viewpoint mismatches. Finally, the distance between vehicle appearances is presented by the discriminative region features and multi-view features together. Therefore, the significance of pairwise distance measure between vehicles is enhanced in a complete feature space. Extensive experiments substantiate the effectiveness of each proposed ingredient, and experimental results indicate that our approach achieves remarkable improvements over the state-of-the-art vehicle Re-ID methods on VehicleID and VeRi-776 datasets.},
  archive      = {J_APIN},
  author       = {Li, Huadong and Wang, Yuefeng and Wei, Ying and Wang, Lin and Li, Ge},
  doi          = {10.1007/s10489-022-03420-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {186-203},
  shortjournal = {Appl. Intell.},
  title        = {Discriminative-region attention and orthogonal-view generation model for vehicle re-identification},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining predictions and attacks in federated learning via
random forests. <em>APIN</em>, <em>53</em>(1), 169–185. (<a
href="https://doi.org/10.1007/s10489-022-03435-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is used for various purposes that are critical to human life. However, most state-of-the-art AI algorithms are black-box models, which means that humans cannot understand how such models make decisions. To forestall an algorithm-based authoritarian society, decisions based on machine learning ought to inspire trust by being explainable. For AI explainability to be practical, it must be feasible to obtain explanations systematically and automatically. A usual methodology to explain predictions made by a (black-box) deep learning model is to build a surrogate model based on a less difficult, more understandable decision algorithm. In this work, we focus on explaining by means of model surrogates the (mis)behavior of black-box models trained via federated learning. Federated learning is a decentralized machine learning technique that aggregates partial models trained by a set of peers on their own private data to obtain a global model. Due to its decentralized nature, federated learning offers some privacy protection to the participating peers. Nonetheless, it remains vulnerable to a variety of security attacks and even to sophisticated privacy attacks. To mitigate the effects of such attacks, we turn to the causes underlying misclassification by the federated model, which may indicate manipulations of the model. Our approach is to use random forests containing decision trees of restricted depth as surrogates of the federated black-box model. Then, we leverage decision trees in the forest to compute the importance of the features involved in the wrong predictions. We have applied our method to detect security and privacy attacks that malicious peers or the model manager may orchestrate in federated learning scenarios. Empirical results show that our method can detect attacks with high accuracy and, unlike other attack detection mechanisms, it can also explain the operation of such attacks at the peers’ side.},
  archive      = {J_APIN},
  author       = {Haffar, Rami and Sánchez, David and Domingo-Ferrer, Josep},
  doi          = {10.1007/s10489-022-03435-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {169-185},
  shortjournal = {Appl. Intell.},
  title        = {Explaining predictions and attacks in federated learning via random forests},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H-BLS: A hierarchical broad learning system with deep and
sparse feature learning. <em>APIN</em>, <em>53</em>(1), 153–168. (<a
href="https://doi.org/10.1007/s10489-022-03498-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) is an emerging machine learning algorithm with high efficiency and good approximation capability. It has been proved that BLS can learn hundreds of times faster than traditional deep learning algorithms while providing a comparable or even better generalization performance. Owing to its superb efficiency and powerful learning ability, the BLS is attracting increasing attention from machine learning community and can be considered as an alternative to deep learning in some situations. However, due to its shallow structure, the feature learning of BLS is not sufficient and which may probably limit its learning performance. For this issue, this paper proposes a novel hierarchical broad learning system (H-BLS) with deep and sparse feature learning. Different from the original BLS which conducts feature learning simply using a single-layer function mapping, the H-BLS adopts a hierarchical feature learning framework with multi-layer and multi-group structure to extract high-level and rich feature information from the original input, so as to improve the feature representation capability of the model. Meanwhile, in the hierarchical feature learning process of H-BLS, a new l1-constrained sparse autoencoder is employed and embedded in each layer of the framework for feature reconstruction, so as to eliminate redundancy of the input and generate more sparse and compact feature representations, thus further enhancing its learning performance. The learning ability of the proposed H-BLS is firstly evaluated by ten commonly used regression data sets, and the experimental results show that H-BLS performs better compared with several representative learning algorithms such as SVM, LSSVM, ELM, BLS and two recently proposed BLS variants. Moreover, the H-BLS also shows advantages over the state-of-the-art methods in terms of classification accuracy and training time on image classification problems.},
  archive      = {J_APIN},
  author       = {Guo, Wei and Chen, Shuangshuang and Yuan, Xiaofeng},
  doi          = {10.1007/s10489-022-03498-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {153-168},
  shortjournal = {Appl. Intell.},
  title        = {H-BLS: A hierarchical broad learning system with deep and sparse feature learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inner-imaging 3D attention module for residual network.
<em>APIN</em>, <em>53</em>(1), 141–152. (<a
href="https://doi.org/10.1007/s10489-022-03225-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an Inner-Imaging three-dimensional (3D) attentional feature fusion module for a residual network, which is a simple yet effective approach for residual networks. In our attention module, we constructed a 3D soft attention feature map to refine the input feature. The map fuses the attentional features from different dimensions, including channel and spatial axes, to create a 3D attention map. Then, we implemented a feature fusion module to further fuse the attentional features. Lastly, the attention module outputs a 3D soft attention map that is applied to the residual branch. The attention module can also model the relationship between attentional features from different dimensions and achieve the interaction between attentional features. This function allows our attention module to acquire more attentional features. To demonstrate the effectiveness of our method, extensive experiments were conducted on several computer vision benchmark datasets, including ImageNet 2012 and Microsoft COCO (MS COCO) 2017 datasets. The experimental results show that our method performed better than the baseline methods in the tasks of image classification, object detection, and instance segmentation tasks.},
  archive      = {J_APIN},
  author       = {Liu, Wenjie and Wu, Guoqing and Ren, Fuji and Shi, Quan},
  doi          = {10.1007/s10489-022-03225-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {141-152},
  shortjournal = {Appl. Intell.},
  title        = {Inner-imaging 3D attention module for residual network},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A new approach for efficiently mining frequent weighted
utility patterns. <em>APIN</em>, <em>53</em>(1), 121–140. (<a
href="https://doi.org/10.1007/s10489-022-03580-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining patterns that satisfy both frequency and utility constraints is an interesting problem in data mining. Currently, there are two main approaches to solving this problem: the first considers these two factors separately (skyline frequent-utility patterns), and the second combines these two factors to form a composite measure (frequent weighted utility patterns). In the second approach, the weighted utility support (wus), which represents the percentage occupancy of the total weighted utility of all transactions in a quantitative database, is used. This approach has the advantage of satisfying the downward-closure property, so it is easy to implement divide-and-conquer and pruning strategies on the candidate search space, as shown by algorithms such as MWIT-FWUI, MBiS-FWUI, and WUN-Miner. This study proposes an SWUN-list structure, a shortened version of the N-list structure, to effectively represent and mine frequent weighted utility patterns (FWUPs) in quantitative databases based on the second approach. Several theorems are also developed to quickly compute the wus values of patterns based on their SWUN-lists, as well as to determine the wus values of some special patterns without computation. The experiments are conducted on a variety of datasets to compare the proposed algorithm with the state-of-the-art methods. Experimental results confirm that the proposed method is superior to the existing methods for mining FWUPs in quantitative databases.},
  archive      = {J_APIN},
  author       = {Nguyen, Ham and Le, Nguyen and Bui, Huong and Le, Tuong},
  doi          = {10.1007/s10489-022-03580-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {121-140},
  shortjournal = {Appl. Intell.},
  title        = {A new approach for efficiently mining frequent weighted utility patterns},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical image watermark decoder by modeling local RDWT
difference domain singular values with bivariate weighted weibull
distribution. <em>APIN</em>, <em>53</em>(1), 96–120. (<a
href="https://doi.org/10.1007/s10489-022-03536-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For any watermarking scheme, how to simultaneously improve robustness, invisibility and watermarking capacity has always been a problem that the scientific research community is committed to solving. Statistical modeling technology is an effective means to solve this problem. In this paper, a new method of digital image watermarking technology is proposed for the first time by using Bivariate Weighted Weibull distribution to model Redundant Discrete Wavelet Transform (RDWT) difference domain local Singular Value (SVD) coefficients. First, according to the decomposition characteristics of RDWT, the difference between the first scale and the second scale subband is calculated, and local SVD is performed on the difference subband to obtain the modeling object. In the experiment, the embedding carrier is nonlinear. Although the linear embedding method is still used in the embedding process, we use the entropy value to modify the different number of coefficients in different coefficient blocks to embed watermark bits, so it meets the requirements of image nonlinear characteristics. Then, the Bivariate Weighted Weibull distribution is used for modeling and constructing the decoder, which can effectively take advantage of the strong correlation between the subband directions of the difference. Finally, through a large number of simulation experiments, the performance of the proposed algorithm is comprehensively evaluated. The verification results confirm that the proposed method can still obtain excellent anti-attack effect and invisibility when the watermark capacity is large.},
  archive      = {J_APIN},
  author       = {Wang, Xiangyang and Yao, Yao and Yu, Yang and Niu, Panpan and Yang, Hongying},
  doi          = {10.1007/s10489-022-03536-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {96-120},
  shortjournal = {Appl. Intell.},
  title        = {Statistical image watermark decoder by modeling local RDWT difference domain singular values with bivariate weighted weibull distribution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VARGAN: Variance enforcing network enhanced GAN.
<em>APIN</em>, <em>53</em>(1), 69–95. (<a
href="https://doi.org/10.1007/s10489-022-03199-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are one of the most widely used generative models. GANs can learn complex multi-modal distributions, and generate real-like samples. Despite the major success of GANs in generating synthetic data, they might suffer from unstable training process, and mode collapse. In this paper, we propose a new GAN architecture called variance enforcing GAN (VARGAN), which incorporates a third network to introduce diversity in the generated samples. The third network measures the diversity of the generated samples, which is used to penalize the generator’s loss for low diversity samples. The network is trained on the available training data and undesired distributions with limited modality. On a set of synthetic and real-world image data, VARGAN generates a more diverse set of samples compared to the recent state-of-the-art models. High diversity and low computational complexity, as well as fast convergence, make VARGAN a promising model to alleviate mode collapse.},
  archive      = {J_APIN},
  author       = {Mohammadjafari, Sanaz and Cevik, Mucahit and Basar, Ayse},
  doi          = {10.1007/s10489-022-03199-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {69-95},
  shortjournal = {Appl. Intell.},
  title        = {VARGAN: Variance enforcing network enhanced GAN},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UGMINE: Utility-based graph mining. <em>APIN</em>,
<em>53</em>(1), 49–68. (<a
href="https://doi.org/10.1007/s10489-022-03385-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent pattern mining extracts most frequent patterns from databases. These frequency-based frameworks have limitations in representing users’ interest in many cases. In business decision-making, not all patterns are of the same importance. To solve this problem, utility has been incorporated in transactional and sequential databases. A graph is a relatively complex but highly useful data structure. Although frequency-based graph mining has many real-life applications, it has limitations similar to other frequency-based frameworks. To the best of our knowledge, there is no complete framework developed for mining utility-based patterns from graphs. In this work, we propose a complete framework for utility-based graph pattern mining. A complete algorithm named UGMINE is presented for high utility subgraph mining. We introduce a pruning technique named RMU pruning for effective pruning of the candidate pattern search space that grows exponentially. We conduct experiments on various datasets to analyze the performance of the algorithm. Our experimental results show the effectiveness of UGMINE to extract high utility subgraph patterns.},
  archive      = {J_APIN},
  author       = {Alam, Md. Tanvir and Roy, Amit and Ahmed, Chowdhury Farhan and Islam, Md. Ashraful and Leung, Carson K.},
  doi          = {10.1007/s10489-022-03385-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {49-68},
  shortjournal = {Appl. Intell.},
  title        = {UGMINE: Utility-based graph mining},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust deep kernel-based fuzzy clustering with spatial
information for image segmentation. <em>APIN</em>, <em>53</em>(1),
23–48. (<a href="https://doi.org/10.1007/s10489-022-03255-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms with deep neural network has attracted wide attention to scholars. A deep fuzzy K-means clustering algorithm model on adaptive loss function and entropy regularization (DFKM) is proposed by combining automatic encoder and clustering algorithm. Although it introduces adaptive loss function and entropy regularization to improve the robustness of the model, its segmentation effect is not ideal for high noise. The research purpose of this paper is to focus on the anti-noise performance of image segmentation. Therefore, on the basis of DFKM, this paper focus on image segmentation, combine neighborhood median and mean information of current pixel, introduce neighborhood information of membership degree, and extend Euclidean distance to kernel space by using kernel function, propose a dual-neighborhood information constrained deep fuzzy clustering based on kernel function (KDFKMS). A large number of experimental results show that compared with DFKM and classical image segmentation algorithms, this algorithm has stronger anti-noise robustness.},
  archive      = {J_APIN},
  author       = {Lei, Lujia and Wu, Chengmao and Tian, Xiaoping},
  doi          = {10.1007/s10489-022-03255-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {23-48},
  shortjournal = {Appl. Intell.},
  title        = {Robust deep kernel-based fuzzy clustering with spatial information for image segmentation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linguistic pythagorean hesitant fuzzy matrix game and its
application in multi-criteria decision making. <em>APIN</em>,
<em>53</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10489-022-03442-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new Linguistic Pythagorean Hesitant Fuzzy Set (LPHFS) is introduced by considering the notions of linguistic fuzzy set and Pythagorean hesitant fuzzy set. LPHFS is a suitable path to deal with the hesitant situation in decision making, which is characterized by linguistic membership and non-membership degrees. Multi-Criteria Decision Making (MCDM) process determines multiple competing criteria in decision making. The traditional decision making approaches assume that each player is independent. But in real world competitive situation, the real fact is that each player tries to maximize individual benefit which causes a negative impact on other player. Here we propose a Linguistic Pythagorean Hesitant Fuzzy (LPHF) distance measure based on game theoretical framework to terminate the cross-influence problem. So our intention is to explore the generalized hybrid Euclidean distance measures of LPHFS. Then we analyze the application of LPHFS to MCDM game by using Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The LPHFS is assumed to explore the uncertainty of Decision Makers (DMs), and the game theory is used to optimize the combination of criteria in interactive conditions. A modified version of TOPSIS and Ambika method are designed in the context of MCDM game with LPHFS. Finally, two real-life problems are considered to illustrate the applicability and feasibility of our proposed method, and then a comparison analysis is drawn among the obtained results with the existing methods to depict the usefulness of it.},
  archive      = {J_APIN},
  author       = {Jana, Jishu and Roy, Sankar Kumar},
  doi          = {10.1007/s10489-022-03442-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Linguistic pythagorean hesitant fuzzy matrix game and its application in multi-criteria decision making},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
