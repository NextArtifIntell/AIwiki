<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftml---6">FTML - 6</h2>
<ul>
<li><details>
<summary>
(2023). Reinforcement learning, bit by bit. <em>FTML</em>,
<em>16</em>(6), 733–865. (<a
href="https://doi.org/10.1561/2200000097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We discuss concepts and regret analysis that together offer principled guidance. This line of thinking sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that highlight data efficiency.},
  archive      = {J_FTML},
  author       = {Xiuyuan Lu and Benjamin Van Roy and Vikranth Dwaracherla and Morteza Ibrahimi and Ian Osband and Zheng Wen},
  doi          = {10.1561/2200000097},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {6},
  pages        = {733-865},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Reinforcement learning, bit by bit},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tutorial on amortized optimization. <em>FTML</em>,
<em>16</em>(5), 592–732. (<a
href="https://doi.org/10.1561/2200000102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.},
  archive      = {J_FTML},
  author       = {Brandon Amos},
  doi          = {10.1561/2200000102},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {5},
  pages        = {592-732},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Tutorial on amortized optimization},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conformal prediction: A gentle introduction. <em>FTML</em>,
<em>16</em>(4), 494–591. (<a
href="https://doi.org/10.1561/2200000101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, timeseries, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run by following the code footnotes.},
  archive      = {J_FTML},
  author       = {Anastasios N. Angelopoulos and Stephen Bates},
  doi          = {10.1561/2200000101},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {4},
  pages        = {494-591},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Conformal prediction: A gentle introduction},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introduction to riemannian geometry and geometric
statistics: From basic theory to implementation with geomstats.
<em>FTML</em>, <em>16</em>(3), 329–493. (<a
href="https://doi.org/10.1561/2200000098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data is a predominant resource in applications, Riemannian geometry is a natural framework to model and unify complex nonlinear sources of data. However, the development of computational tools from the basic theory of Riemannian geometry is laborious. The work presented here forms one of the main contributions to the open-source project geomstats, that consists of a Python package providing efficient implementations of the concepts of Riemannian geometry and geometric statistics, both for mathematicians and for applied scientists for whom most of the difficulties are hidden under high-level functions. The goal of this monograph is two-fold. First, we aim at giving a self-contained exposition of the basic concepts of Riemannian geometry, providing illustrations and examples at each step and adopting a computational point of view. The second goal is to demonstrate how these concepts are implemented in Geomstats, explaining the choices that were made and the conventions chosen. The general concepts are exposed and specific examples are detailed along the text. The culmination of this implementation is to be able to perform statistics and machine learning on manifolds, with as few lines of codes as in the wide-spread machine learning tool scikit-learn. We exemplify this with an introduction to geometric statistics.},
  archive      = {J_FTML},
  author       = {Nicolas Guigui and Nina Miolane and Xavier Pennec},
  doi          = {10.1561/2200000098},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {3},
  pages        = {329-493},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Introduction to riemannian geometry and geometric statistics: From basic theory to implementation with geomstats},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural networks for natural language processing: A
survey. <em>FTML</em>, <em>16</em>(2), 119–328. (<a
href="https://doi.org/10.1561/2200000096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has become the dominant approach in addressing various tasks in Natural Language Processing (NLP). Although text inputs are typically represented as a sequence of tokens, there is a rich variety of NLP problems that can be best expressed with a graph structure. As a result, there is a surge of interest in developing new deep learning techniques on graphs for a large number of NLP tasks. In this survey, we present a comprehensive overview on Graph Neural Networks (GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, which systematically organizes existing research of GNNs for NLP along three axes: graph construction, graph representation learning, and graph based encoder-decoder models. We further introduce a large number of NLP applications that exploits the power of GNNs and summarize the corresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discuss various outstanding challenges for making the full use of GNNs for NLP as well as future research directions. To the best of our knowledge, this is the first comprehensive overview of Graph Neural Networks for Natural Language Processing.},
  archive      = {J_FTML},
  author       = {Lingfei Wu and Yu Chen and Kai Shen and Xiaojie Guo and Hanning Gao and Shucheng Li and Jian Pei and Bo Long},
  doi          = {10.1561/2200000096},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {2},
  pages        = {119-328},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Graph neural networks for natural language processing: A survey},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based reinforcement learning: A survey. <em>FTML</em>,
<em>16</em>(1), 1–118. (<a
href="https://doi.org/10.1561/2200000086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is an important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This survey is an integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization.},
  archive      = {J_FTML},
  author       = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker},
  doi          = {10.1561/2200000086},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {1},
  pages        = {1-118},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Model-based reinforcement learning: A survey},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
