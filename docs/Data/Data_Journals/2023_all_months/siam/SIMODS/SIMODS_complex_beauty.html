<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---46">SIMODS - 46</h2>
<ul>
<li><details>
<summary>
(2023). Poisson reweighted laplacian uncertainty sampling for
graph-based active learning. <em>SIMODS</em>, <em>5</em>(4), 1160–1190.
(<a href="https://doi.org/10.1137/22M1531981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We show that uncertainty sampling is sufficient to achieve exploration versus exploitation in graph-based active learning, as long as the measure of uncertainty properly aligns with the underlying model and the model properly reflects uncertainty in unexplored regions. In particular, we use a recently developed algorithm, Poisson ReWeighted Laplace Learning (PWLL), for the classifier and we introduce an acquisition function designed to measure uncertainty in this graph-based classifier that identifies unexplored regions of the data. We introduce a diagonal perturbation in PWLL which produces exponential localization of solutions, and controls the exploration versus exploitation tradeoff in active learning. We use the well-posed continuum limit of PWLL to rigorously analyze our method and present experimental results on a number of graph-based image classification problems.},
  archive      = {J_SIMODS},
  author       = {Kevin Miller and Jeff Calder},
  doi          = {10.1137/22M1531981},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1160-1190},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Poisson reweighted laplacian uncertainty sampling for graph-based active learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Moment estimation for nonparametric mixture models through
implicit tensor decomposition. <em>SIMODS</em>, <em>5</em>(4),
1130–1159. (<a href="https://doi.org/10.1137/22M153879X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present an alternating least squares (ALS) type numerical optimization scheme to estimate conditionally independent mixture models in , without parametrizing the distributions. Following the method of moments, we tackle an incomplete tensor decomposition problem to learn the mixing weights and componentwise means. Then we compute the cumulative distribution functions, higher moments, and other statistics of the component distributions through linear solves. Crucially for computations in high dimensions, the steep costs associated with high-order tensors are avoided, via the development of efficient tensor-free operations. Numerical experiments demonstrate the competitive performance of the algorithm and its applicability to many models and applications. Furthermore we provide theoretical analyses, establishing identifiability from low-order moments of the mixture and guaranteeing local linear convergence of the ALS algorithm.},
  archive      = {J_SIMODS},
  author       = {Yifan Zhang and Joe Kileel},
  doi          = {10.1137/22M153879X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1130-1159},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Moment estimation for nonparametric mixture models through implicit tensor decomposition},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LASSO reloaded: A variational analysis perspective with
applications to compressed sensing. <em>SIMODS</em>, <em>5</em>(4),
1102–1129. (<a href="https://doi.org/10.1137/22M1498991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper provides a variational analysis of the unconstrained formulation of the LASSO problem, which is ubiquitous in statistical learning, signal processing, and inverse problems. In particular, we establish smoothness results for the optimal value as well as Lipschitz and smoothness properties of the optimal solution as functions of the right-hand side (or measurement vector) and the regularization parameter. Moreover, we show how to apply the proposed variational analysis to study the sensitivity of the optimal solution to the tuning parameter in the context of compressed sensing with subgaussian measurements. Our theoretical findings are validated by numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Aaron Berk and Simone Brugiapaglia and Tim Hoheisel},
  doi          = {10.1137/22M1498991},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1102-1129},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {LASSO reloaded: A variational analysis perspective with applications to compressed sensing},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Target network and truncation overcome the deadly triad in
<span class="math inline"><strong>Q</strong></span>-learning.
<em>SIMODS</em>, <em>5</em>(4), 1078–1101. (<a
href="https://doi.org/10.1137/22M1499261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. -learning with function approximation is one of the most empirically successful while theoretically mysterious reinforcement learning (RL) algorithms and was identified in [R. S. Sutton, in European Conference on Computational Learning Theory, Springer, New York, 1999, pp. 11–17] as one of the most important theoretical open problems in the RL community. Even in the basic setting where linear function approximation is used, there are well-known divergent examples. In this work, we propose a stable online variant of -learning with linear function approximation that uses target network and truncation and is driven by a single trajectory of Markovian samples. We present the finite-sample guarantees of the algorithm, which imply a sample complexity of up to a function approximation error. Importantly, we establish the results under minimal assumptions and do not modify the problem parameters to achieve stability.},
  archive      = {J_SIMODS},
  author       = {Zaiwei Chen and John-Paul Clarke and Siva Theja Maguluri},
  doi          = {10.1137/22M1499261},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1078-1101},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Target network and truncation overcome the deadly triad in \(\boldsymbol{Q}\)-learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep neural networks pruning via the structured perspective
regularization. <em>SIMODS</em>, <em>5</em>(4), 1051–1077. (<a
href="https://doi.org/10.1137/22M1542313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In machine learning, artificial neural networks (ANNs) are a very powerful tool, broadly used in many applications. Often, the selected (deep) architectures include many layers, and therefore a large number of parameters, which makes training, storage, and inference expensive. This motivated a stream of research about compressing the original networks into smaller ones without excessively sacrificing performances. Among the many proposed compression approaches, one of the most popular is pruning, whereby entire elements of the ANN (links, nodes, channels, …) and the corresponding weights are deleted. Since the nature of the problem is inherently combinatorial (what elements to prune and what not), we propose a new pruning method based on operational research tools. We start from a natural mixed-integer-programming model for the problem, and we use the perspective reformulation technique to strengthen its continuous relaxation. Projecting away the indicator variables from this reformulation yields a new regularization term, which we call the structured perspective regularization, that leads to structured pruning of the initial architecture. We test our method on some ResNet architectures applied to CIFAR-10, CIFAR-100, and ImageNet datasets, obtaining competitive performances w.r.t. the state of the art for structured pruning.},
  archive      = {J_SIMODS},
  author       = {Matteo Cacciola and Antonio Frangioni and Xinlin Li and Andrea Lodi},
  doi          = {10.1137/22M1542313},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1051-1077},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Deep neural networks pruning via the structured perspective regularization},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropic optimal transport on random graphs.
<em>SIMODS</em>, <em>5</em>(4), 1028–1050. (<a
href="https://doi.org/10.1137/22M1518281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In graph analysis, a classic task consists of computing similarity measures between (groups of) nodes. For latent space random graphs, nodes are associated to unknown latent variables. One may then seek to compute distances directly in the latent space, using only the graph structure. In this paper, we show that it is possible to consistently estimate entropic-regularized optimal transport (OT) distances between groups of nodes in the latent space. We provide a general stability result for entropic OT with respect to perturbations of the cost matrix. We then apply it to several examples of random graphs, such as graphons and geometric graphs on manifolds. Along the way, we prove concentration results for the so-called universal singular value thresholding estimator, and for the estimation of geodesic distances on a manifold, that are of independent interest.},
  archive      = {J_SIMODS},
  author       = {Nicolas Keriven},
  doi          = {10.1137/22M1518281},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1028-1050},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Entropic optimal transport on random graphs},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating a potential without the agony of the partition
function. <em>SIMODS</em>, <em>5</em>(4), 1005–1027. (<a
href="https://doi.org/10.1137/22M1517135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Estimating a Gibbs density function given a sample is an important problem in computational statistics and statistical learning. Although the well established maximum likelihood method is commonly used, it requires the computation of the partition function (i.e., the normalization of the density). This function can be easily calculated for simple low-dimensional problems but its computation is difficult or even intractable for general densities and high-dimensional problems. In this paper we propose an alternative approach based on maximum a posteriori (MAP) estimators, which we name maximum recovery MAP, to derive estimators that do not require the computation of the partition function, and reformulate the problem as an optimization problem. We further propose a least-action type potential that allows us to quickly solve the optimization problem as a feed-forward hyperbolic neural network. We demonstrate the effectiveness of our methods on some standard data sets.},
  archive      = {J_SIMODS},
  author       = {Eldad Haber and Moshe Eliasof and Luis Tenorio},
  doi          = {10.1137/22M1517135},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1005-1027},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Estimating a potential without the agony of the partition function},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A universal trade-off between the model size, test loss, and
training loss of linear predictors. <em>SIMODS</em>, <em>5</em>(4),
977–1004. (<a href="https://doi.org/10.1137/22M1540302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work we establish an algorithm and distribution independent nonasymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either “classical”—have training loss close to the noise level—or are “modern”—have a much larger number of parameters compared to the minimum needed to fit the training data exactly. We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko–Pastur. Remarkably, while the Marchenko–Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparameterization increases.},
  archive      = {J_SIMODS},
  author       = {Nikhil Ghosh and Mikhail Belkin},
  doi          = {10.1137/22M1540302},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {977-1004},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A universal trade-off between the model size, test loss, and training loss of linear predictors},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximating probability distributions by using wasserstein
generative adversarial networks. <em>SIMODS</em>, <em>5</em>(4),
949–976. (<a href="https://doi.org/10.1137/22M149689X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Studied here are Wasserstein generative adversarial networks (WGANs) with GroupSort neural networks as their discriminators. It is shown that the error bound of the approximation for the target distribution depends on the width and depth (capacity) of the generators and discriminators and the number of samples in training. A quantified generalization bound is established for the Wasserstein distance between the generated and target distributions. According to the theoretical results, WGANs have a higher requirement for the capacity of discriminators than that of generators, which is consistent with some existing results. More importantly, the results with overly deep and wide (high-capacity) generators may be worse than those with low-capacity generators if discriminators are insufficiently strong. Numerical results obtained using Swiss roll and MNIST datasets confirm the theoretical results.},
  archive      = {J_SIMODS},
  author       = {Yihang Gao and Michael K. Ng and Mingjie Zhou},
  doi          = {10.1137/22M149689X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {949-976},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximating probability distributions by using wasserstein generative adversarial networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial robustness of sparse local lipschitz predictors.
<em>SIMODS</em>, <em>5</em>(4), 920–948. (<a
href="https://doi.org/10.1137/22M1478835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work studies the adversarial robustness of parametric functions composed of a linear predictor and a nonlinear representation map. Our analysis relies on sparse local Lipschitzness (SLL), an extension of local Lipschitz continuity that better captures the stability and reduced effective dimensionality of predictors upon local perturbations. SLL functions preserve a certain degree of structure, given by the sparsity pattern in the representation map, and include several popular hypothesis classes, such as piecewise linear models, Lasso and its variants, and deep feedforward ReLU networks. Compared with traditional Lipschitz analysis, we provide a tighter robustness certificate on the minimal energy of an adversarial example, as well as tighter data-dependent nonuniform bounds on the robust generalization error of these predictors. We instantiate these results for the case of deep neural networks and provide numerical evidence that supports our results, shedding new insights into natural regularization strategies to increase the robustness of these models.},
  archive      = {J_SIMODS},
  author       = {Ramchandran Muthukumar and Jeremias Sulam},
  doi          = {10.1137/22M1478835},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {920-948},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Adversarial robustness of sparse local lipschitz predictors},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The GenCol algorithm for high-dimensional optimal transport:
General formulation and application to barycenters and wasserstein
splines. <em>SIMODS</em>, <em>5</em>(4), 899–919. (<a
href="https://doi.org/10.1137/22M1524254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We extend the recently introduced genetic column generation algorithm for high-dimensional multimarginal optimal transport from symmetric to general problems. We use the algorithm to calculate accurate mesh-free Wasserstein barycenters and cubic Wasserstein splines.},
  archive      = {J_SIMODS},
  author       = {Gero Friesecke and Maximilian Penka},
  doi          = {10.1137/22M1524254},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {899-919},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The GenCol algorithm for high-dimensional optimal transport: General formulation and application to barycenters and wasserstein splines},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding graph neural networks with generalized
geometric scattering transforms. <em>SIMODS</em>, <em>5</em>(4),
873–898. (<a href="https://doi.org/10.1137/21M1465056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The scattering transform is a multilayered wavelet-based architecture that acts as a model of convolutional neural networks. Recently, several works have generalized the scattering transform to graph-structured data. Our work builds on these constructions by introducing windowed and nonwindowed geometric scattering transforms for graphs based on two very general classes wavelets, which are in most cases based on asymmetric matrices. We show that these transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. Therefore, it helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for graph-structured data that have learned filters and also provably have desirable theoretical properties.},
  archive      = {J_SIMODS},
  author       = {Michael Perlmutter and Alexander Tong and Feng Gao and Guy Wolf and Matthew Hirn},
  doi          = {10.1137/21M1465056},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {873-898},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Understanding graph neural networks with generalized geometric scattering transforms},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the inconsistency of kernel ridgeless regression in fixed
dimensions. <em>SIMODS</em>, <em>5</em>(4), 854–872. (<a
href="https://doi.org/10.1137/22M1499819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. “Benign overfitting,” the ability of certain algorithms to interpolate noisy training data and yet perform well out-of-sample, has been a topic of considerable recent interest. We show, using a fixed design setup, that an important class of predictors, kernel machines with translation-invariant kernels, does not exhibit benign overfitting in fixed dimensions. In particular, the estimated predictor does not converge to the ground truth with increasing sample size, for any nonzero regression function and any (even adaptive) bandwidth selection. To prove these results, we give exact expressions for the generalization error and its decomposition in terms of an approximation error and an estimation error that elicits a trade-off based on the selection of the kernel bandwidth. Our results apply to commonly used translation-invariant kernels such as Gaussian, Laplace, and Cauchy.},
  archive      = {J_SIMODS},
  author       = {Daniel Beaglehole and Mikhail Belkin and Parthe Pandit},
  doi          = {10.1137/22M1499819},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {854-872},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the inconsistency of kernel ridgeless regression in fixed dimensions},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group-invariant tensor train networks for supervised
learning. <em>SIMODS</em>, <em>5</em>(4), 829–853. (<a
href="https://doi.org/10.1137/22M1506857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Invariance has recently proven to be a powerful inductive bias in machine learning models. One such class of predictive or generative models are tensor networks. We introduce a new numerical algorithm to construct a basis of tensors that are invariant under the action of normal matrix representations of an arbitrary finite group. This method can be up to several orders of magnitude faster than previous approaches. The group-invariant tensors are then combined into a group-invariant tensor train network, which can be used as a supervised machine learning model. We applied this model to a protein binding classification problem, taking into account problem-specific invariances, and obtained prediction accuracy in line with state-of-the-art deep learning approaches.},
  archive      = {J_SIMODS},
  author       = {Brent Sprangers and Nick Vannieuwenhoven},
  doi          = {10.1137/22M1506857},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {829-853},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Group-invariant tensor train networks for supervised learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplicial <span
class="math inline"><strong>q</strong></span> -connectivity of directed
graphs with applications to network analysis. <em>SIMODS</em>,
<em>5</em>(3), 800–828. (<a
href="https://doi.org/10.1137/22M1480021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Directed graphs are ubiquitous models for networks, and topological spaces they generate, such as the directed flag complex, have become useful objects in applied topology. Here the directed cliques form directed simplices. We extend Atkin’s theory of -connectivity to the case of directed simplices. This results in a preorder where simplices are related by sequences of simplices that share a -face with respect to directions specified by chosen face maps. We leverage the Alexandroff equivalence between preorders and topological spaces to introduce a new class of topological spaces for directed graphs, enabling us to assign new homotopy types different from those of directed flag complexes as seen by simplicial homology. We further introduce simplicial path analysis enabled by the connectivity preorders. As an application we characterize structural differences between various brain networks with respect to their simplicial paths.},
  archive      = {J_SIMODS},
  author       = {Henri Riihimäki},
  doi          = {10.1137/22M1480021},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {800-828},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Simplicial \({\boldsymbol{q}}\) -connectivity of directed graphs with applications to network analysis},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Randomly initialized alternating least squares: Fast
convergence for matrix sensing. <em>SIMODS</em>, <em>5</em>(3), 774–799.
(<a href="https://doi.org/10.1137/22M1506456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of reconstructing rank-1 matrices from random linear measurements, a task that appears in a variety of problems in signal processing, statistics, and machine learning. In this paper, we focus on the alternating least squares (ALS) method. While this algorithm has been studied in a number of previous works, most of them only show convergence from an initialization close to the true solution and thus require a carefully designed initialization scheme. However, random initialization has often been preferred by practitioners as it is model-agnostic. In this paper, we show that ALS with random initialization converges to the true solution with -accuracy in iterations using only a near-optimal number of samples, where we assume the measurement matrices to be i.i.d. Gaussian and where by we denote the ambient dimension. We observe that the convergence occurs in two phases. In the first phase, the iterates starting from random initialization become gradually more aligned with the true signal, and in the second phase the iterates converge linearly. Key to our proof is the observation that in the first phase the trajectory of the ALS iterates depends only very mildly on certain entries of the random measurement matrices. Numerical experiments corroborate our theoretical predictions.},
  archive      = {J_SIMODS},
  author       = {Kiryung Lee and Dominik Stöger},
  doi          = {10.1137/22M1506456},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {774-799},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomly initialized alternating least squares: Fast convergence for matrix sensing},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Satisficing paths and independent multiagent reinforcement
learning in stochastic games. <em>SIMODS</em>, <em>5</em>(3), 745–773.
(<a href="https://doi.org/10.1137/22M1515112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In multiagent reinforcement learning, independent learners are those that do not observe the actions of other agents in the system. Due to the decentralization of information, it is challenging to design independent learners that drive play to equilibrium. This paper investigates the feasibility of using satisficing dynamics to guide independent learners to approximate equilibrium in stochastic games. For , an -satisficing policy update rule is any rule that instructs the agent to not change its policy when it is -best-responding to the policies of the remaining players; -satisficing paths are defined to be sequences of joint policies obtained when each agent uses some -satisficing policy update rule to select its next policy. We establish structural results on the existence of -satisficing paths into -equilibrium in both symmetric -player games and general stochastic games with two players. We then present an independent learning algorithm for -player symmetric games and give high probability guarantees of convergence to -equilibrium under self-play. This guarantee is made using symmetry alone, leveraging the previously unexploited structure of -satisficing paths.},
  archive      = {J_SIMODS},
  author       = {Bora Yongacoglu and Gürdal Arslan and Serdar Yüksel},
  doi          = {10.1137/22M1515112},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {745-773},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Satisficing paths and independent multiagent reinforcement learning in stochastic games},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithmic regularization in model-free overparametrized
asymmetric matrix factorization. <em>SIMODS</em>, <em>5</em>(3),
723–744. (<a href="https://doi.org/10.1137/22M1519833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the asymmetric matrix factorization problem under a natural nonconvex formulation with arbitrary overparametrization. The model-free setting is considered, with minimal assumption on the rank or singular values of the observed matrix, where the global optima provably overfit. We show that vanilla gradient descent with small random initialization sequentially recovers the principal components of the observed matrix. Consequently, when equipped with proper early stopping, gradient descent produces the best low-rank approximation of the observed matrix without explicit regularization. We provide a sharp characterization of the relationship between the approximation error, iteration complexity, initialization size, and stepsize. Our complexity bound is almost dimension-free and depends logarithmically on the approximation error, with significantly more lenient requirements on the stepsize and initialization compared to prior work. Our theoretical results provide accurate prediction for the behavior of gradient descent, showing good agreement with numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Liwei Jiang and Yudong Chen and Lijun Ding},
  doi          = {10.1137/22M1519833},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {723-744},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Algorithmic regularization in model-free overparametrized asymmetric matrix factorization},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimality conditions for nonsmooth nonconvex-nonconcave
min-max problems and generative adversarial networks. <em>SIMODS</em>,
<em>5</em>(3), 693–722. (<a
href="https://doi.org/10.1137/22M1482238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper considers a class of nonsmooth nonconvex-nonconcave min-max problems in machine learning and games. We first provide sufficient conditions for the existence of global minimax points and local minimax points. Next, we establish the first-order and second-order optimality conditions for local minimax points by using directional derivatives. These conditions reduce to smooth min-max problems with Fréchet derivatives. We apply our theoretical results to generative adversarial networks (GANs) in which two neural networks contest with each other in a game. Examples are used to illustrate applications of the new theory for training GANs.},
  archive      = {J_SIMODS},
  author       = {Jie Jiang and Xiaojun Chen},
  doi          = {10.1137/22M1482238},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {693-722},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimality conditions for nonsmooth nonconvex-nonconcave min-max problems and generative adversarial networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A note on the regularity of images generated by
convolutional neural networks. <em>SIMODS</em>, <em>5</em>(3), 670–692.
(<a href="https://doi.org/10.1137/22M1525995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The regularity of images generated by a class of convolutional neural networks, such as the U-net, generative networks, or the deep image prior, is analyzed. In a resolution-independent, infinite dimensional setting, it is shown that such images, represented as functions, are always continuous and, in some circumstances, even continuously differentiable, contradicting the widely accepted modeling of sharp edges in images via jump discontinuities. While such statements require an infinite dimensional setting, the connection to (discretized) neural networks used in practice is made by considering the limit as the resolution approaches infinity. As a practical consequence, the results of this paper in particular provide analytical evidence that basic regularization of network weights (also known as weight decay) might lead to oversmoothed outputs.},
  archive      = {J_SIMODS},
  author       = {Andreas Habring and Martin Holler},
  doi          = {10.1137/22M1525995},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {670-692},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A note on the regularity of images generated by convolutional neural networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved central limit theorem and fast convergence rates
for entropic transportation costs. <em>SIMODS</em>, <em>5</em>(3),
639–669. (<a href="https://doi.org/10.1137/22M149260X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove a central limit theorem for the entropic transportation cost between subgaussian probability measures, centered at the population cost. This is the first result which allows for asymptotically valid inference for entropic optimal transport between measures which are not necessarily discrete. In the compactly supported case, we complement these results with new, faster, convergence rates for the expected entropic transportation cost between empirical measures. Our proof is based on strengthening convergence results for dual solutions to the entropic optimal transport problem.},
  archive      = {J_SIMODS},
  author       = {Eustasio del Barrio and Alberto González Sanz and Jean-Michel Loubes and Jonathan Niles-Weed},
  doi          = {10.1137/22M149260X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {639-669},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {An improved central limit theorem and fast convergence rates for entropic transportation costs},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate q learning for controlled diffusion processes
and its near optimality. <em>SIMODS</em>, <em>5</em>(3), 615–638. (<a
href="https://doi.org/10.1137/22M1484201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study a Q learning algorithm for continuous time stochastic control problems. The proposed algorithm uses the sampled state process by discretizing the state and control action spaces under piecewise constant control processes. We show that the algorithm converges to the optimality equation of a finite Markov decision process (MDP). Using this MDP model, we provide an upper bound for the approximation error for the optimal value function of the continuous time control problem. Furthermore, we present provable upper bounds for the performance loss of the learned control process compared to the optimal admissible control process of the original problem. The provided error upper bounds are functions of the time and space discretization parameters, and they reveal the effect of different levels of the approximation: (i) approximation of the continuous time control problem by an MDP, (ii) use of piecewise constant control processes, and (iii) space discretization. Finally, we state a time complexity bound for the proposed algorithm as a function of the time and space discretization parameters.},
  archive      = {J_SIMODS},
  author       = {Erhan Bayraktar and Ali Devran Kara},
  doi          = {10.1137/22M1484201},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {615-638},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximate q learning for controlled diffusion processes and its near optimality},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust inference of manifold density and geometry by doubly
stochastic scaling. <em>SIMODS</em>, <em>5</em>(3), 589–614. (<a
href="https://doi.org/10.1137/22M1516968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Gaussian kernel and its traditional normalizations (e.g., row-stochastic) are popular approaches for assessing similarities between data points. Yet, they can be inaccurate under high-dimensional noise, especially if the noise magnitude varies considerably across the data, e.g., under heteroskedasticity or outliers. In this work, we investigate a more robust alternative—the doubly stochastic normalization of the Gaussian kernel. We consider a setting where points are sampled from an unknown density on a low-dimensional manifold embedded in high-dimensional space and corrupted by possibly strong, non–identically distributed, sub-Gaussian noise. We establish that the doubly stochastic affinity matrix and its scaling factors concentrate around certain population forms, and provide corresponding finite-sample probabilistic bounds. We then utilize these results to develop several tools for robust inference under general high-dimensional noise. First, we derive a robust density estimator that reliably infers the underlying sampling density and can substantially outperform the standard kernel density estimator under heteroskedasticity and outliers. Second, we obtain estimators for the pointwise noise magnitudes, the pointwise signal magnitudes, and the pairwise Euclidean distances between clean data points. Lastly, we derive robust graph Laplacian normalizations that accurately approximate various manifold Laplacians, including the Laplace–Beltrami operator, improving over traditional normalizations in noisy settings. We exemplify our results in simulations and on real single-cell RNA-sequencing data. For the latter, we show that in contrast to traditional methods, our approach is robust to variability in technical noise levels across cell types.},
  archive      = {J_SIMODS},
  author       = {Boris Landa and Xiuyuan Cheng},
  doi          = {10.1137/22M1516968},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {589-614},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Robust inference of manifold density and geometry by doubly stochastic scaling},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven mirror descent with input-convex neural
networks. <em>SIMODS</em>, <em>5</em>(2), 558–587. (<a
href="https://doi.org/10.1137/22M1508613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Learning-to-optimize is an emerging framework that seeks to speed up the solution of certain optimization problems by leveraging training data. Learned optimization solvers have been shown to outperform classical optimization algorithms in terms of convergence speed, especially for convex problems. Many existing data-driven optimization methods are based on parameterizing the update step and learning the optimal parameters (typically scalars) from the available data. We propose a novel functional parameterization approach for learned convex optimization solvers based on the classical mirror descent (MD) algorithm. Specifically, we seek to learn the optimal Bregman distance in MD by modeling the underlying convex function using an input-convex neural network (ICNN). The parameters of the ICNN are learned by minimizing the target objective function evaluated at the MD iterate after a predetermined number of iterations. The inverse of the mirror map is modeled approximately using another neural network, as the exact inverse is intractable to compute. We derive convergence rate bounds for the proposed learned mirror descent approach with an approximate inverse mirror map and perform extensive numerical evaluation on various convex problems such as image inpainting, denoising, and learning a two-class support vector machine classifier and a multiclass linear classifier on fixed features.},
  archive      = {J_SIMODS},
  author       = {Hong Ye Tan and Subhadip Mukherjee and Junqi Tang and Carola-Bibiane Schönlieb},
  doi          = {10.1137/22M1508613},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {558-587},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Data-driven mirror descent with input-convex neural networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical analysis of random objects via metric measure
laplacians. <em>SIMODS</em>, <em>5</em>(2), 528–557. (<a
href="https://doi.org/10.1137/22M1491022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we consider a certain convolutional Laplacian for metric measure spaces and investigate its potential for the statistical analysis of complex objects. The spectrum of that Laplacian serves as a signature of the space under consideration and the eigenvectors provide the principal directions of the shape, its harmonics. These concepts are used to assess the similarity of objects or understand their most important features in a principled way which is illustrated in various examples. Adopting a statistical point of view, we define a mean spectral measure and its empirical counterpart. The corresponding limiting process of interest is derived and statistical applications are discussed.},
  archive      = {J_SIMODS},
  author       = {Gilles Mordant and Axel Munk},
  doi          = {10.1137/22M1491022},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {528-557},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Statistical analysis of random objects via metric measure laplacians},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic registration for gaussian process
three-dimensional shape modelling in the presence of extensive missing
data. <em>SIMODS</em>, <em>5</em>(2), 502–527. (<a
href="https://doi.org/10.1137/22M1495494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a shape fitting/registration method based on a Gaussian processes formulation, suitable for shapes with extensive regions of missing data. Gaussian processes are a proven powerful tool, as they provide a unified setting for shape modelling and fitting. While the existing methods in this area prove to work well for the general case of the human head, when looking at more detailed and deformed data, with a high prevalence of missing data, such as the ears, the results are not satisfactory. In order to overcome this, we formulate the shape fitting problem as a multiannotator Gaussian process regression and establish a parallel with the standard probabilistic registration. The achieved method, the shape fitting Gaussian process (or SFGP), shows better performance when dealing with extensive areas of missing data when compared to a state-of-the-art registration method and current approaches for registration with GP. Experiments are conducted both for a two-dimensional small dataset with several transformations and a three-dimensional dataset of ears.},
  archive      = {J_SIMODS},
  author       = {Filipa M. Valdeira and Ricardo Ferreira and Alessandra Micheletti and Cláudia Soares},
  doi          = {10.1137/22M1495494},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {502-527},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Probabilistic registration for gaussian process three-dimensional shape modelling in the presence of extensive missing data},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wassmap: Wasserstein isometric mapping for image manifold
learning. <em>SIMODS</em>, <em>5</em>(2), 475–501. (<a
href="https://doi.org/10.1137/22M1490053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a nonlinear dimensionality reduction technique that provides solutions to some drawbacks in existing global nonlinear dimensionality reduction algorithms in imaging applications. Wassmap represents images via probability measures in Wasserstein space, then uses pairwise Wasserstein distances between the associated measures to produce a low-dimensional, approximately isometric embedding. We show that the algorithm is able to exactly recover parameters of some image manifolds, including those generated by translations or dilations of a fixed generating measure. Additionally, we show that a discrete version of the algorithm retrieves parameters from manifolds generated from discrete measures by providing a theoretical bridge to transfer recovery results from functional data to discrete data. Testing of the proposed algorithms on various image data manifolds shows that Wassmap yields good embeddings compared with other global and local techniques.},
  archive      = {J_SIMODS},
  author       = {Keaton Hamm and Nick Henscheid and Shujie Kang},
  doi          = {10.1137/22M1490053},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {475-501},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Wassmap: Wasserstein isometric mapping for image manifold learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient global optimization of two-layer ReLU networks:
Quadratic-time algorithms and adversarial training. <em>SIMODS</em>,
<em>5</em>(2), 446–474. (<a
href="https://doi.org/10.1137/21M1467134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The nonconvexity of the artificial neural network (ANN) training landscape brings optimization difficulties. While the traditional back-propagation stochastic gradient descent algorithm and its variants are effective in certain cases, they can become stuck at spurious local minima and are sensitive to initializations and hyperparameters. Recent work has shown that the training of a ReLU-activated ANN can be reformulated as a convex program, bringing hope to globally optimizing interpretable ANNs. However, naively solving the convex training formulation has an exponential complexity, and even an approximation heuristic requires cubic time. In this work, we characterize the quality of this approximation and develop two efficient algorithms that train ANNs with global convergence guarantees. The first algorithm is based on the alternating direction method of multipliers. It can solve both the exact convex formulation and the approximate counterpart, and it generalizes to a family of convex training formulations. Linear global convergence is achieved, and the initial several iterations often yield a solution with high prediction accuracy. When solving the approximate formulation, the per-iteration time complexity is quadratic. The second algorithm, based on the “sampled convex programs” theory, is simpler to implement. It solves unconstrained convex formulations and converges to an approximately globally optimal classifier. The nonconvexity of the ANN training landscape exacerbates when adversarial training is considered. We apply the robust convex optimization theory to convex training and develop convex formulations that train ANNs robust to adversarial inputs. Our analysis explicitly focuses on one-hidden-layer fully connected ANNs, but can extend to more sophisticated architectures.},
  archive      = {J_SIMODS},
  author       = {Yatong Bai and Tanmay Gautam and Somayeh Sojoudi},
  doi          = {10.1137/21M1467134},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {446-474},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Efficient global optimization of two-layer ReLU networks: Quadratic-time algorithms and adversarial training},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring complexity of learning schemes using
hessian-schatten total variation. <em>SIMODS</em>, <em>5</em>(2),
422–445. (<a href="https://doi.org/10.1137/22M147517X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we introduce the Hessian-Schatten total variation (HTV)—a novel seminorm that quantifies the total “rugosity” of multivariate functions. Our motivation for defining HTV is to assess the complexity of supervised-learning schemes. We start by specifying the adequate matrix-valued Banach spaces that are equipped with suitable classes of mixed norms. We then show that the HTV is invariant to rotations, scalings, and translations. Additionally, its minimum value is achieved for linear mappings, which supports the common intuition that linear regression is the least complex learning model. Next, we present closed-form expressions of the HTV for two general classes of functions. The first one is the class of Sobolev functions with a certain degree of regularity, for which we show that the HTV coincides with the Hessian-Schatten seminorm that is sometimes used as a regularizer for image reconstruction. The second one is the class of continuous and piecewise-linear (CPWL) functions. In this case, we show that the HTV reflects the total change in slopes between linear regions that have a common facet. Hence, it can be viewed as a convex relaxation ( -type) of the number of linear regions ( -type) of CPWL mappings. Finally, we illustrate the use of our proposed seminorm.},
  archive      = {J_SIMODS},
  author       = {Shayan Aziznejad and Joaquim Campos and Michael Unser},
  doi          = {10.1137/22M147517X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {422-445},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Measuring complexity of learning schemes using hessian-schatten total variation},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Moving up the cluster tree with the gradient flow.
<em>SIMODS</em>, <em>5</em>(2), 400–421. (<a
href="https://doi.org/10.1137/22M1469869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper establishes a strong correspondence between two important clustering approaches that emerged in the 1970s: clustering by level sets or cluster tree as proposed by Hartigan, and clustering by gradient lines or gradient flow as proposed by Fukunaga and Hostetler. This correspondence is drawn by showing that the gradient ascent flow provides a natural way to move up the cluster tree.},
  archive      = {J_SIMODS},
  author       = {Ery Arias-Castro and Wanli Qiao},
  doi          = {10.1137/22M1469869},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {400-421},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Moving up the cluster tree with the gradient flow},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Post-training quantization for neural networks with provable
guarantees. <em>SIMODS</em>, <em>5</em>(2), 373–399. (<a
href="https://doi.org/10.1137/22M1511709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. While neural networks have been remarkably successful in a wide array of applications, implementing them in resource-constrained hardware remains an area of intense research. By replacing the weights of a neural network with quantized (e.g., 4-bit, or binary) counterparts, massive savings in computation cost, memory, and power consumption are attained. To that end, we generalize a post-training neural network quantization method, GPFQ, that is based on a greedy path-following mechanism. Among other things, we propose modifications to promote sparsity of the weights, and rigorously analyze the associated error. Additionally, our error analysis expands the results of previous work on GPFQ to handle general quantization alphabets, showing that for quantizing a single-layer network, the relative square error essentially decays linearly in the number of weights, i.e., level of overparametrization. Our result holds across a range of input distributions and for both fully connected and convolutional architectures thereby also extending previous results. To empirically evaluate the method, we quantize several common architectures with few bits per weight, and test them on ImageNet, showing only minor loss of accuracy compared to unquantized models. We also demonstrate that standard modifications, such as bias correction and mixed precision quantization, further improve accuracy.},
  archive      = {J_SIMODS},
  author       = {Jinjie Zhang and Yixuan Zhou and Rayan Saab},
  doi          = {10.1137/22M1511709},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {373-399},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Post-training quantization for neural networks with provable guarantees},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-inhomogeneous diffusion geometry and topology.
<em>SIMODS</em>, <em>5</em>(2), 346–372. (<a
href="https://doi.org/10.1137/21M1462945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Diffusion condensation is a dynamic process that yields a sequence of multiscale data representations that aim to encode meaningful abstractions. It has proven effective for manifold learning, denoising, clustering, and visualization of high-dimensional data. Diffusion condensation is constructed as a time-inhomogeneous process where each step first computes a diffusion operator and then applies it to the data. We theoretically analyze the convergence and evolution of this process from geometric, spectral, and topological perspectives. From a geometric perspective, we obtain convergence bounds based on the smallest transition probability and the radius of the data, whereas from a spectral perspective, our bounds are based on the eigenspectrum of the diffusion kernel. Our spectral results are of particular interest since most of the literature on data diffusion is focused on homogeneous processes. From a topological perspective, we show that diffusion condensation generalizes centroid-based hierarchical clustering. We use this perspective to obtain a bound based on the number of data points, independent of their location. To understand the evolution of the data geometry beyond convergence, we use topological data analysis. We show that the condensation process itself defines an intrinsic condensation homology. We use this intrinsic topology, as well as the ambient persistent homology, of the condensation process to study how the data changes over diffusion time. We demonstrate both types of topological information in well-understood toy examples. Our work gives theoretical insight into the convergence of diffusion condensation and shows that it provides a link between topological and geometric data analysis.},
  archive      = {J_SIMODS},
  author       = {Guillaume Huguet and Alexander Tong and Bastian Rieck and Jessie Huang and Manik Kuchroo and Matthew Hirn and Guy Wolf and Smita Krishnaswamy},
  doi          = {10.1137/21M1462945},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {346-372},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Time-inhomogeneous diffusion geometry and topology},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Taming neural networks with TUSLA: Nonconvex learning via
adaptive stochastic gradient langevin algorithms. <em>SIMODS</em>,
<em>5</em>(2), 323–345. (<a
href="https://doi.org/10.1137/22M1514283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Artificial neural networks (ANNs) are typically highly nonlinear systems which are finely tuned via the optimization of their associated, nonconvex loss functions. In many cases, the gradient of any such loss function has superlinear growth, making the use of the widely accepted (stochastic) gradient descent methods, which are based on Euler numerical schemes, problematic. We offer a new learning algorithm based on an appropriately constructed variant of the popular stochastic gradient Langevin dynamics (SGLD), which is called the tamed unadjusted stochastic Langevin algorithm (TUSLA). We also provide a nonasymptotic analysis of the new algorithm’s convergence properties in the context of nonconvex learning problems with the use of ANNs. Thus, we provide finite-time guarantees for TUSLA to find approximate minimizers of both empirical and population risks. The roots of the TUSLA algorithm are based on the taming technology for diffusion processes with superlinear coefficients as developed in [S. Sabanis, Electron. Commun. Probab., 18 (2013), pp. 1–10] and [S. Sabanis, Ann. Appl. Probab., 26 (2016), pp. 2083–2105] and for Markov chain Monte Carlo algorithms in [N. Brosse, A. Durmus, É. Moulines, and S. Sabanis, Stochastic Process. Appl., 129 (2019), pp. 3638–3663]. Numerical experiments are presented which confirm the theoretical findings and illustrate the need for the use of the new algorithm in comparison to vanilla SGLD within the framework of ANNs.},
  archive      = {J_SIMODS},
  author       = {Attila Lovas and Iosif Lytras and Miklós Rásonyi and Sotirios Sabanis},
  doi          = {10.1137/22M1514283},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {323-345},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Taming neural networks with TUSLA: Nonconvex learning via adaptive stochastic gradient langevin algorithms},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximation of lipschitz functions using deep spline
neural networks. <em>SIMODS</em>, <em>5</em>(2), 306–322. (<a
href="https://doi.org/10.1137/22M1504573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Although Lipschitz-constrained neural networks have many applications in machine learning, the design and training of expressive Lipschitz-constrained networks is very challenging. Since the popular rectified linear-unit networks have provable disadvantages in this setting, we propose using learnable spline activation functions with at least three linear regions instead. We prove that our choice is universal among all componentwise 1-Lipschitz activation functions in the sense that no other weight-constrained architecture can approximate a larger class of functions. Additionally, our choice is at least as expressive as the recently introduced non-componentwise Groupsort activation function for spectral-norm-constrained weights. The theoretical findings of this paper are consistent with previously published numerical results.},
  archive      = {J_SIMODS},
  author       = {Sebastian Neumayer and Alexis Goujon and Pakshal Bohra and Michael Unser},
  doi          = {10.1137/22M1504573},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {306-322},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximation of lipschitz functions using deep spline neural networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal structural learning via local graphs.
<em>SIMODS</em>, <em>5</em>(2), 280–305. (<a
href="https://doi.org/10.1137/20M1362796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of learning causal structures in sparse high-dimensional settings that may be subject to the presence of (potentially many) unmeasured confounders, as well as selection bias. Based on structure found in common families of large random networks, we propose a new local notion of sparsity for structure learning in the presence of latent and selection variables, and develop a new version of the fast causal inference (FCI) algorithm, which we refer to as local FCI (lFCI). Under the new sparsity condition and an additional assumption that ensures that conditional dependencies can be determined locally, lFCI is consistent and offers reduced computational and sample complexity when compared to standard FCI algorithms. The new notion of sparsity allows the presence of highly connected hub nodes, which are common in real-world networks but problematic for existing methods. Our numerical experiments indicate that the lFCI algorithm achieves state-of-the-art performance across many classes of large random networks, and its performance is superior to that of existing methods for networks containing hub nodes.},
  archive      = {J_SIMODS},
  author       = {Wenyu Chen and Mathias Drton and Ali Shojaie},
  doi          = {10.1137/20M1362796},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {280-305},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Causal structural learning via local graphs},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonbacktracking spectral clustering of nonuniform
hypergraphs. <em>SIMODS</em>, <em>5</em>(2), 251–279. (<a
href="https://doi.org/10.1137/22M1494713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Spectral methods offer a tractable, global framework for clustering in graphs via eigenvector computations on graph matrices. Hypergraph data, in which entities interact on edges of arbitrary size, poses challenges for matrix representations and therefore for spectral clustering. We study spectral clustering for nonuniform hypergraphs based on the hypergraph nonbacktracking operator. After reviewing the definition of this operator and its basic properties, we prove a theorem of Ihara–Bass type which allows eigenpair computations to take place on a smaller matrix, often enabling faster computation. We then propose an alternating algorithm for inference in a hypergraph stochastic blockmodel via linearized belief-propagation which involves a spectral clustering step again using nonbacktracking operators. We provide proofs related to this algorithm that both formalize and extend several previous results. We pose several conjectures about the limits of spectral methods and detectability in hypergraph stochastic blockmodels in general, supporting these with in-expectation analysis of the eigenpairs of our operators. We perform experiments in real and synthetic data that demonstrate the benefits of hypergraph methods over graph-based ones when interactions of different sizes carry different information about cluster structure.},
  archive      = {J_SIMODS},
  author       = {Philip Chodrow and Nicole Eikmeier and Jamie Haddock},
  doi          = {10.1137/22M1494713},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {251-279},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonbacktracking spectral clustering of nonuniform hypergraphs},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimally weighted PCA for high-dimensional heteroscedastic
data. <em>SIMODS</em>, <em>5</em>(1), 222–250. (<a
href="https://doi.org/10.1137/22M1470244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Modern data are increasingly both high-dimensional and heteroscedastic. This paper considers the challenge of estimating underlying principal components from high-dimensional data with noise that is heteroscedastic across samples, i.e., some samples are noisier than others. Such heteroscedasticity naturally arises, e.g., when combining data from diverse sources or sensors. A natural way to account for this heteroscedasticity is to give noisier blocks of samples less weight in PCA by using the leading eigenvectors of a weighted sample covariance matrix. We consider the problem of choosing weights to optimally recover the underlying components. In general, one cannot know these optimal weights since they depend on the underlying components we seek to estimate. However, we show that under some natural statistical assumptions the optimal weights converge to a simple function of the signal and noise variances for high-dimensional data. Surprisingly, the optimal weights are not the inverse noise variance weights commonly used in practice. We demonstrate the theoretical results through numerical simulations and comparisons with existing weighting schemes. Finally, we briefly discuss how estimated signal and noise variances can be used when the true variances are unknown, and we illustrate the optimal weights on real data from astronomy.},
  archive      = {J_SIMODS},
  author       = {David Hong and Fan Yang and Jeffrey A. Fessler and Laura Balzano},
  doi          = {10.1137/22M1470244},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {222-250},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimally weighted PCA for high-dimensional heteroscedastic data},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A simple and optimal algorithm for strict circular
seriation. <em>SIMODS</em>, <em>5</em>(1), 201–221. (<a
href="https://doi.org/10.1137/22M1495342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recently, Armstrong, Guzmán, and Sing Long [SIAM J. Math. Data Sci., 3 (2021), pp. 1223–1250] presented an optimal time algorithm for strict circular seriation (called also the recognition of strict quasi-circular Robinson spaces). In this paper, we give a very simple time algorithm for computing a compatible circular order for strict circular seriation. When the input space is not known to be strict quasi-circular Robinson, our algorithm is complemented by an time verification of compatibility of the returned order. This algorithm also works for recognition of other types of strict circular Robinson spaces known in the literature. We also prove that the circular Robinson dissimilarities (which are defined by the existence of compatible orders on one of the two arcs between each pair of points) are exactly the pre-circular Robinson dissimilarities (which are defined by a four-point condition).},
  archive      = {J_SIMODS},
  author       = {Mikhael Carmona and Victor Chepoi and Guyslain Naves and Pascal Préa},
  doi          = {10.1137/22M1495342},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {201-221},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A simple and optimal algorithm for strict circular seriation},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated and instance-optimal policy evaluation with
linear function approximation. <em>SIMODS</em>, <em>5</em>(1), 174–200.
(<a href="https://doi.org/10.1137/21M1468668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of policy evaluation with linear function approximation and present efficient and practical algorithms that come with strong optimality guarantees. We begin by proving lower bounds that establish baselines on both the deterministic error and stochastic error in this problem. In particular, we prove an oracle complexity lower bound on the deterministic error in an instance-dependent norm associated with the stationary distribution of the transition kernel, and we use the local asymptotic minimax machinery to prove an instance-dependent lower bound on the stochastic error in the independent and identically distributed (i.i.d.) observation model. Existing algorithms fail to match at least one of these lower bounds: To illustrate, we analyze a variance-reduced variant of temporal difference learning, showing in particular that it fails to achieve the oracle complexity lower bound. To remedy this issue, we develop an accelerated, variance-reduced fast temporal difference (VRFTD) algorithm that simultaneously matches both lower bounds and attains a strong notion of instance-optimality. Finally, we extend the VRFTD algorithm to the setting with Markovian observations and provide instance-dependent convergence results. Our theoretical guarantees of optimality are corroborated by numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Tianjiao Li and Guanghui Lan and Ashwin Pananjady},
  doi          = {10.1137/21M1468668},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {174-200},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Accelerated and instance-optimal policy evaluation with linear function approximation},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe rules for the identification of zeros in the solutions
of the SLOPE problem. <em>SIMODS</em>, <em>5</em>(1), 147–173. (<a
href="https://doi.org/10.1137/21M1457631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper we propose a methodology to accelerate the resolution of the so-called Sorted L-One Penalized Estimation (SLOPE) problem. Our method leverages the concept of “safe screening&quot;, well studied in the literature for group-separable sparsity-inducing norms, and aims ato identify the zeros in the solution of SLOPE. More specifically, we derive a set of inequalities for each element of the -dimensional primal vector and prove that the latter can be safely screened if some subsets of these inequalities are verified. We propose moreover an efficient algorithm to jointly apply the proposed procedure to all the primal variables. Our procedure has a complexity , where is a problem-dependent constant and is the number of zeros identified by the test. Numerical experiments confirm that, for a prescribed computational budget, the proposed methodology leads to significant improvements in the solving precision.},
  archive      = {J_SIMODS},
  author       = {Clément Elvira and Cédric Herzet},
  doi          = {10.1137/21M1457631},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {147-173},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Safe rules for the identification of zeros in the solutions of the SLOPE problem},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAT–GMM: Generative adversarial training for gaussian
mixture models. <em>SIMODS</em>, <em>5</em>(1), 122–146. (<a
href="https://doi.org/10.1137/21M1445831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Generative adversarial networks (GANs) learn the distribution of observed samples through a zero-sum game between two machine players, a generator and a discriminator. While GANs achieve great success in learning the complex distribution of image, sound, and text data, they perform suboptimally in learning multimodal distribution-learning benchmarks such as Gaussian mixture models (GMMs). In this paper, we propose Generative Adversarial Training for Gaussian Mixture Models (GAT-GMM), a minimax GAN framework for learning GMMs. Motivated by optimal transport theory, we design the zero-sum game in GAT-GMM using a random linear generator and a softmax-based quadratic discriminator architecture, which leads to a nonconvex concave minimax optimization problem. We show that a gradient descent ascent (GDA) method converges to an approximate stationary minimax point of the GAT-GMM optimization problem. In the benchmark case of a mixture of two symmetric, well-separated Gaussians, we further show that this stationary point recovers the true parameters of the underlying GMM. We discuss the application of the proposed GAT-GMM framework for learning GMMs in the distributed federated learning setting, where the widely used expectation-maximization (EM) algorithm can incur great computational and communication costs. On the other hand, we show that GAT-GMM provides a scalable learning approach and a distributed GDA algorithm can still solve the GAT-GMM minimax problem without incurring extra computation costs. We numerically support our theoretical results by performing experiments which show that our minimax framework is successful in centralized learning tasks and can outperform standard EM-type algorithms in the federated setting.},
  archive      = {J_SIMODS},
  author       = {Farzan Farnia and William W. Wang and Subhro Das and Ali Jadbabaie},
  doi          = {10.1137/21M1445831},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {122-146},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {GAT–GMM: Generative adversarial training for gaussian mixture models},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Numerical considerations and a new implementation for
invariant coordinate selection. <em>SIMODS</em>, <em>5</em>(1), 97–121.
(<a href="https://doi.org/10.1137/22M1498759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Invariant coordinate selection (ICS) is a multivariate data transformation and a dimension reduction method that can be useful in many different contexts. It can be used for outlier detection or cluster identification, and can be seen as an independent component or a non-Gaussian component analysis method. The usual implementation of ICS is based on a joint diagonalization of two scatter matrices, and may be numerically unstable in some ill-conditioned situations. We focus on one-step M-scatter matrices and propose a new implementation of ICS based on a pivoted QR factorization of the centered data set. This factorization avoids the direct computation of the scatter matrices and their inverse and brings numerical stability to the algorithm. Furthermore, the row and column pivoting leads to a rank revealing procedure that allows computation of ICS when the scatter matrices are not full rank. Several artificial and real data sets illustrate the interest of using the new implementation compared to the original one.},
  archive      = {J_SIMODS},
  author       = {Aurore Archimbaud and Zlatko Drmač and Klaus Nordhausen and Una Radojičić and Anne Ruiz-Gazen},
  doi          = {10.1137/22M1498759},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {97-121},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Numerical considerations and a new implementation for invariant coordinate selection},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold oblique random forests: Towards closing the gap on
convolutional deep networks. <em>SIMODS</em>, <em>5</em>(1), 77–96. (<a
href="https://doi.org/10.1137/21M1449117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Decision forests, in particular random forests and gradient boosting trees have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. Forests dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to a permutation of the feature indices. However, in structured data lying on a manifold—such as images and time-series—deep networks, specifically convolutional deep networks (ConvNets), tend to outperform forests. We conjecture that it is in part due to networks not simply analyzing feature magnitudes, but also their indices. In contrast, naïve forest implementations fail to explicitly consider feature indices. A recent approach demonstrates that forests, for each node, implicitly sample a random matrix from some specific distribution. These forests, like some networks, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach with Manifold Oblique Random Forests (Morf) that chooses distributions in a manifold-aware fashion to incorporate feature locality. Morf runs fast and maintains interpretability and theoretical justification. Morf also has excellent empirical classification performance on simulated data and real images and multivariate time-series. It outperforms non-neural network approaches that ignore feature space structure and challenges the performance of ConvNets in some cases.},
  archive      = {J_SIMODS},
  author       = {Adam Li and Ronan Perry and Chester Huynh and Tyler M. Tomita and Ronak Mehta and Jesus Arroyo and Jesse Patsolic and Ben Falk and Sridevi Sarma and Joshua Vogelstein},
  doi          = {10.1137/21M1449117},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {77-96},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Manifold oblique random forests: Towards closing the gap on convolutional deep networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability of deep neural networks via discrete rough paths.
<em>SIMODS</em>, <em>5</em>(1), 50–76. (<a
href="https://doi.org/10.1137/22M1472358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Using rough path techniques, we provide a priori estimates for the output of deep residual neural networks in terms of both the input data and the (trained) network weights. As trained network weights are typically very rough when seen as functions of the layer, we propose to derive stability bounds in terms of the total -variation of trained weights for any . Unlike the -theory underlying the neural ODE literature, our estimates remain bounded even in the limiting case of weights behaving like Brownian motions, as suggested in [A.-S. Cohen, R. Cont, A. Rossier, and R. Xu, Proceedings of the 38th International Conference on Machine Learning, JMLR, Cambridge, MA, 2021, pp. 2039–2048]. Mathematically, we interpret residual neural network as solutions to (rough) difference equations, and analyze them based on recent results of discrete-time signatures and rough path theory.},
  archive      = {J_SIMODS},
  author       = {Christian Bayer and Peter K. Friz and Nikolas Tapia},
  doi          = {10.1137/22M1472358},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {50-76},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stability of deep neural networks via discrete rough paths},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient identification of butterfly sparse matrix
factorizations. <em>SIMODS</em>, <em>5</em>(1), 22–49. (<a
href="https://doi.org/10.1137/22M1488727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Fast transforms correspond to factorizations of the form , where each factor is sparse and possibly structured. This paper investigates essential uniqueness of such factorizations, i.e., uniqueness up to unavoidable scaling ambiguities. Our main contribution is to prove that any matrix having the so-called butterfly structure admits an essentially unique factorization into butterfly factors (where ), and that the factors can be recovered by a hierarchical factorization method, which consists in recursively factorizing the considered matrix into two factors. This hierarchical identifiability property relies on a simple identifiability condition in the two-layer and fixed-support setting. This approach contrasts with existing ones that fit the product of butterfly factors to a given matrix via gradient descent. The proposed method can be applied in particular to retrieve the factorization of the Hadamard or the discrete Fourier transform matrices of size . Computing such factorizations costs , which is on the order of dense matrix-vector multiplication, while the obtained factorizations enable fast matrix-vector multiplications and have the potential to be applied to compress deep neural networks.},
  archive      = {J_SIMODS},
  author       = {Léon Zheng and Elisa Riccietti and Rémi Gribonval},
  doi          = {10.1137/22M1488727},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {22-49},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Efficient identification of butterfly sparse matrix factorizations},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Core-periphery detection in hypergraphs. <em>SIMODS</em>,
<em>5</em>(1), 1–21. (<a
href="https://doi.org/10.1137/22M1480926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Core-periphery detection is a key task in exploratory network analysis where one aims to find a core, a set of nodes well connected internally and with the periphery, and a periphery, a set of nodes connected only (or mostly) with the core. In this work we propose a model of core-periphery for higher-order networks modeled as hypergraphs, and we propose a method for computing a core-score vector that quantifies how close each node is to the core. In particular, we show that this method solves the corresponding nonconvex core-periphery optimization problem globally to an arbitrary precision. This method turns out to coincide with the computation of the Perron eigenvector of a nonlinear hypergraph operator, suitably defined in terms of the incidence matrix of the hypergraph, generalizing recently proposed centrality models for hypergraphs. We perform several experiments on synthetic and real-world hypergraphs showing that the proposed method outperforms alternative core-periphery detection algorithms, in particular those obtained by transferring established graph methods to the hypergraph setting via clique expansion.},
  archive      = {J_SIMODS},
  author       = {Francesco Tudisco and Desmond J. Higham},
  doi          = {10.1137/22M1480926},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {1-21},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Core-periphery detection in hypergraphs},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
