<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SICOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sicomp---58">SICOMP - 58</h2>
<ul>
<li><details>
<summary>
(2023). Beating the integrality ratio for <span
class="math inline"><em>s</em></span>-<span
class="math inline"><em>t</em></span>-tours in graphs. <em>SICOMP</em>,
<em>52</em>(6), FOCS18-37-84. (<a
href="https://doi.org/10.1137/18M1227135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among various variants of the traveling salesman problem (TSP), the $s$-$t$-path graph TSP has the special feature that we know the exact integrality ratio, $\frac{3}{2}$, and an approximation algorithm matching this ratio. In this paper, we go below this threshold: we devise a polynomial-time algorithm for the $s$-$t$-path graph TSP with approximation ratio $1.497$. Our algorithm can be viewed as a refinement of the $\frac{3}{2}$-approximation algorithm in [A. Sebö and J. Vygen, Combinatorica, 34 (2014), pp. 597--629], but we introduce several completely new techniques. These include a new type of ear-decomposition, an enhanced ear induction that reveals a novel connection to matroid union, a stronger lower bound, and a reduction of general instances to instances in which $s$ and $t$ have small distance (which works for general metrics).},
  archive      = {J_SICOMP},
  author       = {Vera Traub and Jens Vygen},
  doi          = {10.1137/18M1227135},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-37-84},
  shortjournal = {SIAM J. Comput.},
  title        = {Beating the integrality ratio for $s$-$t$-tours in graphs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classical homomorphic encryption for quantum circuits.
<em>SICOMP</em>, <em>52</em>(6), FOCS18-189-215. (<a
href="https://doi.org/10.1137/18M1231055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first leveled fully homomorphic encryption scheme for quantum circuits with classical keys. The scheme allows a classical client to blindly delegate a quantum computation to a quantum server: an honest server is able to run the computation while a malicious server is unable to learn any information about the computation. We show that it is possible to construct such a scheme directly from a quantum secure classical homomorphic encryption scheme with certain properties. Finally, we show that a classical homomorphic encryption scheme with the required properties can be constructed from the learning with errors problem.},
  archive      = {J_SICOMP},
  author       = {Urmila Mahadev},
  doi          = {10.1137/18M1231055},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-189-215},
  shortjournal = {SIAM J. Comput.},
  title        = {Classical homomorphic encryption for quantum circuits},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum algorithm for simulating real time evolution of
lattice hamiltonians. <em>SICOMP</em>, <em>52</em>(6), FOCS18-250-284.
(<a href="https://doi.org/10.1137/18M1231511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of simulating the time evolution of a lattice Hamiltonian, where the qubits are laid out on a lattice and the Hamiltonian only includes geometrically local interactions (i.e., a qubit may only interact with qubits in its vicinity). This class of Hamiltonians is very general and is believed to capture fundamental interactions of physics. Our algorithm simulates the time evolution of such a Hamiltonian on $n$ qubits for time $T$ up to error $\epsilon$ using ${\mathcal O}( nT {polylog} (nT/\epsilon))$ gates with depth ${\mathcal O}(T { polylog} (nT/\epsilon))$. Our algorithm is the first simulation algorithm that achieves gate cost quasilinear in $nT$ and polylogarithmic in $1/\epsilon$. Our algorithm also readily generalizes to time-dependent Hamiltonians and yields an algorithm with similar gate count for any piecewise slowly varying time-dependent bounded local Hamiltonian. We also prove a matching lower bound on the gate count of such a simulation, showing that any quantum algorithm that can simulate a piecewise constant bounded local Hamiltonian in one dimension to constant error requires ${\widetilde{\Omega}}(nT)$ gates in the worst case. The lower bound holds even if we only require the output state to be correct on local measurements. To the best of our knowledge, this is the first nontrivial lower bound on the gate complexity of the simulation problem. Our algorithm is based on a decomposition of the time-evolution unitary into a product of small unitaries using Lieb--Robinson bounds. In the appendix, we prove a Lieb--Robinson bound tailored to Hamiltonians with small commutators between local terms, giving zero Lieb--Robinson velocity in the limit of commuting Hamiltonians. This improves the performance of our algorithm when the Hamiltonian is close to commuting.},
  archive      = {J_SICOMP},
  author       = {Jeongwan Haah and Matthew B. Hastings and Robin Kothari and Guang Hao Low},
  doi          = {10.1137/18M1231511},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-250-284},
  shortjournal = {SIAM J. Comput.},
  title        = {Quantum algorithm for simulating real time evolution of lattice hamiltonians},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limits on all known (and some unknown) approaches to matrix
multiplication. <em>SICOMP</em>, <em>52</em>(6), FOCS18-285-315. (<a
href="https://doi.org/10.1137/19M124695X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the known techniques for designing Matrix Multiplication algorithms. The two main approaches are the Laser method of Strassen, and the group theoretic approach of Cohn and Umans. We define a generalization based on zeroing outs which subsumes these two approaches, which we call the Solar method, and an even more general method based on monomial degenerations, which we call the Galactic method. We then design a suite of techniques for proving lower bounds on the value of $\omega$, the exponent of matrix multiplication, which can be achieved by algorithms using many tensors $T$ and the Galactic method. Some of our techniques exploit “local” properties of $T$, like finding a subtensor of $T$ which is so “weak” that $T$ itself couldn&#39;t be used to achieve a good bound on $\omega$, while others exploit “global” properties, like $T$ being a monomial degeneration of the structural tensor of a group algebra. Our main result is that there is a universal constant $\ell&gt;2$ such that a large class of tensors generalizing the Coppersmith--Winograd tensor $CW_q$ cannot be used within the Galactic method to show a bound on $\omega$ better than $\ell$ for any $q$. We give evidence that previous lower-bounding techniques were not strong enough to show this. We also prove a number of complementary results along the way, including that for any group $G$, the structural tensor of $\C[G]$ can be used to recover the best bound on $\omega$ which the Coppersmith--Winograd approach gets using $CW_{|G|-2}$ as long as the asymptotic rank of the structural tensor is not too large.},
  archive      = {J_SICOMP},
  author       = {Josh Alman and Virginia Vassilevska Williams},
  doi          = {10.1137/19M124695X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-285-315},
  shortjournal = {SIAM J. Comput.},
  title        = {Limits on all known (and some unknown) approaches to matrix multiplication},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-black-box worst-case to average-case reductions within
<span class="math inline">NP</span>. <em>SICOMP</em>, <em>52</em>(6),
FOCS18-349-382. (<a href="https://doi.org/10.1137/19M124705X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. There are significant obstacles to establishing an equivalence between the worst-case and average-case hardness of . Several results suggest that black-box worst-case to average-case reductions are not likely to be used for reducing any worst-case problem outside to a distributional problem. This paper overcomes the barrier. We present the first non-black-box worst-case to average-case reduction from a problem conjectured to be outside to a distributional problem. Specifically, we consider the minimum time-bounded Kolmogorov complexity problem (MINKT) and prove that there exists a zero-error randomized polynomial-time algorithm approximating the minimum time-bounded Kolmogorov complexity within an additive error if its average-case version admits an errorless heuristic polynomial-time algorithm. We observe that the approximation version of MINKT is Random 3SAT-hard, and more generally it is harder than avoiding any polynomial-time computable hitting set generator that extends its seed of length by , which provides strong evidence that the approximation problem is outside and thus our reductions are non-black-box. Our reduction can be derandomized at the cost of the quality of the approximation. We also show that, given a truth table of size , approximating the minimum circuit size within a factor of is in for some constant iff its average-case version is easy. Our results can be seen as a new approach for excluding Heuristica. In particular, proving -hardness of the approximation versions of MINKT or the minimum circuit size problem is sufficient for establishing an equivalence between the worst-case and average-case hardness of .},
  archive      = {J_SICOMP},
  author       = {Shuichi Hirahara},
  doi          = {10.1137/19M124705X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-349-382},
  shortjournal = {SIAM J. Comput.},
  title        = {Non-black-box worst-case to average-case reductions within \(\mathsf{NP}\)},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Near-optimal communication lower bounds for approximate nash
equilibria. <em>SICOMP</em>, <em>52</em>(6), FOCS18-316-348. (<a
href="https://doi.org/10.1137/19M1242069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove an $N^{2-o(1)}$ lower bound on the randomized communication complexity of finding an $\epsilon$-approximate Nash equilibrium (for constant $\epsilon&gt;0$) in a two-player $N\times N$ game.},
  archive      = {J_SICOMP},
  author       = {Mika Göös and Aviad Rubinstein},
  doi          = {10.1137/19M1242069},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-316-348},
  shortjournal = {SIAM J. Comput.},
  title        = {Near-optimal communication lower bounds for approximate nash equilibria},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constant factor approximation algorithm for weighted
flow-time on a single machine in PseudoPolynomial time. <em>SICOMP</em>,
<em>52</em>(6), FOCS18-158-188. (<a
href="https://doi.org/10.1137/19M1244512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the weighted flow-time problem on a single machine, we are given a set of $n$ jobs, where each job has a processing requirement $p_j$, release date $r_j$, and weight $w_j$. The goal is to find a preemptive schedule which minimizes the sum of weighted flow-time of jobs, where the flow-time of a job is the difference between its completion time and its released date. We give the first pseudo-polynomial time constant approximation algorithm for this problem. The algorithm also extends directly to the problem of minimizing the $\ell_p$ norm of weighted flow-times. The running time of our algorithm is polynomial in $n$, the number of jobs, and $P$, which is the ratio of the largest to the smallest processing requirement of a job. Our algorithm relies on a novel reduction of this problem to a generalization of the multicut problem on trees, which we call the \tt Demand MultiCut problem. Even though we do not give a constant factor approximation algorithm for the \tt Demand MultiCut problem on trees, we show that the specific instances of \tt Demand MultiCut obtained by reduction from weighted flow-time problem instances have more structure in them, and we are able to employ techniques based on dynamic programming. Our dynamic programming algorithm relies on showing that there are near optimal solutions which have nice smoothness properties, and we exploit these properties to reduce the size of the dynamic programming table.},
  archive      = {J_SICOMP},
  author       = {Jatin Batra and Naveen Garg and Amit Kumar},
  doi          = {10.1137/19M1244512},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-158-188},
  shortjournal = {SIAM J. Comput.},
  title        = {Constant factor approximation algorithm for weighted flow-time on a single machine in PseudoPolynomial time},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A faster isomorphism test for graphs of small degree.
<em>SICOMP</em>, <em>52</em>(6), FOCS18-1-36. (<a
href="https://doi.org/10.1137/19M1245293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent breakthrough, Babai [Proceedings of STOC, ACM, New York, 2016, pp. 684--697] gave a quasipolynomial-time graph isomorphism test. In this work, we give an improved isomorphism test for graphs of small degree: our algorithm runs in time $n^{\mathcal{O}((\log d)^{c})}$, where $n$ is the number of vertices of the input graphs, $d$ is the maximum degree of the input graphs, and $c$ is an absolute constant. The best previous isomorphism test for graphs of maximum degree $d$ due to Babai, Kantor, and Luks [Proceedings of FOCS, IEEE, New York, 1983, pp. 162--171] runs in time $n^{\mathcal{O}(d/ \log d)}$.},
  archive      = {J_SICOMP},
  author       = {Martin Grohe and Daniel Neuen and Pascal Schweitzer},
  doi          = {10.1137/19M1245293},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-1-36},
  shortjournal = {SIAM J. Comput.},
  title        = {A faster isomorphism test for graphs of small degree},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Random walks and forbidden minors i: An <span
class="math inline"><em>n</em><sup>1/2 + <em>o</em>(1)</sup></span>-query
one-sided tester for minor closed properties on bounded degree graphs.
<em>SICOMP</em>, <em>52</em>(6), FOCS18-216-249. (<a
href="https://doi.org/10.1137/19M1245463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $G$ be an undirected, bounded degree graph with $n$ vertices. Fix a finite graph $H$, and suppose one must remove $\varepsilon n$ edges from $G$ to make it $H$-minor-free (for some small constant $\varepsilon &gt; 0$). We give an $n^{1/2+o(1)}$-time randomized procedure that, with high probability, finds an $H$-minor in such a graph. As an application, suppose one must remove $\varepsilon n$ edges from a bounded degree graph $G$ to make it planar. This result implies an algorithm, with the same running time, that produces a $K_{3,3}$- or $K_5$-minor in $G$. No prior sublinear time bound was known for this problem. By the graph minor theorem, we get an analogous result for any minor-closed property. Up to $n^{o(1)}$ factors, this resolves a conjecture of Benjamini, Schramm, and Shapira [Adv. Math., 223 (2010), pp. 2200--2218] on the existence of one-sided property testers for minor-closed properties. Furthermore, our algorithm is nearly optimal by an $\Omega(\sqrt{n})$ lower bound of Czumaj et al. [Random Structures Algorithms, 45 (2014), pp. 139--184]. Prior to this work, the only graphs $H$ for which nontrivial one-sided property testers were known for $H$-minor-freeness were the following: $H$ being a forest or a cycle [Czumaj et al., Random Structures Algorithms, 45 (2014), pp. 139--184], $K_{2,k}$, $(k\times 2)$-grid, and the $k$-circus [Fichtenberger et al., preprint, arXiv:1707.06126v1, 2017].},
  archive      = {J_SICOMP},
  author       = {Akash Kumar and C. Seshadhri and Andrew Stolman},
  doi          = {10.1137/19M1245463},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-216-249},
  shortjournal = {SIAM J. Comput.},
  title        = {Random walks and forbidden minors i: An $n^{1/2+o(1)}$-query one-sided tester for minor closed properties on bounded degree graphs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph sparsification, spectral sketches, and faster
resistance computation via short cycle decompositions. <em>SICOMP</em>,
<em>52</em>(6), FOCS18-85-157. (<a
href="https://doi.org/10.1137/19M1247632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a framework for graph sparsification and sketching, based on a new tool, short cycle decomposition, which is a decomposition of an unweighted graph into an edge-disjoint collection of short cycles, plus a small number of extra edges. A simple observation shows that every graph $G$ on $n$ vertices with $m$ edges can be decomposed in $O(mn)$ time into cycles of length at most $2 \log n,$ and at most $2n$ extra edges. We give an $(m^{1+o(1)})$-time algorithm for constructing a short cycle decomposition, with cycles of length $n^{o(1)},$ and $n^{1+o(1)}$ extra edges. Both the existential and algorithmic variants of this decomposition enable us to make the following progress on several open problems in randomized graph algorithms: (1) We present an algorithm that runs in time $m^{1+o(1)}\varepsilon^{-1.5}$ and returns $(1\pm\varepsilon)$-approximations to effective resistances of all edges, improving over the previous best runtime of $\widetilde{{O}}(\min{m\varepsilon^{-2}, n^{2} \varepsilon^{-1}})$. This routine in turn gives an algorithm for approximating the determinant of a graph Laplacian up to a factor of $(1\pm \varepsilon)$ in $m^{1 + o(1)} + n^{\nicefrac{15}{8}+o(1)}\varepsilon^{-\nicefrac{7}{4}}$ time. (2) We show the existence of graphical spectral sketches with about $n\varepsilon^{-1}$ edges, and also give efficient algorithms to construct them. A graphical spectral sketch is a distribution over sparse graphs $H$ such that for a fixed vector ${\mathit{x}}$, we have ${{x}}^{\top} {L}_H {{x}} = (1\pm\varepsilon) {{x}}^{\top} {L}_G {{x}}$ and ${{x}}^{\top} {L}^{+}_H {{x}} = (1\pm\varepsilon) {{x}}^{\top} {L}^{+}_G {{x}}$ with high probability, where ${L}$ is the graph Laplacian and ${L}^{+}$ is its pseudoinverse. This implies the existence of resistance sparsifiers with about $n \varepsilon^{-1}$ edges that preserve the effective resistance between every pair of vertices up to $(1\pm\varepsilon)$. (3) By combining short cycle decompositions with known tools in graph sparsification, we show the existence of nearly linear sized degree-preserving spectral sparsifiers, as well as significantly sparser approximations of Eulerian directed graphs. The latter is critical to recent breakthroughs on faster algorithms for solving linear systems in directed Laplacians. The running time and output qualities of our spectral sketch and degree-preserving (directed) sparsification algorithms are limited by the efficiency of our routines for constructing short cycle decompositions. Improved algorithms for short cycle decompositions will lead to improvement in each of these algorithms.},
  archive      = {J_SICOMP},
  author       = {Timothy Chu and Yu Gao and Richard Peng and Sushant Sachdeva and Saurabh Sawlani and Junxing Wang},
  doi          = {10.1137/19M1247632},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-85-157},
  shortjournal = {SIAM J. Comput.},
  title        = {Graph sparsification, spectral sketches, and faster resistance computation via short cycle decompositions},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special section on the fifty-ninth annual IEEE symposium on
foundations of computer science (2018). <em>SICOMP</em>, <em>52</em>(6),
FOCS18–i. (<a href="https://doi.org/10.1137/23M1617011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SICOMP},
  author       = {Elette Boyle and Vincent Cohen-Addad and Alexandra Kolla and Mikkel Thorup},
  doi          = {10.1137/23M1617011},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {FOCS18-i},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the fifty-ninth annual IEEE symposium on foundations of computer science (2018)},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A structural theorem for local algorithms with applications
to coding, testing, and verification. <em>SICOMP</em>, <em>52</em>(6),
1413–1463. (<a href="https://doi.org/10.1137/21M1422781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove a general structural theorem for a wide family of local algorithms, which includes property testers, local decoders, and probabilistically checkable proofs of proximity. Namely, we show that the structure of every algorithm that makes adaptive queries and satisfies a natural robustness condition admits a sample-based algorithm with sample complexity, following the definition of Goldreich and Ron [ACM Trans. Comput. Theory, 8 (2016), 7]. We prove that this transformation is nearly optimal. Our theorem also admits a scheme for constructing privacy-preserving local algorithms. Using the unified view that our structural theorem provides, we obtain results regarding various types of local algorithms, including the following. We strengthen the state-of-the-art lower bound for relaxed locally decodable codes, obtaining an exponential improvement on the dependency in query complexity; this resolves an open problem raised by Gur and Lachish [SIAM J. Comput., 50 (2021), pp. 788–813]. We show that any (constant-query) testable property admits a sample-based tester with sublinear sample complexity; this resolves a problem left open in a work of Fischer, Lachish, and Vasudev [Proceedings of the 56th Annual Symposium on Foundations of Computer Science, IEEE, 2015, pp. 1163–1182], bypassing an exponential blowup caused by previous techniques in the case of adaptive testers. We prove that the known separation between proofs of proximity and testers is essentially maximal; this resolves a problem left open by Gur and Rothblum [Proceedings of the 8th Innovations in Theoretical Computer Science Conference, 2017, pp. 39:1–39:43; Comput. Complexity, 27 (2018), pp. 99–207] regarding sublinear-time delegation of computation. Our techniques strongly rely on relaxed sunflower lemmas and the Hajnal–Szemerédi theorem.},
  archive      = {J_SICOMP},
  author       = {Marcel Dall’Agnol and Tom Gur and Oded Lachish},
  doi          = {10.1137/21M1422781},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1413-1463},
  shortjournal = {SIAM J. Comput.},
  title        = {A structural theorem for local algorithms with applications to coding, testing, and verification},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A faster exponential time algorithm for bin packing with a
constant number of bins via additive combinatorics. <em>SICOMP</em>,
<em>52</em>(6), 1369–1412. (<a
href="https://doi.org/10.1137/22M1478112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the Bin Packing problem one is given items with weights and bins with capacities . The goal is to partition the items into sets such that for every bin , where denotes . Björklund, Husfeldt, and Koivisto [SIAM J. Comput., 39 (2009), pp. 546–563] presented an time algorithm for Bin Packing (the notation omits factors polynomial in the input size). In this paper, we show that for every there exists a constant such that an instance of Bin Packing with bins can be solved in randomized time. Before our work, such improved algorithms were not known even for . A key step in our approach is the following new result in Littlewood–Offord theory on the additive combinatorics of subset sums: For every there exists an such that if for some , then .},
  archive      = {J_SICOMP},
  author       = {Jesper Nederlof and Jakub Pawlewicz and Céline M. F. Swennenhuis and Karol Węgrzycki},
  doi          = {10.1137/22M1478112},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1369-1412},
  shortjournal = {SIAM J. Comput.},
  title        = {A faster exponential time algorithm for bin packing with a constant number of bins via additive combinatorics},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampling graphs without forbidden subgraphs and unbalanced
expanders with negligible error. <em>SICOMP</em>, <em>52</em>(6),
1321–1368. (<a href="https://doi.org/10.1137/22M1484134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Suppose that you wish to sample a random graph over vertices and edges conditioned on the event that does not contain a “small” -size graph (e.g., clique) as a subgraph. Assuming that most such graphs are -free, the problem can be solved by a simple rejected-sampling algorithm (that tests for -cliques) with an expected running time of . Is it possible to solve the problem in a running time that does not grow polynomially with ? In this paper, we introduce the general problem of sampling a “random looking” graph with a given edge density that avoids some arbitrary predefined -size subgraph . As our main result, we show that the problem is solvable with respect to some specially crafted -wise independent distribution over graphs. That is, we design a sampling algorithm for -wise independent graphs that supports efficient testing for subgraph-freeness in time , where is a function of and the constant in the exponent is independent of . Our solution extends to the case where both and are -uniform hypergraphs. We use these algorithms to obtain the first probabilistic construction of constant-degree polynomially unbalanced expander graphs whose failure probability is negligible in (i.e., ). In particular, given constants , we output a bipartite graph that has left nodes and right nodes with right-degree of so that any right set of size at most expands by factor of . This result is extended to the setting of unique expansion as well. We observe that such a negligible-error construction can be employed in many useful settings and present applications in coding theory (batch codes and low-density parity-check codes), pseudorandomness (low-bias generators and randomness extractors), and cryptography. Notably, we show that our constructions yield a collection of polynomial-stretch locally computable cryptographic pseudorandom generators based on Goldreich’s one-wayness assumption resolving a central open problem in the area of parallel-time cryptography (e.g., Applebaum, Ishai, and Kushilevitz [SIAM J. Comput., 36 (2006), pp. 845–888] and Ishai et al. [Proceedings of the 40th Annual ACM Symposium on Theory of Computing, ACM, 2008, pp. 433–442]).},
  archive      = {J_SICOMP},
  author       = {Benny Applebaum and Eliran Kachlon},
  doi          = {10.1137/22M1484134},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1321-1368},
  shortjournal = {SIAM J. Comput.},
  title        = {Sampling graphs without forbidden subgraphs and unbalanced expanders with negligible error},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deterministic near-optimal approximation algorithms for
dynamic set cover. <em>SICOMP</em>, <em>52</em>(5), 1132–1192. (<a
href="https://doi.org/10.1137/21M1428649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the dynamic minimum set cover problem, the challenge is to minimize the update time while guaranteeing a close-to-optimal approximation factor. (Throughout, and are parameters denoting the maximum number of elements, the number of sets, the frequency, and the cost range.) In the high-frequency range, when , this was achieved by a deterministic -approximation algorithm with amortized update time by Gupta et al. [Online and dynamic algorithms for set cover, in Proceedings STOC 2017, ACM, pp. 537–550]. In this paper we consider the low-frequency range, when , and obtain deterministic algorithms with a -approximation ratio and the following guarantees on the update time. (1) amortized update time: Prior to our work, the best approximation ratio guaranteed by deterministic algorithms was of Bhattacharya, Henzinger, and Italiano [Design of dynamic algorithms via primal-dual method, in Proceedings ICALP 2015, Springer, pp. 206–218]. In contrast, the only result with -approximation was that of Abboud et al. [Dynamic set cover: Improved algorithms and lower bounds, in Proceedings STOC 2019, ACM, pp. 114–125], who designed a randomized -approximation algorithm with amortized update time. (2) amortized update time: This result improves the above update time bound for most values of in the low-frequency range, i.e., . It is also the first result that is independent of and . It subsumes the constant amortized update time of Bhattacharya and Kulkarni [Deterministically maintaining a -approximate minimum vertex cover in amortized update time, in Proceedings SODA 2019, SIAM, pp. 1872–1885] for unweighted dynamic vertex cover (i.e., when and ). (3) worst-case update time: No nontrivial worst-case update time was previously known for the dynamic set cover problem. Our bound subsumes and improves by a logarithmic factor the worst-case update time for the unweighted dynamic vertex cover problem (i.e., when and ) of Bhattacharya, Henzinger, and Nanongkai [Fully dynamic approximate maximum matching and minimum vertex cover in worst case update time, in Proceedings SODA 2017, SIAM, pp. 470–489]. We achieve our results via the primal-dual approach, by maintaining a fractional packing solution as a dual certificate. Prior work in dynamic algorithms that employs the primal-dual approach uses a local update scheme that maintains relaxed complementary slackness conditions for every set. For our first result we use instead a global update scheme that does not always maintain complementary slackness conditions. For our second result we combine the global and the local update schema. To achieve our third result we use a hierarchy of background schedulers. It is an interesting open question whether this background scheduler technique can also be used to transform algorithms with amortized running time bounds into algorithms with worst-case running time bounds.},
  archive      = {J_SICOMP},
  author       = {Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai and Xiaowei Wu},
  doi          = {10.1137/21M1428649},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1132-1192},
  shortjournal = {SIAM J. Comput.},
  title        = {Deterministic near-optimal approximation algorithms for dynamic set cover},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact-size sampling of enriched trees in linear time.
<em>SICOMP</em>, <em>52</em>(5), 1097–1131. (<a
href="https://doi.org/10.1137/21M1459733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We create a novel connection between Boltzmann sampling methods and Devroye’s algorithm to develop highly efficient sampling procedures that generate objects from important combinatorial classes with a given size in expected time . This performance is best possible and significantly improves the state of the art for samplers of subcritical graph classes (such as cactus graphs, outerplanar graphs, and series-parallel graphs), subcritical substitution-closed classes of permutations, Bienaymé–Galton–Watson trees conditioned on their number of leaves, and several further examples. Our approach allows for this high level of universality, as it applies in general to classes admitting bijective encodings by so-called enriched trees, which are rooted trees with additional structures on the offspring of each node.},
  archive      = {J_SICOMP},
  author       = {Konstantinos Panagiotou and Leon Ramzews and Benedikt Stufler},
  doi          = {10.1137/21M1459733},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1097-1131},
  shortjournal = {SIAM J. Comput.},
  title        = {Exact-size sampling of enriched trees in linear time},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A polynomial-time algorithm for 1/2-well-supported nash
equilibria in bimatrix games. <em>SICOMP</em>, <em>52</em>(5),
1083–1096. (<a href="https://doi.org/10.1137/23M1549237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Since the seminal PPAD-completeness result for computing a Nash equilibrium even in two-player games, an important line of research has focused on relaxations achievable in polynomial time. In this paper, we consider the notion of an -well-supported Nash equilibrium, where corresponds to the approximation guarantee. Put simply, in an -well-supported equilibrium, every player chooses with positive probability actions that are within of the maximum achievable payoff against the other player’s strategy. Ever since the initial approximation guarantee of 2/3 for well-supported equilibria, which was established more than a decade ago, the progress on this problem has been extremely slow and incremental. Notably, the small improvements to 0.6608, and finally to 0.6528, were achieved by algorithms of growing complexity. Our main result is a simple and intuitive algorithm that improves the approximation guarantee to 1/2. Our algorithm is based on linear programming and in particular on exploiting suitably defined zero-sum games that arise from the payoff matrices of the two players. As a byproduct, we show how to achieve the same approximation guarantee in a query-efficient way.},
  archive      = {J_SICOMP},
  author       = {Argyrios Deligkas and Michail Fasoulakis and Evangelos Markakis},
  doi          = {10.1137/23M1549237},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1083-1096},
  shortjournal = {SIAM J. Comput.},
  title        = {A polynomial-time algorithm for 1/2-well-supported nash equilibria in bimatrix games},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Average sensitivity of graph algorithms. <em>SICOMP</em>,
<em>52</em>(4), 1039–1081. (<a
href="https://doi.org/10.1137/21M1399592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Modern applications of graph algorithms often involve the use of the output sets (usually, a subset of edges or vertices of the input graph) as inputs to other algorithms. Since the input graphs of interest are large and dynamic, it is desirable for an algorithm’s output to not change drastically when a few random edges are removed from the input graph, so as to prevent issues in postprocessing. Alternately, having such a guarantee also means that one can revise the solution obtained by running the algorithm on the original graph in just a few places in order to obtain a solution for the new graph. We formalize this feature by introducing the notion of average sensitivity of graph algorithms, which is the average earth mover’s distance between the output distributions of an algorithm on a graph and its subgraph obtained by removing an edge, where the average is over the edges removed and the distance between two outputs is the Hamming distance. In this work, we initiate a systematic study of average sensitivity of graph algorithms. After deriving basic properties of average sensitivity such as composition, we provide efficient approximation algorithms with low average sensitivities for concrete graph problems, including the minimum spanning forest problem, the global minimum cut problem, the minimum - cut problem, and the maximum matching problem. In addition, we prove that the average sensitivity of our global minimum cut algorithm is almost optimal, by showing a nearly matching lower bound. We also show that every algorithm for the 2-coloring problem has average sensitivity linear in the number of vertices. One of the main ideas involved in designing our algorithms with low average sensitivity is the following fact: if the presence of a vertex or an edge in the solution output by an algorithm can be decided locally, then the algorithm has a low average sensitivity, allowing us to reuse the analyses of known sublinear-time algorithms and local computation algorithms. Using this fact in conjunction with our average sensitivity lower bound for 2-coloring, we show that every local computation algorithm for 2-coloring has query complexity linear in the number of vertices, thereby answering an open question.},
  archive      = {J_SICOMP},
  author       = {Nithin Varma and Yuichi Yoshida},
  doi          = {10.1137/21M1399592},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1039-1081},
  shortjournal = {SIAM J. Comput.},
  title        = {Average sensitivity of graph algorithms},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dequantizing the quantum singular value transformation:
Hardness and applications to quantum chemistry and the quantum PCP
conjecture. <em>SICOMP</em>, <em>52</em>(4), 1009–1038. (<a
href="https://doi.org/10.1137/22M1513721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Quantum Singular Value Transformation (QSVT) is a recent technique that gives a unified framework to describe most quantum algorithms discovered so far, and may lead to the development of novel quantum algorithms. In this paper we investigate the hardness of classically simulating the QSVT. A recent result by Chia et al. [Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning, in Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC 2020), 2020, pp. 387–400] showed that the QSVT can be efficiently “dequantized” for low-rank matrices, and discussed its implication to quantum machine learning. In this work, motivated by establishing the superiority of quantum algorithms for quantum chemistry and making progress on the quantum PCP conjecture, we focus on the other main class of matrices considered in applications of the QSVT, sparse matrices. We first show how to efficiently “dequantize”, with arbitrarily small constant precision, the QSVT associated with a low-degree polynomial. We apply this technique to design classical algorithms that estimate, with constant precision, the singular values of a sparse matrix. We show, in particular, that a central computational problem considered by quantum algorithms for quantum chemistry (estimating the ground state energy of a local Hamiltonian when given, as an additional input, a state sufficiently close to the ground state) can be solved efficiently with constant precision on a classical computer. As a complementary result, we prove that with inverse-polynomial precision, the same problem becomes -complete. This gives theoretical evidence for the superiority of quantum algorithms for chemistry, and strongly suggests that said superiority stems from the improved precision achievable in the quantum setting. We also discuss how this dequantization technique may help make progress on the central quantum PCP conjecture.},
  archive      = {J_SICOMP},
  author       = {Sevag Gharibian and François Le Gall},
  doi          = {10.1137/22M1513721},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1009-1038},
  shortjournal = {SIAM J. Comput.},
  title        = {Dequantizing the quantum singular value transformation: Hardness and applications to quantum chemistry and the quantum PCP conjecture},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sublinear algorithms for local graph-centrality estimation.
<em>SICOMP</em>, <em>52</em>(4), 968–1008. (<a
href="https://doi.org/10.1137/19M1266976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the complexity of local graph-centrality estimation, with the goal of approximating the centrality score of a given target node while exploring only a sublinear number of nodes/arcs of the graph and performing a sublinear number of elementary operations. We develop a technique, which we apply to PageRank and Heat Kernel, for constructing a low-variance score estimator through a local exploration of the graph. We obtain an algorithm that, given any node in any graph of nodes and arcs, with probability computes a multiplicative -approximation of its score by examining only nodes/arcs, where is the maximum outdegree of the graph and and factors are omitted for readability. A similar bound holds for computational cost. We also prove a lower bound of for both query complexity and computational complexity. Moreover, in the jump-and-crawl graph-access model, our technique yields a -queries algorithm; we show that this algorithm is optimal up to a logarithmic factor—in fact, sublogarithmic in the case of PageRank. These are the first algorithms with sublinear worst-case bounds for general directed graphs and any choice of the target node.},
  archive      = {J_SICOMP},
  author       = {Marco Bressan and Enoch Peserico and Luca Pretto},
  doi          = {10.1137/19M1266976},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {968-1008},
  shortjournal = {SIAM J. Comput.},
  title        = {Sublinear algorithms for local graph-centrality estimation},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deterministic <span
class="math inline">$\boldsymbol{(\unicode{x00BD}+\varepsilon)}$</span>
-approximation for submodular maximization over a matroid.
<em>SICOMP</em>, <em>52</em>(4), 945–967. (<a
href="https://doi.org/10.1137/19M125515X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of maximizing a monotone submodular function subject to a matroid constraint and present a deterministic algorithm that achieves -approximation for the problem (for some ). This algorithm is the first deterministic algorithm known to improve over the -approximation ratio of the classical greedy algorithm proved by Nemhauser, Wolsey, and Fisher in 1978.},
  archive      = {J_SICOMP},
  author       = {Niv Buchbinder and Moran Feldman and Mohit Garg},
  doi          = {10.1137/19M125515X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {945-967},
  shortjournal = {SIAM J. Comput.},
  title        = {Deterministic \(\boldsymbol{(\unicode{x00BD}+\varepsilon)}\) -approximation for submodular maximization over a matroid},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Why extension-based proofs fail. <em>SICOMP</em>,
<em>52</em>(4), 913–944. (<a
href="https://doi.org/10.1137/20M1375851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce extension-based proofs, a class of impossibility proofs that includes valency arguments. They are modelled as an interaction between a prover and a protocol. Using proofs based on combinatorial topology, it has been shown that it is impossible to deterministically solve -set agreement among processes or approximate agreement on a cycle of length 4 among processes in a wait-free manner in asynchronous models where processes communicate using objects that can be constructed from shared registers. However, it was unknown whether proofs based on simpler techniques were possible. We show that these impossibility results cannot be obtained by extension-based proofs in the iterated snapshot model and, hence, extension-based proofs are limited in power.},
  archive      = {J_SICOMP},
  author       = {Dan Alistarh and James Aspnes and Faith Ellen and Rati Gelashvili and Leqi Zhu},
  doi          = {10.1137/20M1375851},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {913-944},
  shortjournal = {SIAM J. Comput.},
  title        = {Why extension-based proofs fail},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hitting minors on bounded treewidth graphs. IV. An optimal
algorithm. <em>SICOMP</em>, <em>52</em>(4), 865–912. (<a
href="https://doi.org/10.1137/21M140482X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For a fixed finite collection of graphs the -M-Deletion problem is as follows: given an -vertex input graph find the minimum number of vertices that intersect all minor models in of the graphs in . by Courcelle’s Theorem, this problem can be solved in time where is the treewidth of for some function depending on . In a recent series of articles, we have initiated the program of optimizing asymptotically the function . Here we provide an algorithm showing that for every collection . Prior to this work, the best known function was double-exponential in . In particular, our algorithm vastly extends the results of Jansen, Lokshtanov, and Saurabh [Proc. of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2014, pp. 1802–1811] for the particular case and of Kociumaka and Pilipczuk [Algorithmica, 81 (2019), pp. 3655–3691] for graphs of bounded genus, and answers an open problem posed by Cygan et al. [Inform. Comput., 256 (2017), pp. 62–82]. We combine several ingredients such as the machinery of boundaried graphs in dynamic programming via representatives, the Flat Wall Theorem, bidimensionality, the irrelevant vertex technique, treewidth modulators, and protrusion replacement. Together with our previous results providing single-exponential algorithms for particular collections [J. Baste, I. Sau, and D. M. Thilikos, Theoret. Comput. Sci., 814 (2020), pp. 135–152] and general lower bounds [J. Baste, I. Sau, and D. M. Thilikos, J. Comput. Syst. Sci., 109 (2020), pp. 56–77], our algorithm yields the following complexity dichotomy when contains a single connected graph assuming the Exponential Time Hypothesis: if is a contraction of the chair or the banner, and otherwise.},
  archive      = {J_SICOMP},
  author       = {Julien Baste and Ignasi Sau and Dimitrios M. Thilikos},
  doi          = {10.1137/21M140482X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {865-912},
  shortjournal = {SIAM J. Comput.},
  title        = {Hitting minors on bounded treewidth graphs. IV. an optimal algorithm},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The approximation ratio of the k-opt heuristic for the
euclidean traveling salesman problem. <em>SICOMP</em>, <em>52</em>(4),
841–864. (<a href="https://doi.org/10.1137/21M146199X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The -Opt heuristic is a simple improvement heuristic for the traveling salesman problem. It starts with an arbitrary tour and then repeatedly replaces edges of the tour by other edges, as long as this yields a shorter tour. We will prove that for the 2-dimensional Euclidean traveling salesman problem with cities the approximation ratio of the -Opt heuristic is . This improves the upper bound of given by Chandra, Karloff, and Tovey in [SIAM J. Comput., 28 (1999), pp. 1998–2029] and provides for the first time a nontrivial lower bound for the case . Our results not only hold for the Euclidean norm but extend to arbitrary -norms with .},
  archive      = {J_SICOMP},
  author       = {Ulrich A. Brodowsky and Stefan Hougardy and Xianghui Zhong},
  doi          = {10.1137/21M146199X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {841-864},
  shortjournal = {SIAM J. Comput.},
  title        = {The approximation ratio of the k-opt heuristic for the euclidean traveling salesman problem},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved list decoding of folded reed-solomon and
multiplicity codes. <em>SICOMP</em>, <em>52</em>(3), 794–840. (<a
href="https://doi.org/10.1137/20M1370215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We show new and improved list decoding properties of folded Reed–Solomon (RS) codes and multiplicity codes. Both of these families of codes are based on polynomials over finite fields, and both have been the source of recent advances in coding theory: folded RS codes were the first known explicit construction of capacity-achieving list decodable codes [V. Guruswami and A. Rudra, IEEE Trans. Inform. Theory, 54 (2008), pp. 135–150], and multiplicity codes were the first construction of high-rate locally decodable codes [S. Kopparty, S. Saraf, and S. Yekhanin, J. ACM, 61 (2014), 28]. In this work, we show that folded RS codes and multiplicity codes are in fact better than previously known in the context of list decoding and local list decoding. Our first main result shows that folded RS codes achieve list decoding capacity with constant list sizes, independent of the block length. Prior work with constant list sizes first obtained list sizes that are polynomial in the block length and relied on pre-encoding with subspace evasive sets to reduce the list sizes to a constant [V. Guruswami and C. Wang, IEEE Trans. Inform. Theory, 59 (2013), pp. 3257–3268], [Z. Dvir and S. Lovett, Proc. 44th STOC, ACM, 2012, 351–358]. The list size we obtain is where is the gap to capacity, which matches the list size obtained by pre-encoding with subspace evasive sets. For our second main result, we observe that univariate multiplicity codes exhibit similar behavior, and we use this, together with additional ideas, to show that multivariate multiplicity codes are locally list decodable up to their minimum distance. By known reductions, this gives, in turn, capacity-achieving locally list decodable codes with query complexity . This improves on the tensor-based construction of [B. Hemenway, N. Ron-Zewi, and M. Wootters, SIAM J. Comput., 49 (2019), pp. 157–195], which gave capacity-achieving locally list decodable codes of query complexity , and is close to the best known query complexity of for high-rate locally (uniquely) decodable codes [S. Kopparty et al., J. ACM, 64 (2017), 11].},
  archive      = {J_SICOMP},
  author       = {Swastik Kopparty and Noga Ron-Zewi and Shubhangi Saraf and Mary Wootters},
  doi          = {10.1137/20M1370215},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {794-840},
  shortjournal = {SIAM J. Comput.},
  title        = {Improved list decoding of folded reed-solomon and multiplicity codes},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Near-optimal learning of tree-structured distributions by
chow and liu. <em>SICOMP</em>, <em>52</em>(3), 761–793. (<a
href="https://doi.org/10.1137/22M1489678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide finite sample guarantees for the classical Chow–Liu algorithm [Chow and Liu, IEEE Trans. Inform. Theory, 14 (1968), pp. 462–467] to learn a tree-structured graphical model of a distribution. For a distribution on and a tree on nodes, we say is an -approximate tree for if there is a -structured distribution such that is at most more than the best possible tree-structured distribution for . We show that if itself is tree-structured, then the Chow–Liu algorithm with the plug-in estimator for mutual information with independent and identically distributed samples outputs an -approximate tree for with constant probability. In contrast, for a general (which may not be tree-structured), samples are necessary to find an -approximate tree. Our upper bound is based on a new conditional independence tester that addresses an open problem posed by Canonne et al. [Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, ACM, 2018, pp. 735–748]: we prove that for three random variables each over , testing if is 0 or is possible with samples. Finally, we show that for a specific tree , with samples from a distribution over , one can efficiently learn the closest -structured distribution in KL divergence by applying the add-1 estimator at each node.},
  archive      = {J_SICOMP},
  author       = {Arnab Bhattacharyya and Sutanu Gayen and Eric Price and Vincent Y. F. Tan and N. V. Vinodchandran},
  doi          = {10.1137/22M1489678},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {761-793},
  shortjournal = {SIAM J. Comput.},
  title        = {Near-optimal learning of tree-structured distributions by chow and liu},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ETH-tight exact algorithm for euclidean TSP.
<em>SICOMP</em>, <em>52</em>(3), 740–760. (<a
href="https://doi.org/10.1137/22M1469122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study exact algorithms for Metric TSP in . In the early 1990s, algorithms with running time were presented for the planar case, and some years later an algorithm with running time was presented for any . Despite significant interest in subexponential exact algorithms over the past decade, there has been no progress on Metric TSP, except for a lower bound stating that the problem admits no algorithm unless ETH fails. In this paper we settle the complexity of Metric TSP, up to constant factors in the exponent and under ETH, by giving an algorithm with running time .},
  archive      = {J_SICOMP},
  author       = {Mark de Berg and Hans L. Bodlaender and Sándor Kisfaludi-Bak and Sudeshna Kolay},
  doi          = {10.1137/22M1469122},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {740-760},
  shortjournal = {SIAM J. Comput.},
  title        = {An ETH-tight exact algorithm for euclidean TSP},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaching the 2-approximation barrier for connectivity
augmentation: A reduction to steiner tree. <em>SICOMP</em>,
<em>52</em>(3), 718–739. (<a
href="https://doi.org/10.1137/21M1421143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The basic goal of survivable network design is to build a cheap network that maintains the connectivity between given sets of nodes despite the failure of a few edges/nodes. The connectivity augmentation problem is arguably one of the most basic problems in this area: given a (-edge)-connected graph and a set of extra edges (links), select a minimum cardinality subset of links such that adding to increases its edge connectivity to . Intuitively, one wants to make an existing network more reliable by augmenting it with extra edges. The best known approximation factor for this NP-hard problem is 2, and this can be achieved with multiple approaches (the first such result is in [G. N. Frederickson and Jájá, SIAM J. Comput., 10 (1981), pp. 270–283]. It is known [E. A. Dinitz, A. V. Karzanov, and M. V. Lomonosov, Studies in Discrete Optimization, Nauka, Moscow, 1976, pp. 290–306] that can be reduced to the case , also known as the tree augmentation problem for odd , and to the case , also known as the cactus augmentation problem for even . Prior to the conference version of this paper [J. Byrka, F. Grandoni, and A. Jabal Ameli, STOC’20, ACM, New York, 2020, pp. 815–825], several better than 2 approximation algorithms were known for , culminating with a recent approximation [F. Grandoni, C. Kalaitzis, and R. Zenklusen, STOC’18, ACM, New York, 1918, pp. 632–645]. However, for the best known approximation was 2. In this paper we breach the 2 approximation barrier for , hence, for , by presenting a polynomial-time approximation. From a technical point of view, our approach deviates quite substantially from previous work. In particular, the better-than-2 approximation algorithms for either exploit greedy-style algorithms or are based on rounding carefully designed LPs. We instead use a reduction to the Steiner tree problem which was previously used in parameterized algorithms [Basavaraju et al., ICALP ’14, Springer, Berlin, 2014, pp. 800–811]. This reduction is not approximation preserving, and using the current best approximation factor for a Steiner tree [Byrka et al., J. ACM, 60 (2013), 6] as a black box would not be good enough to improve on 2. To achieve the latter goal, we “open the box” and exploit the specific properties of the instances of a Steiner tree arising from . In our opinion this connection between approximation algorithms for survivable network design and Steiner-type problems is interesting, and might lead to other results in the area.},
  archive      = {J_SICOMP},
  author       = {Jarosław Byrka and Fabrizio Grandoni and Afrouz Jabal Ameli},
  doi          = {10.1137/21M1421143},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {718-739},
  shortjournal = {SIAM J. Comput.},
  title        = {Breaching the 2-approximation barrier for connectivity augmentation: A reduction to steiner tree},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized singleton bound and list-decoding reed–solomon
codes beyond the johnson radius. <em>SICOMP</em>, <em>52</em>(3),
684–717. (<a href="https://doi.org/10.1137/20M138795X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper we take a combinatorial approach to the problem of list-decoding, which allows us to determine the precise relation (up to the exact constant) between the decoding radius, list size, and code rate. We prove a generalized Singleton bound for a given list size, and conjecture that the bound is tight for most Reed–Solomon (RS) codes over large enough finite fields. We also show that the conjecture holds true for list sizes 2 and 3, and as a by product show that most RS codes with a rate of at least 1/9 are list-decodable beyond the Johnson radius. Last, we give the first explicit construction in the literature of such RS codes. The main tools used in the proof are a new type of linear dependency between codewords of a code that are contained in a small Hamming ball, and a surprising connection between list-decoding and the notion of cycle space in graph theory. Both of them are new, and may be of independent interest.},
  archive      = {J_SICOMP},
  author       = {Chong Shangguan and Itzhak Tamo},
  doi          = {10.1137/20M138795X},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {684-717},
  shortjournal = {SIAM J. Comput.},
  title        = {Generalized singleton bound and list-decoding Reed–Solomon codes beyond the johnson radius},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform restricted chase termination. <em>SICOMP</em>,
<em>52</em>(3), 641–683. (<a
href="https://doi.org/10.1137/20M1377035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The chase procedure is a fundamental algorithmic tool in database theory with a variety of applications. A central problem concerning the chase procedure is uniform (a.k.a. all-instances) chase termination: for a given set of tuple-generating dependencies (TGDs), is it the case that the chase terminates for every input database? In view of the fact that this problem is, in general, undecidable, it is natural to ask whether known well-behaved classes of TGDs ensure decidability. We focus on the main paradigms that led to robust TGD-based formalisms, namely guardedness and stickiness, that have been introduced in the context of knowledge-enriched databases. Although uniform chase termination is well understood for the oblivious version of the chase (2EXPTIME-complete for guarded, and PSPACE-complete for sticky TGDs), the more subtle case of the restricted (a.k.a. the standard) chase is rather unexplored. We show that uniform restricted chase termination under guarded single-head TGDs and sticky single-head TGDs is decidable in elementary time. In the case of guardedness, we provide a reduction to the satisfiability problem of monadic second-order logic over infinite trees of bounded degree, while for stickiness we provide a reduction to the emptiness problem of deterministic Büchi automata. Those reductions build on a series of technical results of independent interest related to the notion of fairness of the restricted chase, and the existence of critical databases that characterize nontermination of the restricted chase via databases of a certain form.},
  archive      = {J_SICOMP},
  author       = {Tomasz Gogacz and Jerzy Marcinkowski and Andreas Pieris},
  doi          = {10.1137/20M1377035},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {641-683},
  shortjournal = {SIAM J. Comput.},
  title        = {Uniform restricted chase termination},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral methods from tensor networks. <em>SICOMP</em>,
<em>52</em>(2), STOC19-354-384. (<a
href="https://doi.org/10.1137/20M1311661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A tensor network is a diagram that specifies a way to “multiply” a collection of tensors together to produce another tensor (or matrix). Many existing algorithms for tensor problems (such as tensor decomposition and tensor PCA), although they are not presented this way, can be viewed as spectral methods on matrices built from simple tensor networks. In this work we leverage the full power of this abstraction to design new algorithms for certain continuous tensor decomposition problems. An important and challenging family of tensor problems comes from orbit recovery, a class of inference problems involving group actions (inspired by applications such as cryo-electron microscopy). Orbit recovery problems over finite groups can often be solved via standard tensor methods. However, for infinite groups, no general algorithms are known. We give a new spectral algorithm based on tensor networks for one such problem: continuous multi-reference alignment over the infinite group SO(2). Our algorithm extends to the more general heterogeneous case.},
  archive      = {J_SICOMP},
  author       = {Ankur Moitra and Alexander S. Wein},
  doi          = {10.1137/20M1311661},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-354-384},
  shortjournal = {SIAM J. Comput.},
  title        = {Spectral methods from tensor networks},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decremental strongly connected components and single-source
reachability in near-linear time. <em>SICOMP</em>, <em>52</em>(2),
STOC19-128-155. (<a href="https://doi.org/10.1137/20M1312149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the strongly connected Components (SCCs) in a graph $G=(V,E)$ is known to take only $O(m + n)$ time using an algorithm by Tarjan [SIAM J. Comput., 1 (1972), pp. 146--160] where $m = |E|$, $n=|V|$. For fully dynamic graphs, conditional lower bounds provide evidence that the update time cannot be improved by polynomial factors over recomputing the SCCs from scratch after every update. Nevertheless, substantial progress has been made to find algorithms with fast update time for decremental graphs, i.e., graphs that undergo edge deletions. In this paper, we present the first algorithm for general decremental graphs that maintains the SCCs in total update time $\tilde{O}(m)$, thus only a polylogarithmic factor from the optimal running time. (We use $\tilde{O}(f(n))$ notation to suppress logarithmic factors, i.e., $g(n) = \tilde{O}(f(n))$ if $g(n) = O(f(n) {polylog}(n)).$) Our result also yields the fastest algorithm for the decremental single-source reachability (SSR) problem which can be reduced to decrementally maintaining SCCs. Using a well-known reduction, we use our decremental result to achieve new update/query-time trade-offs in the fully dynamic setting. We can maintain the reachability of pairs $S \times V$, $S \subseteq V$ in fully dynamic graphs with update time $\tilde{O}(\frac{|S|m}{t})$ and query time $O(t)$ for all $t \in [1,|S|]$; this matches to polylogarithmic factors the best all-pairs reachability algorithm for $S = V$.},
  archive      = {J_SICOMP},
  author       = {Aaron Bernstein and Maximilian Probst Gutenberg and Christian Wulff-Nilsen},
  doi          = {10.1137/20M1312149},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-128-155},
  shortjournal = {SIAM J. Comput.},
  title        = {Decremental strongly connected components and single-source reachability in near-linear time},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Competitively chasing convex bodies. <em>SICOMP</em>,
<em>52</em>(2), STOC19-339-353. (<a
href="https://doi.org/10.1137/20M1312332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Let be a family of sets in some metric space. In the -chasing problem, an online algorithm observes a request sequence of sets in and responds (online) by giving a sequence of points in these sets. The movement cost is the distance between consecutive such points. The competitive ratio is the worst case ratio (over request sequences) between the total movement of the online algorithm and the smallest movement one could have achieved by knowing in advance the request sequence. The family is said to be chaseable if there exists an online algorithm with finite competitive ratio. In 1991, Linial and Friedman conjectured that the family of convex sets in Euclidean space is chaseable. We prove this conjecture.},
  archive      = {J_SICOMP},
  author       = {Sébastien Bubeck and Yin Tat Lee and Yuanzhi Li and Mark Sellke},
  doi          = {10.1137/20M1312332},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-339-353},
  shortjournal = {SIAM J. Comput.},
  title        = {Competitively chasing convex bodies},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Near-optimal lower bounds on the threshold degree and
sign-rank of AC<span class="math inline"><sup>0</sup></span>.
<em>SICOMP</em>, <em>52</em>(2), STOC19-1-86. (<a
href="https://doi.org/10.1137/20M1312459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The threshold degree of a Boolean function $f\colon{{{0,1}}^n}\to{{0,1}}$ is the minimum degree of a real polynomial $p$ that represents $f$ in sign: ${sgn}\,p(x)=(-1)^{f(x)}.$ A related notion is sign-rank, defined for a Boolean matrix $F=[F_{ij}]$ as the minimum rank of a real matrix $M$ with ${sgn}\,M_{ij}=(-1)^{F_{ij}}$. Determining the maximum threshold degree and sign-rank achievable by constant-depth circuits (${AC}^{0}$) is a well-known and extensively studied open problem with complexity-theoretic and algorithmic applications. We give an essentially optimal solution to this problem. For any $\epsilon&gt;0,$ we construct an ${AC}^{0}$ circuit in $n$ variables that has threshold degree $\Omega(n^{1-\epsilon})$ and sign-rank $\exp(\Omega(n^{1-\epsilon})),$ improving on the previous best lower bounds of $\Omega(\sqrt{n})$ and $\exp(\tilde{\Omega}(\sqrt{n}))$, respectively. Our results subsume all previous lower bounds on the threshold degree and sign-rank of ${AC}^{0}$ circuits of any given depth, with a strict improvement starting at depth 4. As a corollary, we also obtain near-optimal bounds on the discrepancy, threshold weight, and threshold density of ${AC}^{0}$, strictly subsuming previous work on these quantities. Our work gives some of the strongest lower bounds to date on the communication complexity of ${AC}^{0}$.},
  archive      = {J_SICOMP},
  author       = {Alexander A. Sherstov and Pei Wu},
  doi          = {10.1137/20M1312459},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-1-86},
  shortjournal = {SIAM J. Comput.},
  title        = {Near-optimal lower bounds on the threshold degree and sign-rank of AC$^0$},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The complexity of necklace splitting, consensus-halving, and
discrete ham sandwich. <em>SICOMP</em>, <em>52</em>(2), STOC19-200-268.
(<a href="https://doi.org/10.1137/20M1312678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We resolve the computational complexity of three problems known as Necklace Splitting, Consensus-Halving, and Discrete Ham sandwich, showing that they are PPA-complete. For NECKLACE SPLITTING, this result is specific to the important special case in which two thieves share the necklace. These are the first PPA-completeness results for problems whose definition does not contain an explicit circuit, thus settling the status of PPA as a class that captures the complexity of such “natural&#39; problems.},
  archive      = {J_SICOMP},
  author       = {Aris Filos-Ratsikas and Paul W. Goldberg},
  doi          = {10.1137/20M1312678},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-200-268},
  shortjournal = {SIAM J. Comput.},
  title        = {The complexity of necklace splitting, consensus-halving, and discrete ham sandwich},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed exact weighted all-pairs shortest paths in
randomized near-linear time. <em>SICOMP</em>, <em>52</em>(2),
STOC19-112-127. (<a href="https://doi.org/10.1137/20M1312782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the distributed all-pairs shortest paths problem, every node in the weighted undirected distributed network (the CONGEST model) needs to know the distance from every other node using least number of communication rounds (typically called time complexity). The problem admits a $(1+o(1))$-approximation $\tilde\Theta(n)$-time algorithm and a nearly tight $\tilde \Omega(n)$ lower bound [D. Nanongkai, STOC&#39;14, ACM, New York, 2014, pp. 565--573; C. Lenzen and B. Patt-Shamir, PODC&#39;15, ACM, New York, 2015, pp. 153--162]. ($\tilde \Theta$, $\tilde O$ and $\tilde \Omega$ hide polylogarithmic factors. Note that the lower bounds also hold even in the unweighted case and in the weighted case with polynomial approximation ratios (C. Lenzen and D. Peleg, PODC, ACM, New York, 2013, pp. 375--382; S. Holzer and R. Wattenofer, PODC, ACM, New York, 2012, pp. 355--364; D. Peleg, L. Roditty, and E. Tal, ICALP, Springer, Berlin, 2012, pp. 660--672; D. Nanongkai, STOC, ACM, New York, 2014, pp. 565--573--672). For the exact case, Elkin [STOC&#39;17, ACM, New York, 2017, pp. 757--790] presented an $O(n^{5/3} \log^{2/3} n)$ time bound, which was later improved to $\tilde O(n^{5/4})$ [C.-C. Huang, D. Nanongkai, T. Saranurak, FOCS&#39;17, IEEE Computer Society, Los Alamitos, CA, 2017, pp. 168--179]. It was shown that any superlinear lower bound (in $n$) requires a new technique [K. Censor-Hillel, S. Khoury, A. Paz, DISC&#39;17, LIPIcs Leibniz Int. Proc. Inform., Vol. 91, Schloss-Dagstuhl, Wadern, Germany, 2017, 10], but otherwise it remained widely open whether there exists a $\tilde O(n)$-time algorithm for the exact case, which would match the best possible approximation algorithm. This paper resolves this question positively: we present a randomized (Las Vegas) $\tilde O(n)$-time algorithm, matching the lower bound up to polylogarithmic factors. Like the previous $\tilde O(n^{5/4})$ bound, our result works for directed graphs with zero (and even negative) edge weights. In addition to the improved running time, our algorithm works in a more general setting than that required by the previous $\tilde O(n^{5/4})$ bound; in our setting (i) the communication is only along edge directions (as opposed to bidirectional), and (ii) edge weights are arbitrary (as opposed to integers in ${1, 2, \ldots, \operatorname{poly}(n)}$). As far as we know, ours is the first $o(n^2)$ algorithm that only requires unidirectional communication. For arbitrary weights, the previous state-of-the-art required $\tilde O(n^{4/3})$ time [U. Agarwal and V. Ramachandran, IPDPS 2019, IEEE Computer Society, Los Alamitos, CA, 2019, and SPAA 2020, ACM, New York, 2020, pp. 11--21]. Our algorithm is extremely simple and relies on a new technique called random filtered broadcast. Given any sets of nodes $A,B\subseteq V$ and assuming that every $b \in B$ knows all distances from nodes in $A$, and every node $v \in V$ knows all distances from nodes in $B$, we want every $v\in V$ to know ${\sf DistThrough}_B(a,v) = \min_{b\in B} {dist}(a,b) + {\sf dist}(b,v)$ for every $a\in A$. Previous works typically solve this problem by broadcasting all knowledge of every $b\in B$, causing superlinear edge congestion and time. We show a randomized algorithm that can reduce edge congestions and thus solve this problem in $\tilde O(n)$ expected time.},
  archive      = {J_SICOMP},
  author       = {Aaron Bernstein and Danupon Nanongkai},
  doi          = {10.1137/20M1312782},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-112-127},
  shortjournal = {SIAM J. Comput.},
  title        = {Distributed exact weighted all-pairs shortest paths in randomized near-linear time},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Random walks and forbidden minors II: A <span
class="math inline">poly(<em>d</em><em>ε</em><sup>−1</sup>)</span>-query
tester for minor-closed properties of bounded-degree graphs.
<em>SICOMP</em>, <em>52</em>(2), STOC19-323-338. (<a
href="https://doi.org/10.1137/20M1312824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $G$ be a graph with $n$ vertices and maximum degree $d$. Fix some minor-closed property $\mathcal{P}$ (such as planarity). We say that $G$ is $\varepsilon$-far from $\mathcal{P}$ if one has to remove $\varepsilon dn$ edges to make it have $\mathcal{P}$. The problem of property testing $\mathcal{P}$ was introduced in the seminal work of Benjamini, Schramm, and Shapira (Symposium on the Theory of Computing 2008) that gave a tester with query complexity triply exponential in $\varepsilon^{-1}$. Levi and Ron [ACM Trans. Algorithms, 11 (2005), 24 2015] have given the best tester to date, with a quasi-polynomial (in $\varepsilon^{-1}$) query complexity. It remained an open problem to show whether there is a property tester whose query complexity is $\mathrm{poly}(d\varepsilon^{-1})$, even for planarity. In this paper, we resolve this open question. For any minor-closed property, we give a tester with query complexity $d\cdot \mathrm{poly}(\varepsilon^{-1})$. The previous line of work on (independent of $n$, two-sided) testers is primarily combinatorial. Our work, on the other hand, employs techniques from spectral graph theory. This paper is a continuation of recent work of the authors (Foundations of Computer Science 2018) analyzing random walk algorithms that find forbidden minors.},
  archive      = {J_SICOMP},
  author       = {Akash Kumar and C. Seshadhri and Andrew M. Stolman},
  doi          = {10.1137/20M1312824},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-323-338},
  shortjournal = {SIAM J. Comput.},
  title        = {Random walks and forbidden minors II: A $\mathrm{poly}(d\varepsilon^{-1})$-query tester for minor-closed properties of bounded-degree graphs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span
class="math inline"><em>O</em>(log<sup>2</sup><em>k</em>/log log <em>k</em>)</span>-approximation
algorithm for directed steiner tree: A tight quasi-polynomial time
algorithm. <em>SICOMP</em>, <em>52</em>(2), STOC19-298-322. (<a
href="https://doi.org/10.1137/20M1312988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the directed Steiner tree (DST) problem, we are given an $n$-vertex directed edge-weighted graph, a root $r$, and a collection of $k$ terminal nodes. Our goal is to find a minimum-cost subgraph that contains a directed path from $r$ to every terminal. We present an $O(\log^2 k/\log\log{k})$-approximation algorithm for DST that runs in quasi-polynomial time, i.e., in time $n^{{poly}\log (k)}$. By assuming the projection game conjecture and ${NP}\not\subseteq{\bigcap}_{0&lt;\epsilon&lt;1}{ZPTIME}(2^{n^\epsilon})$ and adjusting the parameters in the hardness result of [Halperin and Krauthgamer, Polylogarithmic inapproximability, in Proceedings of the 35th Annual ACM Symposium on Theory of Computing, 2003, pp. 585--594], we show the matching lower bound of $\Omega(\log^2{k}/\log\log{k})$ for the class of quasi-polynomial time algorithms, meaning that our approximation ratio is asymptotically the best possible. Our algorithm is proceeded by reducing DST to an intermediate problem, namely, the group Steiner tree on trees with dependency constraint problem, which we approximate using the framework developed by [Rothvoß, Directed Steiner Tree and the Lasserre Hierarchy, preprint, arxiv:1111.5473, 2011] and [Friggstad et al., Linear programming hierarchies suffice for directed Steiner tree, in Proceedings of the 17th Annual Conference on Integer Programming and Combinatorial Optimization, 2014, pp. 285--296].},
  archive      = {J_SICOMP},
  author       = {Fabrizio Grandoni and Bundit Laekhanukit and Shi Li},
  doi          = {10.1137/20M1312988},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-298-322},
  shortjournal = {SIAM J. Comput.},
  title        = {$O(\log^2{k}/\log\log{k})$-approximation algorithm for directed steiner tree: A tight quasi-polynomial time algorithm},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weak zero-knowledge beyond the black-box barrier.
<em>SICOMP</em>, <em>52</em>(2), STOC19-156-199. (<a
href="https://doi.org/10.1137/20M1319565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The round complexity of zero-knowledge protocols is a long-standing open question and is yet to be settled under standard assumptions. So far, the question has appeared equally challenging for relaxations such as weak zero-knowledge and witness hiding. Like full-fledged zero- knowledge, protocols satisfying these relaxed notions under standard assumptions have at least four messages. The difficulty in improving round complexity stems from a fundamental barrier: none of these notions can be achieved in three messages via reductions (or simulators) that treat the verifier as a black box. We introduce a new non-black-box technique and use it to obtain the first protocols that cross this barrier under standard assumptions. Our main results are (1) weak zero-knowledge for NP in two messages, assuming quasi-polynomially secure fully homomorphic encryption and other standard primitives (known from quasi-polynomial hardness of learning with errors) as well as subexponentially secure one-way functions; and (2) weak zero-knowledge for NP in three messages under standard polynomial assumptions (following, for example, from fully homomorphic encryption and factoring). We also give, under polynomial assumptions, a two-message witness-hiding protocol for any language L in NP that has a witness encryption scheme. This protocol is also publicly verifiable. Our technique is based on a new homomorphic trapdoor paradigm, which can be seen as a non-black-box analogue of the classic Feige--Lapidot--Shamir trapdoor paradigm.},
  archive      = {J_SICOMP},
  author       = {Nir Bitansky and Dakshita Khurana and Omer Paneth},
  doi          = {10.1137/20M1319565},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-156-199},
  shortjournal = {SIAM J. Comput.},
  title        = {Weak zero-knowledge beyond the black-box barrier},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lower bounds for external memory integer sorting via network
coding. <em>SICOMP</em>, <em>52</em>(2), STOC19-87-111. (<a
href="https://doi.org/10.1137/20M1321887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting extremely large datasets is a frequently occurring task in practice. These datasets are usually much larger than the computer&#39;s main memory; thus external memory sorting algorithms, first introduced by Aggarwal and Vitter [Commun. ACM, 31 (1988), pp. 1116--1127], are often used. The complexity of comparison based external memory sorting has been understood for decades by now, but the situation remains elusive if we assume the keys to be sorted are integers. In internal memory, one can sort a set of $n$ integer keys of $\Theta({lg} n)$ bits each in $O(n)$ time using the classic radix sort algorithm, but in external memory, there are no faster integer sorting algorithms known than the simple comparison based ones. Whether such algorithms exist has remained a central open problem in external memory algorithms for more than three decades. In this paper, we present a tight conditional lower bound on the complexity of external memory sorting of integers. Our lower bound is based on a famous conjecture in network coding by Li and Li [Proceedings of the 42nd Allerton Annual Conference on Communication, Control and Computing, 2004], who conjectured that network coding cannot help anything beyond the standard multicommodity flow rate in undirected graphs. The only previous work connecting the Li and Li conjecture to lower bounds for algorithms is due to Adler et al. [Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithm, 2006, pp. 241--250]. Adler et al. indeed obtain relatively simple lower bounds for oblivious algorithms (the memory access pattern is fixed and independent of the input data). Unfortunately obliviousness is a strong limitation, especially for integer sorting: we show that the Li and Li conjecture implies an $\Omega(n \log n)$ lower bound for internal memory oblivious sorting when the keys are $\Theta({lg} n)$ bits. This is in sharp contrast to the classic (nonoblivious) radix sort algorithm. Indeed going beyond obliviousness is highly nontrivial; we need to introduce several new methods and involved techniques, which are of their own interest, to obtain our tight lower bound for external memory integer sorting.},
  archive      = {J_SICOMP},
  author       = {Alireza Farhadi and MohammadTaghi Hajiaghayi and Kasper Green Larsen and Elaine Shi},
  doi          = {10.1137/20M1321887},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-87-111},
  shortjournal = {SIAM J. Comput.},
  title        = {Lower bounds for external memory integer sorting via network coding},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance of johnson–lindenstrauss transform for <span
class="math inline"><em>k</em></span>-means and <span
class="math inline"><em>k</em></span>-medians clustering.
<em>SICOMP</em>, <em>52</em>(2), STOC19-269-297. (<a
href="https://doi.org/10.1137/20M1330701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider an instance of Euclidean $k$-means or $k$-medians clustering. We show that the cost of the optimal solution is preserved up to a factor of $(1+\varepsilon)$ under a projection onto a random $O(\log(k / \varepsilon) / \varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is preserved within $(1+\varepsilon)$. More generally, our result applies to any dimension reduction map satisfying a mild sub-Gaussian-tail condition. Our bound on the dimension is nearly optimal. Additionally, our result applies to Euclidean $k$-clustering with the distances raised to the $p$th power for any constant $p$. For $k$-means, our result resolves an open problem posed by Cohen et al. [STOC 2015, ACM, New York, 2015, pp. 163--172] for $k$-medians, it answers a question raised by Kannan.},
  archive      = {J_SICOMP},
  author       = {Konstantin Makarychev and Yury Makarychev and Ilya Razenshteyn},
  doi          = {10.1137/20M1330701},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-269-297},
  shortjournal = {SIAM J. Comput.},
  title        = {Performance of johnson--lindenstrauss transform for $k$-means and $k$-medians clustering},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special section on the fifty-first annual ACM symposium on
the theory of computing (STOC 2019). <em>SICOMP</em>, <em>52</em>(2),
STOC19-i-ii. (<a href="https://doi.org/10.1137/23N975661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SICOMP},
  author       = {Dana Moshkovitz and Sushant Sachdeva},
  doi          = {10.1137/23N975661},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {STOC19-i-ii},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the fifty-first annual ACM symposium on the theory of computing (STOC 2019)},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximately counting independent sets of a given size in
bounded-degree graphs. <em>SICOMP</em>, <em>52</em>(2), 618–640. (<a
href="https://doi.org/10.1137/21M1466220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We determine the computational complexity of approximately counting and sampling independent sets of a given size in bounded-degree graphs. That is, we identify a critical density and provide (i) for randomized polynomial-time algorithms for approximately sampling and counting independent sets of given size at most in -vertex graphs of maximum degree , and (ii) a proof that unless NP = RP, no such algorithms exist for . The critical density is the occupancy fraction of the hard-core model on the complete graph at the uniqueness threshold on the infinite -regular tree, giving as . Our methods apply more generally to antiferromagnetic 2-spin systems and motivate new questions in extremal combinatorics.},
  archive      = {J_SICOMP},
  author       = {Ewan Davies and Will Perkins},
  doi          = {10.1137/21M1466220},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {618-640},
  shortjournal = {SIAM J. Comput.},
  title        = {Approximately counting independent sets of a given size in bounded-degree graphs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the complexity of isomorphism problems for tensors,
groups, and polynomials i: Tensor isomorphism-completeness.
<em>SICOMP</em>, <em>52</em>(2), 568–617. (<a
href="https://doi.org/10.1137/21M1441110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the complexity of isomorphism problems for tensors, groups, and polynomials. These problems have been studied in multivariate cryptography, machine learning, quantum information, and computational group theory. We show that these problems are all polynomial-time equivalent, creating bridges between problems traditionally studied in myriad research areas. This prompts us to define the complexity class , namely problems that reduce to the tensor isomorphism problem in polynomial time. Our main technical result is a polynomial-time reduction from -tensor isomorphism to 3-tensor isomorphism. In the context of quantum information, this result gives a multipartite-to-tripartite entanglement transformation procedure that preserves equivalence under stochastic local operations and classical communication.},
  archive      = {J_SICOMP},
  author       = {Joshua Grochow and Youming Qiao},
  doi          = {10.1137/21M1441110},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {568-617},
  shortjournal = {SIAM J. Comput.},
  title        = {On the complexity of isomorphism problems for tensors, groups, and polynomials i: Tensor isomorphism-completeness},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal separation of randomized and quantum query
complexity. <em>SICOMP</em>, <em>52</em>(2), 525–567. (<a
href="https://doi.org/10.1137/22M1468943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove that for every decision tree, the absolute values of the Fourier coefficients of a given order sum to at most , where is the number of variables, is the tree depth, and is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal [Towards optimal separations between quantum and randomized query complexities, in Proceedings of the Sixty-First Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2020, pp. 228–239]. The bounds prior to our work degraded rapidly with , becoming trivial already at . As an application, we obtain, for every integer , a partial Boolean function on bits that has bounded-error quantum query complexity at most and randomized query complexity . This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis [SIAM J. Comput., 47 (2018), pp. 982–1038] and Bravyi et al. [Classical Algorithms for Forrelation, arXiv preprint, 2021]. Prior to our work, the best known separation was polynomially weaker: versus for any [A. Tal, Towards optimal separations between quantum and randomized query complexities, in Proceedings of the Sixty-First Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2020, pp. 228–239]. As another application, we obtain an essentially optimal separation of versus for bounded-error quantum versus randomized communication complexity for any . The best previous separation was polynomially weaker: versus (this is implicit in [A. Tal, Towards optimal separations between quantum and randomized query complexities, in Proceedings of the Sixty-First Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2020, pp. 228–239]).},
  archive      = {J_SICOMP},
  author       = {Alexander A. Sherstov and Andrey A. Storozhenko and Pei Wu},
  doi          = {10.1137/22M1468943},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {525-567},
  shortjournal = {SIAM J. Comput.},
  title        = {An optimal separation of randomized and quantum query complexity},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Greedy algorithm almost dominates in smoothed contextual
bandits. <em>SICOMP</em>, <em>52</em>(2), 487–524. (<a
href="https://doi.org/10.1137/19M1247115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future. While necessary in the worst case, explicit exploration has a number of disadvantages compared to the greedy algorithm that always “exploits” by choosing an action that currently looks optimal. We determine under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm in the linear contextual bandits model. We improve on prior results to show that the greedy algorithm almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold. The key technical finding is that data collected by the greedy algorithm suffices to simulate a run of any other algorithm. Further, we prove that under a particular smoothness assumption, the Bayesian regret of the greedy algorithm is at most in the worst case, where is the time horizon.},
  archive      = {J_SICOMP},
  author       = {Manish Raghavan and Aleksandrs Slivkins and Jennifer Wortman Vaughan and Zhiwei Steven Wu},
  doi          = {10.1137/19M1247115},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {487-524},
  shortjournal = {SIAM J. Comput.},
  title        = {Greedy algorithm almost dominates in smoothed contextual bandits},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tracing isomanifolds in <span class="math inline">ℝ</span> d
in time polynomial in d using coxeter–freudenthal–kuhn triangulations.
<em>SICOMP</em>, <em>52</em>(2), 452–486. (<a
href="https://doi.org/10.1137/21M1412918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Isomanifolds are the generalization of isosurfaces to arbitrary dimension and codimension, i.e., submanifolds of defined as the zero set of some multivariate multivalued smooth function , where is the intrinsic dimension of the manifold. A natural way to approximate a smooth isomanifold is to consider its piecewise linear (PL) approximation based on a triangulation of the ambient space . In this paper, we describe a simple algorithm to trace isomanifolds from a given starting point. The algorithm works for arbitrary dimensions and , and any precision . Our main result is that, when (or ) has bounded complexity, the complexity of the algorithm is polynomial in and (and unavoidably exponential in ). Since it is known that for , is -close and isotopic to , our algorithm produces a faithful PL-approximation of isomanifolds of bounded complexity in time polynomial in . Combining this algorithm with dimensionality reduction techniques, the dependency on in the size of can be completely removed with high probability. We also show that the algorithm can handle isomanifolds with boundary and, more generally, isostratifolds. The algorithm for isomanifolds with boundary has been implemented and experimental results are reported, showing that it is practical and can handle cases that are far ahead of the state-of-the-art.},
  archive      = {J_SICOMP},
  author       = {Jean-Daniel Boissonnat and Siargey Kachanovich and Mathijs Wintraecken},
  doi          = {10.1137/21M1412918},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {452-486},
  shortjournal = {SIAM J. Comput.},
  title        = {Tracing isomanifolds in \(\mathbb{R}\) d in time polynomial in d using Coxeter–Freudenthal–Kuhn triangulations},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus-halving: Does it ever get easier? <em>SICOMP</em>,
<em>52</em>(2), 412–451. (<a
href="https://doi.org/10.1137/20M1387493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the -Consensus-Halving problem, a fundamental problem in fair division, there are agents with valuations over the interval [0,1], and the goal is to divide the interval into pieces and assign a label “ ” or “ ” to each piece, such that every agent values the total amount of “ ” and the total amount of “ ” almost equally. The problem was recently proven by Filos-Ratsikas and Goldberg [Proceedings of the 50th Annual ACM Symposium on Theory of Computing, 2018, pp. 51–64; Proceedings of the 51st Annual ACM Symposium on Theory of Computing, 2019, pp. 638–649] to be the first “natural” complete problem for the computational class PPA, answering a decade-old open question. In this paper, we examine the extent to which the problem becomes easy to solve if one restricts the class of valuation functions. To this end, we provide the following contributions. First, we obtain a strengthening of the PPA-hardness result of Filos-Ratsikas and Goldberg [Proceedings of the 51st Annual ACM Symposium on Theory of Computing, 2019, pp. 638–649] to the case when agents have piecewise uniform valuations with only two blocks. We obtain this result via a new reduction, which is in fact conceptually much simpler than the corresponding one in Filos-Ratsikas and Goldberg [Proceedings of the 51st Annual ACM Symposium on Theory of Computing, 2019, pp. 638–649]. Then, we consider the case of single-block (uniform) valuations and provide a parameterized polynomial-time algorithm for solving -Consensus-Halving for any , as well as a polynomial-time algorithm for . Finally, an important application of our new techniques is the first hardness result for a generalization of Consensus-Halving, the Consensus- -Division problem [F. W. Simmons and F. E. Su, Math. Social Sci., 45 (2003), pp. 15–25]. In particular, we prove that -Consensus- -Division is PPAD-hard.},
  archive      = {J_SICOMP},
  author       = {Aris Filos-Ratsikas and Alexandros Hollender and Katerina Sotiraki and Manolis Zampetakis},
  doi          = {10.1137/20M1387493},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {412-451},
  shortjournal = {SIAM J. Comput.},
  title        = {Consensus-halving: Does it ever get easier?},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Want to gather? No need to chatter! <em>SICOMP</em>,
<em>52</em>(2), 358–411. (<a
href="https://doi.org/10.1137/20M1362899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A team of mobile agents with different labels, starting from different nodes of an unknown anonymous network, must meet at the same node and declare that they all met. This task of gathering was traditionally considered assuming that agents at the same node can exchange information. We ask if this ability of talking is needed. The answer turns out to be no. We design two deterministic algorithms that accomplish gathering in a much weaker model. We only assume that each agent knows how many agents are at the node that it currently occupies. Our first algorithm assumes that agents know some upper bound N on the size of the network, and works in time polynomial in N and in the length of the smallest label. Our second algorithm does not assume any knowledge about the network, but its complexity is at least exponential in the size of the network and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product we solve, in the same weak model, the fundamental problem of leader election among agents. As an application we solve the gossiping problem in this model: if each agent has a message, all agents can learn all messages. This is perhaps our most surprising finding: agents without any transmitting devices can solve the most general information exchange problem if they can count the number of agents present at visited nodes.},
  archive      = {J_SICOMP},
  author       = {Sébastien Bouchard and Yoann Dieudonné and Andrzej Pelc},
  doi          = {10.1137/20M1362899},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {358-411},
  shortjournal = {SIAM J. Comput.},
  title        = {Want to gather? no need to chatter!},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On min sum vertex cover and generalized min sum set cover.
<em>SICOMP</em>, <em>52</em>(2), 327–357. (<a
href="https://doi.org/10.1137/21M1434052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the Generalized Min Sum Set Cover (GMSSC) problem, wherein given a collection of hyperedges with arbitrary covering requirements , the goal is to find an ordering of the vertices to minimize the total cover time of the hyperedges; a hyperedge is considered covered by the first time when and many of its vertices appear in the ordering. We give a approximation algorithm for GMSSC, coming close to the best possible bound of 4, already for the classical special case (with all ) of Min Sum Set Cover (MSSC) studied by Feige, Lovász, and Tetali, and improving upon the previous best known bound of due to Im, Sviridenko, and van der Zwaan. Our algorithm is based on transforming the LP solution by a suitable kernel and applying randomized rounding. As part of the analysis of our algorithm, we also derive an inequality on the lower tail of a sum of independent Bernoulli random variables, which might be of independent interest and broader utility. Min Sum Vertex Cover (MSVC) is a well-known special case of MSSC in which the input hypergraph is a graph (i.e., ) and for every edge . We give a approximation for MSVC and show a matching integrality gap for the natural LP relaxation. This improves upon the previous best approximation of Barenholz, Feige, and Peleg. Finally, we revisit MSSC and consider the norm of cover-time of the hyperedges. Using a dual fitting argument, we show that the natural greedy algorithm achieves tight, up to NP-hardness, approximation guarantees of for all , giving another proof of the result of Golovin, Gupta, Kumar, and Tangwongsan, and showing its tightness up to NP-hardness. For , this gives yet another proof of the 4 approximation for MSSC.},
  archive      = {J_SICOMP},
  author       = {Nikhil Bansal and Jatin Batra and Majid Farhadi and Prasad Tetali},
  doi          = {10.1137/21M1434052},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {327-357},
  shortjournal = {SIAM J. Comput.},
  title        = {On min sum vertex cover and generalized min sum set cover},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree-depth and the formula complexity of subgraph
isomorphism. <em>SICOMP</em>, <em>52</em>(1), 273–325. (<a
href="https://doi.org/10.1137/20M1372925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For a fixed “pattern” graph , the colored -subgraph isomorphism problem (denoted by ) asks, given an -vertex graph and a coloring , whether contains a properly colored copy of . The complexity of this problem is tied to parameterized versions of and , among other questions. An overarching goal is to understand the complexity of , under different computational models, in terms of natural invariants of the pattern graph . In this paper, we establish a close relationship between the formula complexity of and an invariant known as tree-depth (denoted by). is known to be solvable by monotone formulas of size . Our main result is an lower bound for formulas that are monotone or have sublogarithmic depth. This complements a lower bound of Li, Razborov, and Rossman [SIAM J. Comput., 46 (2017), pp. 936–971] relating tree-width and circuit size. As a corollary, it implies a stronger homomorphism preservation theorem for first-order logic on finite structures [B. Rossman, An improved homomorphism preservation theorem from lower bounds in circuit complexity, in 8th Innovations in Theoretical Computer Science Conference, LIPIcs. Leibniz Int. Proc. Inform. 67, Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, Germany, 2017, 27]. The technical core of this result is an lower bound in the special case where is a complete binary tree of height , which we establish using the pathset framework introduced in B. Rossman [SIAM J. Comput., 47 (2018), pp. 1986–2028]. (The lower bound for general patterns follows via a recent excluded-minor characterization of tree-depth [W. Czerwiński, W. Nadara, and M. Pilipczuk, SIAM J. Discrete Math., 35 (2021), pp. 934–947; K. Kawarabayashi and B. Rossman, A polynomial excluded-minor approximation of treedepth, in Proceedings of the 2018 Annual ACM-SIAM Symposium on Discrete Algorithms, 2018, pp. 234–246]. Additional results of this paper extend the pathset framework and improve upon both the best known upper and lower bounds on the average-case formula size of when is a path.},
  archive      = {J_SICOMP},
  author       = {Deepanshu Kush and Benjamin Rossman},
  doi          = {10.1137/20M1372925},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {273-325},
  shortjournal = {SIAM J. Comput.},
  title        = {Tree-depth and the formula complexity of subgraph isomorphism},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Isomorphism testing for graphs excluding small minors.
<em>SICOMP</em>, <em>52</em>(1), 238–272. (<a
href="https://doi.org/10.1137/21M1401930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove that there is a graph isomorphism test running in time on -vertex graphs excluding some -vertex graph as a minor. Previously known bounds were [I. N. Ponomarenko, J. Soviet Math., 55 (1991), pp. 1621–1643] and [L. Babai, Proceedings of the 48th Annual ACM Symposium on Theory of Computing, 2016, pp. 684–697]. For the algorithm we combine recent advances in the group-theoretic graph isomorphism machinery with new graph-theoretic arguments.},
  archive      = {J_SICOMP},
  author       = {Martin Grohe and Daniel Neuen and Daniel Wiebking},
  doi          = {10.1137/21M1401930},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {238-272},
  shortjournal = {SIAM J. Comput.},
  title        = {Isomorphism testing for graphs excluding small minors},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rapid mixing of glauber dynamics up to uniqueness via
contraction. <em>SICOMP</em>, <em>52</em>(1), 196–237. (<a
href="https://doi.org/10.1137/20M136685X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For general antiferromagnetic 2-spin systems, including the hardcore model on weighted independent sets and the antiferromagnetic Ising model, there is an for the partition function on graphs of maximum degree when the infinite regular tree lies in the uniqueness region by Li, Lu, and Yin [Correlation Decay up to Uniqueness in Spin Systems, preprint, https://arxiv.org/abs/1111.7064, 2021]. Moreover, in the tree nonuniqueness region, Sly in [Computational transition at the uniqueness threshold, in Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, 2010, pp. 287–296] showed that there is no to estimate the partition function unless . The algorithmic results follow from the correlation decay approach due to Weitz [Counting independent sets up to the tree threshold, in Proceedings of the 38th Annual ACM Symposium on Theory of Computing, 2006, pp. 140–149] or the polynomial interpolation approach developed by Barvinok [Combinatorics and Complexity of Partition Functions, Springer, 2016]. However, the running time is only polynomial for constant . For the hardcore model, recent work of Anari, Liu, and Oveis Gharan [Spectral independence in high-dimensional expanders and applications to the hardcore model, in Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, 2020, pp. 1319–1330] establishes rapid mixing of the simple single-site Markov chain, known as the Glauber dynamics, in the tree uniqueness region. Our work simplifies their analysis of the Glauber dynamics by considering the total pairwise influence of a fixed vertex on other vertices, as opposed to the total influence of other vertices on , thereby extending their work to all 2-spin models and improving the mixing time. More important, our proof ties together the three disparate algorithmic approaches: we show that contraction of the so-called tree recursions with a suitable potential function, which is the primary technique for establishing efficiency of Weitz’s correlation decay approach and Barvinok’s polynomial interpolation approach, also establishes rapid mixing of the Glauber dynamics. We emphasize that this connection holds for all 2-spin models (both antiferromagnetic and ferromagnetic), and existing proofs for the correlation decay and polynomial interpolation approaches immediately imply rapid mixing of the Glauber dynamics. Our proof utilizes the fact that the graph partition function is a divisor of the partition function for Weitz’s self-avoiding walk tree. This fact leads to new tools for the analysis of the influence of vertices and may be of independent interest for the study of complex zeros.},
  archive      = {J_SICOMP},
  author       = {Zongchen Chen and Kuikui Liu and Eric Vigoda},
  doi          = {10.1137/20M136685X},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {196-237},
  shortjournal = {SIAM J. Comput.},
  title        = {Rapid mixing of glauber dynamics up to uniqueness via contraction},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimum cuts in surface graphs. <em>SICOMP</em>,
<em>52</em>(1), 156–195. (<a
href="https://doi.org/10.1137/19M1291820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We describe algorithms to efficiently compute minimum -cuts and global minimum cuts of undirected surface-embedded graphs. Given an edge-weighted undirected graph with vertices embedded on an orientable surface of genus , our algorithms can solve either problem in or time, whichever is better. When is a constant, our time algorithms match the best running times known for computing minimum cuts in planar graphs. Our algorithms for minimum cuts rely on reductions to the problem of finding a minimum-weight subgraph in a given -homology class, and we give efficient algorithms for this latter problem as well. If is embedded on a surface with genus and boundary components, these algorithms run in and time. We also prove that finding a minimum-weight subgraph homologous to a single input cycle is NP-hard, showing that it is likely impossible to improve upon the exponential dependencies on for this latter problem.},
  archive      = {J_SICOMP},
  author       = {Erin W. Chambers and Jeff Erickson and Kyle Fox and Amir Nayyeri},
  doi          = {10.1137/19M1291820},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {156-195},
  shortjournal = {SIAM J. Comput.},
  title        = {Minimum cuts in surface graphs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inapproximability of matrix <span
class="math inline"><strong>p</strong> <strong>→</strong> <strong>q</strong></span>
norms. <em>SICOMP</em>, <em>52</em>(1), 132–155. (<a
href="https://doi.org/10.1137/18M1233418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of computing the norm of a matrix , defined as . This problem generalizes the spectral norm of a matrix and the Grothendieck problem ( , ) and has been widely studied in various regimes. When , the problem exhibits a dichotomy: constant factor approximation algorithms are known if , and the problem is hard to approximate within almost polynomial factors when . The regime when , known as hypercontractive norms, is particularly significant for various applications but much less well understood. The case with and was studied by Barak et al. [Proceedings of the 44th Annual ACM Symposium on Theory of Computing, 2012, pp. 307–326], who gave subexponential algorithms for a promise version of the problem (which captures small-set expansion) and also proved hardness of approximation results based on the exponential time hypothesis. However, no NP-hardness of approximation is known for these problems for any . We prove the first NP-hardness result (under randomized reductions) for approximating hypercontractive norms. We show that for any with , is hard to approximate within assuming . En route to the above result, we also prove almost tight results for the case when with .},
  archive      = {J_SICOMP},
  author       = {Vijay Bhattiprolu and Mrinal Kanti Ghosh and Venkatesan Guruswami and Euiwoong Lee and Madhur Tulsiani},
  doi          = {10.1137/18M1233418},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {132-155},
  shortjournal = {SIAM J. Comput.},
  title        = {Inapproximability of matrix \(\boldsymbol{p \rightarrow q}\) norms},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the complexity of equilibrium computation in first-price
auctions. <em>SICOMP</em>, <em>52</em>(1), 80–131. (<a
href="https://doi.org/10.1137/21M1435823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of computing a (pure) Bayes–Nash equilibrium in the first-price auction with continuous value distributions and discrete bidding space. We prove that when bidders have independent subjective prior beliefs about the value distributions of the other bidders, computing an -equilibrium of the auction is PPAD-complete, and computing an exact equilibrium is FIXP-complete. We also provide an efficient algorithm for solving a special case of the problem for a fixed number of bidders and available bids.},
  archive      = {J_SICOMP},
  author       = {Aris Filos-Ratsikas and Yiannis Giannakopoulos and Alexandros Hollender and Philip Lazos and Diogo Poças},
  doi          = {10.1137/21M1435823},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {80-131},
  shortjournal = {SIAM J. Comput.},
  title        = {On the complexity of equilibrium computation in first-price auctions},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology and adjunction in promise constraint satisfaction.
<em>SICOMP</em>, <em>52</em>(1), 38–79. (<a
href="https://doi.org/10.1137/20M1378223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The approximate graph coloring problem, whose complexity is unresolved in most cases, concerns finding a -coloring of a graph that is promised to be -colorable, where . This problem naturally generalizes to promise graph homomorphism problems and further to promise constraint satisfaction problems. The complexity of these problems has recently been studied through an algebraic approach. In this paper, we introduce two new techniques to analyze the complexity of promise CSPs: one is based on topology and the other on adjunction. We apply these techniques, together with the previously introduced algebraic approach, to obtain new unconditional NP-hardness results for a significant class of approximate graph coloring and promise graph homomorphism problems.},
  archive      = {J_SICOMP},
  author       = {Andrei Krokhin and Jakub Opršal and Marcin Wrochna and Stanislav Živný},
  doi          = {10.1137/20M1378223},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {38-79},
  shortjournal = {SIAM J. Comput.},
  title        = {Topology and adjunction in promise constraint satisfaction},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLAP: A new algorithm for promise CSPs. <em>SICOMP</em>,
<em>52</em>(1), 1–37. (<a
href="https://doi.org/10.1137/22M1476435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a new algorithm for Promise Constraint Satisfaction Problems (PCSPs). It is a combination of the Constraint Basic LP relaxation and the Affine IP relaxation (CLAP). We give a characterization of the power of CLAP in terms of a minion homomorphism. Using this characterization, we identify a certain weak notion of symmetry which, if satisfied by infinitely many polymorphisms of PCSPs, guarantees tractability. We demonstrate that there are PCSPs solved by CLAP that are not solved by any of the existing algorithms for PCSPs; in particular, not by the algorithm of Brakensiek et al. [SIAM J. Comput., 49 (2020), pp. 1232--1248] and not by a reduction to tractable finite-domain CSPs.},
  archive      = {J_SICOMP},
  author       = {Lorenzo Ciardo and Stanislav Živný},
  doi          = {10.1137/22M1476435},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {1-37},
  shortjournal = {SIAM J. Comput.},
  title        = {CLAP: A new algorithm for promise CSPs},
  volume       = {52},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
