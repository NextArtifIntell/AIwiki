<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---46">JUQ - 46</h2>
<ul>
<li><details>
<summary>
(2023). Fully bayesian inference for latent variable gaussian
process models. <em>JUQ</em>, <em>11</em>(4), 1357–1381. (<a
href="https://doi.org/10.1137/22M1525600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Real engineering and scientific applications often involve one or more qualitative inputs. Standard Gaussian processes (GPs), however, cannot directly accommodate qualitative inputs. The recently introduced latent variable Gaussian process (LVGP) overcomes this issue by first mapping each qualitative factor to underlying latent variables (LVs) and then uses any standard GP covariance function over these LVs. The LVs are estimated similarly to the other GP hyperparameters through maximum likelihood estimation and then plugged into the prediction expressions. However, this plug-in approach will not account for uncertainty in estimation of the LVs, which can be significant especially with limited training data. In this work, we develop a fully Bayesian approach for the LVGP model and for visualizing the effects of the qualitative inputs via their LVs. We also develop approximations for scaling up LVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct numerical studies comparing plug-in inference against fully Bayesian inference over a few engineering models and material design applications. In contrast to previous studies on standard GP modeling that have largely concluded that a fully Bayesian treatment offers limited improvements, our results show that for LVGP modeling it offers significant improvements in prediction accuracy and uncertainty quantification over the plug-in approach.},
  archive      = {J_JUQ},
  author       = {Suraj Yerramilli and Akshay Iyer and Wei Chen and Daniel W. Apley},
  doi          = {10.1137/22M1525600},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1357-1381},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Fully bayesian inference for latent variable gaussian process models},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Space-time multilevel quadrature methods and their
application for cardiac electrophysiology. <em>JUQ</em>, <em>11</em>(4),
1329–1356. (<a href="https://doi.org/10.1137/21M1418320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a novel approach which aims at high-performance uncertainty quantification for cardiac electrophysiology simulations. Employing the monodomain equation to model the transmembrane potential inside the cardiac cells, we evaluate the effect of spatially correlated perturbations of the heart fibers on the statistics of the resulting quantities of interest. Our methodology relies on a close integration of multilevel quadrature methods, parallel iterative solvers, and space-time finite element discretizations, allowing for a fully parallelized framework in space, time, and stochastics. Extensive numerical studies are presented to evaluate convergence rates and to compare the performance of classical Monte Carlo methods such as standard Monte Carlo (MC) and quasi-Monte Carlo (QMC), as well as multilevel strategies, i.e., multilevel Monte Carlo (MLMC) and multilevel quasi-Monte Carlo (MLQMC) on hierarchies of nested meshes. We especially also employ a recently suggested variant of the multilevel approach for nonnested meshes to deal with a realistic heart geometry.},
  archive      = {J_JUQ},
  author       = {Seif Ben Bader and Helmut Harbrecht and Rolf Krause and Michael D. Multerer and Alessio Quaglino and Marc Schmidlin},
  doi          = {10.1137/21M1418320},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1329-1356},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Space-time multilevel quadrature methods and their application for cardiac electrophysiology},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter selection in gaussian process interpolation: An
empirical study of selection criteria. <em>JUQ</em>, <em>11</em>(4),
1308–1328. (<a href="https://doi.org/10.1137/21M1444710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article revisits the fundamental problem of parameter selection for Gaussian process interpolation. By choosing the mean and the covariance functions of a Gaussian process within parametric families, the user obtains a family of Bayesian procedures to perform predictions about the unknown function and must choose a member of the family that will hopefully provide good predictive performances. We base our study on the general concept of scoring rules, which provides an effective framework for building leave-one-out selection and validation criteria and a notion of extended likelihood criteria based on an idea proposed by Fasshauer et al. [“Optimal” scaling and stable computation of meshfree kernel methods, 2009], which makes it possible to recover standard selection criteria, such as the generalized cross-validation criterion. Under this setting, we empirically show on several test problems of the literature that the choice of an appropriate family of models is often more important than the choice of a particular selection criterion (e.g., the likelihood versus a leave-one-out selection criterion). Moreover, our numerical results show that the regularity parameter of a Matérn covariance can be selected effectively by most selection criteria.},
  archive      = {J_JUQ},
  author       = {Sébastien J. Petit and Julien Bect and Paul Feliot and Emmanuel Vazquez},
  doi          = {10.1137/21M1444710},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1308-1328},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Parameter selection in gaussian process interpolation: An empirical study of selection criteria},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theoretical guarantees for the statistical finite element
method. <em>JUQ</em>, <em>11</em>(4), 1278–1307. (<a
href="https://doi.org/10.1137/21M1463963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The statistical finite element method (StatFEM) is an emerging probabilistic method that allows observations of a physical system to be synthesized with the numerical solution of a PDE intended to describe it in a coherent statistical framework, to compensate for model error. This work presents a new theoretical analysis of the StatFEM demonstrating that it has similar convergence properties to the finite element method on which it is based. Our results constitute a bound on the 2-Wasserstein distance between the ideal prior and posterior and the StatFEM approximation thereof, and show that this distance converges at the same mesh-dependent rate as finite element solutions converge to the true solution. Several numerical examples are presented to demonstrate our theory, including an example which tests the robustness of StatFEM when extended to nonlinear quantities of interest.},
  archive      = {J_JUQ},
  author       = {Yanni Papandreou and Jon Cockayne and Mark Girolami and Andrew Duncan},
  doi          = {10.1137/21M1463963},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1278-1307},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Theoretical guarantees for the statistical finite element method},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantification of errors generated by uncertain data in a
linear boundary value problem using neural networks. <em>JUQ</em>,
<em>11</em>(4), 1258–1277. (<a
href="https://doi.org/10.1137/22M1538855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Quantifying errors caused by indeterminacy in data is currently computationally expensive even in relatively simple PDE problems. Efficient methods could prove very useful in, for example, scientific experiments done with simulations. In this paper, we create and test neural networks which quantify uncertainty errors in the case of a linear one-dimensional boundary value problem. Training and testing data is generated numerically. We created three training datasets and three testing datasets and trained four neural networks with differing architectures. The performance of the neural networks is compared to known analytical bounds of errors caused by uncertain data. We find that the trained neural networks accurately approximate the exact error quantity in almost all cases and the neural network outputs are always between the analytical upper and lower bounds. The results of this paper show that after a suitable dataset is used for training even a relatively compact neural network can successfully predict quantitative effects generated by uncertain data. If these methods can be extended to more difficult PDE problems they could potentially have a multitude of real-world applications.},
  archive      = {J_JUQ},
  author       = {Vilho Halonen and Ilkka Pölönen},
  doi          = {10.1137/22M1538855},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1258-1277},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantification of errors generated by uncertain data in a linear boundary value problem using neural networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotic bounds for smoothness parameter estimates in
gaussian process interpolation. <em>JUQ</em>, <em>11</em>(4), 1225–1257.
(<a href="https://doi.org/10.1137/22M149288X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Matérn covariance kernel. The smoothness parameter of a Matérn kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of . That is, if the data-generating response function has Sobolev smoothness , then the smoothness parameter estimate cannot be asymptotically less than . The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound , which, however, is unlikely to be sharp. The results are based on approximation theory in Sobolev spaces and some general theorems that restrict the set of values that the parameter estimators can take.},
  archive      = {J_JUQ},
  author       = {Toni Karvonen},
  doi          = {10.1137/22M149288X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1225-1257},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Asymptotic bounds for smoothness parameter estimates in gaussian process interpolation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An order-theoretic perspective on modes and maximum a
posteriori estimation in bayesian inverse problems. <em>JUQ</em>,
<em>11</em>(4), 1195–1224. (<a
href="https://doi.org/10.1137/22M154243X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is often desirable to summarize a probability measure on a space in terms of a mode, or MAP estimator, i.e., a point of maximum probability. Such points can be rigorously defined using masses of metric balls in the small-radius limit. However, the theory is not entirely straightforward: the literature contains multiple notions of mode and various examples of pathological measures that have no mode in any sense. Since the masses of balls induce natural orderings on the points of , this article aims to shed light on some of the problems in nonparametric MAP estimation by taking an order-theoretic perspective, which appears to be a new one in the inverse problems community. This point of view opens up attractive proof strategies based upon the Cantor and Kuratowski intersection theorems; it also reveals that many of the pathologies arise from the distinction between greatest and maximal elements of an order, and from the existence of incomparable elements of , which we show can be dense in , even for an absolutely continuous measure on .},
  archive      = {J_JUQ},
  author       = {Hefin Lambley and T. J. Sullivan},
  doi          = {10.1137/22M154243X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1195-1224},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An order-theoretic perspective on modes and maximum a posteriori estimation in bayesian inverse problems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity analysis of quasi-stationary distributions
(QSDs) of mass-action systems. <em>JUQ</em>, <em>11</em>(4), 1164–1194.
(<a href="https://doi.org/10.1137/22M1535875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper studies the sensitivity analysis of mass-action systems against their diffusion approximations, particularly the dependence on population sizes. As a continuous-time Markov chain, a mass-action system can be described by an equation driven by finitely many Poisson processes, which has a diffusion approximation that can be pathwisely matched. The magnitude of noise in mass-action systems is proportional to the square root of the molecule count/population, which makes a large class of mass-action systems have quasi-stationary distributions (QSDs) besides invariant probability measures. In this paper, we modify the coupling-based technique developed in [M. Dobson, Y. Li, and J. Zhai, SIAM/ASA J. Uncertain. Quantif., 9 (2021), pp. 135–162] to estimate an upper bound of the 1-Wasserstein distance between two QSDs. Some numerical results of sensitivity with different population sizes are provided.},
  archive      = {J_JUQ},
  author       = {Yao Li and Yaping Yuan},
  doi          = {10.1137/22M1535875},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1164-1194},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sensitivity analysis of quasi-stationary distributions (QSDs) of mass-action systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliable error estimates for optimal control of linear
elliptic PDEs with random inputs. <em>JUQ</em>, <em>11</em>(4),
1139–1163. (<a href="https://doi.org/10.1137/22M1503889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We discretize a risk-neutral optimal control problem governed by a linear elliptic partial differential equation with random inputs using a Monte Carlo sample-based approximation and a finite element discretization, yielding finite dimensional control problems. We establish an exponential tail bound for the distance between the finite dimensional problems’ solutions and the risk-neutral problem’s solution. The tail bound implies that solutions to the risk-neutral optimal control problem can be reliably estimated with the solutions to the finite dimensional control problems. Numerical simulations illustrate our theoretical findings.},
  archive      = {J_JUQ},
  author       = {Johannes Milz},
  doi          = {10.1137/22M1503889},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1139-1163},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Reliable error estimates for optimal control of linear elliptic PDEs with random inputs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Are minimizers of the onsager–machlup functional strong
posterior modes? <em>JUQ</em>, <em>11</em>(4), 1105–1138. (<a
href="https://doi.org/10.1137/23M1546579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work we connect two notions: that of the nonparametric mode of a probability measure, defined by asymptotic small ball probabilities, and that of the Onsager–Machlup functional, a generalized density also defined via asymptotic small ball probabilities. We show that in a separable Hilbert space setting and under mild conditions on the likelihood, modes of a Bayesian posterior distribution based upon a Gaussian prior exist and agree with the minimizers of its Onsager–Machlup functional and thus also with weak posterior modes. We apply this result to inverse problems and derive conditions on the forward mapping under which this variational characterization of posterior modes holds. Our results show rigorously that in the limit case of infinite-dimensional data corrupted by additive Gaussian or Laplacian noise, nonparametric maximum a posteriori estimation is equivalent to Tikhonov–Phillips regularization. In comparison with the work of Dashti et al. [Inverse Problems, 29 (2013), 095017], the assumptions on the likelihood are relaxed so that they cover in particular the important case of white Gaussian process noise. We illustrate our results by applying them to a severely ill-posed linear problem with Laplacian noise, where we express the maximum a posteriori estimator analytically and study its rate of convergence in the small noise limit.},
  archive      = {J_JUQ},
  author       = {Remo Kretschmann},
  doi          = {10.1137/23M1546579},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1105-1138},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Are minimizers of the Onsager–Machlup functional strong posterior modes?},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast calibration for computer models with massive physical
observations. <em>JUQ</em>, <em>11</em>(3), 1069–1104. (<a
href="https://doi.org/10.1137/22M153673X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer model calibration is a crucial step in building a reliable computer model. In the face of massive physical observations, a fast estimation of the calibration parameters is urgently needed. To alleviate the computational burden, we design a two-step algorithm to estimate the calibration parameters by employing the subsampling techniques. Compared with the current state-of-the-art calibration methods, the complexity of the proposed algorithm is greatly reduced without sacrificing too much accuracy. We prove the consistency and asymptotic normality of the proposed estimator. The form of the variance of the proposed estimation is also presented, which provides a natural way to quantify the uncertainty of the calibration parameters. The obtained results of two numerical simulations and two real-case studies demonstrate the advantages of the proposed method.},
  archive      = {J_JUQ},
  author       = {Shurui Lv and Jun Yu and Yan Wang and Jiang Du},
  doi          = {10.1137/22M153673X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1069-1104},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Fast calibration for computer models with massive physical observations},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimension free nonasymptotic bounds on the accuracy of
high-dimensional laplace approximation. <em>JUQ</em>, <em>11</em>(3),
1044–1068. (<a href="https://doi.org/10.1137/22M1495688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper aims at revisiting the classical results on Laplace approximation in a modern nonasymptotic and dimension-free form. Such an extension is motivated by applications to high-dimensional statistical and optimization problems. The established results provide explicit nonasymptotic bounds on the quality of a Gaussian approximation of the posterior distribution in total variation distance in terms of the so-called effective dimension . This value is defined as the interplay between the information contained in the data and in the prior distribution. In contrast to prominent Bernstein–von Mises results, the impact of the prior is not negligible, and it allows one to keep the effective dimension small or moderate even if the true parameter dimension is huge. We also address the issue of using a Gaussian approximation with inexact parameters, with the focus on replacing the maximum a posteriori (MAP) value by the posterior mean and designing the algorithm of Bayesian optimization based on Laplace iterations. The results are specified to the case of nonlinear regression.},
  archive      = {J_JUQ},
  author       = {Vladimir Spokoiny},
  doi          = {10.1137/22M1495688},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1044-1068},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Dimension free nonasymptotic bounds on the accuracy of high-dimensional laplace approximation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inference with projected densities. <em>JUQ</em>,
<em>11</em>(3), 1025–1043. (<a
href="https://doi.org/10.1137/22M150695X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Constraints are a natural choice for prior information in Bayesian inference. In various applications, the parameters of interest lie on the boundary of the constraint set. In this paper, we use a method that implicitly defines a constrained prior such that the posterior assigns positive probability to the boundary of the constraint set. We show that by projecting posterior mass onto a polyhedral constraint set, we obtain a new posterior with a rich probabilistic structure on the boundary of that set. If the original posterior is a Gaussian, then such a projection can be done efficiently. We apply the method to Bayesian linear inverse problems, in which case samples can be obtained by repeatedly solving constrained least squares problems, similar to an MAP estimate but with perturbations in the data. When combined into a Bayesian hierarchical model and the constraint set is a polyhedral cone, we can derive a Gibbs sampler to efficiently sample from the hierarchical model. To show the effect of projecting the posterior, we applied the method to deblurring and CT examples.},
  archive      = {J_JUQ},
  author       = {Jasper M. Everink and Yiqiu Dong and Martin S. Andersen},
  doi          = {10.1137/22M150695X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1025-1043},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian inference with projected densities},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards practical large-scale randomized iterative least
squares solvers through uncertainty quantification. <em>JUQ</em>,
<em>11</em>(3), 996–1024. (<a
href="https://doi.org/10.1137/22M1515057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. As the scale of problems and data used for experimental design, signal processing, and data assimilation grow, the oft-occurring least squares subproblems are correspondingly growing in size. As the scale of these least squares problems creates prohibitive memory movement costs for the usual incremental QR and Krylov-based algorithms, randomized least squares problems are garnering more attention. However, these randomized least squares solvers are difficult to integrate into application algorithms as their uncertainty limits practical tracking of algorithmic progress and reliable stopping. Accordingly, in this work, we develop theoretically rigorous, practical tools for quantifying the uncertainty of an important class of iterative randomized least squares algorithms, which we then use to track algorithmic progress and create a stopping condition. We demonstrate the effectiveness of our algorithm by solving a 0.78 TB least squares subproblem from the inner loop of incremental 4D-Var using only 195 MB of memory.},
  archive      = {J_JUQ},
  author       = {Nathaniel Pritchard and Vivak Patel},
  doi          = {10.1137/22M1515057},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {996-1024},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Towards practical large-scale randomized iterative least squares solvers through uncertainty quantification},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep surrogate accelerated delayed-acceptance hamiltonian
monte carlo for bayesian inference of spatio-temporal heat fluxes in
rotating disc systems. <em>JUQ</em>, <em>11</em>(3), 970–995. (<a
href="https://doi.org/10.1137/22M1513113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a deep learning accelerated methodology to solve PDE-based Bayesian inverse problems with guaranteed accuracy. This is motivated by solving the ill-posed problem of inferring a spatio-temporal heat-flux parameter known as the Biot number in a PDE model given temperature data; however, the methodology is generalizable to other settings. To achieve accelerated Bayesian inference we develop a novel training scheme that uses data to adaptively train a neural-network surrogate simulating the parametric forward model. By simultaneously identifying an approximate posterior distribution over the Biot number and weighting a physics-informed training loss according to this, our approach approximates a forward and inverse solution together without any need for external solves. Using a random Chebyshev series, we outline how to approximate a Gaussian process prior, and using the surrogate we apply Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution. We derive convergence of the surrogate posterior to the true posterior distribution in the Hellinger metric as our adaptive loss approaches zero. Additionally, we describe how this surrogate-accelerated HMC approach can be combined with traditional PDE solvers in a delayed-acceptance scheme to a priori control the posterior accuracy. This overcomes a major limitation of deep learning-based surrogate approaches, which do not achieve guaranteed accuracy a priori due to their nonconvex training. Biot number calculations are involved in turbo-machinery design, which is safety critical and highly regulated, and therefore it is important that our results have such mathematical guarantees. Our approach achieves fast mixing in high-dimensional parameter spaces, while retaining the convergence guarantees of a traditional PDE solver, and without the burden of evaluating this solver for proposals that are likely to be rejected. A range of numerical results is given using real and simulated data that compare adaptive and general training regimes and various gradient-based Markov chain Monte Carlo sampling methods.},
  archive      = {J_JUQ},
  author       = {Teo Deveney and Eike H. Mueller and Tony Shardlow},
  doi          = {10.1137/22M1513113},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {970-995},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Deep surrogate accelerated delayed-acceptance hamiltonian monte carlo for bayesian inference of spatio-temporal heat fluxes in rotating disc systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A simple, bias-free approximation of covariance functions by
the multilevel monte carlo method having nearly optimal complexity.
<em>JUQ</em>, <em>11</em>(3), 941–969. (<a
href="https://doi.org/10.1137/22M1506845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop simple and bias-free Monte Carlo and multilevel Monte Carlo approximations to covariance functions of sufficiently regular random fields in tensor products of Hilbert spaces. We investigate approximating the covariance function by means of full tensor product approximations, and additionally derive sparse tensor product approximation variants to overcome the curse of dimensionality and yield essentially optimal complexity, i.e., up to logarithmic factors. A priori convergence and work estimates for the different variants are given and subsequently compared theoretically and numerically, where experiments for an exemplary elliptic diffusion problem validate the theoretical findings.},
  archive      = {J_JUQ},
  author       = {Alexey Chernov and Erik Marc Schetzke},
  doi          = {10.1137/22M1506845},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {941-969},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A simple, bias-free approximation of covariance functions by the multilevel monte carlo method having nearly optimal complexity},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating forecasts for high-impact events using
transformed kernel scores. <em>JUQ</em>, <em>11</em>(3), 906–940. (<a
href="https://doi.org/10.1137/22M1532184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. To account for uncertainties, forecasts for future events are commonly expressed in terms of probability distributions over the set of possible outcomes. To evaluate the quality of such forecasts, it is customary to employ proper scoring rules, which provide an objective framework with which to compare forecasters. When evaluating forecasts, it may be useful to emphasize outcomes that have a high impact on the forecast user. Weighted scoring rules have become a well-established tool to achieve this. However, while impacts may result from the interaction of events along multiple dimensions, weighted scores have been studied almost exclusively in the univariate case. This paper introduces weighted multivariate scoring rules, which can emphasize multivariate outcomes of interest when evaluating forecast performance. We demonstrate that the threshold-weighted continuous ranked probability score (twCRPS), arguably the most well-known weighted scoring rule, belongs to the class of kernel scores, and we use this to construct univariate and multivariate generalizations of the twCRPS. These generalizations include weighted versions of popular multivariate scoring rules, such as the energy score and variogram score. This result facilitates the introduction of weighted scoring rules that are appropriate in many practical applications, which also enjoy the theoretical framework associated with kernel scores. To illustrate the additional information that these novel weighted scoring rules provide, results are presented for a case study in which they are used to evaluate daily precipitation accumulation forecasts, with particular emphasis on events that could lead to flooding.},
  archive      = {J_JUQ},
  author       = {Sam Allen and David Ginsbourger and Johanna Ziegel},
  doi          = {10.1137/22M1532184},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {906-940},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Evaluating forecasts for high-impact events using transformed kernel scores},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust level-set-based topology optimization under
uncertainties using anchored ANOVA petrov–galerkin method. <em>JUQ</em>,
<em>11</em>(3), 877–905. (<a
href="https://doi.org/10.1137/22M1524722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a nonintrusive approach to robust structural topology optimization. Specifically, we consider optimization of mean- and variance-based robustness metrics of a linear functional output associated with the linear elasticity equation in the presence of probabilistic uncertainties in the loading and material properties. To provide an efficient approximation of higher-dimensional problems, we approximate the solution to the governing stochastic partial differential equations using the anchored ANOVA Petrov–Galerkin projection scheme. We then develop a nonintrusive quadrature-based formulation to evaluate the robustness metric and the associated shape derivative. The formulation is nonintrusive in the sense that it works with any level-set-based topology optimization code that can provide deterministic displacements, outputs, and shape derivatives for selected stochastic parameter values. We demonstrate the effectiveness of the proposed approach on various problems under loading and material uncertainties.},
  archive      = {J_JUQ},
  author       = {Christophe Audouze and Aaron Klein and Adrian Butscher and Nigel Morris and Prasanth Nair and Masayuki Yano},
  doi          = {10.1137/22M1524722},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {877-905},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Robust level-set-based topology optimization under uncertainties using anchored ANOVA Petrov–Galerkin method},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active learning of tree tensor networks using optimal least
squares. <em>JUQ</em>, <em>11</em>(3), 848–876. (<a
href="https://doi.org/10.1137/21M1415911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we propose new learning algorithms for approximating high-dimensional functions using tree tensor networks in a least-squares setting. Given a dimension tree or architecture of the tensor network, we provide an algorithm that generates a sequence of nested tensor subspaces based on a generalization of principal component analysis for multivariate functions. An optimal least-squares method is used for computing projections onto the generated tensor subspaces, using samples generated from a distribution depending on the previously generated subspaces. We provide an error bound in expectation for the obtained approximation. Practical strategies are proposed for adapting the feature spaces and ranks to achieve a prescribed error. Also, we propose an algorithm that progressively constructs the dimension tree by suitable pairings of variables, that allows further reduction of the number of samples necessary to reach that error. Numerical examples illustrate the performance of the proposed algorithms and show that stable approximations are obtained with a number of samples close to the number of free parameters of the estimated tensor networks.},
  archive      = {J_JUQ},
  author       = {Cécile Haberstich and A. Nouy and G. Perrin},
  doi          = {10.1137/21M1415911},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {848-876},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Active learning of tree tensor networks using optimal least squares},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying and managing uncertainty in
piecewise-deterministic markov processes. <em>JUQ</em>, <em>11</em>(3),
814–847. (<a href="https://doi.org/10.1137/20M1357275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In piecewise-deterministic Markov processes (PDMPs) the state of a finite-dimensional system evolves continuously, but the evolutive equation may change randomly as a result of discrete switches. A running cost is integrated along the corresponding piecewise-deterministic trajectory up to the termination to produce the cumulative cost of the process. We address three natural questions related to uncertainty in cumulative cost of PDMP models: (1) how to compute the cumulative distribution function (CDF) of the cumulative cost when the switching rates are fully known; (2) how to accurately bound the CDF when the switching rates are uncertain; and (3) assuming the PDMP is controlled, how to select a control to optimize that CDF. In all three cases, our approach requires posing a system of suitable hyperbolic partial differential equations, which are then solved numerically on an augmented state space. We illustrate our method using simple examples of trajectory planning under uncertainty for several one-dimensional and two-dimensional first-exit time problems. In the appendix, we also apply this method to a model of fish harvesting in an environment with random switches in carrying capacity.},
  archive      = {J_JUQ},
  author       = {Elliot Cartee and Antonio Farah and April Nellis and Jacob Van Hook and Alexander Vladimirsky},
  doi          = {10.1137/20M1357275},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {814-847},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantifying and managing uncertainty in piecewise-deterministic markov processes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large deviation theory-based adaptive importance sampling
for rare events in high dimensions. <em>JUQ</em>, <em>11</em>(3),
788–813. (<a href="https://doi.org/10.1137/22M1524758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a method for the accurate estimation of rare event or failure probabilities for expensive-to-evaluate numerical models in high dimensions. The proposed approach combines ideas from large deviation theory and adaptive importance sampling. The importance sampler uses a cross-entropy method to find an optimal Gaussian biasing distribution, and reuses all samples made throughout the process for both the target probability estimation and for updating the biasing distributions. Large deviation theory is used to find a good initial biasing distribution through the solution of an optimization problem. Additionally, it is used to identify a low-dimensional subspace that is most informative of the rare event probability. This subspace is used for the cross-entropy method, which is known to lose efficiency in higher dimensions. The proposed method does not require smoothing of indicator functions nor does it involve numerical tuning parameters. We compare the method with a state-of-the-art cross-entropy-based importance sampling scheme using three examples: a high-dimensional failure probability estimation benchmark, a problem governed by a diffusion equation, and a tsunami problem governed by the time-dependent shallow water system in one spatial dimension.},
  archive      = {J_JUQ},
  author       = {Shanyin Tong and Georg Stadler},
  doi          = {10.1137/22M1524758},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {788-813},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Large deviation theory-based adaptive importance sampling for rare events in high dimensions},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble-based gradient inference for particle methods in
optimization and sampling. <em>JUQ</em>, <em>11</em>(3), 757–787. (<a
href="https://doi.org/10.1137/22M1533281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose an approach based on function evaluations and Bayesian inference to extract higher-order differential information of objective functions from a given ensemble of particles. Pointwise evaluation of some potential V in an ensemble contains implicit information about first- or higher-order derivatives, which can be made explicit with little computational effort (ensemble-based gradient inference). We suggest using this information for the improvement of established ensemble-based numerical methods for optimization and sampling such as consensus-based optimization and Langevin-based samplers. Numerical studies indicate that the augmented algorithms are often superior to their gradient-free variants; in particular, the augmented methods help the ensembles to escape their initial domain, to explore multimodal, non-Gaussian settings, and to speed up the collapse at the end of optimization dynamics. The code for the numerical examples in this manuscript can be found in the paper’s Github repository.},
  archive      = {J_JUQ},
  author       = {Claudia Schillings and Claudia Totzeck and Philipp Wacker},
  doi          = {10.1137/22M1533281},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {757-787},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Ensemble-based gradient inference for particle methods in optimization and sampling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reduced-order modeling with time-dependent bases for PDEs
with stochastic boundary conditions. <em>JUQ</em>, <em>11</em>(3),
727–756. (<a href="https://doi.org/10.1137/21M1468097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Low-rank approximation using time-dependent bases (TDBs) has proven effective for reduced-order modeling of stochastic partial differential equations (SPDEs). In these techniques, the random field is decomposed to a set of deterministic TDBs and time-dependent stochastic coefficients. When applied to SPDEs with nonhomogeneous stochastic boundary conditions (BCs), appropriate BC must be specified for each of the TDBs. However, determining BCs for TDB is not trivial because (i) the dimension of the random BCs is different than the rank of the TDB subspace and (ii) TDB in most formulations must preserve orthonormality or orthogonality constraints, and specifying BCs for TDB should not violate these constraints in the space-discretized form. In this work, we present a methodology for determining the boundary conditions for TDBs at no additional computational cost beyond that of solving the same SPDE with homogeneous BCs. Our methodology is informed by the fact the TDB evolution equations are the optimality conditions of a variational principle. We leverage the same variational principle to derive an evolution equation for the value of TDB at the boundaries. The presented methodology preserves the orthonormality or orthogonality constraints of TDBs. We present the formulation for the dynamically biorthonormal decomposition [P. Patil and H. Babaee, J. Comput. Phys., (2020), 109511] as well as the dynamically orthogonal decomposition [T. P. Sapsis and P. F. Lermusiaux, Phys. D, 238 (2009), pp. 2347–2360]. We show that the presented methodology can be applied to stochastic Dirichlet, Neumann, and Robin boundary conditions. We assess the performance of the presented method for linear advection-diffusion equation, Burgers’ equation, and 2D advection-diffusion equation with constant and temperature-dependent conduction coefficient.},
  archive      = {J_JUQ},
  author       = {Prerna Patil and Hessam Babaee},
  doi          = {10.1137/21M1468097},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {727-756},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Reduced-order modeling with time-dependent bases for PDEs with stochastic boundary conditions},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable physics-based maximum likelihood estimation using
hierarchical matrices. <em>JUQ</em>, <em>11</em>(2), 682–725. (<a
href="https://doi.org/10.1137/21M1458880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Physics-based covariance models provide a systematic way to construct covariance models that are consistent with the underlying physical laws in Gaussian process analysis. The unknown parameters in the covariance models can be estimated using maximum likelihood estimation, but direct construction of the covariance matrix and classical strategies of computing with it require physical model runs, storage complexity, and computational complexity. To address such challenges, we propose to approximate the discretized covariance function using hierarchical matrices. By utilizing randomized range sketching for individual off-diagonal blocks, the construction process of the hierarchical covariance approximation requires physical model applications and the maximum likelihood computations require effort per iteration. We propose a new approach to compute exactly the trace of products of hierarchical matrices which results in the expected Fisher information matrix being computable in as well. The construction is totally matrix-free and the derivatives of the covariance matrix can then be approximated in the same hierarchical structure by differentiating the whole process. Numerical results are provided to demonstrate the effectiveness, accuracy, and efficiency of the proposed method for parameter estimations and uncertainty quantification.},
  archive      = {J_JUQ},
  author       = {Yian Chen and Mihai Anitescu},
  doi          = {10.1137/21M1458880},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {682-725},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scalable physics-based maximum likelihood estimation using hierarchical matrices},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A continuation method in bayesian inference. <em>JUQ</em>,
<em>11</em>(2), 646–681. (<a
href="https://doi.org/10.1137/19M130251X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a continuation method that entails generating a sequence of transition probability density functions from the prior to the posterior in the context of Bayesian inference for parameter estimation problems. The characterization of transition distributions, by tempering the likelihood function, results in a homogeneous nonlinear partial integro-differential equation for which existence and uniqueness of solutions are addressed. The posterior probability distribution is obtained as the interpretation of the final state of a path of transition distributions. A computationally stable scaling domain for the likelihood is explored for approximation of the expected deviance, where we restrict the evaluations of the forward predictive model at the prior stage. To obtain a solution formulation for the expected deviance, we derive a partial differential equation governing the moment-generating function of the log-likelihood. We show also that a spectral formulation of the expected deviance can be obtained for low-dimensional problems under certain conditions. The effectiveness of the proposed method is demonstrated using four numerical examples. These focus on analyzing the computational bias generated by the method, assessing its use in Bayesian inference with non-Gaussian noise, evaluating its ability to invert a multimodal parameter of interest, and quantifying its performance in terms of computational cost.},
  archive      = {J_JUQ},
  author       = {Ben Mansour Dia},
  doi          = {10.1137/19M130251X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {646-681},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A continuation method in bayesian inference},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On unbiased estimation for discretized models. <em>JUQ</em>,
<em>11</em>(2), 616–645. (<a
href="https://doi.org/10.1137/21M1460788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this article, we consider computing expectations w.r.t. probability measures which are subject to discretization error. Examples include partially observed diffusion processes or inverse problems, where one may have to discretize time and/or space in order to practically work with the probability of interest. Given access only to these discretizations, we consider the construction of unbiased Monte Carlo estimators of expectations w.r.t. such target probability distributions. It is shown how to obtain such estimators using a novel adaptation of randomization schemes and Markov simulation methods. Under appropriate assumptions, these estimators possess finite variance and finite expected cost. There are two important consequences of this approach: (i) unbiased inference is achieved at the canonical complexity rate, and (ii) the resulting estimators can be generated independently, thereby allowing strong scaling to arbitrarily many parallel processors. Several algorithms are presented and applied to some examples of Bayesian inference problems with both simulated and real observed data.},
  archive      = {J_JUQ},
  author       = {Jeremy Heng and Ajay Jasra and Kody J. H. Law and Alexander Tarakanov},
  doi          = {10.1137/21M1460788},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {616-645},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On unbiased estimation for discretized models},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise level free regularization of general linear inverse
problems under unconstrained white noise. <em>JUQ</em>, <em>11</em>(2),
591–615. (<a href="https://doi.org/10.1137/22M1506675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this note we solve a general statistical inverse problem under absence of knowledge of both the noise level and the noise distribution via application of the (modified) heuristic discrepancy principle. Hereby the unbounded (non-Gaussian) noise is controlled via introducing an auxiliary discretization dimension and choosing it in an adaptive fashion. We first show convergence for completely arbitrary compact forward operator and ground solution. Then the uncertainty of reaching the optimal convergence rate is quantified in a specific Bayesian-like environment. We conclude with numerical experiments.},
  archive      = {J_JUQ},
  author       = {Tim Jahn},
  doi          = {10.1137/22M1506675},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {591-615},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Noise level free regularization of general linear inverse problems under unconstrained white noise},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wavenumber-explicit parametric holomorphy of helmholtz
solutions in the context of uncertainty quantification. <em>JUQ</em>,
<em>11</em>(2), 567–590. (<a
href="https://doi.org/10.1137/22M1486170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A crucial role in the theory of uncertainty quantification (UQ) of PDEs is played by the regularity of the solution with respect to the stochastic parameters; indeed, a key property one seeks to establish is that the solution is holomorphic with respect to (the complex extensions of) the parameters. In the context of UQ for the high-frequency Helmholtz equation, a natural question is therefore: how does this parametric holomorphy depend on the wavenumber ? The recent paper [35] showed for a particular nontrapping variable-coefficient Helmholtz problem with affine dependence of the coefficients on the stochastic parameters that the solution operator can be analytically continued a distance into the complex plane. In this paper, we generalize the result in [35] about -explicit parametric holomorphy to a much wider class of Helmholtz problems with arbitrary (holomorphic) dependence on the stochastic parameters; we show that in all cases the region of parametric holomorphy decreases with and show how the rate of decrease with is dictated by whether the unperturbed Helmholtz problem is trapping or nontrapping. We then give examples of both trapping and nontrapping problems where these bounds on the rate of decrease with of the region of parametric holomorphy are sharp, with the trapping examples coming from the recent results of [31]. An immediate implication of these results is that the -dependent restrictions imposed on the randomness in the analysis of quasi-Monte Carlo methods in [35] arise from a genuine feature of the Helmholtz equation with large (and not, for example, a suboptimal bound).},
  archive      = {J_JUQ},
  author       = {E. A. Spence and J. Wunsch},
  doi          = {10.1137/22M1486170},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {567-590},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Wavenumber-explicit parametric holomorphy of helmholtz solutions in the context of uncertainty quantification},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The zero problem: Gaussian process emulators for
range-constrained computer models. <em>JUQ</em>, <em>11</em>(2),
540–566. (<a href="https://doi.org/10.1137/21M1467420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a zero-censored Gaussian process as a systematic, model-based approach to building Gaussian process emulators for range-constrained simulator output. This approach avoids many pitfalls associated with modeling range-constrained data with Gaussian processes. Further, it is flexible enough to be used in conjunction with statistical emulator advancements such as emulators that model high-dimensional vector-valued simulator output. The zero-censored Gaussian process is then applied to two examples of geophysical flow inundation which have the constraint of nonnegativity.},
  archive      = {J_JUQ},
  author       = {Elaine T. Spiller and Robert L. Wolpert and Pablo Tierz and Taylor G. Asher},
  doi          = {10.1137/21M1467420},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {540-566},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {The zero problem: Gaussian process emulators for range-constrained computer models},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multifidelity surrogate modeling for time-series outputs.
<em>JUQ</em>, <em>11</em>(2), 514–539. (<a
href="https://doi.org/10.1137/20M1386694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper considers the surrogate modeling of a complex numerical code in a multifidelity framework when the code output is a time series and two code levels are available: a high-fidelity and expensive code level and a low-fidelity and cheap code level. The goal is to emulate a fast-running approximation of the high-fidelity code level. An original Gaussian process regression method is proposed that uses an experimental design of the low- and high-fidelity code levels. The code output is expanded on a basis built from the experimental design. The first coefficients of the expansion of the code output are processed by a cokriging approach. The last coefficients are processed by a kriging approach with covariance tensorization. The resulting surrogate model provides a predictive mean and a predictive variance of the output of the high-fidelity code level. It is shown to have better performance in terms of prediction errors than standard dimension reduction techniques.},
  archive      = {J_JUQ},
  author       = {Baptiste Kerleguer},
  doi          = {10.1137/20M1386694},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {514-539},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multifidelity surrogate modeling for time-series outputs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence rates for learning linear operators from noisy
data. <em>JUQ</em>, <em>11</em>(2), 480–513. (<a
href="https://doi.org/10.1137/21M1442942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper studies the learning of linear operators between infinite-dimensional Hilbert spaces. The training data comprises pairs of random input vectors in a Hilbert space and their noisy images under an unknown self-adjoint linear operator. Assuming that the operator is diagonalizable in a known basis, this work solves the equivalent inverse problem of estimating the operator’s eigenvalues given the data. Adopting a Bayesian approach, the theoretical analysis establishes posterior contraction rates in the infinite data limit with Gaussian priors that are not directly linked to the forward map of the inverse problem. The main results also include learning-theoretic generalization error guarantees for a wide range of distribution shifts. These convergence rates quantify the effects of data smoothness and true eigenvalue decay or growth, for compact or unbounded operators, respectively, on sample complexity. Numerical evidence supports the theory in diagonal and nondiagonal settings.},
  archive      = {J_JUQ},
  author       = {Maarten V. de Hoop and Nikola B. Kovachki and Nicholas H. Nelsen and Andrew M. Stuart},
  doi          = {10.1137/21M1442942},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {480-513},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Convergence rates for learning linear operators from noisy data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric posterior learning for emission tomography.
<em>JUQ</em>, <em>11</em>(2), 452–479. (<a
href="https://doi.org/10.1137/21M1463367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We continue studies of the uncertainty quantification problem in emission tomographies such as positron emission tomography (PET) or single photon emission computed tomography (SPECT) when additional multimodal data (anatomical magnetic resonance imaging (MRI) images) are available. To solve the aforementioned problem we adapt the recently proposed nonparametric posterior learning technique to the context of Poisson-type data in emission tomography. Using this approach we derive sampling algorithms which are trivially parallelizable, scalable and very easy to implement. In addition, we prove conditional consistency and tightness for the distribution of produced samples in the small noise limit (i.e., when the acquisition time tends to infinity) and derive new geometrical and necessary condition on how MRI images must be used. This condition arises naturally in the context of identifiability problem for misspecified generalized Poisson models with wrong design. We also contrast our approach with Bayesian Markov chain Monte Carlo sampling based on one data augmentation scheme which is very popular in the context of expectation-maximization algorithms for PET or SPECT. We show theoretically and also numerically that such data augmentation significantly increases mixing times for the Markov chain. In view of this, our algorithms seem to give a reasonable trade-off between design complexity, scalability, numerical load and assessment for the uncertainty.},
  archive      = {J_JUQ},
  author       = {Fedor Goncharov and Éric Barat and Thomas Dautremer},
  doi          = {10.1137/21M1463367},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {452-479},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Nonparametric posterior learning for emission tomography},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian process regression on nested spaces. <em>JUQ</em>,
<em>11</em>(2), 426–451. (<a
href="https://doi.org/10.1137/21M1445053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. As computer codes simulate complex physical phenomena, they involve a very large number of variables. To gain time, industrial experts build metamodels on a restricted set of variables, the most influential ones, while the others are fixed. The set of variables is then enlarged progressively to improve knowledge on the studied output. Several designs of experiment are generated, which belong to subspaces included in each other and of increasing dimension. The goal of this paper is to create a metamodel adapted to this inefficient design process, that exploits the structure of all previous runs. An approach based on Gaussian process regression and called seqGPR (sequential Gaussian process regression) is introduced. At each new step of the study (when new variables are released), the output is supposed to be the realization of the sum of two independent Gaussian processes. The first one models the output at the previous step. The second one is a correction term which must be null on the subspace studied at the previous step, that is to say null on a continuum of points. First, some candidate Gaussian processes for the correction terms are suggested. Then, an EM (expectation-maximization) algorithm is implemented to estimate the parameters of the processes. Finally, the metamodel seqGPR is compared to a standard kriging metamodel on three test cases and gives better results.},
  archive      = {J_JUQ},
  author       = {Christophette Blanchet-Scalliet and Bruno Demory and Thierry Gonon and Céline Helbert},
  doi          = {10.1137/21M1445053},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {426-451},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Gaussian process regression on nested spaces},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust kalman and bayesian set-valued filtering and model
validation for linear stochastic systems. <em>JUQ</em>, <em>11</em>(2),
389–425. (<a href="https://doi.org/10.1137/22M1481270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Consider a linear stochastic filtering problem in which the probability measure specifying all randomness is only partially known. The deviation between the real and assumed probability models is constrained by a divergence bound between the respective probability measures under which the models are defined. This bound defines a so-called uncertainty set. A recursive set-valued filtering characterization is derived and is guaranteed (with probability one) to contain the true conditional posterior of the unknown, real world, filtering problem when the real world measure is within this uncertainty set. Some filtering approximations and related results are given. The set-valued characterization is related to the problem of robust model validation and model goodness-of-fit statistical hypothesis testing. It is shown how relevant terms involving the innovation sequence (re)appear in multiple settings from set-valued filtering to statistical model evaluation.},
  archive      = {J_JUQ},
  author       = {Adrian N. Bishop and Pierre Del Moral},
  doi          = {10.1137/22M1481270},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {389-425},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Robust kalman and bayesian set-valued filtering and model validation for linear stochastic systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Certified dimension reduction for bayesian updating with the
cross-entropy method. <em>JUQ</em>, <em>11</em>(1), 358–388. (<a
href="https://doi.org/10.1137/22M1484031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In inverse problems, the parameters of a model are estimated based on observations of the model response. The Bayesian approach is powerful for solving such problems; one formulates a prior distribution for the parameter state that is updated with the observations to compute the posterior parameter distribution. Solving for the posterior distribution can be challenging when, e.g., prior and posterior significantly differ from one another and/or the parameter space is high-dimensional. We use a sequence of importance sampling measures that arise by tempering the likelihood to approach inverse problems exhibiting a significant distance between prior and posterior. Each importance sampling measure is identified by cross-entropy minimization as proposed in the context of Bayesian inverse problems in Engel et al. [J. Comput. Phys., 473 (2023), 111746]. To efficiently address problems with high-dimensional parameter spaces, we set up the minimization procedure in a low-dimensional subspace of the original parameter space. The principal idea is to analyze the spectrum of the second-moment matrix of the gradient of the log-likelihood function to identify a suitable subspace. Following Zahm et al. [Math. Comp., 91 (2022), pp. 1789–1835], an upper bound on the Kullback–Leibler divergence between full-dimensional and subspace posterior is provided, which can be utilized to determine the effective dimension of the inverse problem corresponding to a prescribed approximation error bound. We suggest heuristic criteria for optimally selecting the number of model and model gradient evaluations in each iteration of the importance sampling sequence. We investigate the performance of this approach using examples from engineering mechanics set in various parameter space dimensions.},
  archive      = {J_JUQ},
  author       = {Max Ehre and Rafael Flock and Martin Fußeder and Iason Papaioannou and Daniel Straub},
  doi          = {10.1137/22M1484031},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {358-388},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Certified dimension reduction for bayesian updating with the cross-entropy method},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complete deterministic dynamics and spectral decomposition
of the linear ensemble kalman inversion. <em>JUQ</em>, <em>11</em>(1),
320–357. (<a href="https://doi.org/10.1137/21M1429461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The ensemble Kalman inversion (EKI) for the solution of Bayesian inverse problems of type , with being an unknown parameter, a given datum, and measurement noise, is a powerful tool usually derived from a sequential Monte Carlo point of view. It describes the dynamics of an ensemble of particles , whose initial empirical measure is sampled from the prior, evolving over an artificial time toward an approximate solution of the inverse problem, with emulating the posterior, and corresponding to the underregularized minimum-norm solution of the inverse problem. Using spectral techniques, we provide a complete description of the deterministic dynamics of EKI and its asymptotic behavior in parameter space. In particular, we analyze the dynamics of naive EKI and mean-field EKI with a special focus on their time asymptotic behavior. Furthermore, we show that—even in the deterministic case—residuals in parameter space do not decrease monotonously in the Euclidean norm and suggest a problem-adapted norm, where monotonicity can be proved. Finally, we derive a system of ordinary differential equations governing the spectrum and eigenvectors of the covariance matrix. While the analysis is aimed at the EKI, we believe that it can be applied to understand more general particle-based dynamical systems.},
  archive      = {J_JUQ},
  author       = {Leon Bungert and Philipp Wacker},
  doi          = {10.1137/21M1429461},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {320-357},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Complete deterministic dynamics and spectral decomposition of the linear ensemble kalman inversion},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware surrogate modeling for balancing approximation
and sampling costs in multifidelity importance sampling and bayesian
inverse problems. <em>JUQ</em>, <em>11</em>(1), 285–319. (<a
href="https://doi.org/10.1137/21M1445594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Multifidelity methods leverage low-cost surrogate models to speed up computations and make occasional recourse to expensive high-fidelity models to establish accuracy guarantees. Because surrogate and high-fidelity models are used together, poor predictions by surrogate models can be compensated with frequent recourse to high-fidelity models. Thus, there is a trade-off between investing computational resources to improve the accuracy of surrogate models versus simply making more frequent recourse to expensive high-fidelity models; however, this trade-off is ignored by traditional modeling methods that construct surrogate models that are meant to replace high-fidelity models rather than being used together with high-fidelity models. This work considers multifidelity importance sampling and theoretically and computationally trades off increasing the fidelity of surrogate models for constructing more accurate biasing densities and the numbers of samples that are required from the high-fidelity models to compensate poor biasing densities. Numerical examples demonstrate that such context-aware surrogate models for multifidelity importance sampling have lower fidelity than what typically is set as tolerance in traditional model reduction, leading to runtime speedups of up to one order of magnitude in the presented examples.},
  archive      = {J_JUQ},
  author       = {Terrence Alsup and Benjamin Peherstorfer},
  doi          = {10.1137/21M1445594},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {285-319},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Context-aware surrogate modeling for balancing approximation and sampling costs in multifidelity importance sampling and bayesian inverse problems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized sparse bayesian learning and application to
image reconstruction. <em>JUQ</em>, <em>11</em>(1), 262–284. (<a
href="https://doi.org/10.1137/22M147236X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Image reconstruction based on indirect, noisy, or incomplete data remains an important yet challenging task. While methods such as compressive sensing have demonstrated high-resolution image recovery in various settings, there remain issues of robustness due to parameter tuning. Moreover, since the recovery is limited to a point estimate, it is impossible to quantify the uncertainty, which is often desirable. Due to these inherent limitations, a sparse Bayesian learning approach is sometimes adopted to recover a posterior distribution of the unknown. Sparse Bayesian learning assumes that some linear transformation of the unknown is sparse. However, most of the methods developed are tailored to specific problems, with particular forward models and priors. Here, we present a generalized approach to sparse Bayesian learning. It has the advantage that it can be used for various types of data acquisitions and prior information. Some preliminary results on image reconstruction/recovery indicate its potential use for denoising, deblurring, and magnetic resonance imaging.},
  archive      = {J_JUQ},
  author       = {Jan Glaubitz and Anne Gelb and Guohui Song},
  doi          = {10.1137/22M147236X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {262-284},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Generalized sparse bayesian learning and application to image reconstruction},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast and scalable computational framework for large-scale
high-dimensional bayesian optimal experimental design. <em>JUQ</em>,
<em>11</em>(1), 235–261. (<a
href="https://doi.org/10.1137/21M1466499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop a fast and scalable computational framework to solve Bayesian optimal experimental design problems governed by partial differential equations (PDEs) with application to optimal sensor placement by maximizing expected information gain (EIG). Such problems are particularly challenging due to the curse of dimensionality for high-dimensional parameters and the expensive solution of large-scale PDEs. To address these challenges, we exploit two fundamental properties: (1) the low-rank structure of the Jacobian of the parameter-to-observable map, to extract the intrinsically low-dimensional data-informed subspace, and (2) a series of approximations of the EIG that reduce the number of PDE solves while retaining high correlation with the true EIG. Based on these properties, we propose an efficient offline-online decomposition for the optimization problem. The offline stage dominates the cost and entails precomputing all components that require PDE solves. The online stage optimizes sensor placement and does not require any PDE solves. For the online stage, we propose a new greedy algorithm that first places an initial set of sensors using leverage scores and then swaps the selected sensors with other candidates until certain convergence criteria are met, which we call a swapping greedy algorithm. We demonstrate the efficiency and scalability of the proposed method by both linear and nonlinear inverse problems. In particular, we show that the number of required PDE solves is small, independent of the parameter dimension, and only weakly dependent on the data dimension for both problems.},
  archive      = {J_JUQ},
  author       = {Keyi Wu and Peng Chen and Omar Ghattas},
  doi          = {10.1137/21M1466499},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {235-261},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A fast and scalable computational framework for large-scale high-dimensional bayesian optimal experimental design},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning in high dimension: Neural network expression
rates for analytic functions in <span
class="math inline"><strong>L</strong><sup><strong>2</strong></sup><strong>(</strong>ℝ<sup><strong>d</strong></sup><strong>,</strong> <strong>γ</strong><sub><strong>d</strong></sub><strong>)</strong></span>.
<em>JUQ</em>, <em>11</em>(1), 199–234. (<a
href="https://doi.org/10.1137/21M1462738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For artificial deep neural networks, we prove expression rates for analytic functions in the norm of where . Here denotes the Gaussian product probability measure on . We consider in particular and activations for integer . For , we show exponential convergence rates in . In case , under suitable smoothness and sparsity assumptions on , with denoting an infinite (Gaussian) product measure on , we prove dimension-independent expression rate bounds in the norm of . The rates only depend on quantified holomorphy of (an analytic continuation of) the map to a product of strips in (in for , respectively). As an application, we prove expression rate bounds of deep -NNs for response surfaces of elliptic PDEs with log-Gaussian random field inputs.},
  archive      = {J_JUQ},
  author       = {Christoph Schwab and Jakob Zech},
  doi          = {10.1137/21M1462738},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {199-234},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Deep learning in high dimension: Neural network expression rates for analytic functions in \(\pmb{L^2(\mathbb{R}^d,\gamma_d)}\)},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty quantification and experimental design for
large-scale linear inverse problems under gaussian process priors.
<em>JUQ</em>, <em>11</em>(1), 168–198. (<a
href="https://doi.org/10.1137/21M1445028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the use of Gaussian process (GP) priors for solving inverse problems in a Bayesian framework. As is well known, the computational complexity of GPs scales cubically in the number of datapoints. Here we show that in the context of inverse problems involving integral operators, one faces additional difficulties that hinder inversion on large grids. Furthermore, in that context, covariance matrices can become too large to be stored. By leveraging recent results about sequential disintegrations of Gaussian measures, we are able to introduce an implicit representation of posterior covariance matrices that reduces the memory footprint by only storing low rank intermediate matrices, while allowing individual elements to be accessed on-the-fly without needing to build full posterior covariance matrices. Moreover, it allows for fast sequential inclusion of new observations. These features are crucial when considering sequential experimental design tasks. We demonstrate our approach by computing sequential data collection plans for excursion set recovery for a gravimetric inverse problem, where the goal is to provide fine resolution estimates of high density regions inside the Stromboli volcano, Italy. Sequential data collection plans are computed by extending the weighted integrated variance reduction (wIVR) criterion to inverse problems. Our results show that this criterion is able to significantly reduce the uncertainty on the excursion volume, reaching close to minimal levels of residual uncertainty. Overall, our techniques allow the advantages of probabilistic models to be brought to bear on large-scale inverse problems arising in the natural sciences. Particularly, applying the latest developments in Bayesian sequential experimental design on realistic large-scale problems opens new venues of research at a crossroads between mathematical modelling of natural phenomena, statistical data science, and active learning.},
  archive      = {J_JUQ},
  author       = {Cédric Travelletti and David Ginsbourger and Niklas Linde},
  doi          = {10.1137/21M1445028},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {168-198},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification and experimental design for large-scale linear inverse problems under gaussian process priors},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the generalized langevin equation for simulated
annealing. <em>JUQ</em>, <em>11</em>(1), 139–167. (<a
href="https://doi.org/10.1137/21M1462970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we consider the generalized (higher order) Langevin equation for the purpose of simulated annealing and optimization of nonconvex functions. Our approach modifies the underdamped Langevin equation by replacing the Brownian noise with an appropriate Ornstein–Uhlenbeck process to account for memory in the system. Under reasonable conditions on the loss function and the annealing schedule, we establish convergence of the continuous time dynamics to a global minimum. In addition, we investigate the performance numerically and show better performance and higher exploration of the state space compared to the underdamped Langevin dynamics with the same annealing schedule.},
  archive      = {J_JUQ},
  author       = {Martin Chak and Nikolas Kantas and Grigorios A. Pavliotis},
  doi          = {10.1137/21M1462970},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {139-167},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the generalized langevin equation for simulated annealing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of a class of multilevel markov chain monte carlo
algorithms based on independent metropolis–hastings. <em>JUQ</em>,
<em>11</em>(1), 91–138. (<a
href="https://doi.org/10.1137/21M1420927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we present, analyze, and implement a class of multilevel Markov chain Monte Carlo (ML-MCMC) algorithms based on independent Metropolis–Hastings proposals for Bayesian inverse problems. In this context, the likelihood function involves solving a complex differential model, which is then approximated on a sequence of increasingly accurate discretizations. The key point of this algorithm is to construct highly coupled Markov chains together with the standard multilevel Monte Carlo argument to obtain a better cost-tolerance complexity than a single-level MCMC algorithm. Our method extends the ideas of Dodwell et al., [SIAM/ASA J. Uncertain. Quantif., 3 (2015), pp. 1075–1108] to a wider range of proposal distributions. We present a thorough convergence analysis of the ML-MCMC method proposed, and show, in particular, that (i) under some mild conditions on the (independent) proposals and the family of posteriors, there exists a unique invariant probability measure for the coupled chains generated by our method, and (ii) that such coupled chains are uniformly ergodic. We also generalize the cost-tolerance theorem of Dodwell et al. to our wider class of ML-MCMC algorithms. Finally, we propose a self-tuning continuation-type ML-MCMC algorithm. The presented method is tested on an array of academic examples, where some of our theoretical results are numerically verified. These numerical experiments evidence how our extended ML-MCMC method is robust when targeting some pathological posteriors, for which some of the previously proposed ML-MCMC algorithms fail.},
  archive      = {J_JUQ},
  author       = {Juan P. Madrigal-Cianci and Fabio Nobile and Raúl Tempone},
  doi          = {10.1137/21M1420927},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {91-138},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Analysis of a class of multilevel markov chain monte carlo algorithms based on independent Metropolis–Hastings},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the deep active-subspace method. <em>JUQ</em>,
<em>11</em>(1), 62–90. (<a
href="https://doi.org/10.1137/21M1463240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The deep active-subspace method is a neural-network based tool for the propagation of uncertainty through computational models with high-dimensional input spaces. Unlike the original active-subspace method, it does not require access to the gradient of the model. It relies on an orthogonal projection matrix constructed with Gram–Schmidt orthogonalization to reduce the input dimensionality. This matrix is incorporated into a neural network as the weight matrix of the first hidden layer (acting as an orthogonal encoder), and optimized using back propagation to identify the active subspace of the input. We propose several theoretical extensions, starting with a new analytic relation for the derivatives of Gram–Schmidt vectors, which are required for back propagation. We also study the use of vector-valued model outputs, which is difficult in the case of the original active-subspace method. Additionally, we investigate an alternative neural network with an encoder without embedded orthonormality, which shows equally good performance compared to the deep active-subspace method. Two epidemiological models are considered as applications, where one requires supercomputer access to generate the training data.},
  archive      = {J_JUQ},
  author       = {Wouter Edeling},
  doi          = {10.1137/21M1463240},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {62-90},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the deep active-subspace method},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty quantification of inclusion boundaries in the
context of x-ray tomography. <em>JUQ</em>, <em>11</em>(1), 31–61. (<a
href="https://doi.org/10.1137/21M1433782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we describe a Bayesian framework for reconstructing the boundaries of piecewise smooth regions in the X-ray computed tomography (CT) problem in an infinite-dimensional setting. In addition to the reconstruction, we quantify the uncertainty of the predicted boundaries. Our approach is goal-oriented, meaning that we directly detect the discontinuities from the data instead of reconstructing the entire image. This drastically reduces the dimension of the problem, which makes the application of Markov Chain Monte Carlo (MCMC) methods feasible. We show that our method provides an excellent platform for challenging X-ray CT scenarios (e.g., in the case of noisy data, limited angle imaging, or sparse angle imaging). We investigate the performance and accuracy of our method on synthetic data as well as real-world data. The numerical results indicate that our method provides an accurate method for detecting boundaries of piecewise smooth regions and quantifies the uncertainty in the prediction.},
  archive      = {J_JUQ},
  author       = {Babak Maboudi Afkham and Yiqiu Dong and Per Christian Hansen},
  doi          = {10.1137/21M1433782},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {31-61},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification of inclusion boundaries in the context of X-ray tomography},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel delayed acceptance MCMC. <em>JUQ</em>,
<em>11</em>(1), 1–30. (<a
href="https://doi.org/10.1137/22M1476770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop a novel Markov chain Monte Carlo (MCMC) method that exploits a hierarchy of models of increasing complexity to efficiently generate samples from an unnormalized target distribution. Broadly, the method rewrites the multilevel MCMC approach of Dodwell et al. [SIAM/ASA J. Un‐certain. Quantif., 3 (2015), pp. 1075–1108] in terms of the delayed acceptance MCMC of Christen and Fox [J. Comput. Graph. Statist., 14 (2005), pp. 795–810]. In particular, delayed acceptance is extended to use a hierarchy of models of arbitrary depth and allow subchains of arbitrary length. We show that the algorithm satisfies detailed balance and hence is ergodic for the target distribution. Furthermore, multilevel variance reduction is derived that exploits the multiple levels and subchains, and an adaptive multilevel correction to coarse-level biases is developed. Three numerical examples of Bayesian inverse problems are presented that demonstrate the advantages of these novel methods. The software and examples are available in PyMC3.},
  archive      = {J_JUQ},
  author       = {M. B. Lykkegaard and T. J. Dodwell and C. Fox and G. Mingas and R. Scheichl},
  doi          = {10.1137/22M1476770},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {1-30},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel delayed acceptance MCMC},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
