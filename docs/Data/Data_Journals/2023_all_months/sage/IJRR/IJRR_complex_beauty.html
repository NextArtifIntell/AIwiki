<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJRR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijrr---61">IJRR - 61</h2>
<ul>
<li><details>
<summary>
(2023). Impact-aware task-space quadratic-programming control.
<em>The International Journal of Robotics Research</em>,
<em>42</em>(14), 1265–1282. (<a
href="https://doi.org/10.1177/02783649231198558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robots usually establish contacts at rigid surfaces with near-zero relative velocities. Otherwise, impact-induced energy propagates in the robot’s linkage and may cause irreversible damage to the hardware. Moreover, abrupt changes in task-space contact velocity and peak impact forces also result in abrupt changes in robot joint velocities and torques; which can compromise controllers’ stability, especially for those based on smooth models. In reality, several tasks would require establishing contact with moderately high velocity. We propose to enhance task-space multi-objective controllers formulated as a quadratic program to be resilient to frictional impacts in three dimensions. We devise new constraints and reformulate the usual ones to be robust to the abrupt joint state changes mentioned earlier. The impact event becomes a controlled process once the optimal control search space is aware of: (1) the hardware-affordable impact bounds and (2) analytically computed feasible set (polyhedra) that constrain post-impact critical states. Prior to and nearby the targeted contact spot, we assume, at each control cycle, that the impact will occur at the next iteration. This somewhat one-step preview makes our controller robust to impact time and location. To assess our approach, we experimented its resilience to moderate impacts with the Panda manipulator and achieved swift grabbing tasks with the HRP-4 humanoid robot.},
  archive  = {J},
  author   = {Yuquan Wang and Niels Dehio and Arnaud Tanguy and Abderrahmane Kheddar},
  doi      = {10.1177/02783649231198558},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1265-1282},
  title    = {Impact-aware task-space quadratic-programming control},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised bayesian specification inference from
demonstrations. <em>The International Journal of Robotics Research</em>,
<em>42</em>(14), 1245–1264. (<a
href="https://doi.org/10.1177/02783649231204659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of a task’s execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring specifications, with over 90% similarity observed between the inferred specification and the ground truth—both within a synthetic domain and during a real-world table setting task.},
  archive  = {J},
  author   = {Ankit Shah and Pritish Kamath and Shen Li and Patrick Craven and Kevin Landers and Kevin Oden and Julie Shah},
  doi      = {10.1177/02783649231204659},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1245-1264},
  title    = {Supervised bayesian specification inference from demonstrations},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modal-graph 3D shape servoing of deformable objects with raw
point clouds. <em>The International Journal of Robotics Research</em>,
<em>42</em>(14), 1213–1244. (<a
href="https://doi.org/10.1177/02783649231198900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deformable object manipulation (DOM) with point clouds has great potential as nonrigid 3D shapes can be measured without detecting and tracking image features. However, robotic shape control of deformable objects with point clouds is challenging due to: the unknown point correspondences and the noisy partial observability of raw point clouds; the modeling difficulties of the relationship between point clouds and robot motions. To tackle these challenges, this paper introduces a novel modal-graph framework for the model-free shape servoing of deformable objects with raw point clouds. Unlike the existing works studying the object’s geometry structure, we propose a modal graph to describe the low-frequency deformation structure of the DOM system, which is robust to the measurement irregularities. The modal graph enables us to directly extract low-dimensional deformation features from raw point clouds without extra processing of registrations, refinements, and occlusion removal. It also preserves the spatial structure of the DOM system to inverse the feature changes into robot motions. Moreover, as the framework is built with unknown physical and geometric object models, we design an adaptive robust controller to deform the object toward the desired shape while tackling the modeling uncertainties, noises, and disturbances online. The system is proved to be input-to-state stable (ISS) using Lyapunov-based methods. Extensive experiments are conducted to validate our method using linear, planar, tubular, and volumetric objects under different settings.},
  archive  = {J},
  author   = {Bohan Yang and Congying Sui and Fangxun Zhong and Yun-Hui Liu},
  doi      = {10.1177/02783649231198900},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1213-1244},
  title    = {Modal-graph 3D shape servoing of deformable objects with raw point clouds},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tac2Pose: Tactile object pose estimation from the first
touch. <em>The International Journal of Robotics Research</em>,
<em>42</em>(13), 1185–1209. (<a
href="https://doi.org/10.1177/02783649231196925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we present Tac2Pose, an object-specific approach to tactile pose estimation from the first touch for known objects. Given the object geometry, we learn a tailored perception model in simulation that estimates a probability distribution over possible object poses given a tactile observation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor, we match it against the pre-computed set using an object-specific embedding learned using contrastive learning. We obtain contact shapes from the sensor with an object-agnostic calibration step that maps RGB (red, green, blue) tactile observations to binary contact shapes. This mapping, which can be reused across object and sensor instances, is the only step trained with real sensor data. This results in a perception model that localizes objects from the first real tactile observation. Importantly, it produces pose distributions and can incorporate additional pose constraints coming from other perception systems, multiple contacts, or priors. We provide quantitative results for 20 objects. Tac2Pose provides high accuracy pose estimations from distinctive tactile observations while regressing meaningful pose distributions to account for those contact shapes that could result from different object poses. We extend and test Tac2Pose in multi-contact scenarios where two tactile sensors are simultaneously in contact with the object, as during a grasp with a parallel jaw gripper. We further show that when the output pose distribution is filtered with a prior on the object pose, Tac2Pose is often able to improve significantly on the prior. This suggests synergistic use of Tac2Pose with additional sensing modalities (e.g., vision) even in cases where the tactile observation from a grasp is not sufficiently discriminative. Given a coarse estimate of an object’s pose, even ambiguous contacts can be used to determine an object’s pose precisely. We also test Tac2Pose on object models reconstructed from a 3D scanner, to evaluate the robustness to uncertainty in the object model. We show that even in the presence of model uncertainty, Tac2Pose is able to achieve fine accuracy comparable to when the object model is the manufacturer’s CAD (computer-aided design) model. Finally, we demonstrate the advantages of Tac2Pose compared with three baseline methods for tactile pose estimation: directly regressing the object pose with a neural network, matching an observed contact to a set of possible contacts using a standard classification neural network, and direct pixel comparison of an observed contact with a set of possible contacts. Website: mcube.mit.edu/research/tac2pose.html},
  archive  = {J},
  author   = {Maria Bauza and Antonia Bronars and Alberto Rodriguez},
  doi      = {10.1177/02783649231196925},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1185-1209},
  title    = {Tac2Pose: Tactile object pose estimation from the first touch},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic movement primitives in robotics: A tutorial survey.
<em>The International Journal of Robotics Research</em>,
<em>42</em>(13), 1133–1184. (<a
href="https://doi.org/10.1177/02783649231201196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Biological systems, including human beings, have the innate ability to perform complex tasks in a versatile and agile manner. Researchers in sensorimotor control have aimed to comprehend and formally define this innate characteristic. The idea, supported by several experimental findings, that biological systems are able to combine and adapt basic units of motion into complex tasks finally leads to the formulation of the motor primitives’ theory. In this respect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical formulation of the motor primitives as stable dynamical systems and are well suited to generate motor commands for artificial systems like robots. In the last decades, DMPs have inspired researchers in different robotic fields including imitation and reinforcement learning, optimal control, physical interaction, and human–robot co-working, resulting in a considerable amount of published papers. The goal of this tutorial survey is two-fold. On one side, we present the existing DMP formulations in rigorous mathematical terms and discuss the advantages and limitations of each approach as well as practical implementation details. In the tutorial vein, we also search for existing implementations of presented approaches and release several others. On the other side, we provide a systematic and comprehensive review of existing literature and categorize state-of-the-art work on DMP. The paper concludes with a discussion on the limitations of DMPs and an outline of possible research directions.},
  archive  = {J},
  author   = {Matteo Saveriano and Fares J Abu-Dakka and Aljaž Kramberger and Luka Peternel},
  doi      = {10.1177/02783649231201196},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1133-1184},
  title    = {Dynamic movement primitives in robotics: A tutorial survey},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential monte carlo localization in topometric appearance
maps. <em>The International Journal of Robotics Research</em>,
<em>42</em>(13), 1117–1132. (<a
href="https://doi.org/10.1177/02783649231197723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Representing the scene appearance by a global image descriptor (BoW, NetVLAD, etc.) is a widely adopted choice to address Visual Place Recognition (VPR). The main reasons are that appearance descriptors can be effectively provided with radiometric and perspective invariances as well as they can deal with large environments because of their compactness. However, addressing metric localization with such descriptors (a problem called Appearance-based Localization or AbL) achieves much poorer accuracy than those techniques exploiting the observation of 3D landmarks, which represent the standard for visual localization. In this paper, we propose ALLOM ( A ppearance-based L ocalization with L ocal O bservation M odels) which addresses AbL by leveraging the topological location of a robot within a map to achieve accurate metric estimations. This topology-assisted metric localization is implemented with a sequential Monte Carlo Bayesian filter that applies a specific observation model for each different place of the environment, thus taking advantage of the local correlation between the pose and the appearance descriptor within each region. ALLOM also benefits from the topological structure of the map to detect eventual robot loss-of-tracking and to effectively cope with its relocalization by applying VPR. Our proposal demonstrates superior metric localization capability compared to different state-of-the-art AbL methods under a wide range of situations.},
  archive  = {J},
  author   = {Alberto Jaenal and Francisco-Angel Moreno and Javier Gonzalez-Jimenez},
  doi      = {10.1177/02783649231197723},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1117-1132},
  title    = {Sequential monte carlo localization in topometric appearance maps},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pohang canal dataset: A multimodal maritime dataset for
autonomous navigation in restricted waters. <em>The International
Journal of Robotics Research</em>, <em>42</em>(12), 1104–1114. (<a
href="https://doi.org/10.1177/02783649231191145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a multimodal maritime dataset and the data collection procedure used to gather it, which aims to facilitate autonomous navigation in restricted water environments. The dataset comprises measurements obtained using various perception and navigation sensors, including a stereo camera, an infrared camera, an omnidirectional camera, three LiDARs, a marine radar, a global positioning system, and an attitude heading reference system. The data were collected along a 7.5-km-long route that includes a narrow canal, inner and outer ports, and near-coastal areas in Pohang, South Korea. The collection was conducted under diverse weather and visual conditions. The dataset and its detailed description are available for free download at https://sites.google.com/view/pohang-canal-dataset .},
  archive  = {J},
  author   = {Dongha Chung and Jonghwi Kim and Changyu Lee and Jinwhan Kim},
  doi      = {10.1177/02783649231191145},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1104-1114},
  title    = {Pohang canal dataset: A multimodal maritime dataset for autonomous navigation in restricted waters},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust grasping across diverse sensor qualities: The
GraspNet-1Billion dataset. <em>The International Journal of Robotics
Research</em>, <em>42</em>(12), 1094–1103. (<a
href="https://doi.org/10.1177/02783649231193710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robust object grasping in cluttered scenes is vital to all robotic prehensile manipulation. In this paper, we present the GraspNet-1Billion benchmark that contains rich real-world captured cluttered scenarios and abundant annotations. This benchmark aims at solving two critical problems for the cluttered scenes parallel-finger grasping: the insufficient real-world training data and the lacking of evaluation benchmark. We first contribute a large-scale grasp pose detection dataset. Two different depth cameras based on structured-light and time-of-flight technologies are adopted. Our dataset contains 97,280 RGB-D images with over one billion grasp poses. In total, 190 cluttered scenes are collected, among which 100 are training set and 90 are for testing. Meanwhile, we build an evaluation system that is general and user-friendly. It directly reports a predicted grasp pose’s quality by analytic computation, which is able to evaluate any kind of grasp representation without exhaustively labeling the ground-truth. We further divide the test set into three difficulties to better evaluate algorithms’ generalization ability. Our dataset, accessing API and evaluation code, are publicly available at www.graspnet.net.},
  archive  = {J},
  author   = {Hao-Shu Fang and Minghao Gou and Chenxi Wang and Cewu Lu},
  doi      = {10.1177/02783649231193710},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1094-1103},
  title    = {Robust grasping across diverse sensor qualities: The GraspNet-1Billion dataset},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable nullspace adaptive parameter identification of 6
degree-of-freedom plant and actuator models for underactuated vehicles:
Theory and experimental evaluation. <em>The International Journal of
Robotics Research</em>, <em>42</em>(12), 1070–1093. (<a
href="https://doi.org/10.1177/02783649231191184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Model-based approaches to navigation, control, and fault detection that utilize precise nonlinear models of vehicle plant dynamics will enable more accurate control and navigation, assured autonomy, and more complex missions for such vehicles. This paper reports novel theoretical and experimental results addressing the problem of parameter estimation of plant and actuator models for underactuated underwater vehicles operating in 6 degrees-of-freedom (DOF) whose dynamics are modeled by finite-dimensional Newton-Euler equations. This paper reports the first theoretical approach and experimental validation to identify simultaneously plant-model parameters (parameters such as mass, added mass, hydrodynamic drag, and buoyancy) and control-actuator parameters (control-surface models and thruster models) in 6-DOF. Most previously reported studies on parameter identification assume that the control-actuator parameters are known a priori. Moreover, this paper reports the first proof of convergence of the parameter estimates to the true set of parameters for this class of vehicles under a persistence of excitation condition. The reported adaptive identification (AID) algorithm does not require instrumentation of 6-DOF vehicle acceleration, which is required by conventional approaches to parameter estimation such as least squares. Additionally, the reported AID algorithm is applicable under any arbitrary open-loop or closed-loop control law. We report simulation and experimental results for identifying the plant-model and control-actuator parameters for an L3 OceanServer Iver3 autonomous underwater vehicle. We believe this general approach to AID could be extended to apply to other classes of machines and other classes of marine, land, aerial, and space vehicles.},
  archive  = {J},
  author   = {Zachary J. Harris and Annie M. Mao and Tyler M. Paine and Louis L. Whitcomb},
  doi      = {10.1177/02783649231191184},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1070-1093},
  title    = {Stable nullspace adaptive parameter identification of 6 degree-of-freedom plant and actuator models for underactuated vehicles: Theory and experimental evaluation},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fundamental limits for sensor-based robot control. <em>The
International Journal of Robotics Research</em>, <em>42</em>(12),
1051–1069. (<a href="https://doi.org/10.1177/02783649231190947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot’s sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano&#39;s inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision-making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability of our approach to establish strong limits on achievable performance for these problems by comparing our upper bounds with achievable lower bounds (computed by synthesizing or learning concrete control policies).},
  archive  = {J},
  author   = {Anirudha Majumdar and Zhiting Mei and Vincent Pacelli},
  doi      = {10.1177/02783649231190947},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1051-1069},
  title    = {Fundamental limits for sensor-based robot control},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). USTC FLICAR: A sensors fusion dataset of
LiDAR-inertial-camera for heavy-duty autonomous aerial work robots.
<em>The International Journal of Robotics Research</em>,
<em>42</em>(11), 1015–1047. (<a
href="https://doi.org/10.1177/02783649231195650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we present the USTC FLICAR Dataset , which is dedicated to the development of simultaneous localization and mapping and precise 3D reconstruction of the workspace for heavy-duty autonomous aerial work robots. In recent years, numerous public datasets have played significant roles in the advancement of autonomous cars and unmanned aerial vehicles (UAVs). However, these two platforms differ from aerial work robots: UAVs are limited in their payload capacity, while cars are restricted to two-dimensional movements. To fill this gap, we create the “Giraffe” mapping robot based on a bucket truck, which is equipped with a variety of well-calibrated and synchronized sensors: four 3D LiDARs, two stereo cameras, two monocular cameras, Inertial Measurement Units (IMUs), and a GNSS/INS system. A laser tracker is used to record the millimeter-level ground truth positions. We also make its ground twin, the “Okapi” mapping robot, to gather data for comparison. The proposed dataset extends the typical autonomous driving sensing suite to aerial scenes, demonstrating the potential of combining autonomous driving perception systems with bucket trucks to create a versatile autonomous aerial working platform. Moreover, based on the Segment Anything Model (SAM), we produce the Semantic FLICAR dataset, which provides fine-grained semantic segmentation annotations for multimodal continuous data in both temporal and spatial dimensions. The dataset is available for download at: https://ustc-flicar.github.io/ .},
  archive  = {J},
  author   = {Ziming Wang and Yujiang Liu and Yifan Duan and Xingchen Li and Xinran Zhang and Jianmin Ji and Erbao Dong and Yanyong Zhang},
  doi      = {10.1177/02783649231195650},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {11},
  pages    = {1015-1047},
  title    = {USTC FLICAR: A sensors fusion dataset of LiDAR-inertial-camera for heavy-duty autonomous aerial work robots},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BED-BPP: Benchmarking dataset for robotic bin packing
problems. <em>The International Journal of Robotics Research</em>,
<em>42</em>(11), 1007–1014. (<a
href="https://doi.org/10.1177/02783649231193048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many algorithms that were developed for solving three-dimensional bin packing problems use generic data for either experiments or evaluation. However, none of these datasets became accepted for benchmarking 3D bin packing algorithms throughout the community. To close this gap, this paper presents the benchmarking dataset for robotic bin packing problems (BED-BPP), which is based on realistic data. We show the variety of the dataset by elaborating an n-gram analysis. Besides, we propose an evaluation function, which contains a stability check that uses rigid body simulation. We demonstrated the application of our dataset on four different approaches, which we integrated in our software environment.},
  archive  = {J},
  author   = {Florian Kagerer and Maximilian Beinhofer and Stefan Stricker and Andreas Nüchter},
  doi      = {10.1177/02783649231193048},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {11},
  pages    = {1007-1014},
  title    = {BED-BPP: Benchmarking dataset for robotic bin packing problems},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey of maps of dynamics for mobile robots. <em>The
International Journal of Robotics Research</em>, <em>42</em>(11),
977–1006. (<a href="https://doi.org/10.1177/02783649231190428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robotic mapping provides spatial information for autonomous agents. Depending on the tasks they seek to enable, the maps created range from simple 2D representations of the environment geometry to complex, multilayered semantic maps. This survey article is about maps of dynamics (MoDs), which store semantic information about typical motion patterns in a given environment. Some MoDs use trajectories as input, and some can be built from short, disconnected observations of motion. Robots can use MoDs, for example, for global motion planning, improved localization, or human motion prediction. Accounting for the increasing importance of maps of dynamics, we present a comprehensive survey that organizes the knowledge accumulated in the field and identifies promising directions for future work. Specifically, we introduce field-specific vocabulary, summarize existing work according to a novel taxonomy, and describe possible applications and open research problems. We conclude that the field is mature enough, and we expect that maps of dynamics will be increasingly used to improve robot performance in real-world use cases. At the same time, the field is still in a phase of rapid development where novel contributions could significantly impact this research area.},
  archive  = {J},
  author   = {Tomasz Piotr Kucner and Martin Magnusson and Sariah Mghames and Luigi Palmieri and Francesco Verdoja and Chittaranjan Srinivas Swaminathan and Tomáš Krajník and Erik Schaffernicht and Nicola Bellotto and Marc Hanheide and Achim J Lilienthal},
  doi      = {10.1177/02783649231190428},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {11},
  pages    = {977-1006},
  title    = {Survey of maps of dynamics for mobile robots},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rearrangement on lattices with pick-n-swaps: Optimality
structures and efficient algorithms. <em>The International Journal of
Robotics Research</em>, <em>42</em>(10), 957–973. (<a
href="https://doi.org/10.1177/02783649231153901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study a class of rearrangement problems under a novel pick-n-swap prehensile manipulation model, in which a robotic manipulator, capable of carrying an item and making item swaps, is tasked to sort items stored in lattices of variable dimensions in a time-optimal manner. We systematically analyze the intrinsic optimality structure, which is fairly rich and intriguing, under different levels of item distinguishability (fully-labeled, where each item has a unique label, or partially-labeled, where multiple items may be of the same type) and different lattice dimensions. Focusing on the most practical setting of one and two dimensions, we develop low polynomial time cycle-following-based algorithms that optimally perform rearrangements on 1D lattices under both fully- and partially-labeled settings. On the other hand, we show that rearrangement on 2D and higher-dimensional lattices become computationally intractable to optimally solve. Despite their NP-hardness, we prove that efficient cycle-following-based algorithms remain optimal in the asymptotic sense for 2D fully- and partially-labeled settings, in expectation, using the interesting fact that random permutations induce only a small number of cycles. We further improve these algorithms to provide 1. x -optimality when the number of items is small. Simulation studies corroborate the effectiveness of our algorithms. The implementation of the algorithms from the paper can be found at github.com/arc-l/lattice-rearrangement.},
  archive  = {J},
  author   = {Jingjin Yu},
  doi      = {10.1177/02783649231153901},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {957-973},
  title    = {Rearrangement on lattices with pick-n-swaps: Optimality structures and efficient algorithms},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sampling and learning framework to prove motion planning
infeasibility. <em>The International Journal of Robotics Research</em>,
<em>42</em>(10), 938–956. (<a
href="https://doi.org/10.1177/02783649231154674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a learning-based approach to prove infeasibility of kinematic motion planning problems. Sampling-based motion planners are effective in high-dimensional spaces but are only probabilistically complete. Consequently, these planners cannot provide a definite answer if no plan exists, which is important for high-level scenarios, such as task-motion planning. We apply data generated during multi-directional sampling-based planning (such as PRM) to a machine learning approach to construct an infeasibility proof. An infeasibility proof is a closed manifold in the obstacle region of the configuration space that separates the start and goal into disconnected components of the free configuration space. We train the manifold using common machine learning techniques and then triangulate the manifold into a polytope to prove containment in the obstacle region. Under assumptions about the hyper-parameters and robustness of configuration space optimization, the output is either an infeasibility proof or a motion plan in the limit. We demonstrate proof construction for up to 4-DOF configuration spaces. A large part of the algorithm is parallelizable, which offers potential to address higher dimensional configuration spaces.},
  archive  = {J},
  author   = {Sihui Li and Neil T. Dantam},
  doi      = {10.1177/02783649231154674},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {938-956},
  title    = {A sampling and learning framework to prove motion planning infeasibility},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online and offline learning of player objectives from
partial observations in dynamic games. <em>The International Journal of
Robotics Research</em>, <em>42</em>(10), 917–937. (<a
href="https://doi.org/10.1177/02783649231182453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robots deployed to the real world must be able to interact with other agents in their environment. Dynamic game theory provides a powerful mathematical framework for modeling scenarios in which agents have individual objectives and interactions evolve over time. However, a key limitation of such techniques is that they require a priori knowledge of all players’ objectives. In this work, we address this issue by proposing a novel method for learning players’ objectives in continuous dynamic games from noise-corrupted, partial state observations. Our approach learns objectives by coupling the estimation of unknown cost parameters of each player with inference of unobserved states and inputs through Nash equilibrium constraints. By coupling past state estimates with future state predictions, our approach is amenable to simultaneous online learning and prediction in receding horizon fashion. We demonstrate our method in several simulated traffic scenarios in which we recover players’ preferences, for, e.g. desired travel speed and collision-avoidance behavior. Results show that our method reliably estimates game-theoretic models from noise-corrupted data that closely matches ground-truth objectives, consistently outperforming state-of-the-art approaches.},
  archive  = {J},
  author   = {Lasse Peters and Vicenç Rubies-Royo and Claire J Tomlin and Laura Ferranti and Javier Alonso-Mora and Cyrill Stachniss and David Fridovich-Keil},
  doi      = {10.1177/02783649231182453},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {917-937},
  title    = {Online and offline learning of player objectives from partial observations in dynamic games},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GROUNDED: A localizing ground penetrating radar evaluation
dataset for learning to localize in inclement weather. <em>The
International Journal of Robotics Research</em>, <em>42</em>(10),
901–916. (<a href="https://doi.org/10.1177/02783649231183460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mapping and localization using surface features is prone to failure due to environment changes such as inclement weather. Recently, Localizing Ground Penetrating Radar (LGPR) has been proposed as an alternative means of localizing using underground features that are stable over time and less affected by surface conditions. However, due to the lack of commercially available LGPR sensors, the wider research community has been largely unable to replicate this work or build new and innovative solutions. We present GROUNDED, an open dataset of LGPR scans collected in a variety of environments and weather conditions. By labeling these data with ground truth localization from an RTK-GPS/Inertial Navigation System, and carefully calibrating and time-synchronizing the radar scans with ground truth positions, camera imagery, and lidar data, we enable researchers to build novel localization solutions that are resilient to changing surface conditions. We include 108 individual runs totaling 450 km of driving with LGPR, GPS, odometry, camera, and lidar measurements. We also present two new evaluation benchmarks for 1) localizing in weather and 2) multi-lane localization, to enable comparisons of future work supported by the dataset. Additionally, we present a first application of the new dataset in the form of LGPRNet: an inception-based CNN architecture for learning localization that is resilient to changing weather conditions. The dataset can be accessed at http://lgprdata.com .},
  archive  = {J},
  author   = {Teddy Ort and Igor Gilitschenski and Daniela Rus},
  doi      = {10.1177/02783649231183460},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {901-916},
  title    = {GROUNDED: A localizing ground penetrating radar evaluation dataset for learning to localize in inclement weather},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous latent state preintegration for inertial-aided
systems. <em>The International Journal of Robotics Research</em>,
<em>42</em>(10), 874–900. (<a
href="https://doi.org/10.1177/02783649231199537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditionally, the pose and velocity prediction of a system at time t 2 given inertial measurements from a 6-DoF IMU depends on the knowledge of the system’s state at time t 1 . It involves a series of integration and double integration that can be computationally expensive if performed regularly, in particular in the context of inertial-aided optimisation-based state estimation. The concept of preintegration consists of creating pseudo-measurements that are independent of the system’s initial conditions (pose and velocity at t 1 ) in order to predict the system’s state at t 2 . These pseudo-measurements, so-called preintegrated measurements, were originally computed numerically using the integration rectangle rule. This article presents a novel method to perform continuous preintegration using Gaussian processes (GPs) to model the system’s dynamics focusing on high accuracy. It represents the preintegrated measurement’s derivatives in a continuous latent state that is learnt/optimised according to asynchronous IMU gyroscope and accelerometer measurements. The GP models allow for analytical integration and double integration of the latent state to generate accurate preintegrated measurements called unified Gaussian preintegrated measurements (UGPMs). We show through extensive quantitative experiments that the proposed UGPMs outperform the standard preintegration method by an order of magnitude. Additionally, we demonstrate that the UGPMs can be integrated into off-the-shelf multi-modal estimation frameworks with ease based on lidar-inertial, RGBD-inertial, and visual-inertial real-world experiments.},
  archive  = {J},
  author   = {Cedric Le Gentil and Teresa Vidal-Calleja},
  doi      = {10.1177/02783649231199537},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {874-900},
  title    = {Continuous latent state preintegration for inertial-aided systems},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing contact patterns for robot locomotion via
geometric mechanics. <em>The International Journal of Robotics
Research</em>, <em>42</em>(10), 859–873. (<a
href="https://doi.org/10.1177/02783649231188387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contact planning is crucial to the locomotion performance of robots: to properly self-propel forward, it is not only important to determine the sequence of internal shape changes (e.g., body bending and limb shoulder joint oscillation) but also the sequence by which contact is made and broken between the mechanism and its environment. Prior work observed that properly coupling contact patterns and shape changes allows for computationally tractable gait design and efficient gait performance. The state of the art, however, made assumptions, albeit motivated by biological observation, as to how contact and shape changes can be coupled. In this paper, we extend the geometric mechanics (GM) framework to design contact patterns. Specifically, we introduce the concept of “contact space” to the GM framework. By establishing the connection between velocities in shape and position spaces, we can estimate the benefits of each contact pattern change and therefore optimize the sequence of contact patterns. In doing so, we can also analyze how a contact pattern sequence will respond to perturbations. We apply our framework to sidewinding robots and enable (1) effective locomotion direction control and (2) robust locomotion performance as the spatial resolution decreases. We also apply our framework to a hexapod robot with two back-bending joints and show that we can simplify existing hexapod gaits by properly reducing the number of contact state switches (during a gait cycle) without significant loss of locomotion speed. We test our designed gaits with robophysical experiments, and we obtain good agreement between theory and experiments.},
  archive  = {J},
  author   = {Baxi Chong and Tianyu Wang and Lin Bo and Shengkai Li and Pranav C. Muthukrishnan and Juntao He and Daniel Irvine and Howie Choset and Grigoriy Blekherman and Daniel I. Goldman},
  doi      = {10.1177/02783649231188387},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {859-873},
  title    = {Optimizing contact patterns for robot locomotion via geometric mechanics},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composable energy policies for reactive motion generation
and reinforcement learning. <em>The International Journal of Robotics
Research</em>, <em>42</em>(10), 827–858. (<a
href="https://doi.org/10.1177/02783649231179499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we introduce composable energy policies (CEP), a novel framework for multi-objective motion generation. We frame the problem of composing multiple policy components from a probabilistic view. We consider a set of stochastic policies represented in arbitrary task spaces, where each policy represents a distribution of the actions to solve a particular task. Then, we aim to find the action in the configuration space that optimally satisfies all the policy components. The presented framework allows the fusion of motion generators from different sources: optimal control, data-driven policies, motion planning, and handcrafted policies. Classically, the problem of multi-objective motion generation is solved by the composition of a set of deterministic policies, rather than stochastic policies. However, there are common situations where different policy components have conflicting behaviors, leading to oscillations or the robot getting stuck in an undesirable state. While our approach is not directly able to solve the conflicting policies problem, we claim that modeling each policy as a stochastic policy allows more expressive representations for each component in contrast with the classical reactive motion generation approaches. In some tasks, such as reaching a target in a cluttered environment, we show experimentally that CEP additional expressivity allows us to model policies that reduce these conflicting behaviors. A field that benefits from these reactive motion generators is the one of robot reinforcement learning. Integrating these policy architectures with reinforcement learning allows us to include a set of inductive biases in the learning problem. These inductive biases guide the reinforcement learning agent towards informative regions or improve collision safety while exploring. In our work, we show how to integrate our proposed reactive motion generator as a structured policy for reinforcement learning. Combining the reinforcement learning agent exploration with the prior-based CEP, we can improve the learning performance and explore safer.},
  archive  = {J},
  author   = {Julen Urain and Anqi Li and Puze Liu and Carlo D’Eramo and Jan Peters},
  doi      = {10.1177/02783649231179499},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {827-858},
  title    = {Composable energy policies for reactive motion generation and reinforcement learning},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward certifiable optimal motion planning for medical
steerable needles. <em>The International Journal of Robotics
Research</em>, <em>42</em>(10), 798–826. (<a
href="https://doi.org/10.1177/02783649231165818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Medical steerable needles can follow 3D curvilinear trajectories to avoid anatomical obstacles and reach clinically significant targets inside the human body. Automating steerable needle procedures can enable physicians and patients to harness the full potential of steerable needles by maximally leveraging their steerability to safely and accurately reach targets for medical procedures such as biopsies. For the automation of medical procedures to be clinically accepted, it is critical from a patient care, safety, and regulatory perspective to certify the correctness and effectiveness of the planning algorithms involved in procedure automation. In this paper, we take an important step toward creating a certifiable optimal planner for steerable needles. We present an efficient, resolution-complete motion planner for steerable needles based on a novel adaptation of multi-resolution planning. This is the first motion planner for steerable needles that guarantees to compute in finite time an obstacle-avoiding plan (or notify the user that no such plan exists), under clinically appropriate assumptions. Based on this planner, we then develop the first resolution-optimal motion planner for steerable needles that further provides theoretical guarantees on the quality of the computed motion plan, that is, global optimality, in finite time. Compared to state-of-the-art steerable needle motion planners, we demonstrate with clinically realistic simulations that our planners not only provide theoretical guarantees but also have higher success rates, have lower computation times, and result in higher quality plans.},
  archive  = {J},
  author   = {Mengyu Fu and Kiril Solovey and Oren Salzman and Ron Alterovitz},
  doi      = {10.1177/02783649231165818},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {798-826},
  title    = {Toward certifiable optimal motion planning for medical steerable needles},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Control-oriented meta-learning. <em>The International
Journal of Robotics Research</em>, <em>42</em>(10), 777–797. (<a
href="https://doi.org/10.1177/02783649231165085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real-time adaptation is imperative to the control of robots operating in complex, dynamic environments. Adaptive control laws can endow even nonlinear systems with good trajectory tracking performance, provided that any uncertain dynamics terms are linearly parameterizable with known nonlinear features. However, it is often difficult to specify such features a priori, such as for aerodynamic disturbances on rotorcraft or interaction forces between a manipulator arm and various objects. In this paper, we turn to data-driven modeling with neural networks to learn, offline from past data, an adaptive controller with an internal parametric model of these nonlinear features. Our key insight is that we can better prepare the controller for deployment with control-oriented meta-learning of features in closed-loop simulation, rather than regression-oriented meta-learning of features to fit input-output data. Specifically, we meta-learn the adaptive controller with closed-loop tracking simulation as the base-learner and the average tracking error as the meta-objective. With both fully actuated and underactuated nonlinear planar rotorcraft subject to wind, we demonstrate that our adaptive controller outperforms other controllers trained with regression-oriented meta-learning when deployed in closed-loop for trajectory tracking control.},
  archive  = {J},
  author   = {Spencer M. Richards and Navid Azizan and Jean-Jacques Slotine and Marco Pavone},
  doi      = {10.1177/02783649231165085},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {777-797},
  title    = {Control-oriented meta-learning},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing running buffers for tabletop object
rearrangement: Complexity, fast algorithms, and applications. <em>The
International Journal of Robotics Research</em>, <em>42</em>(10),
755–776. (<a href="https://doi.org/10.1177/02783649231178565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For rearranging objects on tabletops with overhand grasps, temporarily relocating objects to some buffer space may be necessary. This raises the natural question of how many simultaneous storage spaces, or “running buffers,” are required so that certain classes of tabletop rearrangement problems are feasible. In this work, we examine the problem for both labeled and unlabeled settings. On the structural side, we observe that finding the minimum number of running buffers ( MRB ) can be carried out on a dependency graph abstracted from a problem instance and show that computing MRB is NP-hard. We then prove that under both labeled and unlabeled settings, even for uniform cylindrical objects, the number of required running buffers may grow unbounded as the number of objects to be rearranged increases. We further show that the bound for the unlabeled case is tight. On the algorithmic side, we develop effective exact algorithms for finding MRB for both labeled and unlabeled tabletop rearrangement problems, scalable to over a hundred objects under very high object density. More importantly, our algorithms also compute a sequence witnessing the computed MRB that can be used for solving object rearrangement tasks. Employing these algorithms, empirical evaluations reveal that random labeled and unlabeled instances, which more closely mimic real-world setups generally have fairly small MRB s. Using real robot experiments, we demonstrate that the running buffer abstraction leads to state-of-the-art solutions for the in-place rearrangement of many objects in a tight, bounded workspace.},
  archive  = {J},
  author   = {Kai Gao and Si Wei Feng and Baichuan Huang and Jingjin Yu},
  doi      = {10.1177/02783649231178565},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {755-776},
  title    = {Minimizing running buffers for tabletop object rearrangement: Complexity, fast algorithms, and applications},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reactive motion generation on learned riemannian manifolds.
<em>The International Journal of Robotics Research</em>,
<em>42</em>(10), 729–754. (<a
href="https://doi.org/10.1177/02783649231193046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian-manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is intended to recover full-pose end-effector states and joint-space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacle-aware ambient metric. The motion generated using these geodesics may naturally result in multiple-solution tasks that have not been explicitly demonstrated previously. We extensively tested our approach in task-space and joint-space scenarios using a 7-DoF robotic manipulator. We demonstrate that our method is capable of learning and generating motion skills based on complicated motion patterns demonstrated by a human operator. Additionally, we assess several obstacle-avoidance strategies and generate trajectories in multiple-mode settings.},
  archive  = {J},
  author   = {Hadi Beik-Mohammadi and Søren Hauberg and Georgios Arvanitidis and Gerhard Neumann and Leonel Rozo},
  doi      = {10.1177/02783649231193046},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {729-754},
  title    = {Reactive motion generation on learned riemannian manifolds},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convex risk-bounded continuous-time trajectory planning and
tube design in uncertain nonconvex environments. <em>The International
Journal of Robotics Research</em>, <em>42</em>(10), 705–728. (<a
href="https://doi.org/10.1177/02783649231183458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we address the trajectory planning problem in uncertain nonconvex static and dynamic environments that contain obstacles with probabilistic location, size, and geometry. To address this problem, we provide a risk-bounded trajectory planning method that looks for continuous-time trajectories with guaranteed bounded risk over the planning time horizon. Risk is defined as the probability of collision with uncertain obstacles. Existing approaches to address risk-bounded trajectory planning problems either are limited to Gaussian uncertainties and convex obstacles or rely on sampling-based methods that need uncertainty samples and time discretization. To address the risk-bounded trajectory planning problem, we leverage the notion of risk contours to transform the risk-bounded planning problem into a deterministic optimization problem. Risk contours are the set of all points in the uncertain environment with guaranteed bounded risk. The obtained deterministic optimization is, in general, nonlinear and nonconvex time-varying optimization. We provide convex methods based on sum-of-squares optimization to efficiently solve the obtained nonconvex time-varying optimization problem and obtain the continuous-time risk-bounded trajectories without time discretization. The provided approach deals with arbitrary (and known) probabilistic uncertainties, nonconvex and nonlinear, static and dynamic obstacles, and is suitable for online trajectory planning problems. In addition, we provide convex methods based on sum-of-squares optimization to build the max-sized tube with respect to its parameterization along the trajectory so that any state inside the tube is guaranteed to have bounded risk.},
  archive  = {J},
  author   = {Ashkan Jasour and Weiqiao Han and Brian C. Williams},
  doi      = {10.1177/02783649231183458},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10},
  pages    = {705-728},
  title    = {Convex risk-bounded continuous-time trajectory planning and tube design in uncertain nonconvex environments},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selected papers from RSS2021. <em>The International Journal
of Robotics Research</em>, <em>42</em>(10), 703–704. (<a
href="https://doi.org/10.1177/02783649231199044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {M. Ani Hsieh and Dylan A. Shell},
  doi     = {10.1177/02783649231199044},
  journal = {The International Journal of Robotics Research},
  month   = {9},
  number  = {10},
  pages   = {703-704},
  title   = {Selected papers from RSS2021},
  volume  = {42},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eiffel tower: A deep-sea underwater dataset for long-term
visual localization. <em>The International Journal of Robotics
Research</em>, <em>42</em>(9), 689–699. (<a
href="https://doi.org/10.1177/02783649231177322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet, it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of 5 years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at seanoe.org/data/00810/92226/.},
  archive  = {J},
  author   = {Clémentin Boittiaux and Claire Dune and Maxime Ferrera and Aurélien Arnaubec and Ricard Marxer and Marjolaine Matabos and Loïc Van Audenhaege and Vincent Hugel},
  doi      = {10.1177/02783649231177322},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {689-699},
  title    = {Eiffel tower: A deep-sea underwater dataset for long-term visual localization},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust feedback motion planning via contraction theory.
<em>The International Journal of Robotics Research</em>, <em>42</em>(9),
655–688. (<a href="https://doi.org/10.1177/02783649231186165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a framework for online generation of robust motion plans for robotic systems with nonlinear dynamics subject to bounded disturbances, control constraints, and online state constraints such as obstacles. In an offline phase, one computes the structure of a feedback controller that can be efficiently implemented online to track any feasible nominal trajectory. The offline phase leverages contraction theory , specifically, Control Contraction Metrics, and convex optimization to characterize a fixed-size “tube” that the state is guaranteed to remain within while tracking a nominal trajectory (representing the center of the tube). In the online phase, when the robot is faced with obstacles, a motion planner uses such a tube as a robustness margin for collision checking, yielding nominal trajectories that can be safely executed, that is, tracked without collisions under disturbances. In contrast to recent work on robust online planning using funnel libraries, our approach is not restricted to a fixed library of maneuvers computed offline and is thus particularly well-suited to applications such as UAV flight in densely cluttered environments where complex maneuvers may be required to reach a goal. We demonstrate our approach through numerical simulations of planar and 3D quadrotors, and hardware results on a quadrotor platform navigating a complex obstacle environment while subject to aerodynamic disturbances. The results demonstrate the ability of our approach to jointly balance motion safety and efficiency for agile robotic systems.},
  archive  = {J},
  author   = {Sumeet Singh and Benoit Landry and Anirudha Majumdar and Jean-Jacques Slotine and Marco Pavone},
  doi      = {10.1177/02783649231186165},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {655-688},
  title    = {Robust feedback motion planning via contraction theory},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stabilizing deep q-learning with q-graph-based bounds.
<em>The International Journal of Robotics Research</em>, <em>42</em>(9),
633–654. (<a href="https://doi.org/10.1177/02783649231185165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {State-of-the art deep reinforcement learning has enabled autonomous agents to learn complex strategies from scratch on many problems including continuous control tasks. Deep Q-networks (DQN) and deep deterministic policy gradients (DDPGs) are two such algorithms which are both based on Q-learning. They therefore all share function approximation, off-policy behavior, and bootstrapping—the constituents of the so-called deadly triad that is known for its convergence issues. We suggest to take a graph perspective on the data an agent has collected and show that the structure of this data graph is linked to the degree of divergence that can be expected. We further demonstrate that a subset of states and actions from the data graph can be selected such that the resulting finite graph can be interpreted as a simplified Markov decision process (MDP) for which the Q-values can be computed analytically. These Q-values are lower bounds for the Q-values in the original problem, and enforcing these bounds in temporal difference learning can help to prevent soft divergence. We show further effects on a simulated continuous control task, including improved sample efficiency, increased robustness toward hyperparameters as well as a better ability to cope with limited replay memory. Finally, we demonstrate the benefits of our method on a large robotic benchmark with an industrial assembly task and approximately 60 h of real-world interaction.},
  archive  = {J},
  author   = {Sabrina Hoppe and Markus Giftthaler and Robert Krug and Marc Toussaint},
  doi      = {10.1177/02783649231185165},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {633-654},
  title    = {Stabilizing deep Q-learning with Q-graph-based bounds},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ViF-GTAD: A new automotive dataset with ground truth for
ADAS/AD development, testing, and validation. <em>The International
Journal of Robotics Research</em>, <em>42</em>(8), 614–630. (<a
href="https://doi.org/10.1177/02783649231188146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A new dataset for automated driving, which is the subject matter of this paper, identifies and addresses a gap in existing similar perception datasets. While most state-of-the-art perception datasets primarily focus on the provision of various onboard sensor measurements along with the semantic information under various driving conditions, the provided information is often insufficient since the object list and position data provided include unknown and time-varying errors. The current paper and the associated dataset describe the first publicly available perception measurement data that include not only the onboard sensor information from the camera, Lidar, and radar with semantically classified objects but also the high-precision ground-truth position measurements enabled by the accurate RTK-assisted GPS localization systems available on both the ego vehicle and the dynamic target objects. This paper provides insight on the capturing of the data, explicitly explaining the metadata structure and the content, as well as the potential application examples where it has been, and can potentially be, applied and implemented in relation to automated driving and environmental perception systems development, testing, and validation.},
  archive  = {J},
  author   = {Sarah Haas and Selim Solmaz and Jakob Reckenzaun and Simon Genser},
  doi      = {10.1177/02783649231188146},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {614-630},
  title    = {ViF-GTAD: A new automotive dataset with ground truth for ADAS/AD development, testing, and validation},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic drilling for the chinese chang’e 5 lunar
sample-return mission. <em>The International Journal of Robotics
Research</em>, <em>42</em>(8), 586–613. (<a
href="https://doi.org/10.1177/02783649231187918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {On December 2, 2020, a 2-m class robotic drill onboard the Chinese Chang’E 5 lunar lander successfully penetrated 1 m into the lunar regolith and collected 259.72 g of samples. This paper presents the design and development, terrestrial tests, and lunar sampling results of the robotic drill. First, the system design of the robotic drill, including its engineering objectives, drill configuration, drilling and coring methods, and rotational speed determination, was studied. Subsequently, a control strategy was proposed to address the geological uncertainty and complexity of the lunar surface. Terrestrial tests were conducted to assess the sampling performance of the robotic drill under both atmospheric and vacuum conditions. Finally, the results of drilling on the lunar surface were obtained, and the complex geological conditions encountered were analyzed. The success of the Chinese Chang’E 5 lunar sample-return mission demonstrates the feasibility of the proposed robotic drill. This study can serve as an important reference for future extraterrestrial robotic regolith-sampling missions.},
  archive  = {J},
  author   = {Tao Zhang and Yong Pang and Ting Zeng and Guoxin Wang and Shen Yin and Kun Xu and Guidong Mo and Xingwang Zhang and Lusi Wang and Shuai Yang and Zeng Zhao and Junjie Qin and Junshan Gong and Zhongxian Zhao and Xuefeng Tong and Zhongwang Yin and Haiyuan Wang and Fan Zhao and Yanhong Zheng and Xiangjin Deng and Bin Wang and Jinchang Xu and Wei Wang and Shuangfei Yu and Xiaoming Lai and Xilun Ding},
  doi      = {10.1177/02783649231187918},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {586-613},
  title    = {Robotic drilling for the chinese Chang’E 5 lunar sample-return mission},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous navigation of underactuated bipedal robots in
height-constrained environments. <em>The International Journal of
Robotics Research</em>, <em>42</em>(8), 565–585. (<a
href="https://doi.org/10.1177/02783649231187670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Navigating a large-scaled robot in unknown and cluttered height-constrained environments is challenging. Not only is a fast and reliable planning algorithm required to go around obstacles, the robot should also be able to change its intrinsic dimension by crouching in order to travel underneath height-constrained regions. There are few mobile robots that are capable of handling such a challenge, and bipedal robots provide a solution. However, as bipedal robots have nonlinear and hybrid dynamics, trajectory planning while ensuring dynamic feasibility and safety on these robots is challenging. This paper presents an end-to-end autonomous navigation framework which leverages three layers of planners and a variable walking height controller to enable bipedal robots to safely explore height-constrained environments. A vertically actuated spring-loaded inverted pendulum (vSLIP) model is introduced to capture the robot’s coupled dynamics of planar walking and vertical walking height. This reduced-order model is utilized to optimize for long-term and short-term safe trajectory plans. A variable walking height controller is leveraged to enable the bipedal robot to maintain stable periodic walking gaits while following the planned trajectory. The entire framework is tested and experimentally validated using a bipedal robot Cassie. This demonstrates reliable autonomy to drive the robot to safely avoid obstacles while walking to the goal location in various kinds of height-constrained cluttered environments.},
  archive  = {J},
  author   = {Zhongyu Li and Jun Zeng and Shuxiao Chen and Koushil Sreenath},
  doi      = {10.1177/02783649231187670},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {565-585},
  title    = {Autonomous navigation of underactuated bipedal robots in height-constrained environments},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple kinesthetic haptics for object recognition. <em>The
International Journal of Robotics Research</em>, <em>42</em>(7),
537–561. (<a href="https://doi.org/10.1177/02783649231182486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Object recognition is an essential capability when performing various tasks. Humans naturally use either or both visual and tactile perception to extract object class and properties. Typical approaches for robots, however, require complex visual systems or multiple high-density tactile sensors which can be highly expensive. In addition, they usually require actual collection of a large dataset from real objects through direct interaction. In this paper, we propose a kinesthetic-based object recognition method that can be performed with any multi-fingered robotic hand in which the kinematics is known. The method does not require tactile sensors and is based on observing grasps of the objects. We utilize a unique and frame invariant parameterization of grasps to learn instances of object shapes. To train a classifier, training data is generated rapidly and solely in a computational process without interaction with real objects. We then propose and compare between two iterative algorithms that can integrate any trained classifier. The classifiers and algorithms are independent of any particular robot hand and, therefore, can be exerted on various ones. We show in experiments, that with few grasps, the algorithms acquire accurate classification. Furthermore, we show that the object recognition approach is scalable to objects of various sizes. Similarly, a global classifier is trained to identify general geometries (e.g., an ellipsoid or a box) rather than particular ones and demonstrated on a large set of objects. Full scale experiments and analysis are provided to show the performance of the method.},
  archive  = {J},
  author   = {Avishai Sintov and Inbar Meir},
  doi      = {10.1177/02783649231182486},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {537-561},
  title    = {Simple kinesthetic haptics for object recognition},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated planning and control of robotic surgical
instruments for task autonomy. <em>The International Journal of Robotics
Research</em>, <em>42</em>(7), 504–536. (<a
href="https://doi.org/10.1177/02783649231179753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Agile maneuvers are essential for robot-enabled complex tasks such as surgical procedures. Prior explorations on surgery autonomy are limited to feasibility study of completing a single task without systematically addressing generic manipulation safety across different tasks. We present an integrated planning and control framework for 6-DoF robotic instruments for pipeline automation of surgical tasks. We leverage the geometry of a robotic instrument and propose the nodal state space to represent the robot state in SE (3) space. Each elementary robot motion could be encoded by regulation of the state parameters via a dynamical system. This theoretically ensures that every in-process trajectory is globally feasible and stably reached to an admissible target, and the controller is of closed-form without computing 6-DoF inverse kinematics. Then, to plan the motion steps reliably, we propose an interactive (instant) goal state of the robot that transforms manipulation planning through desired path constraints into a goal-varying manipulation (GVM) problem. We detail how GVM could adaptively and smoothly plan the procedure (could proceed or rewind the process as needed) based on on-the-fly situations under dynamic or disturbed environment. Finally, we extend the above policy to characterize complete pipelines of various surgical tasks. Simulations show that our framework could smoothly solve twisted maneuvers while avoiding collisions. Physical experiments using the da Vinci Research Kit validates the capability of automating individual tasks including tissue debridement, dissection, and wound suturing. The results confirm good task-level consistency and reliability compared to state-of-the-art automation algorithms.},
  archive  = {J},
  author   = {Fangxun Zhong and Yun-Hui Liu},
  doi      = {10.1177/02783649231179753},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {504-536},
  title    = {Integrated planning and control of robotic surgical instruments for task autonomy},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling four-arm laparoscopic surgery by controlling two
robotic assistants via haptic foot interfaces. <em>The International
Journal of Robotics Research</em>, <em>42</em>(7), 475–503. (<a
href="https://doi.org/10.1177/02783649231180366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robotic surgery is a promising direction to improve surgeons and assistants’ daily life with respect to conventional surgery. In this work, we propose solo laparoscopic surgery in which two robotic arms, controlled via haptic foot interfaces, assist the task of the hands. Such a system opens the door for simultaneous control of four laparoscopic tools by the same user. Each hand controls a manipulative tool while a foot controls an endoscope/camera and another controls an actuated gripper. In this scenario, the surgeon and robots need to work collaboratively within a concurrent workspace, while meeting the precision demands of surgery. To this end, we propose a control framework for the robotic arms that deals with all the task- and safety-related constraints. Furthermore, to ease the control through the feet, two assistance modalities are proposed: adaptive visual tracking of the laparoscopic instruments with the camera and grasping assistance for the gripper. A user study is conducted on twelve subjects to highlight the ease of use of the system and to evaluate the relevance of the proposed shared control strategies. The results confirm the feasibility of four-arm surgical-like tasks without extensive training in tasks that involve visual-tracking and manipulation goals for the feet, as well as coordination with both hands. Moreover, our study characterizes and motivates the use of robotic assistance for reducing task load, improving performance, increasing fluency, and eliciting higher coordination during four-arm laparoscopic tasks.},
  archive  = {J},
  author   = {Jacob Hernandez Sanchez and Walid Amanhoud and Aude Billard and Mohamed Bouri},
  doi      = {10.1177/02783649231180366},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {475-503},
  title    = {Enabling four-arm laparoscopic surgery by controlling two robotic assistants via haptic foot interfaces},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rubik tables and object rearrangement. <em>The International
Journal of Robotics Research</em>, <em>42</em>(6), 459–472. (<a
href="https://doi.org/10.1177/02783649211059844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A great number of robotics applications demand the rearrangement of many mobile objects, for example, organizing products on store shelves, shuffling containers at shipping ports, reconfiguring fleets of mobile robots, and so on. To boost the efficiency/throughput in systems designed for solving these rearrangement problems, it is essential to minimize the number of atomic operations that are involved, for example, the pick-n-places of individual objects. However, this optimization task poses a rather difficult challenge due to the complex inter-dependency between the objects, especially when they are tightly packed together. In this work, in tackling the aforementioned challenges, we have developed a novel algorithmic tool, called Rubik Tables, that provides a clean abstraction of object rearrangement problems as the proxy problem of shuffling items stored in a table or lattice. In its basic form, a Rubik Table is an n × n table containing n 2 items. We show that the reconfiguration of items in such a Rubik Table can be achieved using at most n column and n row shuffles in the partially labeled setting, where each column (resp., row) shuffle may arbitrarily permute the items stored in a column (resp., row) of the table. When items are fully distinguishable, additional n shuffles are needed. Rubik Tables allow many generalizations, for example, adding an additional depth dimension or extending to higher dimensions. Using Rubik Table results, we have designed a first constant-factor optimal algorithm for stack rearrangement problems where items are stored in stacks, accessible only from the top. We show that, for nd items stored in n stacks of depth d each, using one empty stack as the swap space, O ( nd ) stack pop-push operations are sufficient for an arbitrary reconfiguration of the stacks where d ≤ n m / 2 for arbitrary fixed m &gt; 0. Rubik Table results also allow the development of constant-factor optimal solutions for solving multi-robot motion planning problems under extreme robot density. These algorithms based on Rubik Table results run in low-polynomial time.},
  archive  = {J},
  author   = {Mario Szegedy and Jingjin Yu},
  doi      = {10.1177/02783649211059844},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {459-472},
  title    = {Rubik tables and object rearrangement},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autogenerated manipulation primitives. <em>The International
Journal of Robotics Research</em>, <em>42</em>(6), 433–458. (<a
href="https://doi.org/10.1177/02783649231170897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The central theme in robotic manipulation is that of the robot interacting with the world through physical contact. We tend to describe that physical contact using specific words that capture the nature of the contact and the action, such as grasp, roll, pivot, push, pull, tilt, close, open etc. We refer to these situation-specific actions as manipulation primitives. Due to the nonlinear and nonsmooth nature of physical interaction, roboticists have devoted significant efforts towards studying individual manipulation primitives. However, studying individual primitives one by one is an inherently limited process, due engineering costs, overfitting to specific tasks, and lack of robustness to unforeseen variations. These limitations motivate the main contribution of this paper: a complete and general framework to autogenerate manipulation primitives. To do so, we develop the theory and computation of contact modes as a means to classify and enumerate manipulation primitives. The contact modes form a graph, specifically a lattice. Our algorithm to autogenerate manipulation primitives (AMP) performs graph-based optimization on the contact mode lattice and solves a linear program to generate each primitive. We designed several experiments to validate our approach. We benchmarked a wide range of contact scenarios and our pipeline’s runtime was consistently in the 10 s of milliseconds. In simulation, we planned manipulation sequences using AMP. In the real-world, we showcased the robustness of our approach to real-world modeling errors. We hope that our contributions will lead to more general and robust approaches for robotic manipulation.},
  archive  = {J},
  author   = {Eric Huang and Xianyi Cheng and Yuemin Mao and Arnav Gupta and Matthew T Mason},
  doi      = {10.1177/02783649231170897},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {433-458},
  title    = {Autogenerated manipulation primitives},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning to chronicle: Optimal policies for narrative
observation of unpredictable events. <em>The International Journal of
Robotics Research</em>, <em>42</em>(6), 412–432. (<a
href="https://doi.org/10.1177/02783649211069154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One important class of applications entails a robot scrutinizing, monitoring, or recording the evolution of an uncertain time-extended process. This sort of situation leads to an interesting family of active perception problems that can be cast as planning problems in which the robot is limited in what it sees and must, thus, choose what to pay attention to. The distinguishing characteristic of this setting is that the robot has influence over what it captures via its sensors, but exercises no causal authority over the process evolving in the world. As such, the robot’s objective is to observe the underlying process and to produce a “chronicle” of occurrent events, subject to a goal specification of the sorts of event sequences that may be of interest. This paper examines variants of such problems in which the robot aims to collect sets of observations to meet a rich specification of their sequential structure. We study this class of problems by modeling a stochastic process via a variant of a hidden Markov model and specify the event sequences of interest as a regular language, developing a vocabulary of “mutators” that enable sophisticated requirements to be expressed. Under different suppositions on the information gleaned about the event model, we formulate and solve different planning problems. The core underlying idea is the construction of a product between the event model and a specification automaton. Using this product, we compute a policy that minimizes the expected number of steps to reach a goal state. We introduce a general algorithm for this problem as well as several more efficient algorithms for important special cases. The paper reports and compares performance metrics by drawing on some small case studies analyzed in depth via simulation. Specifically, we study the effect of the robot’s observation model on the average time required for the robot to record a desired story. We also compare our algorithm with a baseline greedy algorithm, showing that our algorithm outperforms the greedy algorithm in terms of the average time to record a desired story. In addition, experiments show that the algorithms tailored to specialized variants of the problem are rather more efficient than the general algorithm.},
  archive  = {J},
  author   = {Hazhar Rahmani and Dylan A Shell and Jason M O’Kane},
  doi      = {10.1177/02783649211069154},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {412-432},
  title    = {Planning to chronicle: Optimal policies for narrative observation of unpredictable events},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reactivity and statefulness: Action-based sensors, plans,
and necessary state. <em>The International Journal of Robotics
Research</em>, <em>42</em>(6), 385–411. (<a
href="https://doi.org/10.1177/02783649221078874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Typically to a roboticist, a plan is the outcome of other work, a synthesized object that realizes ends defined by some problem; plans qua plans are seldom treated as first-class objects of study. Plans designate functionality: a plan can be viewed as defining a robot’s behavior throughout its execution. This informs and reveals many other aspects of the robot’s design, including: necessary sensors and action choices, history, state, task structure, and how to define progress. Interrogating sets of plans helps in comprehending the ways in which differing executions influence the interrelationships between these various aspects. Revisiting Erdmann’s theory of action-based sensors, a classical approach for characterizing fundamental information requirements, we show how plans (in their role of designating behavior) influence sensing requirements. Using an algorithm for enumerating plans, we examine how some plans for which no action-based sensor exists can be transformed into sets of sensors through the identification and handling of features that preclude the existence of action-based sensors. We are not aware of those obstructing features having been previously identified. Action-based sensors may be treated as standalone reactive plans; we relate them to the set of all possible plans through a lattice structure. This lattice reveals a boundary between plans with action-based sensors and those without. Some plans, specifically those that are not reactive plans and require some notion of internal state, can never have associated action-based sensors. Even so, action-based sensors can serve as a framework to explore and interpret how such plans make use of state.},
  archive  = {J},
  author   = {Grace McFassel and Dylan A Shell},
  doi      = {10.1177/02783649221078874},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {385-411},
  title    = {Reactivity and statefulness: Action-based sensors, plans, and necessary state},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally connected interrelated network: A forward
propagation primitive. <em>The International Journal of Robotics
Research</em>, <em>42</em>(6), 371–384. (<a
href="https://doi.org/10.1177/02783649221093092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {End-to-end learning for planning is a promising approach for finding good robot strategies in situations where the state transition, observation, and reward functions are initially unknown. Many neural network architectures for this approach have shown positive results. Across these networks, seemingly small components have been used repeatedly in different architectures, which means improving the efficiency of these components has great potential to improve the overall performance of the network. This paper aims to improve one such component: The forward propagation module. In particular, we propose Locally Connected Interrelated Network (LCI-Net) – a novel type of locally connected layer with unshared but interrelated weights – to improve the efficiency of learning stochastic transition models for planning and propagating information via the learned transition models. LCI-Net is a small differentiable neural network module that can be plugged into various existing architectures. For evaluation purposes, we apply LCI-Net to VIN and QMDP-Net. VIN is an end-to-end neural network for solving Markov Decision Processes (MDPs) whose transition and reward functions are initially unknown, while QMDP-Net is its counterpart for the Partially Observable Markov Decision Process (POMDP) whose transition, observation, and reward functions are initially unknown. Simulation tests on benchmark problems involving 2D and 3D navigation and grasping indicate promising results: Changing only the forward propagation module alone with LCI-Net improves VIN’s and QMDP-Net generalisation capability by more than 3× and 10×, respectively.},
  archive  = {J},
  author   = {Nicholas Collins and Hanna Kurniawati},
  doi      = {10.1177/02783649221093092},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {371-384},
  title    = {Locally connected interrelated network: A forward propagation primitive},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Backpropagation through signal temporal logic
specifications: Infusing logical structure into gradient-based methods.
<em>The International Journal of Robotics Research</em>, <em>42</em>(6),
356–370. (<a href="https://doi.org/10.1177/02783649221082115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a technique, named STLCG, to compute the quantitative semantics of Signal Temporal Logic (STL) formulas using computation graphs. STLCG provides a platform which enables the incorporation of logical specifications into robotics problems that benefit from gradient-based solutions. Specifically, STL is a powerful and expressive formal language that can specify spatial and temporal properties of signals generated by both continuous and hybrid systems. The quantitative semantics of STL provide a robustness metric, that is, how much a signal satisfies or violates an STL specification. In this work, we devise a systematic methodology for translating STL robustness formulas into computation graphs. With this representation, and by leveraging off-the-shelf automatic differentiation tools, we are able to efficiently backpropagate through STL robustness formulas and hence enable a natural and easy-to-use integration of STL specifications with many gradient-based approaches used in robotics. Through a number of examples stemming from various robotics applications, we demonstrate that STLCG is versatile, computationally efficient, and capable of incorporating human-domain knowledge into the problem formulation.},
  archive  = {J},
  author   = {Karen Leung and Nikos Aréchiga and Marco Pavone},
  doi      = {10.1177/02783649221082115},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {356-370},
  title    = {Backpropagation through signal temporal logic specifications: Infusing logical structure into gradient-based methods},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid control for combining model-based and model-free
reinforcement learning. <em>The International Journal of Robotics
Research</em>, <em>42</em>(6), 337–355. (<a
href="https://doi.org/10.1177/02783649221083331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop an approach to improve the learning capabilities of robotic systems by combining learned predictive models with experience-based state-action policy mappings. Predictive models provide an understanding of the task and the dynamics, while experience-based (model-free) policy mappings encode favorable actions that override planned actions. We refer to our approach of systematically combining model-based and model-free learning methods as hybrid learning. Our approach efficiently learns motor skills and improves the performance of predictive models and experience-based policies. Moreover, our approach enables policies (both model-based and model-free) to be updated using any off-policy reinforcement learning method. We derive a deterministic method of hybrid learning by optimally switching between learning modalities. We adapt our method to a stochastic variation that relaxes some of the key assumptions in the original derivation. Our deterministic and stochastic variations are tested on a variety of robot control benchmark tasks in simulation as well as a hardware manipulation task. We extend our approach for use with imitation learning methods, where experience is provided through demonstrations, and we test the expanded capability with a real-world pick-and-place task. The results show that our method is capable of improving the performance and sample efficiency of learning motor skills in a variety of experimental domains.},
  archive  = {J},
  author   = {Allison Pinosky and Ian Abraham and Alexander Broad and Brenna Argall and Todd D Murphey},
  doi      = {10.1177/02783649221083331},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {337-355},
  title    = {Hybrid control for combining model-based and model-free reinforcement learning},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The before, during, and after of multi-robot deadlock.
<em>The International Journal of Robotics Research</em>, <em>42</em>(6),
317–336. (<a href="https://doi.org/10.1177/02783649221074718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Collision avoidance for multi-robot systems is a well-studied problem. Recently, control barrier functions (CBFs) have been proposed for synthesizing controllers that guarantee collision avoidance and goal stabilization for multiple robots. However, it has been noted that reactive control synthesis methods (such as CBFs) are prone to deadlock , an equilibrium of system dynamics that causes the robots to stall before reaching their goals. In this paper, we analyze the closed-loop dynamics of robots using CBFs, to characterize controller parameters, initial conditions, and goal locations that invariably lead the system to deadlock. Using tools from duality theory, we derive geometric properties of robot configurations of an N robot system once it is in deadlock and we justify them using the mechanics interpretation of KKT conditions. Our key deductions are that (1) system deadlock is characterized by a force equilibrium on robots and (2) deadlock occurs to ensure safety when safety is at the brink of being violated. These deductions allow us to interpret deadlock as a subset of the state space, and we show that this set is non-empty and located on the boundary of the safe set. By exploiting these properties, we analyze the number of admissible robot configurations in deadlock and develop a provably correct decentralized algorithm for deadlock resolution to safely deliver the robots to their goals. This algorithm is validated in simulations as well as experimentally on Khepera-IV robots For an interactive version of this paper, please visit https://arxiv.org/abs/2206.01781 .},
  archive  = {J},
  author   = {Jaskaran Grover and Changliu Liu and Katia Sycara},
  doi      = {10.1177/02783649221074718},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {317-336},
  title    = {The before, during, and after of multi-robot deadlock},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selected papers from WAFR2020. <em>The International Journal
of Robotics Research</em>, <em>42</em>(6), 315–316. (<a
href="https://doi.org/10.1177/02783649231187014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jingjin Yu and Ming C. Lin},
  doi     = {10.1177/02783649231187014},
  journal = {The International Journal of Robotics Research},
  month   = {5},
  number  = {6},
  pages   = {315-316},
  title   = {Selected papers from WAFR2020},
  volume  = {42},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IJRR: A look back and a look forward. <em>The International
Journal of Robotics Research</em>, <em>42</em>(6), 313–314. (<a
href="https://doi.org/10.1177/02783649231187463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Antonio Bicchi},
  doi     = {10.1177/02783649231187463},
  journal = {The International Journal of Robotics Research},
  month   = {5},
  number  = {6},
  pages   = {313-314},
  title   = {IJRR: A look back and a look forward},
  volume  = {42},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The blindfolded traveler’s problem: A search framework for
motion planning with contact estimates. <em>The International Journal of
Robotics Research</em>, <em>42</em>(4-5), 289–309. (<a
href="https://doi.org/10.1177/02783649231170893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We address the problem of robot motion planning under uncertainty where the only observations are through contact with the environment. Such problems are typically solved by planning optimistically assuming unknown space is free, moving along the planned path and re-planning if the robot collides. However this approach can be very inefficient, leading to many unnecessary collisions and unproductive motion. We propose a new formulation, the Blindfolded Traveler’s Problem (BTP), for planning on a graph containing edges with unknown validity, with true validity observed only through attempted traversal by the robot. The solution to a BTP is a policy indicating the next edge to attempt given previous observations and an initial belief. We prove that BTP is NP-complete and show that exact modeling of the belief is intractable, therefore we present several approximation-based policies and beliefs. For the policy we propose graph search with edge weights augmented by the probability of collision. For the belief representation we propose a weighted Mixture of Experts of Collision Hypothesis Sets and a Manifold Particle Filter. Empirical evaluation in simulation and on a real robot arm shows that our proposed approach vastly outperforms several baselines as well as a previous approach that does not employ the BTP framework.},
  archive  = {J},
  author   = {Brad Saund and Sanjiban Choudhury and Siddhartha Srinivasa and Dmitry Berenson},
  doi      = {10.1177/02783649231170893},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {289-309},
  title    = {The blindfolded traveler’s problem: A search framework for motion planning with contact estimates},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic encoding and repair of reactive high-level tasks
with learned abstract representations. <em>The International Journal of
Robotics Research</em>, <em>42</em>(4-5), 263–288. (<a
href="https://doi.org/10.1177/02783649231167207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a framework for the automatic encoding and repair of high-level tasks. Given a set of skills a robot can perform, our approach first abstracts sensor data into symbols and then automatically encodes the robot’s capabilities in Linear Temporal Logic (LTL). Using this encoding, a user can specify reactive high-level tasks, for which we can automatically synthesize a strategy that executes on the robot, if the task is feasible. If a task is not feasible given the robot’s capabilities, we present two methods, one enumeration-based and one synthesis-based, for automatically suggesting additional skills for the robot or modifications to existing skills that would make the task feasible. We demonstrate our framework on a Baxter robot manipulating blocks on a table, a Baxter robot manipulating plates on a table, and a Kinova arm manipulating vials, with multiple sensor modalities, including raw images.},
  archive  = {J},
  author   = {Adam Pacheck and Steven James and George Konidaris and Hadas Kress-Gazit},
  doi      = {10.1177/02783649231167207},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {263-288},
  title    = {Automatic encoding and repair of reactive high-level tasks with learned abstract representations},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Certified grasping. <em>The International Journal of
Robotics Research</em>, <em>42</em>(4-5), 249–262. (<a
href="https://doi.org/10.1177/02783649231155952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper studies the robustness of grasping in the frictionless plane from a geometric perspective. By treating grasping as a process that shapes the free-space object over time, we define three types of certificates to guarantee success of a grasp: (a) invariance under an initial set, (b) convergence toward a goal grasp, and (c) observability over the final object pose. We develop convex-combinatorial models for each of these certificates, which can be expressed as simple semi-algebraic relations under mild-modeling assumptions, such as point-fingers and frictionless contact. By leveraging these models to synthesize certificates, we optimize certifiable grasps of planar objects composed as a union of convex polygons, using manipulators described as point-fingers. We validate this approach in simulations by grasping random polygons, and with real sensorless grasps of several objects.},
  archive  = {J},
  author   = {Bernardo Aceituno-Cabezas and Jose Ballester and Alberto Rodriguez},
  doi      = {10.1177/02783649231155952},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {249-262},
  title    = {Certified grasping},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Versatile articulated aerial robot DRAGON: Aerial
manipulation and grasping by vectorable thrust control. <em>The
International Journal of Robotics Research</em>, <em>42</em>(4-5),
214–248. (<a href="https://doi.org/10.1177/02783649221112446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Various state-of-the-art works have achieved aerial manipulation and grasping by attaching additional manipulator to aerial robots. However, such a coupled platform has limitations with respect to the interaction force and mobility. In this paper, we present the successful implementation of aerial manipulation and grasping by a novel articulated aerial robot called DRAGON, in which a vectorable rotor unit is embedded in each link. The key to performing stable manipulation and grasping in the air is the usage of rotor vectoring apparatus having two degrees-of-freedom. First, a comprehensive flight control methodology for aerial transformation using the vectorable thrust force is developed with the consideration of the dynamics of vectoring actuators. This proposed control method can suppress the oscillation due to the dynamics of vectoring actuators and also allow the integration with external and internal wrenches for object manipulation and grasping. Second, an online thrust-level planning method for bimanual object grasping using the two ends of this articulated model is presented. The proposed grasping style is unique in that the vectorable thrust force is used as the internal wrench instead of the joint torque. Finally, we show the experimental results of evaluation on the proposed control and planning methods for object manipulation and grasping.},
  archive  = {J},
  author   = {Moju Zhao and Kei Okada and Masayuki Inaba},
  doi      = {10.1177/02783649221112446},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {214-248},
  title    = {Versatile articulated aerial robot DRAGON: Aerial manipulation and grasping by vectorable thrust control},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel monte carlo for solving POMDPs on-line. <em>The
International Journal of Robotics Research</em>, <em>42</em>(4-5),
196–213. (<a href="https://doi.org/10.1177/02783649221093658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Planning under partial observability is essential for autonomous robots. A principled way to address such planning problems is the Partially Observable Markov Decision Process (POMDP). Although solving POMDPs is computationally intractable, substantial advancements have been achieved in developing approximate POMDP solvers in the past two decades. However, computing robust solutions for systems with complex dynamics remains challenging. Most on-line solvers rely on a large number of forward simulations and standard Monte Carlo methods to compute the expected outcomes of actions the robot can perform. For systems with complex dynamics, for example, those with non-linear dynamics that admit no closed-form solution, even a single forward simulation can be prohibitively expensive. Of course, this issue exacerbates for problems with long planning horizons. This paper aims to alleviate the above difficulty. To this end, we propose a new on-line POMDP solver, called Multilevel POMDP Planner (MLPP), that combines the commonly known Monte-Carlo-Tree-Search with the concept of Multilevel Monte Carlo to speed up our capability in generating approximately optimal solutions for POMDPs with complex dynamics. Experiments on four different problems involving torque control, navigation and grasping indicate that MLPP substantially outperforms state-of-the-art POMDP solvers.},
  archive  = {J},
  author   = {Marcus Hoerger and Hanna Kurniawati and Alberto Elfes},
  doi      = {10.1177/02783649221093658},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {196-213},
  title    = {Multilevel monte carlo for solving POMDPs on-line},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint search of optimal topology and trajectory for planar
linkages. <em>The International Journal of Robotics Research</em>,
<em>42</em>(4-5), 176–195. (<a
href="https://doi.org/10.1177/02783649211069156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an algorithm to compute planar linkage topology and geometry, given a user-specified end-effector trajectory. Planar linkage structures convert rotational or prismatic motions of a single actuator into an arbitrarily complex periodic motion, which is an important component when building low-cost, modular robots, mechanical toys, and foldable structures in our daily lives (chairs, bikes, and shelves). The design of such structures requires trial and error even for experienced engineers. Our research provides semi-automatic methods for exploring novel designs given high-level specifications and constraints. We formulate this problem as a non-smooth numerical optimization with quadratic objective functions and non-convex quadratic constraints involving mixed-integer decision variables (MIQCQP). We propose and compare three approximate algorithms to solve this problem: mixed-integer conic-programming (MICP), mixed-integer nonlinear programming (MINLP), and simulated annealing (SA). We evaluated these algorithms searching for planar linkages involving 10 − 14 rigid links. Our results show that the best performance can be achieved by combining MICP and MINLP, leading to a hybrid algorithm capable of finding the planar linkages within a couple of hours on a desktop machine, which significantly outperforms the SA baseline in terms of optimality. We highlight the effectiveness of our optimized planar linkages by using them as legs of a walking robot.},
  archive  = {J},
  author   = {Zherong Pan and Min Liu and Xifeng Gao and Dinesh Manocha},
  doi      = {10.1177/02783649211069156},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {176-195},
  title    = {Joint search of optimal topology and trajectory for planar linkages},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotically optimal inspection planning via efficient
near-optimal search on sampled roadmaps. <em>The International Journal
of Robotics Research</em>, <em>42</em>(4-5), 150–175. (<a
href="https://doi.org/10.1177/02783649231171646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inspection planning, the task of planning motions for a robot that enable it to inspect a set of points of interest, has applications in domains such as industrial, field, and medical robotics. Inspection planning can be computationally challenging, as the search space over motion plans grows exponentially with the number of points of interest to inspect. We propose a novel method, Incremental Random Inspection-roadmap Search (IRIS), that computes inspection plans whose length and set of successfully inspected points asymptotically converge to those of an optimal inspection plan. IRIS incrementally densifies a motion-planning roadmap using a sampling-based algorithm and performs efficient near-optimal graph search over the resulting roadmap as it is generated. We prove the resulting algorithm is asymptotically optimal under very general assumptions about the robot and the environment. We demonstrate IRIS’s efficacy on a simulated inspection task with a planar five DOF manipulator, on a simulated bridge inspection task with an Unmanned Aerial Vehicle (UAV), and on a medical endoscopic inspection task for a continuum parallel surgical robot in cluttered human anatomy. In all these systems IRIS computes higher-quality inspection plans orders of magnitudes faster than a prior state-of-the-art method.},
  archive  = {J},
  author   = {Mengyu Fu and Alan Kuntz and Oren Salzman and Ron Alterovitz},
  doi      = {10.1177/02783649231171646},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4-5},
  pages    = {150-175},
  title    = {Asymptotically optimal inspection planning via efficient near-optimal search on sampled roadmaps},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selected papers from ISRR’2019. <em>The International
Journal of Robotics Research</em>, <em>42</em>(4-5), 149. (<a
href="https://doi.org/10.1177/02783649231180249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Tamim Asfour and Jaeheung Park and Eiichi Yoshida},
  doi     = {10.1177/02783649231180249},
  journal = {The International Journal of Robotics Research},
  month   = {4},
  number  = {4-5},
  pages   = {149},
  title   = {Selected papers from ISRR&#39;2019},
  volume  = {42},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian controller fusion: Leveraging control priors in
deep reinforcement learning for robotics. <em>The International Journal
of Robotics Research</em>, <em>42</em>(3), 123–146. (<a
href="https://doi.org/10.1177/02783649231167210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. We study BCF on two real-world robotics tasks involving navigation in a vast and long-horizon environment, and a complex reaching task that involves manipulability maximisation. For both these domains, simple handcrafted controllers exist that can solve the task at hand in a risk-averse manner but do not necessarily exhibit the optimal solution given limitations in analytical modelling, controller miscalibration and task variation. As exploration is naturally guided by the prior in the early stages of training, BCF accelerates learning, while substantially improving beyond the performance of the control prior, as the policy gains more experience. More importantly, given the risk-aversity of the control prior, BCF ensures safe exploration and deployment, where the control prior naturally dominates the action distribution in states unknown to the policy. We additionally show BCF’s applicability to the zero-shot sim-to-real setting and its ability to deal with out-of-distribution states in the real world. BCF is a promising approach towards combining the complementary strengths of deep RL and traditional robotic control, surpassing what either can achieve independently. The code and supplementary video material are made publicly available at https://krishanrana.github.io/bcf .},
  archive  = {J},
  author   = {Krishan Rana and Vibhavari Dasagi and Jesse Haviland and Ben Talbot and Michael Milford and Niko Sünderhauf},
  doi      = {10.1177/02783649231167210},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {123-146},
  title    = {Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Passive and active acoustic sensing for soft pneumatic
actuators. <em>The International Journal of Robotics Research</em>,
<em>42</em>(3), 108–122. (<a
href="https://doi.org/10.1177/02783649231168954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a sensorization method for soft pneumatic actuators that uses an embedded microphone and speaker to measure different actuator properties. The physical state of the actuator determines the specific modulation of sound as it travels through the structure. Using simple machine learning, we create a computational sensor that infers the corresponding state from sound recordings. We demonstrate the acoustic sensor on a soft pneumatic continuum actuator and use it to measure contact locations, contact forces, object materials, actuator inflation, and actuator temperature. We show that the sensor is reliable (average classification rate for six contact locations of 93%), precise (mean spatial accuracy of 3.7 mm), and robust against common disturbances like background noise. Finally, we compare different sounds and learning methods and achieve best results with 20 ms of white noise and a support vector classifier as the sensor model.},
  archive  = {J},
  author   = {Vincent Wall and Gabriel Zöller and Oliver Brock},
  doi      = {10.1177/02783649231168954},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {108-122},
  title    = {Passive and active acoustic sensing for soft pneumatic actuators},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining physics and deep learning to learn continuous-time
dynamics models. <em>The International Journal of Robotics
Research</em>, <em>42</em>(3), 83–107. (<a
href="https://doi.org/10.1177/02783649231169492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning has been widely used within learning algorithms for robotics. One disadvantage of deep networks is that these networks are black-box representations. Therefore, the learned approximations ignore the existing knowledge of physics or robotics. Especially for learning dynamics models, these black-box models are not desirable as the underlying principles are well understood and the standard deep networks can learn dynamics that violate these principles. To learn dynamics models with deep networks that guarantee physically plausible dynamics, we introduce physics-inspired deep networks that combine first principles from physics with deep learning. We incorporate Lagrangian mechanics within the model learning such that all approximated models adhere to the laws of physics and conserve energy. Deep Lagrangian Networks (DeLaN) parametrize the system energy using two networks. The parameters are obtained by minimizing the squared residual of the Euler–Lagrange differential equation. Therefore, the resulting model does not require specific knowledge of the individual system, is interpretable, and can be used as a forward, inverse, and energy model. Previously these properties were only obtained when using system identification techniques that require knowledge of the kinematic structure. We apply DeLaN to learning dynamics models and apply these models to control simulated and physical rigid body systems. The results show that the proposed approach obtains dynamics models that can be applied to physical systems for real-time control. Compared to standard deep networks, the physics-inspired models learn better models and capture the underlying structure of the dynamics.},
  archive  = {J},
  author   = {Michael Lutter and Jan Peters},
  doi      = {10.1177/02783649231169492},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {83-107},
  title    = {Combining physics and deep learning to learn continuous-time dynamics models},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Receding horizon navigation and target tracking for aerial
detection of transient radioactivity. <em>The International Journal of
Robotics Research</em>, <em>42</em>(3), 66–82. (<a
href="https://doi.org/10.1177/02783649231169803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper presents a receding horizon planning and control strategy for quadrotor-type micro aerial vehicle ( mav )s to navigate reactively and intercept a moving target in a cluttered unknown and dynamic environment. Leveraging a lightweight short-range sensor that generates a point-cloud within a relatively narrow and short field of view ( fov ), and an ssd -MobileNet based Deep neural network running on board the mav , the proposed motion planning and control strategy produces safe and dynamically feasible mav trajectories within the sensor fov , which the vehicle uses to autonomously navigate, pursue, and intercept its moving target. This task is completed without reliance on a global planner or prior information about the environment or the moving target. The effectiveness of the reported planner is demonstrated numerically and experimentally in cluttered indoor and outdoor environments featuring maximum speeds of up to 4.5–5 m/s.},
  archive  = {J},
  author   = {Indrajeet Yadav and Michael Sebok and Herbert G. Tanner},
  doi      = {10.1177/02783649231169803},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {66-82},
  title    = {Receding horizon navigation and target tracking for aerial detection of transient radioactivity},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modelify: An approach to incrementally build 3D object
models for map completion. <em>The International Journal of Robotics
Research</em>, <em>42</em>(3), 45–65. (<a
href="https://doi.org/10.1177/02783649231166977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The capabilities of discovering new knowledge and updating the previously acquired one are crucial for deploying autonomous robots in unknown and changing environments. Spatial and objectness concepts are at the basis of several robotic functionalities and are part of the intuitive understanding of the physical world for us humans. In this paper, we propose a method, which we call Modelify, to incrementally map the environment at the level of objects in a consistent manner. We follow an approach where no prior knowledge of the environment is required. The only assumption we make is that objects in the environment are separated by concave boundaries. The approach works on an RGB-D camera stream, where object-like segments are extracted and stored in an incremental database. Segment description and matching are performed by exploiting 2D and 3D information, allowing to build a graph of all segments. Finally, a matching score guides a Markov clustering algorithm to merge segments, thus completing object representations. Our approach allows creating single (merged) instances of repeating objects, objects that were observed from different viewpoints, and objects that were observed in previous mapping sessions. Thanks to our matching and merging strategies this also works with only partially overlapping segments. We perform evaluations on indoor and outdoor datasets recorded with different RGB-D sensors and show the benefit of using a clustering method to form merge candidates and keypoints detected in both 2D and 3D. Our new method shows better results than previous approaches while being significantly faster. A newly recorded dataset and the source code are released with this publication.},
  archive  = {J},
  author   = {Fadri Furrer and Tonci Novkovic and Marius Fehr and Margarita Grinvald and Cesar Cadena and Juan Nieto and Roland Siegwart},
  doi      = {10.1177/02783649231166977},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {45-65},
  title    = {Modelify: An approach to incrementally build 3D object models for map completion},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boreas: A multi-season autonomous driving dataset. <em>The
International Journal of Robotics Research</em>, <em>42</em>(1-2),
33–42. (<a href="https://doi.org/10.1177/02783649231160195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Boreas dataset was collected by driving a repeated route over the course of 1 year, resulting in stark seasonal variations and adverse weather conditions such as rain and falling snow. In total, the Boreas dataset includes over 350 km of driving data featuring a 128-channel Velodyne Alpha-Prime lidar, a 360° Navtech CIR304-H scanning radar, a 5MP FLIR Blackfly S camera, and centimetre-accurate post-processed ground truth poses. Our dataset will support live leaderboards for odometry, metric localization, and 3D object detection. The dataset and development kit are available at boreas.utias.utoronto.ca.},
  archive  = {J},
  author   = {Keenan Burnett and David J Yoon and Yuchen Wu and Andrew Z Li and Haowei Zhang and Shichen Lu and Jingxing Qian and Wei-Kang Tseng and Andrew Lambert and Keith YK Leung and Angela P Schoellig and Timothy D Barfoot},
  doi      = {10.1177/02783649231160195},
  journal  = {The International Journal of Robotics Research},
  month    = {1-2},
  number   = {1-2},
  pages    = {33-42},
  title    = {Boreas: A multi-season autonomous driving dataset},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MassMIND: Massachusetts maritime INfrared dataset. <em>The
International Journal of Robotics Research</em>, <em>42</em>(1-2),
21–32. (<a href="https://doi.org/10.1177/02783649231153020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advances in deep learning technology have triggered radical progress in the autonomy of ground vehicles. Marine coastal Autonomous Surface Vehicles (ASVs) that are regularly used for surveillance, monitoring, and other routine tasks can benefit from this autonomy. Long haul deep sea transportation activities are additional opportunities. These two use cases present very different terrains—the first being coastal waters—with many obstacles, structures, and human presence while the latter is mostly devoid of such obstacles. Variations in environmental conditions are common to both terrains. Robust labeled datasets mapping such terrains are crucial in improving the situational awareness that can drive autonomy. However, there are only limited such maritime datasets available and these primarily consist of optical images. Although, long wave infrared (LWIR) is a strong complement to the optical spectrum that helps in extreme light conditions, a labeled public dataset with LWIR images does not currently exist. In this paper, we fill this gap by presenting a labeled dataset of over 2900 LWIR segmented images captured in coastal maritime environment over a period of 2 years. The images are labeled using instance segmentation and classified into seven categories—sky, water, obstacle, living obstacle, bridge, self, and background. We also evaluate this dataset across three deep learning architectures (UNet, PSPNet, DeepLabv3) and provide detailed analysis of its efficacy. While the dataset focuses on the coastal terrain, it can equally help deep sea use cases. Such terrain would have less traffic, and the classifier trained on cluttered environment would be able to handle sparse scenes effectively. We share this dataset with the research community with the hope that it spurs new scene understanding capabilities in the maritime environment.},
  archive  = {J},
  author   = {Shailesh Nirgudkar and Michael DeFilippo and Michael Sacarny and Michael Benjamin and Paul Robinette},
  doi      = {10.1177/02783649231153020},
  journal  = {The International Journal of Robotics Research},
  month    = {1-2},
  number   = {1-2},
  pages    = {21-32},
  title    = {MassMIND: Massachusetts maritime INfrared dataset},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alternating direction method of multipliers-based
distributed control for distributed manipulation by shaping physical
force fields. <em>The International Journal of Robotics Research</em>,
<em>42</em>(1-2), 3–20. (<a
href="https://doi.org/10.1177/02783649231153958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes an algorithm for decomposing and possibly distributing an optimization problem that naturally emerges in distributed manipulation by shaping physical force fields through actuators distributed in space (arrays of actuators). One or several manipulated objects located in this field can “feel the force” and move simultaneously and independently. The control system has to produce commands for all actuators so that desired forces are developed at several prescribed places. This can be formulated as an optimization problem that has to be solved in every sampling period. Exploiting the structure of the optimization problem is crucial for platforms with many actuators and many manipulated objects, hence the goal of decomposing the huge optimization problem into several subproblems. Furthermore, if the platform is composed of interconnected actuator modules with computational capabilities, the decomposition can give guidance for the distribution of the computation to the modules. We propose an algorithm for decomposing/distributing the optimization problem using Alternating Direction Method of Multipliers (ADMM). The proposed algorithm is shown to converge to modest accuracy for various distributed platforms in a few iterations. We demonstrate our algorithm through numerical experiments corresponding to three physical experimental platforms for distributed manipulation using electric, magnetic, and pressure fields. Furthermore, we deploy and test it on real experimental platforms for distributed manipulation using an array of solenoids and ultrasonic transducers.},
  archive  = {J},
  author   = {Martin Gurtner and Jiří Zemánek and Zdeněk Hurák},
  doi      = {10.1177/02783649231153958},
  journal  = {The International Journal of Robotics Research},
  month    = {1-2},
  number   = {1-2},
  pages    = {3-20},
  title    = {Alternating direction method of multipliers-based distributed control for distributed manipulation by shaping physical force fields},
  volume   = {42},
  year     = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
