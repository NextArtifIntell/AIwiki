<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DINT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dint---49">DINT - 49</h2>
<ul>
<li><details>
<summary>
(2023). A civil aviation customer service ontology and its
applications. <em>DINT</em>, <em>5</em>(4), 1063–1081. (<a
href="https://doi.org/10.1162/dint_a_00237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of developing the C919 large aircraft customer service intelligence system, we find that heterogeneous and incomplete data cause the inefficient and inaccurate decision making. Thus, to solve this problem, we propose to introduce the idea of ontology modeling and reasoning into competitive intelligence system building in this paper. We first present the building principles and methods of the civil aviation customer service ontology. We then define the classes and properties to contribute a real-world civil aviation customer service ontology, which is published on the Web ( http://www.openkg.cn/dataset/cacso ). We finally design SWRL rules corresponding to different intelligence analysis targets to support reasoning in our designed competitive intelligence system.},
  archive      = {J_DINT},
  author       = {Lv, Meixiang and Cao, Xudong and Wu, Tianxing and Li, Yuehua},
  doi          = {10.1162/dint_a_00237},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {1063-1081},
  shortjournal = {Data Intell.},
  title        = {A civil aviation customer service ontology and its applications},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slide-detect: An accurate deep learning diagnosis of lung
infiltration. <em>DINT</em>, <em>5</em>(4), 1048–1062. (<a
href="https://doi.org/10.1162/dint_a_00233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung infiltration is a non-communicable condition where materials with higher density than air exist in the parenchyma tissue of the lungs. Lung infiltration can be hard to be detected in an X-ray scan even for a radiologist, especially at the early stages making it a leading cause of death. In response, several deep learning approaches have been evolved to address this problem. This paper proposes the Slide-Detect technique which is a Deep Neural Networks (DNN) model based on Convolutional Neural Networks (CNNs) that is trained to diagnose lung infiltration with Area Under Curve (AUC) up to 91.47\%, accuracy of 93.85\% and relatively low computational resources.},
  archive      = {J_DINT},
  author       = {Mohamed, Ahmed E. and Fayek, Magda B. and Farouk, Mona},
  doi          = {10.1162/dint_a_00233},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {1048-1062},
  shortjournal = {Data Intell.},
  title        = {Slide-detect: An accurate deep learning diagnosis of lung infiltration},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of medical image detection technology based on
deep learning in pneumoconiosis diagnosis. <em>DINT</em>, <em>5</em>(4),
1033–1047. (<a href="https://doi.org/10.1162/dint_a_00228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumoconiosis is a disease characterized by pulmonary tissue deposition caused by dust exposure in the workplace. In China, due to the large number and wide distribution of pneumoconiosis patients, there is a high demand for the case data of lung biopsy during the diagnosis of pneumoconiosis. This text studied the application of medical image detection technology in pneumoconiosis diagnosis based on deep learning (DL). A medical image detection and convolution neural network (CNN) based on DL was analyzed, and the application of DL medical image technology in pneumoconiosis diagnosis was researched. The experimental results in this paper showed that in the last round of testing, the accuracy of ResNet model including deconvolution structure reached 95.2\%. The area under curve (AUC) value of the working characteristics of the subject is 0.987. The sensitivity was 99.66\%, and the specificity was 88.61\%. The non staging diagnosis of pneumoconiosis improved the diagnostic sensitivity while ensuring high specificity. At the same time, Delong test method was used to conduct AUC analysis on the three models, and the results showed that model C was more effective than model A and model B. There is no significant difference between model A and model B, and there is no significant difference in diagnostic efficiency. In a word, the diagnosis of the model has high sensitivity and low probability of missed diagnosis, which can greatly reduce the working pressure of diagnostic doctors and effectively improve the efficiency of diagnosis.},
  archive      = {J_DINT},
  author       = {Peng, Shengguang},
  doi          = {10.1162/dint_a_00228},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {1033-1047},
  shortjournal = {Data Intell.},
  title        = {Application of medical image detection technology based on deep learning in pneumoconiosis diagnosis},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computer-aided detection of tuberculosis from
microbiological and radiographic images. <em>DINT</em>, <em>5</em>(4),
1008–1032. (<a href="https://doi.org/10.1162/dint_a_00198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis caused by Mycobacterium tuberculosis have been a major challenge for medical and healthcare sectors in many underdeveloped countries with limited diagnosis tools. Tuberculosis can be detected from microscopic slides and chest X-ray but as a result of the high cases of tuberculosis, this method can be tedious for both Microbiologists and Radiologists and can lead to miss-diagnosis. These challenges can be solved by employing Computer-Aided Detection (CAD)via AI-driven models which learn features based on convolution and result in an output with high accuracy. In this paper, we described automated discrimination of X-ray and microscope slide images into tuberculosis and non-tuberculosis cases using pretrained AlexNet Models. The study employed Chest X-ray dataset made available on Kaggle repository and microscopic slide images from both Near East University Hospital and Kaggle repository. For classification of tuberculosis using microscopic slide images, the model achieved 90.56\% accuracy, 97.78\% sensitivity and 83.33\% specificity for 70: 30 splits. For classification of tuberculosis using X-ray images, the model achieved 93.89\% accuracy, 96.67\% sensitivity and 91.11\% specificity for 70:30 splits. Our result is in line with the notion that CNN models can be used for classifying medical images with higher accuracy and precision.},
  archive      = {J_DINT},
  author       = {Ibrahim, Abdullahi Umar and Kibarer, Ayse Gunnay and Al-Turjman, Fadi},
  doi          = {10.1162/dint_a_00198},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {1008-1032},
  shortjournal = {Data Intell.},
  title        = {Computer-aided detection of tuberculosis from microbiological and radiographic images},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation index system of green public open space based on
internet of things and mental health. <em>DINT</em>, <em>5</em>(4),
990–1007. (<a href="https://doi.org/10.1162/dint_a_00219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of the IoT era, wireless sensor networks will be more and more widely used. In addition to collecting, transmitting and processing simple data such as humidity, temperature and density of the dome, they can also provide multimedia information services such as video and images. It enables more comprehensive and accurate environmental monitoring. Therefore, MSDs have a huge demand in military, daily, forestry, biomedicine and other fields. The intensive city model has obvious advantages in meeting people&#39;s diverse needs and comfortable life. Most obviously, it speeds up the rhythm of life for residents, thereby increasing efficiency and saving time. Starting from this aspect, this paper conducts a research on the evaluation index system of public built on the following areas of open space IoT and mental health. In this paper, the GRNN neural network model is constructed, the mean condition is calculated, the density function can be estimated, the network output, and the schematic diagram of the generalized regression neural network is improved. Using the established system, the index in 2018 is selected as the base year, and after transformation, the standardized values of the past years are formed, which are substituted into the cells to form different matrices. The value of each cell is counted to obtain the subsystem coordination degree, and the global coordination degree is obtained through calculation. The evaluation results of ecological civilization construction and development in 2018 and 2019, 2020 and 2021 were compared. The experimental data shows that compared with 2018, economic development will change from 1 to 2.000, social harmony will change from 1 to 2.480, ecological health will decrease to 0.850, environmental friendliness will decrease to 0.750, and comprehensive evaluation will decrease to 0.513. This shows that while the economy is developing this year, the construction of ecological civilization has been gradually carried out, and good results have been achieved. This reflects the effectiveness of the system. The subject of the evaluation index system of green public open space based on the Internet of Things and mental health has been well completed.},
  archive      = {J_DINT},
  author       = {Li, Jiexu and binti Abdul Aziz, Faziawati and Zhang, Ning},
  doi          = {10.1162/dint_a_00219},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {990-1007},
  shortjournal = {Data Intell.},
  title        = {Evaluation index system of green public open space based on internet of things and mental health},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving extraction of chinese open relations using
pre-trained language model and knowledge enhancement. <em>DINT</em>,
<em>5</em>(4), 962–989. (<a
href="https://doi.org/10.1162/dint_a_00227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open Relation Extraction (ORE) is a task of extracting semantic relations from a text document. Current ORE systems have significantly improved their efficiency in obtaining Chinese relations, when compared with conventional systems which heavily depend on feature engineering or syntactic parsing. However, the ORE systems do not use robust neural networks such as pre-trained language models to take advantage of large-scale unstructured data effectively. In respons to this issue, a new system entitled Chinese Open Relation Extraction with Knowledge Enhancement (CORE-KE) is presented in this paper. The CORE-KE system employs a pre-trained language model (with the support of a Bidirectional Long Short-Term Memory (BiLSTM) layer and a Masked Conditional Random Field (Masked CRF) layer) on unstructured data in order to improve Chinese open relation extraction. Entity descriptions in Wikidata and additional knowledge (in terms of triple facts) extracted from Chinese ORE datasets are used to fine-tune the pre-trained language model. In addition, syntactic features are further adopted in the training stage of the CORE-KE system for knowledge enhancement. Experimental results of the CORE-KE system on two large-scale datasets of open Chinese entities and relations demonstrate that the CORE-KE system is superior to other ORE systems. The F1-scores of the CORE-KE system on the two datasets have given a relative improvement of 20.1\% and 1.3\%, when compared with benchmark ORE systems, respectively. The source code is available at https://github.com/cjwen15/CORE-KE .},
  archive      = {J_DINT},
  author       = {Wen, Chaojie and Jia, Xudong and Chen, Tao},
  doi          = {10.1162/dint_a_00227},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {962-989},
  shortjournal = {Data Intell.},
  title        = {Improving extraction of chinese open relations using pre-trained language model and knowledge enhancement},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Role of sports based on big data analysis in promoting the
physique and health of children and adolescents. <em>DINT</em>,
<em>5</em>(4), 947–961. (<a
href="https://doi.org/10.1162/dint_a_00207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthy body is the foundation of young people&#39;s growth. With the popularization and globalization of the Internet, multimedia technology is rapidly changing the impact of traditional media on the growth of young people. The current health situation of young people is not optimistic. The decline in physical fitness, obesity and psychological dysplasia of adolescents have aroused the concern of all sectors of society. In recent years, the emergence and dissemination of big data (BD) has brought a new dimension to the value of data applications. The combination of BD and youth health services provides young people with good health opportunities. Through the recording, analysis and release of adolescent physical health data, the system has established an extensive knowledge database on adolescent physical and mental health, thus improving the physical health of adolescents. This paper summarized and combed the overview and application of BD, and analyzed and discussed the reasons for the continuous decline of young people&#39;s physique. Through the analysis of the application of BD in the promotion of young people&#39;s physical health, this paper proposed more achievable improvement strategies and plans, and then summarizes and discusses the experiment. According to the survey and experiment, the random simulation algorithm was introduced into daily exercise, diet and life preference. The new system and health improvement strategy designed for teenagers’ physical health using BD could help students improve their physical health by 55\%.},
  archive      = {J_DINT},
  author       = {Wen, Pengfei and Wang, Menghua and Wu, Jinsong},
  doi          = {10.1162/dint_a_00207},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {947-961},
  shortjournal = {Data Intell.},
  title        = {Role of sports based on big data analysis in promoting the physique and health of children and adolescents},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revealing the trends in the academic landscape of the health
care system using contextual topic modeling. <em>DINT</em>,
<em>5</em>(4), 923–946. (<a
href="https://doi.org/10.1162/dint_a_00217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The health care system encompasses the participation of individuals, groups, agencies, and resources that offer services to address the requirements of the person, community, and population in terms of health. Parallel to the rising debates on the healthcare systems in relation to diseases, treatments, interventions, medication, and clinical practice guidelines, the world is currently discussing the healthcare industry, technology perspectives, and healthcare costs. To gain a comprehensive understanding of the healthcare systems research paradigm, we offered a novel contextual topic modeling approach that links up the CombinedTM model with our healthcare Bert to discover the contextual topics in the domain of healthcare. This research work discovered 60 contextual topics among them fifteen topics are the hottest which include smart medical monitoring systems, causes, and effects of stress and anxiety, and healthcare cost estimation and twelve topics are the coldest. Moreover, thirty-three topics are showing in-significant trends. We further investigated various clusters and correlations among the topics exploring inter-topic distance maps which add depth to the understanding of the research structure of this scientific domain. The current study enhances the prior topic modeling methodologies that examine the healthcare literature from a particular disciplinary perspective. It further extends the existing topic modeling approaches that do not incorporate contextual information in the topic discovery process adding contextual information by creating sentence embedding vectors through transformers-based models. We also utilized corpus tuning, the mean pooling technique, and the hugging face tool. Our method gives a higher coherence score as compared to the state-of-the-art models (LSA, LDA, and Ber Topic).},
  archive      = {J_DINT},
  author       = {Inaam ul haq, Muhammad and Li, Qianmu},
  doi          = {10.1162/dint_a_00217},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {923-946},
  shortjournal = {Data Intell.},
  title        = {Revealing the trends in the academic landscape of the health care system using contextual topic modeling},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison of parallel genetic algorithm and particle swarm
optimization for parameter calibration in hydrological simulation.
<em>DINT</em>, <em>5</em>(4), 904–922. (<a
href="https://doi.org/10.1162/dint_a_00221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter calibration is an important part of hydrological simulation and affects the final simulation results. In this paper, we introduce heuristic optimization algorithms, genetic algorithm (GA) to cope with the complexity of the parameter calibration problem, and use particle swarm optimization algorithm (PSO) as a comparison. For large-scale hydrological simulations, we use a multilevel parallel parameter calibration framework to make full use of processor resources, and accelerate the process of solving high-dimensional parameter calibration. Further, we test and apply the experiments on domestic supercomputers. The results of parameter calibration with GA and PSO can basically reach the ideal value of 0.65 and above, with PSO achieving a speedup of 58.52 on TianHe-2 supercomputer. The experimental results indicate that using a parallel implementation on multicore CPUs makes high-dimensional parameter calibration in large-scale hydrological simulation possible. Moreover, our comparison of the two algorithms shows that the GA obtains better calibration results, and the PSO has a more pronounced acceleration effect.},
  archive      = {J_DINT},
  author       = {Zhang, Xinyu and Li, Yang and Chu, Genshen},
  doi          = {10.1162/dint_a_00221},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {904-922},
  shortjournal = {Data Intell.},
  title        = {Comparison of parallel genetic algorithm and particle swarm optimization for parameter calibration in hydrological simulation},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation on ChatGPT for chinese language understanding.
<em>DINT</em>, <em>5</em>(4), 885–903. (<a
href="https://doi.org/10.1162/dint_a_00232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT has attracted extension attention of academia and industry. This paper aims to evaluate ChatGPT in Chinese language understanding capability on 6 tasks using 11 datasets. Experiments indicate that ChatGPT achieved competitive results in sentiment analysis, summary, and reading comprehension in Chinese, while it is prone to factual errors in closed-book QA. Further, on two more difficult Chinese understanding tasks, that is, idiom fill-in-the-blank and cants understanding, we found that a simple chain-of-thought prompt can improve the accuracy of ChatGPT in complex reasoning. This paper further analyses the possible risks of using ChatGPT based on the results. Finally, we briefly describe the research and development progress of our ChatBIT.},
  archive      = {J_DINT},
  author       = {Li, Linhan and Zhang, Huaping and Li, Chunjin and You, Haowen and Cui, Wenyao},
  doi          = {10.1162/dint_a_00232},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {885-903},
  shortjournal = {Data Intell.},
  title        = {Evaluation on ChatGPT for chinese language understanding},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart management information systems (smis): Concept,
evolution, research hotspots and applications. <em>DINT</em>,
<em>5</em>(4), 857–884. (<a
href="https://doi.org/10.1162/dint_a_00231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Management information system (MIS), a human-computer system that deeply integrates next-generation information technology and management services, has become the nerve center of society and organizations. With the development of next-generation information technology, MIS has gradually entered the smart period. However, research on smart management information systems (SMIS) is still limited, lacking systematic summarization of its conceptual definition, evolution, research hotspots, and typical applications. Therefore, this paper defines the conceptual characteristics of SMIS, provides an overview of the evolution of SMIS, examines research focus areas using bibliometric methods, and elaborates on typical application practices of SMIS in fields such as health care, elderly care, manufacturing, and transportation. Furthermore, we discuss the future development directions of SMIS in four key areas: smart interaction, smart decisionmaking, efficient resource allocation, and flexible system architecture. These discussions provide guidance and a foundation for the theoretical development and practical application of SMIS.},
  archive      = {J_DINT},
  author       = {Liang, Changyong and Wang, Xiaoxiao and Gu, Dongxiao and Li, Pengyu and Chen, Hui and Xu, Zhengfei},
  doi          = {10.1162/dint_a_00231},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {857-884},
  shortjournal = {Data Intell.},
  title        = {Smart management information systems (Smis): Concept, evolution, research hotspots and applications},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on automatic delineation of radiotherapy target
volume based on machine learning. <em>DINT</em>, <em>5</em>(3), 841–856.
(<a href="https://doi.org/10.1162/dint_a_00204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Radiotherapy is one of the main treatment methods for cancer, and the delineation of the radiotherapy target area is the basis and premise of precise treatment. Artificial intelligence technology represented by machine learning has done a lot of research in this area, improving the accuracy and efficiency of target delineation. This article will review the applications and research of machine learning in medical image matching, normal organ delineation and treatment target delineation according to the procudures of doctors to delineate the target volume, and give an outlook on the development prospects.},
  archive      = {J_DINT},
  author       = {Tao, Zhenchao and Lyu, Shengfei},
  doi          = {10.1162/dint_a_00204},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {841-856},
  shortjournal = {Data Intell.},
  title        = {A survey on automatic delineation of radiotherapy target volume based on machine learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation extraction based on prompt information and feature
reuse. <em>DINT</em>, <em>5</em>(3), 824–840. (<a
href="https://doi.org/10.1162/dint_a_00192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. To alleviate the problem of under-utilization features of sentence-level relation extraction, which leads to insufficient performance of the pre-trained language model and underutilization of the feature vector, a sentence-level relation extraction method based on adding prompt information and feature reuse is proposed. At first, in addition to the pair of nominals and sentence information, a piece of prompt information is added, and the overall feature information consists of sentence information, entity pair information, and prompt information, and then the features are encoded by the pre-trained language model ROBERTA. Moreover, in the pre-trained language model, BIGRU is also introduced in the composition of the neural network to extract information, and the feature information is passed through the neural network to form several sets of feature vectors. After that, these feature vectors are reused in different combinations to form multiple outputs, and the outputs are aggregated using ensemble-learning soft voting to perform relation classification. In addition to this, the sum of cross-entropy, KL divergence, and negative log-likelihood loss is used as the final loss function in this paper. In the comparison experiments, the model based on adding prompt information and feature reuse achieved higher results of the SemEval-2010 task 8 relational dataset.},
  archive      = {J_DINT},
  author       = {Feng, Ping and Zhang, Xin and Zhao, Jian and Wang, Yingying and Huang, Biao},
  doi          = {10.1162/dint_a_00192},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {824-840},
  shortjournal = {Data Intell.},
  title        = {Relation extraction based on prompt information and feature reuse},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three heads better than one: Pure entity, relation label and
adversarial training for cross-domain few-shot relation extraction.
<em>DINT</em>, <em>5</em>(3), 807–823. (<a
href="https://doi.org/10.1162/dint_a_00190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. In this paper, we study cross-domain relation extraction. Since new data mapping to feature spaces always differs from the previously seen data due to a domain shift, few-shot relation extraction often perform poorly. To solve the problems caused by cross-domain, we propose a method for combining the pure entity, relation labels and adversarial (PERLA). We first use entities and complete sentences for separate encoding to obtain context-independent entity features. Then, we combine relation labels which are useful for relation extraction to mitigate context noise. We combine adversarial to reduce the noise caused by cross-domain. We conducted experiments on the publicly available cross-domain relation extraction dataset Fewrel 2.0[1]①, and the results show that our approach improves accuracy and has better transferability for better adaptation to cross-domain tasks.},
  archive      = {J_DINT},
  author       = {Fang, Wenlong and Ouyang, Chunping and Lin, Qiang and Yuan, Yue},
  doi          = {10.1162/dint_a_00190},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {807-823},
  shortjournal = {Data Intell.},
  title        = {Three heads better than one: Pure entity, relation label and adversarial training for cross-domain few-shot relation extraction},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial neural collaborative filtering with embedding
dimension correlations. <em>DINT</em>, <em>5</em>(3), 786–806. (<a
href="https://doi.org/10.1162/dint_a_00151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Recently, convolutional neural networks (CNNs) have achieved excellent performance for the recommendation system by extracting deep features and building collaborative filtering models. However, CNNs have been verified susceptible to adversarial examples. This is because adversarial samples are subtle non-random disturbances, which indicates that machine learning models produce incorrect outputs. Therefore, we propose a novel model of Adversarial Neural Collaborative Filtering with Embedding Dimension Correlations, named ANCF in short, to address the adversarial problem of CNN-based recommendation system. In particular, the proposed ANCF model adopts the matrix factorization to train the adversarial personalized ranking in the prediction layer. This is because matrix factorization supposes that the linear interaction of the latent factors, which are captured between the user and the item, can describe the observable feedback, thus the proposed ANCF model can learn more complicated representation of their latent factors to improve the performance of recommendation. In addition, the ANCF model utilizes the outer product instead of the inner product or concatenation to learn explicitly pairwise embedding dimensional correlations and obtain the interaction map from which CNNs can utilize its strengths to learn high-order correlations. As a result, the proposed ANCF model can improve the robustness performance by the adversarial personalized ranking, and obtain more information by encoding correlations between different embedding layers. Experimental results carried out on three public datasets demonstrate that the ANCF model outperforms other existing recommendation models.},
  archive      = {J_DINT},
  author       = {Gao, Yi and Chen, Jianxia and Xiao, Liang and Wang, Hongyang and Pan, Liwei and Wen, Xuan and Ye, Zhiwei and Wu, Xinyun},
  doi          = {10.1162/dint_a_00151},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {786-806},
  shortjournal = {Data Intell.},
  title        = {Adversarial neural collaborative filtering with embedding dimension correlations},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot named entity recognition with joint token and
sentence awareness. <em>DINT</em>, <em>5</em>(3), 767–785. (<a
href="https://doi.org/10.1162/dint_a_00195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Few-shot learning has been proposed and rapidly emerging as a viable means for completing various tasks. Recently, few-shot models have been used for Named Entity Recognition (NER). Prototypical network shows high efficiency on few-shot NER. However, existing prototypical methods only consider the similarity of tokens in query sets and support sets and ignore the semantic similarity among the sentences which contain these entities. We present a novel model, Few-shot Named Entity Recognition with Joint Token and Sentence Awareness (JTSA), to address the issue. The sentence awareness is introduced to probe the semantic similarity among the sentences. The Token awareness is used to explore the similarity of the tokens. To further improve the robustness and results of the model, we adopt the joint learning scheme on the few-shot NER. Experimental results demonstrate that our model outperforms state-of-the-art models on two standard Few-shot NER datasets.},
  archive      = {J_DINT},
  author       = {Wen, Wen and Liu, Yongbin and Lin, Qiang and Ouyang, Chunping},
  doi          = {10.1162/dint_a_00195},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {767-785},
  shortjournal = {Data Intell.},
  title        = {Few-shot named entity recognition with joint token and sentence awareness},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Total electricity consumption forecasting based on
temperature composite index and mixed-frequency models. <em>DINT</em>,
<em>5</em>(3), 750–766. (<a
href="https://doi.org/10.1162/dint_a_00215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. The total electricity consumption (TEC) can accurately reflect the operation of the national economy, and the forecasting of the TEC can help predict the economic development trend, as well as provide insights for the formulation of macro policies. Nowadays, high-frequency and massive multi-source data provide a new way to predict the TEC. In this paper, a “seasonal-cumulative temperature index” is constructed based on high-frequency temperature data, and a mixed-frequency prediction model based on multi-source big data (Mixed Data Sampling with Monthly Temperature and Daily Temperature index, MIDAS-MT-DT) is proposed. Experimental results show that the MIDAS-MT-DT model achieves higher prediction accuracy, and the “seasonal-cumulative temperature index” can improve prediction accuracy.},
  archive      = {J_DINT},
  author       = {Li, Xuerong and Shang, Wei and Zhang, Xun and Shan, Baoguo and Wang, Xiang},
  doi          = {10.1162/dint_a_00215},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {750-766},
  shortjournal = {Data Intell.},
  title        = {Total electricity consumption forecasting based on temperature composite index and mixed-frequency models},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The state of the art of natural language processing—a
systematic automated review of NLP literature using NLP techniques.
<em>DINT</em>, <em>5</em>(3), 707–749. (<a
href="https://doi.org/10.1162/dint_a_00213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Nowadays, natural language processing (NLP) is one of the most popular areas of, broadly understood, artificial intelligence. Therefore, every day, new research contributions are posted, for instance, to the arXiv repository. Hence, it is rather difficult to capture the current “state of the field” and thus, to enter it. This brought the id-art NLP techniques to analyse the NLP-focused literature. As a result, (1) meta-level knowledge, concerning the current state of NLP has been captured, and (2) a guide to use of basic NLP tools is provided. It should be noted that all the tools and the dataset described in this contribution are publicly available. Furthermore, the originality of this review lies in its full automation. This allows easy reproducibility and continuation and updating of this research in the future as new researches emerge in the field of NLP.},
  archive      = {J_DINT},
  author       = {Sawicki, Jan and Ganzha, Maria and Paprzycki, Marcin},
  doi          = {10.1162/dint_a_00213},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {707-749},
  shortjournal = {Data Intell.},
  title        = {The state of the art of natural language Processing—A systematic automated review of NLP literature using NLP techniques},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph based mutual attention for machine reading
comprehension over anti-terrorism corpus. <em>DINT</em>, <em>5</em>(3),
685–706. (<a href="https://doi.org/10.1162/dint_a_00210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Machine reading comprehension has been a research focus in natural language processing and intelligence engineering. However, there is a lack of models and datasets for the MRC tasks in the anti-terrorism domain. Moreover, current research lacks the ability to embed accurate background knowledge and provide precise answers. To address these two problems, this paper first builds a text corpus and testbed that focuses on the anti-terrorism domain in a semi-automatic manner. Then, it proposes a knowledge-based machine reading comprehension model that fuses domain-related triples from a large-scale encyclopedic knowledge base to enhance the semantics of the text. To eliminate knowledge noise that could lead to semantic deviation, this paper uses a mixed mutual attention mechanism among questions, passages, and knowledge triples to select the most relevant triples before embedding their semantics into the sentences. Experiment results indicate that the proposed approach can achieve a 70.70\% EM value and an 87.91\% F1 score, with a 4.23\% and 3.35\% improvement over existing methods, respectively.},
  archive      = {J_DINT},
  author       = {Gao, Feng and Hou, Jin and Gu, Jinguang and Zhang, Lihua},
  doi          = {10.1162/dint_a_00210},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {685-706},
  shortjournal = {Data Intell.},
  title        = {Knowledge graph based mutual attention for machine reading comprehension over anti-terrorism corpus},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge graph-based deep learning framework for
efficient content similarity search of sustainable development goals
data. <em>DINT</em>, <em>5</em>(3), 663–684. (<a
href="https://doi.org/10.1162/dint_a_00230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Sustainable development denotes the enhancement of living standards in the present without compromising future generations’ resources. Sustainable Development Goals (SDGs) quantify the accomplishment of sustainable development and pave the way for a world worth living in for future generations. Scholars can contribute to the achievement of the SDGs by guiding the actions of practitioners based on the analysis of SDG data, as intended by this work. We propose a framework of algorithms based on dimensionality reduction methods with the use of Hilbert Space Filling Curves (HSFCs) in order to semantically cluster new uncategorised SDG data and novel indicators, and efficiently place them in the environment of a distributed knowledge graph store. First, a framework of algorithms for insertion of new indicators and projection on the HSFC curve based on their transformer-based similarity assessment, for retrieval of indicators and load-balancing along with an approach for data classification of entrant-indicators is described. Then, a thorough case study in a distributed knowledge graph environment experimentally evaluates our framework. The results are presented and discussed in light of theory along with the actual impact that can have for practitioners analysing SDG data, including intergovernmental organizations, government agencies and social welfare organizations. Our approach empowers SDG knowledge graphs for causal analysis, inference, and manifold interpretations of the societal implications of SDG-related actions, as data are accessed in reduced retrieval times. It facilitates quicker measurement of influence of users and communities on specific goals and serves for faster distributed knowledge matching, as semantic cohesion of data is preserved.},
  archive      = {J_DINT},
  author       = {Kilanioti, Irene and A. Papadopoulos, George},
  doi          = {10.1162/dint_a_00230},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {663-684},
  shortjournal = {Data Intell.},
  title        = {A knowledge graph-based deep learning framework for efficient content similarity search of sustainable development goals data},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating functional status information into knowledge
graphs to support self-health management. <em>DINT</em>, <em>5</em>(3),
636–662. (<a href="https://doi.org/10.1162/dint_a_00203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Functional Status Information (FSI) describes physical and mental wellness at the whole-person level. It includes information on activity performance, social role participation, and environmental and personal factors that affect the well-being and quality of life. Collecting and analyzing this information is critical to address the needs for caring for an aging global population, and to provide effective care for individuals with chronic conditions, multi-morbidity, and disability. Personal knowledge graphs (PKGs) represent a suitable way for meaning in a complete and structured way all information related to people&#39;s FSI and reasoning over them to build tailored coaching solutions supporting them in daily life for conducting a healthy living. In this paper, we present the development process related to the creation of a PKG by starting from the HeLiS ontology in order to enable the design of an AI-enabled system with the aim of increasing, within people, the self-awareness of their own functional status. In particular, we focus on the three modules extending the HeLiS ontology aiming to represent (i) enablers and (ii) barriers playing potential roles in improving (or deteriorating) own functional status and (iii) arguments driving the FSI collection process. Finally, we show how these modules have been instantiated into real-world scenarios.},
  archive      = {J_DINT},
  author       = {Dragoni, Mauro and Bailoni, Tania and Donadello, Ivan and Martin, Jean-Claude and Lindgren, Helena},
  doi          = {10.1162/dint_a_00203},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {636-662},
  shortjournal = {Data Intell.},
  title        = {Integrating functional status information into knowledge graphs to support self-health management},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source-aware embedding training on heterogeneous information
networks. <em>DINT</em>, <em>5</em>(3), 611–635. (<a
href="https://doi.org/10.1162/dint_a_00200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks. While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN. However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms. This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding)—a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN. Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms.},
  archive      = {J_DINT},
  author       = {Chan, Tsai Hor and Wong, Chi Ho and Shen, Jiajun and Yin, Guosheng},
  doi          = {10.1162/dint_a_00200},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {611-635},
  shortjournal = {Data Intell.},
  title        = {Source-aware embedding training on heterogeneous information networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MillenniumDB: An open-source graph database system.
<em>DINT</em>, <em>5</em>(3), 560–610. (<a
href="https://doi.org/10.1162/dint_a_00229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. In this systems paper, we present MillenniumDB: a novel graph database engine that is modular, persistent, and open source. MillenniumDB is based on a graph data model, which we call domain graphs, that provides a simple abstraction upon which a variety of popular graph models can be supported, thus providing a flexible data management engine for diverse types of knowledge graph. The engine itself is founded on a combination of tried and tested techniques from relational data management, state-of-the-art algorithms for worst-case-optimal joins, as well as graph-specific algorithms for evaluating path queries. In this paper, we present the main design principles underlying MillenniumDB, describing the abstract graph model and query semantics supported, the concrete data model and query syntax implemented, as well as the storage, indexing, query planning and query evaluation techniques used. We evaluate MillenniumDB over real-world data and queries from the Wikidata knowledge graph, where we find that it outperforms other popular persistent graph database engines (including both enterprise and open source alternatives) that support similar query features.},
  archive      = {J_DINT},
  author       = {Vrgoč, Domagoj and Rojas, Carlos and Angles, Renzo and Arenas, Marcelo and Arroyuelo, Diego and Buil-Aranda, Carlos and Hogan, Aidan and Navarro, Gonzalo and Riveros, Cristian and Romero, Juan},
  doi          = {10.1162/dint_a_00229},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {560-610},
  shortjournal = {Data Intell.},
  title        = {MillenniumDB: An open-source graph database system},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HUSS: A heuristic method for understanding the semantic
structure of spreadsheets. <em>DINT</em>, <em>5</em>(3), 537–559. (<a
href="https://doi.org/10.1162/dint_a_00201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Spreadsheets contain a lot of valuable data and have many practical applications. The key technology of these practical applications is how to make machines understand the semantic structure of spreadsheets, e.g., identifying cell function types and discovering relationships between cell pairs. Most existing methods for understanding the semantic structure of spreadsheets do not make use of the semantic information of cells. A few studies do, but they ignore the layout structure information of spreadsheets, which affects the performance of cell function classification and the discovery of different relationship types of cell pairs. In this paper, we propose a Heuristic algorithm for Understanding the Semantic Structure of spreadsheets (HUSS). Specifically, for improving the cell function classification, we propose an error correction mechanism (ECM) based on an existing cell function classification model [11] and the layout features of spreadsheets. For improving the table structure analysis, we propose five types of heuristic rules to extract four different types of cell pairs, based on the cell style and spatial location information. Our experimental results on five real-world datasets demonstrate that HUSS can effectively understand the semantic structure of spreadsheets and outperforms corresponding baselines.},
  archive      = {J_DINT},
  author       = {Wu, Xindong and Chen, Hao and Bu, Chenyang and Ji, Shengwei and Zhang, Zan and Sheng, Victor S.},
  doi          = {10.1162/dint_a_00201},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {537-559},
  shortjournal = {Data Intell.},
  title        = {HUSS: A heuristic method for understanding the semantic structure of spreadsheets},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RCMR 280k: Refined corpus for move recognition based on
PubMed abstracts. <em>DINT</em>, <em>5</em>(3), 511–536. (<a
href="https://doi.org/10.1162/dint_a_00214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Existing datasets for move recognition, such as PubMed 200k RCT, exhibit several problems that significantly impact recognition performance, especially for Background and Objective labels. In order to improve the move recognition performance, we introduce a method and construct a refined corpus based on PubMed, named RCMR 280k. This corpus comprises approximately 280,000 structured abstracts, totaling 3,386,008 sentences, each sentence is labeled with one of five categories: Background, Objective, Method, Result, or Conclusion. We also construct a subset of RCMR, named RCMR_RCT, corresponding to medical subdomain of RCTs. We conduct comparison experiments using our RCMR, RCMR_RCT with PubMed 380k and PubMed 200k RCT, respectively. The best results, obtained using the MSMBERT model, show that: (1) our RCMR outperforms PubMed 380k by 0.82\%, while our RCMR_RCT outperforms PubMed 200k RCT by 9.35\%; (2) compared with PubMed 380k, our corpus achieve better improvement on the Results and Conclusions categories, with average F1 performance improves 1\% and 0.82\%, respectively; (3) compared with PubMed 200k RCT, our corpus significantly improves the performance in the Background and Objective categories, with average F1 scores improves 28.31\% and 37.22\%, respectively. To the best of our knowledge, our RCMR is among the rarely high-quality, resource-rich refined PubMed corpora available. Our work in this paper has been applied in the SciAIEngine, which is openly accessible for researchers to conduct move recognition task.},
  archive      = {J_DINT},
  author       = {Li, Jie and Yu, Gaihong and Zhang, Zhixiong},
  doi          = {10.1162/dint_a_00214},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {511-536},
  shortjournal = {Data Intell.},
  title        = {RCMR 280k: Refined corpus for move recognition based on PubMed abstracts},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging continuous prompt for few-shot named entity
recognition in electric power domain with meta-learning. <em>DINT</em>,
<em>5</em>(2), 494–509. (<a
href="https://doi.org/10.1162/dint_a_00202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Conventional named entity recognition methods usually assume that the model can be trained with sufficient annotated data to obtain good recognition results. However, in Chinese named entity recognition in the electric power domain, existing methods still face the challenges of lack of annotated data and new entities of unseen types. To address these challenges, this paper proposes a meta-learning-based continuous cue adjustment method. A generative pre-trained language model is used so that it does not change its own model structure when dealing with new entity types. To guide the pre-trained model to make full use of its own latent knowledge, a vector of learnable parameters is set as a cue to compensate for the lack of training data. In order to further improve the model&#39;s few-shot learning capability, a meta-learning strategy is used to train the model. Experimental results show that the proposed approach achieves the best results in a few-shot electric Chinese power named entity recognition dataset compared to several traditional named entity approaches.},
  archive      = {J_DINT},
  author       = {Yu, Yang and He, Wei and Kang, Yu-meng and Ji, You-lang},
  doi          = {10.1162/dint_a_00202},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {494-509},
  shortjournal = {Data Intell.},
  title        = {Leveraging continuous prompt for few-shot named entity recognition in electric power domain with meta-learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relational topology-based heterogeneous network embedding
for predicting drug-target interactions. <em>DINT</em>, <em>5</em>(2),
475–493. (<a href="https://doi.org/10.1162/dint_a_00149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Predicting interactions between drugs and target proteins has become an essential task in the drug discovery process. Although the method of validation via wet-lab experiments has become available, experimental methods for drug-target interaction (DTI) identification remain either time consuming or heavily dependent on domain expertise. Therefore, various computational models have been proposed to predict possible interactions between drugs and target proteins. However, most prediction methods do not consider the topological structures characteristics of the relationship. In this paper, we propose a relational topology-based heterogeneous network embedding method to predict drug-target interactions, abbreviated as RTHNE_ DTI. We first construct a heterogeneous information network based on the interaction between different types of nodes, to enhance the ability of association discovery by fully considering the topology of the network. Then drug and target protein nodes can be represented by the other types of nodes. According to the different topological structure of the relationship between the nodes, we divide the relationship in the heterogeneous network into two categories and model them separately. Extensive experiments on the real-world drug datasets, RTHNE_DTI produces high efficiency and outperforms other state-of-the-art methods. RTHNE_DTI can be further used to predict the interaction between unknown interaction drug-target pairs.},
  archive      = {J_DINT},
  author       = {Zhang, Linlin and Ouyang, Chunping and Hu, Fuyu and Liu, Yongbin and Gao, Zheng},
  doi          = {10.1162/dint_a_00149},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {475-493},
  shortjournal = {Data Intell.},
  title        = {Relational topology-based heterogeneous network embedding for predicting drug-target interactions},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards text-to-SQL over aggregate tables. <em>DINT</em>,
<em>5</em>(2), 457–474. (<a
href="https://doi.org/10.1162/dint_a_00194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Text-to-SQL aims at translating textual questions into the corresponding SQL queries. Aggregate tables are widely created for high-frequent queries. Although text-to-SQL has emerged as an important task, recent studies paid little attention to the task over aggregate tables. The increased aggregate tables bring two challenges: (1) mapping of natural language questions and relational databases will suffer from more ambiguity, (2) modern models usually adopt self-attention mechanism to encode database schema and question. The mechanism is of quadratic time complexity, which will make inferring more time-consuming as input sequence length grows. In this paper, we introduce a novel approach named WAGG for text-to-SQL over aggregate tables. To effectively select among ambiguous items, we propose a relation selection mechanism for relation computing. To deal with high computation costs, we introduce a dynamical pruning strategy to discard unrelated items that are common for aggregate tables. We also construct a new large-scale dataset SpiderwAGG extended from Spider dataset for validation, where extensive experiments show the effectiveness and efficiency of our proposed method with 4\% increase of accuracy and 15\% decrease of inference time w.r.t a strong baseline RAT-SQL.},
  archive      = {J_DINT},
  author       = {Li, Shuqin and Zhou, Kaibin and Zhuang, Zeyang and Wang, Haofen and Ma, Jun},
  doi          = {10.1162/dint_a_00194},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {457-474},
  shortjournal = {Data Intell.},
  title        = {Towards text-to-SQL over aggregate tables},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Association discovery and outlier detection of air pollution
emissions from industrial enterprises driven by big data. <em>DINT</em>,
<em>5</em>(2), 438–456. (<a
href="https://doi.org/10.1162/dint_a_00205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Air pollution is a major issue related to national economy and people&#39;s livelihood. At present, the researches on air pollution mostly focus on the pollutant emissions in a specific industry or region as a whole, and is a lack of attention to enterprise pollutant emissions from the micro level. Limited by the amount and time granularity of data from enterprises, enterprise pollutant emissions are still understudied. Driven by big data of air pollution emissions of industrial enterprises monitored in Beijing-Tianjin-Hebei, the data mining of enterprises pollution emissions is carried out in the paper, including the association analysis between different features based on grey association, the association mining between different data based on association rule and the outlier detection based on clustering. The results show that: (1) The industries affecting NOx and SO2 mainly are electric power, heat production and supply industry, metal smelting and processing industries in Beijing-Tianjin-Hebei; (2) These districts nearby Hengshui and Shijiazhuang city in Hebei province form strong association rules; (3) The industrial enterprises in Beijing-Tianjin-Hebei are divided into six clusters, of which three categories belong to outliers with excessive emissions of total VOCs, PM and NH3 respectively.},
  archive      = {J_DINT},
  author       = {Peng, Zhen and Zhang, Yunxiao and Wang, Yunchong and Tang, Tianle},
  doi          = {10.1162/dint_a_00205},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {438-456},
  shortjournal = {Data Intell.},
  title        = {Association discovery and outlier detection of air pollution emissions from industrial enterprises driven by big data},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RS-SVM machine learning approach driven by case data for
selecting urban drainage network restoration scheme. <em>DINT</em>,
<em>5</em>(2), 413–437. (<a
href="https://doi.org/10.1162/dint_a_00208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Urban drainage pipe network is the backbone of urban drainage, flood control and water pollution prevention, and is also an essential symbol to measure the level of urban modernization. A large number of underground drainage pipe networks in aged urban areas have been laid for a long time and have reached or practically reached the service age. The repair of drainage pipe networks has attracted extensive attention from all walks of life. Since the Ministry of ecological environment and the national development and Reform Commission jointly issued the action plan for the Yangtze River Protection and restoration in 2019, various provinces in the Yangtze River Basin, such as Anhui, Jiangxi and Hunan, have extensively carried out PPP projects for urban pipeline restoration, in order to improve the quality and efficiency of sewage treatment. Based on the management practice of urban pipe network restoration project in Wuhu City, Anhui Province, this paper analyzes the problems of lengthy construction period and repeated operation caused by the mismatch between the design schedule of the restoration scheme and the construction schedule of the pipe network restoration in the existing project management mode, and proposes a model of urban drainage pipe network restoration scheme selection based on the improved support vector machine. The validity and feasibility of the model are analyzed and verified by collecting the data in the project practice. The research results show that the model has a favorable effect on the selection of urban drainage pipeline restoration schemes, and its accuracy can reach 90\%. The research results can provide method guidance and technical support for the rapid decision-making of urban drainage pipeline restoration projects.},
  archive      = {J_DINT},
  author       = {Jiang, Li and Geng, Zheng and Gu, Dongxiao and Guo, Shuai and Huang, Rongmin and Cheng, Haoke and Zhu, Kaixuan},
  doi          = {10.1162/dint_a_00208},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {413-437},
  shortjournal = {Data Intell.},
  title        = {RS-SVM machine learning approach driven by case data for selecting urban drainage network restoration scheme},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto insurance fraud detection with multimodal learning.
<em>DINT</em>, <em>5</em>(2), 388–412. (<a
href="https://doi.org/10.1162/dint_a_00191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. In recent years, feature engineering-based machine learning models have made significant progress in auto insurance fraud detection. However, most models or systems focused only on structural data and did not utilize multi-modal data to improve fraud detection efficiency. To solve this problem, we adapt both natural language processing and computer vision techniques to our knowledge-based algorithm and construct an Auto Insurance Multi-modal Learning (AIML) framework. We then apply AIML to detect fraud behavior in auto insurance cases with data from real scenarios and conduct experiments to examine the improvement in model performance with multi-modal data compared to baseline model with structural data only. A self-designed Semi-Auto Feature Engineer (SAFE) algorithm to process auto insurance data and a visual data processing framework are embedded within AIML. Results show that AIML substantially improves the model performance in detecting fraud behavior compared to models that only use structural data.},
  archive      = {J_DINT},
  author       = {Yang, Jiaxi and Chen, Kui and Ding, Kai and Na, Chongning and Wang, Meng},
  doi          = {10.1162/dint_a_00191},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {388-412},
  shortjournal = {Data Intell.},
  title        = {Auto insurance fraud detection with multimodal learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic segmentation of landcover for cropland mapping and
area estimation using machine learning techniques. <em>DINT</em>,
<em>5</em>(2), 370–387. (<a
href="https://doi.org/10.1162/dint_a_00145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. The paper has focussed on the global landcover for the identification of cropland areas. Population growth and rapid industrialization are somehow disturbing the agricultural lands and eventually the food production needed for human survival. Appropriate agricultural land monitoring requires proper management of land resources. The paper has proposed a method for cropland mapping by semantic segmentation of landcover to identify the cropland boundaries and estimate the cropland areas using machine learning techniques. The process has initially applied various filters to identify the features responsible for detecting the land boundaries through the edge detection process. The images are masked or annotated to produce the ground truth for the label identification of croplands, rivers, buildings, and backgrounds. The selected features are transferred to a machine learning model for the semantic segmentation process. The methodology has applied Random Forest, which has compared to two other techniques, Support Vector Machine and Multilayer perceptron, for the semantic segmentation process. Our dataset is composed of satellite images collected from the QGIS application. The paper has derived the conclusion that Random forest has given the best result for segmenting the image into different regions with 99\% training accuracy and 90\% test accuracy. The results are cross-validated by computing the Mean IoU and kappa coefficient that shows 93\% and 69\% score value respectively for Random Forest, found maximum among all. The paper has also calculated the area covered under the different segmented regions. Overall, Random Forest has produced promising results for semantic segmentation of landcover for cropland mapping.},
  archive      = {J_DINT},
  author       = {Lingwal, Surabhi and Bhatia, Komal Kumar and Singh, Manjeet},
  doi          = {10.1162/dint_a_00145},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {370-387},
  shortjournal = {Data Intell.},
  title        = {Semantic segmentation of landcover for cropland mapping and area estimation using machine learning techniques},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research e-infrastructures for open science: The national
example of CSTCloud in china. <em>DINT</em>, <em>5</em>(2), 355–369. (<a
href="https://doi.org/10.1162/dint_a_00196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. This paper focuses on research e-infrastructures in the open science era. We analyze some of the challenges and opportunities of cloud-based science and introduce an example of a national solution in the China Science and Technology Cloud (CSTCloud). We selected three CSTCloud use cases in deploying open science modules, including scalable engineering in astronomical data management, integrated Earth-science resources for SDG-13 decision making, and the coupling of citizen science and artificial intelligence (AI) techniques in biodiversity. We conclude with a forecast on the future development of research e-infrastructures and introduce the idea of the Global Open Science Cloud (GOSC). We hope this analysis can provide some insights into the future development of research e-infrastructures in support of open science.},
  archive      = {J_DINT},
  author       = {Zhang, Lili and Li, Jianhui and Uhlir, Paul F. and Wen, Liangming and Wu, Kaichao and Luo, Ze and Liu, Yude},
  doi          = {10.1162/dint_a_00196},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {355-369},
  shortjournal = {Data Intell.},
  title        = {Research e-infrastructures for open science: The national example of CSTCloud in china},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for medication recommendation: A systematic
survey. <em>DINT</em>, <em>5</em>(2), 303–354. (<a
href="https://doi.org/10.1162/dint_a_00197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Making medication prescriptions in response to the patient&#39;s diagnosis is a challenging task. The number of pharmaceutical companies, their inventory of medicines, and the recommended dosage confront a doctor with the well-known problem of information and cognitive overload. To assist a medical practitioner in making informed decisions regarding a medical prescription to a patient, researchers have exploited electronic health records (EHRs) in automatically recommending medication. In recent years, medication recommendation using EHRs has been a salient research direction, which has attracted researchers to apply various deep learning (DL) models to the EHRs of patients in recommending prescriptions. Yet, in the absence of a holistic survey article, it needs a lot of effort and time to study these publications in order to understand the current state of research and identify the best-performing models along with the trends and challenges. To fill this research gap, this survey reports on state-of-the-art DL-based medication recommendation methods. It reviews the classification of DL-based medication recommendation (MR) models, compares their performance, and the unavoidable issues they face. It reports on the most common datasets and metrics used in evaluating MR models. The findings of this study have implications for researchers interested in MR models.},
  archive      = {J_DINT},
  author       = {Ali, Zafar and Huang, Yi and Ullah, Irfan and Feng, Junlan and Deng, Chao and Thierry, Nimbeshaho and Khan, Asad and Jan, Asim Ullah and Shen, Xiaoli and Rui, Wu and Qi, Guilin},
  doi          = {10.1162/dint_a_00197},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {303-354},
  shortjournal = {Data Intell.},
  title        = {Deep learning for medication recommendation: A systematic survey},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metadata as a methodological commons: From aboutness
description to cognitive modeling. <em>DINT</em>, <em>5</em>(1),
289–302. (<a href="https://doi.org/10.1162/dint_a_00189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Metadata is data about data, which is generated mainly for resources organization and description, facilitating finding, identifying, selecting and obtaining information①. With the advancement of technologies, the acquisition of metadata has gradually become a critical step in data modeling and function operation, which leads to the formation of its methodological commons. A series of general operations has been developed to achieve structured description, semantic encoding and machine-understandable information, including entity definition, relation description, object analysis, attribute extraction, ontology modeling, data cleaning, disambiguation, alignment, mapping, relating, enriching, importing, exporting, service implementation, registry and discovery, monitoring etc. Those operations are not only necessary elements in semantic technologies (including linked data) and knowledge graph technology, but has also developed into the common operation and primary strategy in building independent and knowledge-based information systems.In this paper, a series of metadata-related methods are collectively referred to as ‘metadata methodological commons’, which has a lot of best practices reflected in the various standard specifications of the Semantic Web. In the future construction of a multi-modal metaverse based on Web 3.0, it shall play an important role, for example, in building digital twins through adopting knowledge models, or supporting the modeling of the entire virtual world, etc. Manual-based description and coding obviously cannot adapted to the UGC (User Generated Contents) and AIGC (AI Generated Contents)-based content production in the metaverse era. The automatic processing of semantic formalization must be considered as a sure way to adapt metadata methodological commons to meet the future needs of AI era.},
  archive      = {J_DINT},
  author       = {Liu, Wei and Fu, Yaming and Liu, Qianqian},
  doi          = {10.1162/dint_a_00189},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {289-302},
  shortjournal = {Data Intell.},
  title        = {Metadata as a methodological commons: From aboutness description to cognitive modeling},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous metadata in continuous integration, stream
processing and enterprise DataOps. <em>DINT</em>, <em>5</em>(1),
275–288. (<a href="https://doi.org/10.1162/dint_a_00193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Implementations of metadata tend to favor centralized, static metadata. This depiction is at variance with the past decade of focus on big data, cloud native architectures and streaming platforms. Big data velocity can demand a correspondingly dynamic view of metadata. These trends, which include DevOps, CI/CD, DataOps and data fabric, are surveyed. Several specific cloud native tools are reviewed and weaknesses in their current metadata use are identified. Implementations are suggested which better exploit capabilities of streaming platform paradigms, in which metadata is continuously collected in dynamic contexts. Future cloud native software features are identified which could enable streamed metadata to power real time data fusion or fine tune automated reasoning through real time ontology updates.},
  archive      = {J_DINT},
  author       = {Underwood, Mark},
  doi          = {10.1162/dint_a_00193},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {275-288},
  shortjournal = {Data Intell.},
  title        = {Continuous metadata in continuous integration, stream processing and enterprise DataOps},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving transparency: A metadata perspective.
<em>DINT</em>, <em>5</em>(1), 261–274. (<a
href="https://doi.org/10.1162/dint_a_00188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Transparency is vital to realizing the promise of evidenced-based policymaking, where “evidence-based” means including information as to what data mean and why they should be trusted. Transparency, in turn, requires that enough of this information is provided. Loosely speaking then, transparency is achieved when sufficient documentation is provided. Sufficiency is situation specific, both for the provider and consumer of the documentation. These ideas are presented in two recent US commissioned reports: The Promise of Evidence-Based Policymaking and Transparency in Statistical Information for the National Center for Science and Engineering Statistics and All Federal Statistical Agencies.Metadata are a more formalized kind of documentation, and in this paper, we provide and demonstrate necessary, sufficient, and general conditions for achieving transparency from the metadata perspective: conforming to a specification, providing quality metadata, and creating a usable interface to the metadata. These conditions are important for any metadata system, but here the specification is tied to our framework for metadata quality based on the situation-specific needs for transparency. These ideas are described, and their interrelationships are explored.},
  archive      = {J_DINT},
  author       = {Gillman, Daniel},
  doi          = {10.1162/dint_a_00188},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {261-274},
  shortjournal = {Data Intell.},
  title        = {Achieving transparency: A metadata perspective},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building community consensus for scientific metadata with
YAMZ. <em>DINT</em>, <em>5</em>(1), 242–260. (<a
href="https://doi.org/10.1162/dint_a_00211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. This paper reports on a demonstration of YAMZ (Yet Another Metadata Zoo) as a mechanism for building community consensus around metadata terms. The demonstration is motivated by the complexity of the metadata standards environment and the need for more user-friendly approaches for researchers to achieve vocabulary consensus. The paper reviews a series of metadata standardization challenges, explores crowdsourcing factors that offer possible solutions, and introduces the YAMZ system. A YAMZ demonstration is presented with members of the Toberer materials science laboratory at the Colorado School of Mines, where there is a need to confirm and maintain a shared understanding for the vocabulary supporting research documentation, data management, and their larger metadata infrastructure. The demonstration involves three key steps: 1) Sampling terms for the demonstration, 2) Engaging graduate student researchers in the demonstration, and 3) Reflecting on the demonstration. The results of these steps, including examples of the dialog provenance among lab members and voting, show the ease with YAMZ can facilitate building metadata vocabulary consensus. The conclusion discusses implications and highlights next steps.},
  archive      = {J_DINT},
  author       = {Greenberg, Jane and McClellan, Scott and Rauch, Christopher and Zhao, Xintong and Kelly, Mat and An, Yuan and Kunze, John and Orenstein, Rachel and Porter, Claire and Meschke, Vanessa and Toberer, Eric},
  doi          = {10.1162/dint_a_00211},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {242-260},
  shortjournal = {Data Intell.},
  title        = {Building community consensus for scientific metadata with YAMZ},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAIREST: A framework for assessing research repositories.
<em>DINT</em>, <em>5</em>(1), 202–241. (<a
href="https://doi.org/10.1162/dint_a_00159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. The open science movement has gained significant momentum within the last few years. This comes along with the need to store and share research artefacts, such as publications and research data. For this purpose, research repositories need to be established. A variety of solutions exist for implementing such repositories, covering diverse features, ranging from custom depositing workflows to social media-like functions.In this article, we introduce the FAIREST principles, a framework inspired by the well-known FAIR principles, but designed to provide a set of metrics for assessing and selecting solutions for creating digital repositories for research artefacts. The goal is to support decision makers in choosing such a solution when planning for a repository, especially at an institutional level. The metrics included are therefore based on two pillars: (1) an analysis of established features and functionalities, drawn from existing dedicated, general purpose and commonly used solutions, and (2) a literature review on general requirements for digital repositories for research artefacts and related systems. We further describe an assessment of 11 widespread solutions, with the goal to provide an overview of the current landscape of research data repository solutions, identifying gaps and research challenges to be addressed.},
  archive      = {J_DINT},
  author       = {d&#39;Aquin, Mathieu and Kirstein, Fabian and Oliveira, Daniela and Schimmler, Sonja and Urbanek, Sebastian},
  doi          = {10.1162/dint_a_00159},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {202-241},
  shortjournal = {Data Intell.},
  title        = {FAIREST: A framework for assessing research repositories},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The FAIR data point: Interfaces and tooling. <em>DINT</em>,
<em>5</em>(1), 184–201. (<a
href="https://doi.org/10.1162/dint_a_00161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. While the FAIR Principles do not specify a technical solution for ‘FAIRness’, it was clear from the outset of the FAIR initiative that it would be useful to have commodity software and tooling that would simplify the creation of FAIR-compliant resources. The FAIR Data Point is a metadata repository that follows the DCAT(2) schema, and utilizes the Linked Data Platform to manage the hierarchical metadata layers as LDP Containers. There has been a recent flurry of development activity around the FAIR Data Point that has significantly improved its power and ease-of-use. Here we describe five specific tools—an installer, a loader, two Web-based interfaces, and an indexer—aimed at maximizing the uptake and utility of the FAIR Data Point.},
  archive      = {J_DINT},
  author       = {Benhamed, Oussama Mohammed and Burger, Kees and Kaliyaperumal, Rajaram and da Silva Santos, Luiz Olavo Bonino and Suchánek, Marek and Slifka, Jan and Wilkinson, Mark D.},
  doi          = {10.1162/dint_a_00161},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {184-201},
  shortjournal = {Data Intell.},
  title        = {The FAIR data point: Interfaces and tooling},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAIR data point: A FAIR-oriented approach for metadata
publication. <em>DINT</em>, <em>5</em>(1), 163–183. (<a
href="https://doi.org/10.1162/dint_a_00160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Metadata, data about other digital objects, play an important role in FAIR with a direct relation to all FAIR principles. In this paper we present and discuss the FAIR Data Point (FDP), a software architecture aiming to define a common approach to publish semantically-rich and machine-actionable metadata according to the FAIR principles. We present the core components and features of the FDP, its approach to metadata provision, the criteria to evaluate whether an application adheres to the FDP specifications and the service to register, index and allow users to search for metadata content of available FDPs.},
  archive      = {J_DINT},
  author       = {da Silva Santos, Luiz Olavo Bonino and Burger, Kees and Kaliyaperumal, Rajaram and Wilkinson, Mark D.},
  doi          = {10.1162/dint_a_00160},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {163-183},
  shortjournal = {Data Intell.},
  title        = {FAIR data point: A FAIR-oriented approach for metadata publication},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provenance documentation to enable explainable and
trustworthy AI: A literature review. <em>DINT</em>, <em>5</em>(1),
139–162. (<a href="https://doi.org/10.1162/dint_a_00119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Recently artificial intelligence (AI) and machine learning (ML) models have demonstrated remarkable progress with applications developed in various domains. It is also increasingly discussed that AI and ML models and applications should be transparent, explainable, and trustworthy. Accordingly, the field of Explainable AI (XAI) is expanding rapidly. XAI holds substantial promise for improving trust and transparency in AI-based systems by explaining how complex models such as the deep neural network (DNN) produces their outcomes. Moreover, many researchers and practitioners consider that using provenance to explain these complex models will help improve transparency in AI-based systems. In this paper, we conduct a systematic literature review of provenance, XAI, and trustworthy AI (TAI) to explain the fundamental concepts and illustrate the potential of using provenance as a medium to help accomplish explainability in AI-based systems. Moreover, we also discuss the patterns of recent developments in this area and offer a vision for research in the near future. We hope this literature review will serve as a starting point for scholars and practitioners interested in learning about essential components of provenance, XAI, and TAI.},
  archive      = {J_DINT},
  author       = {Kale, Amruta and Nguyen, Tin and Harris, Frederick C. and Li, Chenhao and Zhang, Jiyin and Ma, Xiaogang},
  doi          = {10.1162/dint_a_00119},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {139-162},
  shortjournal = {Data Intell.},
  title        = {Provenance documentation to enable explainable and trustworthy AI: A literature review},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated metadata annotation: What is and is not possible
with machine learning. <em>DINT</em>, <em>5</em>(1), 122–138. (<a
href="https://doi.org/10.1162/dint_a_00162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Automated metadata annotation is only as good as training dataset, or rules that are available for the domain. It&#39;s important to learn what type of data content a pre-trained machine learning algorithm has been trained on to understand its limitations and potential biases. Consider what type of content is readily available to train an algorithm—what&#39;s popular and what&#39;s available. However, scholarly and historical content is often not available in consumable, homogenized, and interoperable formats at the large volume that is required for machine learning. There are exceptions such as science and medicine, where large, well documented collections are available. This paper presents the current state of automated metadata annotation in cultural heritage and research data, discusses challenges identified from use cases, and proposes solutions.},
  archive      = {J_DINT},
  author       = {Wu, Mingfang and Brandhorst, Hans and Marinescu, Maria-Cristina and Lopez, Joaquim More and Hlava, Margorie and Busch, Joseph},
  doi          = {10.1162/dint_a_00162},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {122-138},
  shortjournal = {Data Intell.},
  title        = {Automated metadata annotation: What is and is not possible with machine learning},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analysis of crosswalks from research data schemas to
schema.org. <em>DINT</em>, <em>5</em>(1), 100–121. (<a
href="https://doi.org/10.1162/dint_a_00186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. The increased number of data repositories has greatly increased the availability of open data. To enable broad discovery and access to research dataset, some data repositories have begun leveraging the web architecture by embedding structured metadata markup in dataset web landing pages using vocabularies from Schema.org and extensions. This paper aims to examine metadata interoperability for supporting global data discovery. Specifically, the paper reports a survey on which metadata schema has been adopted by participating data repositories, and presents an analysis of crosswalks from fourteen research data schemas to Schema.org. The analysis indicates most descriptive metadata are interoperable among the schemas, the most inconsistent mapping is the rights metadata, and a large gap exists in the structural metadata and controlled vocabularies to specify various property values. The analysis and collated crosswalks can serve as a reference for data repositories when they develop crosswalks from their own schemas to Schema.org, and provide the research data community a benchmark of structured metadata implementation.},
  archive      = {J_DINT},
  author       = {Wu, Mingfang and Richard, Stephen M. and Verhey, Chantelle and Castro, Leyla Jael and Cecconi, Baptiste and Juty, Nick},
  doi          = {10.1162/dint_a_00186},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {100-121},
  shortjournal = {Data Intell.},
  title        = {An analysis of crosswalks from research data schemas to schema.org},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on intelligent organization and application of
multi-source heterogeneous knowledge resources for energy internet.
<em>DINT</em>, <em>5</em>(1), 75–99. (<a
href="https://doi.org/10.1162/dint_a_00158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. To improve the informationization and intelligence of the energy Internet industry and enhance the capability of knowledge services, it is necessary to organize the energy Internet body of knowledge from existing knowledge resources of the State Grid, which have the characteristics of large scale, multiple sources, and heterogeneity. At the same time, the business fields of State Grid cover a wide range. There are many sub-fields under each business field, and the relationship between fields is diverse and complex. The key to establishing the energy Internet body of knowledge is how to fuse the heterogeneous knowledge resources from multiple sources, extract the knowledge contents from them, and organize the different relationships. This paper considers transforming the original knowledge resources of State Grid into a unified and well-organized knowledge system described in OWL language to meet the requirements of heterogeneous resource integration, multi-source resource organization, and knowledge service provision. For the State Grid knowledge resources mainly in XML format, this paper proposes a Knowledge Automatic Fusion and Organization idea and method based on XSD Directed Graph. According to the method, the XML corresponding XSD documents are transformed into a directed graph in the first stage during which the graph neural network detects hidden knowledge inside the structure to add semantic information to the graph.In the second stage, for other structured knowledge resources (e.g., databases, spreadsheets), the knowledge contents and the relationships are analyzed manually to establish the mappings from structured resources to graph structures, using which the original knowledge resources are transformed into graph structures, and merged with the directed graphs obtained in the first stage to achieve the fusion of heterogeneous knowledge resources. And expert knowledge is introduced for heterogeneous knowledge fusion to further extend the directed graph. And in the third stage, the expanded directed graph is converted to the body of knowledge in the form of OWL. This paper takes the knowledge resources in the field of human resources of the State Grid as an example, to establish the ontology of the human resources training field in a unified manner, initially demonstrating the effectiveness of the proposed method.},
  archive      = {J_DINT},
  author       = {Yuxuan, Wang and Liqun, Luo and Guangjian, Li},
  doi          = {10.1162/dint_a_00158},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {75-99},
  shortjournal = {Data Intell.},
  title        = {Research on intelligent organization and application of multi-source heterogeneous knowledge resources for energy internet},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAIR data and metadata: GNSS precise positioning user
perspective. <em>DINT</em>, <em>5</em>(1), 43–74. (<a
href="https://doi.org/10.1162/dint_a_00185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. The FAIR principles of Wilkinson et al. [1] are finding their way from research into application domains, one of which is the precise positioning with global satellite navigation systems (GNSS). Current GNSS users demand that data and services are findable online, accessible via open protocols (by both, machines and humans), interoperable with their legacy systems and reusable in various settings. Comprehensive metadata are essential in seamless communication between GNSS data and service providers and their users, and, for decades, geodetic and geospatial standards are efficiently implemented to support this. However, GNSS user community is transforming from precise positioning by highly specialised use by geodetic professionals to every-day precise positioning by autonomous vehicles or wellness obsessed citizens. Moreover, rapid technological developments allow alternative ways of offering data and services to their users. These transforming circumstances warrant a review whether metadata defined in generic geospatial and geodetic standards in use still support FAIR use of modern GNSS data and services across its novel user spectrum. This paper reports the results of current GNSS users’ requirements in various application sectors on the way data, metadata and services are provided. We engaged with GNSS stakeholders to validate our findings and to gain understanding on their perception of the FAIR principles. Our results confirm that offering FAIR GNSS data and services is fundamental, but for a confident use of these, there is a need to review the way metadata are offered to the community. Defining standard compliant GNSS community metadata profile and providing relevant metadata with data on-demand, the approach outlined in this paper, is a way to manage current GNSS users’ expectations and the way to improve FAIR GNSS data and service delivery for both humans and the machines.},
  archive      = {J_DINT},
  author       = {Ivánová, Ivana and Keenan, Ryan and Marshall, Christopher and Mancell, Lori and Rubinov, Eldar and Ruddick, Ryan and Brown, Nicholas and Kernich, Graeme},
  doi          = {10.1162/dint_a_00185},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {43-74},
  shortjournal = {Data Intell.},
  title        = {FAIR data and metadata: GNSS precise positioning user perspective},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards the FAIRification of scanning tunneling microscopy
images. <em>DINT</em>, <em>5</em>(1), 27–42. (<a
href="https://doi.org/10.1162/dint_a_00164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. In this paper, we describe the data management practices and services developed for making FAIR compliant a scientific archive of Scanning Tunneling Microscopy (STM) images. As a first step, we extracted the instrument metadata of each image of the dataset to create a structured database. We then enriched these metadata with information on the structure and composition of the surface by means of a pipeline that leverages human annotation, machine learning techniques, and instrument metadata filtering. To visually explore both images and metadata, as well as to improve the accessibility and usability of the dataset, we developed “STM explorer” as a web service integrated within the Trieste Advanced Data services (TriDAS) website. On top of these data services and tools, we propose an implementation of the W3C PROV standard to describe provenance metadata of STM images.},
  archive      = {J_DINT},
  author       = {Rodani, Tommaso and Osmenaj, Elda and Cazzaniga, Alberto and Panighel, Mirco and Cristina, Africh and Cozzini, Stefano},
  doi          = {10.1162/dint_a_00164},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {27-42},
  shortjournal = {Data Intell.},
  title        = {Towards the FAIRification of scanning tunneling microscopy images},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving domain repository connectivity. <em>DINT</em>,
<em>5</em>(1), 6–26. (<a
href="https://doi.org/10.1162/dint_a_00120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ABSTRACT. Domain repositories, i.e. repositories that store, manage, and persist data pertaining to a specific scientific domain, are common and growing in the research landscape. Many of these repositories develop close, long-term communities made up of individuals and organizations that collect, analyze, and publish results based on the data in the repositories. Connections between these datasets, papers, people, and organizations are an important part of the knowledge infrastructure surrounding the repository.All these research objects, people, and organizations can now be identified using various unique and persistent identifiers (PIDs) and it is possible for domain repositories to build on their existing communities to facilitate and accelerate the identifier adoption process. As community members contribute to multiple datasets and articles, identifiers for them, once found, can be used multiple times.We explore this idea by defining a connectivity metric and applying it to datasets collected and papers published by members of the UNAVCO community. Finding identifiers in DataCite and Crossref metadata and spreading those identifiers through the UNAVCO DataCite metadata can increase connectivity from less than 10\% to close to 50\% for people and organizations.},
  archive      = {J_DINT},
  author       = {Habermann, Ted},
  doi          = {10.1162/dint_a_00120},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {6-26},
  shortjournal = {Data Intell.},
  title        = {Improving domain repository connectivity},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metadata as data intelligence. <em>DINT</em>, <em>5</em>(1),
1–5. (<a href="https://doi.org/10.1162/dint_e_00212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metadata, as a type of data, describes content, provides context, documents transactions, and situates data. Interest in metadata has steadily grown over the last several decades, motivated initially by the increase in digital information, open access, early data sharing policies, and interoperability goals. This foundation has accelerated in more recent times, due to the increase in research data management policies and advances in AI. Specific to research data management, one of the larger factors has been the global adoption of the FAIR (findable, accessible, interoperable, and reusable) data principles [1, 2], which are highly metadata-driven. Additionally, researchers across nearly every domain are interested in leveraging metadata for machine learning and other AI applications. The accelerated interest in metadata expands across other communities as well. For example, industry seeks metadata to meet company goals; and users of information systems and social computing applications wish to know how their metadata is being used and demand greater control of who has access to their data and metadata. All of these developments underscore the fact that metadata is intelligent data, or what Riley has called value added data [3]. Overall, this intense and growing interest in metadata helps to frame the contributions included in this special issue of Data Intelligence.This special issue of Data Intelligence includes a collection of 14 original articles covering metadata related research, practice, and theory. The contributions as a whole, include work complied by over 50 authors from eleven countries, including Australia, Canada, China, France, Germany, Italy, the Netherlands, Portugal, Spain, the United Kingdoms, and the United States. Contributing authors are from range of organizations, national research data programs, such as the Australian Research Data Commons, academic and research institutions, such as the Chinese Academy of Sciences; government agencies and industry such as the United States Bureau of Labor Statistics among other national agencies. Among some of the overriding themes, contributions address FAIR principles in relation to metadata; metadata tools, practice or polices, innovative ideas, and theoretical approaches. The majority of contributions are arranged under “research” and “practice and implementation”, and with three final contributions covering “vision and theory”.Research in the study of any phenomena confirms a level of maturity. The rich collection of research articles in this special issue confirm that metadata work has expanded well beyond scheme development and implementation, and that metadata is a significant research topic [4, 5].The series of research articles is initiated with a piece entitled, Improving Domain Repository Connectivity [6]. This contribution explores the notion of a connectivity metric, which is applied it to datasets collected and papers published by members of the UNAVCO community. The author explains that “as community members contribute to multiple datasets and articles, identifiers for them, once found, can be used multiple times”. Identifiers found in DataCite and Crossref metadata that are shared through UNAVCO DataCite metadata can increase connectivity from less than 10\% to nearly 50\% for people and organizations.Two of the research contributions interconnect with FAIR. First, FAIRification of Scanning Tunneling Microscopy [7] focuses on data management practices and services for making FAIR compliant a scientific archive of Scanning Tunneling Microscopy (STM) images. The authors report on a metadata database that includes metadata extracted from instruments and each image, which have been enriched via human annotation, machine learning techniques, and instrument metadata filtering. Additionally, the W3C PROV standard was explored for STM image.A second work, FAIR Data and Metadata: GNSS Precise Positioning User Perspective [8], presents an analysis of current GNSS users’ requirements in various application sectors on the way data, metadata and services are provided. We engaged with GNSS stakeholders to validate our findings and to gain understanding on their perception of the FAIR principles. Authors indicate that results confirm FAIR GNSS data and services are important for this community and have have had an impact standard compliant GNSS community metadata enabling FAIR GNSS data and service delivery for both humans and the machines.Another research piece includes, Research on Intelligent Organization and Application of Multi-source Heterogeneous Knowledge Resources for Energy Internet [9] focuses on improving the informaionization and intelligence of the energy Internet industry for enhancing the capability of knowledge services. The authors propose methods to synthesis and transform the original multiple, heterogeneous knowledge resources of the State Grid into a unified and well-organized knowledge system. The effectiveness of the proposed methods are demonstrated with knowledge resources in the field of human resources of the State Grid.The collection of research articles also include, An analysis of crosswalks from research data schemas to Schema.org [10]. This work presents an extensive analysis of crosswalks from fourteen research data schemas to Schema.org. The analysis indicates that most descriptive metadata are interoperable among the schemas, the most inconsistent mapping is the rights metadata, and a large gap exists in the structural metadata and controlled vocabularies to specify various property values. The analysis and collated crosswalks can serve as a reference for data repositories when they develop crosswalks from their own schemas to Schema.org, and provide the research data community a benchmark of structured metadata implementation.The research cluster also includes a contribution that underscores metadata as data intelligence with attention to AI/ML methods. The work entitled Automated metadata annotation: What is and is not possible with machine learning [11] presents three use cases that demonstrate the possibility of utilizing AI/ML models in improving subject indexing of culture or data catalogs, and it requires bringing process, technology and interdisciplinary team together to achieve quality of automated subject.The last contribution in the research cluster: Provenance documentation to enable explainable and trustworthy AI: A literature review [12] discusses the importance of capturing and providing provenance information within the context of running AI/ML models, for making AI/ML results explainable, trustworthy and reproducible by capturing provenance metadata about each step of the AI process (e.g. data, AI models, software source code for data preparation and executing models).The practice and implementation articles included in this special issue report on innovative approaches and developing technologies. Three pieces contributions that target practices focus on FAIR principles. The FAIR (data)meta principles provide guidance to the documentation of metadata so that data can be Findable, Accessible, Interoperable and Reusable, by both human and machine. Making metadata fair is a progressive process, it requires to gauge gaps and set up goals for improvement, and methods and tools to assist the process. Two of the FAIR contributions are about tools relating to data repositories “FAIR Data Point: A FAIR-Oriented Approach for Metadata Publication” [13] and “The FAIR Data Point: Interfaces and Toolings” [14] introduce the FAIR Data Point (FDP)—a general-purpose metadata repository that follows the DCAT schema and has been implemented by following the FAIR principles. The first paper introduces the software architecture, core components and features of the FAIR Data Point (FDP), and the second paper describes interfaces and tools for implementing FDP and facilitating the uptakes and utilities of FDP. The two papers will benefit those who want to adopt a metadata repository solution or existing metadata repositories for extending their repository functionalities for publishing semantically-rich and machine-actionable metadata by following the FAIR principles.The third practice paper addressing FAIR, FAIREST: A Framework for Assessing Research Repositories [15], introduces a set of metrics for assessing and selecting solutions for creating digital repositories for research artifacts. The metrics are built on the FAIR principles, Engagement, Social Connections and Trust (this FAIREST). The paper also applied metrics for an assessment of 11 widespread solutions, with the goal to provide an overview of the current landscape of research data repository solutions, identifying gaps and research challenges to be addressed.The last contribution in the practice and implementation contribution, Building Community Consensus for Scientific Metadata with YAMZ [16], introduces YAMZ (Yet Another Metadata Zoo). YAMZ was developed to help address the challenges with the formal metadata standardization process. The paper presents an exploratory demonstration of YAMZ within an academic research lab, where there is a need to standardize metadata to help with data management activities, but researchers lack of time and metadata expertise to proceed through the formal standardization process.Three contributions cover vision and theory. The work, Achieving Transparency: A Metadata Perspective [17], discusses what information should be captured in metadata (schema) and in consistent way (technical specification) to ensure metadata quality and transparency of data; in order to communicate better what data mean and why they should be trusted, within the context of providing datasets from the government and government agencies.The work Continuous Metadata in Continuous Integration, Stream Processing and Enterprise Data Ops [18] argues that metadata is continuous in many real data context, thus one-off metadata collection may be inadequate for future analysis. Based on the review of some current tools in specifying, capturing and consuming metadata; the author suggests features and design patterns for future cloud native software, which could enable streamed metadata to power real time data fusion or fine turn automated reasoning through real time ontology updates.Finally, a contribution entitled, Metadata as a Methodological Commons: From Aboutness Description to Cognitive Modeling [19], discusses the requirement and feasibility for semantic coding and cognitive metadata modeling, as the rise of huge volume of labeled data and ChatGPT, as well as the availability of emerging technologies (e.g Web 3.0, AI/ML, knowledge graph).Metadata is often seen as a practical topic, not worthy of research and, yet, it is a topic that is central to data driven research [18]. This issue of Data Intelligence underscores the significance of metadata as a research worthy and critical topic in advancing our data infrastructure and achieving greater ‘data intelligence’. In closing, this issue presents a unique collection of articles and that confirms the importance of metadata research, practice and implementation, and visionary and theory as we aim to collectively advance our data infrastructure across all information sectors.},
  archive      = {J_DINT},
  author       = {Greenberg, Jane and Wu, Mingfang and Liu, Wei and Liu, Fenghong},
  doi          = {10.1162/dint_e_00212},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Data Intell.},
  title        = {Metadata as data intelligence},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
