<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COLI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coli---30">COLI - 30</h2>
<ul>
<li><details>
<summary>
(2023). Language embeddings sometimes contain typological
generalizations. <em>COLI</em>, <em>49</em>(4), 1003–1051. (<a
href="https://doi.org/10.1162/coli_a_00491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To what extent can neural network models learn generalizations about language structure, and how do we find out what they have learned? We explore these questions by training neural models for a range of natural language processing tasks on a massively multilingual dataset of Bible translations in 1,295 languages. The learned language representations are then compared to existing typological databases as well as to a novel set of quantitative syntactic and morphological features obtained through annotation projection. We conclude that some generalizations are surprisingly close to traditional features from linguistic typology, but that most of our models, as well as those of previous work, do not appear to have made linguistically meaningful generalizations. Careful attention to details in the evaluation turns out to be essential to avoid false positives. Furthermore, to encourage continued work in this field, we release several resources covering most or all of the languages in our data: (1) multiple sets of language representations, (2) multilingual word embeddings, (3) projected and predicted syntactic and morphological features, (4) software to provide linguistically sound evaluations of language representations.},
  archive      = {J_COLI},
  author       = {Östling, Robert and Kurfalı, Murathan},
  doi          = {10.1162/coli_a_00491},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {1003-1051},
  shortjournal = {Comput. Lingu.},
  title        = {Language embeddings sometimes contain typological generalizations},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Languages through the looking glass of BPE compression.
<em>COLI</em>, <em>49</em>(4), 943–1001. (<a
href="https://doi.org/10.1162/coli_a_00489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byte-pair encoding (BPE) is widely used in NLP for performing subword tokenization. It uncovers redundant patterns for compressing the data, and hence alleviates the sparsity problem in downstream applications. Subwords discovered during the first merge operations tend to have the most substantial impact on the compression of texts. However, the structural underpinnings of this effect have not been analyzed cross-linguistically. We conduct in-depth analyses across 47 typologically diverse languages and three parallel corpora, and thereby show that the types of recurrent patterns that have the strongest impact on compression are an indicator of morphological typology. For languages with richer inflectional morphology there is a preference for highly productive subwords on the early merges, while for languages with less inflectional morphology, idiosyncratic subwords are more prominent. Both types of patterns contribute to efficient compression. Counter to the common perception that BPE subwords are not linguistically relevant, we find patterns across languages that resemble those described in traditional typology. We thus propose a novel way to characterize languages according to their BPE subword properties, inspired by the notion of morphological productivity in linguistics. This allows us to have language vectors that encode typological knowledge induced from raw text. Our approach is easily applicable to a wider range of languages and texts, as it does not require annotated data or any external linguistic knowledge. We discuss its potential contributions to quantitative typology and multilingual NLP.},
  archive      = {J_COLI},
  author       = {Gutierrez-Vasques, Ximena and Bentz, Christian and Samardžić, Tanja},
  doi          = {10.1162/coli_a_00489},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {943-1001},
  shortjournal = {Comput. Lingu.},
  title        = {Languages through the looking glass of BPE compression},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing fine-grained regional differences in language use
through voting precinct embeddings. <em>COLI</em>, <em>49</em>(4),
883–942. (<a href="https://doi.org/10.1162/coli_a_00487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linguistic variation across a region of interest can be captured by partitioning the region into areas and using social media data to train embeddings that represent language use in those areas. Recent work has focused on larger areas, such as cities or counties, to ensure that enough social media data is available in each area, but larger areas have a limited ability to find fine-grained distinctions, such as intracity differences in language use. We demonstrate that it is possible to embed smaller areas, which can provide higher resolution analyses of language variation. We embed voting precincts, which are tiny, evenly sized political divisions for the administration of elections. The issue with modeling language use in small areas is that the data becomes incredibly sparse, with many areas having scant social media data. We propose a novel embedding approach that alternates training with smoothing, which mitigates these sparsity issues. We focus on linguistic variation across Texas as it is relatively understudied. We develop two novel quantitative evaluations that measure how well the embeddings can be used to capture linguistic variation. The first evaluation measures how well a model can map a dialect given terms specific to that dialect. The second evaluation measures how well a model can map preference of lexical variants. These evaluations show how embedding models could be used directly by sociolinguists and measure how much sociolinguistic information is contained within the embeddings. We complement this second evaluation with a methodology for using embeddings as a kind of genetic code where we identify “genes” that correspond to a sociological variable and connect those “genes” to a linguistic phenomenon thereby connecting sociological phenomena to linguistic ones. Finally, we explore approaches for inferring isoglosses using embeddings.},
  archive      = {J_COLI},
  author       = {Rosenfeld, Alex and Hinrichs, Lars},
  doi          = {10.1162/coli_a_00487},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {883-942},
  shortjournal = {Comput. Lingu.},
  title        = {Capturing fine-grained regional differences in language use through voting precinct embeddings},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generation and polynomial parsing of graph languages with
non-structural reentrancies. <em>COLI</em>, <em>49</em>(4), 841–882. (<a
href="https://doi.org/10.1162/coli_a_00488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semantic representations are popular in natural language processing, where it is often convenient to model linguistic concepts as nodes and relations as edges between them. Several attempts have been made to find a generative device that is sufficiently powerful to describe languages of semantic graphs, while at the same allowing efficient parsing. We contribute to this line of work by introducing graph extension grammar, a variant of the contextual hyperedge replacement grammars proposed by Hoffmann et al. Contextual hyperedge replacement can generate graphs with non-structural reentrancies , a type of node-sharing that is very common in formalisms such as abstract meaning representation, but that context-free types of graph grammars cannot model. To provide our formalism with a way to place reentrancies in a linguistically meaningful way, we endow rules with logical formulas in counting monadic second-order logic. We then present a parsing algorithm and show as our main result that this algorithm runs in polynomial time on graph languages generated by a subclass of our grammars, the so-called local graph extension grammars.},
  archive      = {J_COLI},
  author       = {Björklund, Johanna and Drewes, Frank and Jonsson, Anna},
  doi          = {10.1162/coli_a_00488},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {841-882},
  shortjournal = {Comput. Lingu.},
  title        = {Generation and polynomial parsing of graph languages with non-structural reentrancies},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring attribution in natural language generation models.
<em>COLI</em>, <em>49</em>(4), 777–840. (<a
href="https://doi.org/10.1162/coli_a_00486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large neural models have brought a new challenge to natural language generation (NLG): It has become imperative to ensure the safety and reliability of the output of models that generate freely. To this end, we present an evaluation framework, Attributable to Identified Sources (AIS), stipulating that NLG output pertaining to the external world is to be verified against an independent, provided source. We define AIS and a two-stage annotation pipeline for allowing annotators to evaluate model output according to annotation guidelines. We successfully validate this approach on generation datasets spanning three tasks (two conversational QA datasets, a summarization dataset, and a table-to-text dataset). We provide full annotation guidelines in the appendices and publicly release the annotated data at https://github.com/google-research-datasets/AIS .},
  archive      = {J_COLI},
  author       = {Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  doi          = {10.1162/coli_a_00486},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {777-840},
  shortjournal = {Comput. Lingu.},
  title        = {Measuring attribution in natural language generation models},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). My tenure as the editor-in-chief of computational
linguistics. <em>COLI</em>, <em>49</em>(4), 773–775. (<a
href="https://doi.org/10.1162/coli_e_00505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Times flies and it has been close to five and a half years since I became the editor-in-chief of Computational Linguistics on 15 July 2018. In this editorial, I will describe the changes that I have introduced at the journal, and highlight the achievements and challenges of the journal.},
  archive      = {J_COLI},
  author       = {Ng, Hwee Tou},
  doi          = {10.1162/coli_e_00505},
  journal      = {Computational Linguistics},
  number       = {4},
  pages        = {773-775},
  shortjournal = {Comput. Lingu.},
  title        = {My tenure as the editor-in-chief of computational linguistics},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obituary: Yorick wilks. <em>COLI</em>, <em>49</em>(3),
767–772. (<a href="https://doi.org/10.1162/coli_a_00485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Yorick Wilks died peacefully on 14 April 2023 at home in Oxford. Although he had been in declining health for some time, he remained active in AI and computational linguistics right up until his death. A new book (on AI and religion) will be published shortly.sYorick had been continuously active in the field since 1962, was a member of the International Committee for Computational Linguistics from the 1970s until his death, and was an ACL Lifetime Achievement Award winner (2008).He made a huge range of contributions to the field, from early work on preference semantics and machine translation to later work on the use of machine readable dictionaries in computational linguistics, word sense disambiguation, frameworks for natural language processing (GATE), message understanding and information extraction, artificial companions, and much else besides. He had broader interests in artificial intelligence and the philosophy of language, especially Wittgenstein’s later philosophy.This breadth might make him seem like some sort of butterfly—not a bit of it. He has been described (accurately in our view) as a heavyweight hiding in lightweight’s clothing.He wrote prodigiously on many topics, sometimes under pseudonyms. He was a talented actor, who once considered pursuing acting as a profession rather than research and academia. He had an excellent voice and some still think he should have pursued opera or operetta as a career, and that he wasted his life on these utopian and, for so much of his life, practically unworkable subjects of artificial intelligence and computational linguistics. Recently, of course, the widespread interest in ChatGPT and other similar systems have vindicated his persistence.Wilks was a formidable debater, contributing to discussions of politics, economics, and other areas of society with a sharp mind and broad knowledge. He could also be a charming and entertaining conversationalist with people from all walks of life. Born of working class parents from Edmonton, North London, just after the start of World War II, Yorick seized the opportunities present in post-war Britain, despite the shadow cast by the early death of his father. From Torquay Boys’ Grammar School he obtained a scholarship to study at Pembroke College, Cambridge. He had gone up to Cambridge to study maths, but in his first year he discovered that what was exceptional maths ability at Torquay was merely average in Cambridge. He therefore swapped to Philosophy, where his tutor was Margaret Masterman, who introduced him to Wittgenstein’s late philosophy of language.Masterman ran a small private research lab called the Cambridge Language Research Unit (CLRU) slightly to the west of the main Cambridge Colleges. The CLRU existed from the 1950s to undertake work on machine translation and computational linguistics, mainly funded from military and intelligence sources, including from the United States.Immediately after completing his undergraduate studies in 1962, Wilks was engaged by Masterman to work on one of these projects, and in parallel enrolled for a Ph.D. with the university.The CLRU was an extraordinary place in that era: It started the careers of two other ACL Lifetime Achievement Award winners (Martin Kay and Karen Spärck Jones) as well as promoting and undertaking some of the very first work on machine learning, machine readable thesaurii and dictionaries, and much else besides.1However, the CLRU had poor access to even the puny computers available at the time. This problem led Wilks to obtain a position at the much better resourced System Development Corporation in California, which he took up in 1965. He obtained his Ph.D. for a thesis entitled “Argument and Proof” in 1968. He also began developing his preference semantics parser written in LISP, which occupied much of his time and energy for the next few years (Wilks 1972, 1975a, 1975b, 1978), in parallel with his work on machine translation (Wilks 1973a, b), which he viewed as the ideal testing ground for his approach to computational semantic analysis.By the early 1970s he was working at Stanford, and then moved to Switzerland and then Edinburgh. This was all on soft research money, and, being then in his middle thirties, married, and with a young son, Yorick felt the need to obtain an established academic position. So when a Readership at the University of Essex came up, where his Stanford buddies Pat Hayes and Bruce Anderson were already established, he applied. It was in Linguistics, but so what. He obtained the position and took it up in the autumn of 1976.On his first visit to Essex (presumably for his interview for the post in linguistics) he passed one of us (John Tait) and asked directions to the Department of Computer Science and Pat Hayes’s office. That moment changed Tait’s life. As a result of that chance meeting he obtained an unpublished paper of Yorick’s which he cited in a final year AI essay (and which he still has). That became his Ph.D. topic, helped get him a place to undertake a Ph.D. at Cambridge under Karen Spärck Jones, and was the foundation of much of his career.The then European Community had become interested in machine translation, and significant funding was available for relevant R&amp;amp;D, mainly under the EUROTRA program. Yorick built up a significant MT group at Essex which continued until recently. He also owned a wonderful 17th-century house in Clacton, a seaside resort not too far from the University, at which many legendary parties were held. However, for a variety of reasons Yorick decided to return to the United States and, in 1985, took up a position at New Mexico State University (NMSU) in Las Cruces as leader of the newly created Computing Research Laboratory.In New Mexico, Yorick built a world leading lab and provided the foundation of the careers of many leading figures in computational linguistics and related fields. Maybury (2007) contains many colorful details about this period of Yorick’s life. Much significant work was done by Yorick and his colleagues at this time. This included work on machine readable dictionaries (Wilks et al. 1990; Wilks, Slator, and Guthrie 1996) (seen at that time as a potential way of overcoming the scaling problems of earlier CL systems), belief systems (Ballim and Wilks 1991a, 1991b), machine translation (Farwell and Wilks 1991), and much else besides. A recurrent theme of accounts of his time in New Mexico was his tolerance and encouragement of able people with whose views he disagreed, which of course is not the case in many labs.In 1993, partly for personal reasons, and partly because the financial support to continue the lab in NMSU at its then level of activity was not forthcoming, Yorick decided to move back to the UK; a suitable professorial position came up at the University of Sheffield. Awarded a lectureship position to fill in order to support his appointment, Wilks chose another one of us (Robert Gaizauskas), inspiring Gaizauskas in just one meeting with his wit, breadth of knowledge, and passion for NLP and AI to move to Sheffield and take up an academic position, a position which led over the coming exciting and productive years of collaboration with Yorick to a full professorship and successful independent research career.Once again Yorick built up a world leading NLP research lab, this time in a department with an undistinguished computer science research history and few if any special opportunities for funding. His lab became a vibrant environment for numerous research assistants and Ph.D. students. This included one of us (Kalina Bontcheva) whom he supported in leading projects as a postdoc and then mentored and inspired to win a prestigious EPSRC fellowship2 and become a successful female professor.Notable work at Sheffield included proposing and building the widely used GATE toolkit for natural language processing (Gaizauskas et al. 1996), as well as pioneering research in information extraction, question answering and summarization (Gaizauskas and Wilks 1998), and on the related topic of the semantic web (Ciravegna et al. 2004; Brewster et al. 2004; Wilks 2008), particularly the derivation of its content and structure (i.e., ontologies) from text. Sheffield continues to be a world leading center for relevant work on these topics to this day. Another focus of interest at this time, reflecting Yorick’s long-standing interest in word meaning and context, was word sense disambiguation (Stevenson and Wilks 2001, 2005.In the early 2000s Yorick began to bring together earlier interests in chatbots and agents to propose the construction of artificial “Companions” to provide technological assistance for the elderly and lonely especially (Wilks 2005). This drew together several themes from earlier stages of Wilks’ career, especially agents (Ballim and Wilks 1991a), and his interest in PARRY (Colby 1975), one of the reasons he had gone to work at Stanford decades earlier, and was to absorb a large part of his creative energy at this time (Wilks 2010).In 2002, on stepping down as Head of Department in Sheffield, Yorick took a sabbatical in Oxford at the Oxford Internet Institute. He then began a slow transition from Sheffield to Oxford, retiring in stages, while living in Oxford.In 2008 he received the Association of Computational Linguistics Lifetime Achievement Award. Extraordinarily, this meant that three of the first seven winners of the Award began their careers in the CLRU.In 2009 he was awarded the British Computer Society’s Lovelace Medal, for outstanding contributions to the understanding or advancement of computing. It is the top award for computing in the UK, and it was especially sweet to get this recognition from the UK Computing establishment that had for so long regarded natural language processing and artificial intelligence as peripheral and suspect.Rather than moving into a quiet retirement in Oxford, in 2010, on finally completing the process of retirement from the University of Sheffield, Yorick took up a new position at the Institute for Human and Machine Cognition in Ocala, Florida, spending much of his time in Florida, while maintaining the family home in Oxford. He continued this way until COVID struck, upon which he began to work remotely which he continued until his death.In 2019–20 he took up a position at Gresham College in London to deliver a series of lectures on broader topics in artificial intelligence. These were always intended to be recorded, but delivery became primarily via video with the advent of COVID. The lectures are well worth watching, being as one would expect entertaining and thought provoking, but also prescient seen with the eyes of 2023.3 A similar reflective preoccupation with what AI is and where it may be going can be seen in his 2019 popular science book Artificial Intelligence: Modern Magic or Dangerous Future? (Wilks 2019).He never really retired: In 2022 he obtained a new grant (with a former Archbishop of Canterbury, Rowan Williams, among others) to work on Spiritual Intelligence. One theme of this was related to Margaret Masterman’s work on religious language.4 The project also gave rise to Yorick’s final book, close to publication at the time of writing, on AI and God.A recurrent theme of Wilks’ perhaps under-represented in the chronological presentation here is his interest in the pervasiveness of metaphor and metaphorical usage in language (Fass and Wilks 1983; Ballim, Wilks, and Barnden 1991). A second theme that is worth drawing out is the connection he drew between recent developments in artificial intelligence and long-standing strands of philosophy. He decried the all-too-pervasive habits of poor scholarship in computer science and computational linguistics, which often lead to missed opportunities for today’s practitioners to stand on the shoulders of giants.It is fitting that he should die during one of the periodic AI summers, when the popular media are full of overblown assessments of the limits of AI’s capabilities, even though its capabilities are massively beyond what they were in the past. One hopes that, as a result, Yorick will get the recognition he deserves for his vision and his sustained contribution to the field’s advances over such a long period.Yorick is survived by his wife Roberta Catizone (also a well-known computational linguist), son Seth, daughters Claire, Octavia, and Zoe, and two grandchildren.The world has lost a good deal of its color with his passing.},
  archive      = {J_COLI},
  author       = {Tait, John and Gaizauskas, Robert and Bontcheva, Kalina},
  doi          = {10.1162/coli_a_00485},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {767-772},
  shortjournal = {Comput. Lingu.},
  title        = {Obituary: Yorick wilks},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical methods for annotation analysis. <em>COLI</em>,
<em>49</em>(3), 763–765. (<a
href="https://doi.org/10.1162/coli_r_00483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common task in Natural Language Processing (NLP) is the development of datasets/corpora. It is the crucial initial step for initiatives aiming to train and evaluate Machine Learning and AI systems, for example. Often, these resources must be annotated with additional information (e.g., part-of-speech and named entities), which leads to the question of how to obtain these values. One of the most natural and widely used approaches is to ask for people (e.g., from untrained annotators to domain experts) to identify this information in a given text or document and possibly for more than one annotator per item. However, this is an incomplete solution. It is still necessary to obtain a final annotation per item and to measure agreement among the different annotators (or coders). Presenting a survey on this topic, Ron Artstein and Massimo Poesio published an article (“Inter-coder Agreement for Computational Linguistics”) in 2008 that addressed the mathematics and underlying assumptions of agreement coefficients (e.g., Krippendorff’s α, Scott’s π, and Cohen’s κ) and the use of coefficients in several annotation tasks. However, it left open questions, such as the interpretability of coefficients of agreement, and it did not cover topics that nowadays are important (e.g., the research within the field of statistical methods for annotation analysis, such as latent models of agreement or probabilistic annotation models). In 2022, Silviu Paun, Ron Artstein, and Massimo Poesio published a book addressing primarily the NLP community but also including other communities, such as Data Science. They intended to offer an introduction to latent models of agreement, probabilistic models of aggregation, and learning directly from multiple coders. They also reintroduced the topics presented in 2008, making an incremental contextualization. Although the reliability (agreement between coders) and the validity (the “correctness” of the annotations) are present in the entire book, it is divided into two parts. The first part covers the development of labeling scheme coefficients of agreement such as π, κ, and their variants used in NLP and AI. The second part includes methods developed to analyze the output of annotators (e.g., the most likely label for an item among those provided).Chapter 2 recaps the content presented by Artstein and Poesio (2008), updating the discussion to include recent progress. It mainly targets the coefficients of agreement and their purpose of reliability, which is a prerequisite for demonstrating the validity of a coding scheme. Also, the term “reliability” can be used in different ways: intercoder agreement (or test stability), measuring reproducibility, and accuracy. After presenting a brief context and the notation used in the book, the question “why are custom coefficients to measure agreement necessary?” is explored by reviewing chance-adjusted measures, the percentage agreement by chance, and the specific agreement coefficients. Then, the authors address some of the most common design questions in any annotation project. They start by addressing missing data caused by the coders failing to classify items (for whatever reason). In this discussion, they provide possible actions, raising pros and cons. Then, they discuss the identification of units in tasks where the coder is also required to identify the item boundaries (e.g., beginning and end of a named entity). Finally, they address severely skewed annotated data, discussing the bias problem and the prevalence problem. They finish Chapter 2 presenting proofs for the theorems presented.Chapter 3 presents agreement measures for computational linguistic annotation tasks, dividing them into three main points: methodology, choice of coefficients, and interpretation of coefficients. They describe the challenges of different annotation tasks (e.g., part-of-speech tagging, dialogue act tagging, and named entities), as well as of labeling with and without a predefined set of categories. This description of methodological choices of various studies goes along with an observation that even if a work may report agreement, it may not necessarily follow a methodology as rigorous as that envisaged by Krippendorff (2004). Concerning the choice of coefficients, they discuss the most basic and common form of coding in computational linguistics (i.e., text segment labeling with a limited number of categories), then present coding schemes with hierarchical tagsets and coding schemes with set-valued interpretations (e.g., anaphora and summarization). The discussion about coefficient interpretation looks at the range of values and different authors’ positions. They also discuss the use of weighted coefficients, arguably more appropriate for some annotation tasks, and the challenges in their interpretability.In Chapter 4 the authors present studies on how to interpret the results of reliability by rephrasing the problem as one of confidence estimation of a particular label given the behavior of the coders. The chapter starts by raising an important topic concerning the annotation: The coders easily agree about some items while other items seem more difficult to agree on. This leads to the concept of item difficulty. The items might also be viewed as latent classes, which can be modeled as the likelihood of a coder assigning a given label to an item given that item’s latent class. Considering this reformulation, the authors discuss how to measure and model the agreement (including different probability distributions) and the coders’ stability. This chapter ends Part 1 by moving the reader from an annotation task carried out by experts, which can accurately identify the labels, to a richer formulation where the interaction between both the label and the annotator may be considered. Therefore, it moves away from the simple majority choice, which ignores the accuracy and biases of coders as well the characteristics of the items.Chapter 5 focuses on the probabilistic models of annotation. It begins with a simple annotation model introducing the terminology and some key assumptions frequently made. Next, this model is extended to cover the annotation pattern of the coders. After this introduction, the authors address the issue of item difficulty and how it can affect coders’ annotation. They also discuss hierarchical priors for the annotators (which can be used to estimate annotators’ behavior when the data is scarce), how to model the characteristics of the items to discriminate between the labels, and how to have a richer model of annotator ability. Moving on, they present models where the items have inter-dependent labels (e.g., named entity recognition or information extraction tasks) and where the labels are not predefined classes (e.g., anaphoric annotations). Then, by the chapter’s end, the authors shift from encoding assumptions about the annotation process when inferring the ground-truth labels to neural networks to aggregate the annotations, using a variational autoencoder. Afterwards, they present notes on modeling other types of annotation data.Chapter 6 addresses a different source of disagreement from that presented in Chapter 5—namely the item’s difficulty, which can come from ambiguity, for example. This chapter covers methods for learning from multi-annotated corpora starting from covering the use of soft labels and the coders’ individual labels. Later, the chapter moves to distill the labels dealing with noise and pooling coder confusion. Finally, the authors finish the chapter and the book with recommendations about when to apply each method depending on the characteristics of the datasets the models are to be trained on. This chapter finishes with a summary of the lessons learned including also topics like (1) the decision aggregate or keep all the annotations, (2) crowdsourced labels versus gold labels, and (3) mixed results and what did not work for them.In summary, this book provides a complete perspective of statistical methods for annotation analysis in NLP, covering meaningful references and contextualizing them critically and historically at the same time, while also putting forth the assumptions behind the different coefficients. Moreover, the book provides several practical examples of annotation designs and how to measure their agreement. Thus, it provides an insightful perspective on what the agreement measures can and cannot do, which is present throughout the entire book. The content is suitable for both those who want to carry out research on the subject and for those who are interested in assessing reliability. From the perspective of someone who has an annotated corpus, some sections may be less interesting (i.e., specialized in different tasks), but the coverage of the various NLP tasks makes this book also a good guide for assessing reliability and validity.},
  archive      = {J_COLI},
  author       = {Wilkens, Rodrigo},
  doi          = {10.1162/coli_r_00483},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {763-765},
  shortjournal = {Comput. Lingu.},
  title        = {Statistical methods for annotation analysis},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensions of explanatory value in NLP models.
<em>COLI</em>, <em>49</em>(3), 749–761. (<a
href="https://doi.org/10.1162/coli_a_00480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance on a dataset is often regarded as the key criterion for assessing NLP models. I argue for a broader perspective, which emphasizes scientific explanation. I draw on a long tradition in the philosophy of science, and on the Bayesian approach to assessing scientific theories, to argue for a plurality of criteria for assessing NLP models. To illustrate these ideas, I compare some recent models of language production with each other. I conclude by asking what it would mean for institutional policies if the NLP community took these ideas onboard.},
  archive      = {J_COLI},
  author       = {Deemter, Kees van},
  doi          = {10.1162/coli_a_00480},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {749-761},
  shortjournal = {Comput. Lingu.},
  title        = {Dimensions of explanatory value in NLP models},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning for ancient languages: A survey.
<em>COLI</em>, <em>49</em>(3), 703–747. (<a
href="https://doi.org/10.1162/coli_a_00481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancient languages preserve the cultures and histories of the past. However, their study is fraught with difficulties, and experts must tackle a range of challenging text-based tasks, from deciphering lost languages to restoring damaged inscriptions, to determining the authorship of works of literature. Technological aids have long supported the study of ancient texts, but in recent years advances in artificial intelligence and machine learning have enabled analyses on a scale and in a detail that are reshaping the field of humanities, similarly to how microscopes and telescopes have contributed to the realm of science. This article aims to provide a comprehensive survey of published research using machine learning for the study of ancient texts written in any language, script, and medium, spanning over three and a half millennia of civilizations around the ancient world. To analyze the relevant literature, we introduce a taxonomy of tasks inspired by the steps involved in the study of ancient documents: digitization, restoration, attribution, linguistic analysis, textual criticism, translation, and decipherment. This work offers three major contributions: first, mapping the interdisciplinary field carved out by the synergy between the humanities and machine learning; second, highlighting how active collaboration between specialists from both fields is key to producing impactful and compelling scholarship; third, highlighting promising directions for future work in this field. Thus, this work promotes and supports the continued collaborative impetus between the humanities and machine learning.},
  archive      = {J_COLI},
  author       = {Sommerschield, Thea and Assael, Yannis and Pavlopoulos, John and Stefanak, Vanessa and Senior, Andrew and Dyer, Chris and Bodel, John and Prag, Jonathan and Androutsopoulos, Ion and de Freitas, Nando},
  doi          = {10.1162/coli_a_00481},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {703-747},
  shortjournal = {Comput. Lingu.},
  title        = {Machine learning for ancient languages: A survey},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grammatical error correction: A survey of the state of the
art. <em>COLI</em>, <em>49</em>(3), 643–701. (<a
href="https://doi.org/10.1162/coli_a_00478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text. The task not only includes the correction of grammatical errors, such as missing prepositions and mismatched subject–verb agreement, but also orthographic and semantic errors, such as misspellings and word choice errors, respectively. The field has seen significant progress in the last decade, motivated in part by a series of five shared tasks, which drove the development of rule-based methods, statistical classifiers, statistical machine translation, and finally neural machine translation systems, which represent the current dominant state of the art. In this survey paper, we condense the field into a single article and first outline some of the linguistic challenges of the task, introduce the most popular datasets that are available to researchers (for both English and other languages), and summarize the various methods and techniques that have been developed with a particular focus on artificial error generation. We next describe the many different approaches to evaluation as well as concerns surrounding metric reliability, especially in relation to subjective human judgments, before concluding with an overview of recent progress and suggestions for future work and remaining challenges. We hope that this survey will serve as a comprehensive resource for researchers who are new to the field or who want to be kept apprised of recent developments.},
  archive      = {J_COLI},
  author       = {Bryant, Christopher and Yuan, Zheng and Qorib, Muhammad Reza and Cao, Hannan and Ng, Hwee Tou and Briscoe, Ted},
  doi          = {10.1162/coli_a_00478},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {643-701},
  shortjournal = {Comput. Lingu.},
  title        = {Grammatical error correction: A survey of the state of the art},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-lingual transfer with language-specific subnetworks
for low-resource dependency parsing. <em>COLI</em>, <em>49</em>(3),
613–641. (<a href="https://doi.org/10.1162/coli_a_00482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large multilingual language models typically share their parameters across all languages, which enables cross-lingual task transfer, but learning can also be hindered when training updates from different languages are in conflict. In this article, we propose novel methods for using language-specific subnetworks, which control cross-lingual parameter sharing, to reduce conflicts and increase positive transfer during fine-tuning. We introduce dynamic subnetworks, which are jointly updated with the model, and we combine our methods with meta-learning, an established, but complementary, technique for improving cross-lingual transfer. Finally, we provide extensive analyses of how each of our methods affects the models.},
  archive      = {J_COLI},
  author       = {Choenni, Rochelle and Garrette, Dan and Shutova, Ekaterina},
  doi          = {10.1162/coli_a_00482},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {613-641},
  shortjournal = {Comput. Lingu.},
  title        = {Cross-lingual transfer with language-specific subnetworks for low-resource dependency parsing},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural data-to-text generation based on small datasets:
Comparing the added value of two semi-supervised learning approaches on
top of a large language model. <em>COLI</em>, <em>49</em>(3), 555–611.
(<a href="https://doi.org/10.1162/coli_a_00484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study discusses the effect of semi-supervised learning in combination with pretrained language models for data-to-text generation. It is not known whether semi-supervised learning is still helpful when a large-scale language model is also supplemented. This study aims to answer this question by comparing a data-to-text system only supplemented with a language model, to two data-to-text systems that are additionally enriched by a data augmentation or a pseudo-labeling semi-supervised learning approach. Results show that semi-supervised learning results in higher scores on diversity metrics. In terms of output quality, extending the training set of a data-to-text system with a language model using the pseudo-labeling approach did increase text quality scores, but the data augmentation approach yielded similar scores to the system without training set extension. These results indicate that semi-supervised learning approaches can bolster output quality and diversity, even when a language model is also present.},
  archive      = {J_COLI},
  author       = {van der Lee, Chris and Ferreira, Thiago Castro and Emmery, Chris and Wiltshire, Travis J. and Krahmer, Emiel},
  doi          = {10.1162/coli_a_00484},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {555-611},
  shortjournal = {Comput. Lingu.},
  title        = {Neural data-to-text generation based on small datasets: Comparing the added value of two semi-supervised learning approaches on top of a large language model},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing selective masking methods for depression detection
in social media. <em>COLI</em>, <em>49</em>(3), 525–553. (<a
href="https://doi.org/10.1162/coli_a_00479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying those at risk for depression is a crucial issue and social media provides an excellent platform for examining the linguistic patterns of depressed individuals. A significant challenge in depression classification problems is ensuring that prediction models are not overly dependent on topic keywords (i.e., depression keywords) such that it fails to predict when such keywords are unavailable. One promising approach is masking—that is, by selectively masking various words and asking the model to predict the masked words, the model is forced to learn the inherent language patterns of depression. This study evaluates seven masking techniques. Moreover, predicting the masked words during the pre-training or fine-tuning phase was also examined. Last, six class imbalanced ratios were compared to determine the robustness of masked words selection methods. Key findings demonstrate that selective masking outperforms random masking in terms of F1-score. The most accurate and robust models are identified. Our research also indicates that reconstructing the masked words during the pre-training phase is more advantageous than during the fine-tuning phase. Further discussion and implications are discussed. This is the first study to comprehensively compare masked words selection methods, which has broad implications for the field of depression classification and general NLP. Our code can be found at: https://github.com/chanapapan/Depression-Detection .},
  archive      = {J_COLI},
  author       = {Pananookooln, Chanapa and Akaranee, Jakrapop and Silpasuwanchai, Chaklam},
  doi          = {10.1162/coli_a_00479},
  journal      = {Computational Linguistics},
  number       = {3},
  pages        = {525-553},
  shortjournal = {Comput. Lingu.},
  title        = {Comparing selective masking methods for depression detection in social media},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From word types to tokens and back: A survey of approaches
to word meaning representation and interpretation. <em>COLI</em>,
<em>49</em>(2), 465–523. (<a
href="https://doi.org/10.1162/coli_a_00474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.},
  archive      = {J_COLI},
  author       = {Apidianaki, Marianna},
  doi          = {10.1162/coli_a_00474},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {465-523},
  shortjournal = {Comput. Lingu.},
  title        = {From word types to tokens and back: A survey of approaches to word meaning representation and interpretation},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The analysis of synonymy and antonymy in discourse
relations: An interpretable modeling approach. <em>COLI</em>,
<em>49</em>(2), 429–464. (<a
href="https://doi.org/10.1162/coli_a_00477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The idea that discourse relations are interpreted both by explicit content and by shared knowledge between producer and interpreter is pervasive in discourse and linguistic studies. How much weight should be ascribed in this process to the lexical semantics of the arguments is, however, uncertain. We propose a computational approach to analyze contrast and concession relations in the PDTB corpus. Our work sheds light on the question of how much lexical relations contribute to the signaling of such explicit and implicit relations, as well as on the contribution of different parts of speech to these semantic relations. This study contributes to bridging the gap between corpus and computational linguistics by proposing transparent and explainable computational models of discourse relations based on the synonymy and antonymy of their arguments.},
  archive      = {J_COLI},
  author       = {Reig Alamillo, Asela and Torres Moreno, David and Morales González, Eliseo and Toledo Acosta, Mauricio and Taroni, Antoine and Hermosillo Valadez, Jorge},
  doi          = {10.1162/coli_a_00477},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {429-464},
  shortjournal = {Comput. Lingu.},
  title        = {The analysis of synonymy and antonymy in discourse relations: An interpretable modeling approach},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Certified robustness to text adversarial attacks by
randomized [MASK]. <em>COLI</em>, <em>49</em>(2), 395–427. (<a
href="https://doi.org/10.1162/coli_a_00476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Very recently, few certified defense methods have been developed to provably guarantee the robustness of a text classifier to adversarial synonym substitutions. However, all the existing certified defense methods assume that the defenders have been informed of how the adversaries generate synonyms, which is not a realistic scenario. In this study, we propose a certifiably robust defense method by randomly masking a certain proportion of the words in an input text, in which the above unrealistic assumption is no longer necessary. The proposed method can defend against not only word substitution-based attacks, but also character-level perturbations. We can certify the classifications of over 50\% of texts to be robust to any perturbation of five words on AGNEWS, and two words on SST2 dataset. The experimental results show that our randomized smoothing method significantly outperforms recently proposed defense methods across multiple datasets under different attack algorithms.},
  archive      = {J_COLI},
  author       = {Zeng, Jiehang and Xu, Jianhan and Zheng, Xiaoqing and Huang, Xuanjing},
  doi          = {10.1162/coli_a_00476},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {395-427},
  shortjournal = {Comput. Lingu.},
  title        = {Certified robustness to text adversarial attacks by randomized [MASK]},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reflection of demographic background on word usage.
<em>COLI</em>, <em>49</em>(2), 373–394. (<a
href="https://doi.org/10.1162/coli_a_00475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The availability of personal writings in electronic format provides researchers in the fields of linguistics, psychology, and computational linguistics with an unprecedented chance to study, on a large scale, the relationship between language use and the demographic background of writers, allowing us to better understand people across different demographics. In this article, we analyze the relation between language and demographics by developing cross-demographic word models to identify words with usage bias, or words that are used in significantly different ways by speakers of different demographics. Focusing on three demographic categories, namely, location, gender, and industry, we identify words with significant usage differences in each category and investigate various approaches of encoding a word’s usage, allowing us to identify language aspects that contribute to the differences. Our word models using topic-based features achieve at least 20\% improvement in accuracy over the baseline for all demographic categories, even for scenarios with classification into 15 categories, illustrating the usefulness of topic-based features in identifying word usage differences. Further, we note that for location and industry, topics extracted from immediate context are the best predictors of word usages, hinting at the importance of word meaning and its grammatical function for these demographics, while for gender, topics obtained from longer contexts are better predictors for word usage.},
  archive      = {J_COLI},
  author       = {Garimella, Aparna and Banea, Carmen and Mihalcea, Rada},
  doi          = {10.1162/coli_a_00475},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {373-394},
  shortjournal = {Comput. Lingu.},
  title        = {Reflection of demographic background on word usage},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Onception: Active learning with expert advice for real world
machine translation. <em>COLI</em>, <em>49</em>(2), 325–372. (<a
href="https://doi.org/10.1162/coli_a_00473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Active learning can play an important role in low-resource settings (i.e., where annotated data is scarce), by selecting which instances may be more worthy to annotate. Most active learning approaches for Machine Translation assume the existence of a pool of sentences in a source language, and rely on human annotators to provide translations or post-edits, which can still be costly. In this article, we apply active learning to a real-world human-in-the-loop scenario in which we assume that: (1) the source sentences may not be readily available, but instead arrive in a stream; (2) the automatic translations receive feedback in the form of a rating, instead of a correct/edited translation, since the human-in-the-loop might be a user looking for a translation, but not be able to provide one. To tackle the challenge of deciding whether each incoming pair source–translations is worthy to query for human feedback, we resort to a number of stream-based active learning query strategies. Moreover, because we do not know in advance which query strategy will be the most adequate for a certain language pair and set of Machine Translation models, we propose to dynamically combine multiple strategies using prediction with expert advice. Our experiments on different language pairs and feedback settings show that using active learning allows us to converge on the best Machine Translation systems with fewer human interactions. Furthermore, combining multiple strategies using prediction with expert advice outperforms several individual active learning strategies with even fewer interactions, particularly in partial feedback settings.},
  archive      = {J_COLI},
  author       = {Mendonça, Vânia and Rei, Ricardo and Coheur, Luísa and Sardinha, Alberto},
  doi          = {10.1162/coli_a_00473},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {325-372},
  shortjournal = {Comput. Lingu.},
  title        = {Onception: Active learning with expert advice for real world machine translation},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradual modifications and abrupt replacements: Two
stochastic lexical ingredients of language evolution. <em>COLI</em>,
<em>49</em>(2), 301–323. (<a
href="https://doi.org/10.1162/coli_a_00471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The evolution of the vocabulary of a language is characterized by two different random processes: abrupt lexical replacements, when a complete new word emerges to represent a given concept (which was at the basis of the Swadesh foundation of glottochronology in the 1950s), and gradual lexical modifications that progressively alter words over the centuries, considered here in detail for the first time. The main discriminant between these two processes is their impact on cognacy within a family of languages or dialects, since the former modifies the subsets of cognate terms and the latter does not. The automated cognate detection, which is here performed following a new approach inspired by graph theory, is a key preliminary step that allows us to later measure the effects of the slow modification process. We test our dual approach on the family of Malagasy dialects using a cladistic analysis, which provides strong evidence that lexical replacements and gradual lexical modifications are two random processes that separately drive the evolution of languages.},
  archive      = {J_COLI},
  author       = {Pasquini, Michele and Serva, Maurizio and Vergni, Davide},
  doi          = {10.1162/coli_a_00471},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {301-323},
  shortjournal = {Comput. Lingu.},
  title        = {Gradual modifications and abrupt replacements: Two stochastic lexical ingredients of language evolution},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven cross-lingual syntax: An agreement study with
massively multilingual models. <em>COLI</em>, <em>49</em>(2), 261–299.
(<a href="https://doi.org/10.1162/coli_a_00472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Massively multilingual models such as mBERT and XLM-R are increasingly valued in Natural Language Processing research and applications, due to their ability to tackle the uneven distribution of resources available for different languages. The models’ ability to process multiple languages relying on a shared set of parameters raises the question of whether the grammatical knowledge they extracted during pre-training can be considered as a data-driven cross-lingual grammar. The present work studies the inner workings of mBERT and XLM-R in order to test the cross-lingual consistency of the individual neural units that respond to a precise syntactic phenomenon, that is, number agreement, in five languages (English, German, French, Hebrew, Russian). We found that there is a significant overlap in the latent dimensions that encode agreement across the languages we considered. This overlap is larger (a) for long- vis-à-vis short-distance agreement and (b) when considering XLM-R as compared to mBERT, and peaks in the intermediate layers of the network. We further show that a small set of syntax-sensitive neurons can capture agreement violations across languages; however, their contribution is not decisive in agreement processing.},
  archive      = {J_COLI},
  author       = {Varda, Andrea Gregor de and Marelli, Marco},
  doi          = {10.1162/coli_a_00472},
  journal      = {Computational Linguistics},
  number       = {2},
  pages        = {261-299},
  shortjournal = {Comput. Lingu.},
  title        = {Data-driven cross-lingual syntax: An agreement study with massively multilingual models},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conversational AI: Dialogue systems, conversational agents,
and chatbots by michael McTear. <em>COLI</em>, <em>49</em>(1), 257–259.
(<a href="https://doi.org/10.1162/coli_r_00470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This book has appeared in the series Synthesis Lectures on Human Language Technologies: monographs from 50 up to 150 pages about specific topics subjects in computational linguistics. The intended audience of the book are researchers and graduate students in NLP, AI, and related fields. I define myself as a computational linguist; my review is from a perspective of a “random” computational linguistics researcher wanting to learn more about this topic or looking for a good guide to teach a course on dialogue systems. I found the book very easy to read and interesting and therefore I believe that McTear fully achieved his purpose to write “a readable introduction to the various concepts, issues and technologies of Conversational AI.” He succeeds remarkably well in staying on the right level of technical details, never losing the purpose of giving an overview, and the reader does not get lost in numerous details about specific algorithms. Additionally, for people who are experts in Conversational AI, the book could still be very useful because its bibliography is exceptionally complete: a very large number of early works and recent studies are cited and commented through the whole book.The book is well structured into six chapters. After an introduction, there are two chapters about specific types of dialogue systems: rule-based systems (Chapter 2) and statistical systems (Chapter 3). This is followed by a chapter about evaluation methods (Chapter 4), after which the more recent neural end-to-end systems are reviewed (Chapter 5). The book ends with a chapter on various challenges and future directions for the research on Conversational AI (Chapter 6). I found that it was meaningful to distinguish the three types of dialogue systems: rule-based systems, statistical but modular systems, and end-to-end neural systems. It might, at first, seem strange that the topic on system evaluation methods is placed between the chapter about modular statistical dialogue systems and neural end-to-end systems, but as a reader, I believe that the discussion about system evaluation comes around at the right place in the book, because it helps to better understand the difference between modular and sequence to sequence systems. In this review, I will discuss the chapters one by one in the same order as they appear in the book.The first chapter, the introduction, explains clearly what a dialogue system is and in what cases it can be introduced to perform tasks. It sketches the historical and present context of the domain and illustrates the different types of existing systems with many examples. The chapter clearly introduces the subject of the book, but as a linguist, I have to admit that I would like to have seen a linguistic description of how a human dialogue can be characterized.The second chapter introduces rule-based systems. It provides a detailed and complete historical overview of the work in the field and shows well how the field has evolved. In particular, the diagram and the explanation about the dialogue system architecture were very helpful to understand such systems and, again, plenty of examples illustrate this chapter and make it easy to read. However, if the book were to be shortened—it is actually about 180 pages instead of the 50 to 150 that are usual in the series Synthesis Lectures on Human Language Technologies—I believe it should be in this chapter, giving slightly fewer details about historical dialogue systems.From the second to the third chapter there is a very smooth transition: Thanks to the comprehensible introduction to the modular dialogue system architecture in Chapter 2, it is easy to understand how this framework can be adapted to become a statistical system. Moreover, the text explains clearly how reinforcement learning can be used for dialogue management and, again, everything is nicely illustrated with clear examples.The fourth chapter discusses how Conversational AI can be evaluated and how training and evaluation data for systems can be collected. I particularly found the comparison between human evaluation (e.g., by using Amazon Mechanical Turk workers) and automated metrics very interesting. However, I would have liked to read a discussion about the ethical issues that can be at stake when collecting large amounts of human data on crowd-sourcing platforms. But, that marginal comment put aside, the chapter is very complete, and also provides concise descriptions of how all the subcomponents of dialogue systems can be evaluated.The fifth chapter presents end-to-end neural dialogue systems. The reader can get a very good understanding about the difference between this type of system and a modular system (be it rule-based or data-driven). Moreover, I found the explanations about technical topics such as word embeddings and recurrent neural networks rather successful: They were easy to read and the technical mechanisms used in these architectures become clear. Throughout the book, but especially in this chapter, the advantages and the disadvantages of different types of system architectures are well explained. The up-to-date bibliographic references are impressive and will, in my opinion, be a good overview for more advanced readers as well. This is also true for the enumeration of available corpora for training and evaluation data.The last chapter discusses a large number of challenges and future directions for the research on dialogue systems, for example: multi-modality, the problem of data sparseness, the handling of discourse phenomena, and ethical issues involved with Conversational AI. Although I find all the topics interesting, their large variety makes Chapter 6 very eclectic and one cannot shake the impression that it serves as a catch-all chapter for subjects that have not been addressed elsewhere in the book. I think that it should be possible to introduce a number of these discussions earlier in the book. For example, I believe that problems with handling discourse and dialogue phenomena, such as anaphora, could be addressed as the different types of systems are presented and maybe even discussed in Chapter 4 (about evaluation). The same would be true for ethical issues. For example, the discussion about how most bots having female voices could be seen as sexist (because the bot has an assisting function) could be introduced at the same time as speech generation in Chapter 2; gender-specific biases that result from biased training data could be discussed after the introduction about the corpora used to train Conversational AI (in Chapter 5). In addition, there were two small ethics topics that I missed in this book. On the one hand, the protection of customer data and privacy issues: People provide personal data through dialogue with the system and some dialogue systems of the “talking speaker type” such as Alexa and Google Home are present in people’s homes and may be exposed to sensitive information. On the other hand, the question of whether it is always ethical to refer people to a bot, instead of letting them speak to a real human. I think that if these discussions could be addressed throughout the book, Chapter 6 could just paint a clear vision of the future development of dialogue systems.In conclusion, McTear’s book provides a very clear overview of different types of dialogue systems, from the very beginning of the field to the most up-to-date research, and is very well illustrated with examples, which makes it an accessible reading for students and non-experts (provided that they have knowledge about AI or NLP). I highly recommend this book to people in search of a comprehensive overview on the topic.},
  archive      = {J_COLI},
  author       = {Seminck, Olga},
  doi          = {10.1162/coli_r_00470},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {257-259},
  shortjournal = {Comput. Lingu.},
  title        = {Conversational AI: Dialogue systems, conversational agents, and chatbots by michael McTear},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pretrained transformers for text ranking: BERT and beyond.
<em>COLI</em>, <em>49</em>(1), 253–255. (<a
href="https://doi.org/10.1162/coli_r_00468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text ranking takes a central place in Information Retrieval (IR), with Web search as its best-known application. More generally, text ranking models are applicable to any Natural Language Processing (NLP) task in which relevance of information plays a role, from filtering and recommendation applications to question answering and semantic similarity comparisons. Since the rise of BERT in 2019, Transformer models have become the most used and studied architectures in both NLP and IR, and they have been applied to basically any task in our research fields—including text ranking.In a fast-changing research context, it can be challenging to keep lecture materials up to date. Lecturers in NLP are grateful for Dan Jurafsky and James Martin for yearly updating the 3rd edition of their textbook, making Speech and Language Processing the most comprehensive, modern textbook for NLP. The IR field is less fortunate, still relying on older textbooks, extended with a collection of recent materials that address neural models. The textbook Pretrained Transformers for Text Ranking: BERT and Beyond by Jimmy Lin, Rodrigo Nogueira, and Andrew Yates is a great effort to collect the recent developments in the use of Transformers for text ranking.The introduction of the book is well-scoped with clear guidance for the reader about topics that are out of scope (such as user aspects). This is followed by an excellent history section, stating for example:We might take for granted today the idea that automatically extracted terms from a document can serve as descriptors or index terms for describing the contents of those documents, but this was an important conceptual leap in the development of information retrieval. (p. 11)Chapter 2 is a complete yet compact overview of the IR field and the context of Transformer-based ranking models, with a substantial section devoted to text collections and a good introduction to keyword search. This includes a discussion of variants of BM25 leading to different results, and a mention of pseudo-relevance feedback. Also, this chapter pays attention to terminology differences between fields, clearly aligning terms such as performance versus effectiveness.Chapter 3 is extensive. It starts with an overview of BERT. This is an excellent introduction to the topic, not as detailed as Jurafsky and Martin’s but sufficiently specific for students to understand it on a conceptual level. I will definitely include parts of these chapters in my lecture materials for master’s students. The subsequent sections, in which the behavior of BERT for ranking is further analyzed quantitatively and qualitatively with even some non-published experiments, are very extensive, and perhaps too extensive—and unnecessary—for a textbook. Section 3.3 provides a clear explanation of the challenge of ranking full documents instead of passages. Interestingly, the whole of Chapter 3 describes the history of Transformer-based ranking, but since the history is so recent, it is all still relevant. To my taste, however, the summaries of individual papers with result tables are in places too detailed. I do appreciate that the authors spoke to the authors of some of these papers, sometimes even leading to a better explanation of the original methods. Sections 3.4 and 3.5 have good discussions of effectiveness–efficiency trade-offs, which is an important topic in text ranking because search engines cannot permit themselves long query latencies. Section 3.5 has an interesting discussion of sequence-to-sequence approaches to classification and ranking tasks.In Chapter 4, applications of Transformer methods for document and query expansion are discussed. This is a topic that gets less attention in the literature than Transformer-based rankers, but it is important to address because it can be a very effective and, importantly, efficient strategy. Moreover, it gives interesting insights in what aspects of document content are the most important in relevance ranking. Especially the discussion of doc2query-T5 is insightful in this respect. The later sections of Chapter 4 are again very detailed and report experimental results from a number of papers; I would personally prefer to read more intuitions, and see examples and visual explanations of some of the most important methods instead.Chapter 5 addresses representation learning for ranking, and in Section 5.1 the authors clearly put this task in the context of other NLP tasks, with a very relevant conclusion: A wide range of tasks can be represented as a concatenation of two texts and then fed into a BERT model, and this appears to be a successful paradigm in many contexts. In the words of the authors:When faced with these myriad tasks, a natural question would be: Do these distinctions matter? With BERT, the answer is, likely not. (p. 201)Section 5.2 relates text ranking with dense vectors to nearest neighbor search; indicating how efficient this approach is at query time because the documents are encoded at index time. After some history in Section 5.3, Section 5.4 provides a good explanation of bi-encoder architectures for retrieval (starting with SentenceBERT). Personally, I think the distinction between cross-encoders and bi-encoders could have been explained earlier in the book. Although cross-encoders reach higher retrieval effectiveness, bi-encoders are much more efficient at inference time and therefore too important to be described this late in the book. ColBERT, explained clearly in 5.5.2, is a key example of a model that has the efficiency of bi-encoders but approaches the effectiveness of cross-encoders.Chapter 6 makes explicit that some related (potentially relevant) topics were omitted from the book on purpose, in particular, question answering, summarization, and conversational (interactive) search. Then it extensively goes into open research questions, which I interpret as an invitation and encouragement to the community for topics to dive into. One topic that I think should have received more attention is the application of Transformer models in domains other than the general domain, in particular, legal and medical. For these domains, more ranking benchmarks have become available in recent years. We have seen that for some of those benchmarks—especially in query-by-document retrieval like case law retrieval or prior art retrieval, it is challenging to beat the lexical retrieval baseline (BM25), because there is less training data than in the common benchmarks, and queries and documents are much longer.A challenge with writing a textbook about recent research topics is that it becomes outdated the moment it is published. I hope the authors will publish regular updates as science progresses. Overall, I think this book is a valuable resource that I have already recommended to my Ph.D. students, and that definitely provides me with materials for my master’s courses. It is also relevant for NLP researchers dealing with text ranking problems—basically any task in which information relevance plays a role. This textbook could be the ideal cross-over from IR knowledge to the NLP field, using our common friend BERT as the bridge.},
  archive      = {J_COLI},
  author       = {Verberne, Suzan},
  doi          = {10.1162/coli_r_00468},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {253-255},
  shortjournal = {Comput. Lingu.},
  title        = {Pretrained transformers for text ranking: BERT and beyond},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Validity, reliability, and significance: Empirical methods
for NLP and data science. <em>COLI</em>, <em>49</em>(1), 249–251. (<a
href="https://doi.org/10.1162/coli_r_00467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When we come up with a new model in NLP and machine learning more generally, we usually look at some performance metric (one number), compare it against the same performance metric for a strong baseline model (one number), and if the new model gets a better number, we mark it in bold and declare it the winner. For anyone with a background in statistics or a field where conclusions must be drawn on the basis of noisy data, this procedure is frankly shocking. Suppose model A gets a BLEU score one point higher than model B: Is that difference reliable? If you used a slightly different dataset for training and evaluation, would that one point difference still hold? Would the difference even survive running the same models on the same datasets but with different random seeds? In fields such as psychology and biology, it is standard to answer such questions using standardized statistical procedures to make sure that differences of interest are larger than some quantification of measurement noise. Making a claim based on a bare difference of two numbers is unthinkable. Yet statistical procedures remain rare in the evaluation of NLP models, whose performance metrics are arguably just as noisy.To these objections, NLP practitioners can respond that they have faithfully followed the hallowed train-(dev-)test split paradigm. As long as proper test set discipline has been followed, the theory goes, the evaluation is secure: By testing on held-out data, we can be sure that our models are performing well in a way that is independent of random accidents of the training data, and by testing on that data only once, we guard against making claims based on differences that would not replicate if we ran the models again. But does the train-test split paradigm really guard against all problems of validity and reliability?Into this situation comes the book under review, Validity, Reliability, and Significance: Empirical Methods for NLP and Data Science, by Stefan Riezler and Michael Hagmann. The authors argue that the train-test split paradigm does not in fact insulate NLP from problems relating to the validity and reliability of its models, their features, and their performance metrics. They present numerous case studies to prove their point, and advocate and teach standard statistical methods as the solution, with rich examples of successful application to problems of NLP system evaluation and interpretation. The statistical methods are presented with motivation from first principles, with deep citations into the fundamental statistical literature and formal derivations presented at the right level for an average NLP or ML practitioner to follow. The methods and theory behind them are frequentist, and represent the current best practices within that paradigm, based on linear mixed-effects models (LMEMs) and generalized additive models (GAMs), and using generalized likelihood ratio tests for hypothesis testing. The authors also discuss fundamental philosophical ideas including the theory of measurement and construct validity, providing an overall deeper-than-average view into this literature in a succinct, comprehensible form.The book takes the form of a kind of taxonomy of problems that can occur in NLP analysis, with each kind of problem paired with statistical solutions to diagnose and treat it, along with case studies involving Natural Language Inference (NLI), biomedical NLP, and Machine Translation (MT) evaluation, among others. Problems are divided into Validity, Robustness, and Significance.Validity (discussed in Chapter 2) refers to the extent to which a model captures real generalizable predictive information based on input features. The authors divide feature validity problems into three kinds: (1) bias features, which are spurious features that are correlated with labels in the training data; (2) illegitimate features, where features contain unexpected and non-generalizable information, such as numerical patient IDs containing information about which hospital a patient was treated in, which in turn contains information about what conditions the patient likely has; and (3) circular features, which are features in the training data that deterministically define the label. Notably, these issues are not guarded against by the train-test split paradigm. Although these three kinds of invalidity may seem difficult to distinguish, they can in fact be cleanly separated by different statistical analyses, which the authors motivate after a discussion of measurement theory and construct validity.Of these, the most interesting analyses are those to diagnose illegitimate features and circular features. Illegitimate features are cast as a form of failure of transformation invariance. Based on measurement theory, the relationships between features and outcomes should be subject to certain symmetries: For example, if patient IDs are to be used as features in a model to predict medical conditions, then the effect of the patient ID on outcomes should be invariant to bijections applied to the patient ID. Any failure of this invariance is an indication that illegitimate information is present in the patient IDs: For example, if some of the digits in the ID indicate which hospital the patient was treated at. Circular features, on the other hand, are diagnosed through a procedure involving fitting GAMs to predict labels given increasing amounts of data; in the limit of large data, any circular feature will end up with a large weight in the GAM while the rest of the feature weights go to zero. The authors call this the “nullification criterion,” and give extensive examples of the application of these ideas using biomedical examples.Reliability (discussed in Chapter 3) is the extent to which a result is robust to replication—for example, if a new set of annotators is tasked with labeling the same data, how much inter-annotator agreement will there be; or if a model is run again with a different random seed or a perturbation of hyperparameters, how likely is it to give the same performance. The authors critically discuss Krippendorf’s α as a measure of intra-annotator reliability, and they discuss using bootstrapping to calculate confidence intervals. The real meat of this chapter is the discussion of model-based reliability tests based on LMEMs to perform Variance Component Analysis (focusing on the random effects). These methods are applied to study reliability of data annotation of MT output in terms of error rate. Reliability of predictions with respect to hyperparameters is also discussed extensively, with LMEM analysis using hyperparameters as predictors.Significance (discussed in Chapter 4) is the extent to which a result provides evidence against some null hypothesis, usually that there is no difference between two models. The authors’ main goal in this chapter is to advocate a general likelihood ratio test performed on LMEMs as the most general and effective significance test. The idea is to compare a model class M1, consisting of models predicting some outcome given a set of input features, against another model class M0, which predicts the same outcome from all the same input features except for some held-out features F, representing the null hypothesis that the features F have no effect. The goodness-of-fit of these models is compared using a likelihood ratio test based on the best-fitting models within each class, and the null hypothesis M0 can be rejected if the likelihood ratio falls in a certain range. In addition to the generalized likelihood ratio test, the authors discuss nonparameteric significance tests such as permutation tests, and include a great deal of useful statistical background on topics such as the Central Limit Theorem and the basic logic of significance testing.One content area that NLP practitioners may be hoping for in this book, but which they will not find, is Bayesian methods as opposed to frequentist methods. Within psychology, over the last decade, there has been an increasing interest in Bayesian approaches to statistical inference, fueled in part by perceived flaws in the frequentist paradigm, for example, (1) difficulties in interpreting frequentist statistics such as p-values and confidence intervals, which can be counter-intuitive; and (2) an over-reliance on significance thresholds for drawing conclusions which leads to “p-hacking,” where a researcher performs data analyses in ways that are unscrupulously targeted toward achieving significance thresholds. Without taking a stance on whether or not these arguments have merit, I think it is worth pointing out that the current book does not much engage with the arguments against frequentism that are currently on the rise, seeing these issues as out of scope.Overall, the book provides a useful introduction to the universe of statistical testing and measurement theory in a way that is immediately applicable for analysis of NLP models. Many of the methods advocated here can be applied straightforwardly, yielding immediate improvements in terms of our confidence in the validity and reliability of our models. More fundamentally, I believe the field of NLP will benefit from engagement with the underlying ideas about validity and reliability that have originated in fields such as psychology; the book provides an ample introduction to these ideas that cuts right to their core.},
  archive      = {J_COLI},
  author       = {Futrell, Richard},
  doi          = {10.1162/coli_r_00467},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {249-251},
  shortjournal = {Comput. Lingu.},
  title        = {Validity, reliability, and significance: Empirical methods for NLP and data science},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-state text processing. <em>COLI</em>, <em>49</em>(1),
245–247. (<a href="https://doi.org/10.1162/coli_r_00466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in popularity of neural network methods in computational linguistics has led to a richness of valuable books on the topic. On the other hand, there is arguably a shortage of recent materials on foundational computational linguistics methods like finite-state technologies. This is unfortunate, as finite-state approaches not only still find much use in applications for speech and text processing (the core focus of this book), but seem also to be valuable in interpreting and improving neural methods. Moreover, the study of finite-state machines (and their corresponding formal languages) is still proving insightful in theoretical linguistics analyses. In this sense, this book by Gorman and Sproat is a welcome, refreshing contribution aimed at a variety of readers.The book is organized in eight main chapters, and can be conceptually divided into two parts. The first half of the book serves as an introduction to core concepts in formal language and automata theory (Chapter 1), the basic design principles of the Python library used through the book (Chapter 2), and a variety of finite-state algorithms (Chapters 3 and 4). The rest of the book exemplifies the formal notions of the previous chapters with practical applications of finite-state technologies to linguistic problems like morphophonological analysis and text normalization (Chapter 5, 6, 7). The last chapter (Chapter 8) presents an interesting discussion of future steps from a variety of perspectives, connecting finite-state methods to current trends in the field. In what follows, I discuss the contents of each chapter in more detail.Chapter 1 provides an accessible introduction to formal languages and automata theory, starting with a concise but helpful historical review of the development of finite-state technologies and their ties to linguistics. The chapter balances intuitive explanations of technical concepts without sacrificing formal rigor, in particular in the presentation of finite-state automata/transducers and their formal definitions. Weighted finite-state automata play a prominent role in the book and the authors introduce them algebraically through the notion of semiring (Pin 1997). This is a welcome choice that makes the book somewhat unique among introductory/application-oriented materials on these topics. Unsurprisingly, this section is the densest part of the chapter and it could have benefitted from an explanation of the intuition behind the connection between wellformedness of a string, recognition by an automaton, and paths over semirings—especially considering how relevant such concepts become when the book transitions into its algorithmic parts. However, the chapter lays down these theoretical notions clearly, and its stands as an intriguing starting point for any reader interested in learning more about the mathematics of monoids and formal languages.Chapter 2 is essentially a short manual for Pynini (Gorman 2016), the Python library used in the rest of the book. The reader will appreciate how the design choices behind Pynini are grounded in the formal notions introduced in the previous chapter. In this sense, the chapter can surely be used as a quick and dry reference list of Python commands, but it goes beyond that and it would serve as a nice pedagogical tool to highlight connections between implementational choices and mathematical definitions—a perk that carries on consistently throughout the book.Chapters 3 and 4 are a walkthrough of key algorithms for finite-state machines. Chapter 3 focuses on essential operations such as concatenation, closure, union, composition, and so forth. The generalization of each algorithm from finite-state machines to weighted finite-state machines is also highlighted. Chapter 4 presents slightly more advanced algorithms, focusing, for example, on the problem of optimization. The optimization section comes with a nice practical example from speech recognition, and speech serves as a good motivation to lead into algorithms to compute shortest distance and shortest paths. One possible complaint is that the discussion of a few of these algorithms could have been improved with a step-by-step example of the procedure, although examples of these can be easily found in other resources. Overall, it is in these chapters that the uniqueness of the book begins to shine, as the algorithms are presented starting from the theoretical automaton operation at their core, then illustrated by constructing the relevant automata, and finally exemplified with code snippets in Pynini.Chapter 5 presents a discussion of phonological rewrite rules in the Sound Pattern of English (SPE) tradition (Chomsky and Halle 1968). The chapter is grounded in the history of rewrite rules in linguistics broadly, and then focuses on the characteristics of SPE rules and their connection to rational relations and finite-state transducers. The authors introduce Pynini’s approach to the implementation of operations like rule compilation, application, and mechanisms for rule interaction. Then, the chapter presents three practical use cases to exemplify the usefulness of rewrite rules: Spanish grapheme-to-phoneme conversion, vowel harmony Finnish case suffixed, and text normalization within the context of currency expression tagging. All three examples lend themselves to be used as practice exercises, should the reader be inclined to, but more importantly well illustrate the advantages of finite-state technology when approaching problems of this type.Chapter 6 continues on the line of reasoning started in the previous chapter. Here, the authors focus on motivating the use of finite-state approaches to morphology, once again adding a richness of insightful historical references. Readers not familiar with morphological analysis will still find the chapter easy to follow, as standard terminology is introduced together with theoretical and practical concerns. As in the previous chapter, this one ends with a few examples of Pynini scripts for specific linguistic problems, such as Russian noun morphology, Tagalog infixation, and Yowlume verbal morphology. The Yowlume data are particularly interesting, and succeed in displaying both the expressivity of finite-state transducers and the flexibility with which Pynini can handle them.Chapter 7 concludes the series of application-oriented chapters, with an overview of use cases including, among others, fuzzy string matching and text generation. In particular, the characterization of noise in terms of transducers across a few different applications is going to be informative for readers new to these technologies. The examples presented in this chapter are also more “practical” than the ones in chapters 5 and 6, concerned less with theoretical analyses and more with solving problems that might arise for existing applications (e.g., T9 disambiguation). In this sense, the three chapters balance each other quite nicely, and illustrate a broad range of possible domains of use for the concepts introduced in the first half of the book.Finally, Chapter 8 discusses the future of finite-state approaches in the landscape of current trends in computational linguistics (i.e., deep learning methods). The authors pragmatically acknowledge the question of the relevance of finite-state technologies today, and clearly map connections between the concepts in the book and neural methods. In line with the approach taken in the rest of the book, the chapter alternates between practical and broader conceptual questions, including a relevant survey of ongoing work on hybridization and possible open venues of research in the theory of formal languages.Several appendixes make up the coda of the book, with pointers to Pynini extensions, additional implemented examples, and nice troubleshooting tidbits. Additionally, each chapter comes with its own list of suggested further readings, expanding on a generally impressive list of references.In sum, the book provides a valuable introduction to finite-state technologies, with an eye to practical software implementation. Alternating linguistic examples with formal definitions and code samples makes both the formal notation and Pynini’s syntactic conventions accessible to a casual reader, and in doing so the authors highlight the importance of understanding theoretical foundations when approaching implementational problems. The well-constructed synthesis of these different perspectives makes the book appropriate for a variety of readers and goals. I can imagine it used, for instance, as a pedagogical tool for introductory courses, a go-to manual for practitioners needing a quick software reference, and a practical guide for theoretically minded researchers dipping their toes in software implementation.},
  archive      = {J_COLI},
  author       = {De Santo, Aniello},
  doi          = {10.1162/coli_r_00466},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {245-247},
  shortjournal = {Comput. Lingu.},
  title        = {Finite-state text processing},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curing the SICK and other NLI maladies. <em>COLI</em>,
<em>49</em>(1), 199–243. (<a
href="https://doi.org/10.1162/coli_a_00465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Against the backdrop of the ever-improving Natural Language Inference (NLI) models, recent efforts have focused on the suitability of the current NLI datasets and on the feasibility of the NLI task as it is currently approached. Many of the recent studies have exposed the inherent human disagreements of the inference task and have proposed a shift from categorical labels to human subjective probability assessments, capturing human uncertainty. In this work, we show how neither the current task formulation nor the proposed uncertainty gradient are entirely suitable for solving the NLI challenges. Instead, we propose an ordered sense space annotation, which distinguishes between logical and common-sense inference. One end of the space captures non-sensical inferences, while the other end represents strictly logical scenarios. In the middle of the space, we find a continuum of common-sense, namely, the subjective and graded opinion of a “person on the street.” To arrive at the proposed annotation scheme, we perform a careful investigation of the SICK corpus and we create a taxonomy of annotation issues and guidelines. We re-annotate the corpus with the proposed annotation scheme, utilizing four symbolic inference systems, and then perform a thorough evaluation of the scheme by fine-tuning and testing commonly used pre-trained language models on the re-annotated SICK within various settings. We also pioneer a crowd annotation of a small portion of the MultiNLI corpus, showcasing that it is possible to adapt our scheme for annotation by non-experts on another NLI corpus. Our work shows the efficiency and benefits of the proposed mechanism and opens the way for a careful NLI task refinement.},
  archive      = {J_COLI},
  author       = {Kalouli, Aikaterini-Lida and Hu, Hai and Webb, Alexander F. and Moss, Lawrence S. and de Paiva, Valeria},
  doi          = {10.1162/coli_a_00465},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {199-243},
  shortjournal = {Comput. Lingu.},
  title        = {Curing the SICK and other NLI maladies},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Annotation error detection: Analyzing the past and present
for a more coherent future. <em>COLI</em>, <em>49</em>(1), 157–198. (<a
href="https://doi.org/10.1162/coli_a_00464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Annotated data is an essential ingredient in natural language processing for training and evaluating machine learning models. It is therefore very desirable for the annotations to be of high quality. Recent work, however, has shown that several popular datasets contain a surprising number of annotation errors or inconsistencies. To alleviate this issue, many methods for annotation error detection have been devised over the years. While researchers show that their approaches work well on their newly introduced datasets, they rarely compare their methods to previous work or on the same datasets. This raises strong concerns on methods’ general performance and makes it difficult to assess their strengths and weaknesses. We therefore reimplement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling. In addition, we define a uniform evaluation setup including a new formalization of the annotation error detection task, evaluation protocol, and general best practices. To facilitate future research and reproducibility, we release our datasets and implementations in an easy-to-use and open source software package.1},
  archive      = {J_COLI},
  author       = {Klie, Jan-Christoph and Webber, Bonnie and Gurevych, Iryna},
  doi          = {10.1162/coli_a_00464},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {157-198},
  shortjournal = {Comput. Lingu.},
  title        = {Annotation error detection: Analyzing the past and present for a more coherent future},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). It takes two flints to make a fire: Multitask learning of
neural relation and explanation classifiers. <em>COLI</em>,
<em>49</em>(1), 117–156. (<a
href="https://doi.org/10.1162/coli_a_00463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose an explainable approach for relation extraction that mitigates the tension between generalization and explainability by jointly training for the two goals. Our approach uses a multi-task learning architecture, which jointly trains a classifier for relation extraction, and a sequence model that labels words in the context of the relations that explain the decisions of the relation classifier. We also convert the model outputs to rules to bring global explanations to this approach. This sequence model is trained using a hybrid strategy: supervised, when supervision from pre-existing patterns is available, and semi-supervised otherwise. In the latter situation, we treat the sequence model’s labels as latent variables, and learn the best assignment that maximizes the performance of the relation classifier. We evaluate the proposed approach on the two datasets and show that the sequence model provides labels that serve as accurate explanations for the relation classifier’s decisions, and, importantly, that the joint training generally improves the performance of the relation classifier. We also evaluate the performance of the generated rules and show that the new rules are a great add-on to the manual rules and bring the rule-based system much closer to the neural models.},
  archive      = {J_COLI},
  author       = {Tang, Zheng and Surdeanu, Mihai},
  doi          = {10.1162/coli_a_00463},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {117-156},
  shortjournal = {Comput. Lingu.},
  title        = {It takes two flints to make a fire: Multitask learning of neural relation and explanation classifiers},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformers and the representation of biomedical background
knowledge. <em>COLI</em>, <em>49</em>(1), 73–115. (<a
href="https://doi.org/10.1162/coli_a_00462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Specialized transformers-based models (such as BioBERT and BioMegatron) are adapted for the biomedical domain based on publicly available biomedical corpora. As such, they have the potential to encode large-scale biological knowledge. We investigate the encoding and representation of biological knowledge in these models, and its potential utility to support inference in cancer precision medicine—namely, the interpretation of the clinical significance of genomic alterations. We compare the performance of different transformer baselines; we use probing to determine the consistency of encodings for distinct entities; and we use clustering methods to compare and contrast the internal properties of the embeddings for genes, variants, drugs, and diseases. We show that these models do indeed encode biological knowledge, although some of this is lost in fine-tuning for specific tasks. Finally, we analyze how the models behave with regard to biases and imbalances in the dataset.},
  archive      = {J_COLI},
  author       = {Wysocki, Oskar and Zhou, Zili and O’Regan, Paul and Ferreira, Deborah and Wysocka, Magdalena and Landers, Dónal and Freitas, André},
  doi          = {10.1162/coli_a_00462},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {73-115},
  shortjournal = {Comput. Lingu.},
  title        = {Transformers and the representation of biomedical background knowledge},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensional modeling of emotions in text with appraisal
theories: Corpus creation, annotation reliability, and prediction.
<em>COLI</em>, <em>49</em>(1), 1–72. (<a
href="https://doi.org/10.1162/coli_a_00461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An important observation for natural language processing is that emotions can be communicated implicitly by referring to events alone, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with their own goals, and so forth. Such appraisals explain which emotions are developed based on an event, for example, that a novel situation can induce surprise or one with uncertain consequences could evoke fear. We analyze the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, we compile a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, we ask readers to reconstruct emotions and appraisals from the text. This set-up allows us to measure if emotions and appraisals can be recovered purely from text and provides a human baseline to judge a model’s performance measures. Our comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. Therefore, appraisals constitute an alternative computational emotion analysis paradigm and further improve the categorization of emotions in text with joint models.},
  archive      = {J_COLI},
  author       = {Troiano, Enrica and Oberländer, Laura and Klinger, Roman},
  doi          = {10.1162/coli_a_00461},
  journal      = {Computational Linguistics},
  number       = {1},
  pages        = {1-72},
  shortjournal = {Comput. Lingu.},
  title        = {Dimensional modeling of emotions in text with appraisal theories: Corpus creation, annotation reliability, and prediction},
  volume       = {49},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
